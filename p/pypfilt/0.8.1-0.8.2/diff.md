# Comparing `tmp/pypfilt-0.8.1-py3-none-any.whl.zip` & `tmp/pypfilt-0.8.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,37 @@
-Zip file size: 106682 bytes, number of entries: 37
--rw-rw-r--  2.0 unx    19223 b- defN 23-Oct-10 23:56 pypfilt/__init__.py
--rw-rw-r--  2.0 unx     8742 b- defN 23-Jul-10 00:36 pypfilt/adaptive.py
--rw-rw-r--  2.0 unx    39195 b- defN 23-Oct-12 05:55 pypfilt/build.py
--rw-rw-r--  2.0 unx    18277 b- defN 23-Jul-05 08:21 pypfilt/cache.py
--rw-rw-r--  2.0 unx     4813 b- defN 23-Jul-05 08:21 pypfilt/crps.py
--rw-rw-r--  2.0 unx     2775 b- defN 23-Oct-12 22:42 pypfilt/event.py
--rw-rw-r--  2.0 unx    28001 b- defN 23-Feb-02 12:18 pypfilt/io.py
--rw-rw-r--  2.0 unx    10724 b- defN 23-Jul-10 00:36 pypfilt/model.py
--rw-rw-r--  2.0 unx    18794 b- defN 23-Jul-07 04:26 pypfilt/obs.py
--rw-rw-r--  2.0 unx    18608 b- defN 23-Oct-12 22:41 pypfilt/pfilter.py
--rw-rw-r--  2.0 unx    31075 b- defN 23-Jul-06 23:48 pypfilt/plot.py
--rw-rw-r--  2.0 unx    13010 b- defN 23-Oct-12 22:55 pypfilt/resample.py
--rw-rw-r--  2.0 unx     5154 b- defN 23-Jan-31 22:50 pypfilt/sampler.py
--rw-rw-r--  2.0 unx    16771 b- defN 23-Jul-10 00:36 pypfilt/scenario.py
--rw-rw-r--  2.0 unx    21945 b- defN 23-Oct-12 05:02 pypfilt/state.py
--rw-rw-r--  2.0 unx     8517 b- defN 23-Jan-31 22:50 pypfilt/stats.py
--rw-rw-r--  2.0 unx    63760 b- defN 23-Oct-12 05:03 pypfilt/summary.py
--rw-rw-r--  2.0 unx    22742 b- defN 23-Jul-07 06:11 pypfilt/time.py
--rw-rw-r--  2.0 unx        0 b- defN 22-May-06 12:25 pypfilt/examples/__init__.py
--rw-rw-r--  2.0 unx      946 b- defN 22-Aug-22 02:36 pypfilt/examples/gaussian_walk.toml
--rw-rw-r--  2.0 unx     9170 b- defN 23-Jan-31 22:50 pypfilt/examples/lorenz.py
--rw-rw-r--  2.0 unx      311 b- defN 22-Sep-01 11:25 pypfilt/examples/predation-counts-x-datetime.ssv
--rw-rw-r--  2.0 unx      182 b- defN 22-Sep-01 11:26 pypfilt/examples/predation-counts-x.ssv
--rw-rw-r--  2.0 unx      311 b- defN 22-Sep-01 11:26 pypfilt/examples/predation-counts-y-datetime.ssv
--rw-rw-r--  2.0 unx      182 b- defN 22-Sep-01 11:26 pypfilt/examples/predation-counts-y.ssv
--rw-rw-r--  2.0 unx     1656 b- defN 23-Jan-31 05:14 pypfilt/examples/predation-datetime.toml
--rw-rw-r--  2.0 unx    14309 b- defN 23-Jan-31 22:50 pypfilt/examples/predation.py
--rw-rw-r--  2.0 unx     1635 b- defN 23-Jan-31 05:14 pypfilt/examples/predation.toml
--rw-rw-r--  2.0 unx     1966 b- defN 23-Jan-31 22:50 pypfilt/examples/simple.py
--rw-rw-r--  2.0 unx    14298 b- defN 23-Jan-31 22:50 pypfilt/examples/sir.py
--rw-rw-r--  2.0 unx     1545 b- defN 23-Jul-05 06:45 pypfilt/examples/sir.toml
--rw-rw-r--  2.0 unx     1499 b- defN 23-Oct-12 23:44 pypfilt-0.8.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     4734 b- defN 23-Oct-12 23:44 pypfilt-0.8.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Oct-12 23:44 pypfilt-0.8.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx        8 b- defN 23-Oct-12 23:44 pypfilt-0.8.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx        1 b- defN 22-Dec-06 23:36 pypfilt-0.8.1.dist-info/zip-safe
--rw-rw-r--  2.0 unx     2980 b- defN 23-Oct-12 23:44 pypfilt-0.8.1.dist-info/RECORD
-37 files, 407951 bytes uncompressed, 101996 bytes compressed:  75.0%
+Zip file size: 105883 bytes, number of entries: 35
+-rw-r--r--  2.0 unx    19221 b- defN 20-Feb-02 00:00 pypfilt/__init__.py
+-rw-r--r--  2.0 unx     8742 b- defN 20-Feb-02 00:00 pypfilt/adaptive.py
+-rw-r--r--  2.0 unx    39166 b- defN 20-Feb-02 00:00 pypfilt/build.py
+-rw-r--r--  2.0 unx    18881 b- defN 20-Feb-02 00:00 pypfilt/cache.py
+-rw-r--r--  2.0 unx     4813 b- defN 20-Feb-02 00:00 pypfilt/crps.py
+-rw-r--r--  2.0 unx     2775 b- defN 20-Feb-02 00:00 pypfilt/event.py
+-rw-r--r--  2.0 unx    28010 b- defN 20-Feb-02 00:00 pypfilt/io.py
+-rw-r--r--  2.0 unx    10722 b- defN 20-Feb-02 00:00 pypfilt/model.py
+-rw-r--r--  2.0 unx    18788 b- defN 20-Feb-02 00:00 pypfilt/obs.py
+-rw-r--r--  2.0 unx    18606 b- defN 20-Feb-02 00:00 pypfilt/pfilter.py
+-rw-r--r--  2.0 unx    30979 b- defN 20-Feb-02 00:00 pypfilt/plot.py
+-rw-r--r--  2.0 unx    13007 b- defN 20-Feb-02 00:00 pypfilt/resample.py
+-rw-r--r--  2.0 unx     2580 b- defN 20-Feb-02 00:00 pypfilt/sampler.py
+-rw-r--r--  2.0 unx    16706 b- defN 20-Feb-02 00:00 pypfilt/scenario.py
+-rw-r--r--  2.0 unx    21959 b- defN 20-Feb-02 00:00 pypfilt/state.py
+-rw-r--r--  2.0 unx     8515 b- defN 20-Feb-02 00:00 pypfilt/stats.py
+-rw-r--r--  2.0 unx    63744 b- defN 20-Feb-02 00:00 pypfilt/summary.py
+-rw-r--r--  2.0 unx    22706 b- defN 20-Feb-02 00:00 pypfilt/time.py
+-rw-r--r--  2.0 unx        0 b- defN 20-Feb-02 00:00 pypfilt/examples/__init__.py
+-rw-r--r--  2.0 unx      946 b- defN 20-Feb-02 00:00 pypfilt/examples/gaussian_walk.toml
+-rw-r--r--  2.0 unx    11007 b- defN 20-Feb-02 00:00 pypfilt/examples/lorenz.py
+-rw-r--r--  2.0 unx      311 b- defN 20-Feb-02 00:00 pypfilt/examples/predation-counts-x-datetime.ssv
+-rw-r--r--  2.0 unx      182 b- defN 20-Feb-02 00:00 pypfilt/examples/predation-counts-x.ssv
+-rw-r--r--  2.0 unx      311 b- defN 20-Feb-02 00:00 pypfilt/examples/predation-counts-y-datetime.ssv
+-rw-r--r--  2.0 unx      182 b- defN 20-Feb-02 00:00 pypfilt/examples/predation-counts-y.ssv
+-rw-r--r--  2.0 unx     1656 b- defN 20-Feb-02 00:00 pypfilt/examples/predation-datetime.toml
+-rw-r--r--  2.0 unx    14306 b- defN 20-Feb-02 00:00 pypfilt/examples/predation.py
+-rw-r--r--  2.0 unx     1635 b- defN 20-Feb-02 00:00 pypfilt/examples/predation.toml
+-rw-r--r--  2.0 unx     1966 b- defN 20-Feb-02 00:00 pypfilt/examples/simple.py
+-rw-r--r--  2.0 unx    14328 b- defN 20-Feb-02 00:00 pypfilt/examples/sir.py
+-rw-r--r--  2.0 unx     1545 b- defN 20-Feb-02 00:00 pypfilt/examples/sir.toml
+?rw-r--r--  2.0 unx     4713 b- defN 20-Feb-02 00:00 pypfilt-0.8.2.dist-info/METADATA
+?rw-r--r--  2.0 unx       87 b- defN 20-Feb-02 00:00 pypfilt-0.8.2.dist-info/WHEEL
+?rw-r--r--  2.0 unx     1499 b- defN 20-Feb-02 00:00 pypfilt-0.8.2.dist-info/licenses/LICENSE
+?rw-r--r--  2.0 unx     2813 b- defN 20-Feb-02 00:00 pypfilt-0.8.2.dist-info/RECORD
+35 files, 407407 bytes uncompressed, 101469 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -87,26 +87,20 @@
 
 Filename: pypfilt/examples/sir.py
 Comment: 
 
 Filename: pypfilt/examples/sir.toml
 Comment: 
 
-Filename: pypfilt-0.8.1.dist-info/LICENSE
+Filename: pypfilt-0.8.2.dist-info/METADATA
 Comment: 
 
-Filename: pypfilt-0.8.1.dist-info/METADATA
+Filename: pypfilt-0.8.2.dist-info/WHEEL
 Comment: 
 
-Filename: pypfilt-0.8.1.dist-info/WHEEL
+Filename: pypfilt-0.8.2.dist-info/licenses/LICENSE
 Comment: 
 
-Filename: pypfilt-0.8.1.dist-info/top_level.txt
-Comment: 
-
-Filename: pypfilt-0.8.1.dist-info/zip-safe
-Comment: 
-
-Filename: pypfilt-0.8.1.dist-info/RECORD
+Filename: pypfilt-0.8.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pypfilt/__init__.py

```diff
@@ -226,15 +226,15 @@
     if ctx.settings['files']['delete_cache_file_before_forecast']:
         cache.remove_cache_file(cache_file)
 
     # Load the most recently cached simulation state that is consistent with
     # the current observations.
     update = cache.load_state(cache_file, ctx, forecast_times)
     if update is not None:
-        for (key, value) in update.items():
+        for key, value in update.items():
             sim[key] = value
 
     # Update the forecasting times.
     if not sim['fs_times']:
         logger.warning(
             'All {} forecasting times precede cached state'.format(
                 len(forecast_times)
```

## pypfilt/build.py

```diff
@@ -273,15 +273,15 @@
         """
         Return a generator that yields a sequence of time-step numbers and
         times (represented as tuples) that span the simulation period.
 
         This sequence does not include the start of the simulation period.
         """
         steps_per_unit = self.settings['time']['steps_per_unit']
-        for (step_num, time_step) in self.time_steps():
+        for step_num, time_step in self.time_steps():
             if step_num % steps_per_unit == 0:
                 yield (step_num, time_step.end)
 
     def summary_times(self):
         """
         Return a generator that yields the sequence of time-step numbers and
         times (represented as tuples) for which summary statistics will be
@@ -293,15 +293,15 @@
         steps_per_unit = self.settings['time']['steps_per_unit']
         summary_freq = self.settings['time']['summaries_per_unit']
         modulos = step_subset_modulos(steps_per_unit, summary_freq)
 
         # Start at the initial state (time-step zero).
         yield (0, self.settings['time']['sim_start'])
 
-        for (step_num, time_step) in self.time_steps():
+        for step_num, time_step in self.time_steps():
             if step_num % steps_per_unit in modulos:
                 yield (step_num, time_step.end)
 
     def summary_count(self):
         """
         Return the number of time-steps for which summary statistics will be
         calculated.
@@ -383,24 +383,24 @@
         settings['sampler'],
         component['random']['prior'],
         settings['filter']['particles'],
         prior_table,
         external_samples,
     )
     # Verify that all sampled values are finite.
-    for (name, values) in data['prior'].items():
+    for name, values in data['prior'].items():
         if not np.all(np.isfinite(values)):
             msg = 'Non-finite prior samples for "{}"'
             raise ValueError(msg.format(name))
 
     # Create lookup tables.
     component['lookup'] = {}
     data['lookup'] = {}
     lookup_tables = build_lookup_tables(settings, component['time'])
-    for (name, (lookup_data, lookup_table)) in lookup_tables.items():
+    for name, (lookup_data, lookup_table) in lookup_tables.items():
         component['lookup'][name] = lookup_table
         data['lookup'][name] = lookup_data
 
     # Store lookup indices for each sample lookup table.
     data['lookup_ixs'] = sample_lookup_columns(
         ctx.settings,
         ctx.component['random']['hist_lookup_cols'],
@@ -412,15 +412,15 @@
     if obs_tables is None:
         obs_tables = load_observations(
             settings, component['obs'], component['time']
         )
     data['obs'] = obs_tables
 
     # Collect all of the observations into a single flat list.
-    for (obs_unit, obs_table) in data['obs'].items():
+    for obs_unit, obs_table in data['obs'].items():
         obs_model = component['obs'][obs_unit]
         for row in obs_table:
             ctx.all_observations.append(obs_model.row_into_obs(row))
 
     # Create summary monitors and tables.
     component['summary_monitor'] = build_summary_monitors(settings)
     component['summary_table'] = build_summary_tables(settings)
@@ -488,15 +488,15 @@
     ...     print('Cannot replace b.c with a dictionary')
     Cannot replace b.c with a dictionary
     >>> print(data)
     {'a': 1, 'b': {'c': 3}, 'x': {'y': {'z': 'Hello'}}}
     """
     keys = list(keys)
     last_ix = len(keys) - 1
-    for (ix, key) in enumerate(keys):
+    for ix, key in enumerate(keys):
         # Insert the value when we reach the final key.
         if last_ix == ix:
             table[key] = value
             return
 
         if key in table:
             if not isinstance(table[key], dict):
@@ -702,15 +702,15 @@
     # Record the defined and used reservoirs.
     used_reservoirs = set()
     known_reservoirs = set()
 
     start_ix = 0
     n_parts = len(partitions)
 
-    for (ix, partition) in enumerate(partitions):
+    for ix, partition in enumerate(partitions):
         for key in ['weight', 'particles']:
             if key not in partition:
                 msg = 'Partition #{} has no field "{}"'.format(ix + 1, key)
                 raise ValueError(msg)
 
         # Ensure the partition weight lies within the unit interval.
         weight = partition['weight']
@@ -895,22 +895,22 @@
     """
     Return samples from the prior distribution for parameters whose values are
     contained in external data files.
     """
     external_prior = get_external_prior_table(settings)
     external_samples = {}
 
-    for (name, config) in external_prior.items():
+    for name, config in external_prior.items():
         if 'table' in config and 'column' in config:
             # Read samples from a plain-text file.
             filename = os.path.join(
                 settings['files']['input_directory'], config['table']
             )
             column = config['column']
-            table_data = read_table(filename, [(column, np.float_)])
+            table_data = read_table(filename, [(column, np.float64)])
             external_samples[name] = table_data[column]
 
         elif 'hdf5' in config and 'dataset' in config and 'column' in config:
             # Read samples from an HDF5 dataset.
             filename = os.path.join(
                 settings['files']['input_directory'], config['hdf5']
             )
@@ -937,15 +937,15 @@
     if table_defns is None:
         return {}
 
     data_dir = get_chained(settings, ['files', 'input_directory'])
     data_path = pathlib.Path(data_dir)
     tables = {}
 
-    for (table_name, file_name) in table_defns.items():
+    for table_name, file_name in table_defns.items():
         if not isinstance(file_name, str):
             try:
                 file_name = file_name['file']
             except KeyError:
                 msg_fmt = 'Lookup table "{}" has no file name'
                 raise ValueError(msg_fmt.format(table_name)) from None
         file_path = data_path / file_name
@@ -958,15 +958,15 @@
 
 def sample_lookup_columns(settings, prng, lookup_tables):
     """
     Return the sampled lookup indices for each sample lookup.
     """
     table_defns = settings.get('lookup_tables', {})
     sample_names = []
-    for (table_name, file_name) in table_defns.items():
+    for table_name, file_name in table_defns.items():
         try:
             do_sample = file_name.get('sample_values', False)
             if do_sample:
                 sample_names.append(table_name)
         except AttributeError:
             pass
 
@@ -994,15 +994,15 @@
     Return the observation models.
     """
     obs_model_defns = settings.get('observations')
     if obs_model_defns is None:
         return {}
 
     obs_models = {}
-    for (obs_unit, obs_settings) in obs_model_defns.items():
+    for obs_unit, obs_settings in obs_model_defns.items():
         # Ensure that an observation model is defined.
         om_name = obs_settings.get('model')
         if om_name is None:
             msg_fmt = 'No observation model defined for {}'
             raise ValueError(msg_fmt.format(obs_unit))
 
         # Instantiate the observation model.
@@ -1020,15 +1020,15 @@
     logger = logging.getLogger(__name__)
 
     data_dir = get_chained(settings, ['files', 'input_directory'])
     data_path = pathlib.Path(data_dir)
 
     obs_tables = {}
 
-    for (obs_unit, obs_model) in obs_models.items():
+    for obs_unit, obs_model in obs_models.items():
         obs_settings = get_chained(settings, ['observations', obs_unit])
         # Skip observation models that have no associated data file.
         if 'file' not in obs_settings:
             msg_fmt = 'No data file for {} observations'
             logger.debug(msg_fmt.format(obs_unit))
             continue
 
@@ -1054,15 +1054,15 @@
     logger = logging.getLogger(__name__)
 
     monitor_defns = get_chained(settings, ['summary', 'monitors'])
     if monitor_defns is None:
         return {}
 
     monitors = {}
-    for (name, monitor_settings) in monitor_defns.items():
+    for name, monitor_settings in monitor_defns.items():
         # Ensure that a monitor is defined.
         monitor_name = monitor_settings.get('component')
         if monitor_name is None:
             # NOTE: allow settings to be defined for monitors that are created
             # by custom summary components, but print a warning.
             msg_fmt = 'No component defined for monitor {}'
             logger.warning(msg_fmt.format(name))
@@ -1082,15 +1082,15 @@
     logger = logging.getLogger(__name__)
 
     table_defns = get_chained(settings, ['summary', 'tables'])
     if table_defns is None:
         return {}
 
     tables = {}
-    for (name, table_settings) in table_defns.items():
+    for name, table_settings in table_defns.items():
         # Ensure that a table is defined.
         table_name = table_settings.get('component')
         if table_name is None:
             # NOTE: allow settings to be defined for tables that are created
             # by custom summary components, but print a warning.
             msg_fmt = 'No component defined for table {}'
             logger.warning(msg_fmt.format(name))
@@ -1158,10 +1158,10 @@
     except TypeError:
         logger = logging.getLogger(__name__)
         logger.error(
             'Attempted to call "{}" with arguments:'.format(full_name)
         )
         for arg in args:
             print('    {}'.format(arg))
-        for (name, value) in kwargs.items():
+        for name, value in kwargs.items():
             print('    {} = {}'.format(name, value))
         raise
```

## pypfilt/cache.py

```diff
@@ -5,14 +5,15 @@
 import h5py
 import numpy as np
 import os.path
 import signal
 import tempfile
 import tomli
 import tomli_w
+import warnings
 
 from .io import load_dataset, save_dataset
 from .state import History
 
 
 def default(ctx, forecast_times):
     """
@@ -193,15 +194,15 @@
     :param rngs: A dictionary that maps names (strings) to random number
         generators.
     """
     dt = h5py.string_dtype(encoding='utf-8')
     dtype = [('name', dt), ('state', dt)]
     shape = (len(rngs),)
     dataset = group.create_dataset(dataset_name, shape, dtype=dtype)
-    for (ix, (name, rng)) in enumerate(rngs.items()):
+    for ix, (name, rng) in enumerate(rngs.items()):
         state = tomli_w.dumps(rng.bit_generator.state)
         dataset[ix] = (name, state)
 
 
 def load_rng_states(group, dataset_name, rngs):
     """
     Restore random number generator states from a cached dataset.
@@ -212,15 +213,15 @@
         generators, whose states will be updated.
 
     :raises ValueError: if there are missing names or additional names in the
         dataset.
     """
     dataset = group[dataset_name][()]
     names = set()
-    for (name, state) in dataset:
+    for name, state in dataset:
         name = name.decode(encoding='utf-8')
         state = state.decode(encoding='utf-8')
         if name not in rngs:
             raise ValueError('Unknown PRNG {}'.format(name))
         names.add(name)
         rngs[name].bit_generator.state = tomli.loads(state)
     rng_names = set(rngs.keys())
@@ -228,15 +229,15 @@
         unknown = rng_names - names
         raise ValueError('No saved state for PRNGs {}'.format(unknown))
 
 
 def __save_data_tables(ctx, hdf5_file, group, data, path=None):
     if path is None:
         path = []
-    for (name, value) in data.items():
+    for name, value in data.items():
         if isinstance(value, dict):
             if value:
                 # Nested, non-empty dictionary.
                 subgroup = group.require_group(name)
                 path.append(name)
                 __save_data_tables(ctx, hdf5_file, subgroup, value, path)
         elif isinstance(value, np.ndarray):
@@ -283,15 +284,15 @@
         logger.debug("Missing cache file: '{}'".format(cache_file))
         return None
 
     try:
         with h5py.File(cache_file, 'r') as f:
             logger.debug("Reading cache file: '{}'".format(cache_file))
             return __find_most_recent_time(ctx, forecast_times, f)
-    except IOError:
+    except OSError:
         logger.debug("Could not read cache file: '{}'".format(cache_file))
         return None
 
 
 def load_state_at_time(cache_file, ctx, when):
     """
     Load the particle history matrix from a cache file at a specific
@@ -331,15 +332,15 @@
             }
             summary.load_state(ctx, group['summary'])
             # Restore the state of the random number generators.
             load_rng_states(group, 'prng_states', ctx.component['random'])
             # Inform the model that we're resuming from a cached state.
             model.resume_from_cache(ctx)
             return result
-    except IOError:
+    except OSError:
         logger.debug("Could not read cache file: '{}'".format(cache_file))
         return None
 
 
 def __find_most_recent_time(ctx, forecast_times, hdf5_file):
     time = ctx.component['time']
     summary = ctx.component['summary']
@@ -461,14 +462,28 @@
                 return False
         elif isinstance(ctx_val, np.ndarray) and isinstance(
             cache_val, h5py.Dataset
         ):
             # Compare data tables using native time values.
             cache_tbl = load_dataset(time, cache_val)
 
+            # Detect non-finite values by comparing each data table to itself.
+            # Note that we apply this check *before* truncating observation
+            # tables.
+            if not np.array_equal(cache_tbl, cache_tbl):
+                warnings.warn(
+                    f'{cache_val.name}: non-finite values in cache',
+                    stacklevel=1,
+                )
+            if not np.array_equal(ctx_val, ctx_val):
+                warnings.warn(
+                    f'{cache_val.name}: non-finite values in context',
+                    stacklevel=1,
+                )
+
             # Detect whether we are comparing observation tables.
             # NOTE: the use of 'obs' to identify observation tables.
             obs_prefix = '/{}/data/obs/'.format(cache_time)
             is_obs = cache_val.name.startswith(obs_prefix)
 
             if is_obs:
                 # NOTE: we only want to compare observations up to the cache
```

## pypfilt/crps.py

```diff
@@ -20,15 +20,15 @@
     if len(true_values) != samples_table.shape[0]:
         raise ValueError('incompatible dimensions')
     return np.fromiter(
         (
             crps_edf_scalar(truth, samples_table[ix])
             for (ix, truth) in enumerate(true_values)
         ),
-        dtype=np.float_,
+        dtype=np.float64,
     )
 
 
 def crps_edf_scalar(true_value, sample_values):
     r"""
     Calculate the CRPS for samples drawn from a predictive distribution for a
     single value, using the probability weighted moment CRPS estimator.
@@ -95,20 +95,20 @@
     score_rows = len(time_combs)
     time_dtype = true_obs.dtype.fields['time'][0]
     scores = np.zeros(
         (score_rows,),
         dtype=[
             ('time', time_dtype),
             ('fs_time', time_dtype),
-            ('score', np.float_),
+            ('score', np.float64),
         ],
     )
 
     # Calculate each CRPS score in turn.
-    for (ix, (fs_time, time)) in enumerate(time_combs):
+    for ix, (fs_time, time) in enumerate(time_combs):
         # Ensure there is only a single true value for this time.
         true_mask = true_obs['time'] == time
         true_value = true_obs['value'][true_mask]
         true_count = len(true_value)
         if true_count != 1:
             msg_fmt = 'Found {} true values for {}'
             raise ValueError(msg_fmt.format(true_count, time))
```

## pypfilt/io.py

```diff
@@ -49,26 +49,27 @@
     :raises ValueError: if ``columns`` contains a string column.
 
     :Examples:
 
     >>> from pypfilt.io import date_column, read_table
     >>> import numpy as np
     >>> import datetime
-    >>> path = "input_data.ssv"
+    >>> path = 'input_data.ssv'
     >>> with open(path, 'w') as f:
-    ...    _ = f.write('time value\\n')
-    ...    _ = f.write('2020-01-01 1\\n')
-    ...    _ = f.write('2020-01-02 3\\n')
-    ...    _ = f.write('2020-01-03 5\\n')
-    >>> columns = [date_column('time'), ('value', np.int_)]
+    ...     _ = f.write('time value\\n')
+    ...     _ = f.write('2020-01-01 1\\n')
+    ...     _ = f.write('2020-01-02 3\\n')
+    ...     _ = f.write('2020-01-03 5\\n')
+    >>> columns = [date_column('time'), ('value', np.int64)]
     >>> data = read_table(path, columns)
     >>> isinstance(data['time'][0], datetime.datetime)
     True
-    >>> observations = [{'time': row['time'], 'value': row['value']}
-    ...                 for row in data]
+    >>> observations = [
+    ...     {'time': row['time'], 'value': row['value']} for row in data
+    ... ]
     >>> # Remove the input file when it is no longer needed.
     >>> import os
     >>> os.remove(path)
     """
 
     # Ensure there are no string fields.
     for column in columns:
@@ -158,16 +159,17 @@
     The following function reads a time series of floating-point values.
 
     .. code-block:: python
 
        import numpy as np
        import pypfilt.io
 
+
        def load_time_series(self, filename, time_scale):
-           fields = [pypfilt.io.time_field('time'), ('value', np.float_)]
+           fields = [pypfilt.io.time_field('time'), ('value', np.float64)]
            return pypfilt.io.read_fields(time_scale, filename, fields)
 
     .. _scalar: https://numpy.org/doc/stable/reference/arrays.scalars.html
     """
     # Convert time fields into ``(name, type, converter)`` tuples for use with
     # read_table().
     columns = []
@@ -236,18 +238,18 @@
     :param encoding: The name of the encoding used to decode the file content.
 
     :Examples:
 
     >>> from pypfilt.io import read_lookup_table, lookup
     >>> from pypfilt.time import Datetime
     >>> import datetime
-    >>> path = "input_data.ssv"
+    >>> path = 'input_data.ssv'
     >>> with open(path, 'w') as f:
-    ...    _ = f.write('time value1 value2 value3\\n')
-    ...    _ = f.write('2020-01-01 1.0 1.5 2.0\\n')
+    ...     _ = f.write('time value1 value2 value3\\n')
+    ...     _ = f.write('2020-01-01 1.0 1.5 2.0\\n')
     >>> time = Datetime()
     >>> table = read_lookup_table(path, time)
     >>> isinstance(table['time'][0], datetime.datetime)
     True
     >>> when = datetime.datetime(2020, 1, 1)
     >>> values = lookup(table, when)
     >>> len(values.shape) == 1
@@ -373,15 +375,15 @@
 
     Use this function to define summary table fields that contain time values.
 
     :Examples:
 
     >>> import numpy as np
     >>> from pypfilt.io import time_field
-    >>> fields = [time_field('time'), ('value', np.float_)]
+    >>> fields = [time_field('time'), ('value', np.float64)]
     """
     return (name, 'TIME')
 
 
 def string_field(name):
     """
     Return a ``(name, type)`` tuple that identifies a field as containing
@@ -390,15 +392,15 @@
     Use this function to define summary table fields that contain string
     values.
 
     :Examples:
 
     >>> import numpy as np
     >>> from pypfilt.io import string_field
-    >>> fields = [string_field('parameter_name'), ('value', np.float_)]
+    >>> fields = [string_field('parameter_name'), ('value', np.float64)]
     """
     return (name, h5py.string_dtype())
 
 
 def fields_dtype(time_scale, fields):
     """
     Return a NumPy data type (dtype) object that describes the provided data
```

## pypfilt/model.py

```diff
@@ -168,15 +168,15 @@
         each particle state vector, allowing for individual fields to be
         scalar or multi-dimensional.
 
         :param state_vec: An array of particle state vectors.
         :raises ValueError: if there are nested fields or non-float fields.
         """
         num_fields = 0
-        for (dtype, _offset) in state_vec.dtype.fields.values():
+        for dtype, _offset in state_vec.dtype.fields.values():
             if not np.issubdtype(dtype.base, float):
                 raise ValueError(f'Unsupported field type: {dtype.base}')
             if len(dtype.shape) == 0:
                 num_fields += 1
             elif len(dtype.shape) == 1:
                 num_fields += dtype.shape[0]
             else:
@@ -198,22 +198,22 @@
     def _d_dt(self, time, xt, ctx, is_forecast, dtype, num_fields):
         """
         A wrapper around the right-hand side function that reshapes the input
         and output arrays.
         """
         x_statevec = xt.view(dtype)
         rhs = self.d_dt(time, x_statevec, ctx, is_forecast)
-        return rhs.view((np.float_, num_fields)).reshape(-1)
+        return rhs.view((np.float64, num_fields)).reshape(-1)
 
     def update(self, ctx, time_step, is_forecast, prev, curr):
         num_fields = self._num_fields(prev)
         time_range = self._time_range(ctx, time_step)
         default_method = getattr(self, 'ode_method', 'RK45')
         method = ctx.get_setting(['model', 'ode_method'], default_method)
-        prev_xt = prev.view((np.float_, num_fields)).reshape(-1)
+        prev_xt = prev.view((np.float64, num_fields)).reshape(-1)
         kwargs = ctx.get_setting(['model', 'ode_solver_options'], {})
         result = scipy.integrate.solve_ivp(
             self._d_dt,
             time_range,
             prev_xt,
             args=(ctx, is_forecast, prev.dtype, num_fields),
             method=method,
@@ -264,15 +264,15 @@
             mini_curr = curr.copy()
 
             # Note that we need to substitute the time scale component.
             # This ensures that if the model uses any time methods, such as
             # to_scalar(), it will be consistent with the mini-step scale.
             ctx.component['time'] = mini_scale
             # Simulate each mini-step.
-            for (_mini_step_num, mini_step) in mini_scale.steps():
+            for _mini_step_num, mini_step in mini_scale.steps():
                 update_method(
                     self, ctx, mini_step, is_fs, mini_prev, mini_curr
                 )
                 mini_prev, mini_curr = mini_curr, mini_prev
             # Restore the original time scale component.
             ctx.component['time'] = full_scale
```

## pypfilt/obs.py

```diff
@@ -178,16 +178,17 @@
         :Example:
 
         .. code-block:: python
 
            import numpy as np
            import pypfilt.io
 
+
            def from_file(self, filename, time_scale):
-               fields = [pypfilt.io.time_field('time'), ('value', np.float_)]
+               fields = [pypfilt.io.time_field('time'), ('value', np.float64)]
                return pypfilt.io.read_fields(time_scale, filename, fields)
         """
         pass
 
     @abc.abstractmethod
     def row_into_obs(self, row):
         """
@@ -301,15 +302,14 @@
     >>> # Define a Gaussian observation model with a known standard deviation.
     >>> class MyObsModel(Univariate):
     ...     def distribution(self, ctx, snapshot):
     ...         # Directly observe the state variable 'x'.
     ...         expect = snapshot.state_vec['x']
     ...         sdev = self.settings['parameters']['sdev']
     ...         return scipy.stats.norm(loc=expect, scale=self.sdev)
-    ...
     >>> observation_unit = 'x'
     >>> settings = {'parameters': {'sdev': 0.1}}
     >>> obs_model = MyObsModel(observation_unit, settings)
     >>> obs_model.unit
     'x'
 
     The observation model shown in the example above can then be used in a
@@ -444,15 +444,15 @@
         :param time_scale: The simulation time scale.
         :param time_col: The name of the observation time column; this will be
             renamed to ``'time'``.
         :param value_col: The name of the observation value column; this will
             be renamed to ``'value'``.
         """
         # Load the data table.
-        fields = [io.time_field(time_col), (value_col, np.float_)]
+        fields = [io.time_field(time_col), (value_col, np.float64)]
         df = io.read_fields(time_scale, filename, fields)
         # Rename the columns to 'time' and 'value'.
         rename_to = {
             time_col: 'time',
             value_col: 'value',
         }
         new_names = tuple(
@@ -478,15 +478,15 @@
         """
         return (obs['time'], obs['value'])
 
     def simulated_field_types(self, ctx):
         """
         Return the field types for simulated observations.
         """
-        cols = [io.time_field('time'), ('value', np.float_)]
+        cols = [io.time_field('time'), ('value', np.float64)]
         return cols
 
     def simulated_obs(self, ctx, snapshot, rng):
         """
         Return a simulated observation with fields ``'time'`` and ``'value'``.
         """
         values = self.simulate(ctx, snapshot, rng)
@@ -517,15 +517,15 @@
     # observation models). So we need to trust the observation models to
     # provide valid initial bounds, rather than checking them here.
     cdf_lower = cdf_fn(y_lower)
 
     bounds_lower = {pr: y_lower for pr in probs}
     bounds_upper = {pr: y_upper for pr in probs}
     qtls = np.zeros(probs.shape)
-    for (ix, pr) in enumerate(probs):
+    for ix, pr in enumerate(probs):
         if cdf_lower >= pr:
             # The lower bound is the first value to meet or exceed this
             # threshold, so we've found y_i for this quantile.
             qtls[ix] = y_lower
             continue
 
         # Use a binary search to find y_i this quantile.
```

## pypfilt/pfilter.py

```diff
@@ -397,15 +397,15 @@
     win_start = start
     # The beginning of the current time-step.
     prev_time = start
 
     # Simulate each time-step.
     # NOTE: the first time-step is number 1 and updates history.matrix[1] from
     # history.matrix[0].
-    for (step_num, time_step, obs) in steps:
+    for step_num, time_step, obs in steps:
         history.set_time_step(step_num, time_step.end)
         # Check whether the end of the history matrix has been reached.
         # If so, shift the sliding window forward in time.
         if history.reached_window_end():
             # Calculate summary statistics in blocks.
             # If most_recent is None, no time-steps have been simulated.
             # This can occur when (for example) a forecasting simulation has
```

## pypfilt/plot.py

```diff
@@ -256,37 +256,37 @@
 
     def __init__(self, data, xlbl, ylbl, **kwargs):
         self.__df = data
         self.__xlbl = xlbl
         self.__ylbl = ylbl
         # Ensure that self.axs is a 2-dimensional array (row x column).
         kwargs['squeeze'] = False
-        super(Single, self).__init__(ncols=1, nrows=1, **kwargs)
+        super().__init__(ncols=1, nrows=1, **kwargs)
 
     def expand_x_lims(self, col, pad_frac=0.05, pad_abs=None):
         """
         Increase the range of the x-axis, relative to the plot data.
 
         :param col: The column name for the x-axis data.
         :param pad_frac: The fractional increase in range.
         :param pad_abs: The absolute increase in range.
         """
         xs = self.__df[col]
-        super(Single, self).expand_x_lims(xs, pad_frac, pad_abs)
+        super().expand_x_lims(xs, pad_frac, pad_abs)
 
     def expand_y_lims(self, col, pad_frac=0.05, pad_abs=None):
         """
         Increase the range of the y-axis, relative to the plot data.
 
         :param col: The column name for the y-axis data.
         :param pad_frac: The fractional increase in range.
         :param pad_abs: The absolute increase in range.
         """
         ys = self.__df[col]
-        super(Single, self).expand_y_lims(ys, pad_frac, pad_abs)
+        super().expand_y_lims(ys, pad_frac, pad_abs)
 
     def subplots(self, hide_axes=False, dx=0.055, dy=0.025):
         """
         Return an iterator that yields ``(axes, data)`` tuples for each
         subplot.
 
         :param hide_axes: Whether to hide x and y axes that are not on their
@@ -321,37 +321,37 @@
         self.__xfac = xfac
         self.__yfac = yfac
         self.__xfac_lvls = sorted(np.unique(data[xfac[0]]))
         self.__yfac_lvls = sorted(np.unique(data[yfac[0]]))
         nc, nr = len(self.__xfac_lvls), len(self.__yfac_lvls)
         # Ensure that self.axs is a 2-dimensional array (row x column).
         kwargs['squeeze'] = False
-        super(Grid, self).__init__(ncols=nc, nrows=nr, **kwargs)
+        super().__init__(ncols=nc, nrows=nr, **kwargs)
 
     def expand_x_lims(self, col, pad_frac=0.05, pad_abs=None):
         """
         Increase the range of the x-axis, relative to the plot data.
 
         :param col: The column name for the x-axis data.
         :param pad_frac: The fractional increase in range.
         :param pad_abs: The absolute increase in range.
         """
         xs = self.__df[col]
-        super(Grid, self).expand_x_lims(xs, pad_frac, pad_abs)
+        super().expand_x_lims(xs, pad_frac, pad_abs)
 
     def expand_y_lims(self, col, pad_frac=0.05, pad_abs=None):
         """
         Increase the range of the y-axis, relative to the plot data.
 
         :param col: The column name for the y-axis data.
         :param pad_frac: The fractional increase in range.
         :param pad_abs: The absolute increase in range.
         """
         ys = self.__df[col]
-        super(Grid, self).expand_y_lims(ys, pad_frac, pad_abs)
+        super().expand_y_lims(ys, pad_frac, pad_abs)
 
     def subplots(self, hide_axes=False, dx=0.055, dy=0.025):
         """
         Return an iterator that yields ``(axes, data)`` tuples for each
         subplot.
 
         :param hide_axes: Whether to hide x and y axes that are not on their
@@ -438,37 +438,37 @@
             nc = np.ceil(self.__num_lvls / nr).astype(int)
         else:
             nr = np.ceil(self.__num_lvls / nc).astype(int)
         self.__num_rows = nr
         self.__num_cols = nc
         # Ensure that self.axs is a 2-dimensional array (row x column).
         kwargs['squeeze'] = False
-        super(Wrap, self).__init__(ncols=nc, nrows=nr, **kwargs)
+        super().__init__(ncols=nc, nrows=nr, **kwargs)
 
     def expand_x_lims(self, col, pad_frac=0.05, pad_abs=None):
         """
         Increase the range of the x-axis, relative to the plot data.
 
         :param col: The column name for the x-axis data.
         :param pad_frac: The fractional increase in range.
         :param pad_abs: The absolute increase in range.
         """
         xs = self.__df[col]
-        super(Wrap, self).expand_x_lims(xs, pad_frac, pad_abs)
+        super().expand_x_lims(xs, pad_frac, pad_abs)
 
     def expand_y_lims(self, col, pad_frac=0.05, pad_abs=None):
         """
         Increase the range of the y-axis, relative to the plot data.
 
         :param col: The column name for the y-axis data.
         :param pad_frac: The fractional increase in range.
         :param pad_abs: The absolute increase in range.
         """
         ys = self.__df[col]
-        super(Wrap, self).expand_y_lims(ys, pad_frac, pad_abs)
+        super().expand_y_lims(ys, pad_frac, pad_abs)
 
     def subplots(self, hide_axes=False, dx=0.055, dy=0.025):
         """
         Return an iterator that yields ``(axes, data)`` tuples for each
         subplot.
 
         :param hide_axes: Whether to hide x and y axes that are not on their
```

## pypfilt/resample.py

```diff
@@ -125,15 +125,15 @@
             return
 
     # Sample the multivariate normal with covariance V and mean of zero.
     std_samples = rnd.normal(size=(num_params, count))
     scaled_samples = np.transpose(np.dot(a_mat, h * std_samples))
 
     # Add the sampled noise and clip to respect parameter bounds.
-    for (ix, name) in enumerate(smooth_fields):
+    for ix, name in enumerate(smooth_fields):
         min_val = bounds[name].get('min')
         max_val = bounds[name].get('max')
         new_px['state_vec'][name] += scaled_samples[:, ix]
         if min_val is not None or max_val is not None:
             # There are lower and/or upper bounds to apply.
             new_px['state_vec'][name] = np.clip(
                 new_px['state_vec'][name], min_val, max_val
@@ -293,19 +293,19 @@
     elif method == 'stratified':
         choices = (rnd.uniform(size=count) + np.arange(count)) / count
     elif method == 'deterministic':
         choices = (rnd.uniform() + np.arange(count)) / count
     else:
         raise ValueError("Invalid resampling method '{}'".format(method))
     # Construct an array to record the index of each resampled particle.
-    new_ixs = np.zeros(count, dtype=np.int_)
+    new_ixs = np.zeros(count, dtype=np.int64)
     # Calculate the upper bounds for each interval.
     # NOTE: normalise bounds to span the unit interval.
     bounds = np.cumsum(sorted_ws) / np.sum(sorted_ws)
     # Since the intervals and random samples are both monotonic increasing, we
     # only need step through the samples and record the current interval.
     bounds_ix = 0
-    for (j, rand_val) in enumerate(choices):
+    for j, rand_val in enumerate(choices):
         while bounds[bounds_ix] < rand_val:
             bounds_ix += 1
         new_ixs[j] = sorted_ix[bounds_ix]
     return new_ixs
```

## pypfilt/sampler.py

```diff
@@ -1,12 +1,11 @@
 """Construct sampling functions for each model parameter."""
 
 import abc
 import lhs
-import logging
 import numpy as np
 
 from .build import lookup
 
 
 class Base(abc.ABC):
     """
@@ -23,81 +22,14 @@
         :param particles: The number of particles.
         :param prior: The prior distribution table.
         :param sampled: Sampled values for specific parameters.
         """
         pass
 
 
-def sample_from(prng, particles, fn_name, args):
-    """
-    Return a function that draws samples from the specified distribution.
-
-    :param fn_name: The name of a ``numpy.random.Generator`` method used to
-        generate samples.
-    :param args: A dictionary of keyword arguments.
-
-    As a special case, ``fn_name`` may be set to ``'inverse_uniform'`` to
-    sample from a uniform distribution and then take the reciprocal:
-
-    .. math:: X \\sim \\frac{1}{\\mathcal{U}(a, b)}
-
-    The bounds ``a`` and ``b`` may be specified by the following keyword
-    arguments:
-
-    + :code:`a = args['low']` **or** :code:`a = 1 / args['inv_low']`
-    + :code:`b = args['high']` **or** :code:`b = 1 / args['inv_high']`
-    """
-    if fn_name == 'inverse_uniform':
-        if 'low' in args:
-            low = args['low']
-        else:
-            low = 1 / args['inv_low']
-
-        if 'high' in args:
-            high = args['high']
-        else:
-            high = 1 / args['inv_high']
-
-        if low > high:
-            # Return consistent outputs for NumPy < 1.21 and NumPy >= 1.21.
-            # See https://github.com/numpy/numpy/pull/17921 for details.
-            return -1 / prng.uniform(low=-low, high=-high, size=particles)
-
-        return 1 / prng.uniform(low=low, high=high, size=particles)
-    else:
-        return getattr(prng, fn_name)(**args, size=particles)
-
-
-def validate_prior(prior) -> None:
-    """
-    Ensure each prior comprises a function name and arguments dictionary,
-    otherwise raise a ValueError.
-
-    Note that this doesn't enforce that each prior corresponds to a known
-    model parameter, because this would prevent us from supporting prior
-    distributions that are expressed in terms of **transformed** parameters
-    (such as reciprocals of rate parameters).
-
-    :param prior: The prior distribution table.
-    """
-    logger = logging.getLogger(__name__)
-    for (name, info) in prior.items():
-        if 'name' not in info:
-            raise ValueError('Missing prior name for {}'.format(name))
-        elif not isinstance(info['name'], str):
-            raise ValueError('Invalid prior name for {}'.format(name))
-        if 'args' not in info:
-            raise ValueError('Missing prior arguments for {}'.format(name))
-        elif not isinstance(info['args'], dict):
-            raise ValueError('Invalid prior arguments for {}'.format(name))
-        if len(info) != 2:
-            extra_keys = [k for k in info if k not in ['name', 'args']]
-            logger.warning('Extra prior keys for %s: %s', name, extra_keys)
-
-
 class LatinHypercube(Base):
     """
     Draw parameter samples using Latin hypercube sampling.
     """
 
     def draw_samples(self, settings, prng, particles, prior, sampled):
         """
```

## pypfilt/scenario.py

```diff
@@ -154,26 +154,26 @@
     >>> import pypfilt.examples.predation
     >>> pypfilt.examples.predation.write_example_files()
     >>> forecast_times = [1.0, 3.0, 5.0, 7.0, 9.0]
     >>> config_file = 'predation.toml'
     >>> data_file = 'output.hdf5'
     >>> for instance in pypfilt.load_instances(config_file):
     ...     context = instance.build_context()
-    ...     state = pypfilt.forecast(context, forecast_times,
-    ...                              filename=data_file)
+    ...     state = pypfilt.forecast(
+    ...         context, forecast_times, filename=data_file
+    ...     )
     >>> # Remove the output file when it is no longer needed.
     >>> import os
     >>> os.remove(data_file)
     """
     for spec in load_specifications(sources):
         for scenario in scenarios(spec):
-            for instance in instances(scenario):
-                # NOTE: this is where the job of this module ends,
-                # and the job of Context begins.
-                yield instance
+            # NOTE: this is where the job of this module ends, and the job of
+            # Context begins.
+            yield from instances(scenario)
 
 
 def load_toml(source):
     """
     Read `TOML`_ content from ``source`` and return the parsed dictionary and
     the `TOML`_ input.
 
@@ -226,15 +226,15 @@
     Iterate over the scenarios in the provided specification ``spec``.
 
     :param spec: The scenario specifications.
     :type spec: Specification
 
     :rtype: Iterator[Scenario]
     """
-    for (scenario_id, scenario_dict) in spec.scenario_settings.items():
+    for scenario_id, scenario_dict in spec.scenario_settings.items():
         # Construct the scenario settings by applying scenario-specific
         # settings on top of the global settings.
         global_dict = copy.deepcopy(spec.global_settings)
         scenario_dict = copy.deepcopy(scenario_dict)
         settings = override_dict(global_dict, scenario_dict)
 
         scenario = Scenario(
@@ -253,26 +253,26 @@
     :type scenario: Scenario
 
     :rtype: Iterator[Instance]
     """
     # Iterate over every combination of observation model parameter values.
     previous_descriptors = set()
     obs_combs = scenario_observation_model_combinations(scenario)
-    for (value_dicts, descriptor) in obs_combs:
 
+    for value_dicts, descriptor in obs_combs:
         # First ensure that the descriptor is unique.
         if descriptor in previous_descriptors:
             msg_fmt = 'Scenario "{}" has a duplicate descriptor "{}"'
             raise ValueError(msg_fmt.format(scenario.scenario_id, descriptor))
         previous_descriptors.add(descriptor)
 
         # Copy the scenario settings, and apply the parameter values for each
         # observation model.
         settings = copy.deepcopy(scenario.settings)
-        for (obs_unit, values) in value_dicts.items():
+        for obs_unit, values in value_dicts.items():
             settings['observations'][obs_unit]['parameters'] = values
 
         # Return this instance of the scenario.
         instance = Instance(
             scenario_id=scenario.scenario_id,
             settings=settings,
             descriptor=descriptor,
@@ -297,17 +297,17 @@
     names = sorted(obs_params.values.keys())
 
     # Create a format string for each parameter.
     if obs_params.value_format and obs_params.display_names:
         # For example, if the 'bg_obs' parameter has the display name 'bg',
         # the format string will be "bg-{val[0]:{fmt[bg_obs]}}".
         out_fields = []
-        for (ix, name) in enumerate(names):
+        for ix, name in enumerate(names):
             # NOTE: produce format strings such as .
-            field = '{0}-{{values[{1}]:{{formats[{2}]}}}}'.format(
+            field = '{}-{{values[{}]:{{formats[{}]}}}}'.format(
                 obs_params.display_names[name], ix, name
             )
             out_fields.append(field)
 
         # Join the format strings into a single format string for all
         # parameters.
         out_fmt = '-'.join(out_fields)
@@ -477,15 +477,15 @@
     replacing their values:
 
     >>> from pypfilt.scenario import override_dict, DELETE_KEY
     >>> x = {'a': 1, 'b': 2, 'c': {'x': 3, 'y': 4}}
     >>> override_dict(x, {'c': {'x': DELETE_KEY}})
     {'a': 1, 'b': 2, 'c': {'y': 4}}
     """
-    for (key, value) in overrides.items():
+    for key, value in overrides.items():
         if isinstance(value, dict):
             if key in defaults and isinstance(defaults[key], dict):
                 # Override the nested default values.
                 sub_defaults = defaults[key]
                 defaults[key] = override_dict(sub_defaults, value)
             else:
                 # Replace the default value with this dictionary.
```

## pypfilt/state.py

```diff
@@ -87,29 +87,31 @@
     * weight: the particle weights (float);
     * prev_ix: the parent indices (int);
     * state_vec: the particle state vectors (float or structured dtype); and
     * lookup: sample indices for lookup tables (if required).
     """
     # We always need to record the particle weight and parent index.
     base_dtype = [
-        ('weight', np.float_),
-        ('prev_ix', np.int_),
+        ('weight', np.float64),
+        ('prev_ix', np.int64),
         ('resampled', np.bool_),
     ]
 
     # Determine the structure of the model state vector.
     svec_dtype = state_vec_dtype(ctx)
 
     # Determine whether we need additional lookup columns.
     # If no lookup columns are required we need to avoid adding the 'lookup'
     # field, because it will have zero dimensions and we will be unable to
     # save the history matrix in HDF5 files (e.g., cache files).
     lookup_cols = ctx.data.get('lookup_ixs', {})
     if lookup_cols:
-        lookup_dtype = [('lookup', [(name, np.int_) for name in lookup_cols])]
+        lookup_dtype = [
+            ('lookup', [(name, np.int64) for name in lookup_cols])
+        ]
         hist_dtype = base_dtype + svec_dtype + lookup_dtype
     else:
         hist_dtype = base_dtype + svec_dtype
 
     return hist_dtype
 
 
@@ -220,19 +222,22 @@
     :raises ValueError: if ``svec`` contains any fields that are incompatible
         with ``astype``.
 
     :Examples:
 
     >>> import numpy as np
     >>> from pypfilt.state import repack
-    >>> xs = np.array([(1.2, (2.2, 3.2)), (4.2, (5.2, 6.2))],
-    ...               dtype=[('x', float), ('y', float, 2)])
+    >>> xs = np.array(
+    ...     [(1.2, (2.2, 3.2)), (4.2, (5.2, 6.2))],
+    ...     dtype=[('x', float), ('y', float, 2)],
+    ... )
     >>> ys = repack(xs)
-    >>> assert np.array_equal(ys, np.array([[1.2, 2.2, 3.2],
-    ...                                     [4.2, 5.2, 6.2]]))
+    >>> assert np.array_equal(
+    ...     ys, np.array([[1.2, 2.2, 3.2], [4.2, 5.2, 6.2]])
+    ... )
     """
 
     def is_compat(dt):
         if dt.subdtype is None:
             return np.issubdtype(dt, astype)
         else:
             return np.issubdtype(dt.subdtype[0], astype)
@@ -481,15 +486,15 @@
         # NOTE: we must save the current index as the offset!
         data_sets = {
             'matrix': self.matrix,
             'offset': self.index,
             'times': times,
         }
         # Remove any existing data sets so that we can replace them.
-        for (name, data) in data_sets.items():
+        for name, data in data_sets.items():
             if name in group:
                 del group[name]
             try:
                 dtype = getattr(data, 'dtype', None)
                 group.create_dataset(name, data=data, dtype=dtype)
             except Exception:
                 logger = logging.getLogger(__name__)
```

## pypfilt/stats.py

```diff
@@ -150,15 +150,15 @@
         if ix_upper > ix_lower + 1:
             return np.rint((ix_lower + ix_upper) / 2).astype(int)
         else:
             return None
 
     # Evaluate each quantile in turn.
     quantiles = np.zeros(len(probs))
-    for (locn_ix, locn) in enumerate(eval_cdf_locns):
+    for locn_ix, locn in enumerate(eval_cdf_locns):
         # Check whether the quantile is the very first or last value.
         if cum_weights[0] >= (locn - atol):
             # NOTE: use strict equality with an absolute tolerance.
             if np.abs(locn - cum_weights[0]) <= atol and nx > 1:
                 # Average over the two matching values.
                 quantiles[locn_ix] = 0.5 * (x[0] + x[1])
             else:
```

## pypfilt/summary.py

```diff
@@ -322,15 +322,15 @@
                             pctl,
                             cred_ints[pctl][0],
                             cred_ints[pctl][1],
                             'model',
                             field,
                         )
                         insert_fn(row)
-                for (val, stat_fn) in self.__stat_info:
+                for val, stat_fn in self.__stat_info:
                     stat_vec = stat_fn(snapshot.state_vec[mask])
                     cred_ints = stats.cred_wt(stat_vec, ws, self.__probs)
                     for pctl in self.__probs:
                         row = (
                             fs_time,
                             snapshot.time,
                             pctl,
@@ -349,15 +349,15 @@
                             pctl,
                             0,
                             0,
                             'model',
                             field,
                         )
                         insert_fn(row)
-                    for (val, _) in self.__stat_info:
+                    for val, _ in self.__stat_info:
                         row = (
                             fs_time,
                             snapshot.time,
                             pctl,
                             0,
                             0,
                             'stat',
@@ -383,27 +383,27 @@
 
     def field_types(self, ctx, obs_list, name):
         # Create a PRNG for resampling the particles.
         prng_seed = ctx.settings['filter'].get('prng_seed')
         self.__resample = np.random.default_rng(prng_seed)
         fs_time = time_field('fs_time')
         time = time_field('time')
-        weight = ('weight', np.float_)
+        weight = ('weight', np.float64)
         fields = ctx.component['model'].field_types(ctx)
         return [fs_time, time, weight] + fields
 
     def n_rows(self, ctx, forecasting):
         num_particles = ctx.particle_count()
         n_times = ctx.summary_count()
         return n_times * num_particles
 
     def add_rows(self, ctx, fs_time, window, insert_fn):
         for snapshot in window:
             # Record the state of each particle.
-            for (row_ix, row) in enumerate(snapshot.state_vec):
+            for row_ix, row in enumerate(snapshot.state_vec):
                 weight = snapshot.weights[row_ix]
                 insert_fn(tuple((fs_time, snapshot.time, weight, *row)))
 
 
 class ForecastSnapshot(Table):
     """
     Record the particle state vectors at the start of each forecasting pass
@@ -633,15 +633,14 @@
         return n_times * n_px * obs_per_px
 
     def add_rows(self, ctx, fs_time, window, insert_fn):
         unit = self.__obs_unit
         obs_model = ctx.component['obs'][unit]
 
         for snapshot in window:
-
             # NOTE: resample the particles so that weights are uniform.
             if self.__sample_ixs is None:
                 # Select new particle indices.
                 (sample_ixs, _weight) = resample.resample_weights(
                     snapshot.weights, self.__rnd, count=self.__px_count
                 )
 
@@ -917,15 +916,15 @@
                                 pctl,
                                 cred_ints[pctl][0],
                                 cred_ints[pctl][1],
                                 'model',
                                 field,
                             )
                             insert_fn(row)
-                    for (val, stat_fn) in self.__stat_info:
+                    for val, stat_fn in self.__stat_info:
                         stat_vec = stat_fn(state_vec)
                         cred_ints = stats.cred_wt(stat_vec, ws, self.__probs)
                         for pctl in self.__probs:
                             row = (
                                 fs_time,
                                 subset.time,
                                 pix + 1,
@@ -946,15 +945,15 @@
                                 pctl,
                                 0,
                                 0,
                                 'model',
                                 field,
                             )
                             insert_fn(row)
-                        for (val, _) in self.__stat_info:
+                        for val, _ in self.__stat_info:
                             row = (
                                 fs_time,
                                 subset.time,
                                 pix + 1,
                                 pctl,
                                 0,
                                 0,
@@ -1103,15 +1102,15 @@
                         pr,
                         qtls[ix],
                         qtls[-(ix + 1)],
                     )
                     insert_fn(row)
 
 
-class HDF5(object):
+class HDF5:
     """
     Save tables of summary statistics to an HDF5 file.
 
     :param ctx: The simulation context.
     """
 
     def __init__(self, ctx):
@@ -1155,24 +1154,24 @@
         meta = Metadata()
         self.__metadata = meta.build(ctx)
 
         # If True, only calculate statistics for forecasting simulations.
         self.__only_fs = ctx.settings['summary']['only_forecasts']
 
         self.__monitors = {}
-        for (name, monitor) in ctx.component['summary_monitor'].items():
+        for name, monitor in ctx.component['summary_monitor'].items():
             if name in self.__monitors:
                 raise ValueError("Monitor '{}' already exists".format(name))
             self.__monitors[name] = monitor
             # NOTE: provide the monitor name here so that the monitor can
             # look for monitor-specific parameters.
             monitor.prepare(ctx, self.__all_obs, name)
 
         self.__tbl_dict = {}
-        for (name, table) in ctx.component['summary_table'].items():
+        for name, table in ctx.component['summary_table'].items():
             if name in self.__tbl_dict:
                 raise ValueError("Table '{}' already exists".format(name))
             self.__tbl_dict[name] = table
             # NOTE: provide the table name here so that the table can look for
             # table-specific parameters.
             table_fields = table.field_types(ctx, self.__all_obs, name)
             self.__dtypes[name] = fields_dtype(ctx, table_fields)
@@ -1185,25 +1184,25 @@
         :param grp: The h5py Group object from which to load the state.
 
         :raises ValueError: if a monitor and a table have the same name.
         """
         grp_names = set()
 
         # Load the monitor states.
-        for (name, mon) in self.__monitors.items():
+        for name, mon in self.__monitors.items():
             if name in grp_names:
                 msg = 'Multiple {} monitors/tables'.format(name)
                 raise ValueError(msg)
             else:
                 grp_names.add(name)
             mon_grp = grp.require_group(name)
             mon.load_state(ctx, mon_grp)
 
         # Load the table states.
-        for (name, tbl) in self.__tbl_dict.items():
+        for name, tbl in self.__tbl_dict.items():
             if name in grp_names:
                 msg = 'Multiple {} monitors/tables'.format(name)
                 raise ValueError(msg)
             else:
                 grp_names.add(name)
             tbl_grp = grp.require_group(name)
             tbl.load_state(ctx, tbl_grp)
@@ -1216,25 +1215,25 @@
         :param grp: The h5py Group object in which to save the state.
 
         :raises ValueError: if a monitor and a table have the same name.
         """
         grp_names = set()
 
         # Save the monitor states.
-        for (name, mon) in self.__monitors.items():
+        for name, mon in self.__monitors.items():
             if name in grp_names:
                 msg = 'Multiple {} monitors/tables'.format(name)
                 raise ValueError(msg)
             else:
                 grp_names.add(name)
             mon_grp = grp.require_group(name)
             mon.save_state(ctx, mon_grp)
 
         # Save the table states.
-        for (name, tbl) in self.__tbl_dict.items():
+        for name, tbl in self.__tbl_dict.items():
             if name in grp_names:
                 msg = 'Multiple {} monitors/tables'.format(name)
                 raise ValueError(msg)
             else:
                 grp_names.add(name)
             tbl_grp = grp.require_group(name)
             tbl.save_state(ctx, tbl_grp)
@@ -1516,20 +1515,20 @@
                                 table.dtype, metadata=dict(metadata)
                             )
                         )
                     save_data(adapt_group, n, table)
 
             # Save the observation tables in ctx.data['obs'].
             obs_group = f.create_group('observations')
-            for (key, value) in ctx.data['obs'].items():
+            for key, value in ctx.data['obs'].items():
                 save_data(obs_group, key, value)
 
             # Save the prior sample tables in ctx.data['prior'].
             prior_group = f.create_group('prior_samples')
-            for (key, value) in ctx.data['prior'].items():
+            for key, value in ctx.data['prior'].items():
                 save_data(prior_group, key, value)
 
             # Save the history matrix state if instructed to do so.
             if ctx.get_setting(['summary', 'save_history'], False):
                 history = ctx.component['history']
                 history_group = f.create_group('history')
                 save_data(history_group, 'matrix', history.matrix)
@@ -1642,15 +1641,16 @@
             # Ensure that numerical values retain their data type.
             return value
         elif isinstance(value, str):
             return value
         elif isinstance(value, (list, tuple)):
             # Save numeric lists and tuples as NumPy arrays.
             all_numbers = all(
-                isinstance(v, (int, float, np.int_, np.float_)) for v in value
+                isinstance(v, (int, float, np.int64, np.float64))
+                for v in value
             )
             if all_numbers:
                 return np.array(value)
             else:
                 return str(value)
         else:
             # Convert non-numerical values to UTF-8 strings.
```

## pypfilt/time.py

```diff
@@ -227,15 +227,15 @@
             of the simulation period).
         :returns: The time-step number, time-step details, and a list of
             observations.
         :rtype: Optional[TimeStepWithObservations]
         :raises ValueError: If observations are not sorted chronologically.
         """
         steps = self.with_observation_tables(ctx, obs_tables, start=from_time)
-        for (step_num, time_step, obs_list) in steps:
+        for step_num, time_step, obs_list in steps:
             if obs_list:
                 return TimeStepWithObservations(step_num, time_step, obs_list)
 
         return None
 
     def with_observation_tables(self, ctx, obs_tables, start=None):
         """
@@ -251,42 +251,42 @@
             the simulation period).
         :raises ValueError: If observations are not sorted chronologically.
         """
         if start is None:
             start = self._start
 
         # Ensure that observations are sorted chronologically.
-        for (obs_unit, obs_data) in obs_tables.items():
+        for obs_unit, obs_data in obs_tables.items():
             for ix in range(1, len(obs_data)):
                 t0 = obs_data['time'][ix - 1]
                 t1 = obs_data['time'][ix]
                 if t1 < t0:
                     msg = '"{}" observations are not ordered chronologically'
                     raise ValueError(msg.format(obs_unit))
 
         # For each observations table, record the row index and observation
         # dictionary of the next observation.
         obs_streams = {}
-        for (obs_unit, obs_data) in obs_tables.items():
+        for obs_unit, obs_data in obs_tables.items():
             obs_model = ctx.component['obs'][obs_unit]
             (row_ix, obs) = find_first_observation_after(
                 obs_model, obs_data, start
             )
             if row_ix is None or obs is None:
                 continue
             obs_streams[obs_unit] = (row_ix, obs)
 
-        for (step, time_step) in self.steps():
+        for step, time_step in self.steps():
             # Skip to the first time-step *after* the starting time.
             if time_step.end <= start:
                 continue
 
             # Find all observations that pertain to this time-step.
             current_obs = []
-            for (obs_unit, (row_ix, obs)) in list(obs_streams.items()):
+            for obs_unit, (row_ix, obs) in list(obs_streams.items()):
                 obs_model = ctx.component['obs'][obs_unit]
                 obs_data = obs_tables[obs_unit]
 
                 # Find all relevant observations from this observation stream.
                 obs_times = set()
                 search_from = row_ix + 1
                 while obs['time'] <= time_step.end:
@@ -398,15 +398,15 @@
     be used as fallback options for reading times.
     """
 
     def __init__(self, settings=None):
         """
         :param settings: An optional dictionary of simulation settings.
         """
-        super(Datetime, self).__init__()
+        super().__init__()
         if settings is None:
             settings = {}
         self.format_strings = settings.get('time', {}).get(
             'datetime_formats', ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d']
         )
         if len(self.format_strings) == 0:
             raise ValueError('The time.datetime_formats setting is empty')
@@ -573,15 +573,15 @@
     A dimensionless time scale.
     """
 
     def __init__(self, settings=None):
         """
         :param settings: An optional dictionary of simulation settings.
         """
-        super(Scalar, self).__init__()
+        super().__init__()
         self.np_dtype = np.float64
 
     def dtype(self, name):
         """Define the dtype for columns that store times."""
         return (name, self.np_dtype)
 
     def native_dtype(self):
```

## pypfilt/examples/lorenz.py

```diff
@@ -87,14 +87,16 @@
 
     :return: The scenario definition, represented as a TOML string.
     :rtype: str
     """
     return (
         inspect.cleandoc(
             """
+    # NOTE: Save this file as 'lorenz63_simulate.toml'
+
     [components]
     model = "pypfilt.examples.lorenz.Lorenz63"
     time = "pypfilt.Scalar"
     sampler = "pypfilt.sampler.LatinHypercube"
     summary = "pypfilt.summary.HDF5"
 
     [time]
@@ -140,14 +142,16 @@
 
     :return: The scenario definition, represented as a TOML string.
     :rtype: str
     """
     return (
         inspect.cleandoc(
             """
+    # NOTE: Save this file as 'lorenz63_forecast.toml'
+
     [components]
     model = "pypfilt.examples.lorenz.Lorenz63"
     time = "pypfilt.Scalar"
     sampler = "pypfilt.sampler.LatinHypercube"
     summary = "pypfilt.summary.HDF5"
 
     [time]
@@ -175,14 +179,16 @@
     [observations.z]
     model = "pypfilt.examples.lorenz.ObsLorenz63"
     file = "lorenz63-z.ssv"
 
     [summary.tables]
     forecasts.component = "pypfilt.summary.PredictiveCIs"
     forecasts.credible_intervals = [50, 60, 70, 80, 90, 95]
+    sim_z.component = "pypfilt.summary.SimulatedObs"
+    sim_z.observation_unit = "z"
 
     [filter]
     particles = 500
     prng_seed = 2001
     history_window = -1
     resample.threshold = 0.25
 
@@ -200,14 +206,16 @@
 
     :return: The scenario definition, represented as a TOML string.
     :rtype: str
     """
     return (
         inspect.cleandoc(
             """
+    # NOTE: Save this file as 'lorenz63_forecast_regularised.toml'
+
     [components]
     model = "pypfilt.examples.lorenz.Lorenz63"
     time = "pypfilt.Scalar"
     sampler = "pypfilt.sampler.LatinHypercube"
     summary = "pypfilt.summary.HDF5"
 
     [time]
@@ -223,14 +231,16 @@
     x = { name = "uniform", args.loc = -5, args.scale = 10 }
     y = { name = "uniform", args.loc = -5, args.scale = 10 }
     z = { name = "uniform", args.loc = -5, args.scale = 10 }
 
     [summary.tables]
     forecasts.component = "pypfilt.summary.PredictiveCIs"
     forecasts.credible_intervals = [50, 60, 70, 80, 90, 95]
+    sim_z.component = "pypfilt.summary.SimulatedObs"
+    sim_z.observation_unit = "z"
 
     [observations.x]
     model = "pypfilt.examples.lorenz.ObsLorenz63"
     file = "lorenz63-x.ssv"
 
     [observations.y]
     model = "pypfilt.examples.lorenz.ObsLorenz63"
@@ -265,14 +275,16 @@
 
     :return: The scenario definitions, represented as a TOML string.
     :rtype: str
     """
     return (
         inspect.cleandoc(
             """
+    # NOTE: Save this file as 'lorenz63_all.toml'
+
     [components]
     model = "pypfilt.examples.lorenz.Lorenz63"
     time = "pypfilt.Scalar"
     sampler = "pypfilt.sampler.LatinHypercube"
     summary = "pypfilt.summary.HDF5"
 
     [time]
@@ -312,25 +324,64 @@
     prior.y = { name = "uniform", args.loc = -5, args.scale = 10 }
     prior.z = { name = "uniform", args.loc = -5, args.scale = 10 }
     observations.x.file = "lorenz63-x.ssv"
     observations.y.file = "lorenz63-y.ssv"
     observations.z.file = "lorenz63-z.ssv"
     summary.tables.forecasts.component = "pypfilt.summary.PredictiveCIs"
     summary.tables.forecasts.credible_intervals = [50, 60, 70, 80, 90, 95]
+    summary.tables.sim_z.component = "pypfilt.summary.SimulatedObs"
+    summary.tables.sim_z.observation_unit = "z"
 
     [scenario.forecast_regularised]
     prior.x = { name = "uniform", args.loc = -5, args.scale = 10 }
     prior.y = { name = "uniform", args.loc = -5, args.scale = 10 }
     prior.z = { name = "uniform", args.loc = -5, args.scale = 10 }
     observations.x.file = "lorenz63-x.ssv"
     observations.y.file = "lorenz63-y.ssv"
     observations.z.file = "lorenz63-z.ssv"
     summary.tables.forecasts.component = "pypfilt.summary.PredictiveCIs"
     summary.tables.forecasts.credible_intervals = [50, 60, 70, 80, 90, 95]
+    summary.tables.sim_z.component = "pypfilt.summary.SimulatedObs"
+    summary.tables.sim_z.observation_unit = "z"
     filter.regularisation.enabled = true
     filter.regularisation.bounds.x = { min = -50, max = 50 }
     filter.regularisation.bounds.y = { min = -50, max = 50 }
     filter.regularisation.bounds.z = {}
     """
         )
         + '\n'
     )
+
+
+def save_lorenz63_scenario_files():
+    """
+    Save all of the example Lorenz-63 scenario files in the working directory.
+
+    This function creates (or overwrites) the following files:
+
+    - ``'lorenz63_simulate.toml'``: a scenario file for simulating
+      observations.
+
+    - ``'lorenz63_forecast.toml'``: a scenario file for running forecasts,
+      using the simulated observations.
+
+    - ``'lorenz63_forecast_regularised.toml'``: a scenario file for running
+      forecasts with post-regularisation, using the simulated observations.
+
+    - ``'lorenz63_all.toml'``: a scenario file that defines all of the above
+      scenarios.
+    """
+    simulate_file = 'lorenz63_simulate.toml'
+    with open(simulate_file, 'w') as f:
+        f.write(lorenz63_simulate_toml())
+
+    forecast_file = 'lorenz63_forecast.toml'
+    with open(forecast_file, 'w') as f:
+        f.write(lorenz63_forecast_toml())
+
+    forecast_reg_file = 'lorenz63_forecast_regularised.toml'
+    with open(forecast_reg_file, 'w') as f:
+        f.write(lorenz63_forecast_regularised_toml())
+
+    all_scenarios_file = 'lorenz63_all.toml'
+    with open(all_scenarios_file, 'w') as f:
+        f.write(lorenz63_all_scenarios_toml())
```

## pypfilt/examples/predation.py

```diff
@@ -20,15 +20,15 @@
 
 
 class LotkaVolterra(OdeModel):
     """An implementation of the (continuous) Lotka-Volterra equations."""
 
     def field_types(self, ctx):
         field_names = ['x', 'y', 'alpha', 'beta', 'gamma', 'delta']
-        return [(name, np.float_) for name in field_names]
+        return [(name, np.float64) for name in field_names]
 
     def d_dt(self, time, xt, ctx, is_forecast):
         """Calculate the derivatives of x(t) and y(t)."""
         x, y = xt['x'], xt['y']
         d_dt = np.zeros(xt.shape, dtype=xt.dtype)
         d_dt['x'] = xt['alpha'] * x - xt['beta'] * x * y
         d_dt['y'] = xt['gamma'] * x * y - xt['delta'] * y
@@ -192,15 +192,15 @@
             'Population Size (1,000s)',
             ('fs_time', 'Forecast @ t = {:0.0f}'),
             ('unit', lambda s: '{}(t)'.format(s)),
         )
         plot.expand_x_lims('time')
         plot.expand_y_lims('ymax')
 
-        for (ax, df) in plot.subplots():
+        for ax, df in plot.subplots():
             ax.axhline(
                 y=0, xmin=0, xmax=1, linewidth=1, linestyle='--', color='k'
             )
             hs = pypfilt.plot.cred_ints(ax, df, 'time', 'prob')
             if df['unit'][0] == 'x':
                 df_obs = x_obs
             else:
@@ -244,15 +244,15 @@
             'Time',
             'Value',
             ('name', lambda s: '$\\{}$'.format(s)),
             nr=1,
         )
         plot.expand_y_lims('ymax')
 
-        for (ax, df) in plot.subplots(dy=-0.025):
+        for ax, df in plot.subplots(dy=-0.025):
             hs = pypfilt.plot.cred_ints(ax, df, 'time', 'prob')
             if df['name'][0] == 'alpha':
                 y_true = 2 / 3
             elif df['name'][0] == 'beta':
                 y_true = 4 / 3
             elif df['name'][0] == 'gamma':
                 y_true = 1
```

## pypfilt/examples/sir.py

```diff
@@ -28,23 +28,23 @@
 
     def field_types(self, ctx):
         """
         Define the state vector structure.
         """
         return [
             # Model state variables.
-            ('S', np.int_),
-            ('I', np.int_),
-            ('R', np.int_),
+            ('S', np.int64),
+            ('I', np.int64),
+            ('R', np.int64),
             # Model parameters.
-            ('R0', np.float_),
-            ('gamma', np.float_),
+            ('R0', np.float64),
+            ('gamma', np.float64),
             # Next event details.
-            ('next_event', np.int_),
-            ('next_time', np.float_),
+            ('next_event', np.int64),
+            ('next_time', np.float64),
         ]
 
     def can_smooth(self):
         """
         The fields that can be smoothed by the post-regularisation filter.
         """
         # Return the continuous model parameters.
@@ -129,15 +129,15 @@
         rng = ctx.component['random']['model']
         dt = -np.log(rng.random(S.shape)) / rate_sum
         vec['next_time'][active] += dt
 
         # Select the event type: False for infection and True for recovery.
         threshold = rng.random(S.shape) * rate_sum
         recovery_event = threshold > s_to_i_rate
-        vec['next_event'][active] = recovery_event.astype(np.int_)
+        vec['next_event'][active] = recovery_event.astype(np.int64)
 
 
 class SirDtmc(Model):
     """
     A discrete-time Markov chain implementation of the SIR model.
 
     The model settings must include the following keys:
@@ -147,20 +147,20 @@
 
     def field_types(self, ctx):
         """
         Define the state vector structure.
         """
         return [
             # Model state variables.
-            ('S', np.int_),
-            ('I', np.int_),
-            ('R', np.int_),
+            ('S', np.int64),
+            ('I', np.int64),
+            ('R', np.int64),
             # Model parameters.
-            ('R0', np.float_),
-            ('gamma', np.float_),
+            ('R0', np.float64),
+            ('gamma', np.float64),
         ]
 
     def can_smooth(self):
         """
         The fields that can be smoothed by the post-regularisation filter.
         """
         # Return the continuous model parameters.
@@ -221,20 +221,20 @@
 
     def field_types(self, ctx):
         """
         Define the state vector structure.
         """
         return [
             # Model state variables.
-            ('S', np.float_),
-            ('I', np.float_),
-            ('R', np.float_),
+            ('S', np.float64),
+            ('I', np.float64),
+            ('R', np.float64),
             # Model parameters.
-            ('R0', np.float_),
-            ('gamma', np.float_),
+            ('R0', np.float64),
+            ('gamma', np.float64),
         ]
 
     def can_smooth(self):
         """
         The fields that can be smoothed by the post-regularisation filter.
         """
         # Return the continuous model parameters.
@@ -287,20 +287,20 @@
 
     def field_types(self, ctx):
         """
         Define the state vector structure.
         """
         return [
             # Model state variables.
-            ('S', np.float_),
-            ('I', np.float_),
-            ('R', np.float_),
+            ('S', np.float64),
+            ('I', np.float64),
+            ('R', np.float64),
             # Model parameters.
-            ('R0', np.float_),
-            ('gamma', np.float_),
+            ('R0', np.float64),
+            ('gamma', np.float64),
         ]
 
     def can_smooth(self):
         """
         The fields that can be smoothed by the post-regularisation filter.
         """
         # Return the continuous model parameters.
@@ -348,20 +348,20 @@
 
     def field_types(self, ctx):
         """
         Define the state vector structure.
         """
         return [
             # Model state variables.
-            ('S', np.float_),
-            ('I', np.float_),
-            ('R', np.float_),
+            ('S', np.float64),
+            ('I', np.float64),
+            ('R', np.float64),
             # Model parameters.
-            ('R0', np.float_),
-            ('gamma', np.float_),
+            ('R0', np.float64),
+            ('gamma', np.float64),
         ]
 
     def can_smooth(self):
         """
         The fields that can be smoothed by the post-regularisation filter.
         """
         # Return the continuous model parameters.
@@ -445,16 +445,16 @@
         occurred during the observation period :math:`\Delta` for each
         particle.
         """
         period = self.settings['observation_period']
         prev = snapshot.back_n_units_state_vec(period)
         new_infs = prev['S'] - snapshot.state_vec['S']
         # Round continuous values to the nearest integer.
-        if not np.issubdtype(new_infs.dtype, np.int_):
-            new_infs = new_infs.round().astype(np.int_)
+        if not np.issubdtype(new_infs.dtype, np.int64):
+            new_infs = new_infs.round().astype(np.int64)
         if np.any(new_infs < 0):
             raise ValueError('Negative number of new infections')
         return new_infs
 
     def distribution(self, ctx, snapshot):
         """
         Return the observation distribution for each particle.
```

## Comparing `pypfilt-0.8.1.dist-info/LICENSE` & `pypfilt-0.8.2.dist-info/licenses/LICENSE`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-Copyright (c) 2014-2023, Rob Moss
+Copyright (c) 2014-2024, Rob Moss
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
 1. Redistributions of source code must retain the above copyright notice, this
    list of conditions and the following disclaimer.
```

## Comparing `pypfilt-0.8.1.dist-info/METADATA` & `pypfilt-0.8.2.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,18 @@
-Metadata-Version: 2.1
+Metadata-Version: 2.3
 Name: pypfilt
-Version: 0.8.1
+Version: 0.8.2
 Summary: Bootstrap particle filter for epidemic forecasting
+Project-URL: homepage, https://bitbucket.org/robmoss/particle-filter-for-python/
+Project-URL: repository, https://bitbucket.org/robmoss/particle-filter-for-python/
+Project-URL: documentation, https://pypfilt.readthedocs.io/en/latest/
+Project-URL: changelog, https://pypfilt.readthedocs.io/en/latest/changelog.html
 Author-email: Rob Moss <rgmoss@unimelb.edu.au>
 Maintainer-email: Rob Moss <rgmoss@unimelb.edu.au>
-License: Copyright (c) 2014-2023, Rob Moss
+License: Copyright (c) 2014-2024, Rob Moss
         All rights reserved.
         
         Redistribution and use in source and binary forms, with or without
         modification, are permitted provided that the following conditions are met:
         
         1. Redistributions of source code must retain the above copyright notice, this
            list of conditions and the following disclaimer.
@@ -27,41 +31,36 @@
         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-        
-Project-URL: homepage, https://bitbucket.org/robmoss/particle-filter-for-python/
-Project-URL: repository, https://bitbucket.org/robmoss/particle-filter-for-python/
-Project-URL: documentation, https://pypfilt.readthedocs.io/en/latest/
-Project-URL: changelog, https://pypfilt.readthedocs.io/en/latest/changelog.html
+License-File: LICENSE
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering :: Mathematics
 Requires-Python: >=3.8
-Description-Content-Type: text/x-rst
-License-File: LICENSE
-Requires-Dist: h5py ~=3.0
-Requires-Dist: lhs ~=0.4
-Requires-Dist: numpy ~=1.17
-Requires-Dist: packaging ~=21.3
-Requires-Dist: scipy ~=1.4
-Requires-Dist: tomli ~=2.0
-Requires-Dist: tomli-w ~=1.0
+Requires-Dist: h5py~=3.0
+Requires-Dist: lhs~=0.4
+Requires-Dist: numpy~=1.17
+Requires-Dist: packaging~=21.3
+Requires-Dist: scipy~=1.4
+Requires-Dist: tomli-w~=1.0
+Requires-Dist: tomli~=2.0
 Provides-Extra: plot
-Requires-Dist: matplotlib ~=3.4 ; extra == 'plot'
+Requires-Dist: matplotlib~=3.4; extra == 'plot'
 Provides-Extra: tests
-Requires-Dist: pytest ; extra == 'tests'
-Requires-Dist: pytest-cov ~=4.0 ; extra == 'tests'
+Requires-Dist: pytest; extra == 'tests'
+Requires-Dist: pytest-cov~=4.0; extra == 'tests'
+Description-Content-Type: text/x-rst
 
 Bootstrap particle filter for epidemic forecasting
 ==================================================
 
 |version| |docs| |tests| |coverage|
 
 Description
```

## Comparing `pypfilt-0.8.1.dist-info/RECORD` & `pypfilt-0.8.2.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,37 +1,35 @@
-pypfilt/__init__.py,sha256=NNFZamZ1ae9k4ZSLTXiEUw3VyWoCd6tqLPnxms_Ieao,19223
+pypfilt/__init__.py,sha256=Eh4j7MtNqgAR9kOeNAi7vSYDz-1o3VzVi4lbeiNK8VQ,19221
 pypfilt/adaptive.py,sha256=6dlVT_dImenm5Se6gk7Dm-Niv1Gx_lFwf365EBPUvAw,8742
-pypfilt/build.py,sha256=2-dGyQ4gOQzaxyCQ_uEhSNrPCnNMyhfjb7kpYAorFiE,39195
-pypfilt/cache.py,sha256=_u0WNcu4PrKwYsKPaQU5_tRZTVj71XGM21iMO4Q98Gk,18277
-pypfilt/crps.py,sha256=DFhfwB6oetdGHWulWBqGLHRJMfAPRAmjOs5-apoaL3U,4813
+pypfilt/build.py,sha256=5TxN2ix5fGPN7eX2OvXDmQAArcTDdqQBHJLND3l490A,39166
+pypfilt/cache.py,sha256=EqjYydQmvMoDo6jiKqpZ6L1adAvEpLPpxNzxZkBC2j8,18881
+pypfilt/crps.py,sha256=Cb_gHVqBhFg8gBiOJ0rBswTfut5JN6UWFssfgZeC2uw,4813
 pypfilt/event.py,sha256=OFT9XQw1dptWaCtJObcredtJWq8Bqum-BbY2oGQvnx4,2775
-pypfilt/io.py,sha256=Aegzubue2-rpEvVTe_c2K9N_IYdeT_TjVMTgf_E2Y0g,28001
-pypfilt/model.py,sha256=W-5AIusPT0CAbf10UEworOq1PXoFcaKPpaEGJkQhlss,10724
-pypfilt/obs.py,sha256=v_qHv_sVdVYchKxLGeugpiFotzrnHr3RHvUtcLjevbo,18794
-pypfilt/pfilter.py,sha256=8Ffh6nSgrIh9wgBVkKqQ38FK1J9nwO2N-7jQJ_oZF7I,18608
-pypfilt/plot.py,sha256=-V9l9hHNWQSQPwPuBsfbCPecVZlimWARTrA_eJIE3dg,31075
-pypfilt/resample.py,sha256=CGE7Y7vtkzguF0XysAoM0dZmbZzorIavUAtTVP1xlCo,13010
-pypfilt/sampler.py,sha256=05MNU3ZGC5ABRTEgY3d2H_3KfE792uFJBx2YlMb49Sw,5154
-pypfilt/scenario.py,sha256=PWnvkRq6WekF-QGb-e7PC1SRgusHdIysSsTmEL6PXzY,16771
-pypfilt/state.py,sha256=DfDQbULmptwRsLh9ch1YwDdzL5bnvao30Xk_Xm-IeMY,21945
-pypfilt/stats.py,sha256=nMU-ywXQzynmhOVDPXEHucXeWXl4AajeWRNmo1ZXNxE,8517
-pypfilt/summary.py,sha256=c8XzbFYLlxXU32KsH5hDExHh15yZLJBRoQTbZrBH5ic,63760
-pypfilt/time.py,sha256=Qqqq2sge4wEpmRnK5rbeznmAvu7plyWdJmP1QVH1tOo,22742
+pypfilt/io.py,sha256=afscsuX_-rWVtrmuhcNefaIQkmgIaGWs5lAPyov5EzQ,28010
+pypfilt/model.py,sha256=w57-6zH3AOFS88MoViBfGWK6F9hswQY2eAseDCY5BxE,10722
+pypfilt/obs.py,sha256=psLGUjsisb-SZQxIQ8yJUO1GInVly3me1-2VzWJwG6A,18788
+pypfilt/pfilter.py,sha256=AZai9Nfhxe8Rgl6HDPY7aH3ULHfUf4TnkcvKN5zLqHI,18606
+pypfilt/plot.py,sha256=NYFS6zjINMfd4NvI1SjXWRA-Ll_WI0EhAD5gusuJisk,30979
+pypfilt/resample.py,sha256=J-XPjjSljk8I1L-CjrbO4Cb06CfwQL_6YwSpPGB7iiw,13007
+pypfilt/sampler.py,sha256=-TTpR6b_Re8lYVfY_3_p7tfncSr36pvwqdGSPMXRl2Q,2580
+pypfilt/scenario.py,sha256=dKjKclVVN5WPNkjIor2bsT3kYUpzAqSBadgfJkVjiNM,16706
+pypfilt/state.py,sha256=ojQBQgcohLHOXaGT-vSnE1a-7PdyWqjz9xbwI31Yl18,21959
+pypfilt/stats.py,sha256=YrdYaWZR-QpsBCn1bZ9GQt6l3CUV_Qa9WQ5EYcF9nO8,8515
+pypfilt/summary.py,sha256=kXRAwiXMj7hs2_td_OZ-zwyshVJzKWnqcXcRMurG_Ck,63744
+pypfilt/time.py,sha256=9KrdN1iL7xOShN4LK17Fkd6KO9r1Du6YKNv_WyPf3p0,22706
 pypfilt/examples/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 pypfilt/examples/gaussian_walk.toml,sha256=mAq_H91ftoBeqFWV5VRwH0SrLMvJW3Rx3KZN4l20SZc,946
-pypfilt/examples/lorenz.py,sha256=zDK0U0zXQrpa96t8BtNUVZpiw6mSasfvxK3twROgDzc,9170
+pypfilt/examples/lorenz.py,sha256=QBZ0QDXg9r3xtew1vtxY36C98Y71xxdV_GXGUDROS48,11007
 pypfilt/examples/predation-counts-x-datetime.ssv,sha256=xOJZWwZ5qMoy-Nm3jU-LH8eWUyrXVIOC6aIwBnRC41I,311
 pypfilt/examples/predation-counts-x.ssv,sha256=-oDDkN-23sxaHvmLiK7u-TYkTIN3wyZ-GavfSNTzYfg,182
 pypfilt/examples/predation-counts-y-datetime.ssv,sha256=enUg0JWKe74jarfx0Jzgn70u9jfQP-bQRgfkbCjEDjM,311
 pypfilt/examples/predation-counts-y.ssv,sha256=JWv7v3JnrsuS59WNrd3D9hRAUeHWAgFU3KVeY6ptgKw,182
 pypfilt/examples/predation-datetime.toml,sha256=pggfXLNXMUBVdQjEFdi1m_0YwQtxfJigig8XdXdxp0k,1656
-pypfilt/examples/predation.py,sha256=CTn1RoV0lUZs7ZsAOTMuu36NcjU_hjuOFYj4OJ7NbBI,14309
+pypfilt/examples/predation.py,sha256=yHJHIOCLhD6H1b65-D5_mh_K430kiqftUG0jLlVAxBw,14306
 pypfilt/examples/predation.toml,sha256=j603IYyp0YnfoiJW5lY7BALjhinHAvTuEwPqbGZB7X8,1635
 pypfilt/examples/simple.py,sha256=_RgywpuxSr_nj32JaOVIR4ZrYGqOe22u270OWGCoQA0,1966
-pypfilt/examples/sir.py,sha256=YeYHVWK4yTDxn-PxSgGqUnS4hLw_ywUreAezpCt4eBA,14298
+pypfilt/examples/sir.py,sha256=paPFZ2LTzTQWqTlDFzTFupFsjKNUCq6qKZQNOcsGdOo,14328
 pypfilt/examples/sir.toml,sha256=Qb1Ljbb194JPXkrqZQzGYE0VSWefzugljlSwKlVfLiQ,1545
-pypfilt-0.8.1.dist-info/LICENSE,sha256=NTBsxePk0rnOKuAxcywVz0cua42yvUM6pmJSDohOhZQ,1499
-pypfilt-0.8.1.dist-info/METADATA,sha256=Z1HxUTzQcMQ7GdKu17WPOwYh3on3CveFb2oThxuxWCg,4734
-pypfilt-0.8.1.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-pypfilt-0.8.1.dist-info/top_level.txt,sha256=7zC5LNqyO_udw6suqRFgX2Ah8B4QRn0mLF_aa9jI0rY,8
-pypfilt-0.8.1.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-pypfilt-0.8.1.dist-info/RECORD,,
+pypfilt-0.8.2.dist-info/METADATA,sha256=vMVJ3jf13TVECtUCq2FJltFyJQ2iZOzouiSiAcx--Vs,4713
+pypfilt-0.8.2.dist-info/WHEEL,sha256=uNdcs2TADwSd5pVaP0Z_kcjcvvTUklh2S7bxZMF8Uj0,87
+pypfilt-0.8.2.dist-info/licenses/LICENSE,sha256=n6NHAuK5rNxjBjRwh8NEWP6Vyd3qH7dZMnynVd3fg1Y,1499
+pypfilt-0.8.2.dist-info/RECORD,,
```

