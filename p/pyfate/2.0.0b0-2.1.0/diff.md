# Comparing `tmp/pyfate-2.0.0b0.tar.gz` & `tmp/pyfate-2.1.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pyfate-2.0.0b0.tar", last modified: Tue Sep 19 06:27:02 2023, max compression
+gzip compressed data, was "pyfate-2.1.0.tar", last modified: Wed Apr  3 07:08:36 2024, max compression
```

## Comparing `pyfate-2.0.0b0.tar` & `pyfate-2.1.0.tar`

### file list

```diff
@@ -1,431 +1,579 @@
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.231140 pyfate-2.0.0b0/
--rw-r--r--   0 sage       (501) staff       (20)     7859 2023-09-19 06:27:02.230742 pyfate-2.0.0b0/PKG-INFO
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.180130 pyfate-2.0.0b0/fate/
--rw-r--r--   0 sage       (501) staff       (20)      659 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      663 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/_info.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.180377 pyfate-2.0.0b0/fate/arch/
--rw-r--r--   0 sage       (501) staff       (20)      761 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    43182 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/_standalone.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.181168 pyfate-2.0.0b0/fate/arch/abc/
--rw-r--r--   0 sage       (501) staff       (20)      133 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/abc/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1323 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/abc/_federation.py
--rw-r--r--   0 sage       (501) staff       (20)      718 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/abc/_party.py
--rw-r--r--   0 sage       (501) staff       (20)    17051 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/abc/_table.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.181614 pyfate-2.0.0b0/fate/arch/computing/
--rw-r--r--   0 sage       (501) staff       (20)      897 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    12674 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/_profile.py
--rw-r--r--   0 sage       (501) staff       (20)      755 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/_type.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.182147 pyfate-2.0.0b0/fate/arch/computing/eggroll/
--rw-r--r--   0 sage       (501) staff       (20)      708 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/eggroll/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3986 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/eggroll/_csession.py
--rw-r--r--   0 sage       (501) staff       (20)     5981 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/eggroll/_table.py
--rw-r--r--   0 sage       (501) staff       (20)      999 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/eggroll/_type.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.182617 pyfate-2.0.0b0/fate/arch/computing/spark/
--rw-r--r--   0 sage       (501) staff       (20)      936 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/spark/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3251 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/spark/_csession.py
--rw-r--r--   0 sage       (501) staff       (20)      932 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/spark/_materialize.py
--rw-r--r--   0 sage       (501) staff       (20)    11604 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/spark/_table.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.183207 pyfate-2.0.0b0/fate/arch/computing/standalone/
--rw-r--r--   0 sage       (501) staff       (20)      708 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/standalone/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3208 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/standalone/_csession.py
--rw-r--r--   0 sage       (501) staff       (20)     5711 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/standalone/_table.py
--rw-r--r--   0 sage       (501) staff       (20)      741 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/computing/standalone/_type.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.184098 pyfate-2.0.0b0/fate/arch/context/
--rw-r--r--   0 sage       (501) staff       (20)      711 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/context/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     5858 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/context/_cipher.py
--rw-r--r--   0 sage       (501) staff       (20)     8128 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/context/_context.py
--rw-r--r--   0 sage       (501) staff       (20)     9198 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/context/_federation.py
--rw-r--r--   0 sage       (501) staff       (20)     6720 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/context/_metrics.py
--rw-r--r--   0 sage       (501) staff       (20)     2378 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/context/_namespace.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.184508 pyfate-2.0.0b0/fate/arch/dataframe/
--rw-r--r--   0 sage       (501) staff       (20)     1167 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    18044 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/_dataframe.py
--rw-r--r--   0 sage       (501) staff       (20)    12318 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/_frame_reader.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.184774 pyfate-2.0.0b0/fate/arch/dataframe/conf/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/conf/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      677 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/conf/default_config.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.185027 pyfate-2.0.0b0/fate/arch/dataframe/entity/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/entity/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)       60 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/entity/types.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.185392 pyfate-2.0.0b0/fate/arch/dataframe/io/
--rw-r--r--   0 sage       (501) staff       (20)      795 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/io/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1241 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/io/_json_schema.py
--rw-r--r--   0 sage       (501) staff       (20)     1591 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/io/_json_serialization.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.185895 pyfate-2.0.0b0/fate/arch/dataframe/manager/
--rw-r--r--   0 sage       (501) staff       (20)      117 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/manager/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    27872 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/manager/block_manager.py
--rw-r--r--   0 sage       (501) staff       (20)    12382 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/manager/data_manager.py
--rw-r--r--   0 sage       (501) staff       (20)    24400 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/manager/schema_manager.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.186151 pyfate-2.0.0b0/fate/arch/dataframe/manager/utils/
--rw-r--r--   0 sage       (501) staff       (20)      669 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/manager/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     4920 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/manager/utils/_anonymous_generator.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.189324 pyfate-2.0.0b0/fate/arch/dataframe/ops/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1977 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_activation.py
--rw-r--r--   0 sage       (501) staff       (20)     7411 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_apply_row.py
--rw-r--r--   0 sage       (501) staff       (20)     4702 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_arithmetic.py
--rw-r--r--   0 sage       (501) staff       (20)     5194 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_cmp.py
--rw-r--r--   0 sage       (501) staff       (20)     2527 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_compress_block.py
--rw-r--r--   0 sage       (501) staff       (20)     1157 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_data_overview.py
--rw-r--r--   0 sage       (501) staff       (20)    16168 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_dimension_scaling.py
--rw-r--r--   0 sage       (501) staff       (20)     9218 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_encoder.py
--rw-r--r--   0 sage       (501) staff       (20)     1811 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_field_extract.py
--rw-r--r--   0 sage       (501) staff       (20)     3597 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_fillna.py
--rw-r--r--   0 sage       (501) staff       (20)     4254 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_histogram.py
--rw-r--r--   0 sage       (501) staff       (20)    13483 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_indexer.py
--rw-r--r--   0 sage       (501) staff       (20)     4767 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_isin.py
--rw-r--r--   0 sage       (501) staff       (20)     1862 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_missing.py
--rw-r--r--   0 sage       (501) staff       (20)     2609 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_promote_types.py
--rw-r--r--   0 sage       (501) staff       (20)     2496 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_quantile.py
--rw-r--r--   0 sage       (501) staff       (20)     3778 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_replace.py
--rw-r--r--   0 sage       (501) staff       (20)    13491 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_set_item.py
--rw-r--r--   0 sage       (501) staff       (20)     8022 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_stat.py
--rw-r--r--   0 sage       (501) staff       (20)     8173 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_transformer.py
--rw-r--r--   0 sage       (501) staff       (20)     1240 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_unary_operator.py
--rw-r--r--   0 sage       (501) staff       (20)     4387 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/_where.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.190065 pyfate-2.0.0b0/fate/arch/dataframe/ops/utils/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1448 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/utils/operators.py
--rw-r--r--   0 sage       (501) staff       (20)     1263 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/ops/utils/series_align.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.190873 pyfate-2.0.0b0/fate/arch/dataframe/utils/
--rw-r--r--   0 sage       (501) staff       (20)      788 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      783 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/utils/_auto_column_name_generated.py
--rw-r--r--   0 sage       (501) staff       (20)     6980 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/utils/_dataloader.py
--rw-r--r--   0 sage       (501) staff       (20)      897 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/utils/_id_generator.py
--rw-r--r--   0 sage       (501) staff       (20)     3575 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/utils/_k_fold.py
--rw-r--r--   0 sage       (501) staff       (20)     8921 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/dataframe/utils/_sample.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.191776 pyfate-2.0.0b0/fate/arch/federation/
--rw-r--r--   0 sage       (501) staff       (20)      686 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1299 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/_datastream.py
--rw-r--r--   0 sage       (501) staff       (20)    22534 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/_federation.py
--rw-r--r--   0 sage       (501) staff       (20)     3346 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/_gc.py
--rw-r--r--   0 sage       (501) staff       (20)     1324 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/_nretry.py
--rw-r--r--   0 sage       (501) staff       (20)     1276 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/_parties.py
--rw-r--r--   0 sage       (501) staff       (20)      722 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/_type.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.192015 pyfate-2.0.0b0/fate/arch/federation/eggroll/
--rw-r--r--   0 sage       (501) staff       (20)      693 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/eggroll/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     6377 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/eggroll/_federation.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.192777 pyfate-2.0.0b0/fate/arch/federation/osx/
--rw-r--r--   0 sage       (501) staff       (20)      858 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/osx/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     7178 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/osx/_federation.py
--rw-r--r--   0 sage       (501) staff       (20)     7337 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/osx/_mq_channel.py
--rw-r--r--   0 sage       (501) staff       (20)     6499 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/osx/osx_pb2.py
--rw-r--r--   0 sage       (501) staff       (20)     5597 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/osx/osx_pb2_grpc.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.193371 pyfate-2.0.0b0/fate/arch/federation/pulsar/
--rw-r--r--   0 sage       (501) staff       (20)      730 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/pulsar/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    16414 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/pulsar/_federation.py
--rw-r--r--   0 sage       (501) staff       (20)     8706 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/pulsar/_mq_channel.py
--rw-r--r--   0 sage       (501) staff       (20)     8779 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/pulsar/_pulsar_manager.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.194026 pyfate-2.0.0b0/fate/arch/federation/rabbitmq/
--rw-r--r--   0 sage       (501) staff       (20)      692 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/rabbitmq/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     9490 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/rabbitmq/_federation.py
--rw-r--r--   0 sage       (501) staff       (20)     4489 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/rabbitmq/_mq_channel.py
--rw-r--r--   0 sage       (501) staff       (20)    15417 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/rabbitmq/_rabbit_manager.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.194307 pyfate-2.0.0b0/fate/arch/federation/standalone/
--rw-r--r--   0 sage       (501) staff       (20)      699 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/standalone/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2995 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/federation/standalone/_federation.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.194975 pyfate-2.0.0b0/fate/arch/histogram/
--rw-r--r--   0 sage       (501) staff       (20)      142 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    10704 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/_histogram_distributed.py
--rw-r--r--   0 sage       (501) staff       (20)     3633 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/_histogram_local.py
--rw-r--r--   0 sage       (501) staff       (20)     2404 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/_histogram_sbt.py
--rw-r--r--   0 sage       (501) staff       (20)     4393 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/_histogram_splits.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.195259 pyfate-2.0.0b0/fate/arch/histogram/indexer/
--rw-r--r--   0 sage       (501) staff       (20)      221 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/indexer/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     7691 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/indexer/_indexer.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.196075 pyfate-2.0.0b0/fate/arch/histogram/values/
--rw-r--r--   0 sage       (501) staff       (20)       86 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/values/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     5422 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/values/_cipher.py
--rw-r--r--   0 sage       (501) staff       (20)     1838 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/values/_encoded.py
--rw-r--r--   0 sage       (501) staff       (20)     6439 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/values/_plain.py
--rw-r--r--   0 sage       (501) staff       (20)     1720 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/values/_value.py
--rw-r--r--   0 sage       (501) staff       (20)     8779 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/histogram/values/_values.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.196264 pyfate-2.0.0b0/fate/arch/protocol/
--rw-r--r--   0 sage       (501) staff       (20)      143 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.196394 pyfate-2.0.0b0/fate/arch/protocol/diffie_hellman/
--rw-r--r--   0 sage       (501) staff       (20)       92 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/diffie_hellman/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.197025 pyfate-2.0.0b0/fate/arch/protocol/phe/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/phe/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    15026 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/phe/mock.py
--rw-r--r--   0 sage       (501) staff       (20)    14478 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/phe/ou.py
--rw-r--r--   0 sage       (501) staff       (20)    14400 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/phe/paillier.py
--rw-r--r--   0 sage       (501) staff       (20)      619 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/phe/type.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.197273 pyfate-2.0.0b0/fate/arch/protocol/psi/
--rw-r--r--   0 sage       (501) staff       (20)      647 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/psi/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      894 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/psi/_psi_run.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.197511 pyfate-2.0.0b0/fate/arch/protocol/psi/ecdh/
--rw-r--r--   0 sage       (501) staff       (20)      616 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/psi/ecdh/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     6530 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/psi/ecdh/_run.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.197800 pyfate-2.0.0b0/fate/arch/protocol/secure_aggregation/
--rw-r--r--   0 sage       (501) staff       (20)      144 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/secure_aggregation/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     4301 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/protocol/secure_aggregation/_secure_aggregation.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.198064 pyfate-2.0.0b0/fate/arch/tensor/
--rw-r--r--   0 sage       (501) staff       (20)      738 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3742 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/_custom_ops.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.199394 pyfate-2.0.0b0/fate/arch/tensor/distributed/
--rw-r--r--   0 sage       (501) staff       (20)      263 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     9766 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_op_matmul.py
--rw-r--r--   0 sage       (501) staff       (20)     2447 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_op_slice.py
--rw-r--r--   0 sage       (501) staff       (20)     1648 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_op_transpose.py
--rw-r--r--   0 sage       (501) staff       (20)    11756 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_agg.py
--rw-r--r--   0 sage       (501) staff       (20)     1888 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_binary.py
--rw-r--r--   0 sage       (501) staff       (20)     1237 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_cipher.py
--rw-r--r--   0 sage       (501) staff       (20)      407 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_others.py
--rw-r--r--   0 sage       (501) staff       (20)      568 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_unary.py
--rw-r--r--   0 sage       (501) staff       (20)    12786 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/distributed/_tensor.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.199873 pyfate-2.0.0b0/fate/arch/tensor/inside/
--rw-r--r--   0 sage       (501) staff       (20)       36 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/inside/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2214 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/inside/_op_quantile.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.200489 pyfate-2.0.0b0/fate/arch/tensor/phe/
--rw-r--r--   0 sage       (501) staff       (20)      133 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/phe/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     5120 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/phe/_keypair.py
--rw-r--r--   0 sage       (501) staff       (20)     9328 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/phe/_ops.py
--rw-r--r--   0 sage       (501) staff       (20)     4180 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/tensor/phe/_tensor.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.201291 pyfate-2.0.0b0/fate/arch/unify/
--rw-r--r--   0 sage       (501) staff       (20)      822 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/unify/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1500 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/unify/_infra_def.py
--rw-r--r--   0 sage       (501) staff       (20)     2411 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/unify/_io.py
--rw-r--r--   0 sage       (501) staff       (20)      891 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/arch/unify/_uuid.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.201523 pyfate-2.0.0b0/fate/components/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      993 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/__main__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.204648 pyfate-2.0.0b0/fate/components/components/
--rw-r--r--   0 sage       (501) staff       (20)     4475 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3773 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/artifact_test.py
--rw-r--r--   0 sage       (501) staff       (20)    16780 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/coordinated_linr.py
--rw-r--r--   0 sage       (501) staff       (20)    17043 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/coordinated_lr.py
--rw-r--r--   0 sage       (501) staff       (20)     2023 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/cross_validation_test.py
--rw-r--r--   0 sage       (501) staff       (20)     4334 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/data_split.py
--rw-r--r--   0 sage       (501) staff       (20)     2308 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/dataframe_io_test.py
--rw-r--r--   0 sage       (501) staff       (20)     2190 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/dataframe_transformer.py
--rw-r--r--   0 sage       (501) staff       (20)     4738 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/evaluation.py
--rw-r--r--   0 sage       (501) staff       (20)     5894 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/feature_scale.py
--rw-r--r--   0 sage       (501) staff       (20)    11167 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/hetero_feature_binning.py
--rw-r--r--   0 sage       (501) staff       (20)     6351 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/hetero_feature_selection.py
--rw-r--r--   0 sage       (501) staff       (20)     6341 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/hetero_sbt.py
--rw-r--r--   0 sage       (501) staff       (20)     6097 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/homo_lr.py
--rw-r--r--   0 sage       (501) staff       (20)     9232 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/homo_nn.py
--rw-r--r--   0 sage       (501) staff       (20)     2740 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/multi_model_test.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.205042 pyfate-2.0.0b0/fate/components/components/nn/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     5233 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/loader.py
--rw-r--r--   0 sage       (501) staff       (20)    10354 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/nn_runner.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.205350 pyfate-2.0.0b0/fate/components/components/nn/runner/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/runner/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    15208 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/runner/default_runner.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.205898 pyfate-2.0.0b0/fate/components/components/nn/torch/
--rw-r--r--   0 sage       (501) staff       (20)      615 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/torch/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2987 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/torch/base.py
--rw-r--r--   0 sage       (501) staff       (20)    80820 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/torch/nn.py
--rw-r--r--   0 sage       (501) staff       (20)    13646 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/torch/optim.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.206331 pyfate-2.0.0b0/fate/components/components/nn/utils/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2081 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/utils/extract_pytorch_optim.py
--rw-r--r--   0 sage       (501) staff       (20)     3475 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/nn/utils/extract_torch_modules.py
--rw-r--r--   0 sage       (501) staff       (20)     1189 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/psi.py
--rw-r--r--   0 sage       (501) staff       (20)     2103 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/reader.py
--rw-r--r--   0 sage       (501) staff       (20)     3869 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/sample.py
--rw-r--r--   0 sage       (501) staff       (20)     3437 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/statistics.py
--rw-r--r--   0 sage       (501) staff       (20)     2076 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/toy_example.py
--rw-r--r--   0 sage       (501) staff       (20)     1240 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/union.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.206735 pyfate-2.0.0b0/fate/components/components/utils/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      349 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/utils/consts.py
--rw-r--r--   0 sage       (501) staff       (20)      513 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/components/utils/tools.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.207709 pyfate-2.0.0b0/fate/components/core/
--rw-r--r--   0 sage       (501) staff       (20)      703 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2030 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/_cpn_reexport.py
--rw-r--r--   0 sage       (501) staff       (20)     3267 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/_cpn_search.py
--rw-r--r--   0 sage       (501) staff       (20)     1537 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/_load_computing.py
--rw-r--r--   0 sage       (501) staff       (20)      954 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/_load_device.py
--rw-r--r--   0 sage       (501) staff       (20)     5270 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/_load_federation.py
--rw-r--r--   0 sage       (501) staff       (20)     1149 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/_load_metric_handler.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.208554 pyfate-2.0.0b0/fate/components/core/component_desc/
--rw-r--r--   0 sage       (501) staff       (20)     1883 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    13847 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/_component.py
--rw-r--r--   0 sage       (501) staff       (20)     9741 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/_component_artifact.py
--rw-r--r--   0 sage       (501) staff       (20)    13953 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/_component_io.py
--rw-r--r--   0 sage       (501) staff       (20)      951 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/_metric.py
--rw-r--r--   0 sage       (501) staff       (20)     4499 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/_parameter.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.208836 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/
--rw-r--r--   0 sage       (501) staff       (20)     1274 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     4473 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/_base_type.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.209370 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/data/
--rw-r--r--   0 sage       (501) staff       (20)     3079 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/data/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3363 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/data/_dataframe.py
--rw-r--r--   0 sage       (501) staff       (20)     1679 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/data/_directory.py
--rw-r--r--   0 sage       (501) staff       (20)     1590 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/data/_table.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.209626 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/metric/
--rw-r--r--   0 sage       (501) staff       (20)      672 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/metric/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2427 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/metric/_json.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.211639 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/model/
--rw-r--r--   0 sage       (501) staff       (20)     2185 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/model/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2046 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/model/_directory.py
--rw-r--r--   0 sage       (501) staff       (20)     2340 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/model/_json.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.212347 pyfate-2.0.0b0/fate/components/core/essential/
--rw-r--r--   0 sage       (501) staff       (20)      366 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/essential/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      897 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/essential/_artifact_type.py
--rw-r--r--   0 sage       (501) staff       (20)      840 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/essential/_label.py
--rw-r--r--   0 sage       (501) staff       (20)     1975 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/essential/_role.py
--rw-r--r--   0 sage       (501) staff       (20)     1749 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/essential/_stage.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.213883 pyfate-2.0.0b0/fate/components/core/params/
--rw-r--r--   0 sage       (501) staff       (20)     1296 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      318 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_cipher.py
--rw-r--r--   0 sage       (501) staff       (20)      251 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_cv_param.py
--rw-r--r--   0 sage       (501) staff       (20)     2419 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_fields.py
--rw-r--r--   0 sage       (501) staff       (20)     3937 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_filter_param.py
--rw-r--r--   0 sage       (501) staff       (20)      872 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_he_param.py
--rw-r--r--   0 sage       (501) staff       (20)      841 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_init_param.py
--rw-r--r--   0 sage       (501) staff       (20)      370 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_learning_rate.py
--rw-r--r--   0 sage       (501) staff       (20)     2426 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_metrics.py
--rw-r--r--   0 sage       (501) staff       (20)      555 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_optimizer.py
--rw-r--r--   0 sage       (501) staff       (20)      352 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/params/_penalty.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.215247 pyfate-2.0.0b0/fate/components/core/spec/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3514 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/artifact.py
--rw-r--r--   0 sage       (501) staff       (20)     3059 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/component.py
--rw-r--r--   0 sage       (501) staff       (20)     1341 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/computing.py
--rw-r--r--   0 sage       (501) staff       (20)      830 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/device.py
--rw-r--r--   0 sage       (501) staff       (20)     3950 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/federation.py
--rw-r--r--   0 sage       (501) staff       (20)     2249 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/logger.py
--rw-r--r--   0 sage       (501) staff       (20)      228 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/metric.py
--rw-r--r--   0 sage       (501) staff       (20)     1415 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/model.py
--rw-r--r--   0 sage       (501) staff       (20)     2365 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/core/spec/task.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.215482 pyfate-2.0.0b0/fate/components/entrypoint/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.215607 pyfate-2.0.0b0/fate/components/entrypoint/cli/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.216620 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      591 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/__main__.py
--rw-r--r--   0 sage       (501) staff       (20)      763 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/artifact_type_cli.py
--rw-r--r--   0 sage       (501) staff       (20)     1268 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/cleanup_cli.py
--rw-r--r--   0 sage       (501) staff       (20)      496 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/desc_cli.py
--rw-r--r--   0 sage       (501) staff       (20)     5833 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/execute_cli.py
--rw-r--r--   0 sage       (501) staff       (20)      408 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/list_cli.py
--rw-r--r--   0 sage       (501) staff       (20)      455 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/component/task_schema_cli.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.216951 pyfate-2.0.0b0/fate/components/entrypoint/cli/test/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/test/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      232 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/test/__main__.py
--rw-r--r--   0 sage       (501) staff       (20)      794 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/cli/test/execute.py
--rw-r--r--   0 sage       (501) staff       (20)     2378 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/components/entrypoint/utils.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.217069 pyfate-2.0.0b0/fate/ml/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.217305 pyfate-2.0.0b0/fate/ml/abc/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/abc/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1335 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/abc/module.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.218292 pyfate-2.0.0b0/fate/ml/aggregator/
--rw-r--r--   0 sage       (501) staff       (20)      648 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/aggregator/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     8733 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/aggregator/base.py
--rw-r--r--   0 sage       (501) staff       (20)      561 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/aggregator/plaintext_aggregator.py
--rw-r--r--   0 sage       (501) staff       (20)      558 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/aggregator/secure_aggregator.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.218438 pyfate-2.0.0b0/fate/ml/ensemble/
--rw-r--r--   0 sage       (501) staff       (20)      370 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.218574 pyfate-2.0.0b0/fate/ml/ensemble/algo/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.218891 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.219268 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/common/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/common/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     6896 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/common/predict.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.219865 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     2847 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/_base.py
--rw-r--r--   0 sage       (501) staff       (20)    13257 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/guest.py
--rw-r--r--   0 sage       (501) staff       (20)     3879 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/host.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.219996 pyfate-2.0.0b0/fate/ml/ensemble/learner/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.220127 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/
--rw-r--r--   0 sage       (501) staff       (20)      170 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.220543 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    15697 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/guest.py
--rw-r--r--   0 sage       (501) staff       (20)     9147 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/host.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.221706 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    17767 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/decision_tree.py
--rw-r--r--   0 sage       (501) staff       (20)     8795 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/hist.py
--rw-r--r--   0 sage       (501) staff       (20)     4435 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/loss.py
--rw-r--r--   0 sage       (501) staff       (20)    22525 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/splitter.py
--rw-r--r--   0 sage       (501) staff       (20)     2202 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/subsample.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.221954 pyfate-2.0.0b0/fate/ml/ensemble/utils/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     1177 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/ensemble/utils/binning.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.222679 pyfate-2.0.0b0/fate/ml/evaluation/
--rw-r--r--   0 sage       (501) staff       (20)      615 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/evaluation/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    28761 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/evaluation/classification.py
--rw-r--r--   0 sage       (501) staff       (20)     4013 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/evaluation/metric_base.py
--rw-r--r--   0 sage       (501) staff       (20)     1392 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/evaluation/regression.py
--rw-r--r--   0 sage       (501) staff       (20)     2668 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/evaluation/tool.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.223009 pyfate-2.0.0b0/fate/ml/feature_binning/
--rw-r--r--   0 sage       (501) staff       (20)      701 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/feature_binning/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    20642 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/feature_binning/hetero_feature_binning.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.223267 pyfate-2.0.0b0/fate/ml/feature_selection/
--rw-r--r--   0 sage       (501) staff       (20)      707 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/feature_selection/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    25665 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/feature_selection/hetero_feature_selection.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.223395 pyfate-2.0.0b0/fate/ml/glm/
--rw-r--r--   0 sage       (501) staff       (20)      316 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.223511 pyfate-2.0.0b0/fate/ml/glm/hetero/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.224103 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/
--rw-r--r--   0 sage       (501) staff       (20)      755 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     8776 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/arbiter.py
--rw-r--r--   0 sage       (501) staff       (20)    10774 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/guest.py
--rw-r--r--   0 sage       (501) staff       (20)     9513 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/host.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.224734 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/
--rw-r--r--   0 sage       (501) staff       (20)      749 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    11928 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/arbiter.py
--rw-r--r--   0 sage       (501) staff       (20)    16911 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/guest.py
--rw-r--r--   0 sage       (501) staff       (20)    13284 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/host.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.224882 pyfate-2.0.0b0/fate/ml/glm/homo/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/homo/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.225272 pyfate-2.0.0b0/fate/ml/glm/homo/lr/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/homo/lr/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    17357 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/homo/lr/client.py
--rw-r--r--   0 sage       (501) staff       (20)      732 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/glm/homo/lr/server.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.225673 pyfate-2.0.0b0/fate/ml/model_selection/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/model_selection/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     9368 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/model_selection/data_split.py
--rw-r--r--   0 sage       (501) staff       (20)     4338 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/model_selection/sample.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.225831 pyfate-2.0.0b0/fate/ml/nn/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.225947 pyfate-2.0.0b0/fate/ml/nn/algo/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/algo/__init__.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.226185 pyfate-2.0.0b0/fate/ml/nn/algo/homo/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/algo/homo/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     4475 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/algo/homo/fedavg.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.226551 pyfate-2.0.0b0/fate/ml/nn/dataset/
--rw-r--r--   0 sage       (501) staff       (20)        0 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/dataset/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      867 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/dataset/base.py
--rw-r--r--   0 sage       (501) staff       (20)     8960 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/dataset/table.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.226873 pyfate-2.0.0b0/fate/ml/nn/model_zoo/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/model_zoo/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)      409 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/model_zoo/multi_model.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.227115 pyfate-2.0.0b0/fate/ml/nn/trainer/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/trainer/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)    36226 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/nn/trainer/trainer_base.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.227528 pyfate-2.0.0b0/fate/ml/preprocessing/
--rw-r--r--   0 sage       (501) staff       (20)      680 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/preprocessing/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     6610 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/preprocessing/feature_scale.py
--rw-r--r--   0 sage       (501) staff       (20)     2160 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/preprocessing/union.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.227768 pyfate-2.0.0b0/fate/ml/statistics/
--rw-r--r--   0 sage       (501) staff       (20)      657 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/statistics/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     5532 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/statistics/statistics.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.228954 pyfate-2.0.0b0/fate/ml/utils/
--rw-r--r--   0 sage       (501) staff       (20)      614 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/__init__.py
--rw-r--r--   0 sage       (501) staff       (20)     3116 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/_convergence.py
--rw-r--r--   0 sage       (501) staff       (20)     2561 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/_model_param.py
--rw-r--r--   0 sage       (501) staff       (20)    13239 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/_optimizer.py
--rw-r--r--   0 sage       (501) staff       (20)     2520 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/callbacks.py
--rw-r--r--   0 sage       (501) staff       (20)      108 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/label_alignment.py
--rw-r--r--   0 sage       (501) staff       (20)      669 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/model_io.py
--rw-r--r--   0 sage       (501) staff       (20)     1231 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/model_serdes.py
--rw-r--r--   0 sage       (501) staff       (20)     5402 2023-09-15 07:18:47.000000 pyfate-2.0.0b0/fate/ml/utils/predict_tools.py
-drwxr-xr-x   0 sage       (501) staff       (20)        0 2023-09-19 06:27:02.229689 pyfate-2.0.0b0/pyfate.egg-info/
--rw-r--r--   0 sage       (501) staff       (20)     7859 2023-09-19 06:27:02.000000 pyfate-2.0.0b0/pyfate.egg-info/PKG-INFO
--rw-r--r--   0 sage       (501) staff       (20)    13549 2023-09-19 06:27:02.000000 pyfate-2.0.0b0/pyfate.egg-info/SOURCES.txt
--rw-r--r--   0 sage       (501) staff       (20)        1 2023-09-19 06:27:02.000000 pyfate-2.0.0b0/pyfate.egg-info/dependency_links.txt
--rw-r--r--   0 sage       (501) staff       (20)      703 2023-09-19 06:27:02.000000 pyfate-2.0.0b0/pyfate.egg-info/requires.txt
--rw-r--r--   0 sage       (501) staff       (20)        5 2023-09-19 06:27:02.000000 pyfate-2.0.0b0/pyfate.egg-info/top_level.txt
--rw-r--r--   0 sage       (501) staff       (20)       38 2023-09-19 06:27:02.231226 pyfate-2.0.0b0/setup.cfg
--rw-r--r--   0 sage       (501) staff       (20)     1874 2023-09-19 06:22:02.000000 pyfate-2.0.0b0/setup.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.977353 pyfate-2.1.0/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8645 2024-04-03 07:08:36.976971 pyfate-2.1.0/PKG-INFO
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.757242 pyfate-2.1.0/fate/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      659 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      658 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/_info.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.757587 pyfate-2.1.0/fate/arch/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      741 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.758570 pyfate-2.1.0/fate/arch/computing/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      671 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3152 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/_builder.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.760885 pyfate-2.1.0/fate/arch/computing/api/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      753 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/api/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    33165 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/api/_table.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      755 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/api/_type.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      866 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/api/_uuid.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.761206 pyfate-2.1.0/fate/arch/computing/backends/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.762810 pyfate-2.1.0/fate/arch/computing/backends/eggroll/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      708 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/eggroll/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4457 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/eggroll/_csession.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5328 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/eggroll/_table.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      999 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/eggroll/_type.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.764175 pyfate-2.1.0/fate/arch/computing/backends/spark/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      936 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/spark/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3370 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/spark/_csession.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      932 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/spark/_materialize.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12805 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/spark/_table.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.766697 pyfate-2.1.0/fate/arch/computing/backends/standalone/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      770 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/standalone/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4116 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/standalone/_csession.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    46371 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/standalone/_standalone.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4553 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/standalone/_table.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      741 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/backends/standalone/_type.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.768416 pyfate-2.1.0/fate/arch/computing/partitioners/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1497 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/partitioners/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      728 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/partitioners/_integer_partitioner.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1021 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/partitioners/_java_string_like_partitioner.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      730 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/partitioners/_mmh3_partitioner.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.770775 pyfate-2.1.0/fate/arch/computing/serdes/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1387 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/serdes/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      895 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/serdes/_integer_serdes.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2878 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/serdes/_restricted_serdes.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2571 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/serdes/_safe_serdes.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      725 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/serdes/_serdes_base.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1079 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/computing/serdes/_unrestricted_serdes.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.771882 pyfate-2.1.0/fate/arch/config/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      659 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/config/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2581 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/config/_config.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1569 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/config/default.yaml
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.775122 pyfate-2.1.0/fate/arch/context/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      907 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8626 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_cipher.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8744 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_context.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3203 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_context_helper.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6720 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_metrics.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6995 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_mpc.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2378 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_namespace.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6616 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/context/_parties.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.776291 pyfate-2.1.0/fate/arch/dataframe/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1168 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    19029 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/_dataframe.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12275 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/_frame_reader.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.777135 pyfate-2.1.0/fate/arch/dataframe/conf/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/conf/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      677 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/conf/default_config.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.777976 pyfate-2.1.0/fate/arch/dataframe/entity/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/entity/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      676 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/entity/types.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.779504 pyfate-2.1.0/fate/arch/dataframe/io/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      795 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/io/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1241 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/io/_json_schema.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1591 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/io/_json_serialization.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.781191 pyfate-2.1.0/fate/arch/dataframe/manager/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      732 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/manager/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    27872 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/manager/block_manager.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11387 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/manager/data_manager.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    24114 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/manager/schema_manager.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.782552 pyfate-2.1.0/fate/arch/dataframe/manager/utils/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      669 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/manager/utils/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4843 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/manager/utils/_anonymous_generator.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.793205 pyfate-2.1.0/fate/arch/dataframe/ops/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1939 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_activation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6825 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_apply_row.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4456 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_arithmetic.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5081 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_cmp.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2527 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_compress_block.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1157 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_data_overview.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    15433 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_dimension_scaling.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6827 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_encoder.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1777 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_field_extract.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3561 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_fillna.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4169 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_histogram.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13351 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_indexer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4732 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_isin.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1810 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_missing.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2611 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_promote_types.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2496 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_quantile.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3610 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_replace.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13083 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_set_item.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4007 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_sort.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7809 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_stat.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7779 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_transformer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1202 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_unary_operator.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4209 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/_where.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.794281 pyfate-2.1.0/fate/arch/dataframe/ops/utils/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/utils/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1958 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/utils/operators.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1267 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/ops/utils/series_align.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.796346 pyfate-2.1.0/fate/arch/dataframe/utils/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      788 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/utils/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      784 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/utils/_auto_column_name_generated.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6895 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/utils/_dataloader.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      897 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/utils/_id_generator.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3319 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/utils/_k_fold.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8461 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/dataframe/utils/_sample.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.796984 pyfate-2.1.0/fate/arch/federation/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      922 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6418 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/_builder.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.799546 pyfate-2.1.0/fate/arch/federation/api/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      984 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/api/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5593 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/api/_federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12724 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/api/_serdes.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      939 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/api/_table_meta.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      827 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/api/_type.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.800086 pyfate-2.1.0/fate/arch/federation/backends/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.801141 pyfate-2.1.0/fate/arch/federation/backends/eggroll/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      693 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/eggroll/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5360 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/eggroll/_federation.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.804321 pyfate-2.1.0/fate/arch/federation/backends/osx/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      858 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/osx/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5263 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/osx/_federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7878 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/osx/_mq_channel.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10906 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/osx/osx_pb2.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11761 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/osx/osx_pb2_grpc.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.806891 pyfate-2.1.0/fate/arch/federation/backends/pulsar/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      730 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/pulsar/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    16303 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/pulsar/_federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8731 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/pulsar/_mq_channel.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8779 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/pulsar/_pulsar_manager.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.809133 pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      692 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9371 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/_federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4514 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/_mq_channel.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    15417 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/_rabbit_manager.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.810275 pyfate-2.1.0/fate/arch/federation/backends/standalone/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      699 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/standalone/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2441 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/backends/standalone/_federation.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.813295 pyfate-2.1.0/fate/arch/federation/message_queue/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      724 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/message_queue/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1299 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/message_queue/_datastream.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    17966 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/message_queue/_federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1324 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/message_queue/_nretry.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1274 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/federation/message_queue/_parties.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.815864 pyfate-2.1.0/fate/arch/histogram/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      757 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11325 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/_histogram_distributed.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4248 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/_histogram_local.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3023 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/_histogram_sbt.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5008 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/_histogram_splits.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.816837 pyfate-2.1.0/fate/arch/histogram/indexer/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      836 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/indexer/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8306 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/indexer/_indexer.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.820051 pyfate-2.1.0/fate/arch/histogram/values/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      701 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/values/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6037 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/values/_cipher.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2453 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/values/_encoded.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7054 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/values/_plain.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2335 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/values/_value.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9394 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/histogram/values/_values.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.823111 pyfate-2.1.0/fate/arch/launchers/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/launchers/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9250 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/launchers/argparser.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4012 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/launchers/context_helper.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1609 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/launchers/logger.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10034 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/launchers/multiprocess_launcher.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      790 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/launchers/paths.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.823588 pyfate-2.1.0/fate/arch/protocol/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      758 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.824126 pyfate-2.1.0/fate/arch/protocol/diffie_hellman/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      707 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/diffie_hellman/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.827427 pyfate-2.1.0/fate/arch/protocol/mpc/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    18200 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.829633 pyfate-2.1.0/fate/arch/protocol/mpc/common/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      254 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/common/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      536 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/common/encoding.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3261 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/common/rng.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5146 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/common/serial.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1218 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/common/tensor_types.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3819 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/common/util.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.830278 pyfate-2.1.0/fate/arch/protocol/mpc/communicator/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      232 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/communicator/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    19521 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/communicator/communicator.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.830703 pyfate-2.1.0/fate/arch/protocol/mpc/config/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      255 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/config/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    26146 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/cryptensor.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.831951 pyfate-2.1.0/fate/arch/protocol/mpc/cuda/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)       71 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/cuda/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    19020 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/cuda/cuda_tensor.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.833116 pyfate-2.1.0/fate/arch/protocol/mpc/debug/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      847 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/debug/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4492 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/debug/debug.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3685 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/encoder.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.837347 pyfate-2.1.0/fate/arch/protocol/mpc/functions/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      430 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    14649 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/approximations.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1434 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/dropout.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2818 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/logic.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13015 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/maximum.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13292 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/pooling.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4126 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/power.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9196 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/regular.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3005 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/functions/sampling.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    64255 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/gradients.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12436 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/mpc.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.837698 pyfate-2.1.0/fate/arch/protocol/mpc/nn/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      201 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.838523 pyfate-2.1.0/fate/arch/protocol/mpc/nn/privacy/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      293 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/privacy/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    20534 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/privacy/dp_split.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.839992 pyfate-2.1.0/fate/arch/protocol/mpc/nn/sshe/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/sshe/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7228 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/sshe/linr_layer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7663 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/sshe/lr_layer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7175 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/nn/sshe/nn_layer.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.844632 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      349 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    26113 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/arithmetic.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7110 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/beaver.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    17166 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/binary.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4714 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/circuit.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2671 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/converters.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.845482 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/ot/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/ot/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4904 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/ot/baseOT.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4115 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/replicated.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9269 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/primitives/sshe.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.848162 pyfate-2.1.0/fate/arch/protocol/mpc/provider/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      446 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/provider/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1217 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/provider/homomorphic_provider.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6410 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/provider/provider.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3065 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/provider/tfp_provider.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12443 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/provider/ttp_provider.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.848623 pyfate-2.1.0/fate/arch/protocol/mpc/provider/tuple_cache/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/provider/tuple_cache/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      688 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/mpc/ptype.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.850885 pyfate-2.1.0/fate/arch/protocol/phe/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/phe/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    15641 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/phe/mock.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13946 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/phe/ou.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13868 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/phe/paillier.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1234 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/phe/type.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.851830 pyfate-2.1.0/fate/arch/protocol/psi/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      647 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/psi/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1178 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/psi/_psi_run.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.852783 pyfate-2.1.0/fate/arch/protocol/psi/ecdh/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      616 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/psi/ecdh/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6469 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/psi/ecdh/_run.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.853840 pyfate-2.1.0/fate/arch/protocol/secure_aggregation/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      759 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/secure_aggregation/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4943 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/protocol/secure_aggregation/_secure_aggregation.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.854955 pyfate-2.1.0/fate/arch/tensor/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      738 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4355 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/_custom_ops.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.861450 pyfate-2.1.0/fate/arch/tensor/distributed/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      932 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1230 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_op_broadcast.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10381 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_op_matmul.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3062 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_op_slice.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1374 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_op_stack.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2263 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_op_transpose.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12371 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_ops_agg.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3048 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_ops_binary.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1852 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_ops_cipher.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1020 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_ops_others.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1290 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_ops_unary.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    15609 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/distributed/_tensor.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.862456 pyfate-2.1.0/fate/arch/tensor/inside/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      651 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/inside/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2829 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/inside/_op_quantile.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.865235 pyfate-2.1.0/fate/arch/tensor/phe/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      941 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/phe/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6032 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/phe/_keypair.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10093 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/phe/_ops.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4859 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/tensor/phe/_tensor.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.866492 pyfate-2.1.0/fate/arch/trace/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1059 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/trace/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11971 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/trace/_profile.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8452 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/trace/_trace.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.867862 pyfate-2.1.0/fate/arch/unify/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      742 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/unify/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1403 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/unify/_infra_def.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2411 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/unify/_io.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      674 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/arch/unify/_uuid.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.868522 pyfate-2.1.0/fate/components/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      993 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/__main__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.882069 pyfate-2.1.0/fate/components/components/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4959 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4388 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/artifact_test.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    16390 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/coordinated_linr.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    16532 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/coordinated_lr.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2638 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/cross_validation_test.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3999 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/data_split.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2308 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/dataframe_io_test.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2242 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/dataframe_transformer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4605 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/evaluation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2561 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/feature_correlation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5682 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/feature_scale.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10448 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/hetero_feature_binning.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6295 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/hetero_feature_selection.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2491 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/hetero_nn.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13200 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/hetero_secureboost.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5624 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/homo_lr.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3229 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/homo_nn.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2740 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/multi_model_test.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.885173 pyfate-2.1.0/fate/components/components/nn/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8207 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/component_utils.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.886231 pyfate-2.1.0/fate/components/components/nn/hook_code/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/hook_code/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2726 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/hook_code/extract_pytorch_optim.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4116 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/hook_code/extract_torch_modules.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6288 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/loader.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12813 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/nn_runner.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.887217 pyfate-2.1.0/fate/components/components/nn/runner/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/runner/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12046 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/runner/hetero_default_runner.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13109 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/runner/homo_default_runner.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)       13 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/source.yaml
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.889008 pyfate-2.1.0/fate/components/components/nn/torch/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      616 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/torch/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3454 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/torch/base.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    76229 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/torch/nn.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13903 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/nn/torch/optim.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1524 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/psi.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1015 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/reader.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4841 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/sample.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10025 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/sshe_linr.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9999 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/sshe_lr.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3341 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/statistics.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2057 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/toy_example.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1552 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/union.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.892947 pyfate-2.1.0/fate/components/components/utils/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/utils/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      965 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/utils/consts.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1157 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/components/utils/tools.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.896230 pyfate-2.1.0/fate/components/core/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1453 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2270 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_cpn_reexport.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3267 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_cpn_search.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      985 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_cpn_task_mode.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1754 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_load_computing.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      954 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_load_device.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4647 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_load_federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1149 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/_load_metric_handler.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.898266 pyfate-2.1.0/fate/components/core/component_desc/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2123 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13846 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/_component.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10356 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/_component_artifact.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    14567 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/_component_io.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1566 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/_metric.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5114 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/_parameter.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.899010 pyfate-2.1.0/fate/components/core/component_desc/artifacts/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2129 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5088 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/_base_type.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.901109 pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4284 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4072 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/_dataframe.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2294 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/_directory.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2205 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/_table.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2019 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/_unresolved.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.901788 pyfate-2.1.0/fate/components/core/component_desc/artifacts/metric/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1287 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/metric/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3033 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/metric/_json.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.903077 pyfate-2.1.0/fate/components/core/component_desc/artifacts/model/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3407 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/model/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2661 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/model/_directory.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2955 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/model/_json.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1974 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/component_desc/artifacts/model/_unresolved.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.904789 pyfate-2.1.0/fate/components/core/essential/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1046 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/essential/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1802 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/essential/_artifact_type.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      840 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/essential/_label.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1975 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/essential/_role.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1749 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/essential/_stage.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.908455 pyfate-2.1.0/fate/components/core/params/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1296 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      933 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_cipher.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      866 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_cv_param.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3034 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_fields.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3947 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_filter_param.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      872 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_he_param.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1456 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_init_param.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      985 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_learning_rate.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2427 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_metrics.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1170 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_optimizer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      967 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/params/_penalty.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.912954 pyfate-2.1.0/fate/components/core/spec/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3595 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/artifact.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3073 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/component.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1552 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/computing.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      830 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/device.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3990 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/federation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2440 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/logger.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      843 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/metric.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1412 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/model.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2365 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/core/spec/task.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.913859 pyfate-2.1.0/fate/components/entrypoint/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.914376 pyfate-2.1.0/fate/components/entrypoint/cli/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.918437 pyfate-2.1.0/fate/components/entrypoint/cli/component/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1206 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/__main__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1378 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/artifact_type_cli.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2000 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/cleanup_cli.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1111 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/desc_cli.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6203 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/execute_cli.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1023 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/list_cli.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1070 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/component/task_schema_cli.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.920045 pyfate-2.1.0/fate/components/entrypoint/cli/test/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/test/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      847 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/test/__main__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1249 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/cli/test/execute.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2994 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/components/entrypoint/utils.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.920630 pyfate-2.1.0/fate/ml/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.921827 pyfate-2.1.0/fate/ml/abc/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/abc/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1379 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/abc/module.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.924386 pyfate-2.1.0/fate/ml/aggregator/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1444 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/aggregator/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5932 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/aggregator/aggregator_wrapper.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9397 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/aggregator/base.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1175 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/aggregator/plaintext_aggregator.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1171 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/aggregator/secure_aggregator.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.924908 pyfate-2.1.0/fate/ml/ensemble/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      985 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.925433 pyfate-2.1.0/fate/ml/ensemble/algo/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.925891 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.926873 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/common/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/common/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6950 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/common/predict.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.929093 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3590 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/_base.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    17364 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/guest.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4659 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/host.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.929580 pyfate-2.1.0/fate/ml/ensemble/learner/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.930186 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      785 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.932052 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    17036 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/guest.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9683 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/host.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.935706 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    18288 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/decision_tree.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9168 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/hist.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4936 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/loss.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    14053 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/splitter.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2202 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/subsample.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.937248 pyfate-2.1.0/fate/ml/ensemble/utils/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/utils/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1177 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/utils/binning.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1489 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/ensemble/utils/sample.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.940215 pyfate-2.1.0/fate/ml/evaluation/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/evaluation/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    28586 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/evaluation/classification.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4790 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/evaluation/metric_base.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2003 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/evaluation/regression.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3340 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/evaluation/tool.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.941263 pyfate-2.1.0/fate/ml/feature_binning/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      701 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/feature_binning/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    20169 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/feature_binning/hetero_feature_binning.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.942631 pyfate-2.1.0/fate/ml/feature_selection/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      707 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/feature_selection/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    25605 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/feature_selection/hetero_feature_selection.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.943466 pyfate-2.1.0/fate/ml/glm/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      948 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.943959 pyfate-2.1.0/fate/ml/glm/hetero/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.946008 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      755 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8776 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/arbiter.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10989 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/guest.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9726 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/host.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.948096 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      749 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11928 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/arbiter.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    17071 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/guest.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13495 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/host.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.949688 pyfate-2.1.0/fate/ml/glm/hetero/sshe/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      705 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/sshe/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    13995 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/sshe/sshe_linr.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    18671 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/hetero/sshe/sshe_lr.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.950433 pyfate-2.1.0/fate/ml/glm/homo/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/homo/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.952330 pyfate-2.1.0/fate/ml/glm/homo/lr/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/homo/lr/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    17195 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/homo/lr/client.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1272 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/glm/homo/lr/server.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.953819 pyfate-2.1.0/fate/ml/model_selection/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/model_selection/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11471 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/model_selection/data_split.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3509 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/model_selection/sample.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.954273 pyfate-2.1.0/fate/ml/nn/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.955757 pyfate-2.1.0/fate/ml/nn/dataset/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/dataset/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1482 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/dataset/base.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     9617 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/dataset/table.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.956791 pyfate-2.1.0/fate/ml/nn/hetero/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/hetero/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7596 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/hetero/hetero_nn.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.957808 pyfate-2.1.0/fate/ml/nn/homo/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/homo/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3822 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/homo/fedavg.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.959520 pyfate-2.1.0/fate/ml/nn/model_zoo/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/__init__.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.960468 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     7470 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/agg_layer.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.962048 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/fedpass/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/fedpass/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    11110 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/fedpass/_passport_block.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5774 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/fedpass/agg_layer.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.962977 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/sshe/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/sshe/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5741 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/agg_layer/sshe/agg_layer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    14248 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/hetero_nn_model.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1024 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/model_zoo/multi_model.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.967434 pyfate-2.1.0/fate/ml/nn/test/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2906 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_agglayer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10046 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_fedpass_alexnet.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    10298 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_fedpass_lenet.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4665 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_fedpass_tabular.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4729 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_hetero_nn_algo.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4371 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_hetero_nn_algo_no_guest.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5023 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_hetero_nn_algo_val.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5006 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_hetero_nn_sshe.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3266 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_homo_nn_binary.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     4010 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/test/test_sshe_nn_layer.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.968063 pyfate-2.1.0/fate/ml/nn/trainer/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/trainer/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    44779 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/nn/trainer/trainer_base.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.969455 pyfate-2.1.0/fate/ml/preprocessing/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      680 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/preprocessing/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6563 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/preprocessing/feature_scale.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2160 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/preprocessing/union.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.970628 pyfate-2.1.0/fate/ml/statistics/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      709 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/statistics/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5799 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/statistics/pearson_correlation.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     5475 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/statistics/statistics.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.973826 pyfate-2.1.0/fate/ml/utils/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      614 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/utils/__init__.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3303 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/utils/_convergence.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3403 2024-04-03 07:01:44.000000 pyfate-2.1.0/fate/ml/utils/_model_param.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    12889 2024-04-03 07:01:45.000000 pyfate-2.1.0/fate/ml/utils/_optimizer.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     3043 2024-04-03 07:01:45.000000 pyfate-2.1.0/fate/ml/utils/callbacks.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      723 2024-04-03 07:01:45.000000 pyfate-2.1.0/fate/ml/utils/label_alignment.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1284 2024-04-03 07:01:45.000000 pyfate-2.1.0/fate/ml/utils/model_io.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     1218 2024-04-03 07:01:45.000000 pyfate-2.1.0/fate/ml/utils/model_serdes.py
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     6087 2024-04-03 07:01:45.000000 pyfate-2.1.0/fate/ml/utils/predict_tools.py
+drwxr-xr-x   0 maguoqiang   (501) staff       (20)        0 2024-04-03 07:08:36.975546 pyfate-2.1.0/pyfate.egg-info/
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     8645 2024-04-03 07:08:36.000000 pyfate-2.1.0/pyfate.egg-info/PKG-INFO
+-rw-r--r--   0 maguoqiang   (501) staff       (20)    19032 2024-04-03 07:08:36.000000 pyfate-2.1.0/pyfate.egg-info/SOURCES.txt
+-rw-r--r--   0 maguoqiang   (501) staff       (20)        1 2024-04-03 07:08:36.000000 pyfate-2.1.0/pyfate.egg-info/dependency_links.txt
+-rw-r--r--   0 maguoqiang   (501) staff       (20)      504 2024-04-03 07:08:36.000000 pyfate-2.1.0/pyfate.egg-info/requires.txt
+-rw-r--r--   0 maguoqiang   (501) staff       (20)        5 2024-04-03 07:08:36.000000 pyfate-2.1.0/pyfate.egg-info/top_level.txt
+-rw-r--r--   0 maguoqiang   (501) staff       (20)       38 2024-04-03 07:08:36.977429 pyfate-2.1.0/setup.cfg
+-rw-r--r--   0 maguoqiang   (501) staff       (20)     2330 2024-04-03 07:01:45.000000 pyfate-2.1.0/setup.py
```

### Comparing `pyfate-2.0.0b0/PKG-INFO` & `pyfate-2.1.0/PKG-INFO`

 * *Files 15% similar despite different names*

```diff
@@ -1,50 +1,49 @@
 Metadata-Version: 2.1
 Name: pyfate
-Version: 2.0.0b0
+Version: 2.1.0
 Home-page: https://fate.fedai.org/
 Author: FederatedAI
 Author-email: contact@FedAI.org
 License: Apache-2.0 License
 Keywords: federated learning
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: lmdb==1.3.0
 Requires-Dist: torch==1.13.1
 Requires-Dist: fate_utils
 Requires-Dist: pydantic==1.10.12
 Requires-Dist: cloudpickle==2.1.0
 Requires-Dist: click
 Requires-Dist: ruamel.yaml==0.16
-Requires-Dist: scikit-learn==1.2.1; sys_platform == "darwin" and platform_machine == "arm64"
-Requires-Dist: scikit-learn==1.0.1; sys_platform != "darwin" or platform_machine != "arm64"
+Requires-Dist: scikit-learn
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: transformers
 Requires-Dist: accelerate
 Requires-Dist: beautifultable
 Requires-Dist: requests
 Requires-Dist: grpcio
 Requires-Dist: protobuf
+Requires-Dist: rich
+Requires-Dist: omegaconf
+Requires-Dist: opentelemetry-api
+Requires-Dist: opentelemetry-sdk
+Requires-Dist: mmh3==3.0.0
+Requires-Dist: safetensors
 Provides-Extra: rabbitmq
-Requires-Dist: pika==1.2.1; extra == "rabbitmq"
+Requires-Dist: pika; extra == "rabbitmq"
 Provides-Extra: pulsar
 Requires-Dist: pulsar-client==2.10.2; sys_platform != "darwin" and extra == "pulsar"
-Requires-Dist: pulsar-client==2.10.1; sys_platform == "darwin" and extra == "pulsar"
-Requires-Dist: urllib3==1.26.5; extra == "pulsar"
+Requires-Dist: pulsar-client; sys_platform == "darwin" and extra == "pulsar"
+Requires-Dist: urllib3; extra == "pulsar"
 Provides-Extra: spark
 Requires-Dist: pyspark; extra == "spark"
 Provides-Extra: eggroll
-Requires-Dist: grpcio==1.46.3; extra == "eggroll"
-Requires-Dist: grpcio-tools==1.46.3; extra == "eggroll"
-Requires-Dist: numba==0.56.4; extra == "eggroll"
-Requires-Dist: protobuf==3.19.6; extra == "eggroll"
-Requires-Dist: mmh3==3.0.0; extra == "eggroll"
-Requires-Dist: cachetools>=3.0.0; extra == "eggroll"
-Requires-Dist: cloudpickle==2.1.0; extra == "eggroll"
+Requires-Dist: grpcio-tools; extra == "eggroll"
 Requires-Dist: psutil>=5.7.0; extra == "eggroll"
 Provides-Extra: all
 Requires-Dist: pyfate[eggroll,pulsar,rabbitmq,spark]; extra == "all"
 
 [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![CodeStyle](https://img.shields.io/badge/Check%20Style-Google-brightgreen)](https://checkstyle.sourceforge.io/google_style.html) [![Style](https://img.shields.io/badge/Check%20Style-Black-black)](https://checkstyle.sourceforge.io/google_style.html) [![Build Status](https://travis-ci.org/FederatedAI/FATE.svg?branch=master)](https://travis-ci.org/FederatedAI/FATE)
 [![codecov](https://codecov.io/gh/FederatedAI/FATE/branch/master/graph/badge.svg)](https://codecov.io/gh/FederatedAI/FATE)
 [![Documentation Status](https://readthedocs.org/projects/fate/badge/?version=latest)](https://fate.readthedocs.io/en/latest/?badge=latest)
@@ -52,55 +51,70 @@
 [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6308/badge)](https://bestpractices.coreinfrastructure.org/projects/6308)
 
 
 <div align="center">
   <img src="./doc/images/FATE_logo.png">
 </div>
 
-[DOCS](./doc) | [](./README_zh.md)
 
 FATE (Federated AI Technology Enabler) is the world's first industrial grade federated learning open source framework to enable enterprises and institutions to collaborate on data while protecting data security and privacy. 
 It implements secure computation protocols based on homomorphic encryption and multi-party computation (MPC). 
 Supporting various federated learning scenarios, FATE now provides a host of federated learning algorithms, including logistic regression, tree-based algorithms, deep learning and transfer learning.
 
 
 FATE is an open source project hosted by Linux Foundation. The [Technical Charter](https://github.com/FederatedAI/FATE-Community/blob/master/FATE_Project_Technical_Charter.pdf) sets forth the responsibilities and procedures for technical contribution to, and oversight of, the FATE (Federated AI Technology Enabler) Project. 
 
 <https://fate.readthedocs.io/en/latest>
 
+
 ## Getting Started
+FATE can be deployed on a single node or on multiple nodes. Choose the deployment approach which matches your environment.
+[Release version can be downloaded here.](https://github.com/FederatedAI/FATE/wiki/Download)
+
 
-### Version < 2.0
-Releases history can be found in [releases](https://github.com/FederatedAI/FATE/releases), deployment resources can be found on [wiki](https://github.com/FederatedAI/FATE/wiki/Download)
+### Version >= 2.0
+### Standalone deployment
 
-### Version == 2.0.0-beta
-#### Standalone deployment
 - Deploying FATE on a single node via PyPI, pre-built docker images or installers. It is for simple testing purposes. Refer to this [guide](./deploy/standalone-deploy/).
 
 ### Cluster deployment
 Deploying FATE to multiple nodes to achieve scalability, reliability and manageability.
-
 - [Cluster deployment by CLI](./deploy/cluster-deploy): Using CLI to deploy a FATE cluster.
 
 ### Quick Start
-- [Training Demo With Installing FATE AND FATE-Flow From Pypi](doc/2.0/quick_start.md)
 - [Training Demo With Installing FATE Only From Pypi](doc/2.0/fate/ml)
+- [Training Demo With Installing FATE AND FATE-Flow From Pypi](doc/2.0/fate/quick_start.md)
+
+### More examples
+- [ML examples](examples/launchers)
+- [PipeLine examples](examples/pipeline)
+
+## Documentation
+
+### FATE Design
+- [Architecture](./doc/architecture/README.md): Building Unified and Standardized API for Heterogeneous Computing Engines Interconnection
+- [FATE Algorithm Components](./doc/2.0/fate/components/README.md): Building Standardized Algorithm Components for different Scheduling Engines
+- [OSX (Open Site Exchange)](./doc/2.0/osx/osx.md): Building Open Platform for Cross-Site Communication Interconnection
+- [FATE-Flow](https://github.com/FederatedAI/FATE-Flow/blob/main/doc/fate_flow.md): Building Open and Standardized Scheduling Platform for Scheduling Interconnection 
+- [PipeLine Design](https://github.com/FederatedAI/FATE-Client/blob/main/doc/pipeline.md): Building Scalable Federated DSL for Application Layer Interconnection And Providing Tools For Fast Federated Modeling
+- [RoadMap](./doc/images/roadmap.png)
+- [Paper & Conference](./doc/resources/README.md)
 
 ## Related Repositories (Projects)
 - [KubeFATE](https://github.com/FederatedAI/KubeFATE): An operational tool for the FATE platform using cloud native technologies such as containers and Kubernetes.
 - [FATE-Flow](https://github.com/FederatedAI/FATE-Flow): A multi-party secure task scheduling platform for federated learning pipeline.
 - [FATE-Board](https://github.com/FederatedAI/FATE-Board): A suite of visualization tools to explore and understand federated models easily and effectively.
 - [FATE-Serving](https://github.com/FederatedAI/FATE-Serving): A high-performance and production-ready serving system for federated learning models.
 - [FATE-Cloud](https://github.com/FederatedAI/FATE-Cloud): An infrastructure for building and managing industrial-grade federated learning cloud services.
 - [EggRoll](https://github.com/WeBankFinTech/eggroll): A simple high-performance computing framework for (federated) machine learning.
 - [AnsibleFATE](https://github.com/FederatedAI/AnsibleFATE): A tool to optimize and automate the configuration and deployment operations via Ansible.
 - [FATE-Builder](https://github.com/FederatedAI/FATE-Builder): A tool to build package and docker image for FATE and KubeFATE.
 - [FATE-Client](https://github.com/FederatedAI/FATE-Client): A tool to enable fast federated modeling tasks for FATE.
 - [FATE-Test](https://github.com/FederatedAI/FATE-Test): An automated testing tool for FATE, including tests and benchmark comparisons.
-
+- [FATE-LLM](https://github.com/FederatedAI/FATE-LLM/blob/main/README.md) : A framework to support federated learning for large language models(LLMs).
 ## Governance 
 
 [FATE-Community](https://github.com/FederatedAI/FATE-Community) contains all the documents about how the community members coopearte with each other. 
 
 - [GOVERNANCE.md](https://github.com/FederatedAI/FATE-Community/blob/master/GOVERNANCE.md) documents the governance model of the project. 
 - [Minutes](https://github.com/FederatedAI/FATE-Community/blob/master/meeting-minutes) of working meetings
 - [Development Process Guidelines](https://github.com/FederatedAI/FATE-Community/blob/master/FederatedAI_PROJECT_PROCESS_GUIDELINE.md)
```

### Comparing `pyfate-2.0.0b0/fate/__init__.py` & `pyfate-2.1.0/fate/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/_info.py` & `pyfate-2.1.0/fate/_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,9 +8,9 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-__version__ = "2.0.0-beta"
+__version__ = "2.1.0"
 __provider__ = "fate"
```

### Comparing `pyfate-2.0.0b0/fate/arch/__init__.py` & `pyfate-2.1.0/fate/arch/tensor/inside/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -8,13 +8,9 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-#
-
-from .context import CipherKit, Context
-from .unify import URI, Backend, device
 
-__all__ = ["Backend", "device", "Context", "URI", "CipherKit"]
+from ._op_quantile import GKSummary
```

### Comparing `pyfate-2.0.0b0/fate/arch/_standalone.py` & `pyfate-2.1.0/fate/arch/computing/backends/standalone/_standalone.py`

 * *Files 23% similar despite different names*

```diff
@@ -15,56 +15,34 @@
 #
 
 import hashlib
 import itertools
 import logging
 import logging.config
 import os
-import pickle as c_pickle
 import shutil
 import signal
 import threading
 import time
 import uuid
-from collections.abc import Iterable
 from concurrent.futures import ProcessPoolExecutor as Executor
 from contextlib import ExitStack
 from functools import partial
 from heapq import heapify, heappop, heapreplace
 from operator import is_not
 from pathlib import Path
-from typing import List, Tuple
+from typing import Callable, Any, Iterable, Optional
+from typing import List, Tuple, Literal
 
 import cloudpickle as f_pickle
 import lmdb
-import numpy as np
-from fate.arch.abc import PartyMeta
 
-from .federation import FederationDataType
+PartyMeta = Tuple[Literal["guest", "host", "arbiter", "local"], str]
 
-LOGGER = logging.getLogger(__name__)
-
-serialize = c_pickle.dumps
-deserialize = c_pickle.loads
-
-# default message max size in bytes = 1MB
-DEFAULT_MESSAGE_MAX_SIZE = 1048576
-
-if (STANDALONE_DATA_PATH := os.getenv("STANDALONE_DATA_PATH")) is not None:
-    _data_dir = Path(STANDALONE_DATA_PATH)
-    LOGGER.debug(f"env STANDALONE_DATA_PATH is set to {STANDALONE_DATA_PATH}, using {_data_dir} as data dir")
-else:
-    _data_dir = Path(
-        os.path.abspath(
-            os.path.join(
-                os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, os.pardir, "__standalone_data__"
-            )
-        )
-    )
-    LOGGER.debug(f"env STANDALONE_DATA_PATH is not set, using {_data_dir} as data dir")
+logger = logging.getLogger(__name__)
 
 
 def _watch_thread_react_to_parent_die(ppid, logger_config):
     """
     this function is call when a process is created, and it will watch parent process and initialize loggers
     Args:
         ppid: parent process id
@@ -87,68 +65,107 @@
 
     thread = threading.Thread(target=f, daemon=True)
     thread.start()
 
     # initialize loggers
     if logger_config is not None:
         logging.config.dictConfig(logger_config)
-    # else:
-    #     level = os.getenv("DEBUG_MODE_LOG_LEVEL", "DEBUG")
-    #     try:
-    #         import rich.logging
-    #
-    #         logging_class = "rich.logging.RichHandler"
-    #         logging_formatters = {}
-    #         handlers = {
-    #             "console": {
-    #                 "class": logging_class,
-    #                 "level": level,
-    #                 "filters": [],
-    #             }
-    #         }
-    #     except ImportError:
-    #         logging_class = "logging.StreamHandler"
-    #         logging_formatters = {
-    #             "console": {
-    #                 "format": "[%(levelname)s][%(asctime)-8s][%(process)s][%(module)s.%(funcName)s][line:%(lineno)d]: %(message)s"
-    #             }
-    #         }
-    #         handlers = {
-    #             "console": {
-    #                 "class": logging_class,
-    #                 "level": level,
-    #                 "formatter": "console",
-    #             }
-    #         }
-    #     logging.config.dictConfig(dict(
-    #         version=1,
-    #         formatters=logging_formatters,
-    #         handlers=handlers,
-    #         filters={},
-    #         loggers={},
-    #         root=dict(handlers=["console"], level="DEBUG"),
-    #         disable_existing_loggers=False,
-    #     ))
+
+
+class BasicProcessPool:
+    def __init__(self, pool, log_level):
+        self._pool = pool
+        self._exception_tb = {}
+        self.log_level = log_level
+
+    def submit(self, func, process_infos):
+        features = []
+        outputs = {}
+        num_partitions = len(process_infos)
+
+        for p, process_info in enumerate(process_infos):
+            features.append(
+                self._pool.submit(
+                    BasicProcessPool._process_wrapper,
+                    func,
+                    process_info,
+                    self.log_level,
+                )
+            )
+
+        from concurrent.futures import wait, FIRST_COMPLETED
+
+        not_done = features
+        while not_done:
+            done, not_done = wait(not_done, return_when=FIRST_COMPLETED)
+            for f in done:
+                partition_id, output, e = f.result()
+                if e is not None:
+                    logger.error(f"partition {partition_id} exec failed: {e}")
+                    raise RuntimeError(f"Partition {partition_id} exec failed: {e}")
+                else:
+                    outputs[partition_id] = output
+
+        outputs = [outputs[p] for p in range(num_partitions)]
+        return outputs
+
+    @classmethod
+    def _process_wrapper(cls, do_func, process_info, log_level):
+        try:
+            if log_level is not None:
+                pass
+            output = do_func(process_info)
+            return process_info.partition_id, output, None
+        except Exception as e:
+            logger.error(f"exception in rank {process_info.partition_id}: {e}")
+            return process_info.partition_id, None, e
+
+    def shutdown(self):
+        self._pool.shutdown()
 
 
 # noinspection PyPep8Naming
 class Table(object):
     def __init__(
         self,
         session: "Session",
+        data_dir: str,
         namespace: str,
         name: str,
         partitions,
+        key_serdes_type: int,
+        value_serdes_type: int,
+        partitioner_type: int,
         need_cleanup=True,
     ):
         self._need_cleanup = need_cleanup
+        self._data_dir = data_dir
         self._namespace = namespace
         self._name = name
         self._partitions = partitions
         self._session = session
+        self._key_serdes_type = key_serdes_type
+        self._value_serdes_type = value_serdes_type
+        self._partitioner_type = partitioner_type
+
+    @property
+    def num_partitions(self):
+        return self._partitions
+
+    @property
+    def key_serdes_type(self):
+        return self._key_serdes_type
+
+    @property
+    def value_serdes_type(self):
+        return self._value_serdes_type
+
+    @property
+    def partitioner_type(self):
+        return self._partitioner_type
 
     @property
     def partitions(self):
         return self._partitions
 
     @property
     def name(self):
@@ -168,631 +185,823 @@
     def __str__(self):
         return f"<Table {self._namespace}|{self._name}|{self._partitions}|{self._need_cleanup}>"
 
     def __repr__(self):
         return self.__str__()
 
     def destroy(self):
-        for p in range(self._partitions):
+        for p in range(self.num_partitions):
             with self._get_env_for_partition(p, write=True) as env:
                 db = env.open_db()
                 with env.begin(write=True) as txn:
                     txn.drop(db)
-        _TableMetaManager.destroy_table(self._namespace, self._name)
+        _TableMetaManager.destroy_table(data_dir=self._data_dir, namespace=self._namespace, name=self._name)
 
-    def take(self, n, **kwargs):
-        if n <= 0:
-            raise ValueError(f"{n} <= 0")
-        return list(itertools.islice(self.collect(**kwargs), n))
+    def take(self, num, **kwargs):
+        if num <= 0:
+            raise ValueError(f"{num} <= 0")
+        return list(itertools.islice(self.collect(**kwargs), num))
 
     def count(self):
         cnt = 0
-        for p in range(self._partitions):
+        for p in range(self.num_partitions):
             with self._get_env_for_partition(p) as env:
                 cnt += env.stat()["entries"]
         return cnt
 
     # noinspection PyUnusedLocal
     def collect(self, **kwargs):
         iterators = []
         with ExitStack() as s:
-            for p in range(self._partitions):
+            for p in range(self.num_partitions):
                 env = s.enter_context(self._get_env_for_partition(p))
                 txn = s.enter_context(env.begin())
                 iterators.append(s.enter_context(txn.cursor()))
 
             # Merge sorted
             entries = []
             for _id, it in enumerate(iterators):
                 if it.next():
                     key, value = it.item()
                     entries.append([key, value, _id, it])
             heapify(entries)
             while entries:
                 key, value, _, it = entry = entries[0]
-                yield deserialize(key), deserialize(value)
+                yield key, value
                 if it.next():
                     entry[0], entry[1] = it.item()
                     heapreplace(entries, entry)
                 else:
                     _, _, _, it = heappop(entries)
 
     def reduce(self, func):
-        # noinspection PyProtectedMember
-        rs = self._session._submit_unary(func, _do_reduce, self._partitions, self._name, self._namespace)
-        rs = [r for r in filter(partial(is_not, None), rs)]
-        if len(rs) <= 0:
-            return None
-        rtn = rs[0]
-        for r in rs[1:]:
-            rtn = func(rtn, r)
-        return rtn
-
-    def map(self, func):
-        return self._unary(func, _do_map)
-
-    def mapValues(self, func):
-        return self._unary(func, _do_map_values)
-
-    def flatMap(self, func):
-        _flat_mapped = self._unary(func, _do_flat_map)
-        return _flat_mapped.save_as(
-            name=str(uuid.uuid1()),
-            namespace=_flat_mapped.namespace,
-            partition=self._partitions,
-            need_cleanup=True,
-        )
-
-    def applyPartitions(self, func):
-        return self._unary(func, _do_apply_partitions)
-
-    def mapPartitions(self, func, preserves_partitioning=False):
-        un_shuffled = self._unary(func, _do_map_partitions)
-        if preserves_partitioning:
-            return un_shuffled
-        return un_shuffled.save_as(
-            name=str(uuid.uuid1()),
-            namespace=un_shuffled.namespace,
-            partition=self._partitions,
-            need_cleanup=True,
-        )
-
-    def mapPartitionsWithIndex(self, func, preserves_partitioning=False):
-        un_shuffled = self._unary(func, _do_map_partitions_with_index)
-        if preserves_partitioning:
-            return un_shuffled
-        return un_shuffled.save_as(
-            name=str(uuid.uuid1()),
-            namespace=un_shuffled.namespace,
-            partition=self._partitions,
-            need_cleanup=True,
-        )
-
-    def mapReducePartitions(self, mapper, reducer):
-        dup = _create_table(
-            self._session,
-            str(uuid.uuid1()),
-            self.namespace,
-            self._partitions,
-            need_cleanup=True,
+        return self._session.submit_reduce(
+            func,
+            data_dir=self._data_dir,
+            num_partitions=self.num_partitions,
+            name=self._name,
+            namespace=self._namespace,
         )
 
-        def _dict_reduce(a: dict, b: dict):
-            for k, v in b.items():
-                if k not in a:
-                    a[k] = v
-                else:
-                    a[k] = reducer(a[k], v)
-            return a
-
-        def _local_map_reduce(it):
-            ret = {}
-            for _k, _v in mapper(it):
-                if _k not in ret:
-                    ret[_k] = _v
-                else:
-                    ret[_k] = reducer(ret[_k], _v)
-            return ret
-
-        dup.put_all(self.applyPartitions(_local_map_reduce).reduce(_dict_reduce).items())
-        return dup
-
-    def glom(self):
-        return self._unary(None, _do_glom)
-
-    def sample(self, fraction, seed=None):
-        return self._unary((fraction, seed), _do_sample)
-
-    def filter(self, func):
-        return self._unary(func, _do_filter)
-
-    def join(self, other: "Table", func):
-        return self._binary(other, func, _do_join)
-
-    def subtractByKey(self, other: "Table"):
-        func = f"{self._namespace}.{self._name}-{other._namespace}.{other._name}"
-        return self._binary(other, func, _do_subtract_by_key)
-
-    def union(self, other: "Table", func=lambda v1, v2: v1):
-        return self._binary(other, func, _do_union)
-
-    # noinspection PyProtectedMember
-    def _map_reduce(self, mapper, reducer):
-        results = self._session._submit_map_reduce_in_partition(
-            mapper, reducer, self._partitions, self._name, self._namespace
-        )
-        result = results[0]
-        # noinspection PyProtectedMember
-        return _create_table(
-            session=self._session,
-            name=result.name,
-            namespace=result.namespace,
-            partitions=self._partitions,
+    def binary_sorted_map_partitions_with_index(
+        self,
+        other: "Table",
+        binary_map_partitions_with_index_op: Callable[[int, Iterable, Iterable], Iterable],
+        key_serdes_type,
+        partitioner_type,
+        output_value_serdes_type,
+        need_cleanup=True,
+        output_name=None,
+        output_namespace=None,
+        output_data_dir=None,
+    ):
+        if output_data_dir is None:
+            output_data_dir = self._data_dir
+        if output_name is None:
+            output_name = str(uuid.uuid1())
+        if output_namespace is None:
+            output_namespace = self._namespace
+
+        self._session._submit_sorted_binary_map_partitions_with_index(
+            func=binary_map_partitions_with_index_op,
+            do_func=_do_binary_sorted_map_with_index,
+            num_partitions=self.num_partitions,
+            first_input_data_dir=self._data_dir,
+            first_input_name=self._name,
+            first_input_namespace=self._namespace,
+            second_input_data_dir=other._data_dir,
+            second_input_name=other._name,
+            second_input_namespace=other._namespace,
+            output_data_dir=output_data_dir,
+            output_name=output_name,
+            output_namespace=output_namespace,
         )
-
-    def _unary(self, func, do_func):
-        # noinspection PyProtectedMember
-        results = self._session._submit_unary(func, do_func, self._partitions, self._name, self._namespace)
-        result = results[0]
-        # noinspection PyProtectedMember
         return _create_table(
             session=self._session,
-            name=result.name,
-            namespace=result.namespace,
-            partitions=self._partitions,
+            data_dir=self._data_dir,
+            name=output_name,
+            namespace=output_namespace,
+            partitions=self.num_partitions,
+            need_cleanup=need_cleanup,
+            key_serdes_type=key_serdes_type,
+            value_serdes_type=output_value_serdes_type,
+            partitioner_type=partitioner_type,
         )
 
-    def _binary(self, other: "Table", func, do_func):
-        session_id = self._session.session_id
-        left, right = self, other
-        if left._partitions != right._partitions:
-            if other.count() > self.count():
-                left = left.save_as(str(uuid.uuid1()), session_id, partition=right._partitions)
-            else:
-                right = other.save_as(str(uuid.uuid1()), session_id, partition=left._partitions)
+    def map_reduce_partitions_with_index(
+        self,
+        map_partition_op: Callable[[int, Iterable], Iterable],
+        reduce_partition_op: Optional[Callable[[Any, Any], Any]],
+        output_partitioner: Optional[Callable[[bytes, int], int]],
+        shuffle,
+        output_key_serdes_type,
+        output_value_serdes_type,
+        output_partitioner_type,
+        output_num_partitions,
+        need_cleanup=True,
+        output_name=None,
+        output_namespace=None,
+        output_data_dir=None,
+    ):
+        if output_data_dir is None:
+            output_data_dir = self._data_dir
+        if output_name is None:
+            output_name = str(uuid.uuid1())
+        if output_namespace is None:
+            output_namespace = self._namespace
+        if not shuffle:
+            assert output_num_partitions == self.num_partitions and output_partitioner_type == self.partitioner_type
+            # noinspection PyProtectedMember
+            self._session._submit_map_reduce_partitions_with_index(
+                _do_mrwi_no_shuffle,
+                mapper=map_partition_op,
+                reducer=reduce_partition_op,
+                input_num_partitions=self.num_partitions,
+                input_data_dir=self._data_dir,
+                input_name=self._name,
+                input_namespace=self._namespace,
+                output_num_partitions=output_num_partitions,
+                output_data_dir=output_data_dir,
+                output_name=output_name,
+                output_namespace=output_namespace,
+                output_partitioner=output_partitioner,
+            )
+            return _create_table(
+                session=self._session,
+                data_dir=output_data_dir,
+                name=output_name,
+                namespace=output_namespace,
+                partitions=output_num_partitions,
+                need_cleanup=need_cleanup,
+                key_serdes_type=output_key_serdes_type,
+                value_serdes_type=output_value_serdes_type,
+                partitioner_type=output_partitioner_type,
+            )
 
+        if reduce_partition_op is None:
+            _do_shuffle_write_func = _do_mrwi_map_and_shuffle_write_unique
+            _do_shuffle_read_func = _do_mrwi_shuffle_read_no_reduce
+        else:
+            _do_shuffle_write_func = _do_mrwi_map_and_shuffle_write
+            _do_shuffle_read_func = _do_mrwi_shuffle_read_and_reduce
+        # Step 1: do map and write intermediate results to cache table
+        intermediate_name = str(uuid.uuid1())
+        intermediate_namespace = self._namespace
+        intermediate_data_dir = self._data_dir
         # noinspection PyProtectedMember
-        results = self._session._submit_binary(
-            func,
-            do_func,
-            left._partitions,
-            left._name,
-            left._namespace,
-            right._name,
-            right._namespace,
+        self._session._submit_map_reduce_partitions_with_index(
+            _do_shuffle_write_func,
+            mapper=map_partition_op,
+            reducer=None,
+            input_data_dir=self._data_dir,
+            input_num_partitions=self.num_partitions,
+            input_name=self._name,
+            input_namespace=self._namespace,
+            output_data_dir=intermediate_data_dir,
+            output_num_partitions=output_num_partitions,
+            output_name=intermediate_name,
+            output_namespace=intermediate_namespace,
+            output_partitioner=output_partitioner,
         )
-        result: _Operand = results[0]
+        # Step 2: do shuffle read and reduce
         # noinspection PyProtectedMember
-        return _create_table(
+        self._session._submit_map_reduce_partitions_with_index(
+            _do_shuffle_read_func,
+            mapper=None,
+            reducer=reduce_partition_op,
+            input_data_dir=intermediate_data_dir,
+            input_num_partitions=self.num_partitions,
+            input_name=intermediate_name,
+            input_namespace=intermediate_namespace,
+            output_data_dir=output_data_dir,
+            output_num_partitions=output_num_partitions,
+            output_name=output_name,
+            output_namespace=output_namespace,
+        )
+        output = _create_table(
             session=self._session,
-            name=result.name,
-            namespace=result.namespace,
-            partitions=left._partitions,
+            data_dir=output_data_dir,
+            name=output_name,
+            namespace=output_namespace,
+            partitions=output_num_partitions,
+            need_cleanup=need_cleanup,
+            key_serdes_type=output_key_serdes_type,
+            value_serdes_type=output_value_serdes_type,
+            partitioner_type=output_partitioner_type,
         )
 
-    def save_as(self, name, namespace, partition=None, need_cleanup=True):
-        if partition is None:
-            partition = self._partitions
-        # noinspection PyProtectedMember
-        dup = _create_table(self._session, name, namespace, partition, need_cleanup)
-        dup.put_all(self.collect())
-        return dup
+        # drop cache table
+        for p in range(self._partitions):
+            with _get_env_with_data_dir(
+                intermediate_data_dir, intermediate_namespace, intermediate_name, str(p), write=True
+            ) as env:
+                db = env.open_db()
+                with env.begin(write=True) as txn:
+                    txn.drop(db)
+
+        path = Path(self._data_dir).joinpath(intermediate_namespace, intermediate_name)
+        shutil.rmtree(path, ignore_errors=True)
+        return output
+
+    def copy_as(self, name, namespace, need_cleanup=True):
+        return self.map_reduce_partitions_with_index(
+            map_partition_op=lambda i, x: x,
+            reduce_partition_op=None,
+            output_partitioner=None,
+            shuffle=False,
+            need_cleanup=need_cleanup,
+            output_name=name,
+            output_namespace=namespace,
+            output_key_serdes_type=self._key_serdes_type,
+            output_value_serdes_type=self._value_serdes_type,
+            output_partitioner_type=self._partitioner_type,
+            output_num_partitions=self.num_partitions,
+        )
 
     def _get_env_for_partition(self, p: int, write=False):
-        return _get_env(self._namespace, self._name, str(p), write=write)
+        return _get_env_with_data_dir(self._data_dir, self._namespace, self._name, str(p), write=write)
 
-    def put(self, k, v):
-        k_bytes, v_bytes = _kv_to_bytes(k=k, v=v)
-        p = _hash_key_to_partition(k_bytes, self._partitions)
+    def put(self, k_bytes: bytes, v_bytes: bytes, partitioner: Callable[[bytes, int], int] = None):
+        p = partitioner(k_bytes, self._partitions)
         with self._get_env_for_partition(p, write=True) as env:
             with env.begin(write=True) as txn:
                 return txn.put(k_bytes, v_bytes)
 
-    def put_all(self, kv_list: Iterable):
+    def put_all(self, kv_list: Iterable[Tuple[bytes, bytes]], partitioner: Callable[[bytes, int], int]):
         txn_map = {}
-        is_success = True
         with ExitStack() as s:
             for p in range(self._partitions):
                 env = s.enter_context(self._get_env_for_partition(p, write=True))
                 txn_map[p] = env, env.begin(write=True)
-            for k, v in kv_list:
-                try:
-                    k_bytes, v_bytes = _kv_to_bytes(k=k, v=v)
-                    p = _hash_key_to_partition(k_bytes, self._partitions)
-                    is_success = is_success and txn_map[p][1].put(k_bytes, v_bytes)
-                except Exception as e:
-                    is_success = False
-                    LOGGER.exception(f"put_all for k={k} v={v} fail. exception: {e}")
-                    break
-            for p, (env, txn) in txn_map.items():
-                txn.commit() if is_success else txn.abort()
-
-    def get(self, k):
-        k_bytes = _k_to_bytes(k=k)
-        p = _hash_key_to_partition(k_bytes, self._partitions)
+            try:
+                for k_bytes, v_bytes in kv_list:
+                    p = partitioner(k_bytes, self._partitions)
+                    if not txn_map[p][1].put(k_bytes, v_bytes):
+                        break
+            except Exception as e:
+                for p, (env, txn) in txn_map.items():
+                    txn.abort()
+                raise e
+            else:
+                for p, (env, txn) in txn_map.items():
+                    txn.commit()
+
+    def get(self, k_bytes: bytes, partitioner: Callable[[bytes, int], int]) -> bytes:
+        p = partitioner(k_bytes, self._partitions)
         with self._get_env_for_partition(p) as env:
             with env.begin(write=True) as txn:
-                old_value_bytes = txn.get(k_bytes)
-                return None if old_value_bytes is None else deserialize(old_value_bytes)
+                return txn.get(k_bytes)
 
-    def delete(self, k):
-        k_bytes = _k_to_bytes(k=k)
-        p = _hash_key_to_partition(k_bytes, self._partitions)
+    def delete(self, k_bytes: bytes, partitioner: Callable[[bytes, int], int]):
+        p = partitioner(k_bytes, self._partitions)
         with self._get_env_for_partition(p, write=True) as env:
             with env.begin(write=True) as txn:
                 old_value_bytes = txn.get(k_bytes)
                 if txn.delete(k_bytes):
-                    return None if old_value_bytes is None else deserialize(old_value_bytes)
+                    return old_value_bytes
                 return None
 
 
 # noinspection PyMethodMayBeStatic
 class Session(object):
-    def __init__(self, session_id, max_workers=None, logger_config=None):
+    def __init__(
+        self,
+        session_id,
+        data_dir: str,
+        max_workers=None,
+        logger_config=None,
+        executor_pool_cls=BasicProcessPool,
+    ):
         self.session_id = session_id
-        self._pool = Executor(
-            max_workers=max_workers,
-            initializer=_watch_thread_react_to_parent_die,
-            initargs=(
-                os.getpid(),
-                logger_config,
+        self._data_dir = data_dir
+        self._max_workers = max_workers
+        if self._max_workers is None:
+            self._max_workers = os.cpu_count()
+
+        self._enable_process_logger = True
+        if self._enable_process_logger:
+            log_level = logging.getLevelName(logger.getEffectiveLevel())
+        else:
+            log_level = None
+        self._pool = executor_pool_cls(
+            pool=Executor(
+                max_workers=max_workers,
+                initializer=_watch_thread_react_to_parent_die,
+                initargs=(
+                    os.getpid(),
+                    logger_config,
+                ),
             ),
+            log_level=log_level,
         )
 
+    @property
+    def data_dir(self):
+        return self._data_dir
+
+    @property
+    def max_workers(self):
+        return self._max_workers
+
     def __getstate__(self):
         # session won't be pickled
         pass
 
     def load(self, name, namespace):
-        return _load_table(session=self, name=name, namespace=namespace)
+        return _load_table(session=self, data_dir=self._data_dir, name=name, namespace=namespace)
 
-    def create_table(self, name, namespace, partitions, need_cleanup, error_if_exist):
+    def create_table(
+        self,
+        name,
+        namespace,
+        partitions,
+        need_cleanup,
+        error_if_exist,
+        key_serdes_type,
+        value_serdes_type,
+        partitioner_type,
+    ):
         return _create_table(
             session=self,
+            data_dir=self._data_dir,
             name=name,
             namespace=namespace,
             partitions=partitions,
             need_cleanup=need_cleanup,
             error_if_exist=error_if_exist,
+            key_serdes_type=key_serdes_type,
+            value_serdes_type=value_serdes_type,
+            partitioner_type=partitioner_type,
         )
 
     # noinspection PyUnusedLocal
-    def parallelize(self, data: Iterable, partition: int, include_key: bool = False, **kwargs):
-        if not include_key:
-            data = enumerate(data)
+    def parallelize(
+        self,
+        data: Iterable,
+        partition: int,
+        partitioner: Callable[[bytes, int], int],
+        key_serdes_type,
+        value_serdes_type,
+        partitioner_type,
+    ):
         table = _create_table(
             session=self,
+            data_dir=self._data_dir,
             name=str(uuid.uuid1()),
             namespace=self.session_id,
             partitions=partition,
+            need_cleanup=True,
+            key_serdes_type=key_serdes_type,
+            value_serdes_type=value_serdes_type,
+            partitioner_type=partitioner_type,
         )
-        table.put_all(data)
+        table.put_all(data, partitioner=partitioner)
         return table
 
     def cleanup(self, name, namespace):
-        if not _data_dir.is_dir():
-            LOGGER.error(f"illegal data dir: {_data_dir}")
+        path = Path(self._data_dir)
+        if not path.is_dir():
             return
-
-        namespace_dir = _data_dir.joinpath(namespace)
-
+        namespace_dir = path.joinpath(namespace)
         if not namespace_dir.is_dir():
             return
-
         if name == "*":
             shutil.rmtree(namespace_dir, True)
             return
-
         for table in namespace_dir.glob(name):
             shutil.rmtree(table, True)
 
     def stop(self):
         self.cleanup(name="*", namespace=self.session_id)
         self._pool.shutdown()
 
     def kill(self):
         self.cleanup(name="*", namespace=self.session_id)
         self._pool.shutdown()
 
-    def _submit_unary(self, func, _do_func, partitions, name, namespace):
-        task_info = _TaskInfo(
-            self.session_id,
-            function_id=str(uuid.uuid1()),
-            function_bytes=f_pickle.dumps(func),
-        )
-        futures = []
-        for p in range(partitions):
-            futures.append(
-                self._pool.submit(_do_func, _UnaryProcess(task_info, _Operand(namespace, name, p, partitions)))
-            )
-        results = [r.result() for r in futures]
-        return results
+    def submit_reduce(self, func, data_dir: str, num_partitions: int, name: str, namespace: str):
+        rs = self._pool.submit(
+            _do_reduce,
+            [
+                _ReduceProcess(p, _TaskInputInfo(data_dir, namespace, name, num_partitions), _ReduceFunctorInfo(func))
+                for p in range(num_partitions)
+            ],
+        )
+        rs = [r for r in filter(partial(is_not, None), rs)]
+        if len(rs) <= 0:
+            return None
+        rtn = rs[0]
+        for r in rs[1:]:
+            rtn = func(rtn, r)
+        return rtn
 
-    def _submit_map_reduce_in_partition(self, mapper, reducer, partitions, name, namespace):
-        task_info = _MapReduceTaskInfo(
-            self.session_id,
-            function_id=str(uuid.uuid1()),
-            map_function_bytes=f_pickle.dumps(mapper),
-            reduce_function_bytes=f_pickle.dumps(reducer),
-        )
-        futures = []
-        for p in range(partitions):
-            futures.append(
-                self._pool.submit(
-                    _do_map_reduce_in_partitions,
-                    _MapReduceProcess(task_info, _Operand(namespace, name, p, partitions)),
+    def _submit_map_reduce_partitions_with_index(
+        self,
+        _do_func,
+        mapper,
+        reducer,
+        input_data_dir: str,
+        input_num_partitions,
+        input_name,
+        input_namespace,
+        output_data_dir: str,
+        output_num_partitions,
+        output_name,
+        output_namespace,
+        output_partitioner=None,
+    ):
+        input_info = _TaskInputInfo(input_data_dir, input_namespace, input_name, input_num_partitions)
+        output_info = _TaskOutputInfo(
+            output_data_dir, output_namespace, output_name, output_num_partitions, partitioner=output_partitioner
+        )
+        return self._submit_process(
+            _do_func,
+            [
+                _MapReduceProcess(
+                    partition_id=p,
+                    input_info=input_info,
+                    output_info=output_info,
+                    operator_info=_MapReduceFunctorInfo(mapper=mapper, reducer=reducer),
                 )
-            )
-        results = [r.result() for r in futures]
-        return results
-
-    def _submit_binary(self, func, do_func, partitions, name, namespace, other_name, other_namespace):
-        task_info = _TaskInfo(
-            self.session_id,
-            function_id=str(uuid.uuid1()),
-            function_bytes=f_pickle.dumps(func),
-        )
-        futures = []
-        for p in range(partitions):
-            left = _Operand(namespace, name, p, partitions)
-            right = _Operand(other_namespace, other_name, p, partitions)
-            futures.append(self._pool.submit(do_func, _BinaryProcess(task_info, left, right)))
-        results = []
-        for f in futures:
-            r = f.result()
-            results.append(r)
-        return results
+                for p in range(max(input_num_partitions, output_num_partitions))
+            ],
+        )
 
+    def _submit_sorted_binary_map_partitions_with_index(
+        self,
+        func,
+        do_func,
+        num_partitions: int,
+        first_input_data_dir: str,
+        first_input_name: str,
+        first_input_namespace: str,
+        second_input_data_dir: str,
+        second_input_name: str,
+        second_input_namespace: str,
+        output_data_dir: str,
+        output_name: str,
+        output_namespace: str,
+    ):
+        first_input_info = _TaskInputInfo(
+            first_input_data_dir, first_input_namespace, first_input_name, num_partitions
+        )
+        second_input_info = _TaskInputInfo(
+            second_input_data_dir, second_input_namespace, second_input_name, num_partitions
+        )
+        output_info = _TaskOutputInfo(output_data_dir, output_namespace, output_name, num_partitions, partitioner=None)
+        return self._submit_process(
+            do_func,
+            [
+                _BinarySortedMapProcess(
+                    partition_id=p,
+                    first_input_info=first_input_info,
+                    second_input_info=second_input_info,
+                    output_info=output_info,
+                    operator_info=_BinarySortedMapFunctorInfo(func),
+                )
+                for p in range(num_partitions)
+            ],
+        )
 
-def _get_splits(obj, max_message_size):
-    obj_bytes = serialize(obj, protocol=4)
-    byte_size = len(obj_bytes)
-    num_slice = (byte_size - 1) // max_message_size + 1
-    if num_slice <= 1:
-        return obj, num_slice
-    else:
-        _max_size = max_message_size
-        kv = [(i, obj_bytes[slice(i * _max_size, (i + 1) * _max_size)]) for i in range(num_slice)]
-        return kv, num_slice
+    def _submit_process(self, do_func, process_infos):
+        return self._pool.submit(do_func, process_infos)
 
 
 class Federation(object):
-    def _federation_object_key(self, name: str, tag: str, s_party: Tuple[str, str], d_party: Tuple[str, str]):
-        return f"{self._session_id}-{name}-{tag}-{s_party[0]}-{s_party[1]}-{d_party[0]}-{d_party[1]}"
+    def _federation_object_key(self, name: str, tag: str, s_party: Tuple[str, str], d_party: Tuple[str, str]) -> bytes:
+        return f"{self._session_id}-{name}-{tag}-{s_party[0]}-{s_party[1]}-{d_party[0]}-{d_party[1]}".encode("utf-8")
 
-    def __init__(self, session: Session, session_id: str, party: Tuple[str, str]):
+    def __init__(self, session: Session, data_dir: str, session_id: str, party: Tuple[str, str]):
+        self._session = session
+        self._data_dir = data_dir
         self._session_id = session_id
         self._party = party
-        self._session = session
-        self._max_message_size = DEFAULT_MESSAGE_MAX_SIZE
         self._other_status_tables = {}
         self._other_object_tables = {}
         self._federation_status_table_cache = None
         self._federation_object_table_cache = None
 
-        self._meta = _FederationMetaManager(session_id, party)
+        self._meta = _FederationMetaManager(session_id=session_id, data_dir=data_dir, party=party)
+
+    @classmethod
+    def create(cls, session: Session, session_id: str, party: Tuple[str, str]):
+        federation = cls(session, session._data_dir, session_id, party)
+        return federation
 
     def destroy(self):
         self._session.cleanup(namespace=self._session_id, name="*")
 
-    # noinspection PyUnusedLocal
-    def remote(self, v, name: str, tag: str, parties: List[PartyMeta]):
-        log_str = f"federation.standalone.remote.{name}.{tag}"
+    def push_table(self, table, name: str, tag: str, parties: List[PartyMeta]):
+        for party in parties:
+            _tagged_key = self._federation_object_key(name, tag, self._party, party)
+            saved_name = str(uuid.uuid1())
+            _table = table.copy_as(name=saved_name, namespace=table.namespace, need_cleanup=False)
+            self._meta.set_status(party, _tagged_key, _serialize_tuple_of_str(_table.name, _table.namespace))
 
-        if v is None:
-            raise ValueError(f"[{log_str}]remote `None` to {parties}")
+    def push_bytes(self, v: bytes, name: str, tag: str, parties: List[PartyMeta]):
+        for party in parties:
+            _tagged_key = self._federation_object_key(name, tag, self._party, party)
+            self._meta.set_object(party, _tagged_key, v)
+            self._meta.set_status(party, _tagged_key, _tagged_key)
 
-        LOGGER.debug(f"[{log_str}]remote data, type={type(v)}")
+    def pull_table(self, name: str, tag: str, parties: List[PartyMeta]) -> List[Table]:
+        results: List[bytes] = []
+        for party in parties:
+            _tagged_key = self._federation_object_key(name, tag, party, self._party)
 
-        if isinstance(v, Table):
-            dtype = FederationDataType.TABLE
-            LOGGER.debug(
-                f"[{log_str}]remote "
-                f"Table(namespace={v.namespace}, name={v.name}, partitions={v.partitions}), dtype={dtype}"
-            )
-        else:
-            v_splits, num_slice = _get_splits(v, self._max_message_size)
-            if num_slice > 1:
-                v = _create_table(
-                    session=self._session,
-                    name=str(uuid.uuid1()),
-                    namespace=self._session_id,
-                    partitions=1,
-                    need_cleanup=True,
-                    error_if_exist=False,
-                )
-                v.put_all(kv_list=v_splits)
-                dtype = FederationDataType.SPLIT_OBJECT
-                LOGGER.debug(
-                    f"[{log_str}]remote "
-                    f"Table(namespace={v.namespace}, name={v.name}, partitions={v.partitions}), dtype={dtype}"
-                )
-            else:
-                LOGGER.debug(f"[{log_str}]remote object with type: {type(v)}")
-                dtype = FederationDataType.OBJECT
+            results.append(self._meta.wait_status_set(_tagged_key))
 
-        for party in parties:
-            _tagged_key = self._federation_object_key(name, tag, self._party, party)
-            if isinstance(v, Table):
-                saved_name = str(uuid.uuid1())
-                LOGGER.debug(
-                    f"[{log_str}]save Table(namespace={v.namespace}, name={v.name}, partitions={v.partitions}) as "
-                    f"Table(namespace={v.namespace}, name={saved_name}, partitions={v.partitions})"
-                )
-                _v = v.save_as(name=saved_name, namespace=v.namespace, need_cleanup=False)
-                self._meta.set_status(party, _tagged_key, (_v.name, _v.namespace, dtype))
-            else:
-                self._meta.set_object(party, _tagged_key, v)
-                self._meta.set_status(party, _tagged_key, _tagged_key)
+        rtn = []
+        for r in results:
+            name, namespace = _deserialize_tuple_of_str(self._meta.get_status(r))
+            table: Table = _load_table(
+                session=self._session, data_dir=self._data_dir, name=name, namespace=namespace, need_cleanup=True
+            )
+            rtn.append(table)
+            self._meta.ack_status(r)
+        return rtn
 
-    # noinspection PyProtectedMember
-    def get(self, name: str, tag: str, parties: List[PartyMeta]) -> List:
-        log_str = f"federation.standalone.get.{name}.{tag}"
-        LOGGER.debug(f"[{log_str}]")
+    def pull_bytes(self, name: str, tag: str, parties: List[PartyMeta]) -> List[bytes]:
         results = []
-
         for party in parties:
             _tagged_key = self._federation_object_key(name, tag, party, self._party)
             results.append(self._meta.wait_status_set(_tagged_key))
 
         rtn = []
         for r in results:
-            if isinstance(r, tuple):
-                # noinspection PyTypeChecker
-                table: Table = _load_table(session=self._session, name=r[0], namespace=r[1], need_cleanup=True)
-
-                dtype = r[2]
-                LOGGER.debug(
-                    f"[{log_str}] got "
-                    f"Table(namespace={table.namespace}, name={table.name}, partitions={table.partitions}), dtype={dtype}"
-                )
-
-                if dtype == FederationDataType.SPLIT_OBJECT:
-                    obj_bytes = b"".join(map(lambda t: t[1], sorted(table.collect(), key=lambda x: x[0])))
-                    obj = deserialize(obj_bytes)
-                    rtn.append(obj)
-                else:
-                    rtn.append(table)
-            else:
-                obj = self._meta.get_object(r)
-                if obj is None:
-                    raise EnvironmentError(f"federation get None from {parties} with name {name}, tag {tag}")
-                rtn.append(obj)
-                self._meta.ack_object(r)
-                LOGGER.debug(f"[{log_str}] got object with type: {type(obj)}")
+            obj = self._meta.get_object(r)
+            if obj is None:
+                raise EnvironmentError(f"object not found: {r}")
+            rtn.append(obj)
+            self._meta.ack_object(r)
             self._meta.ack_status(r)
         return rtn
 
 
 def _create_table(
     session: "Session",
+    data_dir: str,
     name: str,
     namespace: str,
     partitions: int,
+    key_serdes_type: int,
+    value_serdes_type: int,
+    partitioner_type: int,
     need_cleanup=True,
     error_if_exist=False,
 ):
     assert isinstance(name, str)
     assert isinstance(namespace, str)
     assert isinstance(partitions, int)
-    if (exist_partitions := _TableMetaManager.get_table_meta(namespace, name)) is None:
-        _TableMetaManager.add_table_meta(namespace, name, partitions)
+    if (exist_partitions := _TableMetaManager.get_table_meta(data_dir, namespace, name)) is None:
+        _TableMetaManager.add_table_meta(
+            data_dir, namespace, name, partitions, key_serdes_type, value_serdes_type, partitioner_type
+        )
     else:
         if error_if_exist:
             raise RuntimeError(f"table already exist: name={name}, namespace={namespace}")
         partitions = exist_partitions
 
     return Table(
         session=session,
+        data_dir=data_dir,
         namespace=namespace,
         name=name,
         partitions=partitions,
+        key_serdes_type=key_serdes_type,
+        value_serdes_type=value_serdes_type,
+        partitioner_type=partitioner_type,
         need_cleanup=need_cleanup,
     )
 
 
-def _load_table(session, name: str, namespace: str, need_cleanup=False):
-    partitions = _TableMetaManager.get_table_meta(namespace, name)
-    if partitions is None:
+def _load_table(session, data_dir: str, name: str, namespace: str, need_cleanup=False):
+    table_meta = _TableMetaManager.get_table_meta(data_dir, namespace, name)
+    if table_meta is None:
         raise RuntimeError(f"table not exist: name={name}, namespace={namespace}")
     return Table(
         session=session,
+        data_dir=data_dir,
         namespace=namespace,
         name=name,
-        partitions=partitions,
         need_cleanup=need_cleanup,
+        partitions=table_meta.num_partitions,
+        key_serdes_type=table_meta.key_serdes_type,
+        value_serdes_type=table_meta.value_serdes_type,
+        partitioner_type=table_meta.partitioner_type,
     )
 
 
-class _TaskInfo:
-    def __init__(self, task_id, function_id, function_bytes):
-        self.task_id = task_id
-        self.function_id = function_id
-        self.function_bytes = function_bytes
-        self._function_deserialized = None
+class _TaskInputInfo:
+    def __init__(self, data_dir: str, namespace: str, name: str, num_partitions: int):
+        self.data_dir = data_dir
+        self.namespace = namespace
+        self.name = name
+        self.num_partitions = num_partitions
+
+    def get_env(self, pid, write=False):
+        return _get_env_with_data_dir(self.data_dir, self.namespace, self.name, str(pid), write=write)
 
-    def get_func(self):
-        if self._function_deserialized is None:
-            self._function_deserialized = f_pickle.loads(self.function_bytes)
-        return self._function_deserialized
 
+class _TaskOutputInfo:
+    def __init__(self, data_dir: str, namespace: str, name: str, num_partitions: int, partitioner):
+        self.data_dir = data_dir
+        self.namespace = namespace
+        self.name = name
+        self.num_partitions = num_partitions
+        self.partitioner = partitioner
+
+    def get_env(self, pid, write=True):
+        return _get_env_with_data_dir(self.data_dir, self.namespace, self.name, str(pid), write=write)
+
+    def get_partition_id(self, key):
+        if self.partitioner is None:
+            raise RuntimeError("partitioner is None")
+        return self.partitioner(key, self.num_partitions)
 
-class _MapReduceTaskInfo:
-    def __init__(self, task_id, function_id, map_function_bytes, reduce_function_bytes):
-        self.task_id = task_id
-        self.function_id = function_id
-        self.map_function_bytes = map_function_bytes
-        self.reduce_function_bytes = reduce_function_bytes
-        self._reduce_function_deserialized = None
-        self._mapper_function_deserialized = None
+
+class _MapReduceFunctorInfo:
+    def __init__(self, mapper, reducer):
+        if mapper is not None:
+            self.mapper_bytes = f_pickle.dumps(mapper)
+        else:
+            self.mapper_bytes = None
+        if reducer is not None:
+            self.reducer_bytes = f_pickle.dumps(reducer)
+        else:
+            self.reducer_bytes = None
 
     def get_mapper(self):
-        if self._mapper_function_deserialized is None:
-            self._mapper_function_deserialized = f_pickle.loads(self.map_function_bytes)
-        return self._mapper_function_deserialized
+        if self.mapper_bytes is None:
+            raise RuntimeError("mapper is None")
+        return f_pickle.loads(self.mapper_bytes)
 
     def get_reducer(self):
-        if self._reduce_function_deserialized is None:
-            self._reduce_function_deserialized = f_pickle.loads(self.reduce_function_bytes)
-        return self._reduce_function_deserialized
+        if self.reducer_bytes is None:
+            raise RuntimeError("reducer is None")
+        return f_pickle.loads(self.reducer_bytes)
 
 
-class _Operand:
-    def __init__(self, namespace, name, partition, num_partitions: int):
-        self.namespace = namespace
-        self.name = name
-        self.partition = partition
-        self.num_partitions = num_partitions
+class _BinarySortedMapFunctorInfo:
+    def __init__(self, mapper):
+        if mapper is not None:
+            self.mapper_bytes = f_pickle.dumps(mapper)
+        else:
+            self.mapper_bytes = None
 
-    def as_env(self, write=False):
-        return _get_env(self.namespace, self.name, str(self.partition), write=write)
+    def get_mapper(self):
+        if self.mapper_bytes is None:
+            raise RuntimeError("mapper is None")
+        return f_pickle.loads(self.mapper_bytes)
 
 
-class _UnaryProcess:
-    def __init__(self, task_info: _TaskInfo, operand: _Operand):
-        self.info = task_info
-        self.operand = operand
+class _ReduceFunctorInfo:
+    def __init__(self, reducer):
+        if reducer is not None:
+            self.reducer_bytes = f_pickle.dumps(reducer)
+        else:
+            self.reducer_bytes = None
 
-    def output_operand(self):
-        return _Operand(self.info.task_id, self.info.function_id, self.operand.partition, self.operand.num_partitions)
+    def get_reducer(self):
+        if self.reducer_bytes is None:
+            raise RuntimeError("reducer is None")
+        return f_pickle.loads(self.reducer_bytes)
 
-    def get_func(self):
-        return self.info.get_func()
+
+class _ReduceProcess:
+    def __init__(
+        self,
+        partition_id: int,
+        input_info: _TaskInputInfo,
+        operator_info: _ReduceFunctorInfo,
+    ):
+        self.partition_id = partition_id
+        self.input_info = input_info
+        self.operator_info = operator_info
+
+    def as_input_env(self, pid, write=False):
+        return self.input_info.get_env(pid, write=write)
+
+    def input_cursor(self, stack: ExitStack):
+        return stack.enter_context(stack.enter_context(self.as_input_env(self.partition_id).begin()).cursor())
+
+    def get_reducer(self):
+        return self.operator_info.get_reducer()
 
 
 class _MapReduceProcess:
-    def __init__(self, task_info: _MapReduceTaskInfo, operand: _Operand):
-        self.info = task_info
-        self.operand = operand
+    def __init__(
+        self,
+        partition_id: int,
+        input_info: _TaskInputInfo,
+        output_info: _TaskOutputInfo,
+        operator_info: _MapReduceFunctorInfo,
+    ):
+        self.partition_id = partition_id
+        self.input_info = input_info
+        self.output_info = output_info
+        self.operator_info = operator_info
+
+    def get_input_partition_num(self):
+        return self.input_info.num_partitions
+
+    def get_output_partition_num(self):
+        return self.output_info.num_partitions
+
+    def get_input_env(self, pid, write=False):
+        return self.input_info.get_env(pid, write=write)
+
+    def get_output_env(self, pid, write=True):
+        return self.output_info.get_env(pid, write=write)
+
+    def get_input_cursor(self, stack: ExitStack, pid=None):
+        if pid is None:
+            pid = self.partition_id
+        if isinstance(pid, int) and pid >= self.input_info.num_partitions:
+            raise RuntimeError(f"pid {pid} >= input_info.num_partitions {self.input_info.num_partitions}")
+        return stack.enter_context(
+            stack.enter_context(stack.enter_context(self.get_input_env(pid, write=False)).begin(write=False)).cursor()
+        )
+
+    def has_partition(self, pid):
+        return pid < self.input_info.num_partitions
 
-    def output_operand(self):
-        return _Operand(self.info.task_id, self.info.function_id, self.operand.partition, self.operand.num_partitions)
+    def get_output_transaction(self, pid, stack: ExitStack):
+        return stack.enter_context(stack.enter_context(self.get_output_env(pid, write=True)).begin(write=True))
+
+    def get_output_partition_id(self, key: bytes):
+        return self.output_info.get_partition_id(key)
 
     def get_mapper(self):
-        return self.info.get_mapper()
+        return self.operator_info.get_mapper()
 
     def get_reducer(self):
-        return self.info.get_reducer()
+        return self.operator_info.get_reducer()
 
 
-class _BinaryProcess:
-    def __init__(self, task_info: _TaskInfo, left: _Operand, right: _Operand):
-        self.info = task_info
-        self.left = left
-        self.right = right
+class _BinarySortedMapProcess:
+    def __init__(
+        self,
+        partition_id,
+        first_input_info: _TaskInputInfo,
+        second_input_info: _TaskInputInfo,
+        output_info: _TaskOutputInfo,
+        operator_info: _BinarySortedMapFunctorInfo,
+    ):
+        self.partition_id = partition_id
+        self.first_input = first_input_info
+        self.second_input = second_input_info
+        self.output_info = output_info
+        self.operator_info = operator_info
+
+    def get_input_partition_num(self):
+        return self.first_input.num_partitions
+
+    def get_output_partition_num(self):
+        return self.output_info.num_partitions
+
+    def get_first_input_env(self, pid, write=False):
+        return self.first_input.get_env(pid, write=write)
+
+    def get_second_input_env(self, pid, write=False):
+        return self.second_input.get_env(pid, write=write)
+
+    def get_output_env(self, pid, write=True):
+        return self.output_info.get_env(pid, write=write)
+
+    def get_first_input_cursor(self, stack: ExitStack, pid=None):
+        if pid is None:
+            pid = self.partition_id
+        return stack.enter_context(
+            stack.enter_context(
+                stack.enter_context(self.get_first_input_env(pid, write=False)).begin(write=False)
+            ).cursor()
+        )
 
-    def output_operand(self):
-        return _Operand(self.info.task_id, self.info.function_id, self.left.partition, self.left.num_partitions)
+    def get_second_input_cursor(self, stack: ExitStack, pid=None):
+        if pid is None:
+            pid = self.partition_id
+        return stack.enter_context(
+            stack.enter_context(
+                stack.enter_context(self.get_second_input_env(pid, write=False)).begin(write=False)
+            ).cursor()
+        )
+
+    def get_output_transaction(self, pid, stack: ExitStack):
+        return stack.enter_context(stack.enter_context(self.get_output_env(pid, write=True)).begin(write=True))
+
+    def get_output_partition_id(self, key: bytes):
+        return self.output_info.get_partition_id(key)
 
     def get_func(self):
-        return self.info.get_func()
+        return self.operator_info.get_mapper()
 
 
-def _get_env(*args, write=False):
-    _path = _data_dir.joinpath(*args)
+def _get_env_with_data_dir(data_dir: str, *args, write=False):
+    _path = Path(data_dir).joinpath(*args)
     return _open_env(_path, write=write)
 
 
 def _open_env(path, write=False):
     path.mkdir(parents=True, exist_ok=True)
 
     t = 0
@@ -806,437 +1015,292 @@
                 lock=write,
                 sync=True,
                 map_size=10_737_418_240,
             )
             return env
         except lmdb.Error as e:
             if "No such file or directory" in e.args[0]:
-                time.sleep(0.01)
+                time.sleep(0.001)
                 t += 1
             else:
                 raise e
     raise lmdb.Error(f"No such file or directory: {path}, with {t} times retry")
 
 
-def _hash_key_to_partition(key, partitions):
-    _key = hashlib.sha1(key).digest()
-    if isinstance(_key, bytes):
-        _key = int.from_bytes(_key, byteorder="little", signed=False)
-    if partitions < 1:
-        raise ValueError("partitions must be a positive number")
-    b, j = -1, 0
-    while j < partitions:
-        b = int(j)
-        _key = ((_key * 2862933555777941757) + 1) & 0xFFFFFFFFFFFFFFFF
-        j = float(b + 1) * (float(1 << 31) / float((_key >> 33) + 1))
-    return int(b)
-
-
-def _do_map(p: _UnaryProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        txn_map = {}
-        for partition in range(p.operand.num_partitions):
-            env = s.enter_context(_get_env(rtn.namespace, rtn.name, str(partition), write=True))
-            txn_map[partition] = s.enter_context(env.begin(write=True))
-        source_txn = s.enter_context(source_env.begin())
-        cursor = s.enter_context(source_txn.cursor())
-        for k_bytes, v_bytes in cursor:
-            k, v = deserialize(k_bytes), deserialize(v_bytes)
-            k1, v1 = p.get_func()(k, v)
-            k1_bytes, v1_bytes = serialize(k1), serialize(v1)
-            partition = _hash_key_to_partition(k1_bytes, p.operand.num_partitions)
-            txn_map[partition].put(k1_bytes, v1_bytes)
-    return rtn
-
-
 def _generator_from_cursor(cursor):
     for k, v in cursor:
-        yield deserialize(k), deserialize(v)
+        yield k, v
 
 
-def _do_apply_partitions(p: _UnaryProcess):
+def _do_mrwi_no_shuffle(p: _MapReduceProcess):
+    rtn = p.output_info
     with ExitStack() as s:
-        rtn = p.output_operand()
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(source_txn.cursor())
-        v = p.get_func()(_generator_from_cursor(cursor))
-        if cursor.last():
-            k_bytes = cursor.key()
-            dst_txn.put(k_bytes, serialize(v))
+        dst_txn = p.get_output_transaction(p.partition_id, s)
+        cursor = p.get_input_cursor(s)
+        v = p.get_mapper()(p.partition_id, _generator_from_cursor(cursor))
+        for k1, v1 in v:
+            dst_txn.put(k1, v1)
         return rtn
 
 
-def _do_map_partitions(p: _UnaryProcess):
+def _do_binary_sorted_map_with_index(p: _BinarySortedMapProcess):
+    rtn = p.output_info
     with ExitStack() as s:
-        rtn = p.output_operand()
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(source_txn.cursor())
-        v = p.get_func()(_generator_from_cursor(cursor))
-
-        if isinstance(v, Iterable):
-            for k1, v1 in v:
-                dst_txn.put(serialize(k1), serialize(v1))
-        else:
-            k_bytes = cursor.key()
-            dst_txn.put(k_bytes, serialize(v))
-        return rtn
-
-
-def _do_map_partitions_with_index(p: _UnaryProcess):
-    with ExitStack() as s:
-        rtn = p.output_operand()
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(source_txn.cursor())
-        v = p.get_func()(p.operand.partition, _generator_from_cursor(cursor))
-
-        if isinstance(v, Iterable):
-            for k1, v1 in v:
-                dst_txn.put(serialize(k1), serialize(v1))
-        else:
-            k_bytes = cursor.key()
-            dst_txn.put(k_bytes, serialize(v))
+        first_cursor = p.get_first_input_cursor(s)
+        second_cursor = p.get_second_input_cursor(s)
+        dst_txn = p.get_output_transaction(p.partition_id, s)
+        output_kv_iter = p.get_func()(
+            p.partition_id, _generator_from_cursor(first_cursor), _generator_from_cursor(second_cursor)
+        )
+        for k_bytes, v_bytes in output_kv_iter:
+            dst_txn.put(k_bytes, v_bytes)
         return rtn
 
 
-def _do_map_reduce_in_partitions(p: _MapReduceProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        txn_map = {}
-        for partition in range(p.operand.num_partitions):
-            env = s.enter_context(_get_env(rtn.namespace, rtn.name, str(partition), write=True))
-            txn_map[partition] = s.enter_context(env.begin(write=True))
-        source_txn = s.enter_context(source_env.begin())
-        cursor = s.enter_context(source_txn.cursor())
-        mapped = p.get_mapper()(_generator_from_cursor(cursor))
-        if not isinstance(mapped, Iterable):
-            raise ValueError("mapper function should return a iterable of pair")
-        reducer = p.get_reducer()
-
-        for k, v in mapped:
-            k_bytes = serialize(k)
-            partition = _hash_key_to_partition(k_bytes, p.operand.num_partitions)
-            # todo: not atomic, fix me
-            pre_v = txn_map[partition].get(k_bytes, None)
-            if pre_v is None:
-                txn_map[partition].put(k_bytes, serialize(v))
-            else:
-                txn_map[partition].put(k_bytes, serialize(reducer(deserialize(pre_v), v)))
-    return rtn
+def _serialize_shuffle_write_key(iteration_index: int, k_bytes: bytes) -> bytes:
+    iteration_bytes = iteration_index.to_bytes(4, "big")  # 4 bytes for the iteration index
+    serialized_key = iteration_bytes + k_bytes
 
+    return serialized_key
 
-def _do_map_values(p: _UnaryProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
 
-        cursor = s.enter_context(source_txn.cursor())
-        for k_bytes, v_bytes in cursor:
-            v = deserialize(v_bytes)
-            v1 = p.get_func()(v)
-            dst_txn.put(k_bytes, serialize(v1))
-    return rtn
+def _deserialize_shuffle_write_key(serialized_key: bytes) -> (int, int, bytes):
+    iteration_bytes = serialized_key[:4]
+    k_bytes = serialized_key[4:]
+    iteration_index = int.from_bytes(iteration_bytes, "big")
+    return iteration_index, k_bytes
 
 
-def _do_flat_map(p: _UnaryProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
+def _get_shuffle_partition_id(shuffle_source_partition_id: int, shuffle_destination_partition_id: int) -> str:
+    return f"{shuffle_source_partition_id}_{shuffle_destination_partition_id}"
 
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
 
-        cursor = s.enter_context(source_txn.cursor())
-        for k_bytes, v_bytes in cursor:
-            k = deserialize(k_bytes)
-            v = deserialize(v_bytes)
-            map_result = p.get_func()(k, v)
-            for result_k, result_v in map_result:
-                dst_txn.put(serialize(result_k), serialize(result_v))
+def _do_mrwi_map_and_shuffle_write(p: _MapReduceProcess):
+    rtn = p.output_info
+    if p.has_partition(p.partition_id):
+        with ExitStack() as s:
+            cursor = p.get_input_cursor(s)
+            shuffle_write_txn_map = {}
+            for output_partition_id in range(p.get_output_partition_num()):
+                shuffle_partition_id = _get_shuffle_partition_id(p.partition_id, output_partition_id)
+                shuffle_write_txn_map[output_partition_id] = p.get_output_transaction(shuffle_partition_id, s)
+
+            output_kv_iter = p.get_mapper()(p.partition_id, _generator_from_cursor(cursor))
+            for index, (k_bytes, v_bytes) in enumerate(output_kv_iter):
+                shuffle_write_txn_map[p.get_output_partition_id(k_bytes)].put(
+                    _serialize_shuffle_write_key(index, k_bytes), v_bytes, overwrite=False
+                )
     return rtn
 
 
-def _do_reduce(p: _UnaryProcess):
-    value = None
-    with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        source_txn = s.enter_context(source_env.begin())
-        cursor = s.enter_context(source_txn.cursor())
-        for _, v_bytes in cursor:
-            v = deserialize(v_bytes)
-            if value is None:
-                value = v
-            else:
-                value = p.get_func()(value, v)
-    return value
-
-
-def _do_glom(p: _UnaryProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dest_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(source_txn.cursor())
-        v_list = []
-        k_bytes = None
-        for k, v in cursor:
-            v_list.append((deserialize(k), deserialize(v)))
-            k_bytes = k
-        if k_bytes is not None:
-            dest_txn.put(k_bytes, serialize(v_list))
+def _do_mrwi_map_and_shuffle_write_unique(p: _MapReduceProcess):
+    rtn = p.output_info
+    if p.has_partition(p.partition_id):
+        with ExitStack() as s:
+            cursor = p.get_input_cursor(s)
+            shuffle_write_txn_map = {}
+            for output_partition_id in range(p.get_output_partition_num()):
+                shuffle_partition_id = _get_shuffle_partition_id(p.partition_id, output_partition_id)
+                shuffle_write_txn_map[output_partition_id] = p.get_output_transaction(shuffle_partition_id, s)
+
+            output_kv_iter = p.get_mapper()(p.partition_id, _generator_from_cursor(cursor))
+            for k_bytes, v_bytes in output_kv_iter:
+                shuffle_write_txn_map[p.get_output_partition_id(k_bytes)].put(k_bytes, v_bytes, overwrite=False)
     return rtn
 
 
-def _do_sample(p: _UnaryProcess):
-    rtn = p.output_operand()
-    fraction, seed = deserialize(p.info.function_bytes)
+def _do_mrwi_shuffle_read_and_reduce(p: _MapReduceProcess):
+    rtn = p.output_info
+    reducer = p.get_reducer()
     with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(source_txn.cursor())
-        cursor.first()
-        random_state = np.random.RandomState(seed)
-        for k, v in cursor:
-            # noinspection PyArgumentList
-            if random_state.rand() < fraction:
-                dst_txn.put(k, v)
+        dst_txn = p.get_output_transaction(p.partition_id, s)
+        for input_partition_id in range(p.get_input_partition_num()):
+            for k_bytes, v_bytes in p.get_input_cursor(
+                s, pid=_get_shuffle_partition_id(input_partition_id, p.partition_id)
+            ):
+                _, key = _deserialize_shuffle_write_key(k_bytes)
+                if (old := dst_txn.get(key)) is None:
+                    dst_txn.put(key, v_bytes)
+                else:
+                    dst_txn.put(key, reducer(old, v_bytes))
     return rtn
 
 
-def _do_filter(p: _UnaryProcess):
-    rtn = p.output_operand()
+def _do_mrwi_shuffle_read_no_reduce(p: _MapReduceProcess):
+    rtn = p.output_info
     with ExitStack() as s:
-        source_env = s.enter_context(p.operand.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        source_txn = s.enter_context(source_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(source_txn.cursor())
-        for k_bytes, v_bytes in cursor:
-            k = deserialize(k_bytes)
-            v = deserialize(v_bytes)
-            if p.get_func()(k, v):
+        dst_txn = p.get_output_transaction(p.partition_id, s)
+        for input_partition_id in range(p.get_input_partition_num()):
+            for k_bytes, v_bytes in p.get_input_cursor(
+                s, pid=_get_shuffle_partition_id(input_partition_id, p.partition_id)
+            ):
                 dst_txn.put(k_bytes, v_bytes)
     return rtn
 
 
-def _do_subtract_by_key(p: _BinaryProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        left_op = p.left
-        right_op = p.right
-        right_env = s.enter_context(right_op.as_env())
-        left_env = s.enter_context(left_op.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        left_txn = s.enter_context(left_env.begin())
-        right_txn = s.enter_context(right_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(left_txn.cursor())
-        for k_bytes, left_v_bytes in cursor:
-            right_v_bytes = right_txn.get(k_bytes)
-            if right_v_bytes is None:
-                dst_txn.put(k_bytes, left_v_bytes)
-    return rtn
-
-
-def _do_join(p: _BinaryProcess):
-    rtn = p.output_operand()
-    with ExitStack() as s:
-        right_env = s.enter_context(p.right.as_env())
-        left_env = s.enter_context(p.left.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        left_txn = s.enter_context(left_env.begin())
-        right_txn = s.enter_context(right_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        cursor = s.enter_context(left_txn.cursor())
-        for k_bytes, v1_bytes in cursor:
-            v2_bytes = right_txn.get(k_bytes)
-            if v2_bytes is None:
-                continue
-            v1 = deserialize(v1_bytes)
-            v2 = deserialize(v2_bytes)
-            try:
-                v3 = p.get_func()(v1, v2)
-            except Exception as e:
-                raise RuntimeError(
-                    f"Error when joining:\n" f"left:\n" f"{v1}\n" f"right:\n" f"{v2}\n" f"error: {e}"
-                ) from e
-            dst_txn.put(k_bytes, serialize(v3))
-    return rtn
-
-
-def _do_union(p: _BinaryProcess):
-    rtn = p.output_operand()
+def _do_reduce(p: _ReduceProcess):
+    value = None
     with ExitStack() as s:
-        left_env = s.enter_context(p.left.as_env())
-        right_env = s.enter_context(p.right.as_env())
-        dst_env = s.enter_context(rtn.as_env(write=True))
-
-        left_txn = s.enter_context(left_env.begin())
-        right_txn = s.enter_context(right_env.begin())
-        dst_txn = s.enter_context(dst_env.begin(write=True))
-
-        # process left op
-        with left_txn.cursor() as left_cursor:
-            for k_bytes, left_v_bytes in left_cursor:
-                right_v_bytes = right_txn.get(k_bytes)
-                if right_v_bytes is None:
-                    dst_txn.put(k_bytes, left_v_bytes)
-                else:
-                    left_v = deserialize(left_v_bytes)
-                    right_v = deserialize(right_v_bytes)
-                    final_v = p.get_func()(left_v, right_v)
-                    dst_txn.put(k_bytes, serialize(final_v))
-
-        # process right op
-        with right_txn.cursor() as right_cursor:
-            for k_bytes, right_v_bytes in right_cursor:
-                final_v_bytes = dst_txn.get(k_bytes)
-                if final_v_bytes is None:
-                    dst_txn.put(k_bytes, right_v_bytes)
-    return rtn
-
-
-def _kv_to_bytes(k, v):
-    return serialize(k), serialize(v)
-
-
-def _k_to_bytes(k):
-    return serialize(k)
+        cursor = p.input_cursor(s)
+        for _, v_bytes in cursor:
+            if value is None:
+                value = v_bytes
+            else:
+                value = p.get_reducer()(value, v_bytes)
+    return value
 
 
 class _FederationMetaManager:
     STATUS_TABLE_NAME_PREFIX = "__federation_status__"
     OBJECT_TABLE_NAME_PREFIX = "__federation_object__"
 
-    def __init__(self, session_id, party: Tuple[str, str]) -> None:
+    def __init__(self, data_dir: str, session_id, party: Tuple[str, str]) -> None:
         self.session_id = session_id
         self.party = party
+        self._data_dir = data_dir
         self._env = {}
 
-    def wait_status_set(self, key):
+    def wait_status_set(self, key: bytes) -> bytes:
         value = self.get_status(key)
         while value is None:
-            time.sleep(0.1)
+            time.sleep(0.001)
             value = self.get_status(key)
-        LOGGER.debug("[GET] Got {} type {}".format(key, "Table" if isinstance(value, tuple) else "Object"))
-        return value
+        return key
 
-    def get_status(self, key):
+    def get_status(self, key: bytes):
         return self._get(self._get_status_table_name(self.party), key)
 
-    def set_status(self, party: Tuple[str, str], key: str, value):
+    def set_status(self, party: Tuple[str, str], key: bytes, value: bytes):
         return self._set(self._get_status_table_name(party), key, value)
 
-    def ack_status(self, key):
+    def ack_status(self, key: bytes):
         return self._ack(self._get_status_table_name(self.party), key)
 
-    def get_object(self, key):
+    def get_object(self, key: bytes):
         return self._get(self._get_object_table_name(self.party), key)
 
-    def set_object(self, party: Tuple[str, str], key, value):
+    def set_object(self, party: Tuple[str, str], key: bytes, value: bytes):
         return self._set(self._get_object_table_name(party), key, value)
 
-    def ack_object(self, key):
+    def ack_object(self, key: bytes):
         return self._ack(self._get_object_table_name(self.party), key)
 
     def _get_status_table_name(self, party: Tuple[str, str]):
         return f"{self.STATUS_TABLE_NAME_PREFIX}.{party[0]}_{party[1]}"
 
     def _get_object_table_name(self, party: Tuple[str, str]):
         return f"{self.OBJECT_TABLE_NAME_PREFIX}.{party[0]}_{party[1]}"
 
     def _get_env(self, name):
         if name not in self._env:
-            self._env[name] = _get_env(self.session_id, name, str(0), write=True)
+            self._env[name] = _get_env_with_data_dir(self._data_dir, self.session_id, name, str(0), write=True)
         return self._env[name]
 
-    def _get(self, name, key):
+    def _get(self, name: str, key: bytes) -> bytes:
         env = self._get_env(name)
         with env.begin(write=False) as txn:
-            old_value_bytes = txn.get(serialize(key))
-            if old_value_bytes is not None:
-                old_value_bytes = deserialize(old_value_bytes)
-            return old_value_bytes
+            return txn.get(key)
 
-    def _set(self, name, key, value):
+    def _set(self, name, key: bytes, value: bytes):
         env = self._get_env(name)
         with env.begin(write=True) as txn:
-            return txn.put(serialize(key), serialize(value))
+            return txn.put(key, value)
 
-    def _ack(self, name, key):
+    def _ack(self, name, key: bytes):
         env = self._get_env(name)
         with env.begin(write=True) as txn:
-            txn.delete(serialize(key))
+            txn.delete(key)
+
+
+def _hash_namespace_name_to_partition(namespace: str, name: str, partitions: int) -> Tuple[bytes, int]:
+    k_bytes = f"{name}.{namespace}".encode("utf-8")
+    partition_id = int.from_bytes(hashlib.sha256(k_bytes).digest(), "big") % partitions
+    return k_bytes, partition_id
 
 
 class _TableMetaManager:
     namespace = "__META__"
     name = "fragments"
-    num_partitions = 10
+    num_partitions = 11
     _env = {}
 
     @classmethod
-    def _get_meta_env(cls, namespace: str, name: str):
-        k_bytes = _k_to_bytes(f"{namespace}.{name}")
-        p = _hash_key_to_partition(k_bytes, cls.num_partitions)
+    def _get_or_create_meta_env(cls, data_dir: str, p):
         if p not in cls._env:
-            cls._env[p] = _get_env(cls.namespace, cls.name, str(p), write=True)
-        return k_bytes, cls._env[p]
+            cls._env[p] = _get_env_with_data_dir(data_dir, cls.namespace, cls.name, str(p), write=True)
+        return cls._env[p]
+
+    @classmethod
+    def _get_meta_env(cls, data_dir: str, namespace: str, name: str):
+        k_bytes, p = _hash_namespace_name_to_partition(namespace, name, cls.num_partitions)
+        env = cls._get_or_create_meta_env(data_dir, p)
+        return k_bytes, env
 
     @classmethod
-    def add_table_meta(cls, namespace: str, name: str, num_partitions: int):
-        k_bytes, env = cls._get_meta_env(namespace, name)
+    def add_table_meta(
+        cls,
+        data_dir: str,
+        namespace: str,
+        name: str,
+        num_partitions: int,
+        key_serdes_type: int,
+        value_serdes_type: int,
+        partitioner_type: int,
+    ):
+        k_bytes, env = cls._get_meta_env(data_dir, namespace, name)
+        meta = _TableMeta(num_partitions, key_serdes_type, value_serdes_type, partitioner_type)
         with env.begin(write=True) as txn:
-            return txn.put(k_bytes, serialize(num_partitions))
+            return txn.put(k_bytes, meta.serialize())
 
     @classmethod
-    def get_table_meta(cls, namespace: str, name: str):
-        k_bytes, env = cls._get_meta_env(namespace, name)
+    def get_table_meta(cls, data_dir: str, namespace: str, name: str) -> "_TableMeta":
+        k_bytes, env = cls._get_meta_env(data_dir, namespace, name)
         with env.begin(write=False) as txn:
             old_value_bytes = txn.get(k_bytes)
             if old_value_bytes is not None:
-                old_value_bytes = deserialize(old_value_bytes)
+                old_value_bytes = _TableMeta.deserialize(old_value_bytes)
             return old_value_bytes
 
     @classmethod
-    def destroy_table(cls, namespace: str, name: str):
-        k_bytes, env = cls._get_meta_env(namespace, name)
+    def destroy_table(cls, data_dir: str, namespace: str, name: str):
+        k_bytes, env = cls._get_meta_env(data_dir, namespace, name)
         with env.begin(write=True) as txn:
             txn.delete(k_bytes)
-        path = _data_dir.joinpath(namespace, name)
+        path = Path(data_dir).joinpath(namespace, name)
         shutil.rmtree(path, ignore_errors=True)
+
+
+class _TableMeta:
+    def __init__(self, num_partitions: int, key_serdes_type: int, value_serdes_type: int, partitioner_type: int):
+        self.num_partitions = num_partitions
+        self.key_serdes_type = key_serdes_type
+        self.value_serdes_type = value_serdes_type
+        self.partitioner_type = partitioner_type
+
+    def serialize(self) -> bytes:
+        num_partitions_bytes = self.num_partitions.to_bytes(4, "big")
+        key_serdes_type_bytes = self.key_serdes_type.to_bytes(4, "big")
+        value_serdes_type_bytes = self.value_serdes_type.to_bytes(4, "big")
+        partitioner_type_bytes = self.partitioner_type.to_bytes(4, "big")
+        return num_partitions_bytes + key_serdes_type_bytes + value_serdes_type_bytes + partitioner_type_bytes
+
+    @classmethod
+    def deserialize(cls, serialized_bytes: bytes) -> "_TableMeta":
+        num_partitions = int.from_bytes(serialized_bytes[:4], "big")
+        key_serdes_type = int.from_bytes(serialized_bytes[4:8], "big")
+        value_serdes_type = int.from_bytes(serialized_bytes[8:12], "big")
+        partitioner_type = int.from_bytes(serialized_bytes[12:16], "big")
+        return cls(num_partitions, key_serdes_type, value_serdes_type, partitioner_type)
+
+
+def _serialize_tuple_of_str(name: str, namespace: str):
+    name_bytes = name.encode("utf-8")
+    namespace_bytes = namespace.encode("utf-8")
+    split_index_bytes = len(name_bytes).to_bytes(4, "big")
+    return split_index_bytes + name_bytes + namespace_bytes
+
+
+def _deserialize_tuple_of_str(serialized_bytes: bytes):
+    split_index = int.from_bytes(serialized_bytes[:4], "big")
+    name = serialized_bytes[4 : 4 + split_index].decode("utf-8")
+    namespace = serialized_bytes[4 + split_index :].decode("utf-8")
+    return name, namespace
```

### Comparing `pyfate-2.0.0b0/fate/arch/abc/_federation.py` & `pyfate-2.1.0/fate/arch/tensor/distributed/_op_stack.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,41 +8,40 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from typing import List, Optional, Protocol
 
-from ._party import PartyMeta
+import typing
 
+import torch
+import logging
 
-class GarbageCollector(Protocol):
-    def register_clean_action(self, name: str, tag: str, obj, method: str, kwargs):
-        ...
+from ._tensor import DTensor, implements
 
-    def clean(self, name: str, tag: str):
-        ...
+logger = logging.getLogger(__name__)
 
 
-class FederationEngine(Protocol):
-    session_id: str
-    get_gc: Optional[GarbageCollector]
-    remote_gc: Optional[GarbageCollector]
-    local_party: PartyMeta
-    parties: List[PartyMeta]
-
-    def pull(self, name: str, tag: str, parties: List[PartyMeta]) -> List:
-        ...
-
-    def push(
-        self,
-        v,
-        name: str,
-        tag: str,
-        parties: List[PartyMeta],
-    ):
-        ...
-
-    def destroy(self):
-        ...
+@implements(torch.stack)
+def stack(
+    tensors: typing.Union[typing.Tuple[DTensor, ...], typing.List[DTensor]],
+    dim: int = 0,
+    *,
+    out: DTensor = None,
+):
+    logger.warning("stack DTensors may be slow")
+    raise ValueError("stack DTensors may be slow")
+    if out is not None:
+        raise NotImplementedError("stack does not support out")
+
+    out = tensors[0]
+    for tensor in tensors[1:]:
+        # TODO: check shapes
+        out = DTensor(
+            out.shardings.join_shard(
+                tensor.shardings,
+                func=lambda x, y: torch.stack([x, y], dim=dim),
+            )
+        )
+    return out
```

### Comparing `pyfate-2.0.0b0/fate/arch/abc/_party.py` & `pyfate-2.1.0/fate/arch/dataframe/manager/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,10 +8,11 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from typing import Literal, Tuple
 
-PartyMeta = Tuple[Literal["guest", "host", "arbiter", "local"], str]
+from .schema_manager import Schema
+from .data_manager import DataManager
+from .block_manager import BlockType, Block
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/__init__.py` & `pyfate-2.1.0/fate/components/core/params/_he_param.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 #
-#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#  Copyright 2023 The FATE Authors. All Rights Reserved.
 #
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+#
 
+import pydantic
 
-from ._profile import enable_profile_remote, profile_ends, profile_start
-from ._type import ComputingEngine
-
+from ._fields import string_choice
 
-def is_table(v):
-    from fate.arch.abc import CTableABC
 
-    return isinstance(v, CTableABC)
+class HEParam(pydantic.BaseModel):
+    kind: string_choice(["paillier", "ou", "mock"])
+    key_length: int = 1024
 
 
-__all__ = ["is_table", "ComputingEngine", "profile_start", "profile_ends"]
+def he_param():
+    namespace = {}
+    return type("HEParam", (HEParam,), namespace)
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/_profile.py` & `pyfate-2.1.0/fate/arch/trace/_profile.py`

 * *Files 3% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 import hashlib
 import inspect
 import logging
 import time
 import typing
 from functools import wraps
 
-import beautifultable
 
 profile_logger = logging.getLogger(__name__)
 _PROFILE_LOG_ENABLED = False
 _START_TIME = None
 _END_TIME = None
 
 
@@ -90,14 +89,16 @@
         elapse = time.time() - self._start
         self._STATS[self._hash].item.add(elapse)
         if _PROFILE_LOG_ENABLED:
             profile_logger.debug(f"[computing#{self._hash}]done, elapse: {elapse}, function: {function_string}")
 
     @classmethod
     def computing_statistics_table(cls, timer_aggregator: _TimerItem = None):
+        import beautifultable
+
         stack_table = beautifultable.BeautifulTable(110, precision=4, detect_numerics=False)
         stack_table.columns.header = [
             "function",
             "n",
             "sum(s)",
             "mean(s)",
             "max(s)",
@@ -150,14 +151,16 @@
 
 class _FederationTimer(object):
     _GET_STATS: typing.MutableMapping[str, _TimerItem] = {}
     _REMOTE_STATS: typing.MutableMapping[str, _TimerItem] = {}
 
     @classmethod
     def federation_statistics_table(cls, timer_aggregator: _TimerItem = None):
+        import beautifultable
+
         total = _TimerItem()
         get_table = beautifultable.BeautifulTable(110)
         get_table.columns.header = ["name", "n", "sum(s)", "mean(s)", "max(s)"]
         for name, item in cls._GET_STATS.items():
             get_table.rows.append([name, *item.as_list()])
             total.union(item)
         get_table.rows.sort("sum(s)", reverse=True)
@@ -195,30 +198,21 @@
         self._parties = parties
         self._start_time = time.time()
         self._end_time = None
 
         if self._full_name not in self._REMOTE_STATS:
             self._REMOTE_STATS[self._full_name] = _TimerItem()
 
-    def done(self, federation):
+    def done(self):
         self._end_time = time.time()
         self._REMOTE_STATS[self._full_name].add(self.elapse)
         profile_logger.debug(
             f"[federation.remote.{self._full_name}.{self._tag}]" f"{self._local_party}->{self._parties} done"
         )
 
-        if is_profile_remote_enable():
-            federation.remote(
-                v={"start_time": self._start_time, "end_time": self._end_time},
-                name=self._name,
-                tag=profile_remote_tag(self._tag),
-                parties=self._parties,
-                gc=None,
-            )
-
     @property
     def elapse(self):
         return self._end_time - self._start_time
 
 
 class _FederationGetTimer(_FederationTimer):
     def __init__(self, name, full_name, tag, local, parties):
@@ -229,33 +223,21 @@
         self._parties = parties
         self._start_time = time.time()
         self._end_time = None
 
         if self._full_name not in self._GET_STATS:
             self._GET_STATS[self._full_name] = _TimerItem()
 
-    def done(self, federation):
+    def done(self):
         self._end_time = time.time()
         self._GET_STATS[self._full_name].add(self.elapse)
         profile_logger.debug(
             f"[federation.get.{self._full_name}.{self._tag}]" f"{self._local_party}<-{self._parties} done"
         )
 
-        if is_profile_remote_enable():
-            remote_meta = federation.get(
-                name=self._name,
-                tag=profile_remote_tag(self._tag),
-                parties=self._parties,
-                gc=None,
-            )
-            for party, meta in zip(self._parties, remote_meta):
-                profile_logger.debug(
-                    f"[federation.meta.{self._full_name}.{self._tag}]{self._local_party}<-{party}]" f"meta={meta}"
-                )
-
     @property
     def elapse(self):
         return self._end_time - self._start_time
 
 
 def federation_remote_timer(name, full_name, tag, local, parties):
     profile_logger.debug(f"[federation.remote.{full_name}.{tag}]{local}->{parties} start")
@@ -309,18 +291,18 @@
     profile_logger.debug(f"\nDetailed Computing:\n{computing_detailed_table}\n")
 
     global _PROFILE_LOG_ENABLED
     _PROFILE_LOG_ENABLED = False
 
 
 def _pretty_table_str(v):
-    from ..computing import is_table
+    from fate.arch.computing.api import is_table
 
     if is_table(v):
-        return f"Table(partition={v.partitions})"
+        return f"Table(partition={v.num_partitions})"
     else:
         return f"{type(v).__name__}"
 
 
 def _func_annotated_string(func, *args, **kwargs):
     pretty_args = []
     for k, v in inspect.signature(func).bind(*args, **kwargs).arguments.items():
@@ -332,15 +314,18 @@
     call_stack_strings = []
     frames = inspect.getouterframes(inspect.currentframe(), 10)[2:-2]
     for frame in frames:
         call_stack_strings.append(f"[{frame.filename.split('/')[-1]}:{frame.lineno}]{frame.function}")
     return call_stack_strings
 
 
-def computing_profile(func):
+T = typing.TypeVar("T", bound=typing.Callable)
+
+
+def computing_profile(func: T) -> T:
     @wraps(func)
     def _fn(*args, **kwargs):
         function_call_stack = _call_stack_strings()
         timer = _ComputingTimer(func.__name__, function_call_stack)
         rtn = func(*args, **kwargs)
         function_string = f"{_func_annotated_string(func, *args, **kwargs)} -> {_pretty_table_str(rtn)}"
         timer.done(function_string)
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/_type.py` & `pyfate-2.1.0/fate/arch/computing/api/_type.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/computing/eggroll/__init__.py` & `pyfate-2.1.0/fate/arch/computing/backends/eggroll/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/computing/eggroll/_csession.py` & `pyfate-2.1.0/fate/arch/computing/backends/eggroll/_csession.py`

 * *Files 17% similar despite different names*

```diff
@@ -12,102 +12,132 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
 
 import logging
 
-from fate.arch.abc import CSessionABC
-
-from ...unify import URI, uuid
-from .._profile import computing_profile
+from fate.arch.computing.api import KVTableContext
+from fate.arch.unify import URI, uuid
 from ._table import Table
 
 try:
-    from eggroll.core.session import session_init
-    from eggroll.roll_pair.roll_pair import runtime_init
+    from eggroll.session import session_init
+    from eggroll.computing import runtime_init
 except ImportError:
     raise EnvironmentError("eggroll not found in pythonpath")
 
-LOGGER = logging.getLogger(__name__)
+logger = logging.getLogger(__name__)
 
 
-class CSession(CSessionABC):
-    def __init__(self, session_id, options: dict = None):
+class CSession(KVTableContext):
+    def __init__(
+        self,
+        session_id,
+        host: str = None,
+        port: int = None,
+        options: dict = None,
+        config=None,
+        config_options=None,
+        config_properties_file=None,
+    ):
         if options is None:
             options = {}
-        if "eggroll.session.deploy.mode" not in options:
-            options["eggroll.session.deploy.mode"] = "cluster"
-        if "eggroll.rollpair.inmemory_output" not in options:
-            options["eggroll.rollpair.inmemory_output"] = True
-        self._rp_session = session_init(session_id=session_id, options=options)
-        self._rpc = runtime_init(session=self._rp_session)
-        self._session_id = self._rp_session.get_session_id()
+        self._eggroll_session = session_init(
+            session_id=session_id,
+            host=host,
+            port=port,
+            options=options,
+            config=config,
+            config_options=config_options,
+            config_properties_file=config_properties_file,
+        )
+        self._rpc = runtime_init(session=self._eggroll_session)
+        self._session_id = self._eggroll_session.get_session_id()
 
     def get_rpc(self):
         return self._rpc
 
     @property
     def session_id(self):
         return self._session_id
 
-    @computing_profile
-    def load(self, uri: URI, schema: dict, options: dict = None) -> Table:
+    def _load(self, uri: URI, schema: dict, options: dict) -> Table:
         from ._type import EggRollStoreType
 
         if uri.scheme != "eggroll":
             raise ValueError(f"uri scheme {uri.scheme} not supported with eggroll backend")
         try:
             _, namespace, name = uri.path_splits()
         except Exception as e:
             raise ValueError(f"uri {uri} not valid, demo format: eggroll:///namespace/name") from e
 
         if options is None:
             options = {}
-        if "store_type" not in options:
-            options["store_type"] = EggRollStoreType.ROLLPAIR_LMDB
-        options["create_if_missing"] = False
-        rp = self._rpc.load(namespace=namespace, name=name, options=options)
+        store_type = options.get("store_type", EggRollStoreType.ROLLPAIR_LMDB)
+        rp = self._rpc.load_rp(namespace=namespace, name=name, store_type=store_type)
         if rp is None or rp.get_partitions() == 0:
             raise RuntimeError(f"no exists: {name}, {namespace}")
 
-        if options["store_type"] != EggRollStoreType.ROLLPAIR_IN_MEMORY:
-            rp = rp.save_as(
+        if store_type != EggRollStoreType.ROLLPAIR_IN_MEMORY:
+            rp = rp.copy_as(
                 name=f"{name}_{uuid()}",
                 namespace=self.session_id,
-                partition=rp.get_partitions(),
-                options={"store_type": EggRollStoreType.ROLLPAIR_IN_MEMORY},
+                store_type=EggRollStoreType.ROLLPAIR_IN_MEMORY,
             )
 
             table = Table(rp=rp)
             table.schema = schema
             return table
 
-    @computing_profile
-    def parallelize(self, data, partition: int, include_key: bool, **kwargs) -> Table:
-        options = dict()
-        options["total_partitions"] = partition
-        options["include_key"] = include_key
-        rp = self._rpc.parallelize(data=data, options=options)
+    def _parallelize(
+        self,
+        data,
+        total_partitions,
+        key_serdes,
+        key_serdes_type,
+        value_serdes,
+        value_serdes_type,
+        partitioner,
+        partitioner_type,
+    ):
+        rp = self._rpc.parallelize(
+            data=data,
+            total_partitions=total_partitions,
+            partitioner=partitioner,
+            partitioner_type=partitioner_type,
+            key_serdes_type=key_serdes_type,
+            value_serdes_type=value_serdes_type,
+        )
         return Table(rp)
 
+    def _info(self):
+        if hasattr(self._rpc, "info"):
+            return self._rpc.info()
+        else:
+            return {
+                "session_id": self.session_id,
+            }
+
     def cleanup(self, name, namespace):
         self._rpc.cleanup(name=name, namespace=namespace)
 
     def stop(self):
-        return self._rp_session.stop()
+        return self._eggroll_session.stop()
 
     def kill(self):
-        return self._rp_session.kill()
+        return self._eggroll_session.kill()
 
-    def destroy(self):
+    def _destroy(self):
         try:
-            LOGGER.info(f"clean table namespace {self.session_id}")
+            logger.info(f"clean table namespace {self.session_id}")
             self.cleanup(namespace=self.session_id, name="*")
         except Exception:
-            LOGGER.warning(f"no found table namespace {self.session_id}")
+            logger.warning(f"no found table namespace {self.session_id}")
 
         try:
             self.stop()
         except Exception as e:
-            LOGGER.warning(f"stop storage session {self.session_id} failed, try to kill", e)
+            logger.exception(f"stop storage session {self.session_id} failed, try to kill")
             self.kill()
+        else:
+            logger.info(f"stop storage session {self.session_id} success")
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/eggroll/_type.py` & `pyfate-2.1.0/fate/arch/computing/backends/eggroll/_type.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/computing/spark/__init__.py` & `pyfate-2.1.0/fate/arch/computing/backends/spark/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/computing/spark/_csession.py` & `pyfate-2.1.0/fate/arch/computing/backends/spark/_csession.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,33 +13,32 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
 import logging
 import typing
 from typing import Iterable
 
-from fate.arch.abc import CSessionABC
-
-from ...unify import URI
+from fate.arch.computing.api import KVTableContext
+from fate.arch.unify import URI
 from ._table import from_hdfs, from_hive, from_localfs, from_rdd
 
 if typing.TYPE_CHECKING:
     from ._table import Table
 LOGGER = logging.getLogger(__name__)
 
 
-class CSession(CSessionABC):
+class CSession(KVTableContext):
     """
     manage RDDTable
     """
 
     def __init__(self, session_id):
         self._session_id = session_id
 
-    def load(self, uri: URI, schema, options: dict = None) -> "Table":
+    def _load(self, uri: URI, schema, options: dict = None) -> "Table":
         if not options:
             options = {}
         partitions = options.get("partitions", None)
 
         if uri.scheme == "hdfs":
             in_serialized = (options.get("in_serialized", True),)
             id_delimiter = (options.get("id_delimiter", ","),)
@@ -76,20 +75,29 @@
                 id_delimiter=id_delimiter,
             )
             table.schema = schema
             return table
 
         raise NotImplementedError(f"uri type {uri} not supported with spark backend")
 
-    def parallelize(self, data: Iterable, partition: int, include_key: bool, **kwargs):
+    def _parallelize(
+        self,
+        data: Iterable,
+        total_partitions,
+        key_serdes,
+        key_serdes_type,
+        value_serdes,
+        value_serdes_type,
+        partitioner,
+        partitioner_type,
+    ):
         # noinspection PyPackageRequirements
         from pyspark import SparkContext
 
-        _iter = data if include_key else enumerate(data)
-        rdd = SparkContext.getOrCreate().parallelize(_iter, partition)
+        rdd = SparkContext.getOrCreate().parallelize(data, total_partitions)
         return from_rdd(rdd)
 
     @property
     def session_id(self):
         return self._session_id
 
     def cleanup(self, name, namespace):
@@ -97,9 +105,9 @@
 
     def stop(self):
         pass
 
     def kill(self):
         pass
 
-    def destroy(self):
+    def _destroy(self):
         pass
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/spark/_materialize.py` & `pyfate-2.1.0/fate/arch/computing/backends/spark/_materialize.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/computing/standalone/__init__.py` & `pyfate-2.1.0/fate/arch/federation/backends/standalone/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,11 +10,10 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
 
-from ._csession import CSession
-from ._table import Table
+from ._federation import StandaloneFederation
 
-__all__ = ["Table", "CSession"]
+__all__ = ["StandaloneFederation"]
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/standalone/_csession.py` & `pyfate-2.1.0/fate/arch/computing/backends/standalone/_csession.py`

 * *Files 22% similar despite different names*

```diff
@@ -10,82 +10,120 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 import logging
-from collections.abc import Iterable
 from typing import Optional
 
-from fate.arch.abc import CSessionABC
-
-from ..._standalone import Session
-from ...unify import URI, generate_computing_uuid, uuid
+from fate.arch.computing.api import KVTableContext, generate_computing_uuid
+from fate.arch.unify import URI, uuid
+from ._standalone import Session
 from ._table import Table
 
-LOGGER = logging.getLogger(__name__)
+logger = logging.getLogger(__name__)
 
 
-class CSession(CSessionABC):
+class CSession(KVTableContext):
     def __init__(
-        self, session_id: Optional[str] = None, logger_config: Optional[dict] = None, options: Optional[dict] = None
+        self,
+        session_id: Optional[str] = None,
+        data_dir=None,
+        logger_config: Optional[dict] = None,
+        options: Optional[dict] = None,
     ):
         if session_id is None:
             session_id = generate_computing_uuid()
+        if data_dir is None:
+            raise ValueError("data_dir is None")
         if options is None:
             options = {}
         max_workers = options.get("task_cores", None)
-        self._session = Session(session_id, max_workers=max_workers, logger_config=logger_config)
+        self._session = Session(session_id, data_dir=data_dir, max_workers=max_workers, logger_config=logger_config)
 
     def get_standalone_session(self):
         return self._session
 
     @property
     def session_id(self):
         return self._session.session_id
 
-    def load(self, uri: URI, schema: dict, options: dict = None):
+    def _load(
+        self,
+        uri: URI,
+        schema: dict,
+        options: dict,
+    ):
         if uri.scheme != "standalone":
             raise ValueError(f"uri scheme `{uri.scheme}` not supported with standalone backend")
         try:
             *database, namespace, name = uri.path_splits()
         except Exception as e:
             raise ValueError(f"uri `{uri}` not valid, demo format: standalone://database_path/namespace/name") from e
 
         raw_table = self._session.load(name=name, namespace=namespace)
-        partitions = raw_table.partitions
-        raw_table = raw_table.save_as(
+        raw_table = raw_table.copy_as(
             name=f"{name}_{uuid()}",
             namespace=namespace,
-            partition=partitions,
             need_cleanup=True,
         )
         table = Table(raw_table)
         table.schema = schema
         return table
 
-    def parallelize(self, data: Iterable, partition: int, include_key: bool, **kwargs):
-        table = self._session.parallelize(data=data, partition=partition, include_key=include_key, **kwargs)
+    def _parallelize(
+        self,
+        data,
+        total_partitions,
+        key_serdes,
+        key_serdes_type,
+        value_serdes,
+        value_serdes_type,
+        partitioner,
+        partitioner_type,
+    ):
+        table = self._session.parallelize(
+            data=data,
+            partition=total_partitions,
+            partitioner=partitioner,
+            key_serdes_type=key_serdes_type,
+            value_serdes_type=value_serdes_type,
+            partitioner_type=partitioner_type,
+        )
         return Table(table)
 
+    def _info(self, level=0):
+        if level == 0:
+            return f"Standalone<session_id={self.session_id}, max_workers={self._session.max_workers}, data_dir={self._session.data_dir}>"
+
+        elif level == 1:
+            import inspect
+
+            return {
+                "session_id": self.session_id,
+                "data_dir": self._session.data_dir,
+                "max_workers": self._session.max_workers,
+                "code_path": inspect.getfile(self._session.__class__),
+            }
+
     def cleanup(self, name, namespace):
         return self._session.cleanup(name=name, namespace=namespace)
 
     def stop(self):
         return self._session.stop()
 
     def kill(self):
         return self._session.kill()
 
-    def destroy(self):
+    def _destroy(self):
         try:
-            LOGGER.info(f"clean table namespace {self.session_id}")
+            logger.debug(f"clean table namespace {self.session_id}")
             self.cleanup(namespace=self.session_id, name="*")
-        except Exception:
-            LOGGER.warning(f"no found table namespace {self.session_id}")
+        except:
+            logger.warning(f"no found table namespace {self.session_id}")
 
         try:
             self.stop()
         except Exception as e:
-            LOGGER.warning(f"stop storage session {self.session_id} failed, try to kill", e)
+            logger.warning(f"stop storage session {self.session_id} failed, try to kill", e)
             self.kill()
```

### Comparing `pyfate-2.0.0b0/fate/arch/computing/standalone/_type.py` & `pyfate-2.1.0/fate/arch/computing/backends/standalone/_type.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/context/__init__.py` & `pyfate-2.1.0/fate/ml/feature_binning/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 #
-#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#  Copyright 2023 The FATE Authors. All Rights Reserved.
 #
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from ._cipher import CipherKit
-from ._context import Context
 
-__all__ = ["Context", "CipherKit"]
+from .hetero_feature_binning import HeteroBinningModuleGuest, HeteroBinningModuleHost
```

### Comparing `pyfate-2.0.0b0/fate/arch/context/_cipher.py` & `pyfate-2.1.0/fate/arch/context/_cipher.py`

 * *Files 24% similar despite different names*

```diff
@@ -12,52 +12,66 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 import logging
 import typing
 
-from ..unify import device
+from fate.arch.config import cfg
+from ..unify import device as device_type
 
+if typing.TYPE_CHECKING:
+    from ._context import Context
+    from ..tensor.phe import PHETensorCipher, PHETensorCipherPublic
 logger = logging.getLogger(__name__)
 
 
 class CipherKit:
-    def __init__(self, device: device, cipher_mapping: typing.Optional[dict] = None) -> None:
+    def __init__(self, device: device_type, cipher_mapping: typing.Optional[dict] = None) -> None:
         self._device = device
         if cipher_mapping is None:
             self._cipher_mapping = {}
         else:
             self._cipher_mapping = cipher_mapping
         self._allow_custom_random_seed = False
         self._custom_random_seed = 42
 
-    def set_phe(self, device: device, options: typing.Optional[dict]):
+        self.ctx = None
+
+    def set_ctx(self, ctx: "Context"):
+        self.ctx = ctx
+
+    def set_phe(self, device: device_type, options: typing.Optional[dict]):
         if "phe" not in self._cipher_mapping:
             self._cipher_mapping["phe"] = {}
         self._cipher_mapping["phe"][device] = options
 
     def _set_default_phe(self):
         if "phe" not in self._cipher_mapping:
             self._cipher_mapping["phe"] = {}
         if self._device not in self._cipher_mapping["phe"]:
-            if self._device == device.CPU:
-                self._cipher_mapping["phe"][device.CPU] = {"kind": "paillier", "key_length": 1024}
+            if self._device == device_type.CPU:
+                self._cipher_mapping["phe"][device_type.CPU] = {
+                    "kind": "paillier",
+                    "key_length": cfg.safety.phe.paillier.minimum_key_size,
+                }
             else:
                 logger.warning(f"no impl exists for device {self._device}, fallback to CPU")
-                self._cipher_mapping["phe"][device.CPU] = self._cipher_mapping["phe"].get(
-                    device.CPU, {"kind": "paillier", "key_length": 1024}
+                self._cipher_mapping["phe"][device_type.CPU] = self._cipher_mapping["phe"].get(
+                    device_type.CPU, {"kind": "paillier", "key_length": cfg.safety.phe.paillier.minimum_key_size}
                 )
 
     @property
     def phe(self):
+        if self.ctx is None:
+            raise ValueError("context not set")
         self._set_default_phe()
         if self._device not in self._cipher_mapping["phe"]:
             raise ValueError(f"no impl exists for device {self._device}")
-        return PHECipherBuilder(**self._cipher_mapping["phe"][self._device])
+        return PHECipherBuilder(self.ctx, **self._cipher_mapping["phe"][self._device])
 
     @property
     def allow_custom_random_seed(self):
         return self._allow_custom_random_seed
 
     def set_allow_custom_random_seed(self, allow_custom_random_seed):
         self._allow_custom_random_seed = allow_custom_random_seed
@@ -66,75 +80,98 @@
         self._custom_random_seed = custom_random_seed
 
     def get_custom_random_seed(self):
         return self._custom_random_seed
 
 
 class PHECipherBuilder:
-    def __init__(self, kind, key_length) -> None:
+    def __init__(self, ctx: "Context", kind, key_length) -> None:
+        self.ctx = ctx
         self.kind = kind
         self.key_length = key_length
 
+    def broadcast(self, src: int = 0, options: typing.Optional[dict] = None, tag: str = "phe_cipher"):
+        if src == self.ctx.rank:
+            cipher = self.setup(options)
+            cipher_public = cipher.to_public()
+            for p in self.ctx.parties:
+                if p.rank != src:
+                    p.put(tag, cipher_public)
+            return cipher
+        else:
+            return self.ctx.parties[src].get(name=tag)
+
     def setup(self, options: typing.Optional[dict] = None):
         if options is None:
             kind = self.kind
             key_size = self.key_length
         else:
             kind = options.get("kind", self.kind)
-            key_size = options.get("key_length", 1024)
+            key_size = options.get("key_length", self.key_length)
 
         if kind == "paillier":
+            if not cfg.safety.phe.paillier.allow:
+                raise ValueError("paillier is not allowed in config")
+            if key_size < cfg.safety.phe.paillier.minimum_key_size:
+                raise ValueError(
+                    f"key size {key_size} is too small, minimum is {cfg.safety.phe.paillier.minimum_key_size}"
+                )
             from fate.arch.protocol.phe.paillier import evaluator, keygen
             from fate.arch.tensor.phe import PHETensorCipher
 
             sk, pk, coder = keygen(key_size)
             tensor_cipher = PHETensorCipher.from_raw_cipher(pk, coder, sk, evaluator)
+
             return PHECipher(kind, key_size, pk, sk, evaluator, coder, tensor_cipher, True, True, True)
 
         if kind == "ou":
+            if not cfg.safety.phe.ou.allow:
+                raise ValueError("ou is not allowed in config")
+            if key_size < cfg.safety.phe.ou.minimum_key_size:
+                raise ValueError(f"key size {key_size} is too small, minimum is {cfg.safety.phe.ou.minimum_key_size}")
             from fate.arch.protocol.phe.ou import evaluator, keygen
             from fate.arch.tensor.phe import PHETensorCipher
 
             sk, pk, coder = keygen(key_size)
             tensor_cipher = PHETensorCipher.from_raw_cipher(pk, coder, sk, evaluator)
             return PHECipher(kind, key_size, pk, sk, evaluator, coder, tensor_cipher, False, False, True)
 
         elif kind == "mock":
+            if not cfg.safety.phe.mock.allow:
+                raise ValueError("mock is not allowed in config")
             from fate.arch.protocol.phe.mock import evaluator, keygen
             from fate.arch.tensor.phe import PHETensorCipher
 
             sk, pk, coder = keygen(key_size)
             tensor_cipher = PHETensorCipher.from_raw_cipher(pk, coder, sk, evaluator)
             return PHECipher(kind, key_size, pk, sk, evaluator, coder, tensor_cipher, True, False, False)
 
         else:
             raise ValueError(f"Unknown PHE keygen kind: {self.kind}")
 
 
-class PHECipher:
+class PHECipherPublic:
     def __init__(
         self,
         kind,
         key_size,
         pk,
-        sk,
         evaluator,
         coder,
-        tensor_cipher,
+        tensor_cipher: "PHETensorCipherPublic",
         can_support_negative_number,
         can_support_squeeze,
         can_support_pack,
     ) -> None:
         self._kind = kind
         self._key_size = key_size
         self._pk = pk
-        self._sk = sk
         self._coder = coder
         self._evaluator = evaluator
-        self._tensor_cipher = tensor_cipher
+        self._tensor_cipher: "PHETensorCipherPublic" = tensor_cipher
         self._can_support_negative_number = can_support_negative_number
         self._can_support_squeeze = can_support_squeeze
         self._can_support_pack = can_support_pack
 
     @property
     def kind(self):
         return self._kind
@@ -157,25 +194,67 @@
 
     def get_tensor_encryptor(self):
         return self._tensor_cipher.pk
 
     def get_tensor_coder(self):
         return self._tensor_cipher.coder
 
-    def get_tensor_decryptor(self):
-        return self._tensor_cipher.sk
-
     @property
     def pk(self):
         return self._pk
 
     @property
     def coder(self):
         return self._coder
 
     @property
+    def evaluator(self):
+        return self._evaluator
+
+
+class PHECipher(PHECipherPublic):
+    def __init__(
+        self,
+        kind,
+        key_size,
+        pk,
+        sk,
+        evaluator,
+        coder,
+        tensor_cipher: "PHETensorCipher",
+        can_support_negative_number,
+        can_support_squeeze,
+        can_support_pack,
+    ) -> None:
+        super().__init__(
+            kind,
+            key_size,
+            pk,
+            evaluator,
+            coder,
+            tensor_cipher,
+            can_support_negative_number,
+            can_support_squeeze,
+            can_support_pack,
+        )
+        self._sk = sk
+        self._tensor_cipher = tensor_cipher
+
+    def get_tensor_decryptor(self):
+        return self._tensor_cipher.sk
+
+    @property
     def sk(self):
         return self._sk
 
-    @property
-    def evaluator(self):
-        return self._evaluator
+    def to_public(self):
+        return PHECipherPublic(
+            self.kind,
+            self.key_size,
+            self.pk,
+            self.evaluator,
+            self.coder,
+            self._tensor_cipher.to_public(),
+            self.can_support_negative_number,
+            self.can_support_squeeze,
+            self.can_support_pack,
+        )
```

### Comparing `pyfate-2.0.0b0/fate/arch/context/_context.py` & `pyfate-2.1.0/fate/arch/context/_context.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,42 +8,47 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 import logging
+import typing
 from typing import Iterable, Literal, Optional, Tuple, TypeVar, overload
 
-from fate.arch.abc import CSessionABC, FederationEngine
-
-from ..unify import device
+from fate.arch.trace import auto_trace
 from ._cipher import CipherKit
-from ._federation import Parties, Party
 from ._metrics import InMemoryMetricsHandler, MetricsWrap
 from ._namespace import NS, default_ns
+from ._parties import Parties, Party
+from ..unify import device as device_type
 
 logger = logging.getLogger(__name__)
 
 T = TypeVar("T")
 
+if typing.TYPE_CHECKING:
+    from fate.arch.federation.api import Federation
+    from fate.arch.computing.api import KVTableContext
+
 
 class Context:
     """
     Note: most parameters has default dummy value,
           which is convenient when used in script.
           please pass in custom implements as you wish
     """
 
     def __init__(
         self,
-        device: device = device.CPU,
-        computing: Optional["CSessionABC"] = None,
-        federation: Optional["FederationEngine"] = None,
+        device: device_type = device_type.CPU,
+        computing: Optional["KVTableContext"] = None,
+        federation: Optional["Federation"] = None,
         metrics_handler: Optional = None,
         namespace: Optional[NS] = None,
         cipher: Optional[CipherKit] = None,
     ) -> None:
         self._device = device
         self._computing = computing
         self._federation = federation
@@ -51,18 +56,30 @@
         self._namespace = namespace
         self._cipher = cipher
 
         if self._namespace is None:
             self._namespace = default_ns
         if self._cipher is None:
             self._cipher: CipherKit = CipherKit(device)
+        self._cipher.set_ctx(self)
 
         self._role_to_parties = None
         self._is_destroyed = False
 
+        self._mpc = None
+
+    @property
+    def mpc(self):
+        from ._mpc import MPC
+
+        if self._mpc is None:
+            self._mpc = MPC(self)
+
+        return self._mpc
+
     @property
     def device(self):
         return self._device
 
     @property
     def namespace(self):
         return self._namespace
@@ -90,19 +107,19 @@
             federation=self._federation,
             metrics_handler=self._metrics_handler,
             namespace=namespace,
             cipher=self._cipher,
         )
 
     @property
-    def computing(self):
+    def computing(self) -> "KVTableContext":
         return self._get_computing()
 
     @property
-    def federation(self) -> "FederationEngine":
+    def federation(self) -> "Federation":
         return self._get_federation()
 
     def sub_ctx(self, name: str) -> "Context":
         return self.with_namespace(self._namespace.sub_ns(name=name))
 
     def indexed_ctx(self, index: int) -> "Context":
         return self.with_namespace(self._namespace.indexed_ns(index))
@@ -124,15 +141,14 @@
         ...
 
     @overload
     def ctxs_range(self, start: int, end: int) -> Iterable[Tuple[int, "Context"]]:
         ...
 
     def ctxs_range(self, *args, **kwargs) -> Iterable[Tuple[int, "Context"]]:
-
         """
         create contexes with namespaces indexed from 0 to end(excluded)
         """
 
         if "start" in kwargs:
             start = kwargs["start"]
             if "end" not in kwargs:
@@ -163,15 +179,15 @@
     def ctxs_zip(self, iterable: Iterable[T]) -> Iterable[Tuple["Context", T]]:
         """
         zip contexts with iterable with namespaces indexed from 0
         """
         for i, it in enumerate(iterable):
             yield self.with_namespace(self._namespace.indexed_ns(index=i)), it
 
-    def set_federation(self, federation: "FederationEngine"):
+    def set_federation(self, federation: "Federation"):
         self._federation = federation
 
     @property
     def guest(self) -> Party:
         return self._get_parties("guest")[0]
 
     @property
@@ -179,14 +195,18 @@
         return self._get_parties("host")
 
     @property
     def arbiter(self) -> Party:
         return self._get_parties("arbiter")[0]
 
     @property
+    def rank(self):
+        return self.local.rank
+
+    @property
     def local(self):
         role, party_id = self._get_federation().local_party
         for party in self._get_parties(role):
             if party.party[1] == party_id:
                 return party
         raise RuntimeError("local party not found")
 
@@ -202,15 +222,19 @@
     def is_on_arbiter(self):
         return self._federation.local_party[0] == "arbiter"
 
     @property
     def parties(self) -> Parties:
         return self._get_parties()
 
-    def _get_parties(self, role: Optional[Literal["guest", "host", "arbiter"]] = None) -> Parties:
+    @property
+    def world_size(self):
+        return self._get_federation().world_size
+
+    def _get_parties(self, role: Optional[Literal["guest", "host", "arbiter", "local"]] = None) -> Parties:
         # update role to parties mapping
         if self._role_to_parties is None:
             self._role_to_parties = {}
             for i, party in enumerate(self._get_federation().parties):
                 self._role_to_parties.setdefault(party[0], []).append((i, party))
 
         parties = []
@@ -222,35 +246,37 @@
                 raise RuntimeError(f"no {role} party has configured")
             else:
                 parties.extend(self._role_to_parties[role])
         parties.sort(key=lambda x: x[0])
         return Parties(
             self,
             self._get_federation(),
+            self._get_computing(),
             parties,
             self._namespace,
         )
 
     def _get_federation(self):
         if self._federation is None:
             raise RuntimeError(f"federation not set")
         return self._federation
 
-    def _get_computing(self):
+    def _get_computing(self) -> "KVTableContext":
         if self._computing is None:
             raise RuntimeError(f"computing not set")
         return self._computing
 
+    @auto_trace
     def destroy(self):
         if not self._is_destroyed:
             try:
                 self.federation.destroy()
                 logger.debug("federation engine destroy done")
-            except:
-                logger.exception("federation engine destroy failed", stack_info=True)
+            except Exception as e:
+                logger.exception(f"federation engine destroy failed: {e}")
 
             try:
                 self.computing.destroy()
                 logger.debug("computing engine destroy done")
-            except:
-                logger.exception("computing engine destroy failed", stack_info=True)
+            except Exception as e:
+                logger.exception(f"computing engine destroy failed: {e}")
             self._is_destroyed = True
```

### Comparing `pyfate-2.0.0b0/fate/arch/context/_metrics.py` & `pyfate-2.1.0/fate/arch/context/_metrics.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/context/_namespace.py` & `pyfate-2.1.0/fate/arch/context/_namespace.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,9 +33,9 @@
     "parse_schema",
     "build_schema",
     "serialize",
     "deserialize",
     "DataFrame",
     "KFold",
     "DataLoader",
-    "BatchEncoding"
+    "BatchEncoding",
 ]
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/_dataframe.py` & `pyfate-2.1.0/fate/arch/dataframe/_dataframe.py`

 * *Files 21% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 from typing import List, Union
 
 import numpy as np
 import pandas as pd
 
 from fate.arch.tensor import DTensor
 from .manager import DataManager, Schema
+from fate.arch.trace import auto_trace
 
 if typing.TYPE_CHECKING:
     from fate.arch.histogram import DistributedHistogram, HistogramBuilder
 
 
 class DataFrame(object):
     def __init__(self, ctx, block_table, partition_order_mappings, data_manager: DataManager):
@@ -71,14 +72,15 @@
             return None
 
         return self.__extract_fields(
             with_sample_id=True, with_match_id=True, with_label=False, with_weight=False, columns=self.columns.tolist()
         )
 
     @property
+    @auto_trace
     def label(self):
         if not self.schema.label_name:
             return None
 
         if self._label is None:
             self._label = self.__extract_fields(
                 with_sample_id=True, with_match_id=True, with_label=True, with_weight=False
@@ -136,14 +138,15 @@
     def data_manager(self, data_manager):
         self._data_manager = data_manager
 
     @property
     def dtypes(self):
         return self._data_manager.dtypes
 
+    @auto_trace
     def as_tensor(self, dtype=None):
         """
         df.weight.as_tensor()
         df.label.as_tensor()
         df.values.as_tensor()
         """
         from .ops._transformer import transform_to_tensor
@@ -153,120 +156,140 @@
         )
 
     def as_pd_df(self) -> "pd.DataFrame":
         from .ops._transformer import transform_to_pandas_dataframe
 
         return transform_to_pandas_dataframe(self._block_table, self._data_manager)
 
+    @auto_trace
     def apply_row(self, func, columns=None, with_label=False, with_weight=False, enable_type_align_checking=False):
         from .ops._apply_row import apply_row
 
         return apply_row(
             self,
             func,
             columns=columns,
             with_label=with_label,
             with_weight=with_weight,
             enable_type_align_checking=enable_type_align_checking,
         )
 
+    @auto_trace
     def create_frame(self, with_label=False, with_weight=False, columns: Union[list, pd.Index] = None) -> "DataFrame":
         if columns is not None and isinstance(columns, pd.Index):
             columns = columns.tolist()
 
         return self.__extract_fields(
             with_sample_id=True, with_match_id=True, with_label=with_label, with_weight=with_weight, columns=columns
         )
 
+    @auto_trace
     def empty_frame(self) -> "DataFrame":
         return DataFrame(
             self._ctx,
-            self._ctx.computing.parallelize([], include_key=False, partition=self._block_table.partitions),
+            self._ctx.computing.parallelize([], include_key=False, partition=self._block_table.num_partitions),
             partition_order_mappings=dict(),
             data_manager=self._data_manager.duplicate(),
         )
 
+    @auto_trace
     def drop(self, index) -> "DataFrame":
         from .ops._dimension_scaling import drop
 
         return drop(self, index)
 
+    @auto_trace
     def fillna(self, value):
         from .ops._fillna import fillna
 
         return fillna(self, value)
 
+    @auto_trace
     def get_dummies(self, dtype="int32"):
         from .ops._encoder import get_dummies
 
         return get_dummies(self, dtype=dtype)
 
+    @auto_trace
     def isna(self):
         from .ops._missing import isna
 
         return isna(self)
 
+    @auto_trace
     def isin(self, values):
         from .ops._isin import isin
 
         return isin(self, values)
 
+    @auto_trace
     def na_count(self):
         return self.isna().sum()
 
+    @auto_trace
     def max(self) -> "pd.Series":
         from .ops._stat import max
 
         return max(self)
 
+    @auto_trace
     def min(self, *args, **kwargs) -> "pd.Series":
         from .ops._stat import min
 
         return min(self)
 
+    @auto_trace
     def mean(self, *args, **kwargs) -> "pd.Series":
         from .ops._stat import mean
 
         return mean(self)
 
+    @auto_trace
     def sum(self, *args, **kwargs) -> "pd.Series":
         from .ops._stat import sum
 
         return sum(self)
 
+    @auto_trace
     def std(self, ddof=1, **kwargs) -> "pd.Series":
         from .ops._stat import std
 
         return std(self, ddof=ddof)
 
+    @auto_trace
     def var(self, ddof=1, **kwargs):
         from .ops._stat import var
 
         return var(self, ddof=ddof)
 
+    @auto_trace
     def variation(self, ddof=1):
         from .ops._stat import variation
 
         return variation(self, ddof=ddof)
 
+    @auto_trace
     def skew(self, unbiased=False):
         from .ops._stat import skew
 
         return skew(self, unbiased=unbiased)
 
+    @auto_trace
     def kurt(self, unbiased=False):
         from .ops._stat import kurt
 
         return kurt(self, unbiased=unbiased)
 
+    @auto_trace
     def sigmoid(self) -> "DataFrame":
         from .ops._activation import sigmoid
 
         return sigmoid(self)
 
+    @auto_trace
     def rename(
         self,
         sample_id_name: str = None,
         match_id_name: str = None,
         label_name: str = None,
         weight_name: str = None,
         columns: dict = None,
@@ -275,99 +298,122 @@
             sample_id_name=sample_id_name,
             match_id_name=match_id_name,
             label_name=label_name,
             weight_name=weight_name,
             columns=columns,
         )
 
+    @auto_trace
     def count(self) -> "int":
         return self.shape[0]
 
+    @auto_trace
     def describe(self, ddof=1, unbiased=False):
         from .ops._stat import describe
 
         return describe(self, ddof=ddof, unbiased=unbiased)
 
+    @auto_trace
     def quantile(self, q, relative_error: float = 1e-4):
         from .ops._quantile import quantile
 
         return quantile(self, q, relative_error)
 
+    @auto_trace
     def qcut(self, q: int):
         from .ops._quantile import qcut
 
         return qcut(self, q)
 
+    @auto_trace
     def bucketize(self, boundaries: Union[dict, pd.DataFrame]) -> "DataFrame":
         from .ops._encoder import bucketize
 
         return bucketize(self, boundaries)
 
-    def distributed_hist_stat(self,
-                              histogram_builder: "HistogramBuilder",
-                              position: "DataFrame" = None,
-                              targets: Union[dict, "DataFrame"] = None,
-                              ) -> "DistributedHistogram":
+    @auto_trace
+    def distributed_hist_stat(
+        self,
+        histogram_builder: "HistogramBuilder",
+        position: "DataFrame" = None,
+        targets: Union[dict, "DataFrame"] = None,
+    ) -> "DistributedHistogram":
         from .ops._histogram import distributed_hist_stat
 
         if targets is None:
             raise ValueError("To use distributed hist stat, targets should not be None")
         if position is None:
             position = self.create_frame()
             position["node_idx"] = 0
 
         return distributed_hist_stat(self, histogram_builder, position, targets)
 
+    @auto_trace
     def replace(self, to_replace=None) -> "DataFrame":
         from .ops._replace import replace
 
         return replace(self, to_replace)
 
+    @auto_trace
     def __add__(self, other: Union[int, float, list, "np.ndarray", "DataFrame", "pd.Series"]) -> "DataFrame":
         return self.__arithmetic_operate(operator.add, other)
 
+    @auto_trace
     def __radd__(self, other: Union[int, float, list, "np.ndarray", "pd.Series"]) -> "DataFrame":
         return self + other
 
+    @auto_trace
     def __sub__(self, other: Union[int, float, list, "np.ndarray", "pd.Series"]) -> "DataFrame":
         return self.__arithmetic_operate(operator.sub, other)
 
+    @auto_trace
     def __rsub__(self, other: Union[int, float, list, "np.ndarray", "pd.Series"]) -> "DataFrame":
         return self * (-1) + other
 
+    @auto_trace
     def __mul__(self, other) -> "DataFrame":
         return self.__arithmetic_operate(operator.mul, other)
 
+    @auto_trace
     def __rmul__(self, other) -> "DataFrame":
         return self * other
 
+    @auto_trace
     def __truediv__(self, other) -> "DataFrame":
         return self.__arithmetic_operate(operator.truediv, other)
 
+    @auto_trace
     def __pow__(self, power) -> "DataFrame":
         return self.__arithmetic_operate(operator.pow, power)
 
+    @auto_trace
     def __lt__(self, other) -> "DataFrame":
         return self.__cmp_operate(operator.lt, other)
 
+    @auto_trace
     def __le__(self, other) -> "DataFrame":
         return self.__cmp_operate(operator.le, other)
 
+    @auto_trace
     def __gt__(self, other) -> "DataFrame":
         return self.__cmp_operate(operator.gt, other)
 
+    @auto_trace
     def __ge__(self, other) -> "DataFrame":
         return self.__cmp_operate(operator.ge, other)
 
+    @auto_trace
     def __eq__(self, other) -> "DataFrame":
         return self.__cmp_operate(operator.eq, other)
 
+    @auto_trace
     def __ne__(self, other) -> "DataFrame":
         return self.__cmp_operate(operator.ne, other)
 
+    @auto_trace
     def __invert__(self):
         from .ops._unary_operator import invert
 
         return invert(self)
 
     def __arithmetic_operate(self, op, other) -> "DataFrame":
         from .ops._arithmetic import arith_operate
@@ -474,86 +520,103 @@
             index = header_mapping.get(col, None)
             if index is None:
                 raise ValueError(f"Can not find column: {col}")
             indexes.append(index)
 
         return indexes
 
+    @auto_trace
     def get_indexer(self, target):
         if target not in ["sample_id", "match_id"]:
             raise ValueError(f"Target should be sample_id or match_id, but {target} found")
 
         if self.shape[0] == 0:
-            return self._ctx.computing.parallelize([], include_key=False, partition=self._block_table.partitions)
+            return self._ctx.computing.parallelize([], include_key=False, partition=self._block_table.num_partitions)
 
         target_name = getattr(self.schema, f"{target}_name")
         indexer = self.__convert_to_table(target_name)
         if target == "sample_id":
             self._sample_id_indexer = indexer
         else:
             self._match_id_indexer = indexer
 
         return indexer
 
+    @auto_trace
     def loc(self, indexer, target="sample_id", preserve_order=False):
         from .ops._indexer import loc
 
         return loc(self, indexer, target=target, preserve_order=preserve_order)
 
+    @auto_trace
     def iloc(self, indexer: "DataFrame") -> "DataFrame":
         from .ops._dimension_scaling import retrieval_row
+
         return retrieval_row(self, indexer)
 
+    @auto_trace
     def loc_with_sample_id_replacement(self, indexer):
         """
         indexer: table,
             row: (key=random_key,
             value=(sample_id, (src_block_id, src_block_offset))
         """
         from .ops._indexer import loc_with_sample_id_replacement
 
         return loc_with_sample_id_replacement(self, indexer)
 
+    @auto_trace
     def flatten(self, key_type="block_id", with_sample_id=True):
         """
         flatten data_frame
         """
         from .ops._indexer import flatten_data
+
         return flatten_data(self, key_type=key_type, with_sample_id=with_sample_id)
 
+    @auto_trace
     def copy(self) -> "DataFrame":
         return DataFrame(
             self._ctx,
             self._block_table.mapValues(lambda v: v),
             copy.deepcopy(self.partition_order_mappings),
             self._data_manager.duplicate(),
         )
 
     @classmethod
     def from_flatten_data(cls, ctx, flatten_table, data_manager, key_type) -> "DataFrame":
         from .ops._indexer import transform_flatten_data_to_df
+
         return transform_flatten_data_to_df(ctx, flatten_table, data_manager, key_type)
 
     @classmethod
+    @auto_trace
     def hstack(cls, stacks: List["DataFrame"]) -> "DataFrame":
         from .ops._dimension_scaling import hstack
 
         return hstack(stacks)
 
     @classmethod
+    @auto_trace
     def vstack(cls, stacks: List["DataFrame"]) -> "DataFrame":
         from .ops._dimension_scaling import vstack
 
         return vstack(stacks)
 
+    @auto_trace
     def sample(self, n: int = None, frac: float = None, random_state=None) -> "DataFrame":
         from .ops._dimension_scaling import sample
 
         return sample(self, n, frac, random_state)
 
+    def nlargest(self, n, columns, keep="first", error=1e-4):
+        from .ops._sort import nlargest
+
+        return nlargest(self, n, columns, keep=keep, error=error)
+
     def __extract_fields(
         self,
         with_sample_id=True,
         with_match_id=True,
         with_label=True,
         with_weight=True,
         columns: Union[str, list] = None,
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/_frame_reader.py` & `pyfate-2.1.0/fate/arch/dataframe/_frame_reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,20 +17,21 @@
 from typing import Union
 
 
 from .conf.default_config import DATAFRAME_BLOCK_ROW_SIZE
 from .entity import types
 from ._dataframe import DataFrame
 from .manager import DataManager
+from fate.arch.trace import auto_trace
 
 
 class TableReader(object):
     def __init__(
         self,
-        sample_id_name: str =None,
+        sample_id_name: str = None,
         match_id_name: str = None,
         match_id_list: list = None,
         match_id_range: int = 0,
         label_name: Union[None, str] = None,
         label_type: str = "int",
         weight_name: Union[None, str] = None,
         weight_type: str = "float32",
@@ -38,15 +39,15 @@
         delimiter: str = ",",
         dtype: Union[str, dict] = "float32",
         anonymous_site_name: str = None,
         na_values: Union[str, list, dict] = None,
         input_format: str = "dense",
         tag_with_value: bool = False,
         tag_value_delimiter: str = ":",
-        block_row_size: int = None
+        block_row_size: int = None,
     ):
         self._sample_id_name = sample_id_name
         self._match_id_name = match_id_name
         self._match_id_list = match_id_list
         self._match_id_range = match_id_range
         self._label_name = label_name
         self._label_type = label_type
@@ -67,47 +68,57 @@
     def check_params(self):
         if not self._sample_id_name:
             raise ValueError("Please provide sample_id_name")
 
         if not isinstance(self._block_row_size, int) or self._block_row_size < 0:
             raise ValueError("block_row_size should be positive integer")
 
+    @auto_trace
     def to_frame(self, ctx, table):
         if self._input_format != "dense":
             raise ValueError("Only support dense input format in this version.")
 
         return self._dense_format_to_frame(ctx, table)
 
     def _dense_format_to_frame(self, ctx, table):
         data_manager = DataManager(block_row_size=self._block_row_size)
         columns = self._header.split(self._delimiter, -1)
         columns.remove(self._sample_id_name)
         retrieval_index_dict = data_manager.init_from_local_file(
-            sample_id_name=self._sample_id_name, columns=columns, match_id_list=self._match_id_list,
-            match_id_name=self._match_id_name, label_name=self._label_name, weight_name=self._weight_name,
-            label_type=self._label_type, weight_type=self._weight_type,
-            dtype=self._dtype, default_type=types.DEFAULT_DATA_TYPE)
+            sample_id_name=self._sample_id_name,
+            columns=columns,
+            match_id_list=self._match_id_list,
+            match_id_name=self._match_id_name,
+            label_name=self._label_name,
+            weight_name=self._weight_name,
+            label_type=self._label_type,
+            weight_type=self._weight_type,
+            dtype=self._dtype,
+            default_type=types.DEFAULT_DATA_TYPE,
+        )
 
         from .ops._indexer import get_partition_order_by_raw_table
+
         partition_order_mappings = get_partition_order_by_raw_table(table, data_manager.block_row_size)
         # partition_order_mappings = _get_partition_order(table)
         table = table.mapValues(lambda value: value.split(self._delimiter, -1))
-        to_block_func = functools.partial(_to_blocks,
-                                          data_manager=data_manager,
-                                          retrieval_index_dict=retrieval_index_dict,
-                                          partition_order_mappings=partition_order_mappings)
-        block_table = table.mapPartitions(
-            to_block_func,
-            use_previous_behavior=False
+        to_block_func = functools.partial(
+            _to_blocks,
+            data_manager=data_manager,
+            retrieval_index_dict=retrieval_index_dict,
+            partition_order_mappings=partition_order_mappings,
         )
+        block_table = table.mapPartitions(to_block_func, use_previous_behavior=False)
 
-        return DataFrame(ctx=ctx,
-                         block_table=block_table,
-                         partition_order_mappings=partition_order_mappings,
-                         data_manager=data_manager)
+        return DataFrame(
+            ctx=ctx,
+            block_table=block_table,
+            partition_order_mappings=partition_order_mappings,
+            data_manager=data_manager,
+        )
 
 
 class ImageReader(object):
     """
     Image Reader now support convert image to a 3D tensor, dtype=torch.float64
     """
 
@@ -129,43 +140,44 @@
         label_name: Union[None, str] = None,
         label_type: str = "int",
         weight_name: Union[None, str] = None,
         weight_type: str = "float32",
         dtype: str = "float32",
         na_values: Union[None, str, list, dict] = None,
         partition: int = 4,
-        block_row_size: int = None
+        block_row_size: int = None,
     ):
         self._sample_id_name = sample_id_name
         self._match_id_list = match_id_list
         self._match_id_name = match_id_name
         self._delimiter = delimiter
         self._label_name = label_name
         self._label_type = label_type
         self._weight_name = weight_name
         self._weight_type = weight_type
         self._dtype = dtype
         self._na_values = na_values
         self._partition = partition
         self._block_row_size = block_row_size if block_row_size is not None else DATAFRAME_BLOCK_ROW_SIZE
 
+    @auto_trace
     def to_frame(self, ctx, path):
         # TODO: use table put data instead of read all data
         df = pd.read_csv(path, delimiter=self._delimiter, na_values=self._na_values)
 
         return PandasReader(
             sample_id_name=self._sample_id_name,
             match_id_list=self._match_id_list,
             match_id_name=self._match_id_name,
             label_name=self._label_name,
             label_type=self._label_type,
             weight_name=self._weight_name,
             dtype=self._dtype,
             partition=self._partition,
-            block_row_size=self._block_row_size
+            block_row_size=self._block_row_size,
         ).to_frame(ctx, df)
 
 
 class HiveReader(object):
     ...
 
 
@@ -212,74 +224,80 @@
         self._dtype = dtype
         self._partition = partition
         self._block_row_size = block_row_size if block_row_size is not None else DATAFRAME_BLOCK_ROW_SIZE
 
         if self._sample_id_name and not self._match_id_name:
             raise ValueError(f"As sample_id {self._sample_id_name} is given, match_id should be given too")
 
+    @auto_trace
     def to_frame(self, ctx, df: "pd.DataFrame"):
         if not self._sample_id_name:
             self._sample_id_name = types.DEFAULT_SID_NAME
             df.index.name = self._sample_id_name
         else:
             df = df.set_index(self._sample_id_name)
 
         data_manager = DataManager(block_row_size=self._block_row_size)
         retrieval_index_dict = data_manager.init_from_local_file(
-            sample_id_name=self._sample_id_name, columns=df.columns.tolist(), match_id_list=self._match_id_list,
-            match_id_name=self._match_id_name, label_name=self._label_name, weight_name=self._weight_name,
-            label_type=self._label_type, weight_type=self._weight_type,
-            dtype=self._dtype, default_type=types.DEFAULT_DATA_TYPE)
+            sample_id_name=self._sample_id_name,
+            columns=df.columns.tolist(),
+            match_id_list=self._match_id_list,
+            match_id_name=self._match_id_name,
+            label_name=self._label_name,
+            weight_name=self._weight_name,
+            label_type=self._label_type,
+            weight_type=self._weight_type,
+            dtype=self._dtype,
+            default_type=types.DEFAULT_DATA_TYPE,
+        )
 
         site_name = ctx.local.name
         local_role = ctx.local.party[0]
 
         if local_role != "local":
             data_manager.fill_anonymous_site_name(site_name=site_name)
 
         buf = zip(df.index.tolist(), df.values.tolist())
-        table = ctx.computing.parallelize(
-            buf, include_key=True, partition=self._partition
-        )
+        table = ctx.computing.parallelize(buf, include_key=True, partition=self._partition)
 
         from .ops._indexer import get_partition_order_by_raw_table
+
         partition_order_mappings = get_partition_order_by_raw_table(table, data_manager.block_row_size)
         # partition_order_mappings = _get_partition_order(table)
-        to_block_func = functools.partial(_to_blocks,
-                                          data_manager=data_manager,
-                                          retrieval_index_dict=retrieval_index_dict,
-                                          partition_order_mappings=partition_order_mappings)
-
-        block_table = table.mapPartitions(
-            to_block_func,
-            use_previous_behavior=False
+        to_block_func = functools.partial(
+            _to_blocks,
+            data_manager=data_manager,
+            retrieval_index_dict=retrieval_index_dict,
+            partition_order_mappings=partition_order_mappings,
         )
 
-        return DataFrame(ctx=ctx,
-                         block_table=block_table,
-                         partition_order_mappings=partition_order_mappings,
-                         data_manager=data_manager)
+        block_table = table.mapPartitions(to_block_func, use_previous_behavior=False)
+
+        return DataFrame(
+            ctx=ctx,
+            block_table=block_table,
+            partition_order_mappings=partition_order_mappings,
+            data_manager=data_manager,
+        )
 
 
-def _to_blocks(kvs,
-               data_manager=None,
-               retrieval_index_dict=None,
-               partition_order_mappings=None,
-               na_values=None):
+def _to_blocks(kvs, data_manager=None, retrieval_index_dict=None, partition_order_mappings=None, na_values=None):
     """
     sample_id/match_id,label(maybe missing),weight(maybe missing),X
     """
     block_id = None
 
     schema = data_manager.schema
 
     splits = [[] for _ in range(data_manager.block_num)]
-    sample_id_block = data_manager.loc_block(schema.sample_id_name, with_offset=False) if schema.sample_id_name else None
+    sample_id_block = (
+        data_manager.loc_block(schema.sample_id_name, with_offset=False) if schema.sample_id_name else None
+    )
 
-    match_id_block = data_manager.loc_block(schema.match_id_name, with_offset=False)if schema.match_id_name else None
+    match_id_block = data_manager.loc_block(schema.match_id_name, with_offset=False) if schema.match_id_name else None
     match_id_column_index = retrieval_index_dict["match_id_index"]
 
     label_block = data_manager.loc_block(schema.label_name, with_offset=False) if schema.label_name else None
     label_column_index = retrieval_index_dict["label_index"]
 
     weight_block = data_manager.loc_block(schema.weight_name, with_offset=False) if schema.weight_name else None
     weight_column_index = retrieval_index_dict["weight_index"]
@@ -313,14 +331,14 @@
             splits[weight_block].append([value[weight_column_index]])
 
         for bid, col_id_list in column_blocks_mapping.items():
             splits[bid].append([value[col_id] for col_id in col_id_list])
 
         if lid % block_row_size == 0:
             converted_blocks = data_manager.convert_to_blocks(splits)
-            yield  block_id, converted_blocks
+            yield block_id, converted_blocks
             block_id += 1
             splits = [[] for _ in range(data_manager.block_num)]
 
     if lid % block_row_size:
         converted_blocks = data_manager.convert_to_blocks(splits)
         yield block_id, converted_blocks
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/conf/default_config.py` & `pyfate-2.1.0/fate/arch/dataframe/conf/default_config.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/io/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/io/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/io/_json_schema.py` & `pyfate-2.1.0/fate/arch/dataframe/io/_json_schema.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/io/_json_serialization.py` & `pyfate-2.1.0/fate/arch/dataframe/io/_json_serialization.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/manager/block_manager.py` & `pyfate-2.1.0/fate/arch/dataframe/manager/block_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -97,17 +97,17 @@
             return BlockType(data_type)
         elif isinstance(data_type, (bool, np.bool_)) or data_type == torch.bool:
             return BlockType.bool
         elif isinstance(data_type, np.int64) or data_type == torch.int64:
             return BlockType.int64
         elif isinstance(data_type, (int, np.int32)) or data_type == torch.int32:
             return BlockType.int32
-        elif isinstance(data_type, np.float64) or data_type == torch.float64:
+        elif isinstance(data_type, (float, np.float64)) or data_type == torch.float64:
             return BlockType.float64
-        elif isinstance(data_type, (float, np.float32)) or data_type == torch.float32:
+        elif isinstance(data_type, np.float32) or data_type == torch.float32:
             return BlockType.float32
         else:
             return BlockType.np_object
 
     @staticmethod
     def is_tensor(block_type):
         return block_type in [BlockType.bool, BlockType.int32, BlockType.int64, BlockType.float32, BlockType.float64]
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/manager/data_manager.py` & `pyfate-2.1.0/fate/arch/dataframe/manager/data_manager.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,18 +21,18 @@
 from ..entity import types
 from typing import Union, List, Tuple
 from ..conf.default_config import DATAFRAME_BLOCK_ROW_SIZE
 
 
 class DataManager(object):
     def __init__(
-            self,
-            schema_manager: SchemaManager = None,
-            block_manager: BlockManager = None,
-            block_row_size: int = DATAFRAME_BLOCK_ROW_SIZE
+        self,
+        schema_manager: SchemaManager = None,
+        block_manager: BlockManager = None,
+        block_row_size: int = DATAFRAME_BLOCK_ROW_SIZE,
     ):
         self._schema_manager = schema_manager
         self._block_manager = block_manager
         self._block_row_size = block_row_size
 
     @property
     def blocks(self):
@@ -83,32 +83,41 @@
     def split_columns(self, columns: List[str], block_types: Union["BlockType", List["BlockType"]]):
         field_indexes = self._schema_manager.split_columns(columns, block_types)
         narrow_blocks, dst_blocks = self._block_manager.split_fields(field_indexes, block_types)
 
         return narrow_blocks, dst_blocks
 
     def duplicate(self) -> "DataManager":
-        return DataManager(
-            self._schema_manager.duplicate(),
-            self._block_manager.duplicate()
-        )
+        return DataManager(self._schema_manager.duplicate(), self._block_manager.duplicate())
 
-    def init_from_local_file(self, sample_id_name, columns, match_id_list, match_id_name, label_name, weight_name,
-                             label_type, weight_type, dtype, default_type=types.DEFAULT_DATA_TYPE,
-                             anonymous_site_name=None):
+    def init_from_local_file(
+        self,
+        sample_id_name,
+        columns,
+        match_id_list,
+        match_id_name,
+        label_name,
+        weight_name,
+        label_type,
+        weight_type,
+        dtype,
+        default_type=types.DEFAULT_DATA_TYPE,
+        anonymous_site_name=None,
+    ):
         schema_manager = SchemaManager()
-        retrieval_index_dict = schema_manager.parse_local_file_schema(sample_id_name,
-                                                                      columns,
-                                                                      match_id_list,
-                                                                      match_id_name,
-                                                                      label_name,
-                                                                      weight_name,
-                                                                      anonymous_site_name=anonymous_site_name)
-        schema_manager.init_field_types(label_type, weight_type, dtype,
-                                        default_type=default_type)
+        retrieval_index_dict = schema_manager.parse_local_file_schema(
+            sample_id_name,
+            columns,
+            match_id_list,
+            match_id_name,
+            label_name,
+            weight_name,
+            anonymous_site_name=anonymous_site_name,
+        )
+        schema_manager.init_field_types(label_type, weight_type, dtype, default_type=default_type)
         block_manager = BlockManager()
         block_manager.initialize_blocks(schema_manager)
 
         self._schema_manager = schema_manager
         self._block_manager = block_manager
 
         return retrieval_index_dict
@@ -116,27 +125,27 @@
     def convert_to_blocks(self, splits):
         converted_blocks = []
         for bid, block in enumerate(self._block_manager.blocks):
             converted_blocks.append(block.convert_block(splits[bid]))
 
         return converted_blocks
 
-    def derive_new_data_manager(self, with_sample_id, with_match_id, with_label, with_weight, columns) \
-            -> Tuple["DataManager", List[Tuple[int, int, bool, List]]]:
-        schema_manager, derive_indexes = self._schema_manager.derive_new_schema_manager(with_sample_id=with_sample_id,
-                                                                                        with_match_id=with_match_id,
-                                                                                        with_label=with_label,
-                                                                                        with_weight=with_weight,
-                                                                                        columns=columns)
+    def derive_new_data_manager(
+        self, with_sample_id, with_match_id, with_label, with_weight, columns
+    ) -> Tuple["DataManager", List[Tuple[int, int, bool, List]]]:
+        schema_manager, derive_indexes = self._schema_manager.derive_new_schema_manager(
+            with_sample_id=with_sample_id,
+            with_match_id=with_match_id,
+            with_label=with_label,
+            with_weight=with_weight,
+            columns=columns,
+        )
         block_manager, blocks_loc = self._block_manager.derive_new_block_manager(derive_indexes)
 
-        return DataManager(
-            schema_manager=schema_manager,
-            block_manager=block_manager
-        ), blocks_loc
+        return DataManager(schema_manager=schema_manager, block_manager=block_manager), blocks_loc
 
     def loc_block(self, name: Union[str, List[str]], with_offset=True):
         if isinstance(name, str):
             field_index = self._schema_manager.get_field_offset(name)
             return self._block_manager.loc_block(field_index, with_offset)
         else:
             loc_ret = []
@@ -178,61 +187,60 @@
 
         return ret_fields_loc
 
     def get_field_name(self, field_index):
         return self._schema_manager.get_field_name(field_index)
 
     def get_field_name_list(self, with_sample_id=True, with_match_id=True, with_label=True, with_weight=True):
-        return self._schema_manager.get_field_name_list(with_sample_id=with_sample_id,
-                                                        with_match_id=with_match_id,
-                                                        with_label=with_label,
-                                                        with_weight=with_weight)
+        return self._schema_manager.get_field_name_list(
+            with_sample_id=with_sample_id, with_match_id=with_match_id, with_label=with_label, with_weight=with_weight
+        )
 
     def get_field_type_by_name(self, name):
         return self._schema_manager.get_field_types(name)
 
     def get_field_offset(self, name):
         return self._schema_manager.get_field_offset(name)
 
     def get_block(self, block_id):
         return self._block_manager.blocks[block_id]
 
     def infer_operable_blocks(self):
         operable_field_offsets = self._schema_manager.infer_operable_filed_offsets()
-        block_index_set = set(self._block_manager.loc_block(offset, with_offset=False) for offset in operable_field_offsets)
+        block_index_set = set(
+            self._block_manager.loc_block(offset, with_offset=False) for offset in operable_field_offsets
+        )
         return sorted(list(block_index_set))
 
     def infer_operable_field_names(self):
         return self._schema_manager.infer_operable_field_names()
 
     def infer_non_operable_blocks(self):
         non_operable_field_offsets = self._schema_manager.infer_non_operable_field_offsets()
-        block_index_set = set(self._block_manager.loc_block(offset, with_offset=False) for offset in non_operable_field_offsets)
+        block_index_set = set(
+            self._block_manager.loc_block(offset, with_offset=False) for offset in non_operable_field_offsets
+        )
         return sorted(list(block_index_set))
 
-    def try_to_promote_types(self,
-                             block_indexes: List[int],
-                             block_type: Union[bool, list, int, float, np.dtype, BlockType]) -> List[Tuple[int, BlockType]]:
+    def try_to_promote_types(
+        self, block_indexes: List[int], block_type: Union[bool, list, int, float, np.dtype, BlockType]
+    ) -> List[Tuple[int, BlockType]]:
         promote_types = []
         if isinstance(block_type, (bool, int, float, np.dtype)):
             block_type = BlockType.get_block_type(block_type)
 
         if isinstance(block_type, BlockType):
             for idx, bid in enumerate(block_indexes):
                 if self.get_block(bid).block_type < block_type:
-                    promote_types.append(
-                        (bid, block_type)
-                    )
+                    promote_types.append((bid, block_type))
         else:
             for idx, (bid, r_type) in enumerate(zip(block_indexes, block_type)):
                 block_type = BlockType.get_block_type(r_type)
                 if self.get_block(bid).block_type < block_type:
-                    promote_types.append(
-                        (bid, block_type)
-                    )
+                    promote_types.append((bid, block_type))
 
         return promote_types
 
     def promote_types(self, to_promote_blocks: list):
         for bid, block_type in to_promote_blocks:
             self._block_manager.blocks[bid] = self._block_manager.blocks[bid].convert_block_type(block_type)
             for field_index in self._block_manager.blocks[bid].field_indexes:
@@ -242,19 +250,21 @@
         new_blocks, to_compress_block_loc, non_compress_block_changes = self._block_manager.compress()
         if to_compress_block_loc:
             self._block_manager.reset_blocks(new_blocks)
 
         return to_compress_block_loc, non_compress_block_changes
 
     def rename(self, sample_id_name=None, match_id_name=None, label_name=None, weight_name=None, columns: dict = None):
-        self._schema_manager.rename(sample_id_name=sample_id_name,
-                                    match_id_name=match_id_name,
-                                    label_name=label_name,
-                                    weight_name=weight_name,
-                                    columns=columns)
+        self._schema_manager.rename(
+            sample_id_name=sample_id_name,
+            match_id_name=match_id_name,
+            label_name=label_name,
+            weight_name=weight_name,
+            columns=columns,
+        )
 
     def serialize(self):
         schema_serialization = self._schema_manager.serialize()
         fields = schema_serialization["fields"]
         for col_id, field in enumerate(fields):
             block_id = self._block_manager.loc_block(col_id, with_offset=False)
             should_compress = self._block_manager.blocks[block_id].should_compress
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/manager/schema_manager.py` & `pyfate-2.1.0/fate/arch/dataframe/manager/schema_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,15 +30,15 @@
         match_id_name=None,
         weight_name=None,
         label_name=None,
         columns: Union[list, pd.Index] = None,
         anonymous_label_name=None,
         anonymous_weight_name=None,
         anonymous_columns: Union[list, pd.Index] = None,
-        anonymous_summary: dict = None
+        anonymous_summary: dict = None,
     ):
         self._sample_id_name = sample_id_name
         self._match_id_name = match_id_name
         self._weight_name = weight_name
         self._label_name = label_name
         self._columns = pd.Index(columns) if columns else pd.Index([])
         self._anonymous_label_name = anonymous_label_name
@@ -128,30 +128,32 @@
         self._anonymous_summary = anonymous_summary
 
     def append_columns(self, names):
         self._columns = self._columns.append(pd.Index(names))
         # TODO: extend anonymous column
         anonymous_generator = AnonymousGenerator(site_name=self._anonymous_summary["site_name"])
 
-        anonymous_columns, anonymous_summary = anonymous_generator.add_anonymous_columns(names, self._anonymous_summary)
+        anonymous_columns, anonymous_summary = anonymous_generator.add_anonymous_columns(
+            names, self._anonymous_summary
+        )
         self._anonymous_columns = self._anonymous_columns.append(pd.Index(anonymous_columns))
         self._anonymous_summary = anonymous_summary
 
     def init_anonymous_names(self, anonymous_site_name):
         anonymous_generator = AnonymousGenerator(anonymous_site_name)
         anonymous_ret_dict = anonymous_generator.generate_anonymous_names(self)
         self._set_anonymous_info_by_dict(anonymous_ret_dict)
 
     def fill_anonymous_site_name(self, anonymous_site_name):
         anonymous_generator = AnonymousGenerator(anonymous_site_name)
         anonymous_ret_dict = anonymous_generator.fill_anonymous_site_name(
             anonymous_label_name=self.anonymous_label_name,
             anonymous_weight_name=self._anonymous_weight_name,
             anonymous_columns=self._anonymous_columns,
-            anonymous_summary=self._anonymous_summary
+            anonymous_summary=self._anonymous_summary,
         )
 
         self._set_anonymous_info_by_dict(anonymous_ret_dict)
 
     def _set_anonymous_info_by_dict(self, anonymous_ret_dict):
         if self._label_name:
             self._anonymous_label_name = anonymous_ret_dict["anonymous_label_name"]
@@ -176,54 +178,39 @@
             if name not in names:
                 columns.append(name)
         self._columns = pd.Index(columns)
 
         # TODO: pop anonymous columns
 
     def __eq__(self, other: "Schema"):
-        return self.label_name == other.label_name and self.weight_name == other.weight_name \
-               and self.sample_id_name == other.sample_id_name and self.match_id_name == other.match_id_name \
-               and self.columns.tolist() == other.columns.tolist()
+        return (
+            self.label_name == other.label_name
+            and self.weight_name == other.weight_name
+            and self.sample_id_name == other.sample_id_name
+            and self.match_id_name == other.match_id_name
+            and self.columns.tolist() == other.columns.tolist()
+        )
 
     def serialize(self):
         s_obj = list()
-        s_obj.append(
-            dict(name=self._sample_id_name,
-                 property="sample_id")
-        )
+        s_obj.append(dict(name=self._sample_id_name, property="sample_id"))
 
         if self._match_id_name:
-            s_obj.append(
-                dict(name=self._match_id_name,
-                     property="match_id")
-            )
+            s_obj.append(dict(name=self._match_id_name, property="match_id"))
 
         if self._label_name:
-            s_obj.append(
-                dict(name=self._label_name,
-                     anonymous_name=self._anonymous_label_name,
-                     property="label")
-            )
+            s_obj.append(dict(name=self._label_name, anonymous_name=self._anonymous_label_name, property="label"))
         if self._weight_name:
-            s_obj.append(
-                dict(name=self._weight_name,
-                     anonymous_name=self._anonymous_weight_name,
-                     property="weight")
-            )
+            s_obj.append(dict(name=self._weight_name, anonymous_name=self._anonymous_weight_name, property="weight"))
 
         if len(self._columns):
             for name, anonymous_name in zip(self._columns, self._anonymous_columns):
-                s_obj.append(
-                    dict(name=name,
-                         anonymous_name=anonymous_name,
-                         property="column")
-                    )
+                s_obj.append(dict(name=name, anonymous_name=anonymous_name, property="column"))
 
-        return dict(fields=s_obj,
-                    anonymous_summary=self._anonymous_summary)
+        return dict(fields=s_obj, anonymous_summary=self._anonymous_summary)
 
 
 class SchemaManager(object):
     def __init__(self):
         self._schema = None
         self._type_mapping = dict()
         self._name_offset_mapping = dict()
@@ -238,15 +225,15 @@
         self._schema = schema
 
     def rename(self, sample_id_name=None, match_id_name=None, label_name=None, weight_name=None, columns: dict = None):
         attr_dict = {
             "sample_id_name": sample_id_name,
             "match_id_name": match_id_name,
             "label_name": label_name,
-            "weight_name": weight_name
+            "weight_name": weight_name,
         }
 
         for attr, value in attr_dict.items():
             if not value:
                 continue
             o_name = getattr(self._schema, attr)
             setattr(self._schema, attr, value)
@@ -360,16 +347,17 @@
         dup_schema_manager._offset_name_mapping = copy.deepcopy(self._offset_name_mapping)
 
         return dup_schema_manager
 
     def get_all_keys(self):
         return list(self._name_offset_mapping.keys())
 
-    def parse_local_file_schema(self, sample_id_name, columns, match_id_list, match_id_name, label_name, weight_name,
-                                anonymous_site_name=None):
+    def parse_local_file_schema(
+        self, sample_id_name, columns, match_id_list, match_id_name, label_name, weight_name, anonymous_site_name=None
+    ):
         column_indexes = list(range(len(columns)))
 
         match_id_index, label_index, weight_index = None, None, None
         if match_id_list:
             if match_id_name and match_id_name not in match_id_list:
                 raise ValueError(f"{match_id_name} not exist match_id_list={match_id_list}")
             if not match_id_name and len(match_id_list) > 1:
@@ -393,25 +381,25 @@
             column_indexes.pop(idx)
 
         self._schema = Schema(
             sample_id_name=sample_id_name,
             match_id_name=match_id_name,
             weight_name=weight_name,
             label_name=label_name,
-            columns=columns
+            columns=columns,
         )
 
         self._schema.init_anonymous_names(anonymous_site_name)
         self.init_name_mapping()
 
         return dict(
             match_id_index=match_id_index,
             label_index=label_index,
             weight_index=weight_index,
-            column_indexes=column_indexes
+            column_indexes=column_indexes,
         )
 
     def fill_anonymous_site_name(self, anonymous_site_name):
         self._schema.fill_anonymous_site_name(anonymous_site_name)
 
     @staticmethod
     def extract_column_index_by_name(columns, column_indexes, name, drop=True):
@@ -422,16 +410,23 @@
                 columns.pop(idx)
                 column_indexes.pop(idx)
 
             return ret
         except ValueError:
             raise ValueError(f"{name} does not exist in {columns}")
 
-    def init_field_types(self, label_type="float32", weight_type="float32", dtype="float32",
-                         default_type="float32", match_id_type="index", sample_id_type="index"):
+    def init_field_types(
+        self,
+        label_type="float32",
+        weight_type="float32",
+        dtype="float32",
+        default_type="float32",
+        match_id_type="index",
+        sample_id_type="index",
+    ):
         self._type_mapping[self._schema.sample_id_name] = "index"
 
         if self._schema.match_id_name:
             self._type_mapping[self._schema.match_id_name] = "index"
 
         if self._schema.label_name:
             self._type_mapping[self._schema.label_name] = label_type
@@ -514,16 +509,22 @@
         else:
             return self._type_mapping[name]
 
     def set_field_type_by_offset(self, field_index, field_type):
         name = self._offset_name_mapping[field_index]
         self._type_mapping[name] = field_type
 
-    def derive_new_schema_manager(self, with_sample_id=True, with_match_id=True,
-                                  with_label=True, with_weight=True, columns: Union[str, list] = None):
+    def derive_new_schema_manager(
+        self,
+        with_sample_id=True,
+        with_match_id=True,
+        with_label=True,
+        with_weight=True,
+        columns: Union[str, list] = None,
+    ):
         derived_schema_manager = SchemaManager()
         derived_schema = Schema()
 
         indexes = []
 
         derived_schema.anonymous_summary = self._schema.anonymous_summary
         if with_sample_id:
@@ -621,19 +622,15 @@
         schema_serialization["fields"] = schema_serialization_with_type
         return schema_serialization
 
     @classmethod
     def deserialize(cls, schema_meta: dict):
         schema_manager = SchemaManager()
 
-        schema_dict = dict(
-            columns=[],
-            anonymous_columns=[],
-            anonymous_summary=schema_meta["anonymous_summary"]
-        )
+        schema_dict = dict(columns=[], anonymous_columns=[], anonymous_summary=schema_meta["anonymous_summary"])
         type_dict = {}
         for field in schema_meta["fields"]:
             p = field["property"]
             name = field["name"]
             if field["property"] in ["sample_id", "match_id", "weight", "label"]:
                 schema_dict[f"{p}_name"] = name
                 type_dict[f"{p}_type"] = field["dtype"]
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/manager/utils/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/manager/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/manager/utils/_anonymous_generator.py` & `pyfate-2.1.0/fate/arch/dataframe/manager/utils/_anonymous_generator.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,30 +35,29 @@
             return SPLICES.join([DEFAULT_SITE_NAME, suf])
 
     def generate_anonymous_names(self, schema):
         column_len = len(schema.columns.tolist())
         anonymous_label_name = None
         anonymous_weight_name = None
 
-        anonymous_columns = [self._generate_anonymous_column(
-            ANONYMOUS_COLUMN_PREFIX + str(i)) for i in range(column_len)]
+        anonymous_columns = [
+            self._generate_anonymous_column(ANONYMOUS_COLUMN_PREFIX + str(i)) for i in range(column_len)
+        ]
 
         if schema.label_name:
             anonymous_label_name = self._generate_anonymous_column(ANONYMOUS_LABEL)
 
         if schema.weight_name:
             anonymous_weight_name = self._generate_anonymous_column(ANONYMOUS_WEIGHT)
 
         return dict(
             anonymous_label_name=anonymous_label_name,
             anonymous_weight_name=anonymous_weight_name,
             anonymous_columns=anonymous_columns,
-            anonymous_summary=dict(column_len=column_len,
-                                   site_name=self._site_name
-                                   )
+            anonymous_summary=dict(column_len=column_len, site_name=self._site_name),
         )
 
     def _check_site_name_consistency(self, anonymous_summary):
         anonymous_site_name = anonymous_summary["site_name"]
 
         if anonymous_site_name and self._site_name is not None and anonymous_site_name != self._site_name:
             raise ValueError(f"previous_site_name={anonymous_site_name} != current_site_name={self._site_name}")
@@ -70,37 +69,39 @@
         return self._generate_anonymous_column(ANONYMOUS_WEIGHT)
 
     def add_anonymous_columns(self, columns, anonymous_summary: dict):
         self._check_site_name_consistency(anonymous_summary)
         anonymous_summary = copy.deepcopy(anonymous_summary)
 
         column_len = anonymous_summary["column_len"]
-        anonymous_columns = [self._generate_anonymous_column(ANONYMOUS_COLUMN_PREFIX + str(i + column_len))
-                             for i in range(len(columns))]
+        anonymous_columns = [
+            self._generate_anonymous_column(ANONYMOUS_COLUMN_PREFIX + str(i + column_len)) for i in range(len(columns))
+        ]
 
         anonymous_summary["column_len"] = column_len + len(columns)
         return anonymous_columns, anonymous_summary
 
-    def fill_anonymous_site_name(self, anonymous_label_name, anonymous_weight_name,
-                        anonymous_columns, anonymous_summary):
+    def fill_anonymous_site_name(
+        self, anonymous_label_name, anonymous_weight_name, anonymous_columns, anonymous_summary
+    ):
         anonymous_summary = copy.deepcopy(anonymous_summary)
 
         self._check_site_name_consistency(anonymous_summary)
 
         if anonymous_summary["site_name"] is None:
             anonymous_label_name = self._fill_site_name(anonymous_label_name)
             anonymous_weight_name = self._fill_site_name(anonymous_weight_name)
             anonymous_columns = self._fill_site_name(anonymous_columns)
             anonymous_summary["site_name"] = self._site_name
 
         return dict(
             anonymous_label_name=anonymous_label_name,
             anonymous_weight_name=anonymous_weight_name,
             anonymous_columns=anonymous_columns,
-            anonymous_summary=anonymous_summary
+            anonymous_summary=anonymous_summary,
         )
 
     def _fill_site_name(self, name):
         if name is None:
             return name
 
         if isinstance(name, str):
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/__init__.py` & `pyfate-2.1.0/fate/arch/computing/backends/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_activation.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_activation.py`

 * *Files 3% similar despite different names*

```diff
@@ -39,13 +39,8 @@
 
         return ret_blocks
 
     _sigmoid_func = functools.partial(_sigmoid, op_blocks=operable_blocks, reserved_blocks=non_operable_blocks)
 
     block_table = df.block_table.mapValues(_sigmoid_func)
 
-    return DataFrame(
-        df._ctx,
-        block_table,
-        df.partition_order_mappings,
-        data_manager
-    )
+    return DataFrame(df._ctx, block_table, df.partition_order_mappings, data_manager)
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_apply_row.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_apply_row.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,75 +20,86 @@
 
 from .._dataframe import DataFrame
 from ..manager.block_manager import Block, BlockType
 from ..manager.data_manager import DataManager
 from ..utils._auto_column_name_generated import generated_default_column_names
 
 
-def apply_row(df: "DataFrame", func,
-              columns: list=None, with_label=False, with_weight=False,
-              enable_type_align_checking=True) -> "DataFrame":
+def apply_row(
+    df: "DataFrame", func, columns: list = None, with_label=False, with_weight=False, enable_type_align_checking=True
+) -> "DataFrame":
     """
     In current version, assume that the apply_row results' lengths are equal
     """
     data_manager = df.data_manager
-    dst_data_manager, _ = data_manager.derive_new_data_manager(with_sample_id=True,
-                                                               with_match_id=True,
-                                                               with_label=not with_weight,
-                                                               with_weight=not with_weight,
-                                                               columns=None)
+    dst_data_manager, _ = data_manager.derive_new_data_manager(
+        with_sample_id=True, with_match_id=True, with_label=not with_label, with_weight=not with_weight, columns=None
+    )
 
     non_operable_field_names = dst_data_manager.get_field_name_list()
-    non_operable_blocks = [data_manager.loc_block(field_name,
-                                                  with_offset=False) for field_name in non_operable_field_names]
-    operable_blocks = data_manager.infer_operable_blocks()
+    non_operable_blocks = [
+        data_manager.loc_block(field_name, with_offset=False) for field_name in non_operable_field_names
+    ]
+    fields_loc = data_manager.get_fields_loc(
+        with_sample_id=False, with_match_id=False, with_label=with_label, with_weight=with_weight
+    )
+
+    fields_name = data_manager.get_field_name_list(
+        with_sample_id=False, with_match_id=False, with_label=with_label, with_weight=with_weight
+    )
+
+    operable_blocks = sorted(list(set(data_manager.loc_block(fields_name, with_offset=False))))
     is_numeric = True
     for bid in operable_blocks:
         if not data_manager.get_block(bid).is_numeric():
             is_numeric = False
             break
     block_column_in_orders = list()
     if is_numeric:
         for bid in operable_blocks:
             field_indexes = data_manager.get_block(bid).field_indexes
             block_column_in_orders.extend([data_manager.get_field_name(field_index) for field_index in field_indexes])
 
-    fields_loc = data_manager.get_fields_loc(with_sample_id=False, with_match_id=False,
-                                             with_label=with_label, with_weight=with_weight)
-
-    fields_name = data_manager.get_field_name_list(with_sample_id=False,
-                                                   with_match_id=False,
-                                                   with_label=with_label,
-                                                   with_weight=with_weight)
-
-    _apply_func = functools.partial(_apply, func=func, src_operable_blocks=operable_blocks, src_field_names=fields_name,
-                                    src_fields_loc=fields_loc, src_non_operable_blocks=non_operable_blocks,
-                                    ret_columns=columns, dst_dm=dst_data_manager, is_numeric=is_numeric,
-                                    need_shuffle=True if block_column_in_orders == fields_name else False,
-                                    block_column_in_orders=block_column_in_orders,
-                                    enable_type_align_checking=enable_type_align_checking)
+    _apply_func = functools.partial(
+        _apply,
+        func=func,
+        src_operable_blocks=operable_blocks,
+        src_field_names=fields_name,
+        src_fields_loc=fields_loc,
+        src_non_operable_blocks=non_operable_blocks,
+        ret_columns=columns,
+        dst_dm=dst_data_manager,
+        is_numeric=is_numeric,
+        need_shuffle=True if block_column_in_orders == fields_name else False,
+        block_column_in_orders=block_column_in_orders,
+        enable_type_align_checking=enable_type_align_checking,
+    )
 
     dst_block_table_with_dm = df.block_table.mapValues(_apply_func)
 
     dst_data_manager = dst_block_table_with_dm.first()[1][1]
     dst_block_table = dst_block_table_with_dm.mapValues(lambda blocks_with_dm: blocks_with_dm[0])
 
-    return DataFrame(
-        df._ctx,
-        dst_block_table,
-        df.partition_order_mappings,
-        dst_data_manager
-    )
+    return DataFrame(df._ctx, dst_block_table, df.partition_order_mappings, dst_data_manager)
 
 
-def _apply(blocks, func=None, src_operable_blocks=None, src_field_names=None,
-           src_fields_loc=None, src_non_operable_blocks=None, ret_columns=None,
-           dst_dm: "DataManager"=None, is_numeric=True,
-           block_column_in_orders=None,
-           need_shuffle=False, enable_type_align_checking=True):
+def _apply(
+    blocks,
+    func=None,
+    src_operable_blocks=None,
+    src_field_names=None,
+    src_fields_loc=None,
+    src_non_operable_blocks=None,
+    ret_columns=None,
+    dst_dm: "DataManager" = None,
+    is_numeric=True,
+    block_column_in_orders=None,
+    need_shuffle=False,
+    enable_type_align_checking=True,
+):
     dm = dst_dm.duplicate()
     apply_blocks = []
 
     if is_numeric:
         apply_data = []
         for bid in src_operable_blocks:
             apply_data.append(blocks[bid])
@@ -107,55 +118,58 @@
         apply_data = pd.DataFrame(apply_data, columns=src_field_names)
 
     apply_ret = apply_data.apply(lambda row: func(row), axis=1).values.tolist()
 
     if isinstance(apply_ret[0], Iterable):
         first_row = list(apply_ret[0])
         ret_column_len = len(first_row)
-        block_types = [BlockType.np_object if BlockType.is_arr(value) else BlockType.get_block_type(value) for value in first_row]
+        block_types = [
+            BlockType.np_object if BlockType.is_arr(value) else BlockType.get_block_type(value) for value in first_row
+        ]
         apply_blocks = [[] for _ in range(ret_column_len)]
         for ret in apply_ret:
             for idx, value in enumerate(ret):
                 apply_blocks[idx].append([value])
 
                 if enable_type_align_checking:
                     block_type = BlockType.np_object if BlockType.is_arr(value) else BlockType.get_block_type(value)
                     if block_types[idx] < block_type:
                         block_types[idx] = block_type
     else:
-        block_types = [BlockType.np_object if BlockType.is_arr(apply_ret[0]) else BlockType.get_block_type(apply_ret[0])]
+        block_types = [
+            BlockType.np_object if BlockType.is_arr(apply_ret[0]) else BlockType.get_block_type(apply_ret[0])
+        ]
         apply_blocks.append([[ret] for ret in apply_ret])
         ret_column_len = 1
 
         if enable_type_align_checking:
             for ret in apply_ret:
                 block_type = BlockType.np_object if BlockType.is_arr(ret) else BlockType.get_block_type(ret)
                 if block_types[0] < block_type:
                     block_types[0] = block_type
 
-
     if not ret_columns:
         ret_columns = generated_default_column_names(ret_column_len)
 
-    block_indexes = dm.append_columns(
-        ret_columns, block_types
-    )
+    block_indexes = dm.append_columns(ret_columns, block_types)
 
     ret_blocks = [[] for _ in range(len(src_non_operable_blocks) + ret_column_len)]
     for idx, bid in enumerate(src_non_operable_blocks):
         ret_blocks[idx] = blocks[bid]
 
     for idx, bid in enumerate(block_indexes):
         if dm.blocks[bid].is_phe_tensor():
             single_value = apply_blocks[idx][0][0]
-            dm.blocks[bid].set_extra_kwargs(pk=single_value.pk,
-                                            evaluator=single_value.evaluator,
-                                            coder=single_value.coder,
-                                            dtype=single_value.dtype,
-                                            device=single_value.device)
+            dm.blocks[bid].set_extra_kwargs(
+                pk=single_value.pk,
+                evaluator=single_value.evaluator,
+                coder=single_value.coder,
+                dtype=single_value.dtype,
+                device=single_value.device,
+            )
             ret = [v[0]._data for v in apply_blocks[idx]]
             ret_blocks[bid] = dm.blocks[bid].convert_block(ret)
             # ret_blocks[bid] = dm.blocks[bid].convert_to_phe_tensor(ret, shape=(len(ret), 1))
         else:
             ret_blocks[bid] = dm.blocks[bid].convert_block(apply_blocks[idx])
 
     return ret_blocks, dm
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_arithmetic.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_arithmetic.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 import numpy as np
 import pandas as pd
-from fate.arch.computing import is_table
+from fate.arch.computing.api import is_table
 from .._dataframe import DataFrame
 from ._promote_types import promote_types
 from .utils.series_align import series_to_ndarray
 from .utils.operators import binary_operate
 
 
 def arith_operate(lhs: DataFrame, rhs, op) -> "DataFrame":
@@ -29,16 +29,17 @@
     if isinstance(rhs, DataFrame):
         rhs_column_names = rhs.data_manager.infer_operable_field_names()
         if len(column_names) != len(rhs_column_names) or len(column_names) > 1:
             raise ValueError(f"Operation={op} of two dataframe should have same column length=1")
 
         rhs_block_id = rhs.data_manager.infer_operable_blocks()[0]
         block_table = _operate(lhs.block_table, rhs.block_table, op, block_indexes, rhs_block_id)
-        to_promote_blocks = data_manager.try_to_promote_types(block_indexes,
-                                                              rhs.data_manager.get_block(rhs_block_id).block_type)
+        to_promote_blocks = data_manager.try_to_promote_types(
+            block_indexes, rhs.data_manager.get_block(rhs_block_id).block_type
+        )
     elif isinstance(rhs, (np.ndarray, list, pd.Series)):
         if isinstance(rhs, pd.Series):
             rhs = series_to_ndarray(rhs, column_names)
         if isinstance(rhs, list):
             rhs = np.array(rhs)
         if len(rhs.shape) > 2:
             raise ValueError("NdArray's Dimension should <= 2")
@@ -62,48 +63,36 @@
         to_promote_blocks = data_manager.try_to_promote_types(block_indexes, rhs)
     else:
         raise ValueError(f"Operation={op} between dataframe and {type(rhs)} is not implemented")
 
     if to_promote_blocks:
         block_table, data_manager = promote_types(block_table, data_manager, to_promote_blocks)
 
-    return type(lhs) (
-        lhs._ctx,
-        block_table,
-        lhs.partition_order_mappings,
-        data_manager
-    )
+    return type(lhs)(lhs._ctx, block_table, lhs.partition_order_mappings, data_manager)
 
 
 def _operate(lhs, rhs, op, block_indexes, rhs_block_id=None):
     block_index_set = set(block_indexes)
     if isinstance(rhs, list):
         op_ret = lhs.mapValues(
-            lambda blocks:
-            [
-                op(blocks[bid], rhs[bid]) if bid in block_index_set
-                                          else blocks[bid]
-                for bid in range(len(blocks))
+            lambda blocks: [
+                op(blocks[bid], rhs[bid]) if bid in block_index_set else blocks[bid] for bid in range(len(blocks))
             ]
         )
     elif isinstance(rhs, (bool, int, float, np.int32, np.float32, np.int64, np.float64, np.bool_)):
         op_ret = lhs.mapValues(
-            lambda blocks:
-            [
-                op(blocks[bid], rhs) if bid in block_index_set
-                                     else blocks[bid]
-                for bid in range(len(blocks))
-             ]
+            lambda blocks: [
+                op(blocks[bid], rhs) if bid in block_index_set else blocks[bid] for bid in range(len(blocks))
+            ]
         )
     elif is_table(rhs):
-        op_ret = lhs.join(rhs,
-            lambda blocks1, blocks2:
-            [
-                op(blocks1[bid], blocks2[rhs_block_id]) if bid in block_index_set
-                                     else blocks1[bid]
+        op_ret = lhs.join(
+            rhs,
+            lambda blocks1, blocks2: [
+                op(blocks1[bid], blocks2[rhs_block_id]) if bid in block_index_set else blocks1[bid]
                 for bid in range(len(blocks1))
-            ]
+            ],
         )
     else:
         raise ValueError(f"Not implement type between dataframe nad {type(rhs)}")
 
     return op_ret
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_cmp.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_cmp.py`

 * *Files 3% similar despite different names*

```diff
@@ -63,26 +63,22 @@
         field_indexes = [data_manager.get_field_offset(name) for name in column_names]
         field_indexes_mappings = dict(zip(field_indexes, range(len(field_indexes))))
         indexers = [
             [field_indexes_mappings[field] for field in data_manager.get_block(bid).field_indexes]
             for bid in block_indexes
         ]
 
-        block_table = _cmp_dfs(lhs.block_table, rhs.block_table, op, lhs_block_loc, rhs_block_loc,
-                               block_indexes, indexers)
+        block_table = _cmp_dfs(
+            lhs.block_table, rhs.block_table, op, lhs_block_loc, rhs_block_loc, block_indexes, indexers
+        )
     else:
         raise ValueError(f"Not implement comparison of rhs type={type(rhs)}")
 
     block_table, data_manager = _merge_bool_blocks(block_table, data_manager, block_indexes)
-    return type(lhs)(
-        lhs._ctx,
-        block_table,
-        lhs.partition_order_mappings,
-        data_manager
-    )
+    return type(lhs)(lhs._ctx, block_table, lhs.partition_order_mappings, data_manager)
 
 
 def _merge_bool_blocks(block_table, data_manager: DataManager, block_indexes):
     """
     all blocks are bool type, they should be merge into one blocks
     """
     dst_data_manager = data_manager.duplicate()
@@ -92,18 +88,15 @@
 
     dst_data_manager.promote_types(to_promote_types)
     dst_block_table, dst_data_manager = compress_blocks(block_table, dst_data_manager)
 
     return dst_block_table, dst_data_manager
 
 
-def _cmp_dfs(lhs_block_table, rhs_block_table, op,
-             lhs_block_loc, rhs_block_loc,
-             block_indexes, indexers):
-
+def _cmp_dfs(lhs_block_table, rhs_block_table, op, lhs_block_loc, rhs_block_loc, block_indexes, indexers):
     block_index_set = set(block_indexes)
 
     def _cmp_partition(l_blocks, r_blocks):
         ret_blocks = [[] for i in range(l_blocks)]
         for bid in range(len(l_blocks)):
             if bid not in block_index_set:
                 ret_blocks[bid] = l_blocks[bid]
@@ -115,11 +108,10 @@
                 r_bid, r_offset = rhs_block_loc[idx]
                 cmp_ret[:, l_offset] = op(l_blocks[bid][l_offset], r_blocks[r_bid][r_offset])
 
             ret_blocks[bid] = cmp_ret
 
         return ret_blocks
 
-    block_table = lhs_block_table.join(rhs_block_table,
-                                       _cmp_partition)
+    block_table = lhs_block_table.join(rhs_block_table, _cmp_partition)
 
     return block_table
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_compress_block.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_compress_block.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_data_overview.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_data_overview.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_dimension_scaling.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_dimension_scaling.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,18 +27,20 @@
 from fate.arch.tensor import DTensor
 
 
 def hstack(data_frames: List["DataFrame"]) -> "DataFrame":
     if len(data_frames) == 1:
         return data_frames[0]
 
-    l_df = DataFrame(data_frames[0]._ctx,
-                     data_frames[0].block_table,
-                     data_frames[0].partition_order_mappings,
-                     data_frames[0].data_manager.duplicate())
+    l_df = DataFrame(
+        data_frames[0]._ctx,
+        data_frames[0].block_table,
+        data_frames[0].partition_order_mappings,
+        data_frames[0].data_manager.duplicate(),
+    )
 
     column_set = set(l_df.schema.columns)
     for r_df in data_frames[1:]:
         other_column_set = set(r_df.schema.columns)
         if column_set & other_column_set:
             raise ValueError("Hstack does not support duplicate columns")
 
@@ -73,16 +75,19 @@
             else:
                 _align_block = []
                 lines = len(blocks[0]) if lines is None else lines
                 for lid in range(lines):
                     row = []
                     for _field_index in _field_indexes:
                         _src_bid, _offset = align_fields_loc[_field_index]
-                        row.append(blocks[_src_bid][lid][_offset].item() if isinstance(blocks[_src_bid], torch.Tensor)
-                                   else blocks[_src_bid][lid][_offset])
+                        row.append(
+                            blocks[_src_bid][lid][_offset].item()
+                            if isinstance(blocks[_src_bid], torch.Tensor)
+                            else blocks[_src_bid][lid][_offset]
+                        )
 
                     _align_block.append(row)
 
                 ret_blocks.append(dst_dm.blocks[dst_bid].convert_block(_align_block))
 
         return ret_blocks
 
@@ -95,15 +100,16 @@
     type_change = False
     for r_df in data_frames[1:]:
         if set(l_df.schema.columns) != set(r_df.schema.columns):
             raise ValueError("vstack of dataframes should have same schemas")
 
         for idx, field_name in enumerate(l_field_names):
             block_type = r_df.data_manager.get_block(
-                r_df.data_manager.loc_block(field_name, with_offset=False)).block_type
+                r_df.data_manager.loc_block(field_name, with_offset=False)
+            ).block_type
             if block_type > l_field_types[idx]:
                 l_field_types[idx] = block_type
                 type_change = True
 
     if type_change:
         changed_fields, changed_block_types, changed_fields_loc = [], [], []
         changed_block_types = []
@@ -111,106 +117,98 @@
             field_name, block_type, (bid, offset) = l_field_names[idx], l_field_types[idx], l_fields_loc[idx]
             if block_type != data_manager.get_block(bid).block_type:
                 changed_fields.append(field_name)
                 changed_block_types.append(block_type)
                 changed_fields_loc.append((bid, offset))
 
         narrow_blocks, dst_blocks = data_manager.split_columns(changed_fields, changed_block_types)
-        l_block_table = promote_partial_block_types(l_block_table, narrow_blocks=narrow_blocks, dst_blocks=dst_blocks,
-                                                    data_manager=data_manager, dst_fields_loc=changed_fields_loc)
+        l_block_table = promote_partial_block_types(
+            l_block_table,
+            narrow_blocks=narrow_blocks,
+            dst_blocks=dst_blocks,
+            data_manager=data_manager,
+            dst_fields_loc=changed_fields_loc,
+        )
 
     for r_df in data_frames[1:]:
         r_field_names = r_df.data_manager.get_field_name_list()
         r_fields_loc = r_df.data_manager.get_fields_loc()
         r_field_types = [data_manager.get_block(_bid).block_type for _bid, _ in r_fields_loc]
-        r_type_change = False if l_field_types != r_field_types else True
+        r_type_change = False if l_field_types == r_field_types else True
         r_block_table = r_df.block_table
         if l_field_names != r_field_names or r_type_change:
             shuffle_r_fields_loc, full_migrate_set = [() for _ in range(len(r_field_names))], set()
             for field_name, loc in zip(r_field_names, r_fields_loc):
                 l_offset = data_manager.get_field_offset(field_name)
                 shuffle_r_fields_loc[l_offset] = loc
 
             for bid in range(r_df.data_manager.block_num):
                 r_field_indexes = r_df.data_manager.get_block(bid).field_indexes
                 field_indexes = [data_manager.get_field_offset(r_field_names[idx]) for idx in r_field_indexes]
                 l_bid = data_manager.loc_block(r_field_names[r_field_indexes[0]], with_offset=False)
                 if field_indexes == data_manager.get_block(l_bid).field_indexes:
                     full_migrate_set.add(bid)
 
-            _align_func = functools.partial(_align_blocks, align_fields_loc=shuffle_r_fields_loc,
-                                            full_block_migrate_set=full_migrate_set, dst_dm=data_manager)
+            _align_func = functools.partial(
+                _align_blocks,
+                align_fields_loc=shuffle_r_fields_loc,
+                full_block_migrate_set=full_migrate_set,
+                dst_dm=data_manager,
+            )
             r_block_table = r_block_table.mapValues(_align_func)
 
         l_block_table = l_block_table.union(
             r_block_table,
             lambda l_blocks, r_blocks: [
                 Block.vstack([l_block, r_block]) for l_block, r_block in zip(l_blocks, r_blocks)
-            ]
+            ],
         )
 
     partition_order_mappings = get_partition_order_mappings_by_block_table(l_block_table, data_manager.block_row_size)
-    _balance_block_func = functools.partial(_balance_blocks,
-                                            partition_order_mappings=partition_order_mappings,
-                                            block_row_size=data_manager.block_row_size)
-    l_block_table = l_block_table.mapPartitions(_balance_block_func,
-                                                use_previous_behavior=False)
+    _balance_block_func = functools.partial(
+        _balance_blocks, partition_order_mappings=partition_order_mappings, block_row_size=data_manager.block_row_size
+    )
+    l_block_table = l_block_table.mapPartitions(_balance_block_func, use_previous_behavior=False)
     l_block_table, data_manager = compress_blocks(l_block_table, data_manager)
 
-    return DataFrame(
-        l_df._ctx,
-        l_block_table,
-        partition_order_mappings,
-        data_manager
-    )
+    return DataFrame(l_df._ctx, l_block_table, partition_order_mappings, data_manager)
 
 
 def drop(df: "DataFrame", index: "DataFrame" = None) -> "DataFrame":
     if index.shape[0] == 0:
         return DataFrame(
             df._ctx,
             block_table=df.block_table,
             partition_order_mappings=copy.deepcopy(df.partition_order_mappings),
-            data_manager=df.data_manager.duplicate()
+            data_manager=df.data_manager.duplicate(),
         )
 
     if index.shape[0] == df.shape[0]:
         return df.empty_frame()
 
     data_manager = df.data_manager.duplicate()
-    l_flatten_func = functools.partial(
-        _flatten_partition,
-        block_num=data_manager.block_num
-    )
+    l_flatten_func = functools.partial(_flatten_partition, block_num=data_manager.block_num)
     l_flatten_table = df.block_table.mapPartitions(l_flatten_func, use_previous_behavior=False)
 
     r_flatten_func = functools.partial(_flatten_partition_without_value)
     r_flatten_table = index.block_table.mapPartitions(r_flatten_func, use_previous_behavior=False)
 
     drop_flatten = l_flatten_table.subtractByKey(r_flatten_table)
-    partition_order_mappings = get_partition_order_by_raw_table(
-        drop_flatten, data_manager.block_row_size
-    ) if drop_flatten.count() else dict()
-
-    _convert_to_block_func = functools.partial(to_blocks,
-                                               dm=data_manager,
-                                               partition_mappings=partition_order_mappings)
-
-    block_table = drop_flatten.mapPartitions(_convert_to_block_func,
-                                             use_previous_behavior=False)
-
-    return DataFrame(
-        df._ctx,
-        block_table,
-        partition_order_mappings,
-        data_manager
+    partition_order_mappings = (
+        get_partition_order_by_raw_table(drop_flatten, data_manager.block_row_size) if drop_flatten.count() else dict()
     )
 
+    _convert_to_block_func = functools.partial(to_blocks, dm=data_manager, partition_mappings=partition_order_mappings)
+
+    block_table = drop_flatten.mapPartitions(_convert_to_block_func, use_previous_behavior=False)
+
+    return DataFrame(df._ctx, block_table, partition_order_mappings, data_manager)
+
 
-def sample(df: "DataFrame", n=None, frac: float =None, random_state=None) -> "DataFrame":
+def sample(df: "DataFrame", n=None, frac: float = None, random_state=None) -> "DataFrame":
     """
     only support down sample, n should <= df.shape, or fact = 1
     """
 
     if n is not None and frac is not None:
         raise ValueError("sample's parameters n and frac should not be set in the same time.")
 
@@ -224,17 +222,17 @@
 
     if n == 0:
         raise ValueError(f"sample's parameter n={n} should >= 1")
 
     indexer = list(df.get_indexer(target="sample_id").collect())
     sample_indexer = resample(indexer, replace=False, n_samples=n, random_state=random_state)
 
-    sample_indexer = df._ctx.computing.parallelize(sample_indexer,
-                                                   include_key=True,
-                                                   partition=df.block_table.partitions)
+    sample_indexer = df._ctx.computing.parallelize(
+        sample_indexer, include_key=True, partition=df.block_table.num_partitions
+    )
 
     sample_frame = df.loc(sample_indexer)
 
     return sample_frame
 
 
 def retrieval_row(df: "DataFrame", indexer: Union["DTensor", "DataFrame"]):
@@ -243,70 +241,61 @@
     elif isinstance(indexer, DataFrame):
         operable_field_len = len(indexer.data_manager.infer_operable_field_names())
         if operable_field_len != 1:
             raise ValueError("Row indexing by DataFrame should have only one column filling with True/False")
 
     data_manager = df.data_manager.duplicate()
 
-    def _block_counter(kvs, value_type="tensor"):
+    if isinstance(indexer, DataFrame):
+        bid = indexer.data_manager.infer_operable_blocks()[0]
+        block_table = df.block_table.join(indexer.block_table, lambda v1, v2: (v1, v2[bid]))
+    else:
+        block_table = df.block_table.join(indexer.shardings._data, lambda v1, v2: (v1, v2))
+
+    def _block_counter(kvs):
         size = 0
         first_block_id = None
         for k, value in kvs:
-            if first_block_id is None:
+            if first_block_id is None or first_block_id > k:
                 first_block_id = k
 
-            if value_type == "tensor":
-                size += value.sum().item()
-            else:
-                size += len(value[0])
+            size += value[1].sum().item()
 
         return first_block_id, size
 
-    if isinstance(indexer, DataFrame):
-        _block_counter_func = functools.partial(_block_counter, value_type="dataframe")
-        block_info = sorted([summary[1] for summary in indexer.block_table.applyPartitions(_block_counter_func).collect()])
-    else:
-        block_info = sorted([summary[1] for summary in indexer.shardings._data.applyPartitions(_block_counter).collect()])
+    _block_counter_func = functools.partial(_block_counter)
+    block_info = sorted(
+        [summary[1] for summary in block_table.applyPartitions(_block_counter_func).collect()]
+    )
+
     block_order_mappings = dict()
     start_index = 0
     acc_block_num = 0
     for block_id, block_size in block_info:
         block_num = (block_size + data_manager.block_row_size - 1) // data_manager.block_row_size
         block_order_mappings[block_id] = dict(
             start_index=start_index,
             end_index=start_index + block_size - 1,
             start_block_id=acc_block_num,
-            end_block_id=acc_block_num + block_num - 1
+            end_block_id=acc_block_num + block_num - 1,
         )
         start_index += block_size
         acc_block_num += block_num
 
     if start_index == 0:
         return df.empty_frame()
 
-    if isinstance(indexer, DataFrame):
-        bid = indexer.data_manager.infer_operable_blocks()[0]
-        block_table = df.block_table.join(indexer.block_table, lambda v1, v2: (v1, v2[bid]))
-    else:
-        block_table = df.block_table.join(indexer.shardings._data, lambda v1, v2: (v1, v2))
-
-    _balance_block_func = functools.partial(_balance_blocks_with_index,
-                                            partition_order_mappings=block_order_mappings,
-                                            data_manager=data_manager)
-    block_table = block_table.mapPartitions(_balance_block_func,
-                                           use_previous_behavior=False)
+    _balance_block_func = functools.partial(
+        _balance_blocks_with_index, partition_order_mappings=block_order_mappings, data_manager=data_manager
+    )
+    block_table = block_table.mapPartitions(_balance_block_func, use_previous_behavior=False)
     block_table, data_manager = compress_blocks(block_table, data_manager)
     partition_order_mappings = get_partition_order_mappings_by_block_table(block_table, data_manager.block_row_size)
 
-    return DataFrame(
-        df._ctx,
-        block_table,
-        partition_order_mappings,
-        data_manager
-    )
+    return DataFrame(df._ctx, block_table, partition_order_mappings, data_manager)
 
 
 def _flatten_partition_without_value(kvs):
     for block_id, blocks in kvs:
         for sample_id in blocks[0]:
             yield sample_id, []
 
@@ -315,46 +304,49 @@
     for block_id, blocks in kvs:
         flat_blocks = [Block.transform_block_to_list(block) for block in blocks]
         lines = len(flat_blocks[0])
         for i in range(lines):
             yield flat_blocks[0][i], [flat_blocks[j][i] for j in range(1, block_num)]
 
 
-def _balance_blocks(kvs, partition_order_mappings: dict=None, block_row_size: int=None):
+def _balance_blocks(kvs, partition_order_mappings: dict = None, block_row_size: int = None):
     block_id = None
     previous_blocks = list()
     for _, blocks in kvs:
         if block_id is None and len(blocks[0]):
             sample_id = blocks[0][0]
             block_id = partition_order_mappings[sample_id]["start_block_id"]
 
         if previous_blocks:
             blocks = [Block.vstack([pre_block, block]) for pre_block, block in zip(previous_blocks, blocks)]
             previous_blocks = list()
 
         row_size = len(blocks[0])
         for i in range(0, row_size, block_row_size):
             if row_size - i < block_row_size:
-                previous_blocks = [block[i: row_size] for block in blocks]
+                previous_blocks = [block[i:row_size] for block in blocks]
             else:
-                yield block_id, [block[i: i + block_row_size] for block in blocks]
+                yield block_id, [block[i : i + block_row_size] for block in blocks]
                 block_id += 1
 
     if previous_blocks:
-        yield  block_id, previous_blocks
+        yield block_id, previous_blocks
 
     if block_id is None:
         return []
 
 
-def _balance_blocks_with_index(kvs, partition_order_mappings: dict=None, data_manager: DataManager=None):
+def _balance_blocks_with_index(kvs, partition_order_mappings: dict = None, data_manager: DataManager = None):
     block_id = None
     block_num = data_manager.block_num
     ret_blocks = [[] for _ in range(block_num)]
     block_size = 0
+
+    kvs = sorted(kvs)
+
     for _, (blocks, t) in kvs:
         if block_id is None:
             block_id = partition_order_mappings[_]["start_block_id"]
 
         flat_blocks = [Block.transform_block_to_list(block) for block in blocks]
         for i, v in enumerate(t):
             v = v.item()
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_encoder.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_encoder.py`

 * *Files 22% similar despite different names*

```diff
@@ -41,18 +41,15 @@
     dst_data_manager = data_manager.duplicate()
     dst_data_manager.pop_blocks(block_indexes)
     dst_data_manager.append_columns(dst_field_names, block_types=BlockType.get_block_type(dtype))
 
     block_table = _one_hot_encode(df.block_table, block_indexes, dst_data_manager, [[categories]], dtype=dtype)
 
     return DataFrame(
-        df._ctx,
-        block_table,
-        partition_order_mappings=df.partition_order_mappings,
-        data_manager=dst_data_manager
+        df._ctx, block_table, partition_order_mappings=df.partition_order_mappings, data_manager=dst_data_manager
     )
 
 
 def _get_categories(block_table, block_indexes):
     block_index_set = set(block_indexes)
 
     def _mapper(blocks):
@@ -79,29 +76,42 @@
 
     categories = [[sorted(cate) for cate in cate_block] for cate_block in categories]
 
     return categories
 
 
 def _one_hot_encode(block_table, block_indexes, data_manager, categories, dtype):
-    categories = [np.array(category) for category in categories]
+    transform_categories = []
+    mock_data = []
+    for category in categories:
+        transform_categories.append([np.array(cate) for cate in category])
+        shapes = [cate.shape[0] for cate in transform_categories[-1]]
+        row_shape = max(shapes)
+        column_shape = len(categories[-1])
+        mock_cate_data = torch.zeros((row_shape, column_shape), dtype=getattr(torch, dtype))
+        for rid in range(row_shape):
+            for cid in range(column_shape):
+                mock_cate_data[rid][cid] = rid % shapes[cid]
+
+        mock_data.append(mock_cate_data)
+
     block_index_set = set(block_indexes)
 
     def _encode(blocks):
         ret_blocks = []
         enc_blocks = []
         idx = 0
         for bid, block in enumerate(blocks):
             if bid not in block_index_set:
                 ret_blocks.append(block)
                 continue
 
             enc = OneHotEncoder(dtype=dtype)
-            enc.fit([[1]])  # one hot encoder need to fit first.
-            enc.categories_ = categories[idx]
+            enc.fit(mock_data[idx])  # one hot encoder need to fit first.
+            enc.categories_ = transform_categories[idx]
             idx += 1
             enc_blocks.append(enc.transform(block).toarray())
 
         ret_blocks.append(data_manager.blocks[-1].convert_block(np.hstack(enc_blocks)))
 
         return ret_blocks
 
@@ -109,87 +119,14 @@
 
 
 def bucketize(df: DataFrame, boundaries: Union[pd.DataFrame, dict]):
     if isinstance(boundaries, pd.DataFrame):
         boundaries = dict([(_name, boundaries[_name].tolist()) for _name in boundaries])
     elif not isinstance(boundaries, dict):
         raise ValueError("boundaries should be pd.DataFrame or dict")
-
-    data_manager = df.data_manager.duplicate()
-    field_names = list(filter(lambda field_name: field_name in boundaries, data_manager.infer_operable_field_names()))
-    blocks_loc = data_manager.loc_block(field_names)
-
-    _boundaries_list = []
-    for name, (_bid, _) in zip(field_names, blocks_loc):
-        if BlockType.is_tensor(data_manager.blocks[_bid].block_type):
-            _boundary = torch.tensor(boundaries[name])
-            _boundary[-1] = torch.inf
-        else:
-            _boundary = np.array(boundaries[name])
-            _boundary[-1] = np.inf
-
-        _boundaries_list.append((_bid, _, _boundary))
-
-    narrow_blocks, dst_blocks = data_manager.split_columns(
-        field_names, BlockType.get_block_type(BUCKETIZE_RESULT_TYPE)
-    )
-
-    def _mapper(
-        blocks, boundaries_list: list = None, narrow_loc: list = None, dst_bids: list = None, dm: DataManager = None
-    ):
-        ret_blocks = []
-        for block in blocks:
-            if isinstance(block, torch.Tensor):
-                ret_blocks.append(block.clone())
-            elif isinstance(block, np.ndarray):
-                ret_blocks.append(block.copy())
-            else:
-                ret_blocks.append(block)
-
-        for i in range(len(ret_blocks), dm.block_num):
-            ret_blocks.append([])
-
-        for bid, offsets in narrow_loc:
-            ret_blocks[bid] = ret_blocks[bid][:, offsets]
-
-        for dst_bid, (src_bid, src_offset, boundary) in zip(dst_bids, boundaries_list):
-            if isinstance(blocks[src_bid], torch.Tensor):
-                ret = torch.bucketize(blocks[src_bid][:, [src_offset]], boundary, out_int32=False)
-            else:
-                ret = torch.bucketize(blocks[src_bid][:, [src_offset]], boundary)
-
-            ret_blocks[dst_bid] = dm.blocks[dst_bid].convert_block(ret)
-
-        return ret_blocks
-
-    bucketize_mapper = functools.partial(
-        _mapper, boundaries_list=_boundaries_list, narrow_loc=narrow_blocks, dst_bids=dst_blocks, dm=data_manager
-    )
-
-    block_table = df.block_table.mapValues(bucketize_mapper)
-
-    block_indexes = data_manager.infer_operable_blocks()
-    if len(block_indexes) > 1:
-        to_promote_types = []
-        for bid in block_indexes:
-            to_promote_types.append((bid, BlockType.get_block_type(BUCKETIZE_RESULT_TYPE)))
-
-        data_manager.promote_types(to_promote_types)
-        block_table, data_manager = compress_blocks(block_table, data_manager)
-
-    return DataFrame(
-        df._ctx, block_table, partition_order_mappings=df.partition_order_mappings, data_manager=data_manager
-    )
-
-
-def bucketize(df: DataFrame, boundaries: Union[pd.DataFrame, dict]):
-    if isinstance(boundaries, pd.DataFrame):
-        boundaries = dict([(_name, boundaries[_name].tolist()) for _name in boundaries])
-    elif not isinstance(boundaries, dict):
-        raise ValueError("boundaries should be pd.DataFrame or dict")
 
     data_manager = df.data_manager.duplicate()
     field_names = list(filter(lambda field_name: field_name in boundaries, data_manager.infer_operable_field_names()))
     blocks_loc = data_manager.loc_block(field_names)
 
     _boundaries_list = []
     for name, (_bid, _) in zip(field_names, blocks_loc):
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_field_extract.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_field_extract.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,19 +11,21 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 from .._dataframe import DataFrame
 
 
-def field_extract(df: "DataFrame", with_sample_id=True, with_match_id=True, with_weight=True,
-                  with_label=True, columns=None):
+def field_extract(
+    df: "DataFrame", with_sample_id=True, with_match_id=True, with_weight=True, with_label=True, columns=None
+):
     """
     blocks_loc: list, each element: (src_block_id, dst_block_id, changed=True/False, block_indexes)
     """
+
     def _extract_columns(src_blocks):
         extract_blocks = [None] * len(blocks_loc)
 
         for src_block_id, dst_block_id, is_changed, block_column_indexes in blocks_loc:
             block = src_blocks[src_block_id]
             if is_changed:
                 extract_blocks[dst_block_id] = block[:, block_column_indexes]
@@ -33,17 +35,14 @@
         return extract_blocks
 
     data_manager, blocks_loc = df.data_manager.derive_new_data_manager(
         with_sample_id=with_sample_id,
         with_match_id=with_match_id,
         with_label=with_label,
         with_weight=with_weight,
-        columns=columns
+        columns=columns,
     )
     extract_table = df.block_table.mapValues(_extract_columns)
 
     return DataFrame(
-        df._ctx,
-        extract_table,
-        partition_order_mappings=df.partition_order_mappings,
-        data_manager=data_manager
+        df._ctx, extract_table, partition_order_mappings=df.partition_order_mappings, data_manager=data_manager
     )
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_fillna.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_fillna.py`

 * *Files 8% similar despite different names*

```diff
@@ -40,39 +40,36 @@
             value_indexers[bid][offset] = fill_value
 
         block_table = _fillna(df.block_table, value_indexers, block_indexes)
 
     else:
         raise ValueError(f"Not support value type={type(value)}")
 
-    return DataFrame(
-        df._ctx,
-        block_table,
-        df.partition_order_mappings,
-        data_manager.duplicate()
-    )
+    return DataFrame(df._ctx, block_table, df.partition_order_mappings, data_manager.duplicate())
 
 
 def _fillna(block_table, value, block_indexes):
     block_index_set = set(block_indexes)
     if isinstance(value, (int, float, np.int32, np.int64, np.float32, np.float64)):
+
         def _fill(blocks):
             ret_blocks = []
             for bid, block in enumerate(blocks):
                 if bid not in block_index_set:
                     ret_blocks.append(block)
                 elif isinstance(block, torch.Tensor):
                     ret_blocks.append(torch.nan_to_num(block, value))
                 elif isinstance(block, np.ndarray):
                     ret_blocks.append(np.nan_to_num(block, value))
 
             return ret_blocks
 
         return block_table.mapValues(_fill)
     else:
+
         def _fill_with_dict(blocks):
             ret_blocks = []
             for bid, block in enumerate(blocks):
                 if bid not in block_index_set:
                     ret_blocks.append(block)
                 elif isinstance(block, torch.Tensor):
                     block = block.clone()
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_histogram.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_histogram.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,54 +22,55 @@
 from ..manager import BlockType, DataManager
 from ._compress_block import compress_blocks
 
 if typing.TYPE_CHECKING:
     from fate.arch.histogram import DistributedHistogram, HistogramBuilder
 
 
-def distributed_hist_stat(df: DataFrame, histogram_builder: "HistogramBuilder", position: DataFrame, targets: Union[dict, DataFrame]) -> "DistributedHistogram":
+def distributed_hist_stat(
+    df: DataFrame, histogram_builder: "HistogramBuilder", position: DataFrame, targets: Union[dict, DataFrame]
+) -> "DistributedHistogram":
     block_table, data_manager = _try_to_compress_table(df.block_table, df.data_manager, force_compress=True)
     data_block_id = data_manager.infer_operable_blocks()[0]
     position_block_id = position.data_manager.infer_operable_blocks()[0]
 
     if isinstance(targets, dict):
+
         def _pack_data_with_position(l_blocks, r_blocks, l_block_id=None, r_block_id=None):
             return l_blocks[l_block_id], r_blocks[r_block_id], dict()
 
         def _pack_with_target(l_values, r_value, target_name):
             l_values[2][target_name] = r_value
 
             return l_values
 
-        _pack_func = functools.partial(_pack_data_with_position,
-                                       l_block_id=data_block_id,
-                                       r_block_id=position_block_id)
+        _pack_func = functools.partial(
+            _pack_data_with_position, l_block_id=data_block_id, r_block_id=position_block_id
+        )
 
         data_with_position = block_table.join(position.block_table, _pack_func)
 
         for name, target in targets.items():
             _pack_with_target_func = functools.partial(_pack_with_target, target_name=name)
             data_with_position = data_with_position.join(target.shardings._data, _pack_with_target_func)
     else:
         data_with_position = block_table.join(
-            position.block_table,
-            lambda l_blocks, r_blocks: (l_blocks[data_block_id], r_blocks[position_block_id])
+            position.block_table, lambda l_blocks, r_blocks: (l_blocks[data_block_id], r_blocks[position_block_id])
         )
 
         target_data_manager = targets.data_manager
         target_field_names = target_data_manager.infer_operable_field_names()
         fields_loc = target_data_manager.loc_block(target_field_names, with_offset=True)
 
         def _pack_with_targets(l_blocks, r_blocks):
             target_blocks = dict()
             for field_name, (block_id, offset) in zip(target_field_names, fields_loc):
                 if (block := target_data_manager.get_block(block_id)).is_phe_tensor():
                     target_blocks[field_name] = block.convert_to_phe_tensor(
-                        r_blocks[block_id],
-                        shape=(len(r_blocks[0]), 1)
+                        r_blocks[block_id], shape=(len(r_blocks[0]), 1)
                     )
                 else:
                     target_blocks[field_name] = r_blocks[block_id][:, [offset]]
 
             return l_blocks[0], l_blocks[1], target_blocks
 
         data_with_position = data_with_position.join(targets.block_table, _pack_with_targets)
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_indexer.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_indexer.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,29 +22,28 @@
 
 def transform_to_table(block_table, block_index, partition_order_mappings):
     def _convert_to_order_index(kvs):
         for block_id, blocks in kvs:
             for _idx, _id in enumerate(blocks[block_index]):
                 yield _id, (block_id, _idx)
 
-    return block_table.mapPartitions(_convert_to_order_index,
-                                     use_previous_behavior=False)
+    return block_table.mapPartitions(_convert_to_order_index, use_previous_behavior=False)
 
 
 def get_partition_order_mappings_by_block_table(block_table, block_row_size):
     def _block_counter(kvs):
         partition_key = None
         size = 0
-        first_block_id = ''
+        first_block_id = ""
 
         for k, v in kvs:
             if size == 0 and len(v[0]):
                 partition_key = v[0][0]
 
-            if first_block_id == '':
+            if first_block_id == "":
                 first_block_id = k
 
             size += len(v[0])
 
         if size == 0:
             partition_key = str(first_block_id) + "_" + str(uuid.uuid1())
 
@@ -56,15 +55,15 @@
     acc_block_num = 0
     for block_key, block_size in block_info:
         block_num = (block_size + block_row_size - 1) // block_row_size
         block_order_mappings[block_key] = dict(
             start_index=start_index,
             end_index=start_index + block_size - 1,
             start_block_id=acc_block_num,
-            end_block_id=acc_block_num + block_num - 1
+            end_block_id=acc_block_num + block_num - 1,
         )
         start_index += block_size
         acc_block_num += block_num
 
     return block_order_mappings
 
 
@@ -92,15 +91,15 @@
 
     for blk_key, blk_size in sorted(block_summary.items()):
         block_num = (blk_size + block_row_size - 1) // block_row_size
         block_order_mappings[blk_key] = dict(
             start_index=start_index,
             end_index=start_index + blk_size - 1,
             start_block_id=acc_block_num,
-            end_block_id=acc_block_num + block_num - 1
+            end_block_id=acc_block_num + block_num - 1,
         )
 
         start_index += blk_size
         acc_block_num += block_num
 
     return block_order_mappings
 
@@ -165,45 +164,52 @@
 def flatten_data(df: DataFrame, key_type="block_id", with_sample_id=True):
     """
     key_type="block_id":
         key=(block_id, block_offset), value=data_row
     key_type="sample_id":
         key=sample_id, value=data_row
     """
-    sample_id_index = df.data_manager.loc_block(
-        df.data_manager.schema.sample_id_name, with_offset=False
-    ) if (with_sample_id or key_type == "sample_id") else  None
+    sample_id_index = (
+        df.data_manager.loc_block(df.data_manager.schema.sample_id_name, with_offset=False)
+        if (with_sample_id or key_type == "sample_id")
+        else None
+    )
 
     def _flatten(kvs):
         for block_id, blocks in kvs:
             flat_blocks = [Block.transform_block_to_list(block) for block in blocks]
             block_num = len(flat_blocks)
             if key_type == "block_id":
                 for row_id in range(len(blocks[0])):
                     if with_sample_id:
                         yield (block_id, row_id), (
                             flat_blocks[sample_id_index][row_id],
-                            [flat_blocks[i][row_id] for i in range(block_num)]
+                            [flat_blocks[i][row_id] for i in range(block_num)],
                         )
                     else:
                         yield (block_id, row_id), [flat_blocks[i][row_id] for i in range(block_num)]
             else:
                 for row_id in range(len(blocks[0])):
                     yield flat_blocks[sample_id_index][row_id], [flat_blocks[i][row_id] for i in range(block_num)]
 
     if key_type in ["block_id", "sample_id"]:
         return df.block_table.mapPartitions(_flatten, use_previous_behavior=False)
     else:
         raise ValueError(f"Not Implement key_type={key_type} of flatten_data.")
 
 
 def transform_flatten_data_to_df(ctx, flatten_table, data_manager: DataManager, key_type, value_with_sample_id=True):
-    partition_order_mappings = get_partition_order_by_raw_table(flatten_table,
-                                                                data_manager.block_row_size,
-                                                                key_type=key_type)
+    if flatten_table.count() == 0:
+        return DataFrame(
+            ctx=ctx, block_table=flatten_table, partition_order_mappings=dict(), data_manager=data_manager.duplicate()
+        )
+
+    partition_order_mappings = get_partition_order_by_raw_table(
+        flatten_table, data_manager.block_row_size, key_type=key_type
+    )
     block_num = data_manager.block_num
 
     def _convert_to_blocks(kvs):
         bid = None
         ret_blocks = [[] for _ in range(block_num)]
 
         lid = 0
@@ -232,31 +238,32 @@
 
     block_table = flatten_table.mapPartitions(_convert_to_blocks, use_previous_behavior=False)
 
     return DataFrame(
         ctx=ctx,
         block_table=block_table,
         partition_order_mappings=partition_order_mappings,
-        data_manager=data_manager.duplicate()
+        data_manager=data_manager.duplicate(),
     )
 
 
 def loc(df: DataFrame, indexer, target="sample_id", preserve_order=False):
     """
     indexer: table, key=sample_id, value=(block_id, block_offset)
     """
     if target != "sample_id":
         raise ValueError(f"Only target=sample_id is supported, but target={target} is found")
     flatten_table = flatten_data(df, key_type="sample_id")
     if not preserve_order:
         flatten_table = flatten_table.join(indexer, lambda v1, v2: v1)
         if not flatten_table.count():
             return df.empty_frame()
-        return transform_flatten_data_to_df(df._ctx, flatten_table, df.data_manager,
-                                            key_type="sample_id", value_with_sample_id=False)
+        return transform_flatten_data_to_df(
+            df._ctx, flatten_table, df.data_manager, key_type="sample_id", value_with_sample_id=False
+        )
     else:
         flatten_table_with_dst_indexer = flatten_table.join(indexer, lambda v1, v2: (v2[0], (v2[1], v1)))
         if not flatten_table_with_dst_indexer.count():
             return df.empty_frame()
 
         def _aggregate(kvs):
             values = [value for key, value in kvs]
@@ -288,40 +295,39 @@
 
             return ret_blocks
 
         agg_data = flatten_table_with_dst_indexer.mapReducePartitions(_aggregate, lambda v1, v2: v1 + v2)
         block_table = agg_data.mapValues(_to_blocks)
 
         partition_order_mappings = get_partition_order_mappings_by_block_table(
-            block_table,
-            block_row_size=data_manager.block_row_size
+            block_table, block_row_size=data_manager.block_row_size
         )
 
         return DataFrame(
             df._ctx,
             block_table=block_table,
             partition_order_mappings=partition_order_mappings,
-            data_manager=data_manager.duplicate()
+            data_manager=data_manager.duplicate(),
         )
 
 
 def loc_with_sample_id_replacement(df: DataFrame, indexer):
     """
     indexer: table,
             row: (key=random_key,
             value=(sample_id, (src_block_id, src_offset))
     """
     if indexer.count() == 0:
         return df.empty_frame()
 
     data_manager = df.data_manager
-    partition_order_mappings = get_partition_order_by_raw_table(indexer,
-                                                                data_manager.block_row_size,
-                                                                key_type="block_id")
-    
+    partition_order_mappings = get_partition_order_by_raw_table(
+        indexer, data_manager.block_row_size, key_type="block_id"
+    )
+
     def _aggregate(kvs):
         bid, offset = None, 0
         flat_ret = []
         for k, values in kvs:
             sample_id, (src_block_id, src_offset) = values
             if bid is None:
                 bid = partition_order_mappings[sample_id]["start_block_id"]
@@ -338,35 +344,33 @@
         l = len(flat_ret)
         while i < l:
             j = i
             while j < l and flat_ret[i][0] == flat_ret[j][0]:
                 j += 1
 
             agg_ret = [flat_ret[k][1:] for k in range(i, j)]
-            yield  flat_ret[i][0], agg_ret
+            yield flat_ret[i][0], agg_ret
 
             i = j
 
     sample_id_index = data_manager.loc_block(data_manager.schema.sample_id_name, with_offset=False)
     block_num = data_manager.block_num
-    
+
     def _convert_to_row(kvs):
         ret_dict = {}
         for block_id, (blocks, block_indexer) in kvs:
             flat_blocks = [Block.transform_block_to_list(block) for block in blocks]
             for src_row_id, sample_id, dst_block_id, dst_row_id in block_indexer:
                 if dst_block_id not in ret_dict:
                     ret_dict[dst_block_id] = []
 
                 row_data = [flat_blocks[i][src_row_id] for i in range(block_num)]
                 row_data[sample_id_index] = sample_id
 
-                ret_dict[dst_block_id].append(
-                    (dst_row_id, row_data)
-                )
+                ret_dict[dst_block_id].append((dst_row_id, row_data))
 
         for dst_block_id, value_list in ret_dict.items():
             yield dst_block_id, sorted(value_list)
 
     agg_indexer = indexer.mapReducePartitions(_aggregate, lambda l1, l2: l1 + l2)
     block_table = df.block_table.join(agg_indexer, lambda v1, v2: (v1, v2))
     block_table = block_table.mapReducePartitions(_convert_to_row, _merge_list)
@@ -382,9 +386,9 @@
     _convert_to_frame_block_func = functools.partial(_convert_to_frame_block, data_manager=data_manager)
     block_table = block_table.mapValues(_convert_to_frame_block_func)
 
     return DataFrame(
         ctx=df._ctx,
         block_table=block_table,
         partition_order_mappings=partition_order_mappings,
-        data_manager=data_manager
+        data_manager=data_manager,
     )
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_isin.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_isin.py`

 * *Files 5% similar despite different names*

```diff
@@ -49,32 +49,29 @@
     to_promote_types = []
     for bid in block_indexes:
         to_promote_types.append((bid, torch.bool))
 
     dst_data_manager.promote_types(to_promote_types)
     dst_block_table, dst_data_manager = compress_blocks(block_table, dst_data_manager)
 
-    return type(df) (
-        df._ctx,
-        dst_block_table,
-        df.partition_order_mappings,
-        dst_data_manager
-    )
+    return type(df)(df._ctx, dst_block_table, df.partition_order_mappings, dst_data_manager)
 
 
 def _isin(block_table, values, block_indexes):
     block_index_set = set(block_indexes)
+
     def _has_nan_value(v_list):
         for v in v_list:
             if np.isnan(v):
                 return True
 
         return False
 
     if isinstance(values, list):
+
         def _is_in_list(blocks):
             ret_blocks = []
             for bid, block in enumerate(blocks):
                 if bid not in block_index_set:
                     ret_blocks.append(block)
                 elif isinstance(block, torch.Tensor):
                     ret_block = torch.isin(block, torch.Tensor(values))
@@ -85,14 +82,15 @@
                     ret_block = np.isin(block, values)
                     if _has_nan_value(values):
                         ret_block |= np.isnan(block)
                     ret_blocks.append(torch.tensor(ret_block, dtype=torch.bool))
 
         block_table = block_table.mapValues(_is_in_list)
     else:
+
         def _is_in_dict(blocks):
             ret_blocks = []
             for bid, block in enumerate(blocks):
                 if bid not in block_index_set:
                     ret_blocks.append(block)
                     continue
                 elif isinstance(block, torch.Tensor):
@@ -108,8 +106,8 @@
                         ret_block[:, offset] = np.isin(block[:, offset], in_values)
                         if _has_nan_value(in_values):
                             ret_block[:, offset] = np.isnan(block[:, offset])
                     ret_blocks.append(torch.tensor(ret_block, dtype=torch.bool))
 
         block_table = block_table.mapValues(_is_in_dict)
 
-    return block_table
+    return block_table
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_missing.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_missing.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,20 +28,15 @@
     to_promote_types = []
     for bid in block_indexes:
         to_promote_types.append((bid, BlockType.get_block_type(torch.bool)))
 
     dst_data_manager.promote_types(to_promote_types)
     dst_block_table, dst_data_manager = compress_blocks(block_table, dst_data_manager)
 
-    return DataFrame(
-        df._ctx,
-        dst_block_table,
-        df.partition_order_mappings,
-        dst_data_manager
-    )
+    return DataFrame(df._ctx, dst_block_table, df.partition_order_mappings, dst_data_manager)
 
 
 def _isna(block_table, block_indexes):
     block_index_set = set(block_indexes)
 
     def _isna_judgement(blocks):
         ret_blocks = []
@@ -49,10 +44,8 @@
             if bid not in block_index_set:
                 ret_blocks.append(block)
             else:
                 ret_blocks.append(torch.isnan(block) if isinstance(block, torch.Tensor) else np.isnan(block))
 
         return ret_blocks
 
-    return block_table.mapValues(
-        _isna_judgement
-    )
+    return block_table.mapValues(_isna_judgement)
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_promote_types.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_promote_types.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,27 +21,35 @@
 
 
 def promote_types(block_table, data_manager: DataManager, to_promote_blocks):
     data_manager.promote_types(to_promote_blocks)
     to_promote_block_dict = dict((bid, block_type) for bid, block_type in to_promote_blocks)
     block_table = block_table.mapValues(
         lambda blocks: [
-            blocks[bid] if bid not in to_promote_block_dict
+            blocks[bid]
+            if bid not in to_promote_block_dict
             else Block.get_block_by_type(to_promote_block_dict[bid]).convert_block(blocks[bid].tolist())
             for bid in range(len(blocks))
         ]
     )
 
     return block_table, data_manager
 
 
-def promote_partial_block_types(block_table, narrow_blocks, dst_blocks, dst_fields_loc,
-                                data_manager: DataManager, inplace=True):
-    def _mapper(blocks, narrow_loc: list = None, dst_bids: list = None,
-                dst_loc: List[Tuple[str, str]] = None, dm: DataManager = None, inp: bool = True):
+def promote_partial_block_types(
+    block_table, narrow_blocks, dst_blocks, dst_fields_loc, data_manager: DataManager, inplace=True
+):
+    def _mapper(
+        blocks,
+        narrow_loc: list = None,
+        dst_bids: list = None,
+        dst_loc: List[Tuple[str, str]] = None,
+        dm: DataManager = None,
+        inp: bool = True,
+    ):
         ret_blocks = []
         for block in blocks:
             if inp:
                 if isinstance(block, torch.Tensor):
                     ret_blocks.append(block.clone())
                 else:
                     ret_blocks.append(block.copy())
@@ -56,11 +64,12 @@
 
         for dst_bid, (src_bid, src_offset) in zip(dst_bids, dst_loc):
             block_values = blocks[src_bid][:, [src_offset]]
             ret_blocks[dst_bid] = dm.blocks[dst_bid].convert_block(block_values)
 
         return ret_blocks
 
-    _mapper_func = functools.partial(_mapper, narrow_loc=narrow_blocks, dst_bids=dst_blocks,
-                                     dst_loc=dst_fields_loc, dm=data_manager, inp=inplace)
+    _mapper_func = functools.partial(
+        _mapper, narrow_loc=narrow_blocks, dst_bids=dst_blocks, dst_loc=dst_fields_loc, dm=data_manager, inp=inplace
+    )
 
     return block_table.mapValues(_mapper_func)
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_quantile.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_quantile.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_replace.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_replace.py`

 * *Files 7% similar despite different names*

```diff
@@ -37,16 +37,17 @@
                 block_type = v_type
 
         dst_block_types.append(block_type)
         _to_replace_list.append((_bid, _, to_replace[name]))
 
     narrow_blocks, dst_blocks = data_manager.split_columns(field_names, dst_block_types)
 
-    def _mapper(blocks, to_replace_list: list = None, narrow_loc: list = None,
-                dst_bids: list = None, dm: DataManager = None):
+    def _mapper(
+        blocks, to_replace_list: list = None, narrow_loc: list = None, dst_bids: list = None, dm: DataManager = None
+    ):
         ret_blocks = []
         for block in blocks:
             if isinstance(block, torch.Tensor):
                 ret_blocks.append(block.clone())
             elif isinstance(block, np.ndarray):
                 ret_blocks.append(block.copy())
             else:
@@ -70,30 +71,25 @@
                 else:
                     replace_ret.append([_to_replace_dict[value]])
 
             ret_blocks[dst_bid] = dm.blocks[dst_bid].convert_block(replace_ret)
 
         return ret_blocks
 
-    replace_mapper = functools.partial(_mapper,
-                                       to_replace_list=_to_replace_list,
-                                       narrow_loc=narrow_blocks,
-                                       dst_bids=dst_blocks,
-                                       dm=data_manager)
+    replace_mapper = functools.partial(
+        _mapper, to_replace_list=_to_replace_list, narrow_loc=narrow_blocks, dst_bids=dst_blocks, dm=data_manager
+    )
 
     block_table = df.block_table.mapValues(replace_mapper)
 
     block_indexes = data_manager.infer_operable_blocks()
     if len(block_indexes) > 1:
         to_promote_types = []
         for _bid in block_indexes:
             to_promote_types.append((_bid, data_manager.get_block(_bid).block_type))
 
         data_manager.promote_types(to_promote_types)
         block_table, data_manager = compress_blocks(block_table, data_manager)
 
     return DataFrame(
-        df._ctx,
-        block_table,
-        partition_order_mappings=df.partition_order_mappings,
-        data_manager=data_manager
+        df._ctx, block_table, partition_order_mappings=df.partition_order_mappings, data_manager=data_manager
     )
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_set_item.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_set_item.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,42 +43,42 @@
 
     if len(other_field_names) > 1:
         raise ValueError(f"Too many columns of rhs, only one is supported")
 
     other_block_type = other_data_manager.blocks[other_block_id].block_type
     if (name := getattr(df.schema, f"{key_type}_name")) is not None:
         block_id = data_manager.loc_block(name, with_offset=False)
-        block_table = df.block_table.join(item.block_table,
-                                          lambda blocks1, blocks2:
-                                          [block if bid != block_id else blocks2[other_block_id]
-                                           for bid, block in enumerate(blocks1)]
+        block_table = df.block_table.join(
+            item.block_table,
+            lambda blocks1, blocks2: [
+                block if bid != block_id else blocks2[other_block_id] for bid, block in enumerate(blocks1)
+            ],
         )
         if data_manager.blocks[block_id].block_type < other_block_type:
             data_manager.blocks[block_id].convert_block_type(other_block_type)
     else:
-        data_manager.add_label_or_weight(key_type=key_type,
-                                         name=other_field_names[0],
-                                         block_type=other_block_type)
+        data_manager.add_label_or_weight(key_type=key_type, name=other_field_names[0], block_type=other_block_type)
 
-        block_table = df.block_table.join(item.block_table,
-                                          lambda blocks1, blocks2: blocks1 + [blocks2[other_block_id]])
+        block_table = df.block_table.join(
+            item.block_table, lambda blocks1, blocks2: blocks1 + [blocks2[other_block_id]]
+        )
 
     df.block_table = block_table
     df.data_manager = data_manager
 
 
 def _set_new_item(df: "DataFrame", keys, items):
-    def _append_single(blocks, item, col_len, bid=None, dm: DataManager=None):
+    def _append_single(blocks, item, col_len, bid=None, dm: DataManager = None):
         lines = len(blocks[0])
         ret_blocks = [block for block in blocks]
         ret_blocks.append(dm.blocks[bid].convert_block([[item for _ in range(col_len)] for idx in range(lines)]))
 
         return ret_blocks
 
-    def _append_multi(blocks, item_list, bid_list=None, dm: DataManager=None):
+    def _append_multi(blocks, item_list, bid_list=None, dm: DataManager = None):
         lines = len(blocks[0])
         ret_blocks = [block for block in blocks]
         for bid, item in zip(bid_list, item_list):
             ret_blocks.append(dm.blocks[bid].convert_block([[item] for _ in range(lines)]))
 
         return ret_blocks
 
@@ -95,15 +95,15 @@
             l_bid += 1
 
         return ret_blocks
 
     def _append_tensor(l_blocks, r_tensor, bid_list=None, dm: DataManager = None):
         ret_blocks = [block for block in l_blocks]
         for offset, bid in enumerate(bid_list):
-            ret_blocks.append(dm.blocks[bid].convert_block(r_tensor[:, offset: offset+1]))
+            ret_blocks.append(dm.blocks[bid].convert_block(r_tensor[:, offset : offset + 1]))
 
         return ret_blocks
 
     def _append_phe_tensor(l_blocks, r_tensor):
         ret_blocks = [block for block in l_blocks]
         ret_blocks.append(r_tensor._data)
 
@@ -116,16 +116,17 @@
         block_table = df.block_table.mapValues(_append_func)
 
     elif isinstance(items, list):
         if len(keys) != len(items):
             if len(keys) > 1:
                 raise ValueError("Must have equal len keys and value when setting with an iterable")
             bids = data_manager.append_columns(keys, BlockType.get_block_type("object"))
-            _append_func = functools.partial(_append_single, item=items, col_len=len(keys),
-                                             bid=bids[0], dm=data_manager)
+            _append_func = functools.partial(
+                _append_single, item=items, col_len=len(keys), bid=bids[0], dm=data_manager
+            )
         else:
             bids = data_manager.append_columns(keys, [BlockType.get_block_type(items[i]) for i in range(len(keys))])
             _append_func = functools.partial(_append_multi, item_list=items, bid_list=bids, dm=data_manager)
         block_table = df.block_table.mapValues(_append_func)
     elif isinstance(items, DataFrame):
         other_dm = items.data_manager
         operable_fields = other_dm.infer_operable_field_names()
@@ -135,34 +136,37 @@
             raise ValueError("Setitem with rhs=DataFrame must have equal len keys")
         data_manager.append_columns(keys, block_types)
 
         l = len(keys)
         for idx, (other_block_id, _) in enumerate(operable_blocks_loc):
             if data_manager.blocks[-l + idx].is_phe_tensor():
                 other_block = other_dm.blocks[other_block_id]
-                data_manager.blocks[-l + idx].set_extra_kwargs(pk=other_block._pk,
-                                                               evaluator=other_block._evaluator,
-                                                               coder=other_block._coder,
-                                                               dtype=other_block._dtype,
-                                                               device=other_block._device)
+                data_manager.blocks[-l + idx].set_extra_kwargs(
+                    pk=other_block._pk,
+                    evaluator=other_block._evaluator,
+                    coder=other_block._coder,
+                    dtype=other_block._dtype,
+                    device=other_block._device,
+                )
 
         _append_func = functools.partial(_append_df, r_blocks_loc=operable_blocks_loc, dm=data_manager)
         block_table = df.block_table.join(items.block_table, _append_func)
     elif isinstance(items, DTensor):
         meta_data = items.shardings._data.mapValues(
             lambda v: (v.pk, v.evaluator, v.coder, v.dtype) if isinstance(v, PHETensor) else None
         ).first()[1]
 
         if isinstance(meta_data, tuple):
             block_type = BlockType.phe_tensor
             if len(keys) != 1:
                 raise ValueError("to set item of PHETensor, lhs should has only one columns.")
             data_manager.append_columns(keys, block_type)
-            data_manager.blocks[-1].set_extra_kwargs(pk=meta_data[0], evaluator=meta_data[1], coder=meta_data[2],
-                                                     dtype=meta_data[3], device=items.device)
+            data_manager.blocks[-1].set_extra_kwargs(
+                pk=meta_data[0], evaluator=meta_data[1], coder=meta_data[2], dtype=meta_data[3], device=items.device
+            )
             _append_func = functools.partial(_append_phe_tensor)
             block_table = df.block_table.join(items.shardings._data, _append_func)
         else:
             block_type = BlockType.get_block_type(items.dtype)
             if len(keys) != items.shape[1]:
                 raise ValueError("Setitem with rhs=DTensor must have equal len keys")
             bids = data_manager.append_columns(keys, block_type)
@@ -172,15 +176,15 @@
         raise ValueError(f"Seiitem with rhs_type={type(items)} is not supported")
 
     df.block_table = block_table
     df.data_manager = data_manager
 
 
 def _set_old_item(df: "DataFrame", keys, items):
-    def _replace_single(blocks, item=None, narrow_loc=None, dst_bids=None, dm: DataManager=None):
+    def _replace_single(blocks, item=None, narrow_loc=None, dst_bids=None, dm: DataManager = None):
         ret_blocks = [block for block in blocks]
         for i in range(len(ret_blocks), dm.block_num):
             ret_blocks.append([])
 
         for bid, offsets in narrow_loc:
             ret_blocks[bid] = ret_blocks[bid][:, offsets]
 
@@ -200,15 +204,15 @@
 
         lines = len(blocks[0])
         for dst_bid, item in zip(dst_bids, item_list):
             ret_blocks[dst_bid] = dm.blocks[dst_bid].convert_block([[item] for idx in range(lines)])
 
         return ret_blocks
 
-    def _replace_df(l_blocks, r_blocks, narrow_loc=None, dst_bids=None, r_blocks_loc=None, dm: DataManager=None):
+    def _replace_df(l_blocks, r_blocks, narrow_loc=None, dst_bids=None, r_blocks_loc=None, dm: DataManager = None):
         ret_blocks = [block for block in l_blocks]
         for i in range(len(ret_blocks), dm.block_num):
             ret_blocks.append([])
 
         for bid, offsets in narrow_loc:
             ret_blocks[bid] = ret_blocks[bid][:, offsets]
 
@@ -229,49 +233,59 @@
             ret_blocks[dst_bid] = dm.blocks[dst_bid].convert_block(r_tensor[:, offset : offset + 1])
 
         return ret_blocks
 
     data_manager = df.data_manager.duplicate()
     if isinstance(items, (bool, int, float, str, np.int32, np.float32, np.int64, np.float64, np.bool_)):
         narrow_blocks, dst_blocks = data_manager.split_columns(keys, BlockType.get_block_type(items))
-        replace_func = functools.partial(_replace_single, item=items, narrow_loc=narrow_blocks,
-                                         dst_bids=dst_blocks, dm=data_manager)
+        replace_func = functools.partial(
+            _replace_single, item=items, narrow_loc=narrow_blocks, dst_bids=dst_blocks, dm=data_manager
+        )
         block_table = df.block_table.mapValues(replace_func)
     elif isinstance(items, list):
         if len(keys) != len(items):
             if len(keys) > 1:
                 raise ValueError("Must have equal len keys and value when setting with an iterable")
             narrow_blocks, dst_blocks = data_manager.split_columns(keys, BlockType.get_block_type("object"))
-            replace_func = functools.partial(_replace_single, item=items[0], narrow_loc=narrow_blocks,
-                                             dst_bids=dst_blocks, dm=data_manager)
+            replace_func = functools.partial(
+                _replace_single, item=items[0], narrow_loc=narrow_blocks, dst_bids=dst_blocks, dm=data_manager
+            )
         else:
-            narrow_blocks, dst_blocks = data_manager.split_columns(keys,
-                                                                   [BlockType.get_block_type(item) for item in items])
-            replace_func = functools.partial(_replace_multi, item_list=items, narrow_loc=narrow_blocks,
-                                             dst_bids=dst_blocks, dm=data_manager)
+            narrow_blocks, dst_blocks = data_manager.split_columns(
+                keys, [BlockType.get_block_type(item) for item in items]
+            )
+            replace_func = functools.partial(
+                _replace_multi, item_list=items, narrow_loc=narrow_blocks, dst_bids=dst_blocks, dm=data_manager
+            )
 
         block_table = df.block_table.mapValues(replace_func)
     elif isinstance(items, DataFrame):
         other_dm = items.data_manager
         operable_fields = other_dm.infer_operable_field_names()
         operable_blocks_loc = other_dm.loc_block(operable_fields)
         block_types = [other_dm.blocks[bid].block_type for bid, _ in operable_blocks_loc]
         if len(keys) != len(operable_fields):
             raise ValueError("Setitem with rhs=DataFrame must have equal len keys")
         narrow_blocks, dst_blocks = data_manager.split_columns(keys, block_types)
-        replace_func = functools.partial(_replace_df, narrow_loc=narrow_blocks, dst_bids=dst_blocks,
-                                         r_blocks_loc=operable_blocks_loc, dm=data_manager)
+        replace_func = functools.partial(
+            _replace_df,
+            narrow_loc=narrow_blocks,
+            dst_bids=dst_blocks,
+            r_blocks_loc=operable_blocks_loc,
+            dm=data_manager,
+        )
         block_table = df.block_table.join(items.block_table, replace_func)
     elif isinstance(items, DTensor):
         if len(keys) != items.shape[1]:
             raise ValueError("Setitem with rhs=DTensor must have equal len keys")
         block_type = BlockType.get_block_type(items.dtype)
         narrow_blocks, dst_blocks = data_manager.split_columns(keys, block_type)
-        replace_func = functools.partial(_replace_tensor, narrow_loc=narrow_blocks,
-                                         dst_bids=dst_blocks, dm=data_manager)
+        replace_func = functools.partial(
+            _replace_tensor, narrow_loc=narrow_blocks, dst_bids=dst_blocks, dm=data_manager
+        )
         block_table = df.block_table.join(items.shardings._data, replace_func)
 
     else:
         raise ValueError(f"Seiitem with rhs_type={type(items)} is not supported")
 
     df.block_table = block_table
     df.data_manager = data_manager
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_stat.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_stat.py`

 * *Files 3% similar despite different names*

```diff
@@ -44,18 +44,15 @@
             if isinstance(block1, torch.Tensor):
                 ret.append(torch.minimum(block1, block2))
             else:
                 ret.append(np.minimum(block1, block2))
 
         return ret
 
-    mapper_func = functools.partial(
-        _mapper,
-        op_bids=operable_blocks
-    )
+    mapper_func = functools.partial(_mapper, op_bids=operable_blocks)
 
     reduce_ret = df.block_table.mapValues(mapper_func).reduce(_reducer)
     return _post_process(reduce_ret, operable_blocks, data_manager)
 
 
 def max(df: "DataFrame") -> "pd.Series":
     data_manager = df.data_manager
@@ -77,18 +74,15 @@
             if isinstance(block1, torch.Tensor):
                 ret.append(torch.maximum(block1, block2))
             else:
                 ret.append(np.maximum(block1, block2))
 
         return ret
 
-    mapper_func = functools.partial(
-        _mapper,
-        op_bids=operable_blocks
-    )
+    mapper_func = functools.partial(_mapper, op_bids=operable_blocks)
 
     reduce_ret = df.block_table.mapValues(mapper_func).reduce(_reducer)
     return _post_process(reduce_ret, operable_blocks, data_manager)
 
 
 def sum(df: DataFrame) -> "pd.Series":
     data_manager = df.data_manager
@@ -100,18 +94,15 @@
             ret.append(blocks[bid].sum(axis=0))
 
         return ret
 
     def _reducer(blocks1, blocks2):
         return [block1 + block2 for block1, block2 in zip(blocks1, blocks2)]
 
-    mapper_func = functools.partial(
-        _mapper,
-        op_bids=operable_blocks
-    )
+    mapper_func = functools.partial(_mapper, op_bids=operable_blocks)
 
     reduce_ret = df.block_table.mapValues(mapper_func).reduce(_reducer)
     return _post_process(reduce_ret, operable_blocks, data_manager)
 
 
 def mean(df: "DataFrame") -> "pd.Series":
     return sum(df) / df.shape[0]
@@ -130,41 +121,33 @@
                 ret.append(
                     (
                         torch.sum(torch.square(block), dim=0, keepdim=True),
                         torch.sum(block, dim=0, keepdim=True),
                     )
                 )
             else:
-                ret.append(
-                    (
-                        np.sum(np.square(block), axis=0),
-                        np.sum(block, axis=0)
-                    )
-                )
+                ret.append((np.sum(np.square(block), axis=0), np.sum(block, axis=0)))
 
         return ret
 
     def _reducer(blocks1, block2):
         ret = []
         for block1, block2 in zip(blocks1, block2):
             if isinstance(block1, torch.Tensor):
                 ret.append((torch.add(block1[0], block2[0]), torch.add(block1[1], block2[1])))
             else:
                 ret.append((np.add(block1[0], block2[0]), np.add(block1[1], block2[1])))
 
         return ret
 
-    mapper_func = functools.partial(
-        _mapper,
-        op_bids=operable_blocks
-    )
+    mapper_func = functools.partial(_mapper, op_bids=operable_blocks)
     reduce_ret = df.block_table.mapValues(mapper_func).reduce(_reducer)
 
     ret_blocks = []
-    for (lhs, rhs) in reduce_ret:
+    for lhs, rhs in reduce_ret:
         if isinstance(lhs, torch.Tensor):
             rhs = torch.mul(torch.square(torch.div(rhs, n)), n)
             ret_blocks.append(torch.div(torch.sub(lhs, rhs), n - ddof))
         else:
             rhs = np.mul(np.square(np.div(rhs, n)), n)
             ret_blocks.append(np.div(np.sub(lhs, rhs), n - ddof))
 
@@ -181,52 +164,52 @@
 
     if unbiased and n < 3:
         field_names = data_manager.infer_operable_field_names()
         return pd.Series([np.nan for _ in range(len(field_names))], index=field_names)
 
     _mean = mean(df)
     m1 = df - _mean
-    m2 = (m1 ** 2).mean()
-    m3 = (m1 ** 3).mean()
+    m2 = (m1**2).mean()
+    m3 = (m1**3).mean()
 
     """
     if abs(value) in m2 < eps=1e-14, we regard it as 0, but eps=1e-14 should be global instead of this file.
     """
     non_zero_mask = abs(m2) >= FLOATING_POINT_ZERO
     m3[~non_zero_mask] = 0
     m2[~non_zero_mask] = 1
 
     if unbiased:
-        return (n * (n - 1)) ** 0.5 / (n - 2) * (m3 / m2 ** 1.5)
+        return (n * (n - 1)) ** 0.5 / (n - 2) * (m3 / m2**1.5)
     else:
-        return m3 / m2 ** 1.5
+        return m3 / m2**1.5
 
 
 def kurt(df: "DataFrame", unbiased=False):
     data_manager = df.data_manager
     n = df.shape[0]
     if unbiased and n < 4:
         field_names = data_manager.infer_operable_field_names()
         return pd.Series([np.nan for _ in range(len(field_names))], index=field_names)
 
     _mean = mean(df)
     m1 = df - _mean
-    m2 = m1 ** 2
-    m4 = m2 ** 2
+    m2 = m1**2
+    m4 = m2**2
     m2 = m2.mean()
     m4 = m4.mean()
 
     non_zero_mask = abs(m2) >= FLOATING_POINT_ZERO
     m4[~non_zero_mask] = 0
     m2[~non_zero_mask] = 1
 
     if unbiased:
         return (n - 1) / ((n - 2) * (n - 3)) * ((n + 1) * m4 / m2**2 - 3 * (n - 1))
     else:
-        return m4 / m2 ** 4 - 3
+        return m4 / m2**4 - 3
 
 
 def variation(df: "DataFrame", ddof=1):
     return std(df, ddof=ddof) / mean(df)
 
 
 def describe(df: "DataFrame", ddof=1, unbiased=False):
@@ -255,8 +238,7 @@
     for idx, bid in enumerate(operable_blocks):
         field_indexes = data_manager.blocks[bid].field_indexes
         for offset, field_index in enumerate(field_indexes):
             loc = field_indexes_loc[field_index]
             ret[loc] = reduce_ret[idx][offset]
 
     return pd.Series(ret, index=field_names)
-
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_transformer.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_transformer.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,18 +18,15 @@
 from typing import List, Tuple
 import torch
 from fate.arch import tensor
 import numpy as np
 from ..manager.data_manager import DataManager
 
 
-def transform_to_tensor(block_table,
-                        data_manager: "DataManager",
-                        dtype=None,
-                        partition_order_mappings=None):
+def transform_to_tensor(block_table, data_manager: "DataManager", dtype=None, partition_order_mappings=None):
     def _merge_blocks(src_blocks, bids=None, fields=None):
         if len(bids) == 1:
             bid = bids[0]
             t = src_blocks[bid]
         else:
             i = 0
             tensors = []
@@ -64,56 +61,47 @@
         if not data_manager.get_block(block_id).is_numeric:
             raise ValueError("Transform to distributed tensor should ensure every field is numeric")
 
     if is_phe_tensor:
         if len(block_indexes) > 1:
             raise ValueError("To use as_tensor of phe_tensor type, it should be only single column")
         block_id = block_indexes[0]
-        _convert_to_phe_tensor_func = functools.partial(_convert_to_phe_tensor,
-                                                        bid=block_id,
-                                                        dm=data_manager)
+        _convert_to_phe_tensor_func = functools.partial(_convert_to_phe_tensor, bid=block_id, dm=data_manager)
         phe_table = block_table.mapValues(_convert_to_phe_tensor_func)
 
         shape_table = block_table.mapValues(lambda blocks: (len(blocks[0]), 1))
         shapes = [shape_obj for k, shape_obj in sorted(shape_table.collect())]
 
-        return tensor.DTensor.from_sharding_table(phe_table,
-                                                  shapes=shapes,
-                                                  dtype=data_manager.get_block(block_id).dtype,
-                                                  device=data_manager.get_block(block_id).device)
+        return tensor.DTensor.from_sharding_table(
+            phe_table,
+            shapes=shapes,
+            dtype=data_manager.get_block(block_id).dtype,
+            device=data_manager.get_block(block_id).device,
+        )
     else:
         field_names = data_manager.infer_operable_field_names()
         fields_loc = data_manager.loc_block(field_names)
 
-        _merged_func = functools.partial(
-            _merge_blocks,
-            bids=block_indexes,
-            fields=fields_loc
-        )
+        _merged_func = functools.partial(_merge_blocks, bids=block_indexes, fields=fields_loc)
         merged_table = block_table.mapValues(_merged_func)
 
         shape_table = merged_table.mapValues(lambda v: v.shape)
         shapes = [shape_obj for k, shape_obj in sorted(shape_table.collect())]
 
-        return tensor.DTensor.from_sharding_table(merged_table,
-                                                  shapes=shapes)
+        return tensor.DTensor.from_sharding_table(merged_table, shapes=shapes)
 
 
 def transform_block_table_to_list(block_table, data_manager):
     fields_loc = data_manager.get_fields_loc()
-    transform_block_to_list_func = functools.partial(
-        transform_block_to_list,
-        fields_loc=fields_loc
-    )
+    transform_block_to_list_func = functools.partial(transform_block_to_list, fields_loc=fields_loc)
 
     return block_table.mapValues(transform_block_to_list_func)
 
 
 def transform_block_to_list(blocks, fields_loc):
-
     if blocks[0].shape[0] == 0:
         return []
 
     i = 0
     dst_list = None
     lines = 0
     while i < len(fields_loc):
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_unary_operator.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_unary_operator.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,13 +23,8 @@
     data_manager = df.data_manager
     block_indexes = data_manager.infer_operable_blocks()
     for bid in block_indexes:
         if data_manager.blocks[bid] != BlockType.bool:
             raise ValueError("to use ~df syntax, data types should be bool")
 
     block_table = unary_operate(df.block_table, operator.invert, block_indexes)
-    return type(df)(
-        df.ctx,
-        block_table,
-        df.partition_order_mappings,
-        data_manager.duplicate()
-    )
+    return type(df)(df.ctx, block_table, df.partition_order_mappings, data_manager.duplicate())
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/_where.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/_where.py`

 * *Files 4% similar despite different names*

```diff
@@ -49,39 +49,31 @@
     for name in column_names:
         bid = data_manager.loc_block(name, with_offset=False)
         if not BlockType.is_float(data_manager.get_block(bid).block_type):
             need_promoted = True
             break
 
     if not need_promoted:
-        block_table = _where_float_type(df.block_table, other.block_table,
-                                        data_manager, other.data_manager, column_names)
-        return DataFrame(
-            df._ctx,
-            block_table,
-            df.partition_order_mappings,
-            data_manager.duplicate()
+        block_table = _where_float_type(
+            df.block_table, other.block_table, data_manager, other.data_manager, column_names
         )
+        return DataFrame(df._ctx, block_table, df.partition_order_mappings, data_manager.duplicate())
 
 
 def _get_false_columns(df: DataFrame):
     block_table = df.block_table
     data_manager = df.data_manager
     block_index_set = set(data_manager.infer_operable_blocks())
 
     false_table = block_table.mapValues(
-        lambda blocks: [
-            block.all(axis=0) if bid in block_index_set else []
-            for bid, block in enumerate(blocks)
-        ]
+        lambda blocks: [block.all(axis=0) if bid in block_index_set else [] for bid, block in enumerate(blocks)]
     )
 
     false_values = false_table.reduce(
-        lambda blocks1, blocks2:
-        [
+        lambda blocks1, blocks2: [
             block1 & block2 if bid in block_index_set else []
             for bid, (block1, block2) in enumerate(zip(blocks1, blocks2))
         ]
     )
 
     false_columns = set()
     column_names = data_manager.infer_operable_field_names()
@@ -93,18 +85,17 @@
         elif isinstance(false_values[_bid], np.ndarray):
             if not false_values[_bid][_offset]:
                 false_columns.add(name)
 
     return false_columns
 
 
-def _where_float_type(l_block_table, r_block_table,
-                      l_data_manager: "DataManager",
-                      r_data_manager: "DataManager",
-                      column_names: List[str]):
+def _where_float_type(
+    l_block_table, r_block_table, l_data_manager: "DataManager", r_data_manager: "DataManager", column_names: List[str]
+):
     l_loc_info = [l_data_manager.loc_block(name) for name in column_names]
     r_loc_info = [r_data_manager.loc_block(name) for name in column_names]
 
     def __convert_na(l_blocks, r_blocks):
         ret_blocks = []
         for block in l_blocks:
             if isinstance(block, torch.Tensor):
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/ops/utils/series_align.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/utils/series_align.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,21 +12,21 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 import pandas as pd
 from typing import List
 
 
-def series_to_ndarray(series_obj: "pd.Series", fields_to_align: List[str]=None):
+def series_to_ndarray(series_obj: "pd.Series", fields_to_align: List[str] = None):
     if isinstance(series_obj.index, pd.RangeIndex) or not fields_to_align:
         return series_obj.values
     else:
         if len(series_obj) != len(fields_to_align):
             raise ValueError(f"Can't not align fields, src={fields_to_align}, dst={series_obj}")
 
         indexer = series_obj.index.get_indexer(fields_to_align)
 
         return series_obj[indexer].values
 
 
-def series_to_list(series_obj: "pd.Series", fields_to_align: List[str]=None):
+def series_to_list(series_obj: "pd.Series", fields_to_align: List[str] = None):
     return series_to_ndarray(series_obj, fields_to_align).tolist()
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/utils/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/utils/_auto_column_name_generated.py` & `pyfate-2.1.0/fate/arch/dataframe/utils/_auto_column_name_generated.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,9 +10,10 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 DEFAULT_COL_NAME_PREFIX = "default_col_"
 
+
 def generated_default_column_names(column_length):
     return [DEFAULT_COL_NAME_PREFIX + str(i) for i in range(column_length)]
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/utils/_dataloader.py` & `pyfate-2.1.0/fate/arch/dataframe/utils/_dataloader.py`

 * *Files 2% similar despite different names*

```diff
@@ -123,18 +123,18 @@
             if self._mode in ["homo", "local"] or self._role == "guest":
                 indexer = sorted(list(self._dataset.get_indexer(target="sample_id").collect()))
                 if self._shuffle:
                     random.seed = self._random_state
                     random.shuffle(indexer)
 
                 for i, iter_ctx in self._ctx.sub_ctx("dataloader_batch").ctxs_range(self._batch_num):
-                    batch_indexer = indexer[self._batch_size * i: self._batch_size * (i + 1)]
-                    batch_indexer = self._ctx.computing.parallelize(batch_indexer,
-                                                                    include_key=True,
-                                                                    partition=self._dataset.block_table.partitions)
+                    batch_indexer = indexer[self._batch_size * i : self._batch_size * (i + 1)]
+                    batch_indexer = self._ctx.computing.parallelize(
+                        batch_indexer, include_key=True, partition=self._dataset.block_table.num_partitions
+                    )
 
                     sub_frame = self._dataset.loc(batch_indexer, preserve_order=False)
 
                     if self._mode == "hetero" and self._role == "guest":
                         iter_ctx.hosts.put("batch_indexes", sub_frame.get_indexer(target="sample_id"))
 
                     self._batch_splits.append(BatchEncoding(sub_frame, batch_id=i))
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/utils/_id_generator.py` & `pyfate-2.1.0/fate/arch/dataframe/utils/_id_generator.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,12 +15,12 @@
 #
 import time
 import random
 import hashlib
 
 
 def generate_sample_id(n, prefix):
-    return [hashlib.sha256(bytes(prefix + str(i), encoding='utf-8')).hexdigest() for i in range(n)]
+    return [hashlib.sha256(bytes(prefix + str(i), encoding="utf-8")).hexdigest() for i in range(n)]
 
 
 def generate_sample_id_prefix():
     return str(time.time()) + str(random.randint(1000000, 9999999))
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/utils/_k_fold.py` & `pyfate-2.1.0/fate/arch/dataframe/utils/_k_fold.py`

 * *Files 10% similar despite different names*

```diff
@@ -14,21 +14,15 @@
 #  limitations under the License.
 #
 from .._dataframe import DataFrame
 from sklearn.model_selection import KFold as sk_KFold
 
 
 class KFold(object):
-    def __init__(self,
-                 ctx,
-                 mode="hetero",
-                 role="guest",
-                 n_splits=5,
-                 shuffle=False,
-                 random_state=None):
+    def __init__(self, ctx, mode="hetero", role="guest", n_splits=5, shuffle=False, random_state=None):
         self._ctx = ctx
         self._mode = mode
         self._role = role
         self._n_splits = n_splits
         self._shuffle = shuffle
         self._random_state = random_state
 
@@ -61,27 +55,28 @@
         kf = sk_KFold(n_splits=self._n_splits, shuffle=self._shuffle, random_state=self._random_state)
         indexer = list(df.get_indexer(target="sample_id").collect())
 
         for train, test in kf.split(indexer):
             train_indexer = [indexer[idx] for idx in train]
             test_indexer = [indexer[idx] for idx in test]
 
-            train_indexer = self._ctx.computing.parallelize(train_indexer,
-                                                          include_key=True,
-                                                          partition=df.block_table.partitions)
-
-            test_indexer = self._ctx.computing.parallelize(test_indexer,
-                                                           include_key=True,
-                                                           partition=df.block_table.partitions)
+            train_indexer = self._ctx.computing.parallelize(
+                train_indexer, include_key=True, partition=df.block_table.num_partitions
+            )
+
+            test_indexer = self._ctx.computing.parallelize(
+                test_indexer, include_key=True, partition=df.block_table.num_partitions
+            )
 
             train_frame = df.loc(train_indexer)
             test_frame = df.loc(test_indexer)
 
             if return_indexer:
-                yield  train_frame, test_frame, \
-                       train_frame.get_indexer(target="sample_id"), test_frame.get_indexer(target="sample_id")
+                yield train_frame, test_frame, train_frame.get_indexer(target="sample_id"), test_frame.get_indexer(
+                    target="sample_id"
+                )
             else:
                 yield train_frame, test_frame
 
     def _check_param(self):
         if not isinstance(self._n_splits, int) or self._n_splits < 2:
             raise ValueError("n_splits should be positive integer >= 2")
```

### Comparing `pyfate-2.0.0b0/fate/arch/dataframe/utils/_sample.py` & `pyfate-2.1.0/fate/arch/dataframe/utils/_sample.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,30 +25,31 @@
 SAMPLE_INDEX_TAG = "sample_index"
 REGENERATED_IDS = "regenerated_ids"
 
 
 def local_sample(
     ctx,
     df: DataFrame,
-    n: int=None,
+    n: int = None,
     frac: Union[float, Dict[Any, float]] = None,
     replace: bool = True,
-    random_state=None
+    random_state=None,
 ):
     return _sample_guest(ctx, df, n, frac, replace, random_state, sync=False)
 
 
 def federated_sample(
-        ctx,
-        df: DataFrame,
-        n: int = None,
-        frac: Union[float, Dict[Any, float]] = None,
-        replace: bool = True,
-        random_state=None,
-        role: str = "guest"):
+    ctx,
+    df: DataFrame,
+    n: int = None,
+    frac: Union[float, Dict[Any, float]] = None,
+    replace: bool = True,
+    random_state=None,
+    role: str = "guest",
+):
     if role == "guest":
         return _sample_guest(ctx, df, n, frac, replace, random_state, sync=True)
     else:
         return _federated_sample_host(ctx, df)
 
 
 def _sample_guest(
@@ -67,31 +68,30 @@
         if isinstance(frac, float):
             if frac > 1:
                 raise ValueError(f"sample's parameter frac={frac} should <= 1.0")
             n = max(1, int(frac * df.shape[0]))
         else:
             for k, f in frac.items():
                 if f > 1 and replace is False:
-                    raise ValueError(f"sample's parameter frac's label={k}, fraction={f} "
-                                     f"should <= 1.0 if replace=False")
+                    raise ValueError(
+                        f"sample's parameter frac's label={k}, fraction={f} " f"should <= 1.0 if replace=False"
+                    )
 
     if n is not None:
         if n > df.shape[0] and replace is False:
             raise ValueError(f"sample's parameter n={n} should <= data_size={df.shape[0]} if replace=False")
 
         if replace:
             choices = resample(list(range(df.shape[0])), replace=True, n_samples=n, random_state=random_state)
             indexer = list(df.get_indexer(target="sample_id").collect())
             regenerated_sample_id_prefix = generate_sample_id_prefix()
             regenerated_ids = generate_sample_id(n, regenerated_sample_id_prefix)
-            choice_with_regenerated_ids = _agg_choices(ctx,
-                                                       indexer,
-                                                       choices,
-                                                       regenerated_ids,
-                                                       df.block_table.partitions)
+            choice_with_regenerated_ids = _agg_choices(
+                ctx, indexer, choices, regenerated_ids, df.block_table.num_partitions
+            )
 
             if sync:
                 ctx.hosts.put(REGENERATED_TAG, True)
                 ctx.hosts.put(REGENERATED_IDS, choice_with_regenerated_ids)
 
             regenerated_raw_table = _regenerated_sample_ids(df, choice_with_regenerated_ids)
             sample_df = _convert_raw_table_to_df(df._ctx, regenerated_raw_table, df.data_manager)
@@ -113,20 +113,22 @@
 
         if up_sample:
             regenerated_sample_id_prefix = generate_sample_id_prefix()
             choice_with_regenerated_ids = None
             for label, f in frac.items():
                 label_df = df.iloc(df.label == label)
                 label_n = max(1, int(label_df.shape[0] * f))
-                choices = resample(list(range(label_df.shape[0])), replace=True,
-                                   n_samples=label_n, random_state=random_state)
+                choices = resample(
+                    list(range(label_df.shape[0])), replace=True, n_samples=label_n, random_state=random_state
+                )
                 label_indexer = list(label_df.get_indexer(target="sample_id").collect())
                 regenerated_ids = generate_sample_id(label_n, regenerated_sample_id_prefix)
-                label_choice_with_regenerated_ids = _agg_choices(ctx, label_indexer, choices,
-                                                                 regenerated_ids, df.block_table.partitions)
+                label_choice_with_regenerated_ids = _agg_choices(
+                    ctx, label_indexer, choices, regenerated_ids, df.block_table.num_partitions
+                )
                 if choice_with_regenerated_ids is None:
                     choice_with_regenerated_ids = label_choice_with_regenerated_ids
                 else:
                     choice_with_regenerated_ids = choice_with_regenerated_ids.union(label_choice_with_regenerated_ids)
 
             if sync:
                 ctx.hosts.put(REGENERATED_TAG, True)
@@ -152,18 +154,15 @@
                 sample_indexer = sample_df.get_indexer(target="sample_id")
                 ctx.hosts.put(REGENERATED_TAG, False)
                 ctx.hosts.put(SAMPLE_INDEX_TAG, sample_indexer)
 
     return sample_df
 
 
-def _federated_sample_host(
-    ctx,
-    df: DataFrame
-):
+def _federated_sample_host(ctx, df: DataFrame):
     regenerated_tag = ctx.guest.get(REGENERATED_TAG)
     if regenerated_tag is False:
         sample_indexer = ctx.guest.get(SAMPLE_INDEX_TAG)
         sample_df = df.loc(sample_indexer, preserve_order=True)
     else:
         regenerated_ids = ctx.guest.get(REGENERATED_IDS)
         regenerated_raw_table = _regenerated_sample_ids(df, regenerated_ids)
@@ -173,44 +172,32 @@
         sample_df = sample_df.loc(sample_indexer, preserve_order=True)
 
     return sample_df
 
 
 def _regenerated_sample_ids(df, regenerated_ids):
     from ..ops._indexer import regenerated_sample_id
+
     regenerated_raw_table = regenerated_sample_id(df.block_table, regenerated_ids, df.data_manager)
 
     return regenerated_raw_table
 
 
-def _convert_raw_table_to_df(
-    ctx,
-    table,
-    data_manager
-):
+def _convert_raw_table_to_df(ctx, table, data_manager):
     from ..ops._indexer import get_partition_order_by_raw_table
     from ..ops._dimension_scaling import to_blocks
+
     partition_order_mapping = get_partition_order_by_raw_table(table, data_manager.block_row_size)
     to_block_func = functools.partial(to_blocks, dm=data_manager, partition_mappings=partition_order_mapping)
-    block_table = table.mapPartitions(to_block_func,
-                                      use_previous_behavior=False)
+    block_table = table.mapPartitions(to_block_func, use_previous_behavior=False)
+
+    return DataFrame(ctx, block_table, partition_order_mapping, data_manager)
+
 
-    return DataFrame(
-        ctx,
-        block_table,
-        partition_order_mapping,
-        data_manager
-    )
-
-
-def _agg_choices(ctx,
-                 indexer,
-                 choices,
-                 regenerated_ids,
-                 partition):
+def _agg_choices(ctx, indexer, choices, regenerated_ids, partition):
     """
     indexer: (sample_id, (partition_id, block_offset))
     """
     choice_dict = dict()
     choice_indexer = []
     for idx, choice in enumerate(choices):
         if choice not in choice_dict:
@@ -220,10 +207,8 @@
 
         choice_indexer[choice_dict[choice]].append(regenerated_ids[idx])
 
     for choice, idx in choice_dict.items():
         choice_regenerated_sample_ids = choice_indexer[idx]
         choice_indexer[idx] = (indexer[choice][0], choice_regenerated_sample_ids)
 
-    return ctx.computing.parallelize(choice_indexer,
-                                     include_key=True,
-                                     partition=partition)
+    return ctx.computing.parallelize(choice_indexer, include_key=True, partition=partition)
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/__init__.py` & `pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,10 +8,10 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from ._type import FederationDataType
+from ._federation import RabbitmqFederation
 
-__all__ = ["FederationDataType"]
+__all__ = ["RabbitmqFederation"]
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/_datastream.py` & `pyfate-2.1.0/fate/arch/federation/message_queue/_datastream.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/_federation.py` & `pyfate-2.1.0/fate/arch/federation/message_queue/_federation.py`

 * *Files 13% similar despite different names*

```diff
@@ -15,291 +15,213 @@
 #
 
 
 import json
 import logging
 import sys
 import typing
-from pickle import dumps as p_dumps
-from pickle import loads as p_loads
+from typing import List
 
-from fate.arch.abc import CTableABC, FederationEngine, PartyMeta
-
-from ..federation import FederationDataType
-from ..federation._datastream import Datastream
-from ._gc import GarbageCollector
+from fate.arch.computing.api import KVTableContext
+from fate.arch.federation.api import Federation, PartyMeta, TableMeta
+from ._datastream import Datastream
 from ._parties import Party
 
 LOGGER = logging.getLogger(__name__)
 
-NAME_DTYPE_TAG = "<dtype>"
 _SPLIT_ = "^"
 
 
-def _get_splits(obj, max_message_size):
-    obj_bytes = p_dumps(obj, protocol=4)
-    byte_size = len(obj_bytes)
-    num_slice = (byte_size - 1) // max_message_size + 1
-    if num_slice <= 1:
-        return obj, num_slice
-    else:
-        _max_size = max_message_size
-        kv = [(i, obj_bytes[slice(i * _max_size, (i + 1) * _max_size)]) for i in range(num_slice)]
-        return kv, num_slice
-
-
-class FederationBase(FederationEngine):
+class MessageQueueBasedFederation(Federation):
     def __init__(
         self,
         session_id,
-        computing_session,
+        computing_session: KVTableContext,
         party: PartyMeta,
         parties: typing.List[PartyMeta],
         mq,
         max_message_size,
         conf=None,
+        default_partition_num=None,
     ):
-        self._session_id = session_id
         self._mq = mq
         self._topic_map = {}
         self._channels_map = {}
-        self._name_dtype_map = {}
         self._message_cache = {}
         self._max_message_size = max_message_size
+        if self._max_message_size is None:
+            self._max_message_size = self.get_default_max_message_size()
+        self._default_partition_num = default_partition_num
         self._conf = conf
-        self.get_gc: GarbageCollector = GarbageCollector()
-        self.remote_gc: GarbageCollector = GarbageCollector()
-        self.local_party = party
-        self.parties = parties
         self.computing_session = computing_session
 
-        # temp
-        self._party = Party(party[0], party[1])
-
-    def pull(self, name: str, tag: str, parties: typing.List[PartyMeta]) -> typing.List:
-        # wrap as party
-        _parties = [Party(role=p[0], party_id=p[1]) for p in parties]
-        log_str = f"[federation.get](name={name}, tag={tag}, parties={parties})"
-        LOGGER.debug(f"[{log_str}]start to get")
-
-        _name_dtype_keys = [_SPLIT_.join([party.role, party.party_id, name, tag, "get"]) for party in _parties]
+        super().__init__(session_id, party, parties)
 
-        if _name_dtype_keys[0] not in self._name_dtype_map:
-            party_topic_infos = self._get_party_topic_infos(_parties, dtype=NAME_DTYPE_TAG)
-            channel_infos = self._get_channels(party_topic_infos=party_topic_infos)
-            rtn_dtype = []
-            for i, info in enumerate(channel_infos):
-                obj = self._receive_obj(info, name, tag=_SPLIT_.join([tag, NAME_DTYPE_TAG]))
-                rtn_dtype.append(obj)
-                LOGGER.debug(f"[federation.get] _name_dtype_keys: {_name_dtype_keys}, dtype: {obj}")
-
-            for k in _name_dtype_keys:
-                if k not in self._name_dtype_map:
-                    self._name_dtype_map[k] = rtn_dtype[0]
-
-        rtn_dtype = self._name_dtype_map[_name_dtype_keys[0]]
+        # TODO: remove this
+        self._party = Party(party[0], party[1])
 
-        rtn = []
-        dtype = rtn_dtype.get("dtype", None)
-        partitions = rtn_dtype.get("partitions", None)
+    def _get_channel(
+        self,
+        topic_pair,
+        src_party_id,
+        src_role,
+        dst_party_id,
+        dst_role,
+        mq=None,
+        conf: dict = None,
+    ):
+        raise NotImplementedError()
 
-        if dtype == FederationDataType.TABLE or dtype == FederationDataType.SPLIT_OBJECT:
-            party_topic_infos = self._get_party_topic_infos(_parties, name, partitions=partitions)
-            for i in range(len(party_topic_infos)):
-                party = _parties[i]
-                role = party.role
-                party_id = party.party_id
-                topic_infos = party_topic_infos[i]
-                receive_func = self._get_partition_receive_func(
-                    name=name,
-                    tag=tag,
-                    src_party_id=self.local_party[1],
-                    src_role=self.local_party[0],
-                    dst_party_id=party_id,
-                    dst_role=role,
-                    topic_infos=topic_infos,
-                    mq=self._mq,
-                    conf=self._conf,
-                )
+    def _maybe_create_topic_and_replication(self, party, topic_suffix):
+        # gen names
+        raise NotImplementedError()
 
-                table = self.computing_session.parallelize(range(partitions), partitions, include_key=False)
-                table = table.mapPartitionsWithIndex(receive_func)
+    def _get_consume_message(self, channel_info):
+        raise NotImplementedError()
 
-                # add gc
-                self.get_gc.register_clean_action(name, tag, table, "__del__", {})
+    def _consume_ack(self, channel_info, id):
+        return
 
-                LOGGER.debug(f"[{log_str}]received table({i + 1}/{len(parties)}), party: {parties[i]} ")
-                if dtype == FederationDataType.TABLE:
-                    rtn.append(table)
-                else:
-                    obj_bytes = b"".join(map(lambda t: t[1], sorted(table.collect(), key=lambda x: x[0])))
-                    obj = p_loads(obj_bytes)
-                    rtn.append(obj)
+    def get_default_max_message_size(self):
+        if self._max_message_size is None:
+            return super().get_default_max_message_size()
         else:
-            party_topic_infos = self._get_party_topic_infos(_parties, name)
-            channel_infos = self._get_channels(party_topic_infos=party_topic_infos)
-            for i, info in enumerate(channel_infos):
-                obj = self._receive_obj(info, name, tag)
-                LOGGER.debug(f"[{log_str}]received obj({i + 1}/{len(parties)}), party: {parties[i]} ")
-                rtn.append(obj)
-
-        LOGGER.debug(f"[{log_str}]finish to get")
-        return rtn
+            return self._max_message_size
 
-    def push(self, v, name: str, tag: str, parties: typing.List[PartyMeta]):
+    def get_default_partition_num(self):
+        if self._default_partition_num is None:
+            return super().get_default_partition_num()
+        else:
+            return self._default_partition_num
 
+    def _pull_bytes(self, name: str, tag: str, parties: typing.List[PartyMeta]) -> typing.List:
         _parties = [Party(role=p[0], party_id=p[1]) for p in parties]
-        log_str = f"[federation.remote](name={name}, tag={tag}, parties={parties})"
-
-        _name_dtype_keys = [_SPLIT_.join([party.role, party.party_id, name, tag, "remote"]) for party in _parties]
-
-        if _name_dtype_keys[0] not in self._name_dtype_map:
-            party_topic_infos = self._get_party_topic_infos(_parties, dtype=NAME_DTYPE_TAG)
-            channel_infos = self._get_channels(party_topic_infos=party_topic_infos)
-
-            if not isinstance(v, CTableABC):
-                v, num_slice = _get_splits(v, self._max_message_size)
-                if num_slice > 1:
-                    v = self.computing_session.parallelize(data=v, partition=1, include_key=True)
-                    body = {
-                        "dtype": FederationDataType.SPLIT_OBJECT,
-                        "partitions": v.partitions,
-                    }
-                else:
-                    body = {"dtype": FederationDataType.OBJECT}
-
-            else:
-                body = {"dtype": FederationDataType.TABLE, "partitions": v.partitions}
+        rtn = []
+        party_topic_infos = self._get_party_topic_infos_by_name(_parties, name)
+        channel_infos = self._get_channels(party_topic_infos=party_topic_infos)
+        for i, info in enumerate(channel_infos):
+            obj = self._receive_obj(info, name, tag)
+            rtn.append(obj)
 
-            LOGGER.debug(f"[federation.remote] _name_dtype_keys: {_name_dtype_keys}, dtype: {body}")
-            self._send_obj(
-                name=name,
-                tag=_SPLIT_.join([tag, NAME_DTYPE_TAG]),
-                data=p_dumps(body),
-                channel_infos=channel_infos,
-            )
+        return rtn
 
-            for k in _name_dtype_keys:
-                if k not in self._name_dtype_map:
-                    self._name_dtype_map[k] = body
-
-        if isinstance(v, CTableABC):
-            total_size = v.count()
-            partitions = v.partitions
-            LOGGER.debug(f"[{log_str}]start to remote table, total_size={total_size}, partitions={partitions}")
-
-            party_topic_infos = self._get_party_topic_infos(_parties, name, partitions=partitions)
-            # add gc
-            self.remote_gc.register_clean_action(name, tag, v, "__del__", {})
+    def _push_bytes(
+        self,
+        v: bytes,
+        name: str,
+        tag: str,
+        parties: List[PartyMeta],
+    ):
+        _parties = [Party(role=p[0], party_id=p[1]) for p in parties]
+        party_topic_infos = self._get_party_topic_infos_by_name(_parties, name)
+        channel_infos = self._get_channels(party_topic_infos=party_topic_infos)
+        self._send_obj(name=name, tag=tag, data=v, channel_infos=channel_infos)
 
-            send_func = self._get_partition_send_func(
+    def _pull_table(
+        self, name: str, tag: str, parties: typing.List[PartyMeta], table_metas: List[TableMeta]
+    ) -> typing.List:
+        rtn = []
+        for table_meta, party in zip(table_metas, parties):
+            party = Party(role=party[0], party_id=party[1])
+            party_topic_infos = self._get_party_topic_infos_by_name_and_partitions(
+                [party], name, partitions=table_meta.num_partitions
+            )[0]
+            topic_infos = party_topic_infos
+            receive_func = self._get_partition_receive_func(
                 name=name,
                 tag=tag,
-                partitions=partitions,
-                party_topic_infos=party_topic_infos,
                 src_party_id=self.local_party[1],
                 src_role=self.local_party[0],
+                dst_party_id=party.party_id,
+                dst_role=party.role,
+                topic_infos=topic_infos,
                 mq=self._mq,
-                max_message_size=self._max_message_size,
                 conf=self._conf,
             )
-            # noinspection PyProtectedMember
-            v.mapPartitionsWithIndex(send_func)
-        else:
-            LOGGER.debug(f"[{log_str}]start to remote obj")
-            party_topic_infos = self._get_party_topic_infos(_parties, name)
-            channel_infos = self._get_channels(party_topic_infos=party_topic_infos)
-            self._send_obj(name=name, tag=tag, data=p_dumps(v), channel_infos=channel_infos)
+            table = self.computing_session.parallelize(
+                range(table_meta.num_partitions),
+                include_key=False,
+                partition=table_meta.num_partitions,
+            )
+            table = table.mapPartitionsWithIndexNoSerdes(
+                receive_func,
+                output_key_serdes_type=table_meta.key_serdes_type,
+                output_value_serdes_type=table_meta.value_serdes_type,
+                output_partitioner_type=table_meta.partitioner_type,
+            )
+            rtn.append(table)
+        return rtn
 
-        LOGGER.debug(f"[{log_str}]finish to remote")
+    def _push_table(self, table, name: str, tag: str, parties: typing.List[PartyMeta]):
+        _parties = [Party(role=p[0], party_id=p[1]) for p in parties]
+        party_topic_infos = self._get_party_topic_infos_by_name_and_partitions(
+            _parties, name, partitions=table.num_partitions
+        )
+        send_func = self._get_partition_send_func(
+            name=name,
+            tag=tag,
+            partitions=table.num_partitions,
+            party_topic_infos=party_topic_infos,
+            src_party_id=self.local_party[1],
+            src_role=self.local_party[0],
+            mq=self._mq,
+            max_message_size=self._max_message_size,
+            conf=self._conf,
+        )
+        # noinspection PyProtectedMember
+        table.mapPartitionsWithIndexNoSerdes(
+            send_func, output_key_serdes_type=0, output_value_serdes_type=0, output_partitioner_type=0
+        )
 
     @property
     def session_id(self) -> str:
         return self._session_id
 
     def __getstate__(self):
         pass
 
-    def destroy(self, parties):
-        raise NotImplementedError()
+    def _get_party_topic_infos_by_name(self, parties: typing.List[Party], name: str):
+        topic_infos = [[self._get_or_create_topic(party, "-".join([name]))] for party in parties]
+        return topic_infos
 
-    def _get_party_topic_infos(
-        self, parties: typing.List[Party], name=None, partitions=None, dtype=None
+    def _get_party_topic_infos_by_name_and_partitions(
+        self, parties: typing.List[Party], name=None, partitions=None
     ) -> typing.List:
-        topic_infos = [self._get_or_create_topic(party, name, partitions, dtype) for party in parties]
+        topic_infos = [
+            [self._get_or_create_topic(party, "-".join([name, str(i)])) for i in range(partitions)]
+            for party in parties
+        ]
         return topic_infos
 
-    def _maybe_create_topic_and_replication(self, party, topic_suffix):
-        # gen names
-        raise NotImplementedError()
-
-    def _get_or_create_topic(self, party: Party, name=None, partitions=None, dtype=None) -> typing.Tuple:
-        topic_key_list = []
-        topic_infos = []
-
-        if dtype is not None:
-            topic_key = _SPLIT_.join([party.role, party.party_id, dtype, dtype])
-            topic_key_list.append(topic_key)
-        else:
-            if partitions is not None:
-                for i in range(partitions):
-                    topic_key = _SPLIT_.join([party.role, party.party_id, name, str(i)])
-                    topic_key_list.append(topic_key)
-            elif name is not None:
-                topic_key = _SPLIT_.join([party.role, party.party_id, name])
-                topic_key_list.append(topic_key)
-            else:
-                topic_key = _SPLIT_.join([party.role, party.party_id])
-                topic_key_list.append(topic_key)
-
-        for topic_key in topic_key_list:
-            if topic_key not in self._topic_map:
-                topic_key_splits = topic_key.split(_SPLIT_)
-                topic_suffix = "-".join(topic_key_splits[2:])
-                topic_pair = self._maybe_create_topic_and_replication(party, topic_suffix)
-                self._topic_map[topic_key] = topic_pair
+    def _get_or_create_topic(self, party, topic_suffix) -> typing.Tuple[Party, str, typing.Any]:
+        if (party, topic_suffix) not in self._topic_map:
+            topic_pair = self._maybe_create_topic_and_replication(party, topic_suffix)
+            self._topic_map[(party, topic_suffix)] = topic_pair
 
-            topic_pair = self._topic_map[topic_key]
-            topic_infos.append((topic_key, topic_pair))
-
-        return topic_infos
-
-    def _get_channel(
-        self,
-        topic_pair,
-        src_party_id,
-        src_role,
-        dst_party_id,
-        dst_role,
-        mq=None,
-        conf: dict = None,
-    ):
-        raise NotImplementedError()
+        topic_pair = self._topic_map[(party, topic_suffix)]
+        return party, topic_suffix, topic_pair
 
     def _get_channels(self, party_topic_infos):
         channel_infos = []
         for e in party_topic_infos:
-            for topic_key, topic_pair in e:
-                topic_key_splits = topic_key.split(_SPLIT_)
-                role = topic_key_splits[0]
-                party_id = topic_key_splits[1]
-                info = self._channels_map.get(topic_key)
+            for party, topic_suffix, topic_pair in e:
+                info = self._channels_map.get((party, topic_suffix))
 
                 if info is None:
                     info = self._get_channel(
                         topic_pair=topic_pair,
                         src_party_id=self.local_party[1],
                         src_role=self.local_party[0],
-                        dst_party_id=party_id,
-                        dst_role=role,
+                        dst_party_id=party.party_id,
+                        dst_role=party.role,
                         mq=self._mq,
                         conf=self._conf,
                     )
 
-                    self._channels_map[topic_key] = info
+                    self._channels_map[(party, topic_suffix)] = info
                 channel_infos.append(info)
         return channel_infos
 
     def _get_channels_index(
         self,
         index,
         party_topic_infos,
@@ -307,24 +229,21 @@
         src_role,
         mq=None,
         conf: dict = None,
     ):
         channel_infos = []
         for e in party_topic_infos:
             # select specified topic_info for a party
-            topic_key, topic_pair = e[index]
-            topic_key_splits = topic_key.split(_SPLIT_)
-            role = topic_key_splits[0]
-            party_id = topic_key_splits[1]
+            party, topic_suffix, topic_pair = e[index]
             info = self._get_channel(
                 topic_pair=topic_pair,
                 src_party_id=src_party_id,
                 src_role=src_role,
-                dst_party_id=party_id,
-                dst_role=role,
+                dst_party_id=party.party_id,
+                dst_role=party.role,
                 mq=mq,
                 conf=conf,
             )
             channel_infos.append(info)
         return channel_infos
 
     def _send_obj(self, name, tag, data, channel_infos):
@@ -350,15 +269,15 @@
             properties = {
                 "content_type": "application/json",
                 "app_id": info._dst_party_id,
                 "message_id": name,
                 "correlation_id": tag,
                 "headers": headers,
             }
-            print(f"[federation._send_kv]info: {info}, properties: {properties}.")
+            LOGGER.debug(f"[federation._send_kv]info: {info}, properties: {properties}.")
             info.produce(body=data, properties=properties)
 
     def _get_partition_send_func(
         self,
         name,
         tag,
         partitions,
@@ -412,18 +331,18 @@
         datastream = Datastream()
         base_message_key = str(index)
         message_key_idx = 0
         count = 0
 
         for k, v in kvs:
             count += 1
-            el = {"k": p_dumps(k).hex(), "v": p_dumps(v).hex()}
+            el = {"k": k.hex(), "v": v.hex()}
             # roughly caculate the size of package to avoid serialization ;)
             if datastream.get_size() + sys.getsizeof(el["k"]) + sys.getsizeof(el["v"]) >= max_message_size:
-                print(f"[federation._partition_send]The size of message is: {datastream.get_size()}")
+                LOGGER.debug(f"[federation._partition_send]The size of message is: {datastream.get_size()}")
                 message_key_idx += 1
                 message_key = base_message_key + "_" + str(message_key_idx)
                 self._send_kv(
                     name=name,
                     tag=tag,
                     data=datastream.get_data().encode(),
                     channel_infos=channel_infos,
@@ -443,63 +362,42 @@
             data=datastream.get_data().encode(),
             channel_infos=channel_infos,
             partition_size=count,
             partitions=partitions,
             message_key=message_key,
         )
 
-        return [(index, 1)]
-
-    def _get_message_cache_key(self, name, tag, party_id, role):
-        cache_key = _SPLIT_.join([name, tag, str(party_id), role])
-        return cache_key
-
-    def _get_consume_message(self, channel_info):
-        raise NotImplementedError()
-
-    def _consume_ack(self, channel_info, id):
-        raise NotImplementedError()
-
-    def _query_receive_topic(self, channel_info):
-        return channel_info
+        return []
 
     def _receive_obj(self, channel_info, name, tag):
         party_id = channel_info._dst_party_id
         role = channel_info._dst_role
 
-        wish_cache_key = self._get_message_cache_key(name, tag, party_id, role)
+        wish_cache_key = _get_message_cache_key(name, tag, party_id, role)
 
         if wish_cache_key in self._message_cache:
-            recv_obj = self._message_cache[wish_cache_key]
+            recv_bytes = self._message_cache[wish_cache_key]
             del self._message_cache[wish_cache_key]
-            return recv_obj
+            return recv_bytes
 
-        channel_info = self._query_receive_topic(channel_info)
+        # channel_info = self._query_receive_topic(channel_info)
 
-        for id, properties, body in self._get_consume_message(channel_info):
-            LOGGER.debug(f"[federation._receive_obj] properties: {properties}")
-            if properties["message_id"] != name or properties["correlation_id"] != tag:
-                # todo: fix this
-                LOGGER.warning(
-                    f"[federation._receive_obj] require {name}.{tag}, got {properties['message_id']}.{properties['correlation_id']}"
-                )
-
-            cache_key = self._get_message_cache_key(
-                properties["message_id"], properties["correlation_id"], party_id, role
-            )
+        for _id, properties, body in self._get_consume_message(channel_info):
+            LOGGER.debug(f"properties: {properties}")
+            cache_key = _get_message_cache_key(properties["message_id"], properties["correlation_id"], party_id, role)
             # object
             if properties["content_type"] == "text/plain":
-                recv_obj = p_loads(body)
-                self._consume_ack(channel_info, id)
+                recv_bytes = body
+                self._consume_ack(channel_info, _id)
                 LOGGER.debug(f"[federation._receive_obj] cache_key: {cache_key}, wish_cache_key: {wish_cache_key}")
                 if cache_key == wish_cache_key:
                     channel_info.cancel()
-                    return recv_obj
+                    return recv_bytes
                 else:
-                    self._message_cache[cache_key] = recv_obj
+                    self._message_cache[cache_key] = recv_bytes
             else:
                 raise ValueError(
                     f"[federation._receive_obj] properties.content_type is {properties['content_type']}, but must be text/plain"
                 )
 
     def _get_partition_receive_func(
         self,
@@ -509,18 +407,17 @@
         src_role,
         dst_party_id,
         dst_role,
         topic_infos,
         mq,
         conf: dict,
     ):
-        def _fn(index, kvs):
+        def _fn(index, _):
             return self._partition_receive(
                 index=index,
-                kvs=kvs,
                 name=name,
                 tag=tag,
                 src_party_id=src_party_id,
                 src_role=src_role,
                 dst_party_id=dst_party_id,
                 dst_role=dst_role,
                 topic_infos=topic_infos,
@@ -529,26 +426,25 @@
             )
 
         return _fn
 
     def _partition_receive(
         self,
         index,
-        kvs,
         name,
         tag,
         src_party_id,
         src_role,
         dst_party_id,
         dst_role,
         topic_infos,
         mq,
         conf: dict,
     ):
-        topic_pair = topic_infos[index][1]
+        _, _, topic_pair = topic_infos[index]
         channel_info = self._get_channel(
             topic_pair=topic_pair,
             src_party_id=src_party_id,
             src_role=src_role,
             dst_party_id=dst_party_id,
             dst_role=dst_role,
             mq=mq,
@@ -556,51 +452,49 @@
         )
 
         message_key_cache = set()
         count = 0
         partition_size = -1
         all_data = []
 
-        channel_info = self._query_receive_topic(channel_info)
-
         while True:
             try:
                 for id, properties, body in self._get_consume_message(channel_info):
-                    print(f"[federation._partition_receive] properties: {properties}.")
+                    LOGGER.debug(f"[federation._partition_receive] properties: {properties}.")
                     if properties["message_id"] != name or properties["correlation_id"] != tag:
                         # todo: fix this
                         self._consume_ack(channel_info, id)
-                        print(
+                        LOGGER.debug(
                             f"[federation._partition_receive]: require {name}.{tag}, got {properties['message_id']}.{properties['correlation_id']}"
                         )
                         continue
 
                     if properties["content_type"] == "application/json":
                         header = json.loads(properties["headers"])
                         message_key = header["message_key"]
                         if message_key in message_key_cache:
-                            print(f"[federation._partition_receive] message_key : {message_key} is duplicated")
+                            LOGGER.debug(f"[federation._partition_receive] message_key : {message_key} is duplicated")
                             self._consume_ack(channel_info, id)
                             continue
 
                         message_key_cache.add(message_key)
 
                         if header["partition_size"] >= 0:
                             partition_size = header["partition_size"]
 
                         data = json.loads(body.decode())
                         data_iter = (
                             (
-                                p_loads(bytes.fromhex(el["k"])),
-                                p_loads(bytes.fromhex(el["v"])),
+                                bytes.fromhex(el["k"]),
+                                bytes.fromhex(el["v"]),
                             )
                             for el in data
                         )
                         count += len(data)
-                        print(f"[federation._partition_receive] count: {count}")
+                        LOGGER.debug(f"[federation._partition_receive] count: {count}")
                         all_data.extend(data_iter)
                         self._consume_ack(channel_info, id)
 
                         if count == partition_size:
                             channel_info.cancel()
                             return all_data
                     else:
@@ -612,7 +506,12 @@
                 LOGGER.error(f"[federation._partition_receive]catch exception {e}, while receiving {name}.{tag}")
                 # avoid hang on consume()
                 if count == partition_size:
                     channel_info.cancel()
                     return all_data
                 else:
                     raise e
+
+
+def _get_message_cache_key(name: str, tag: str, party_id, role: str):
+    cache_key = _SPLIT_.join([name, tag, str(party_id), role])
+    return cache_key
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/_nretry.py` & `pyfate-2.1.0/fate/arch/federation/message_queue/_nretry.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/_parties.py` & `pyfate-2.1.0/fate/arch/federation/message_queue/_parties.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,8 +35,8 @@
     def __lt__(self, other):
         return (self.role, self.party_id) < (other.role, other.party_id)
 
     def __eq__(self, other):
         return self.party_id == other.party_id and self.role == other.role
 
     def as_tuple(self):
-        return (self.role, self.party_id)
+        return self.role, self.party_id
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/_type.py` & `pyfate-2.1.0/fate/arch/computing/partitioners/_integer_partitioner.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,14 +8,11 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-#
 
 
-class FederationDataType(object):
-    OBJECT = "obj"
-    TABLE = "Table"
-    SPLIT_OBJECT = "split_obj"
+def integer_partitioner(key: bytes, total_partitions):
+    return int.from_bytes(key, "big") % total_partitions
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/eggroll/__init__.py` & `pyfate-2.1.0/fate/arch/federation/backends/eggroll/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/osx/__init__.py` & `pyfate-2.1.0/fate/arch/federation/backends/osx/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/pulsar/__init__.py` & `pyfate-2.1.0/fate/arch/federation/backends/pulsar/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/pulsar/_federation.py` & `pyfate-2.1.0/fate/arch/federation/backends/pulsar/_federation.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,28 +13,25 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
 
 import logging
 from typing import List, Optional
 
-from fate.arch.abc import PartyMeta
-
-from .._federation import FederationBase
+from fate.arch.federation.api import PartyMeta
+from fate.arch.federation.message_queue import MessageQueueBasedFederation
 from ._mq_channel import (
     DEFAULT_CLUSTER,
     DEFAULT_SUBSCRIPTION_NAME,
     DEFAULT_TENANT,
     MQChannel,
 )
 from ._pulsar_manager import PulsarManager
 
 LOGGER = logging.getLogger(__name__)
-# default message max size in bytes = 1MB
-DEFAULT_MESSAGE_MAX_SIZE = 104857
 
 
 class MQ(object):
     def __init__(self, host, port, route_table):
         self.host = host
         self.port = port
         self.route_table = route_table
@@ -50,15 +47,15 @@
     def __init__(self, tenant, namespace, send, receive):
         self.tenant = tenant
         self.namespace = namespace
         self.send = send
         self.receive = receive
 
 
-class PulsarFederation(FederationBase):
+class PulsarFederation(MessageQueueBasedFederation):
     @staticmethod
     def from_conf(
         federation_session_id: str,
         computing_session,
         party: PartyMeta,
         parties: List[PartyMeta],
         route_table: dict,
@@ -77,16 +74,14 @@
     ):
         if topic_ttl is None:
             topic_ttl = 0
         if cluster is None:
             cluster = DEFAULT_CLUSTER
         if tenant is None:
             tenant = DEFAULT_TENANT
-        if max_message_size is None:
-            max_message_size = DEFAULT_MESSAGE_MAX_SIZE
         if "max_message_size" in pulsar_run:
             max_message_size = pulsar_run["max_message_size"]
         if "topic_ttl" in pulsar_run:
             topic_ttl = pulsar_run["topic_ttl"]
 
         # pulsar not use user and password so far
         # TODO add credential to connections
@@ -148,15 +143,15 @@
         self._cluster = cluster
         self._tenant = tenant
         self._mode = mode
 
     def __getstate__(self):
         pass
 
-    def destroy(self):
+    def _destroy(self):
         # The idea cleanup strategy is to consume all message in topics,
         # and let pulsar cluster to collect the used topics.
 
         LOGGER.debug("[pulsar.cleanup]start to cleanup...")
 
         # 1. remove subscription
         response = self._pulsar_manager.unsubscribe_namespace_all_topics(
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/pulsar/_mq_channel.py` & `pyfate-2.1.0/fate/arch/federation/backends/pulsar/_mq_channel.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 #
 
 
 import logging
 
 import pulsar
 
-from .._nretry import nretry
+from fate.arch.federation.message_queue import nretry
 
 LOGGER = logging.getLogger(__name__)
 CHANNEL_TYPE_PRODUCER = "producer"
 CHANNEL_TYPE_CONSUMER = "consumer"
 DEFAULT_TENANT = "fl-tenant"
 DEFAULT_CLUSTER = "standalone"
 TOPIC_PREFIX = "{}/{}/{}"
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/pulsar/_pulsar_manager.py` & `pyfate-2.1.0/fate/arch/federation/backends/pulsar/_pulsar_manager.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/rabbitmq/__init__.py` & `pyfate-2.1.0/fate/ml/preprocessing/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,10 +8,10 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from ._federation import RabbitmqFederation
 
-__all__ = ["RabbitmqFederation"]
+from .feature_scale import FeatureScale
+from .union import Union
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/rabbitmq/_federation.py` & `pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/_federation.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,26 +14,21 @@
 #  limitations under the License.
 #
 
 import json
 from logging import getLogger
 from typing import List, Optional
 
-from fate.arch.abc import PartyMeta
-
-from .._federation import FederationBase
-from .._parties import Party
+from fate.arch.federation.api import PartyMeta
+from fate.arch.federation.message_queue import MessageQueueBasedFederation, Party
 from ._mq_channel import MQChannel
 from ._rabbit_manager import RabbitManager
 
 LOGGER = getLogger(__name__)
 
-# default message max size in bytes = 1MB
-DEFAULT_MESSAGE_MAX_SIZE = 1048576
-
 
 class MQ(object):
     def __init__(self, host, port, union_name, policy_id, route_table):
         self.host = host
         self.port = port
         self.union_name = union_name
         self.policy_id = policy_id
@@ -54,15 +49,15 @@
         self.tenant = tenant
         self.namespace = namespace
         self.vhost = vhost
         self.send = send
         self.receive = receive
 
 
-class RabbitmqFederation(FederationBase):
+class RabbitmqFederation(MessageQueueBasedFederation):
     @staticmethod
     def from_conf(
         federation_session_id: str,
         computing_session,
         party: PartyMeta,
         parties: List[PartyMeta],
         route_table: dict,
@@ -75,16 +70,14 @@
         max_message_size: Optional[int],
         rabbitmq_run: dict = {},
         connection: dict = {},
     ):
         LOGGER.debug(
             f"federation_session_id={federation_session_id}, party={party}, parties={parties},  route_table={route_table}, rabbitmq_run={rabbitmq_run}",
         )
-        if max_message_size is None:
-            max_message_size = DEFAULT_MESSAGE_MAX_SIZE
         union_name = federation_session_id
         policy_id = federation_session_id
         rabbit_manager = RabbitManager(base_user, base_password, f"{host}:{mng_port}", rabbitmq_run)
         rabbit_manager.create_user(union_name, policy_id)
         mq = MQ(host, port, union_name, policy_id, route_table)
 
         return RabbitmqFederation(
@@ -123,28 +116,28 @@
         self._rabbit_manager = rabbit_manager
         self._vhost_set = set()
         self._mode = mode
 
     def __getstate__(self):
         pass
 
-    def destroy(self):
+    def _destroy(self):
         LOGGER.debug("[rabbitmq.cleanup]start to cleanup...")
         for party in self.parties:
             if self.local_party == party:
                 continue
             vhost = self._get_vhost(Party(role=party[0], party_id=party[1]))
             LOGGER.debug(f"[rabbitmq.cleanup]start to cleanup vhost {vhost}...")
             self._rabbit_manager.clean(vhost)
             LOGGER.debug(f"[rabbitmq.cleanup]cleanup vhost {vhost} done")
         if self._mq.union_name:
             LOGGER.debug(f"[rabbitmq.cleanup]clean user {self._mq.union_name}.")
             self._rabbit_manager.delete_user(user=self._mq.union_name)
 
-    def _get_vhost(self, party):
+    def _get_vhost(self, party: PartyMeta):
         low, high = (self._party, party) if self._party < party else (party, self._party)
         vhost = f"{self._session_id}-{low.role}-{low.party_id}-{high.role}-{high.party_id}"
         return vhost
 
     def _maybe_create_topic_and_replication(self, party, topic_suffix):
         if self._mode == "replication":
             return self._create_topic_by_replication_mode(party, topic_suffix)
@@ -224,15 +217,15 @@
     def _get_channel(
         self,
         topic_pair,
         src_party_id,
         src_role,
         dst_party_id,
         dst_role,
-        mq,
+        mq=None,
         conf: dict = None,
     ):
         LOGGER.debug(
             f"rabbitmq federation _get_channel, src_party_id={src_party_id}, src_role={src_role},"
             f"dst_party_id={dst_party_id}, dst_role={dst_role}"
         )
         return MQChannel(
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/rabbitmq/_mq_channel.py` & `pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/_mq_channel.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 #
 
 import json
 import logging
 
 import pika
 
-from .._nretry import nretry
+from fate.arch.federation.message_queue import nretry
 
 LOGGER = logging.getLogger(__name__)
 
 
 class MQChannel(object):
     def __init__(
         self,
```

### Comparing `pyfate-2.0.0b0/fate/arch/federation/rabbitmq/_rabbit_manager.py` & `pyfate-2.1.0/fate/arch/federation/backends/rabbitmq/_rabbit_manager.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/federation/standalone/__init__.py` & `pyfate-2.1.0/fate/arch/protocol/diffie_hellman/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,12 +8,11 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-#
 
-from ._federation import StandaloneFederation
+from fate_utils.secure_aggregation_helper import DiffieHellman
 
-__all__ = ["StandaloneFederation"]
+__all__ = ["DiffieHellman"]
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/_histogram_distributed.py` & `pyfate-2.1.0/fate/arch/histogram/_histogram_distributed.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,29 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 import typing
 from typing import List, MutableMapping, Tuple, Optional
 
 import torch
 
-from fate.arch.abc import CTableABC
+from fate.arch.computing.api import KVTable
 from ._histogram_local import Histogram
 from ._histogram_splits import HistogramSplits
 from .indexer import HistogramIndexer, Shuffler
 
 logger = logging.getLogger(__name__)
 
 
@@ -24,15 +39,15 @@
 
     return _decrypt
 
 
 class DistributedHistogram:
     def __init__(
         self,
-        splits: CTableABC[int, HistogramSplits],
+        splits: KVTable[int, HistogramSplits],
         k,
         node_size,
         node_data_size,
         global_seed,
         seed=None,
         squeezed=False,
         shuffled=False,
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/_histogram_local.py` & `pyfate-2.1.0/fate/arch/histogram/_histogram_local.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 import typing
 
 from ._histogram_splits import HistogramSplits
 from .indexer import HistogramIndexer, Shuffler
 from .values import HistogramValuesContainer
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/_histogram_splits.py` & `pyfate-2.1.0/fate/arch/histogram/_histogram_splits.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 import typing
 from typing import List, Tuple
 
 from .values import HistogramValuesContainer
 from .indexer import Shuffler
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/indexer/_indexer.py` & `pyfate-2.1.0/fate/arch/histogram/indexer/_indexer.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import List, Tuple, Dict
 
 import numpy as np
 import torch
 
 
 class Shuffler:
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/values/_cipher.py` & `pyfate-2.1.0/fate/arch/histogram/values/_cipher.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 import typing
 from typing import List, Tuple
 
 import torch
 
 from ._encoded import HistogramEncodedValues
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/values/_plain.py` & `pyfate-2.1.0/fate/arch/histogram/values/_plain.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 import typing
 from typing import List, Tuple
 
 import torch
 
 from ._value import HistogramValues
```

### Comparing `pyfate-2.0.0b0/fate/arch/histogram/values/_values.py` & `pyfate-2.1.0/fate/arch/histogram/values/_values.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import typing
 from typing import MutableMapping
 from ._value import HistogramValues
 from ._plain import HistogramPlainValues
 from ._cipher import HistogramEncryptedValues
 
 if typing.TYPE_CHECKING:
```

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/phe/mock.py` & `pyfate-2.1.0/fate/arch/protocol/phe/mock.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import List, Optional, Tuple
 
 import torch
 
 from .type import TensorEvaluator
 
 V = torch.Tensor
```

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/phe/ou.py` & `pyfate-2.1.0/fate/arch/protocol/phe/ou.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,30 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import List, Optional, Tuple
 
 import torch
-from fate_utils.ou import PK as _PK
-from fate_utils.ou import SK as _SK
+from fate_utils.ou import CiphertextVector, PlaintextVector
 from fate_utils.ou import Coder as _Coder
 from fate_utils.ou import Evaluator as _Evaluator
-from fate_utils.ou import CiphertextVector, PlaintextVector
+from fate_utils.ou import PK as _PK
+from fate_utils.ou import SK as _SK
 from fate_utils.ou import keygen as _keygen
 
 from .type import TensorEvaluator
 
 V = torch.Tensor
 EV = CiphertextVector
 FV = PlaintextVector
@@ -378,38 +393,7 @@
             the cumsum result
         """
         return a.chunking_cumsum_with_step(pk.pk, chunk_sizes, step)
 
     @staticmethod
     def pack_squeeze(a: EV, pack_num: int, shift_bit: int, pk: PK) -> EV:
         return a.pack_squeeze(pack_num, shift_bit, pk.pk)
-
-
-def test_pack_float():
-    offset_bit = 32
-    precision = 16
-    coder = Coder(_Coder())
-    vec = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])
-    packed = coder.pack_floats(vec, offset_bit, 2, precision)
-    unpacked = coder.unpack_floats(packed, offset_bit, 2, precision, 5)
-    assert torch.allclose(vec, unpacked, rtol=1e-3, atol=1e-3)
-
-
-def test_pack_squeeze():
-    offset_bit = 32
-    precision = 16
-    pack_num = 2
-    pack_packed_num = 2
-    vec1 = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])
-    vec2 = torch.tensor([0.6, 0.7, 0.8, 0.9, 1.0])
-    sk, pk, coder = keygen(1024)
-    a = coder.pack_floats(vec1, offset_bit, pack_num, precision)
-    ea = pk.encrypt_encoded(a, obfuscate=False)
-    b = coder.pack_floats(vec2, offset_bit, pack_num, precision)
-    eb = pk.encrypt_encoded(b, obfuscate=False)
-    ec = evaluator.add(ea, eb, pk)
-
-    # pack packed encrypted
-    ec_pack = evaluator.pack_squeeze(ec, pack_packed_num, offset_bit * 2, pk)
-    c_pack = sk.decrypt_to_encoded(ec_pack)
-    c = coder.unpack_floats(c_pack, offset_bit, pack_num * pack_packed_num, precision, 5)
-    assert torch.allclose(vec1 + vec2, c, rtol=1e-3, atol=1e-3)
```

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/phe/paillier.py` & `pyfate-2.1.0/fate/arch/protocol/phe/paillier.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,30 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import List, Optional, Tuple
 
 import torch
-from fate_utils.paillier import PK as _PK
-from fate_utils.paillier import SK as _SK
+from fate_utils.paillier import CiphertextVector, PlaintextVector
 from fate_utils.paillier import Coder as _Coder
 from fate_utils.paillier import Evaluator as _Evaluator
-from fate_utils.paillier import CiphertextVector, PlaintextVector
+from fate_utils.paillier import PK as _PK
+from fate_utils.paillier import SK as _SK
 from fate_utils.paillier import keygen as _keygen
 
 from .type import TensorEvaluator
 
 V = torch.Tensor
 EV = CiphertextVector
 FV = PlaintextVector
@@ -378,38 +393,7 @@
             the cumsum result
         """
         return a.chunking_cumsum_with_step(pk.pk, chunk_sizes, step)
 
     @staticmethod
     def pack_squeeze(a: EV, pack_num: int, shift_bit: int, pk: PK) -> EV:
         return a.pack_squeeze(pack_num, shift_bit, pk.pk)
-
-
-def test_pack_float():
-    offset_bit = 32
-    precision = 16
-    coder = Coder(_Coder())
-    vec = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])
-    packed = coder.pack_floats(vec, offset_bit, 2, precision)
-    unpacked = coder.unpack_floats(packed, offset_bit, 2, precision, 5)
-    assert torch.allclose(vec, unpacked, rtol=1e-3, atol=1e-3)
-
-
-def test_pack_squeeze():
-    offset_bit = 32
-    precision = 16
-    pack_num = 2
-    pack_packed_num = 2
-    vec1 = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])
-    vec2 = torch.tensor([0.6, 0.7, 0.8, 0.9, 1.0])
-    sk, pk, coder = keygen(1024)
-    a = coder.pack_floats(vec1, offset_bit, pack_num, precision)
-    ea = pk.encrypt_encoded(a, obfuscate=False)
-    b = coder.pack_floats(vec2, offset_bit, pack_num, precision)
-    eb = pk.encrypt_encoded(b, obfuscate=False)
-    ec = evaluator.add(ea, eb, pk)
-
-    # pack packed encrypted
-    ec_pack = evaluator.pack_squeeze(ec, pack_packed_num, offset_bit * 2, pk)
-    c_pack = sk.decrypt_to_encoded(ec_pack)
-    c = coder.unpack_floats(c_pack, offset_bit, pack_num * pack_packed_num, precision, 5)
-    assert torch.allclose(vec1 + vec2, c, rtol=1e-3, atol=1e-3)
```

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/psi/__init__.py` & `pyfate-2.1.0/fate/arch/protocol/psi/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/psi/_psi_run.py` & `pyfate-2.1.0/fate/arch/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,15 +9,12 @@
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
-from .ecdh._run import psi_ecdh
 
+from .context import CipherKit, Context
+from .unify import URI, device
 
-def psi_run(ctx, df, protocol="ecdh_psi", curve_type="curve25519"):
-    if protocol == "ecdh_psi":
-        return psi_ecdh(ctx, df, curve_type=curve_type)
-    else:
-        raise ValueError(f"PSI protocol={protocol} does not implemented yet.")
+__all__ = ["device", "Context", "URI", "CipherKit"]
```

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/psi/ecdh/__init__.py` & `pyfate-2.1.0/fate/arch/protocol/psi/ecdh/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/psi/ecdh/_run.py` & `pyfate-2.1.0/fate/arch/protocol/psi/ecdh/_run.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,14 +38,15 @@
     return curve.diffie_hellman_vec(values)
 
 
 def _flat_block_with_possible_duplicate_keys(block_table, duplicate_allow=False):
     """
     row_value: (encrypt_id, [(block_id, _offset)])
     """
+
     def _mapper(kvs):
         for block_id, id_list in kvs:
             for _i, _id in enumerate(id_list):
                 yield _id, [(block_id, _i)]
 
     def _reducer(v1, v2):
         if not v1:
@@ -61,14 +62,15 @@
     return block_table.mapReducePartitions(_mapper, _reducer)
 
 
 def _flat_block_key(intersect_id):
     """
     key=eid, value = ((guest_block_id, guest_block_offset), [(host0_block_id, host0_block_offset)...])
     """
+
     def _mapper(kvs):
         for _, value in kvs:
             guest_loc = value[0]
             host_loc = value[1]
             for guest_block_id, guest_offset in guest_loc:
                 yield (guest_block_id, guest_offset), host_loc
 
@@ -97,23 +99,24 @@
     host_second_sign_match_ids = []
 
     dh_func = functools.partial(_diffie_hellman, curve=curve)
 
     for i, host_first_sign_match_id in enumerate(host_first_sign_match_ids):
         host_second_sign_match_ids.append(
             _flat_block_with_possible_duplicate_keys(
-                host_first_sign_match_ids[i].mapValues(dh_func),
-                duplicate_allow=False
+                host_first_sign_match_ids[i].mapValues(dh_func), duplicate_allow=False
             )
         )
 
     guest_second_sign_match_ids = ctx.hosts.get(GUEST_SECOND_SIGN)
 
     flat_intersect_id = None
-    for guest_second_sign_id, host_second_sign_match_id in zip(guest_second_sign_match_ids, host_second_sign_match_ids):
+    for guest_second_sign_id, host_second_sign_match_id in zip(
+        guest_second_sign_match_ids, host_second_sign_match_ids
+    ):
         intersect_eid = guest_second_sign_id.join(
             host_second_sign_match_id, lambda id_list_l, id_list_r: (id_list_l, id_list_r)
         )
         intersect_single = _flat_block_key(intersect_eid)
         if not flat_intersect_id:
             flat_intersect_id = intersect_single
         else:
@@ -135,15 +138,15 @@
     """
     for host_id in range(len(guest_second_sign_match_ids)):
         host_indexer = intersect_with_offset_ids.mapValues(lambda v: (v[0][0], v[1][host_id]))
         ctx.hosts[host_id].put(HOST_INDEXER, host_indexer)
 
     intersect_guest_data = intersect_with_offset_ids.mapValues(lambda v: v[0])
 
-    guest_df =  DataFrame.from_flatten_data(ctx, intersect_guest_data, df.data_manager, key_type="block_id")
+    guest_df = DataFrame.from_flatten_data(ctx, intersect_guest_data, df.data_manager, key_type="block_id")
     ctx.metrics.log_metrics({"intersect_count": guest_df.shape[0]}, name="intersect_id_count", type="custom")
 
     """
     the following just for debug, need to be delete 
     """
     # ids = [v[0] for k, v in sorted(guest_df.block_table.collect())]
     # logger.debug(f"intersect ids is: {ids}")
@@ -159,16 +162,17 @@
     host_first_sign_match_id = match_id.mapValues(encrypt_func)
     ctx.guest.put(HOST_FIRST_SIGN, host_first_sign_match_id)
 
     dh_func = functools.partial(_diffie_hellman, curve=curve)
     guest_first_sign_match_id = ctx.guest.get(GUEST_FIRST_SIGN)
     guest_second_sign_match_id = guest_first_sign_match_id.mapValues(dh_func)
 
-    guest_second_sign_match_id = _flat_block_with_possible_duplicate_keys(guest_second_sign_match_id,
-                                                                          duplicate_allow=True)
+    guest_second_sign_match_id = _flat_block_with_possible_duplicate_keys(
+        guest_second_sign_match_id, duplicate_allow=True
+    )
     ctx.guest.put(GUEST_SECOND_SIGN, guest_second_sign_match_id)
 
     """
     host_indexer: key=(block_id, offset), value=(sample_id, (bid, offset))
     """
     host_indexer = ctx.guest.get(HOST_INDEXER)
```

### Comparing `pyfate-2.0.0b0/fate/arch/protocol/secure_aggregation/_secure_aggregation.py` & `pyfate-2.1.0/fate/arch/protocol/secure_aggregation/_secure_aggregation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import typing
 
 import numpy
 from fate.arch import Context
 from fate.arch.protocol.diffie_hellman import DiffieHellman
 from fate_utils.secure_aggregation_helper import MixAggregate, RandomMix
 
@@ -111,7 +126,9 @@
                     aggregated_weight += weight
             if not has_weight:
                 aggregated_weight = None
             aggregated = mix_aggregator.finalize(aggregated_weight)
 
         for rank in ranks:
             ctx.parties[rank].put(self._get_name(self._recv_name), aggregated)
+
+        return aggregated
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/__init__.py` & `pyfate-2.1.0/fate/arch/tensor/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/distributed/_op_matmul.py` & `pyfate-2.1.0/fate/arch/tensor/distributed/_op_matmul.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 
 import torch
 from fate.arch.tensor import _custom_ops
 
 from ._tensor import DTensor, implements
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/distributed/_op_slice.py` & `pyfate-2.1.0/fate/arch/tensor/distributed/_op_slice.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import torch
 from fate.arch.tensor import _custom_ops
 
 from ._tensor import DTensor, implements
 
 
 @implements(_custom_ops.slice_f)
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_agg.py` & `pyfate-2.1.0/fate/arch/tensor/distributed/_ops_agg.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import Tuple
 
 import torch
 
 from ._tensor import DTensor, implements
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/distributed/_ops_cipher.py` & `pyfate-2.1.0/fate/arch/tensor/distributed/_ops_cipher.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from fate.arch.tensor import _custom_ops
 
 from ._tensor import DTensor, implements
 
 
 @implements(_custom_ops.encrypt_encoded_f)
 def encrypt_encoded_f(input: DTensor, encryptor):
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/distributed/_tensor.py` & `pyfate-2.1.0/fate/arch/tensor/distributed/_tensor.py`

 * *Files 16% similar despite different names*

```diff
@@ -14,110 +14,156 @@
 #  limitations under the License.
 
 import functools
 import typing
 from typing import List, Optional, Tuple, TypeVar, cast
 
 import torch
-from fate.arch.abc import CTableABC
+from fate.arch.computing.api import KVTable
 from fate.arch.context import Context
+from fate.arch.tensor.phe import PHETensor
+from fate.arch.trace import auto_trace
 
 _HANDLED_FUNCTIONS = {}
 
 
 def implements(torch_function):
     """Register a torch function override for DStorage"""
 
     @functools.wraps(torch_function)
     def decorator(func):
-        _HANDLED_FUNCTIONS[torch_function] = func
+        _HANDLED_FUNCTIONS[torch_function] = auto_trace(func)
         return func
 
     return decorator
 
 
 class DTensor:
     @classmethod
     def __torch_function__(cls, func, types, args=(), kwargs=None):
         if kwargs is None:
             kwargs = {}
-        if func not in _HANDLED_FUNCTIONS or not all(issubclass(t, (torch.Tensor, DTensor)) for t in types):
+        if func not in _HANDLED_FUNCTIONS or not all(issubclass(t, (torch.Tensor, DTensor, PHETensor)) for t in types):
             return NotImplemented
         return _HANDLED_FUNCTIONS[func](*args, **kwargs)
 
+    def to(self, device):
+        if self.shardings.device == device:
+            return self
+        else:
+            raise NotImplementedError(f"device {device} not supported")
+
+    def size(self):
+        return self.shardings.shapes
+
+    def dim(self):
+        return self.shardings.dim()
+
     @property
+    @auto_trace
     def T(self):
         return torch.transpose(self, 0, 1)
 
+    @property
+    def is_cuda(self):
+        return self.shardings.device.type == "cuda"
+
     def elem_type(self) -> Optional[str]:
         return self.shardings._type
 
     def __init__(self, shardings: "Shardings") -> None:
         self.shardings = shardings
 
+    @auto_trace
     def __add__(self, other):
         try:
             return torch.add(self, other)
         except Exception as e:
             raise RuntimeError(f"Failed to add {self} and {other}") from e
 
+    @auto_trace
     def __radd__(self, other):
         return torch.add(other, self)
 
+    @auto_trace
     def __sub__(self, other):
         return torch.sub(self, other)
 
+    @auto_trace
     def __rsub__(self, other):
         return torch.rsub(self, other)
 
+    @auto_trace
     def __mul__(self, other):
         return torch.mul(self, other)
 
+    @auto_trace
     def __rmul__(self, other):
         return torch.mul(other, self)
 
+    @auto_trace
     def __truediv__(self, other):
         return torch.div(self, other)
 
+    @auto_trace
     def __rtruediv__(self, other):
         return torch.div(other, self)
 
+    @auto_trace
     def __matmul__(self, other):
         return torch.matmul(self, other)
 
+    @auto_trace
     def __rmatmul__(self, other):
         return torch.matmul(other, self)
 
+    def matmul(self, other):
+        return torch.matmul(self, other)
+
+    @auto_trace
     def encrypt(self, encryptor):
         return torch.encrypt_f(self, encryptor)
 
+    @auto_trace
     def encrypt_encoded(self, encryptor):
         return torch.encrypt_encoded_f(self, encryptor)
 
+    @auto_trace
     def decrypt_encoded(self, decryptor):
         return torch.decrypt_encoded_f(self, decryptor)
 
+    @auto_trace
     def encode(self, encoder):
         return torch.encode_f(self, encoder)
 
+    @auto_trace
     def decode(self, decoder):
         return torch.decode_f(self, decoder)
 
+    @auto_trace
     def decrypt(self, decryptor):
         return torch.decrypt_f(self, decryptor)
 
+    @auto_trace
     def exp(self):
         return torch.exp(self)
 
+    @auto_trace
     def log(self):
         return torch.log(self)
 
+    @auto_trace
+    def sum(self, dim=None, **kwargs):
+        return torch.sum(self, dim=dim, **kwargs)
+
+    @auto_trace
     def square(self):
         return torch.square(self)
 
+    @auto_trace
     def sigmoid(self):
         return torch.sigmoid(self)
 
     @property
     def shape(self):
         return self.shardings.shape
 
@@ -131,26 +177,30 @@
 
     def __eq__(self, __o: object) -> bool:
         return isinstance(__o, DTensor) and self.shardings == __o.shardings
 
     def __str__(self) -> str:
         return f"<DTensor(shardings={self.shardings})>"
 
+    def __repr__(self):
+        return self.__str__()
+
     @classmethod
     def from_sharding_table(
         cls,
-        data: CTableABC,
+        data: KVTable,
         shapes: Optional[List[torch.Size]],
         axis=0,
         dtype: Optional[torch.dtype] = None,
         device: Optional[torch.device] = None,
     ):
         return DTensor(Shardings(data, shapes, axis, dtype, device))
 
     @classmethod
+    @auto_trace
     def from_sharding_list(cls, ctx: Context, data: List[torch.Tensor], num_partitions=16, axis=0):
         dtype = data[0].dtype
         device = data[0].device
         shapes = []
         for t in data:
             shapes.append(t.shape)
             assert dtype == t.dtype
@@ -162,23 +212,88 @@
                     continue
                 else:
                     assert s1 == s2
         return cls.from_sharding_table(
             ctx.computing.parallelize(data, partition=num_partitions, include_key=False), shapes, axis, dtype, device
         )
 
+    @auto_trace
+    def clone(self):
+        return DTensor(self.shardings.map_shard(lambda t: t))
+
+    @auto_trace
+    def add(self, other) -> "DTensor":
+        return torch.add(self, other)
+
+    @auto_trace
+    def mul(self, other) -> "DTensor":
+        return torch.mul(self, other)
+
+    @auto_trace
+    def div(self, other, *, rounding_mode=None) -> "DTensor":
+        return torch.div(self, other, rounding_mode=rounding_mode)
+
+    @auto_trace
+    def sub(self, other):
+        return torch.sub(self, other)
+
+    @auto_trace
+    def neg(self):
+        return torch.neg(self)
+
+    @auto_trace
+    def nelement(self):
+        return self.shardings.shape.numel()
+
+    @auto_trace
+    def long(self):
+        return DTensor(self.shardings.map_shard(lambda t: t.long(), dtype=torch.long))
+
+    def set_(self, other):
+        if isinstance(other, DTensor):
+            self.shardings = other.shardings
+        elif isinstance(other, Shardings):
+            self.shardings = other
+        else:
+            raise NotImplementedError(f"type `{other}`")
+        return self
+
+    def copy_(self, other):
+        self.shardings = other.shardings
+        return self
+
+    def add_(self, other):
+        self.shardings = self.add(other).shardings
+        return self
+
+    def div_(self, other, *, rounding_mode=None):
+        self.shardings = self.div(other, rounding_mode=rounding_mode).shardings
+        return self
+
+    def neg_(self):
+        self.shardings = self.neg().shardings
+        return self
+
+    @property
+    def data(self):
+        return self.shardings
+
+    @auto_trace
+    def __getitem__(self, item):
+        return DTensor(self.shardings.map_shard(lambda t: t[item]))
+
 
 T1 = TypeVar("T1")
 T2 = TypeVar("T2")
 
 
 class Shardings:
     def __init__(
         self,
-        data: CTableABC[int, torch.Tensor],
+        data: KVTable[int, torch.Tensor],
         shapes: Optional[List[torch.Size]] = None,
         axis: int = 0,
         dtype: Optional[torch.dtype] = None,
         device: Optional[torch.device] = None,
         type: Optional[str] = None,
     ):
         self._data = data
@@ -208,14 +323,17 @@
             self._dtype = dtype
             self._device = device
 
     @property
     def shapes(self):
         return self._shapes
 
+    def dim(self):
+        return len(self._shapes.shapes[0])
+
     @property
     def dtype(self):
         return self._dtype
 
     @property
     def shape(self):
         return self.shapes.merge_shapes()
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/inside/_op_quantile.py` & `pyfate-2.1.0/fate/arch/tensor/inside/_op_quantile.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import List, Union
 
 import numpy
 import torch
 from fate_utils import quantile
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/phe/_keypair.py` & `pyfate-2.1.0/fate/arch/tensor/phe/_keypair.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,43 +1,66 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import typing
 
 import torch
 
 if typing.TYPE_CHECKING:
     from fate.arch.protocol.phe.paillier import PK, SK, Coder
 
     from ._tensor import PHETensor, PHETensorEncoded
 
 
-class PHETensorCipher:
-    def __init__(self, pk: "PHETensorEncryptor", coder: "PHETensorCoder", sk: "PHETensorDecryptor", evaluator) -> None:
+class PHETensorCipherPublic:
+    def __init__(self, pk: "PHETensorEncryptor", coder: "PHETensorCoder", evaluator) -> None:
         self._pk = pk
         self._coder = coder
-        self._sk = sk
         self._evaluator = evaluator
 
-    @classmethod
-    def from_raw_cipher(cls, pk: "PK", coder: "Coder", sk: "SK", evaluator):
-        coder = PHETensorCoder(coder)
-        encryptor = PHETensorEncryptor(pk, coder, evaluator)
-        decryptor = PHETensorDecryptor(sk, coder)
-        return cls(encryptor, coder, decryptor, evaluator)
-
     @property
     def pk(self):
         return self._pk
 
     @property
     def coder(self):
         return self._coder
 
+
+class PHETensorCipher(PHETensorCipherPublic):
+    def __init__(self, pk: "PHETensorEncryptor", coder: "PHETensorCoder", sk: "PHETensorDecryptor", evaluator) -> None:
+        super().__init__(pk, coder, evaluator)
+        self._sk = sk
+
+    @classmethod
+    def from_raw_cipher(cls, pk: "PK", coder: "Coder", sk: "SK", evaluator):
+        coder = PHETensorCoder(coder)
+        encryptor = PHETensorEncryptor(pk, coder, evaluator)
+        decryptor = PHETensorDecryptor(sk, coder)
+        return cls(encryptor, coder, decryptor, evaluator)
+
     @property
     def sk(self):
         return self._sk
 
+    def to_public(self):
+        return PHETensorCipherPublic(self.pk, self.coder, self._evaluator)
+
 
 class PHETensorCoder:
     def __init__(self, coder: "Coder") -> None:
         self._coder = coder
 
     def encode(self, tensor: torch.Tensor):
         if isinstance(tensor, torch.Tensor):
@@ -108,14 +131,15 @@
 
     def encrypt_tensor(self, tensor: torch.Tensor, obfuscate=False):
         coded = self._coder.encode(tensor)
         return self.encrypt_encoded(coded, obfuscate)
 
     def lift(self, data, shape, dtype, device):
         from ._tensor import PHETensor
+
         return PHETensor(self._pk, self._evaluator, self._coder, shape, data, dtype, device)
 
 
 class PHETensorDecryptor:
     def __init__(self, sk: "SK", coder: "PHETensorCoder") -> None:
         self._sk = sk
         self._coder = coder
```

### Comparing `pyfate-2.0.0b0/fate/arch/tensor/phe/_ops.py` & `pyfate-2.1.0/fate/arch/tensor/phe/_ops.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import torch
 from fate.arch.tensor import _custom_ops
 
 from ._tensor import PHETensor, implements, implements_encoded
 
 
 @implements(_custom_ops.encrypt_f)
@@ -47,25 +62,28 @@
     if isinstance(other, PHETensor):
         assert shape == other.shape, f"shape mismatch {shape} != {other.shape}"
         output_dtype = torch.promote_types(dtype, other.dtype)
         data = evaluator.add(input.data, other.data, pk)
         return input.with_template(data, dtype=output_dtype)
 
     elif isinstance(other, torch.Tensor):
-        # TODO: support broadcast
         if shape == other.shape:
             output_dtype = torch.promote_types(dtype, other.dtype)
             data = evaluator.add_plain(input.data, other, pk, coder, output_dtype)
             return input.with_template(data, dtype=output_dtype)
         elif other.ndim == 0:
             output_dtype = torch.promote_types(dtype, other.dtype)
             data = evaluator.add_plain_scalar(input.data, other.detach().item(), pk, coder, output_dtype)
             return input.with_template(data, dtype=output_dtype)
         else:
-            raise NotImplementedError(f"broadcast not supported")
+            # TODO: support broadcast
+            broadcast_shape = torch.broadcast_shapes(shape, other.shape)
+            raise RuntimeError(
+                f"broadcast shape {shape} and {other.shape} to {broadcast_shape} is fine, but not yet implemented"
+            )
 
     elif isinstance(other, (float, int)):
         output_dtype = torch.promote_types(dtype, torch.get_default_dtype())
         data = evaluator.add_plain_scalar(input.data, other, pk, coder, output_dtype)
         return input.with_template(data, dtype=output_dtype)
     else:
         return NotImplemented
@@ -186,17 +204,15 @@
     if not isinstance(input, PHETensor) and isinstance(other, PHETensor):
         return matmul(other, input)
 
     if input.ndim > 2 or input.ndim < 1:
         raise ValueError(f"can't rmatmul `PHETensor` with `torch.Tensor` with dim `{input.ndim}`")
 
     if isinstance(other, PHETensor):
-        raise NotImplementedError(
-            f"rmatmul {input} with {other} not supported, phe is not multiplicative homomorphic"
-        )
+        raise NotImplementedError(f"rmatmul {input} with {other} not supported, phe is not multiplicative homomorphic")
 
     if not isinstance(other, torch.Tensor):
         return NotImplemented
 
     evaluator = input.evaluator
     pk = input.pk
     coder = input.coder
```

### Comparing `pyfate-2.0.0b0/fate/arch/unify/__init__.py` & `pyfate-2.1.0/fate/arch/computing/api/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,18 +8,11 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from ._infra_def import Backend, device
-from ._io import URI
-from ._uuid import generate_computing_uuid, uuid
 
-__all__ = [
-    "generate_computing_uuid",
-    "Backend",
-    "device",
-    "uuid",
-    "URI",
-]
+from ._table import KVTable, KVTableContext, K, V, is_table
+from ._type import ComputingEngine
+from ._uuid import generate_computing_uuid
```

### Comparing `pyfate-2.0.0b0/fate/arch/unify/_infra_def.py` & `pyfate-2.1.0/fate/arch/unify/_infra_def.py`

 * *Files 18% similar despite different names*

```diff
@@ -37,13 +37,7 @@
 
         if self.type == "CPU":
             return torch.device("cpu")
         elif self.type == "CUDA":
             return torch.device("cuda", self.index)
         else:
             raise ValueError(f"device type {self.type} not supported")
-
-
-class Backend(Enum):
-    STANDALONE = "STANDALONE"
-    EGGROLL = "EGGROLL"
-    SPARK = "SPARK"
```

### Comparing `pyfate-2.0.0b0/fate/arch/unify/_io.py` & `pyfate-2.1.0/fate/arch/unify/_io.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/arch/unify/_uuid.py` & `pyfate-2.1.0/fate/arch/computing/api/_uuid.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,20 +8,18 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from typing import Optional
-from uuid import uuid1
 
+from typing import Optional
 
-def uuid():
-    return uuid1().hex
+from fate.arch.unify import uuid
 
 
 def generate_computing_uuid(session_id: Optional[str] = None):
     if session_id is None:
         return f"computing_{uuid()}"
     else:
         return f"{session_id}_computing_{uuid()}"
```

### Comparing `pyfate-2.0.0b0/fate/components/__main__.py` & `pyfate-2.1.0/fate/components/__main__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/components/__init__.py` & `pyfate-2.1.0/fate/components/components/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -69,24 +69,30 @@
     @_lazy_cpn
     def homo_nn(self):
         from .homo_nn import homo_nn
 
         return homo_nn
 
     @_lazy_cpn
+    def hetero_nn(self):
+        from .hetero_nn import hetero_nn
+
+        return hetero_nn
+
+    @_lazy_cpn
     def homo_lr(self):
         from .homo_lr import homo_lr
-        
+
         return homo_lr
-    
+
     @_lazy_cpn
-    def hetero_sbt(self):
-        from .hetero_sbt import hetero_sbt
+    def hetero_secureboost(self):
+        from .hetero_secureboost import hetero_secureboost
 
-        return hetero_sbt
+        return hetero_secureboost
 
     @_lazy_cpn
     def dataframe_transformer(self):
         from .dataframe_transformer import dataframe_transformer
 
         return dataframe_transformer
 
@@ -123,14 +129,20 @@
     @_lazy_cpn
     def hetero_feature_selection(self):
         from .hetero_feature_selection import hetero_feature_selection
 
         return hetero_feature_selection
 
     @_lazy_cpn
+    def feature_correlation(self):
+        from .feature_correlation import feature_correlation
+
+        return feature_correlation
+
+    @_lazy_cpn
     def union(self):
         from .union import union
 
         return union
 
     @_lazy_cpn
     def sample(self):
@@ -141,14 +153,26 @@
     @_lazy_cpn
     def data_split(self):
         from .data_split import data_split
 
         return data_split
 
     @_lazy_cpn
+    def sshe_lr(self):
+        from .sshe_lr import sshe_lr
+
+        return sshe_lr
+
+    @_lazy_cpn
+    def sshe_linr(self):
+        from .sshe_linr import sshe_linr
+
+        return sshe_linr
+
+    @_lazy_cpn
     def toy_example(self):
         from .toy_example import toy_example
 
         return toy_example
 
     @_lazy_cpn
     def dataframe_io_test(self):
```

### Comparing `pyfate-2.0.0b0/fate/components/components/coordinated_linr.py` & `pyfate-2.1.0/fate/components/components/coordinated_linr.py`

 * *Files 6% similar despite different names*

```diff
@@ -27,138 +27,174 @@
 @cpn.component(roles=[GUEST, HOST, ARBITER], provider="fate")
 def coordinated_linr(ctx, role):
     ...
 
 
 @coordinated_linr.train()
 def train(
-        ctx: Context,
-        role: Role,
-        train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        validate_data: cpn.dataframe_input(roles=[GUEST, HOST], optional=True),
-        learning_rate_scheduler: cpn.parameter(type=params.lr_scheduler_param(),
-                                               default=params.LRSchedulerParam(method="linear",
-                                                                               scheduler_params={"start_factor": 1.0}),
-                                               desc="learning rate scheduler, "
-                                                    "select method from {'step', 'linear', 'constant'}"
-                                                    "for list of configurable arguments, "
-                                                    "refer to torch.optim.lr_scheduler"),
-        epochs: cpn.parameter(type=params.conint(gt=0), default=20,
-                              desc="max iteration num"),
-        batch_size: cpn.parameter(
-            type=params.conint(ge=10),
-            default=None, desc="batch size, None means full batch, otherwise should be no less than 10, default None"
+    ctx: Context,
+    role: Role,
+    train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    validate_data: cpn.dataframe_input(roles=[GUEST, HOST], optional=True),
+    learning_rate_scheduler: cpn.parameter(
+        type=params.lr_scheduler_param(),
+        default=params.LRSchedulerParam(method="linear", scheduler_params={"start_factor": 1.0}),
+        desc="learning rate scheduler, "
+        "select method from {'step', 'linear', 'constant'}"
+        "for list of configurable arguments, "
+        "refer to torch.optim.lr_scheduler",
+    ),
+    epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
+    batch_size: cpn.parameter(
+        type=params.conint(ge=10),
+        default=None,
+        desc="batch size, None means full batch, otherwise should be no less than 10, default None",
+    ),
+    optimizer: cpn.parameter(
+        type=params.optimizer_param(),
+        default=params.OptimizerParam(
+            method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
         ),
-        optimizer: cpn.parameter(type=params.optimizer_param(),
-                                 default=params.OptimizerParam(method="sgd", penalty='l2', alpha=1.0,
-                                                               optimizer_params={"lr": 1e-2, "weight_decay": 0})),
-        tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
-        early_stop: cpn.parameter(type=params.string_choice(["weight_diff", "diff", "abs"]), default="diff",
-                                  desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}"),
-        init_param: cpn.parameter(
-            type=params.init_param(),
-            default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
-            desc="Model param init setting.",
-        ),
-        he_param: cpn.parameter(type=params.he_param(), default=params.HEParam(kind="paillier", key_length=1024),
-                                desc="homomorphic encryption param"),
-        floating_point_precision: cpn.parameter(
-            type=params.conint(ge=0),
-            default=23,
-            desc="floating point precision, "),
-        train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
-        output_model: cpn.json_model_output(roles=[GUEST, HOST, ARBITER]),
-        warm_start_model: cpn.json_model_input(roles=[GUEST, HOST, ARBITER], optional=True),
-
+    ),
+    tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
+    early_stop: cpn.parameter(
+        type=params.string_choice(["weight_diff", "diff", "abs"]),
+        default="diff",
+        desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
+    ),
+    init_param: cpn.parameter(
+        type=params.init_param(),
+        default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
+        desc="Model param init setting.",
+    ),
+    he_param: cpn.parameter(
+        type=params.he_param(),
+        default=params.HEParam(kind="paillier", key_length=1024),
+        desc="homomorphic encryption param",
+    ),
+    floating_point_precision: cpn.parameter(type=params.conint(ge=0), default=23, desc="floating point precision, "),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    output_model: cpn.json_model_output(roles=[GUEST, HOST, ARBITER]),
+    warm_start_model: cpn.json_model_input(roles=[GUEST, HOST, ARBITER], optional=True),
 ):
     logger.info(f"enter coordinated linr train")
     # temp code start
     optimizer = optimizer.dict()
     learning_rate_scheduler = learning_rate_scheduler.dict()
     init_param = init_param.dict()
     ctx.cipher.set_phe(ctx.device, he_param.dict())
     # temp code end
     if role.is_guest:
         train_guest(
-            ctx, train_data, validate_data, train_output_data, output_model, epochs,
-            batch_size, optimizer, learning_rate_scheduler, init_param, floating_point_precision,
-            warm_start_model
+            ctx,
+            train_data,
+            validate_data,
+            train_output_data,
+            output_model,
+            epochs,
+            batch_size,
+            optimizer,
+            learning_rate_scheduler,
+            init_param,
+            floating_point_precision,
+            warm_start_model,
         )
     elif role.is_host:
         train_host(
-            ctx, train_data, validate_data, train_output_data, output_model, epochs,
-            batch_size, optimizer, learning_rate_scheduler, init_param, floating_point_precision,
-            warm_start_model
+            ctx,
+            train_data,
+            validate_data,
+            train_output_data,
+            output_model,
+            epochs,
+            batch_size,
+            optimizer,
+            learning_rate_scheduler,
+            init_param,
+            floating_point_precision,
+            warm_start_model,
         )
     elif role.is_arbiter:
-        train_arbiter(ctx, epochs, early_stop, tol, batch_size, optimizer, learning_rate_scheduler,
-                      output_model, warm_start_model)
+        train_arbiter(
+            ctx,
+            epochs,
+            early_stop,
+            tol,
+            batch_size,
+            optimizer,
+            learning_rate_scheduler,
+            output_model,
+            warm_start_model,
+        )
 
 
 @coordinated_linr.predict()
 def predict(
-        ctx,
-        role: Role,
-        test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        input_model: cpn.json_model_input(roles=[GUEST, HOST]),
-        test_output_data: cpn.dataframe_output(roles=[GUEST, HOST])
+    ctx,
+    role: Role,
+    test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    input_model: cpn.json_model_input(roles=[GUEST, HOST]),
+    test_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
 ):
     if role.is_guest:
         predict_guest(ctx, input_model, test_data, test_output_data)
     if role.is_host:
         predict_host(ctx, input_model, test_data, test_output_data)
 
 
 @coordinated_linr.cross_validation()
 def cross_validation(
-        ctx: Context,
-        role: Role,
-        cv_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        learning_rate_scheduler: cpn.parameter(
-            type=params.lr_scheduler_param(),
-            default=params.LRSchedulerParam(method="linear", scheduler_params={"start_factor": 1.0}),
-            desc="learning rate scheduler, "
-                 "select method from {'step', 'linear', 'constant'}"
-                 "for list of configurable arguments, "
-                 "refer to torch.optim.lr_scheduler",
-        ),
-        epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
-        batch_size: cpn.parameter(
-            type=params.conint(ge=10),
-            default=None, desc="batch size, None means full batch, otherwise should be no less than 10, default None"
-        ),
-        optimizer: cpn.parameter(
-            type=params.optimizer_param(),
-            default=params.OptimizerParam(
-                method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
-            ),
-        ),
-        tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
-        early_stop: cpn.parameter(
-            type=params.string_choice(["weight_diff", "diff", "abs"]),
-            default="diff",
-            desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
-        ),
-        init_param: cpn.parameter(
-            type=params.init_param(),
-            default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
-            desc="Model param init setting.",
+    ctx: Context,
+    role: Role,
+    cv_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    learning_rate_scheduler: cpn.parameter(
+        type=params.lr_scheduler_param(),
+        default=params.LRSchedulerParam(method="linear", scheduler_params={"start_factor": 1.0}),
+        desc="learning rate scheduler, "
+        "select method from {'step', 'linear', 'constant'}"
+        "for list of configurable arguments, "
+        "refer to torch.optim.lr_scheduler",
+    ),
+    epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
+    batch_size: cpn.parameter(
+        type=params.conint(ge=10),
+        default=None,
+        desc="batch size, None means full batch, otherwise should be no less than 10, default None",
+    ),
+    optimizer: cpn.parameter(
+        type=params.optimizer_param(),
+        default=params.OptimizerParam(
+            method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
         ),
-        cv_param: cpn.parameter(type=params.cv_param(),
-                                default=params.CVParam(n_splits=5, shuffle=False, random_state=None),
-                                desc="cross validation param"),
-        floating_point_precision: cpn.parameter(
-            type=params.conint(ge=0),
-            default=23,
-            desc="floating point precision, "),
-        he_param: cpn.parameter(type=params.he_param(), default=params.HEParam(kind="paillier", key_length=1024),
-                                desc="homomorphic encryption param"),
-        metrics: cpn.parameter(type=params.metrics_param(), default=["mse"]),
-        output_cv_data: cpn.parameter(type=bool, default=True, desc="whether output prediction result per cv fold"),
-        cv_output_datas: cpn.dataframe_outputs(roles=[GUEST, HOST], optional=True),
+    ),
+    tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
+    early_stop: cpn.parameter(
+        type=params.string_choice(["weight_diff", "diff", "abs"]),
+        default="diff",
+        desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
+    ),
+    init_param: cpn.parameter(
+        type=params.init_param(),
+        default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
+        desc="Model param init setting.",
+    ),
+    cv_param: cpn.parameter(
+        type=params.cv_param(),
+        default=params.CVParam(n_splits=5, shuffle=False, random_state=None),
+        desc="cross validation param",
+    ),
+    floating_point_precision: cpn.parameter(type=params.conint(ge=0), default=23, desc="floating point precision, "),
+    he_param: cpn.parameter(
+        type=params.he_param(),
+        default=params.HEParam(kind="paillier", key_length=1024),
+        desc="homomorphic encryption param",
+    ),
+    metrics: cpn.parameter(type=params.metrics_param(), default=["mse"]),
+    output_cv_data: cpn.parameter(type=bool, default=True, desc="whether output prediction result per cv fold"),
+    cv_output_datas: cpn.dataframe_outputs(roles=[GUEST, HOST], optional=True),
 ):
     # temp code start
     optimizer = optimizer.dict()
     learning_rate_scheduler = learning_rate_scheduler.dict()
     init_param = init_param.dict()
     ctx.cipher.set_phe(ctx.device, he_param.dict())
     # temp code end
@@ -175,26 +211,29 @@
                 learning_rate_param=learning_rate_scheduler,
             )
             module.fit(fold_ctx)
             i += 1
         return
 
     from fate.arch.dataframe import KFold
-    kf = KFold(ctx, role=role, n_splits=cv_param.n_splits, shuffle=cv_param.shuffle, random_state=cv_param.random_state)
+
+    kf = KFold(
+        ctx, role=role, n_splits=cv_param.n_splits, shuffle=cv_param.shuffle, random_state=cv_param.random_state
+    )
     i = 0
     for fold_ctx, (train_data, validate_data) in ctx.on_cross_validations.ctxs_zip(kf.split(cv_data.read())):
         logger.info(f"enter fold {i}")
         if role.is_guest:
             module = CoordinatedLinRModuleGuest(
                 epochs=epochs,
                 batch_size=batch_size,
                 optimizer_param=optimizer,
                 learning_rate_param=learning_rate_scheduler,
                 init_param=init_param,
-                floating_point_precision=floating_point_precision
+                floating_point_precision=floating_point_precision,
             )
             module.fit(fold_ctx, train_data, validate_data)
             if output_cv_data:
                 sub_ctx = fold_ctx.sub_ctx("predict_train")
                 train_predict_df = module.predict(sub_ctx, train_data)
                 """train_predict_result = transform_to_predict_result(
                     train_data, predict_score, data_type="train"
@@ -214,46 +253,64 @@
         elif role.is_host:
             module = CoordinatedLinRModuleHost(
                 epochs=epochs,
                 batch_size=batch_size,
                 optimizer_param=optimizer,
                 learning_rate_param=learning_rate_scheduler,
                 init_param=init_param,
-                floating_point_precision=floating_point_precision
+                floating_point_precision=floating_point_precision,
             )
             module.fit(fold_ctx, train_data, validate_data)
             if output_cv_data:
                 sub_ctx = fold_ctx.sub_ctx("predict_train")
                 module.predict(sub_ctx, train_data)
                 sub_ctx = fold_ctx.sub_ctx("predict_validate")
                 module.predict(sub_ctx, validate_data)
         i += 1
 
 
-def train_guest(ctx, train_data, validate_data, train_output_data, output_model, epochs,
-                batch_size, optimizer_param, learning_rate_param, init_param, floating_point_precision, input_model):
+def train_guest(
+    ctx,
+    train_data,
+    validate_data,
+    train_output_data,
+    output_model,
+    epochs,
+    batch_size,
+    optimizer_param,
+    learning_rate_param,
+    init_param,
+    floating_point_precision,
+    input_model,
+):
     if input_model is not None:
         logger.info(f"warm start model provided")
         model = input_model.read()
         module = CoordinatedLinRModuleGuest.from_model(model)
         module.set_epochs(epochs)
         module.set_batch_size(batch_size)
     else:
-        module = CoordinatedLinRModuleGuest(epochs=epochs, batch_size=batch_size,
-                                            optimizer_param=optimizer_param, learning_rate_param=learning_rate_param,
-                                            init_param=init_param, floating_point_precision=floating_point_precision)
+        module = CoordinatedLinRModuleGuest(
+            epochs=epochs,
+            batch_size=batch_size,
+            optimizer_param=optimizer_param,
+            learning_rate_param=learning_rate_param,
+            init_param=init_param,
+            floating_point_precision=floating_point_precision,
+        )
     logger.info(f"coordinated linr guest start train")
     sub_ctx = ctx.sub_ctx("train")
     train_data = train_data.read()
     if validate_data is not None:
         validate_data = validate_data.read()
     module.fit(sub_ctx, train_data, validate_data)
     model = module.get_model()
-    output_model.write(model, metadata={"optimizer_param": optimizer_param,
-                                        "learning_rate_param": learning_rate_param})
+    output_model.write(
+        model, metadata={"optimizer_param": optimizer_param, "learning_rate_param": learning_rate_param}
+    )
 
     sub_ctx = ctx.sub_ctx("predict")
 
     predict_df = module.predict(sub_ctx, train_data)
     """predict_result = transform_to_predict_result(train_data, predict_score,
                                                  data_type="train")"""
     predict_result = tools.add_dataset_type(predict_df, consts.TRAIN_SET)
@@ -265,63 +322,89 @@
         """validate_predict_result = transform_to_predict_result(validate_data, predict_score,
                                                               data_type="validate")
                                                               """
         predict_result = DataFrame.vstack([predict_result, validate_predict_result])
     train_output_data.write(predict_result)
 
 
-def train_host(ctx, train_data, validate_data, train_output_data, output_model, epochs, batch_size,
-               optimizer_param, learning_rate_param, init_param, floating_point_precision, input_model):
+def train_host(
+    ctx,
+    train_data,
+    validate_data,
+    train_output_data,
+    output_model,
+    epochs,
+    batch_size,
+    optimizer_param,
+    learning_rate_param,
+    init_param,
+    floating_point_precision,
+    input_model,
+):
     if input_model is not None:
         logger.info(f"warm start model provided")
         model = input_model.read()
         module = CoordinatedLinRModuleHost.from_model(model)
         module.set_epochs(epochs)
         module.set_batch_size(batch_size)
     else:
-        module = CoordinatedLinRModuleHost(epochs=epochs, batch_size=batch_size,
-                                           optimizer_param=optimizer_param, learning_rate_param=learning_rate_param,
-                                           init_param=init_param, floating_point_precision=floating_point_precision)
+        module = CoordinatedLinRModuleHost(
+            epochs=epochs,
+            batch_size=batch_size,
+            optimizer_param=optimizer_param,
+            learning_rate_param=learning_rate_param,
+            init_param=init_param,
+            floating_point_precision=floating_point_precision,
+        )
     logger.info(f"coordinated linr host start train")
     sub_ctx = ctx.sub_ctx("train")
 
     train_data = train_data.read()
     if validate_data is not None:
         validate_data = validate_data.read()
     module.fit(sub_ctx, train_data, validate_data)
     model = module.get_model()
-    output_model.write(model, metadata={"optimizer_param": optimizer_param,
-                                        "learning_rate_param": learning_rate_param})
+    output_model.write(
+        model, metadata={"optimizer_param": optimizer_param, "learning_rate_param": learning_rate_param}
+    )
 
     sub_ctx = ctx.sub_ctx("predict")
     module.predict(sub_ctx, train_data)
     if validate_data is not None:
         sub_ctx = ctx.sub_ctx("validate_predict")
         module.predict(sub_ctx, validate_data)
 
 
-def train_arbiter(ctx, epochs, early_stop, tol, batch_size, optimizer_param,
-                  learning_rate_param, output_model, input_model):
+def train_arbiter(
+    ctx, epochs, early_stop, tol, batch_size, optimizer_param, learning_rate_param, output_model, input_model
+):
     if input_model is not None:
         logger.info(f"warm start model provided")
         model = input_model.read()
         module = CoordinatedLinRModuleArbiter.from_model(model)
         module.set_epochs(epochs)
         module.set_batch_size(batch_size)
     else:
-        module = CoordinatedLinRModuleArbiter(epochs=epochs, early_stop=early_stop, tol=tol, batch_size=batch_size,
-                                              optimizer_param=optimizer_param, learning_rate_param=learning_rate_param)
+        module = CoordinatedLinRModuleArbiter(
+            epochs=epochs,
+            early_stop=early_stop,
+            tol=tol,
+            batch_size=batch_size,
+            optimizer_param=optimizer_param,
+            learning_rate_param=learning_rate_param,
+        )
     logger.info(f"coordinated linr arbiter start train")
 
     sub_ctx = ctx.sub_ctx("train")
     module.fit(sub_ctx)
 
     model = module.get_model()
-    output_model.write(model, metadata={"optimizer_param": optimizer_param,
-                                        "learning_rate_param": learning_rate_param})
+    output_model.write(
+        model, metadata={"optimizer_param": optimizer_param, "learning_rate_param": learning_rate_param}
+    )
 
 
 def predict_guest(ctx, input_model, test_data, test_output_data):
     sub_ctx = ctx.sub_ctx("predict")
     model = input_model.read()
 
     module = CoordinatedLinRModuleGuest.from_model(model)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/coordinated_lr.py` & `pyfate-2.1.0/fate/components/components/coordinated_lr.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,47 +40,48 @@
         default=params.LRSchedulerParam(method="linear", scheduler_params={"start_factor": 1.0}),
         desc="learning rate scheduler, "
         "select method from {'step', 'linear', 'constant'}"
         "for list of configurable arguments, "
         "refer to torch.optim.lr_scheduler",
     ),
     epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
-        batch_size: cpn.parameter(
-            type=params.conint(ge=10),
-            default=None, desc="batch size, None means full batch, otherwise should be no less than 10, default None"
-        ),
-        optimizer: cpn.parameter(
-            type=params.optimizer_param(),
-            default=params.OptimizerParam(
-                method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
-            ),
-        ),
-        floating_point_precision: cpn.parameter(
-            type=params.conint(ge=0),
-            default=23,
-            desc="floating point precision, "),
-        tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
-        early_stop: cpn.parameter(
-            type=params.string_choice(["weight_diff", "diff", "abs"]),
-            default="diff",
-            desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
-        ),
-        he_param: cpn.parameter(type=params.he_param(), default=params.HEParam(kind="paillier", key_length=1024),
-                                desc="homomorphic encryption param"),
-        init_param: cpn.parameter(
-            type=params.init_param(),
-            default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
-            desc="Model param init setting.",
-        ),
-        threshold: cpn.parameter(
-            type=params.confloat(ge=0.0, le=1.0), default=0.5, desc="predict threshold for binary data"
+    batch_size: cpn.parameter(
+        type=params.conint(ge=10),
+        default=None,
+        desc="batch size, None means full batch, otherwise should be no less than 10, default None",
+    ),
+    optimizer: cpn.parameter(
+        type=params.optimizer_param(),
+        default=params.OptimizerParam(
+            method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
         ),
-        train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
-        output_model: cpn.json_model_output(roles=[GUEST, HOST, ARBITER]),
-        warm_start_model: cpn.json_model_input(roles=[GUEST, HOST, ARBITER], optional=True),
+    ),
+    floating_point_precision: cpn.parameter(type=params.conint(ge=0), default=23, desc="floating point precision, "),
+    tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
+    early_stop: cpn.parameter(
+        type=params.string_choice(["weight_diff", "diff", "abs"]),
+        default="diff",
+        desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
+    ),
+    he_param: cpn.parameter(
+        type=params.he_param(),
+        default=params.HEParam(kind="paillier", key_length=1024),
+        desc="homomorphic encryption param",
+    ),
+    init_param: cpn.parameter(
+        type=params.init_param(),
+        default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
+        desc="Model param init setting.",
+    ),
+    threshold: cpn.parameter(
+        type=params.confloat(ge=0.0, le=1.0), default=0.5, desc="predict threshold for binary data"
+    ),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    output_model: cpn.json_model_output(roles=[GUEST, HOST, ARBITER]),
+    warm_start_model: cpn.json_model_input(roles=[GUEST, HOST, ARBITER], optional=True),
 ):
     logger.info(f"enter coordinated lr train")
     # temp code start
     optimizer = optimizer.dict()
     learning_rate_scheduler = learning_rate_scheduler.dict()
     init_param = init_param.dict()
     ctx.cipher.set_phe(ctx.device, he_param.dict())
@@ -95,40 +96,43 @@
             epochs,
             batch_size,
             optimizer,
             learning_rate_scheduler,
             init_param,
             threshold,
             floating_point_precision,
-            warm_start_model
+            warm_start_model,
         )
     elif role.is_host:
         train_host(
             ctx,
             train_data,
             validate_data,
             train_output_data,
             output_model,
             epochs,
             batch_size,
             optimizer,
             learning_rate_scheduler,
             init_param,
             floating_point_precision,
-            warm_start_model
+            warm_start_model,
         )
     elif role.is_arbiter:
-        train_arbiter(ctx,
-                      epochs,
-                      early_stop,
-                      tol, batch_size,
-                      optimizer,
-                      learning_rate_scheduler,
-                      output_model,
-                      warm_start_model)
+        train_arbiter(
+            ctx,
+            epochs,
+            early_stop,
+            tol,
+            batch_size,
+            optimizer,
+            learning_rate_scheduler,
+            output_model,
+            warm_start_model,
+        )
 
 
 @coordinated_lr.predict()
 def predict(
     ctx,
     role: Role,
     # threshold: cpn.parameter(type=params.confloat(ge=0.0, le=1.0), default=0.5),
@@ -140,62 +144,65 @@
         predict_guest(ctx, input_model, test_data, test_output_data)
     if role.is_host:
         predict_host(ctx, input_model, test_data, test_output_data)
 
 
 @coordinated_lr.cross_validation()
 def cross_validation(
-        ctx: Context,
-        role: Role,
-        cv_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        learning_rate_scheduler: cpn.parameter(
-            type=params.lr_scheduler_param(),
-            default=params.LRSchedulerParam(method="linear", scheduler_params={"start_factor": 1.0}),
-            desc="learning rate scheduler, "
-                 "select method from {'step', 'linear', 'constant'}"
-                 "for list of configurable arguments, "
-                 "refer to torch.optim.lr_scheduler",
-        ),
-        epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
-        batch_size: cpn.parameter(
-            type=params.conint(ge=10),
-            default=None, desc="batch size, None means full batch, otherwise should be no less than 10, default None"
-        ),
-        optimizer: cpn.parameter(
-            type=params.optimizer_param(),
-            default=params.OptimizerParam(
-                method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
-            ),
-        ),
-        tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
-        early_stop: cpn.parameter(
-            type=params.string_choice(["weight_diff", "diff", "abs"]),
-            default="diff",
-            desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
-        ),
-        init_param: cpn.parameter(
-            type=params.init_param(),
-            default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
-            desc="Model param init setting.",
-        ),
-        threshold: cpn.parameter(
-            type=params.confloat(ge=0.0, le=1.0), default=0.5, desc="predict threshold for binary data"
+    ctx: Context,
+    role: Role,
+    cv_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    learning_rate_scheduler: cpn.parameter(
+        type=params.lr_scheduler_param(),
+        default=params.LRSchedulerParam(method="linear", scheduler_params={"start_factor": 1.0}),
+        desc="learning rate scheduler, "
+        "select method from {'step', 'linear', 'constant'}"
+        "for list of configurable arguments, "
+        "refer to torch.optim.lr_scheduler",
+    ),
+    epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
+    batch_size: cpn.parameter(
+        type=params.conint(ge=10),
+        default=None,
+        desc="batch size, None means full batch, otherwise should be no less than 10, default None",
+    ),
+    optimizer: cpn.parameter(
+        type=params.optimizer_param(),
+        default=params.OptimizerParam(
+            method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
         ),
-        he_param: cpn.parameter(type=params.he_param(), default=params.HEParam(kind="paillier", key_length=1024),
-                                desc="homomorphic encryption param"),
-        floating_point_precision: cpn.parameter(
-            type=params.conint(ge=0),
-            default=23,
-            desc="floating point precision, "),
-        cv_param: cpn.parameter(type=params.cv_param(),
-                                default=params.CVParam(n_splits=5, shuffle=False, random_state=None),
-                                desc="cross validation param"),
-        metrics: cpn.parameter(type=params.metrics_param(), default=["auc"]),
-        output_cv_data: cpn.parameter(type=bool, default=True, desc="whether output prediction result per cv fold"),
-        cv_output_datas: cpn.dataframe_outputs(roles=[GUEST, HOST], optional=True),
+    ),
+    tol: cpn.parameter(type=params.confloat(ge=0), default=1e-4),
+    early_stop: cpn.parameter(
+        type=params.string_choice(["weight_diff", "diff", "abs"]),
+        default="diff",
+        desc="early stopping criterion, choose from {weight_diff, diff, abs, val_metrics}",
+    ),
+    init_param: cpn.parameter(
+        type=params.init_param(),
+        default=params.InitParam(method="random_uniform", fit_intercept=True, random_state=None),
+        desc="Model param init setting.",
+    ),
+    threshold: cpn.parameter(
+        type=params.confloat(ge=0.0, le=1.0), default=0.5, desc="predict threshold for binary data"
+    ),
+    he_param: cpn.parameter(
+        type=params.he_param(),
+        default=params.HEParam(kind="paillier", key_length=1024),
+        desc="homomorphic encryption param",
+    ),
+    floating_point_precision: cpn.parameter(type=params.conint(ge=0), default=23, desc="floating point precision, "),
+    cv_param: cpn.parameter(
+        type=params.cv_param(),
+        default=params.CVParam(n_splits=5, shuffle=False, random_state=None),
+        desc="cross validation param",
+    ),
+    metrics: cpn.parameter(type=params.metrics_param(), default=["auc"]),
+    output_cv_data: cpn.parameter(type=bool, default=True, desc="whether output prediction result per cv fold"),
+    cv_output_datas: cpn.dataframe_outputs(roles=[GUEST, HOST], optional=True),
 ):
     optimizer = optimizer.dict()
     learning_rate_scheduler = learning_rate_scheduler.dict()
     init_param = init_param.dict()
     ctx.cipher.set_phe(ctx.device, he_param.dict())
 
     if role.is_arbiter:
@@ -211,15 +218,18 @@
                 learning_rate_param=learning_rate_scheduler,
             )
             module.fit(fold_ctx)
             i += 1
         return
 
     from fate.arch.dataframe import KFold
-    kf = KFold(ctx, role=role, n_splits=cv_param.n_splits, shuffle=cv_param.shuffle, random_state=cv_param.random_state)
+
+    kf = KFold(
+        ctx, role=role, n_splits=cv_param.n_splits, shuffle=cv_param.shuffle, random_state=cv_param.random_state
+    )
     i = 0
     for fold_ctx, (train_data, validate_data) in ctx.on_cross_validations.ctxs_zip(kf.split(cv_data.read())):
         logger.info(f"enter fold {i}")
         if role.is_guest:
             module = CoordinatedLRModuleGuest(
                 epochs=epochs,
                 batch_size=batch_size,
@@ -252,39 +262,39 @@
         elif role.is_host:
             module = CoordinatedLRModuleHost(
                 epochs=epochs,
                 batch_size=batch_size,
                 optimizer_param=optimizer,
                 learning_rate_param=learning_rate_scheduler,
                 init_param=init_param,
-                floating_point_precision=floating_point_precision
+                floating_point_precision=floating_point_precision,
             )
             module.fit(fold_ctx, train_data, validate_data)
             if output_cv_data:
                 sub_ctx = fold_ctx.sub_ctx("predict_train")
                 module.predict(sub_ctx, train_data)
                 sub_ctx = fold_ctx.sub_ctx("predict_validate")
                 module.predict(sub_ctx, validate_data)
         i += 1
 
 
 def train_guest(
     ctx,
-        train_data,
-        validate_data,
-        train_output_data,
-        output_model,
-        epochs,
-        batch_size,
-        optimizer_param,
-        learning_rate_param,
-        init_param,
-        threshold,
-        floating_point_precision,
-        input_model
+    train_data,
+    validate_data,
+    train_output_data,
+    output_model,
+    epochs,
+    batch_size,
+    optimizer_param,
+    learning_rate_param,
+    init_param,
+    threshold,
+    floating_point_precision,
+    input_model,
 ):
     if input_model is not None:
         logger.info(f"warm start model provided")
         model = input_model.read()
         module = CoordinatedLRModuleGuest.from_model(model)
         module.set_epochs(epochs)
         module.set_batch_size(batch_size)
@@ -293,15 +303,15 @@
         module = CoordinatedLRModuleGuest(
             epochs=epochs,
             batch_size=batch_size,
             optimizer_param=optimizer_param,
             learning_rate_param=learning_rate_param,
             init_param=init_param,
             threshold=threshold,
-            floating_point_precision=floating_point_precision
+            floating_point_precision=floating_point_precision,
         )
     # optimizer = optimizer_factory(optimizer_param)
     logger.info(f"coordinated lr guest start train")
     sub_ctx = ctx.sub_ctx("train")
     train_data = train_data.read()
 
     if validate_data is not None:
@@ -332,41 +342,41 @@
         )"""
         validate_predict_result = tools.add_dataset_type(predict_df, consts.VALIDATE_SET)
         predict_result = DataFrame.vstack([predict_result, validate_predict_result])
     train_output_data.write(predict_result)
 
 
 def train_host(
-        ctx,
-        train_data,
-        validate_data,
-        train_output_data,
-        output_model,
-        epochs,
-        batch_size,
-        optimizer_param,
-        learning_rate_param,
-        init_param,
-        floating_point_precision,
-        input_model
+    ctx,
+    train_data,
+    validate_data,
+    train_output_data,
+    output_model,
+    epochs,
+    batch_size,
+    optimizer_param,
+    learning_rate_param,
+    init_param,
+    floating_point_precision,
+    input_model,
 ):
     if input_model is not None:
         logger.info(f"warm start model provided")
         model = input_model.read()
         module = CoordinatedLRModuleHost.from_model(model)
         module.set_epochs(epochs)
         module.set_batch_size(batch_size)
     else:
         module = CoordinatedLRModuleHost(
             epochs=epochs,
             batch_size=batch_size,
             optimizer_param=optimizer_param,
             learning_rate_param=learning_rate_param,
             init_param=init_param,
-            floating_point_precision=floating_point_precision
+            floating_point_precision=floating_point_precision,
         )
     logger.info(f"coordinated lr host start train")
     sub_ctx = ctx.sub_ctx("train")
     train_data = train_data.read()
 
     if validate_data is not None:
         logger.info(f"validate data provided")
@@ -378,16 +388,17 @@
     sub_ctx = ctx.sub_ctx("predict")
     module.predict(sub_ctx, train_data)
     if validate_data is not None:
         sub_ctx = ctx.sub_ctx("validate_predict")
         module.predict(sub_ctx, validate_data)
 
 
-def train_arbiter(ctx, epochs, early_stop, tol, batch_size, optimizer_param, learning_rate_scheduler,
-                  output_model, input_model):
+def train_arbiter(
+    ctx, epochs, early_stop, tol, batch_size, optimizer_param, learning_rate_scheduler, output_model, input_model
+):
     if input_model is not None:
         logger.info(f"warm start model provided")
         model = input_model.read()
         module = CoordinatedLRModuleArbiter.from_model(model)
         module.set_epochs(epochs)
         module.set_batch_size(batch_size)
     else:
```

### Comparing `pyfate-2.0.0b0/fate/components/components/data_split.py` & `pyfate-2.1.0/fate/components/components/data_split.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,71 +9,82 @@
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-import logging
 from typing import Union
 
 from fate.arch import Context
 from fate.components.core import GUEST, HOST, Role, cpn, params
 from fate.ml.model_selection.data_split import DataSplitModuleGuest, DataSplitModuleHost
 
-logger = logging.getLogger(__name__)
-
 
 @cpn.component(roles=[GUEST, HOST], provider="fate")
 def data_split(
-        ctx: Context,
-        role: Role,
-        input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        train_size: cpn.parameter(type=Union[params.confloat(ge=0.0, le=1.0), params.conint(ge=0)], default=None,
-                                  desc="size of output training data, "
-                                       "should be either int for exact sample size or float for fraction"),
-        validate_size: cpn.parameter(type=Union[params.confloat(ge=0.0, le=1.0), params.conint(ge=0)], default=None,
-                                     desc="size of output validation data, "
-                                          "should be either int for exact sample size or float for fraction"),
-        test_size: cpn.parameter(type=Union[params.confloat(ge=0.0, le=1.0), params.conint(ge=0)], default=None,
-                                 desc="size of output test data, "
-                                      "should be either int for exact sample size or float for fraction"),
-        stratified: cpn.parameter(type=bool, default=False,
-                                  desc="whether sample with stratification, "
-                                       "should not use this for data with continuous label values"),
-        random_state: cpn.parameter(type=params.conint(ge=0), default=None, desc="random state"),
-        hetero_sync: cpn.parameter(type=bool, default=True,
-                                   desc="whether guest sync data set sids with host, "
-                                        "default True for hetero scenario, "
-                                        "should set to False for local and homo scenario"),
-        train_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
-        validate_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
-        test_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
+    ctx: Context,
+    role: Role,
+    input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    train_size: cpn.parameter(
+        type=Union[params.confloat(ge=0.0, le=1.0), params.conint(ge=0)],
+        default=None,
+        desc="size of output training data, " "should be either int for exact sample size or float for fraction",
+    ),
+    validate_size: cpn.parameter(
+        type=Union[params.confloat(ge=0.0, le=1.0), params.conint(ge=0)],
+        default=None,
+        desc="size of output validation data, " "should be either int for exact sample size or float for fraction",
+    ),
+    test_size: cpn.parameter(
+        type=Union[params.confloat(ge=0.0, le=1.0), params.conint(ge=0)],
+        default=None,
+        desc="size of output test data, " "should be either int for exact sample size or float for fraction",
+    ),
+    stratified: cpn.parameter(
+        type=bool,
+        default=False,
+        desc="whether sample with stratification, " "should not use this for data with continuous label values",
+    ),
+    random_state: cpn.parameter(type=params.conint(ge=0), default=None, desc="random state"),
+    hetero_sync: cpn.parameter(
+        type=bool,
+        default=True,
+        desc="whether guest sync data set sids with host, "
+        "default True for hetero scenario, "
+        "should set to False for local and homo scenario",
+    ),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
+    validate_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
+    test_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
 ):
     if train_size is None and validate_size is None and test_size is None:
         train_size = 0.8
         validate_size = 0.2
         test_size = 0.0
 
-    # logger.info(f"in cpn received train_size: {train_size}, validate_size: {validate_size}, test_size: {test_size}")
     # check if local but federated sample
     if hetero_sync and len(ctx.parties.ranks) < 2:
         raise ValueError(f"federated sample can only be called when both 'guest' and 'host' present. Please check")
 
     sub_ctx = ctx.sub_ctx("train")
     if role.is_guest:
         module = DataSplitModuleGuest(train_size, validate_size, test_size, stratified, random_state, hetero_sync)
     elif role.is_host:
         module = DataSplitModuleHost(train_size, validate_size, test_size, stratified, random_state, hetero_sync)
     input_data = input_data.read()
 
     train_data_set, validate_data_set, test_data_set = module.fit(sub_ctx, input_data)
     # train_data_set, validate_data_set, test_data_set = module.split_data(train_data)
-    logger.info(f"output train size: {train_data_set.shape if train_data_set else None}, "
-                f"validate size: {validate_data_set.shape if validate_data_set else None},"
-                f"test size: {test_data_set.shape if test_data_set else None}")
+    data_split_summary = {
+        "original_count": input_data.shape[0],
+        "train_count": train_data_set.shape[0] if train_data_set else 0,
+        "validate_count": validate_data_set.shape[0] if validate_data_set else 0,
+        "test_count": test_data_set.shape[0] if test_data_set else 0,
+    }
+    ctx.metrics.log_metrics(data_split_summary, name="summary", type="data_split")
     if train_data_set:
         train_output_data.write(train_data_set)
     if validate_data_set:
         validate_output_data.write(validate_data_set)
     if test_data_set:
         test_output_data.write(test_data_set)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/dataframe_io_test.py` & `pyfate-2.1.0/fate/components/components/dataframe_io_test.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/components/dataframe_transformer.py` & `pyfate-2.1.0/fate/components/components/dataframe_transformer.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,23 +9,23 @@
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
-from fate.components.core import LOCAL, Role, cpn
+from fate.components.core import LOCAL, Role, cpn, GUEST, HOST
 
 
-@cpn.component(roles=[LOCAL])
+@cpn.component(roles=[LOCAL, GUEST, HOST])
 def dataframe_transformer(
     ctx,
     role: Role,
-    table: cpn.table_input(roles=[LOCAL]),
-    dataframe_output: cpn.dataframe_output(roles=[LOCAL]),
+    table: cpn.table_input(roles=[LOCAL, GUEST, HOST]),
+    dataframe_output: cpn.dataframe_output(roles=[LOCAL, GUEST, HOST]),
     namespace: cpn.parameter(type=str, default=None, optional=True),
     name: cpn.parameter(type=str, default=None, optional=True),
     site_name: cpn.parameter(type=str, default=None, optional=True),
 ):
     from fate.arch.dataframe import TableReader
 
     table = table.read()
```

### Comparing `pyfate-2.0.0b0/fate/components/components/evaluation.py` & `pyfate-2.1.0/fate/components/components/evaluation.py`

 * *Files 8% similar despite different names*

```diff
@@ -29,82 +29,83 @@
 from fate.ml.utils.predict_tools import PREDICT_SCORE, PREDICT_RESULT, LABEL
 from fate.components.components.utils.consts import BINARY, REGRESSION, MULTI
 
 logger = logging.getLogger(__name__)
 
 
 def split_dataframe_by_type(input_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
-
     if "type" in input_df.columns:
         return {dataset_type: input_df[input_df["type"] == dataset_type] for dataset_type in input_df["type"].unique()}
     else:
         return {"origin": input_df}
 
 
 @cpn.component(roles=[GUEST, HOST])
 def evaluation(
     ctx: Context,
     role: Role,
-    input_data: cpn.dataframe_inputs(roles=[GUEST, HOST]),
+    input_datas: cpn.dataframe_inputs(roles=[GUEST, HOST]),
     default_eval_setting: cpn.parameter(
         type=string_choice(choice=["binary", "multi", "regression"]), default="binary", optional=True
     ),
     metrics: cpn.parameter(type=list, default=None, optional=True),
-    predict_column_name: cpn.parameter(type=str, default=None, optional=True,
-                                        desc="predict data column name, if None(default), will use \
+    predict_column_name: cpn.parameter(
+        type=str,
+        default=None,
+        optional=True,
+        desc="predict data column name, if None(default), will use \
                                         'predict_score' in the input dataframe when the default setting are binary and regression, \
-                                        and use 'predict_result' if default setting is multi"),
-    label_column_name: cpn.parameter(type=str, default=None, optional=True, desc="label data column namem if None(default), \
-                                     will use 'label' in the input dataframe")
+                                        and use 'predict_result' if default setting is multi",
+    ),
+    label_column_name: cpn.parameter(
+        type=str,
+        default=None,
+        optional=True,
+        desc="label data column namem if None(default), \
+                                     will use 'label' in the input dataframe",
+    ),
 ):
-
     if role.is_arbiter:
         return
     else:
-
         if metrics is not None:
             metrics_ensemble = get_specified_metrics(metrics)
             predict_col = predict_column_name if predict_column_name is not None else PREDICT_SCORE
             label_col = label_column_name if label_column_name is not None else LABEL
         else:
             if default_eval_setting == MULTI:
                 metrics_ensemble = get_multi_metrics()
                 predict_col = predict_column_name if predict_column_name is not None else PREDICT_RESULT
                 label_col = label_column_name if label_column_name is not None else LABEL
             else:
                 if default_eval_setting == BINARY:
                     metrics_ensemble = get_binary_metrics()
-                elif default_eval_setting ==  REGRESSION:
+                elif default_eval_setting == REGRESSION:
                     metrics_ensemble = get_regression_metrics()
                 else:
                     raise ValueError("default_eval_setting should be one of binary, multi, regression, got {}")
                 predict_col = predict_column_name if predict_column_name is not None else PREDICT_SCORE
                 label_col = label_column_name if label_column_name is not None else LABEL
 
-        df_list = [_input.read() for _input in input_data]
-        task_names = [_input.artifact.metadata.source.task_name for _input in input_data]
+        df_list = [_input.read() for _input in input_datas]
+        task_names = [_input.artifact.metadata.source.task_name for _input in input_datas]
         eval_rs = {}
-        logger.info('components names are {}'.format(task_names))
+        logger.info("components names are {}".format(task_names))
         for name, df in zip(task_names, df_list):
             rs_ = evaluate(df, metrics_ensemble, predict_col, label_col)
             eval_rs[name] = rs_
 
-    ctx.metrics.log_metrics(eval_rs, name='evaluation', type='evaluation')
-    logger.info("eval result: {}".format(eval_rs))
-
+    ctx.metrics.log_metrics(eval_rs, name="evaluation", type="evaluation")
 
-def evaluate(input_data, metrics, predict_col, label_col):
 
-    data = input_data.as_pd_df()
+def evaluate(input_datas, metrics, predict_col, label_col):
+    data = input_datas.as_pd_df()
     split_dict = split_dataframe_by_type(data)
     rs_dict = {}
-    logger.info('eval dataframe is {}'.format(data))
     for name, df in split_dict.items():
-        
-        logger.info('eval dataframe is \n\n{}'.format(df))
         y_true = df[label_col]
         # in case is multi result, use tolist
         y_pred = df[predict_col]
         rs = metrics(predict=y_pred, label=y_true)
         rs_dict[name] = rs
 
     return rs_dict
```

### Comparing `pyfate-2.0.0b0/fate/components/components/feature_scale.py` & `pyfate-2.1.0/fate/components/components/feature_scale.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,51 +25,51 @@
 @cpn.component(roles=[GUEST, HOST], provider="fate")
 def feature_scale(ctx, role):
     ...
 
 
 @feature_scale.train()
 def feature_scale_train(
-        ctx: Context,
-        role: Role,
-        train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        method: cpn.parameter(type=params.string_choice(["standard", "min_max"]), default="standard", optional=False),
-        feature_range: cpn.parameter(
-            type=Union[list, dict],
-            default=[0, 1],
-            optional=True,
-            desc="Result feature value range for `min_max` method, "
-                 "take either dict in format: {col_name: [min, max]} for specific columns "
-                 "or [min, max] for all columns. Columns unspecified will be scaled to default range [0,1]",
+    ctx: Context,
+    role: Role,
+    train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    method: cpn.parameter(type=params.string_choice(["standard", "min_max"]), default="standard", optional=False),
+    feature_range: cpn.parameter(
+        type=Union[list, dict],
+        default=[0, 1],
+        optional=True,
+        desc="Result feature value range for `min_max` method, "
+        "take either dict in format: {col_name: [min, max]} for specific columns "
+        "or [min, max] for all columns. Columns unspecified will be scaled to default range [0,1]",
     ),
-        scale_col: cpn.parameter(
-            type=List[str],
-            default=None,
-            optional=True,
-            desc="list of column names to be scaled, if None, all columns will be scaled; "
-                 "only one of {scale_col, scale_idx} should be specified",
-        ),
-        scale_idx: cpn.parameter(
-            type=List[params.conint(ge=0)],
-            default=None,
-            optional=True,
-            desc="list of column index to be scaled, if None, all columns will be scaled; "
-                 "only one of {scale_col, scale_idx} should be specified",
-        ),
-        strict_range: cpn.parameter(
-            type=bool,
-            default=True,
-            desc="whether transformed value to be strictly restricted within given range; "
-                 "effective for 'min_max' scale method only",
-        ),
-        use_anonymous: cpn.parameter(
-            type=bool, default=False, desc="bool, whether interpret `scale_col` as anonymous column names"
-        ),
-        train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
-        output_model: cpn.json_model_output(roles=[GUEST, HOST]),
+    scale_col: cpn.parameter(
+        type=List[str],
+        default=None,
+        optional=True,
+        desc="list of column names to be scaled, if None, all columns will be scaled; "
+        "only one of {scale_col, scale_idx} should be specified",
+    ),
+    scale_idx: cpn.parameter(
+        type=List[params.conint(ge=0)],
+        default=None,
+        optional=True,
+        desc="list of column index to be scaled, if None, all columns will be scaled; "
+        "only one of {scale_col, scale_idx} should be specified",
+    ),
+    strict_range: cpn.parameter(
+        type=bool,
+        default=True,
+        desc="whether transformed value to be strictly restricted within given range; "
+        "effective for 'min_max' scale method only",
+    ),
+    use_anonymous: cpn.parameter(
+        type=bool, default=False, desc="bool, whether interpret `scale_col` as anonymous column names"
+    ),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    output_model: cpn.json_model_output(roles=[GUEST, HOST]),
 ):
     train(
         ctx,
         train_data,
         train_output_data,
         output_model,
         method,
@@ -79,19 +79,19 @@
         strict_range,
         use_anonymous,
     )
 
 
 @feature_scale.predict()
 def feature_scale_predict(
-        ctx: Context,
-        role: Role,
-        test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        input_model: cpn.json_model_input(roles=[GUEST, HOST]),
-        test_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    ctx: Context,
+    role: Role,
+    test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    input_model: cpn.json_model_input(roles=[GUEST, HOST]),
+    test_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
 ):
     predict(ctx, input_model, test_data, test_output_data)
 
 
 def train(
     ctx,
     train_data,
@@ -162,10 +162,9 @@
         if isinstance(feature_range, dict):
             for col in select_col:
                 if col not in feature_range:
                     feature_range[col] = [0, 1]
         else:
             feature_range = {col: feature_range for col in select_col}
     if len(select_col) == 0:
-        logger.warning(f"No cols provided. "
-                       f"To scale all columns, please set `scale_col` to None.")
+        logger.warning(f"No cols provided. " f"To scale all columns, please set `scale_col` to None.")
     return select_col, feature_range
```

### Comparing `pyfate-2.0.0b0/fate/components/components/hetero_feature_binning.py` & `pyfate-2.1.0/fate/components/components/hetero_feature_binning.py`

 * *Files 13% similar despite different names*

```diff
@@ -33,111 +33,190 @@
                desc="dict of format {col_name: [bins]} which specifies bin edges for each feature, "
                     "including right edge of last bin")
 """
 
 
 @hetero_feature_binning.train()
 def feature_binning_train(
-        ctx: Context,
-        role: Role,
-        train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        method: cpn.parameter(type=params.string_choice(["quantile", "bucket", "manual"]),
-                              default="quantile", optional=False,
-                              desc="binning method, options: {quantile, bucket, manual}"),
-        n_bins: cpn.parameter(type=params.conint(gt=1), default=10,
-                              desc="max number of bins, should be no less than 2"),
-        split_pt_dict: cpn.parameter(type=dict, default=None, optional=True,
-                                     desc="dict, manually provided split points, "
-                                          "only effective when `method`='manual'"),
-        bin_col: cpn.parameter(type=List[str], default=None,
-                               desc="list of column names to be binned, if None, all columns will be binned; "
-                                    "only one of {bin_col, bin_idx} should be specified"),
-        bin_idx: cpn.parameter(type=List[params.conint(ge=0)], default=None,
-                               desc="list of column index to be binned, if None, all columns will be binned; "
-                                    "only one of {bin_col, bin_idx} should be specified"),
-        category_col: cpn.parameter(type=List[str], default=None,
-                                    desc="list of column names to be treated as categorical "
-                                         "features and will not be binned; "
-                                         "only one of {category_col, category_idx} should be specified"
-                                         "note that metrics will be computed over categorical features "
-                                         "if this param is specified"),
-        category_idx: cpn.parameter(type=List[params.conint(ge=0)], default=None,
-                                    desc="list of column index to be treated as categorical features "
-                                         "and will not be binned; "
-                                         "only one of {category_col, category_idx} should be specified"
-                                         "note that metrics will be computed over categorical features "
-                                         "if this param is specified"),
-        use_anonymous: cpn.parameter(type=bool, default=False,
-                                     desc="bool, whether interpret `bin_col` & `category_col` "
-                                          "as anonymous column names"),
-        transform_method: cpn.parameter(type=params.string_choice(['woe', 'bin_idx']),
-                                        default=None,  # may support user-provided dict in future release
-                                        desc="str, values to which binned data will be transformed, "
-                                             "select from {'woe', 'bin_idx'}; "
-                                             "note that host will not transform features "
-                                             "to woe values regardless of setting"),
-        skip_metrics: cpn.parameter(type=bool, default=False,
-                                    desc="bool, whether compute host's metrics or not"),
-        local_only: cpn.parameter(type=bool, default=False, desc="bool, whether compute host's metrics or not"),
-        relative_error: cpn.parameter(type=params.confloat(gt=0, le=1), default=1e-6,
-                                      desc="float, error rate for quantile"),
-        adjustment_factor: cpn.parameter(type=params.confloat(gt=0), default=0.5,
-                                         desc="float, useful when here is no event or non-event in a bin"),
-        he_param: cpn.parameter(type=params.he_param(), default=params.HEParam(kind="paillier", key_length=1024),
-                                desc="homomorphic encryption param"),
-        train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
-        output_model: cpn.json_model_output(roles=[GUEST, HOST]),
+    ctx: Context,
+    role: Role,
+    train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    method: cpn.parameter(
+        type=params.string_choice(["quantile", "bucket", "manual"]),
+        default="quantile",
+        optional=False,
+        desc="binning method, options: {quantile, bucket, manual}",
+    ),
+    n_bins: cpn.parameter(type=params.conint(gt=1), default=10, desc="max number of bins, should be no less than 2"),
+    split_pt_dict: cpn.parameter(
+        type=dict,
+        default=None,
+        optional=True,
+        desc="dict, manually provided split points, " "only effective when `method`='manual'",
+    ),
+    bin_col: cpn.parameter(
+        type=List[str],
+        default=None,
+        desc="list of column names to be binned, if None, all columns will be binned; "
+        "only one of {bin_col, bin_idx} should be specified",
+    ),
+    bin_idx: cpn.parameter(
+        type=List[params.conint(ge=0)],
+        default=None,
+        desc="list of column index to be binned, if None, all columns will be binned; "
+        "only one of {bin_col, bin_idx} should be specified",
+    ),
+    category_col: cpn.parameter(
+        type=List[str],
+        default=None,
+        desc="list of column names to be treated as categorical "
+        "features and will not be binned; "
+        "only one of {category_col, category_idx} should be specified"
+        "note that metrics will be computed over categorical features "
+        "if this param is specified",
+    ),
+    category_idx: cpn.parameter(
+        type=List[params.conint(ge=0)],
+        default=None,
+        desc="list of column index to be treated as categorical features "
+        "and will not be binned; "
+        "only one of {category_col, category_idx} should be specified"
+        "note that metrics will be computed over categorical features "
+        "if this param is specified",
+    ),
+    use_anonymous: cpn.parameter(
+        type=bool,
+        default=False,
+        desc="bool, whether interpret `bin_col` & `category_col` " "as anonymous column names",
+    ),
+    transform_method: cpn.parameter(
+        type=params.string_choice(["woe", "bin_idx"]),
+        default=None,  # may support user-provided dict in future release
+        desc="str, values to which binned data will be transformed, "
+        "select from {'woe', 'bin_idx'}; "
+        "note that host will not transform features "
+        "to woe values regardless of setting",
+    ),
+    skip_metrics: cpn.parameter(type=bool, default=False, desc="bool, whether compute host's metrics or not"),
+    local_only: cpn.parameter(type=bool, default=False, desc="bool, whether compute host's metrics or not"),
+    relative_error: cpn.parameter(
+        type=params.confloat(gt=0, le=1), default=1e-6, desc="float, error rate for quantile"
+    ),
+    adjustment_factor: cpn.parameter(
+        type=params.confloat(gt=0), default=0.5, desc="float, useful when here is no event or non-event in a bin"
+    ),
+    he_param: cpn.parameter(
+        type=params.he_param(),
+        default=params.HEParam(kind="paillier", key_length=1024),
+        desc="homomorphic encryption param",
+    ),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    output_model: cpn.json_model_output(roles=[GUEST, HOST]),
 ):
     ctx.cipher.set_phe(ctx.device, he_param.dict())
-    train(ctx, train_data, train_output_data, output_model, role, method, n_bins, split_pt_dict,
-          bin_col, bin_idx, category_col, category_idx, use_anonymous, transform_method,
-          skip_metrics, local_only, relative_error, adjustment_factor)
+    train(
+        ctx,
+        train_data,
+        train_output_data,
+        output_model,
+        role,
+        method,
+        n_bins,
+        split_pt_dict,
+        bin_col,
+        bin_idx,
+        category_col,
+        category_idx,
+        use_anonymous,
+        transform_method,
+        skip_metrics,
+        local_only,
+        relative_error,
+        adjustment_factor,
+    )
 
 
 @hetero_feature_binning.predict()
 def feature_binning_predict(
-        ctx: Context,
-        role: Role,
-        test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        input_model: cpn.json_model_input(roles=[GUEST, HOST]),
-        transform_method: cpn.parameter(type=params.string_choice(['woe', 'bin_idx']),
-                                        default=None,  # may support user-provided dict in future release
-                                        desc="str, values to which binned data will be transformed, "
-                                             "select from {'woe', 'bin_idx'}; "
-                                             "note that host will not transform features "
-                                             "to woe values regardless of setting"),
-        skip_metrics: cpn.parameter(type=bool, default=False,
-                                    desc="bool, whether compute host's metrics or not"),
-        test_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    ctx: Context,
+    role: Role,
+    test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    input_model: cpn.json_model_input(roles=[GUEST, HOST]),
+    transform_method: cpn.parameter(
+        type=params.string_choice(["woe", "bin_idx"]),
+        default=None,  # may support user-provided dict in future release
+        desc="str, values to which binned data will be transformed, "
+        "select from {'woe', 'bin_idx'}; "
+        "note that host will not transform features "
+        "to woe values regardless of setting",
+    ),
+    skip_metrics: cpn.parameter(type=bool, default=False, desc="bool, whether compute host's metrics or not"),
+    test_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
 ):
     predict(ctx, input_model, test_data, test_output_data, role, transform_method, skip_metrics)
 
 
-def train(ctx, train_data, train_output_data, output_model, role, method, n_bins, split_pt_dict,
-          bin_col, bin_idx, category_col, category_idx, use_anonymous, transform_method,
-          skip_metrics, local_only, relative_error, adjustment_factor):
+def train(
+    ctx,
+    train_data,
+    train_output_data,
+    output_model,
+    role,
+    method,
+    n_bins,
+    split_pt_dict,
+    bin_col,
+    bin_idx,
+    category_col,
+    category_idx,
+    use_anonymous,
+    transform_method,
+    skip_metrics,
+    local_only,
+    relative_error,
+    adjustment_factor,
+):
     logger.info(f"start binning train")
     sub_ctx = ctx.sub_ctx("train")
     train_data = train_data.read()
     columns = train_data.schema.columns.to_list()
     anonymous_columns = None
     if use_anonymous:
         anonymous_columns = train_data.schema.anonymous_columns.to_list()
         split_pt_dict = {columns[anonymous_columns.index(col)]: split_pt_dict[col] for col in split_pt_dict.keys()}
-    to_bin_cols, merged_category_col = get_to_bin_cols(columns, anonymous_columns,
-                                                       bin_col, bin_idx, category_col, category_idx)
+    to_bin_cols, merged_category_col = get_to_bin_cols(
+        columns, anonymous_columns, bin_col, bin_idx, category_col, category_idx
+    )
     if split_pt_dict:
         to_bin_cols = list(set(to_bin_cols).intersection(split_pt_dict.keys()))
 
     if role.is_guest:
-        binning = HeteroBinningModuleGuest(method, n_bins, split_pt_dict, to_bin_cols, transform_method,
-                                           merged_category_col, local_only, relative_error, adjustment_factor)
+        binning = HeteroBinningModuleGuest(
+            method,
+            n_bins,
+            split_pt_dict,
+            to_bin_cols,
+            transform_method,
+            merged_category_col,
+            local_only,
+            relative_error,
+            adjustment_factor,
+        )
     elif role.is_host:
-        binning = HeteroBinningModuleHost(method, n_bins, split_pt_dict, to_bin_cols, transform_method,
-                                          merged_category_col, local_only, relative_error, adjustment_factor)
+        binning = HeteroBinningModuleHost(
+            method,
+            n_bins,
+            split_pt_dict,
+            to_bin_cols,
+            transform_method,
+            merged_category_col,
+            local_only,
+            relative_error,
+            adjustment_factor,
+        )
     else:
         raise ValueError(f"unknown role: {role}")
     binning.fit(sub_ctx, train_data)
     binned_data = None
     if not skip_metrics:
         binned_data = binning._bin_obj.bucketize_data(train_data)
         binning.compute_metrics(sub_ctx, binned_data)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/hetero_feature_selection.py` & `pyfate-2.1.0/fate/components/components/hetero_feature_selection.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,28 +25,28 @@
 @cpn.component(roles=[GUEST, HOST], provider="fate")
 def hetero_feature_selection(ctx, role):
     ...
 
 
 @hetero_feature_selection.train()
 def train(
-        ctx: Context,
-        role: Role,
-        train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        input_models: cpn.json_model_inputs(roles=[GUEST, HOST], optional=True),
-        method: cpn.parameter(
-            type=List[params.string_choice(["manual", "iv", "statistics"])],
-            default=["manual"],
-            optional=False,
-            desc="selection method, options: {manual, binning, statistics}",
-        ),
-        select_col: cpn.parameter(
-            type=List[str],
-            default=None,
-            desc="list of column names to be selected, if None, all columns will be considered",
+    ctx: Context,
+    role: Role,
+    train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    input_models: cpn.json_model_inputs(roles=[GUEST, HOST], optional=True),
+    method: cpn.parameter(
+        type=List[params.string_choice(["manual", "iv", "statistics"])],
+        default=["manual"],
+        optional=False,
+        desc="selection method, options: {manual, binning, statistics}",
+    ),
+    select_col: cpn.parameter(
+        type=List[str],
+        default=None,
+        desc="list of column names to be selected, if None, all columns will be considered",
     ),
     iv_param: cpn.parameter(
         type=params.iv_filter_param(),
         default=params.IVFilterParam(
             metrics="iv",
             take_high=True,
             threshold=1,
```

### Comparing `pyfate-2.0.0b0/fate/components/components/hetero_sbt.py` & `pyfate-2.1.0/fate/components/components/homo_lr.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,132 +1,142 @@
 #
-#  Copyright 2023 The FATE Authors. All Rights Reserved.
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
 #
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 import logging
-
 from fate.arch import Context
+from fate.ml.glm.homo.lr.client import HomoLRClient
+from fate.ml.glm.homo.lr.server import HomoLRServer
+from fate.components.core import ARBITER, GUEST, HOST, Role, cpn, params
 from fate.components.components.utils import consts
-from fate.components.core import GUEST, HOST, Role, cpn, params
-from fate.ml.ensemble import HeteroSecureBoostGuest, HeteroSecureBoostHost, BINARY_BCE, MULTI_CE, REGRESSION_L2
+from fate.ml.utils.model_io import ModelIO
 from fate.components.components.utils.tools import add_dataset_type
-from fate.components.components.utils import consts
-
+from fate.arch.dataframe import DataFrame
 
 logger = logging.getLogger(__name__)
 
 
-@cpn.component(roles=[GUEST, HOST], provider="fate")
-def hetero_sbt(ctx, role):
+@cpn.component(roles=[GUEST, HOST, ARBITER])
+def homo_lr(ctx, role):
     ...
 
 
-@hetero_sbt.train()
+@homo_lr.train()
 def train(
     ctx: Context,
     role: Role,
     train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
     validate_data: cpn.dataframe_input(roles=[GUEST, HOST], optional=True),
-    num_trees: cpn.parameter(type=params.conint(gt=0), default=3,
-                          desc="max tree num"),
-    learning_rate: cpn.parameter(type=params.confloat(gt=0), default=0.3, desc='decay factor of each tree'),
-    max_depth: cpn.parameter(type=params.conint(gt=0), default=3, desc='max depth of a tree'),
-    max_bin: cpn.parameter(type=params.conint(gt=0), default=32, desc='max bin number of feature binning'),
-    objective: cpn.parameter(type=params.string_choice(choice=[BINARY_BCE, MULTI_CE, REGRESSION_L2]), default=BINARY_BCE, \
-                                       desc='objective function, available: {}'.format([BINARY_BCE, MULTI_CE, REGRESSION_L2])),
-    num_class: cpn.parameter(type=params.conint(gt=0), default=2, desc='class number of multi classification, active when objective is {}'.format(MULTI_CE)),
-    l2: cpn.parameter(type=params.confloat(gt=0), default=0.1, desc='L2 regularization'),
-    min_impurity_split: cpn.parameter(type=params.confloat(gt=0), default=1e-2, desc='min impurity when splitting a tree node'),
-    min_sample_split: cpn.parameter(type=params.conint(gt=0), default=2, desc='min sample to split a tree node'),
-    min_leaf_node: cpn.parameter(type=params.conint(gt=0), default=1, desc='mininum sample contained in a leaf node'),
-    min_child_weight: cpn.parameter(type=params.confloat(gt=0), default=1, desc='minumum hessian contained in a leaf node'),
-    gh_pack: cpn.parameter(type=bool, default=True, desc='whether to pack gradient and hessian together'),
-    split_info_pack: cpn.parameter(type=bool, default=True, desc='for host side, whether to pack split info together'),
-    hist_sub: cpn.parameter(type=bool, default=True, desc='whether to use histogram subtraction'),
-    he_param: cpn.parameter(type=params.he_param(), default=params.HEParam(kind='paillier', key_length=1024), desc='homomorphic encryption param, support paillier, ou and mock in current version'),
-    train_data_output: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
-    train_model_output: cpn.json_model_output(roles=[GUEST, HOST], optional=True),
-    train_model_input: cpn.json_model_input(roles=[GUEST, HOST], optional=True)
+    learning_rate_scheduler: cpn.parameter(
+        type=params.lr_scheduler_param(),
+        default=params.LRSchedulerParam(method="constant", scheduler_params={"factor": 1.0}),
+        desc="learning rate scheduler, "
+        "select method from {'step', 'linear', 'constant'}"
+        "for list of configurable arguments, "
+        "refer to torch.optim.lr_scheduler",
+    ),
+    epochs: cpn.parameter(type=params.conint(gt=0), default=20, desc="max iteration num"),
+    batch_size: cpn.parameter(
+        type=params.conint(ge=0), default=100, desc="batch size, int > 0, if None means full batch" "non"
+    ),
+    optimizer: cpn.parameter(
+        type=params.optimizer_param(),
+        default=params.OptimizerParam(
+            method="sgd", penalty="l2", alpha=1.0, optimizer_params={"lr": 1e-2, "weight_decay": 0}
+        ),
+    ),
+    init_param: cpn.parameter(
+        type=params.init_param(),
+        default=params.InitParam(method="random", fit_intercept=True),
+        desc="Model param init setting.",
+    ),
+    threshold: cpn.parameter(
+        type=params.confloat(ge=0.0, le=1.0), default=0.5, desc="predict threshold for binary data"
+    ),
+    ovr: cpn.parameter(type=bool, default=False, desc="enable ovr for multi-classifcation"),
+    label_num: cpn.parameter(type=params.conint(ge=2), default=None),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    warm_start_model: cpn.json_model_input(roles=[GUEST, HOST], optional=True),
+    output_model: cpn.json_model_output(roles=[GUEST, HOST]),
 ):
-    
-    train_data = train_data.read()
-    if validate_data is not None:
-        validate_data = validate_data.read()
-
-    if train_model_input is not None:
-        train_model_input = train_model_input.read()
-
-    if role.is_guest:
-        
-        # initialize encrypt kit
-        ctx.cipher.set_phe(ctx.device, he_param.dict())
-
-        booster = HeteroSecureBoostGuest(num_trees=num_trees, max_depth=max_depth, learning_rate=learning_rate, max_bin=max_bin,
-                                         l2=l2, min_impurity_split=min_impurity_split, min_sample_split=min_sample_split,
-                                        min_leaf_node=min_leaf_node, min_child_weight=min_child_weight, objective=objective, num_class=num_class, 
-                                        gh_pack=gh_pack, split_info_pack=split_info_pack, hist_sub=hist_sub
-                                        )
-        if train_model_input is not None:
-            booster.from_model(train_model_input)
-            logger.info('sbt input model loaded, will start warmstarting')
-        booster.fit(ctx, train_data, validate_data)
-        # get cached train data score
-        train_scores = booster.get_train_predict()
-        train_scores = add_dataset_type(train_scores, consts.TRAIN_SET)
-        train_data_output.write(train_scores)
-        # get tree param
-        tree_dict = booster.get_model()
-        train_model_output.write(tree_dict, metadata={})
-
-    elif role.is_host:
-        
-        booster = HeteroSecureBoostHost(num_trees=num_trees, max_depth=max_depth, learning_rate=learning_rate, max_bin=max_bin, hist_sub=hist_sub)
-        if train_model_input is not None:
-            booster.from_model(train_model_input)
-            logger.info('sbt input model loaded, will start warmstarting')
-        booster.fit(ctx, train_data, validate_data)
-        tree_dict = booster.get_model()
-        train_model_output.write(tree_dict, metadata={})
+    sub_ctx = ctx.sub_ctx(consts.TRAIN)
 
-    else:
-        raise RuntimeError(f"Unknown role: {role}")
+    if role.is_guest or role.is_host:  # is client
+        logger.info("homo lr component: client start training")
+        logger.info(
+            "optim param {} \n init param {} \n learning rate param {}".format(
+                optimizer.dict(), init_param.dict(), learning_rate_scheduler.dict()
+            )
+        )
+
+        client = HomoLRClient(
+            epochs=epochs,
+            batch_size=batch_size,
+            optimizer_param=optimizer.dict(),
+            init_param=init_param.dict(),
+            learning_rate_scheduler=learning_rate_scheduler.dict(),
+            threshold=threshold,
+            ovr=ovr,
+            label_num=label_num,
+        )
+
+        if warm_start_model is not None:
+            model_input = warm_start_model.read()
+            client.from_model(model_input)
+            logger.info("model input loaded")
+        train_df = train_data.read()
+        validate_df = validate_data.read() if validate_data else None
+        client.fit(sub_ctx, train_df, validate_df)
+        model_dict = client.get_model()
+
+        train_rs = client.predict(sub_ctx, train_df)
+        train_rs = add_dataset_type(train_rs, consts.TRAIN_SET)
+        if validate_df:
+            validate_rs = client.predict(sub_ctx, validate_df)
+            validate_rs = add_dataset_type(validate_rs, consts.VALIDATE_SET)
+            ret_df = DataFrame.vstack([train_rs, validate_rs])
+        else:
+            ret_df = train_rs
+
+        train_output_data.write(ret_df)
+        output_model.write(model_dict, metadata=model_dict["meta"])
+
+    elif role.is_arbiter:  # is server
+        logger.info("homo lr component: server start training")
+        server = HomoLRServer()
+        server.fit(sub_ctx)
 
 
-
-@hetero_sbt.predict()
+@homo_lr.predict()
 def predict(
     ctx,
     role: Role,
     test_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-    predict_model_input: cpn.json_model_input(roles=[GUEST, HOST]),
+    batch_size: cpn.parameter(
+        type=params.conint(ge=-1), default=100, desc="batch size, " "value less or equals to 0 means full batch"
+    ),
+    threshold: cpn.parameter(
+        type=params.confloat(ge=0.0, le=1.0), default=0.5, desc="predict threshold for binary data"
+    ),
+    input_model: cpn.json_model_input(roles=[GUEST, HOST]),
     test_output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
 ):
-    
-    model_input = predict_model_input.read()
-    test_data = test_data.read()
-    if role.is_guest:
-        booster = HeteroSecureBoostGuest()
-        booster.from_model(model_input)
-        pred_table_rs = booster.predict(ctx, test_data)
-        pred_table_rs = add_dataset_type(pred_table_rs, consts.TEST_SET)
-        test_output_data.write(pred_table_rs)
-
-    elif role.is_host:
-        booster = HeteroSecureBoostHost()
-        booster.from_model(model_input)
-        booster.predict(ctx, test_data)
-
-    else:
-        raise RuntimeError(f"Unknown role: {role}")
-
+    if role.is_guest or role.is_host:  # is client
+        client = HomoLRClient(batch_size=batch_size, threshold=threshold)
+        model_input = input_model.read()
+        client.from_model(model_input)
+        pred_rs = client.predict(ctx, test_data.read())
+        pred_rs = add_dataset_type(pred_rs, consts.TEST_SET)
+        test_output_data.write(pred_rs)
 
+    elif role.is_arbiter:  # is server
+        logger.info("arbiter skip predict")
```

### Comparing `pyfate-2.0.0b0/fate/components/components/homo_nn.py` & `pyfate-2.1.0/fate/components/components/nn/component_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,238 +8,207 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-import logging
-import os
-from fate.arch import Context
+
 from fate.components.components.nn.loader import Loader
 from fate.components.components.nn.nn_runner import NNRunner
-from fate.components.components.nn.runner.default_runner import DefaultRunner
 from fate.components.components.utils import consts
-from fate.components.core import ARBITER, GUEST, HOST, Role, cpn
 from fate.arch.dataframe import DataFrame
 from fate.components.components.utils.tools import add_dataset_type
-
-logger = logging.getLogger(__name__)
-
-
-def is_path(s):
-    return os.path.exists(s)
+from fate.arch import Context
+from fate.components.core import ARBITER, GUEST, HOST, Role, cpn
+import logging
+from fate.components.core.component_desc.artifacts.data._dataframe import DataframeReader, DataframeWriter
+from fate.components.core.component_desc.artifacts.data._directory import DataDirectoryReader
+from fate.components.core.component_desc.artifacts.model._directory import ModelDirectoryReader, ModelDirectoryWriter
 
 
-"""
-Input Functions
-"""
+logger = logging.getLogger(__name__)
 
 
 def prepare_runner_class(runner_module, runner_class, runner_conf, source):
     logger.info("runner conf is {}".format(runner_conf))
     logger.info("source is {}".format(source))
-    if runner_module != "fate_runner":
-        if source is None:
-            # load from default folder
-            runner = Loader(
-                "fate.components.components.nn.runner." +
-                runner_module,
-                runner_class,
-                **runner_conf)()
-        else:
-            runner = Loader(
-                runner_module,
-                runner_class,
-                source=source,
-                **runner_conf)()
-        assert isinstance(
-            runner, NNRunner), "loaded class must be a subclass of NNRunner class, but got {}".format(
-            type(runner))
+    if source is None:
+        # load from default folder
+        try:
+            runner = Loader("fate.components.components.nn.runner." + runner_module, runner_class, **runner_conf)()
+        except Exception as e1:
+            try:
+                runner = Loader("fate_llm.runner." + runner_module, runner_class, **runner_conf)()
+            except Exception as e2:
+                raise Exception("Both loader attempts failed. First attempt error: {}. Second attempt error: {}.".format(e1, e2))
     else:
-        logger.info("using default fate runner")
-        runner = DefaultRunner(**runner_conf)
-
+        runner = Loader(runner_module, runner_class, source=source, **runner_conf)()
+    assert isinstance(runner, NNRunner), "loaded class must be a subclass of NNRunner class, but got {}".format(
+        type(runner)
+    )
     return runner
 
 
 def prepare_context_and_role(runner, ctx, role, sub_ctx_name):
     sub_ctx = ctx.sub_ctx(sub_ctx_name)
     runner.set_context(sub_ctx)
     runner.set_role(role)
 
 
-def get_input_data(stage, cpn_input_data):
+def _parse_data(data):
+    if isinstance(data, DataframeReader):
+        data = data.read()
+    elif isinstance(data, DataDirectoryReader):
+        data = str(data.get_directory())
+    else:
+        raise ValueError(f"Unknown type of data {type(data)}")
+    return data
+
 
-    if stage == 'train':
+def get_input_data(stage, cpn_input_data):
+    if stage == "train":
         train_data, validate_data = cpn_input_data
-        train_data = train_data.read()
+        train_data = _parse_data(train_data)
         if validate_data is not None:
-            validate_data = validate_data.read()
+            validate_data = _parse_data(validate_data)
+        else:
+            validate_data = None
 
         return train_data, validate_data
 
-    elif stage == 'predict':
+    elif stage == "predict":
         test_data = cpn_input_data
-        test_data = test_data.read()
+        test_data = _parse_data(test_data)
         return test_data
     else:
         raise ValueError(f"Unknown stage {stage}")
 
 
-""""
-Output functions
-"""
-
-
-def get_model_output_conf(runner_module,
-                          runner_class,
-                          runner_conf,
-                          source,
-                          ):
-    return {
-        "runner_module": runner_module,
-        "runner_class": runner_class,
-        "runner_conf": runner_conf,
-        "source": source,
-    }
-
-
-def prepared_saved_conf(
-        model_conf,
-        runner_class,
-        runner_module,
-        runner_conf,
-        source):
-
+def prepared_saved_conf(model_conf, runner_class, runner_module, runner_conf, source):
     logger.info("loaded model_conf is: {}".format(model_conf))
     if "source" in model_conf:
         if source is None:
             source = model_conf["source"]
 
-    runner_class_, runner_module_ = model_conf['runner_class'], model_conf['runner_module']
+    runner_class_, runner_module_ = model_conf["runner_class"], model_conf["runner_module"]
     if runner_class_ == runner_class and runner_module_ == runner_module:
         if "runner_conf" in model_conf:
-            saved_conf = model_conf['runner_conf']
+            saved_conf = model_conf["runner_conf"]
             saved_conf.update(runner_conf)
             runner_conf = saved_conf
             logger.info("runner_conf is updated: {}".format(runner_conf))
     else:
         logger.warning(
             "runner_class or runner_module is not equal to the saved model, "
             "use the new runner_conf, runner_class and runner module to train the model,\
                         saved module & class: {} {}, new module & class: {} {}".format(
-                runner_module_, runner_class_, runner_module, runner_class))
+                runner_module_, runner_class_, runner_module, runner_class
+            )
+        )
 
     return runner_conf, source, runner_class, runner_module
 
 
-@cpn.component(roles=[GUEST, HOST, ARBITER])
-def homo_nn(ctx, role):
-    ...
+def get_model_output_conf(
+    runner_module,
+    runner_class,
+    runner_conf,
+    source,
+):
+    return {
+        "runner_module": runner_module,
+        "runner_class": runner_class,
+        "runner_conf": runner_conf,
+        "source": source,
+    }
 
 
-@homo_nn.train()
-def train(
+def train_procedure(
     ctx: Context,
     role: Role,
-    train_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-    validate_data: cpn.dataframe_input(roles=[GUEST, HOST], optional=True),
-    runner_module: cpn.parameter(type=str, default="default_runner", desc="name of your runner script"),
-    runner_class: cpn.parameter(type=str, default="DefaultRunner", desc="class name of your runner class"),
-    runner_conf: cpn.parameter(type=dict, default={}, desc="the parameter dict of the NN runner class"),
-    source: cpn.parameter(type=str, default=None, desc="path to your runner script folder"),
-    train_data_output: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
-    train_model_output: cpn.model_directory_output(roles=[GUEST, HOST], optional=True),
-    train_model_input: cpn.model_directory_input(roles=[GUEST, HOST], optional=True)
+    train_data: DataframeReader,
+    validate_data: DataframeReader,
+    runner_module: str,
+    runner_class: str,
+    runner_conf: dict,
+    source: str,
+    train_data_output: DataframeWriter,
+    train_model_output: ModelDirectoryWriter,
+    train_model_input: ModelDirectoryReader,
+    is_hetero=False,
 ):
+    if train_model_input is not None:
+        model_conf = train_model_input.get_metadata()
+        runner_conf, source, runner_class, runner_module = prepared_saved_conf(
+            model_conf, runner_class, runner_module, runner_conf, source
+        )
+        saved_model_path = str(train_model_input.get_directory())
+    else:
+        saved_model_path = None
 
+    runner: NNRunner = prepare_runner_class(runner_module, runner_class, runner_conf, source)
+    prepare_context_and_role(runner, ctx, role, consts.TRAIN)
 
-    if role.is_guest or role.is_host:  # is client
-        
-        if train_model_input is not None:
-            model_conf = train_model_input.get_metadata()
-            runner_conf, source, runner_class, runner_module = prepared_saved_conf(
-                model_conf, runner_class, runner_module, runner_conf, source)
-            saved_model_path = str(train_model_input.get_directory())
-        else:
-            saved_model_path = None
+    output_dir = str(train_model_output.get_directory())
+    train_data_, validate_data_ = get_input_data(consts.TRAIN, [train_data, validate_data])
 
-        runner: NNRunner = prepare_runner_class(
-            runner_module, runner_class, runner_conf, source)
-        prepare_context_and_role(runner, ctx, role, consts.TRAIN)
-
-        output_dir = str(train_model_output.get_directory())
-        train_data_, validate_data_ = get_input_data(
-            consts.TRAIN, [train_data, validate_data])
-        runner.train(train_data_, validate_data_, output_dir, saved_model_path)
-
-        logger.info('Predicting Train & Validate Data')
-        train_pred = runner.predict(train_data_, saved_model_path)
-        if train_pred is not None:
-            assert isinstance(
-                train_pred, DataFrame), "train predict result should be a DataFrame"
-            add_dataset_type(train_pred, consts.TRAIN_SET)
-
-            if validate_data_ is not None:
-                validate_pred = runner.predict(validate_data_)
-                assert isinstance(
-                    validate_pred, DataFrame), "validate predict result should be a DataFrame"
-                add_dataset_type(validate_pred, consts.VALIDATE_SET)
-                output_df = DataFrame.vstack([train_pred, validate_pred])
-            else:
-                output_df = train_pred
-            logger.info('write result dataframe')
-            train_data_output.write(output_df)
+    runner.train(train_data_, validate_data_, output_dir, saved_model_path)
+
+    logger.info("Predicting Train & Validate Data")
+    train_pred = runner.predict(train_data_, saved_model_path)
+    validate_pred = None
+    if validate_data_ is not None:
+        validate_pred = runner.predict(validate_data_)
+
+    logger.info("predicting done")
+    if train_pred is not None:
+        assert isinstance(train_pred, DataFrame), "train predict result should be a DataFrame"
+        add_dataset_type(train_pred, consts.TRAIN_SET)
+
+        if validate_pred is not None:
+            assert isinstance(validate_pred, DataFrame), "validate predict result should be a DataFrame"
+            add_dataset_type(validate_pred, consts.VALIDATE_SET)
+            output_df = DataFrame.vstack([train_pred, validate_pred])
         else:
-            logger.warning(
-                "train_pred is None, It seems that the runner is not able to predict. Failed to output data")
-
-        output_conf = get_model_output_conf(runner_module,
-                                            runner_class,
-                                            runner_conf,
-                                            source
-                                            )
-        train_model_output.write_metadata(output_conf)
-
-    elif role.is_arbiter:  # is server
-        runner: NNRunner = prepare_runner_class(
-            runner_module, runner_class, runner_conf, source)
-        prepare_context_and_role(runner, ctx, role, consts.TRAIN)
-        runner.train()
-
-
-@homo_nn.predict()
-def predict(
-    ctx, role: Role, test_data: cpn.dataframe_input(
-        roles=[
-            GUEST, HOST]), predict_model_input: cpn.model_directory_input(
-                roles=[
-                    GUEST, HOST]), predict_data_output: cpn.dataframe_output(
-                        roles=[
-                            GUEST, HOST], optional=True)):
-
-    if role.is_guest or role.is_host:  # is client
-
-        model_conf = predict_model_input.get_metadata()
-        runner_module = model_conf['runner_module']
-        runner_class = model_conf['runner_class']
-        runner_conf = model_conf['runner_conf']
-        source = model_conf['source']
-        saved_model_path = str(predict_model_input.get_directory())
-        test_data_ = get_input_data(consts.PREDICT, test_data)
-        runner: NNRunner = prepare_runner_class(
-            runner_module, runner_class, runner_conf, source)
-        prepare_context_and_role(runner, ctx, role, consts.PREDICT)
-        test_pred = runner.predict(
-            test_data_, saved_model_path=saved_model_path)
-        if test_pred is not None:
-            assert isinstance(
-                test_pred, DataFrame), "test predict result should be a DataFrame"
-            add_dataset_type(test_pred, consts.TEST_SET)
-            predict_data_output.write(test_pred)
+            output_df = train_pred
+        logger.info("write result dataframe")
+        train_data_output.write(output_df)
+    else:
+        if is_hetero and role.is_host:
+            pass
         else:
             logger.warning(
-                "test_pred is None, It seems that the runner is not able to predict. Failed to output data")
+                "train_pred is None, It seems that the runner is not able to predict. Failed to output data"
+            )
+
+    output_conf = get_model_output_conf(runner_module, runner_class, runner_conf, source)
+    train_model_output.write_metadata(output_conf)
 
-    elif role.is_arbiter:  # is server
-        logger.info("arbiter skip predict")
+
+def predict_procedure(
+    ctx: Context,
+    role: Role,
+    test_data: DataframeReader,
+    predict_model_input: ModelDirectoryReader,
+    predict_data_output: DataframeWriter,
+    is_hetero=False,
+):
+    model_conf = predict_model_input.get_metadata()
+    runner_module = model_conf["runner_module"]
+    runner_class = model_conf["runner_class"]
+    runner_conf = model_conf["runner_conf"]
+    source = model_conf["source"]
+    saved_model_path = str(predict_model_input.get_directory())
+    test_data_ = get_input_data(consts.PREDICT, test_data)
+    runner: NNRunner = prepare_runner_class(runner_module, runner_class, runner_conf, source)
+    prepare_context_and_role(runner, ctx, role, consts.PREDICT)
+    test_pred = runner.predict(test_data_, saved_model_path=saved_model_path)
+    if test_pred is not None:
+        assert isinstance(test_pred, DataFrame), "test predict result should be a DataFrame"
+        add_dataset_type(test_pred, consts.TEST_SET)
+        predict_data_output.write(test_pred)
+    else:
+        if is_hetero and role.is_host:
+            pass
+        else:
+            logger.warning("test_pred is None, It seems that the runner is not able to predict. Failed to output data")
```

### Comparing `pyfate-2.0.0b0/fate/components/components/multi_model_test.py` & `pyfate-2.1.0/fate/components/components/multi_model_test.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/conf/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/nn_runner.py` & `pyfate-2.1.0/fate/components/components/nn/nn_runner.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,61 +1,122 @@
-import numpy as np
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+import os
 import torch
+import torch as t
+import logging
+import numpy as np
+from torch import optim, nn
 import pandas as pd
-from typing import Union, Optional, Literal
+from typing import Literal, Type, Callable
+from fate.ml.nn.trainer.trainer_base import TrainingArguments
 from fate.components.core import Role
 from fate.arch import Context
 from typing import Optional, Union
 from transformers.trainer_utils import PredictionOutput
-import numpy as np
 from fate.arch.dataframe._dataframe import DataFrame
 from fate.components.components.utils import consts
-import logging
+from torch.optim.lr_scheduler import _LRScheduler
 from fate.ml.utils.predict_tools import to_dist_df, array_to_predict_df
-from fate.ml.utils.predict_tools import BINARY, MULTI, REGRESSION, OTHER, LABEL, PREDICT_SCORE
+from fate.ml.utils.predict_tools import BINARY, MULTI, REGRESSION, OTHER, CAUSAL_LM, LABEL, PREDICT_SCORE
+from fate.components.components.nn.loader import Loader
 
 
 logger = logging.getLogger(__name__)
 
 
-def _convert_to_numpy_array(
-        data: Union[pd.Series, pd.DataFrame, np.ndarray, torch.Tensor]) -> np.ndarray:
+def convert_to_numpy_array(data: Union[pd.Series, pd.DataFrame, np.ndarray, torch.Tensor]) -> np.ndarray:
     if isinstance(data, pd.Series) or isinstance(data, pd.DataFrame):
         return data.to_numpy()
     elif isinstance(data, torch.Tensor):
         return data.cpu().numpy()
     else:
         return np.array(data)
 
 
 def task_type_infer(predict_result, true_label):
-
     pred_shape = predict_result.shape
 
     if true_label.max() == 1.0 and true_label.min() == 0.0:
         return consts.BINARY
 
     if (len(pred_shape) > 1) and (pred_shape[1] > 1):
-        if np.isclose(
-            predict_result.sum(
-                axis=1), np.array(
-                [1.0])).all():
+        if np.isclose(predict_result.sum(axis=1), np.array([1.0])).all():
             return consts.MULTI
         else:
             return None
     elif (len(pred_shape) == 1) or (pred_shape[1] == 1):
         return consts.REGRESSION
 
     return None
 
 
-class NNRunner(object):
+def load_model_dict_from_path(path):
+    # Ensure that the path is a string
+    assert isinstance(path, str), "Path must be a string, but got {}".format(type(path))
 
-    def __init__(self) -> None:
+    # Append the filename to the path
+    model_path = os.path.join(path, "pytorch_model.bin")
+
+    # Check if the file exists
+    if not os.path.isfile(model_path):
+        raise FileNotFoundError(f"No 'pytorch_model.bin' file found at {model_path}, no saved model found")
+
+    # Load the state dict from the specified path
+    model_dict = t.load(model_path)
+
+    return model_dict
+
+
+def dir_warning(train_args):
+    if "output_dir" in train_args or "resume_from_checkpoint" in train_args:
+        logger.warning(
+            "The output_dir, logging_dir, and resume_from_checkpoint arguments are not supported in the "
+            "DefaultRunner when running the Pipeline. These arguments will be replaced by FATE provided paths."
+        )
+
+
+def loader_load_from_conf(conf, return_class=False):
+    if conf is None:
+        return None
+    if return_class:
+        return Loader.from_dict(conf).load_item()
+    return Loader.from_dict(conf).call_item()
 
+
+def run_dataset_func(dataset, func_name):
+    if hasattr(dataset, func_name):
+        output = getattr(dataset, func_name)()
+        if output is None:
+            logger.info(
+                f"dataset {type(dataset)}: {func_name} returns None, this will influence the output of predict"
+            )
+        return output
+    else:
+        logger.info(
+            f"dataset {type(dataset)} not implemented {func_name}, classes set to None, this will influence the output of predict"
+        )
+        return None
+
+
+class NNRunner(object):
+    def __init__(self) -> None:
         self._role = None
         self._party_id = None
         self._ctx: Context = None
 
     def set_context(self, context: Context):
         assert isinstance(context, Context)
         self._ctx = context
@@ -69,142 +130,155 @@
 
     def is_client(self) -> bool:
         return self._role.is_guest or self._role.is_host
 
     def is_server(self) -> bool:
         return self._role.is_arbiter
 
+    def is_guest(self) -> bool:
+        return self._role.is_guest
+
+    def is_host(self) -> bool:
+        return self._role.is_host
+
     def set_party_id(self, party_id: int):
         assert isinstance(self._party_id, int)
         self._party_id = party_id
 
     def get_fateboard_tracker(self):
         pass
 
     def get_nn_output_dataframe(
-            self,
-            ctx,
-            predictions: Union[np.ndarray, torch.Tensor, DataFrame, PredictionOutput],
-            labels: Union[np.ndarray, torch.Tensor, DataFrame, PredictionOutput] = None,
-            match_ids: Union[pd.DataFrame, np.ndarray] = None,
-            sample_ids: Union[pd.DataFrame, np.ndarray] = None,
-            match_id_name: str = None,
-            sample_id_name: str = None,
-            dataframe_format: Literal['default', 'fate_std'] = 'default',
-            task_type: Literal['binary', 'multi', 'regression', 'others'] = None,
-            threshold: float = 0.5,
-            classes: list = None
+        self,
+        ctx,
+        predictions: Union[np.ndarray, torch.Tensor, DataFrame, PredictionOutput],
+        labels: Union[np.ndarray, torch.Tensor, DataFrame, PredictionOutput] = None,
+        match_ids: Union[pd.DataFrame, np.ndarray] = None,
+        sample_ids: Union[pd.DataFrame, np.ndarray] = None,
+        match_id_name: str = None,
+        sample_id_name: str = None,
+        dataframe_format: Literal["default", "dist_df"] = "default",
+        task_type: Literal["binary", "multi", "regression", "others"] = None,
+        threshold: float = 0.5,
+        classes: list = None,
     ) -> DataFrame:
         """
         Constructs a FATE DataFrame from predictions and labels. This Dataframe is able to flow through FATE components.
 
         Parameters:
             ctx (Context): The Context Instance.
             predictions (Union[np.ndarray, torch.Tensor, DataFrame, PredictionOutput]): The model's predictions, which can be numpy arrays, torch tensors, pandas DataFrames, or PredictionOutputs.
             labels (Union[np.ndarray, torch.Tensor, DataFrame, PredictionOutput]): The true labels, which can be numpy arrays, torch tensors, pandas DataFrames, or PredictionOutputs.
             match_ids (Union[pd.DataFrame, np.ndarray], optional): Match IDs, if applicable. Defaults to None. If None, will auto generate match_ids.
             sample_ids (Union[pd.DataFrame, np.ndarray], optional): Sample IDs, if applicable. Defaults to None. If None, will auto generate sample_ids.
             match_id_name (str, optional): Column name for match IDs in the resulting DataFrame. If None, Defaults to 'id'.
             sample_id_name (str, optional): Column name for sample IDs in the resulting DataFrame. If None, Defaults to 'sample_id'.
-            dataframe_format (Literal['default', 'fate_std'], optional): Output format of the resulting DataFrame. If 'default', simply combines labels and predictions into a DataFrame.
-                                                                         If 'fate_std', organizes output according to the FATE framework's format. Defaults to 'default'.
-            task_type (Literal['binary', 'multi', 'regression', 'others'], optional):  This parameter is only needed when dataframe_format is 'fate_std'. Defaults to None.
-                                                                                       The type of machine learning task, which can be 'binary', 'multi', 'regression', or 'others'.
-            threshold (float, optional): This parameter is only needed when dataframe_format is 'fate_std' and task_type is 'binary'. Defaults to 0.5.
-            classes (list, optional): This parameter is only needed when dataframe_format is 'fate_std'. List of classes.
+            dataframe_format (Literal['default', 'dist_df'], optional): Output format of the resulting DataFrame. If 'default', simply combines labels and predictions into a DataFrame.
+                                                                         If 'dist_df', organizes output according to the FATE framework's format. Defaults to 'default'.
+            task_type (Literal['binary', 'multi', 'regression', 'causal_lm', 'others'], optional):  This parameter is only needed when dataframe_format is 'dist_df'. Defaults to None.
+                                                                                                    The type of machine learning task, which can be 'binary', 'multi', 'regression', 'causal', or 'others'.
+            threshold (float, optional): This parameter is only needed when dataframe_format is 'dist_df' and task_type is 'binary'. Defaults to 0.5.
+            classes (list, optional): This parameter is only needed when dataframe_format is 'dist_df'. List of classes.
         Returns:
             DataFrame: A DataFrame that contains the neural network's predictions and the true labels, possibly along with match IDs and sample IDs, formatted according to the specified format.
         """
         # check parameters
-        assert task_type in [BINARY, MULTI, REGRESSION, OTHER], f"task_type {task_type} is not supported"
-        assert dataframe_format in [
-            'default', 'fate_std'], f"dataframe_format {dataframe_format} is not supported"
+        assert task_type in [BINARY, MULTI, REGRESSION, CAUSAL_LM, OTHER], f"task_type {task_type} is not supported"
+        assert dataframe_format in ["default", "dist_df"], f"dataframe_format {dataframe_format} is not supported"
 
         if match_id_name is None:
-            match_id_name = 'id'
+            match_id_name = "id"
         if sample_id_name is None:
-            sample_id_name = 'sample_id'
+            sample_id_name = "sample_id"
 
         if isinstance(predictions, PredictionOutput):
             predictions = predictions.predictions
-        
+
         if labels is not None:
             if isinstance(labels, PredictionOutput):
                 labels = labels.label_ids
-            predictions = _convert_to_numpy_array(predictions)
-            labels = _convert_to_numpy_array(labels)
+            predictions = convert_to_numpy_array(predictions)
+            labels = convert_to_numpy_array(labels)
             assert len(predictions) == len(
-                labels), f"predictions length {len(predictions)} != labels length {len(labels)}"
+                labels
+            ), f"predictions length {len(predictions)} != labels length {len(labels)}"
 
         # check match ids
         if match_ids is not None:
-            match_ids = _convert_to_numpy_array(match_ids).flatten()
+            match_ids = convert_to_numpy_array(match_ids).flatten()
         else:
-            logger.info(
-                "match_ids is not provided, will auto generate match_ids")
-            match_ids = np.array(
-                [i for i in range(len(predictions))]).flatten()
+            logger.info("match_ids is not provided, will auto generate match_ids")
+            match_ids = np.array([i for i in range(len(predictions))]).flatten()
 
         # check sample ids
         if sample_ids is not None:
-            sample_ids = _convert_to_numpy_array(sample_ids).flatten()
+            sample_ids = convert_to_numpy_array(sample_ids).flatten()
         else:
-            logger.info(
-                "sample_ids is not provided, will auto generate sample_ids")
-            sample_ids = np.array(
-                [i for i in range(len(predictions))]).flatten()
+            logger.info("sample_ids is not provided, will auto generate sample_ids")
+            sample_ids = np.array([i for i in range(len(predictions))]).flatten()
 
         assert len(match_ids) == len(
-            predictions), f"match_ids length {len(match_ids)} != predictions length {len(predictions)}"
+            predictions
+        ), f"match_ids length {len(match_ids)} != predictions length {len(predictions)}"
         assert len(sample_ids) == len(
-            predictions), f"sample_ids length {len(sample_ids)} != predictions length {len(predictions)}"
+            predictions
+        ), f"sample_ids length {len(sample_ids)} != predictions length {len(predictions)}"
 
         # match id name and sample id name must be str
-        assert isinstance(
-            match_id_name, str), f"match_id_name must be str, but got {type(match_id_name)}"
-        assert isinstance(
-            sample_id_name, str), f"sample_id_name must be str, but got {type(sample_id_name)}"
+        assert isinstance(match_id_name, str), f"match_id_name must be str, but got {type(match_id_name)}"
+        assert isinstance(sample_id_name, str), f"sample_id_name must be str, but got {type(sample_id_name)}"
 
-        if dataframe_format == 'default' or (
-                dataframe_format == 'fate_std' and task_type == OTHER):
+        if dataframe_format == "default" or \
+                (dataframe_format == "dist_df" and task_type == OTHER):
             df = pd.DataFrame()
             if labels is not None:
                 df[LABEL] = labels.to_list()
+
             df[PREDICT_SCORE] = predictions.to_list()
+
             df[match_id_name] = match_ids.flatten()
             df[sample_id_name] = sample_ids.flatten()
             df = to_dist_df(ctx, sample_id_name, match_id_name, df)
             return df
-        elif dataframe_format == 'fate_std' and task_type in [BINARY, MULTI, REGRESSION]:
-            df = array_to_predict_df(ctx, task_type, predictions, match_ids, sample_ids, match_id_name, sample_id_name, labels, threshold, classes)
+        elif dataframe_format == "dist_df" and task_type in [BINARY, MULTI, REGRESSION]:
+            df = array_to_predict_df(
+                ctx,
+                task_type,
+                predictions,
+                match_ids,
+                sample_ids,
+                match_id_name,
+                sample_id_name,
+                labels,
+                threshold,
+                classes,
+            )
             return df
 
-    def train(self,
-              train_data: Optional[Union[str,
-                                         DataFrame]] = None,
-              validate_data: Optional[Union[str,
-                                            DataFrame]] = None,
-              output_dir: str = None,
-              saved_model_path: str = None) -> None:
+    def train(
+        self,
+        train_data: Optional[Union[str, DataFrame]] = None,
+        validate_data: Optional[Union[str, DataFrame]] = None,
+        output_dir: str = None,
+        saved_model_path: str = None,
+    ) -> None:
         """
         Train interface.
 
         Parameters:
             train_data (Union[str, DataFrame]): The training data, which can be a FATE DataFrame containing the data, or a string path representing the bound data.Train data is Optional on the server side.
             validate_data (Optional[Union[str, DataFrame]]): The validation data, which can be a FATE DataFrame containing the data,  or a string path representing the bound data . This argument is optional.
             output_dir (str, optional): The path to the directory where the trained model should be saved. If this class is running in the fate pipeline, this path will provided by FATE framework.
             saved_model_path (str, optional): The path to the saved model that should be loaded before training starts.If this class is running in the fate pipeline, this path will provided by FATE framework.
         """
         pass
 
-    def predict(self,
-                test_data: Optional[Union[str,
-                                          DataFrame]] = None,
-                output_dir: str = None,
-                saved_model_path: str = None) -> DataFrame:
+    def predict(
+        self, test_data: Optional[Union[str, DataFrame]] = None, output_dir: str = None, saved_model_path: str = None
+    ) -> DataFrame:
         """
         Predict interface.
 
         Parameters:
             test_data (Union[str, DataFrame]): The data to predict, which can be a FATE DataFrame containing the data, or a string path representing the bound data.Test data is Optional on the server side.
             output_dir (str, optional): The path to the directory where the trained model should be saved. If this class is running in the fate pipeline, this path will provided by FATE framework.
             saved_model_path (str, optional): The path to the saved model that should be loaded before training starts.If this class is running in the fate pipeline, this path will provided by FATE framework.
```

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/runner/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/entity/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/runner/default_runner.py` & `pyfate-2.1.0/fate/components/components/nn/runner/homo_default_runner.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,157 +1,112 @@
-import torch as t
-import os
-from fate.components.components.nn.nn_runner import NNRunner
-from fate.ml.nn.algo.homo.fedavg import FedAVG, FedAVGArguments, FedAVGCLient, FedAVGServer, TrainingArguments
-from typing import Optional, Dict, Union
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+from fate.components.components.nn.nn_runner import (
+    NNRunner,
+    load_model_dict_from_path,
+    dir_warning,
+    loader_load_from_conf,
+    run_dataset_func,
+)
+from fate.ml.nn.homo.fedavg import FedAVG, FedAVGArguments, FedAVGClient, FedAVGServer
+from typing import Dict
 from fate.components.components.nn.loader import Loader
 import torch.nn as nn
 import torch.optim as optim
-import torch.utils.data as data_utils
 from torch.optim.lr_scheduler import _LRScheduler
-from fate.ml.nn.trainer.trainer_base import FedArguments, TrainingArguments, FedTrainerClient, FedTrainerServer
+from fate.ml.nn.trainer.trainer_base import FedArguments, TrainingArguments, HomoTrainerClient, HomoTrainerServer
 from typing import Union, Type, Callable, Optional
 from transformers.trainer_utils import get_last_checkpoint
-from fate.ml.nn.dataset.table import TableDataset
 from typing import Literal
 import logging
 from fate.components.components.utils import consts
-from fate.ml.nn.dataset.table import TableDataset
+from fate.ml.nn.dataset.table import TableDataset, Dataset
 from fate.arch.dataframe import DataFrame
 
 
 logger = logging.getLogger(__name__)
 
 
-SUPPORTED_ALGO = ['fedavg']
+SUPPORTED_ALGO = ["fedavg"]
 
 
-def load_model_dict_from_path(path):
-    # Ensure that the path is a string
-    assert isinstance(
-        path, str), "Path must be a string, but got {}".format(
-        type(path))
-
-    # Append the filename to the path
-    model_path = os.path.join(path, 'pytorch_model.bin')
-
-    # Check if the file exists
-    if not os.path.isfile(model_path):
-        raise FileNotFoundError(
-            f"No 'pytorch_model.bin' file found at {model_path}, no saved model found")
-
-    # Load the state dict from the specified path
-    model_dict = t.load(model_path)
-
-    return model_dict
-
-
-def dir_warning(train_args):
-    if 'output_dir' in train_args or 'logging_dir' in train_args or 'resume_from_checkpoint' in train_args:
-        logger.warning(
-            "The output_dir, logging_dir, and resume_from_checkpoint arguments are not supported in the "
-            "DefaultRunner when running the Pipeline. These arguments will be replaced by FATE provided paths.")
-
-
-class SetupReturn:
-    """
-    Class to encapsulate the return objects from the setup.
-    """
-
-    def __init__(self,
-                 trainer: Union[Type[FedTrainerClient],
-                                Type[FedTrainerServer]] = None,
-                 model: Type[nn.Module] = None,
-                 optimizer: Type[optim.Optimizer] = None,
-                 loss: Callable = None,
-                 scheduler: Type[_LRScheduler] = None,
-                 train_args: TrainingArguments = None,
-                 fed_args: FedArguments = None,
-                 data_collator: Callable = None) -> None:
-
-        if trainer is not None and not (
-            issubclass(
-                type(trainer),
-                FedTrainerClient) or issubclass(
-                type(trainer),
-                FedTrainerServer)):
-            raise TypeError(
-                f"SetupReturn Error: trainer must be a subclass of either FedTrainerClient or FedTrainerServer but got {type(trainer)}")
-
-        if model is not None and not issubclass(type(model), nn.Module):
-            raise TypeError(
-                f"SetupReturn Error: model must be a subclass of torch.nn.Module but got {type(model)}")
-
-        if optimizer is not None and not issubclass(
-                type(optimizer), optim.Optimizer):
-            raise TypeError(
-                f"SetupReturn Error: optimizer must be a subclass of torch.optim.Optimizer but got {type(optimizer)}")
-
-        if loss is not None and not callable(loss):
-            raise TypeError(
-                f"SetupReturn Error: loss must be callable but got {type(loss)}")
-
-        if scheduler is not None and not issubclass(
-                type(scheduler), _LRScheduler):
-            raise TypeError(
-                f"SetupReturn Error: scheduler must be a subclass of torch.optim.lr_scheduler._LRScheduler but got {type(scheduler)}")
+def _check_instances(
+    trainer: Union[Type[HomoTrainerClient], Type[HomoTrainerServer]] = None,
+    fed_args: FedArguments = None,
+    model: nn.Module = None,
+    optimizer: optim.Optimizer = None,
+    loss: Callable = None,
+    scheduler: _LRScheduler = None,
+    train_args: TrainingArguments = None,
+    data_collator: Callable = None,
+) -> None:
+    if trainer is not None and not (
+        issubclass(type(trainer), HomoTrainerClient) or issubclass(type(trainer), HomoTrainerServer)
+    ):
+        raise TypeError(
+            f"SetupReturn Error: trainer must be a subclass of either FedTrainerClient or FedTrainerServer but got {type(trainer)}"
+        )
+
+    if fed_args is not None and not isinstance(fed_args, FedArguments):
+        raise TypeError(f"SetupReturn Error: fed_args must be an instance of FedArguments but got {type(fed_args)}")
+
+    if model is not None and not issubclass(type(model), nn.Module):
+        raise TypeError(f"SetupReturn Error: model must be a subclass of torch.nn.Module but got {type(model)}")
+
+    if optimizer is not None and not issubclass(type(optimizer), optim.Optimizer):
+        raise TypeError(
+            f"SetupReturn Error: optimizer must be a subclass of torch.optim.Optimizer but got {type(optimizer)}"
+        )
+
+    if loss is not None and not callable(loss):
+        raise TypeError(f"SetupReturn Error: loss must be callable but got {type(loss)}")
+
+    if scheduler is not None and not issubclass(type(scheduler), _LRScheduler):
+        raise TypeError(
+            f"SetupReturn Error: scheduler must be a subclass of torch.optim.lr_scheduler._LRScheduler but got {type(scheduler)}"
+        )
+
+    if train_args is not None and not isinstance(train_args, TrainingArguments):
+        raise TypeError(
+            f"SetupReturn Error: train_args must be an instance of TrainingArguments but got {type(train_args)}"
+        )
 
-        if train_args is not None and not isinstance(
-                train_args, TrainingArguments):
-            raise TypeError(
-                f"SetupReturn Error: train_args must be an instance of TrainingArguments but got {type(train_args)}")
-
-        if fed_args is not None and not isinstance(fed_args, FedArguments):
-            raise TypeError(
-                f"SetupReturn Error: fed_args must be an instance of FedArguments but got {type(fed_args)}")
-
-        if data_collator is not None and not callable(data_collator):
-            raise TypeError(
-                f"SetupReturn Error: data_collator must be callable but got {type(data_collator)}")
-
-        self.trainer = trainer
-        self.model = model
-        self.optimizer = optimizer
-        self.loss = loss
-        self.scheduler = scheduler
-        self.train_args = train_args
-        self.fed_args = fed_args
-        self.data_collator = data_collator
-
-    def __getitem__(self, item):
-        return getattr(self, item)
-
-    def __repr__(self):
-        repr_string = "SetupReturn(\n"
-        for key, value in self.__dict__.items():
-            repr_string += f"  {key}={type(value)},\n"
-        repr_string = repr_string.rstrip(',\n')
-        repr_string += "\n)"
-        return repr_string
+    if data_collator is not None and not callable(data_collator):
+        raise TypeError(f"SetupReturn Error: data_collator must be callable but got {type(data_collator)}")
 
 
 class DefaultRunner(NNRunner):
-
-    def __init__(self,
-                 algo: str = 'fedavg',
-                 model_conf: Optional[Dict] = None,
-                 dataset_conf: Optional[Dict] = None,
-                 optimizer_conf: Optional[Dict] = None,
-                 training_args_conf: Optional[Dict] = None,
-                 fed_args_conf: Optional[Dict] = None,
-                 loss_conf: Optional[Dict] = None,
-                 data_collator_conf: Optional[Dict] = None,
-                 tokenizer_conf: Optional[Dict] = None,
-                 task_type: Literal['binary',
-                                    'multi',
-                                    'regression',
-                                    'others'] = 'binary',
-                 threshold: float = 0.5,
-                 local_mode: bool = False) -> None:
-
+    def __init__(
+        self,
+        algo: str = "fedavg",
+        model_conf: Optional[Dict] = None,
+        dataset_conf: Optional[Dict] = None,
+        optimizer_conf: Optional[Dict] = None,
+        training_args_conf: Optional[Dict] = None,
+        fed_args_conf: Optional[Dict] = None,
+        loss_conf: Optional[Dict] = None,
+        data_collator_conf: Optional[Dict] = None,
+        tokenizer_conf: Optional[Dict] = None,
+        task_type: Literal["binary", "multi", "regression", "causal_lm", "others"] = "binary",
+        threshold: float = 0.5,
+        local_mode: bool = False,
+    ) -> None:
         super().__init__()
         self.algo = algo
         self.model_conf = model_conf
         self.dataset_conf = dataset_conf
         self.optimizer_conf = optimizer_conf
         self.training_args_conf = training_args_conf
         self.fed_args_conf = fed_args_conf
@@ -160,223 +115,201 @@
         self.local_mode = local_mode
         self.tokenizer_conf = tokenizer_conf
         self.task_type = task_type
         self.threshold = threshold
 
         # check param
         if self.algo not in SUPPORTED_ALGO:
-            raise ValueError('algo should be one of [fedavg]')
-        if self.task_type not in ['binary', 'multi', 'regression', 'others']:
-            raise ValueError(
-                'task_type should be one of [binary, multi, regression, others]')
-        assert self.threshold >= 0 and self.threshold <= 1, 'threshold should be in [0, 1]'
-        assert isinstance(self.local_mode, bool), 'local should be bool'
+            raise ValueError("algo should be one of [fedavg]")
+        if self.task_type not in ["binary", "multi", "regression", "causal_lm", "others"]:
+            raise ValueError("task_type should be one of [binary, multi, regression, others]")
+        assert self.threshold >= 0 and self.threshold <= 1, "threshold should be in [0, 1]"
+        assert isinstance(self.local_mode, bool), "local should be bool"
 
         # setup var
         self.trainer = None
+        self.training_args = None
 
-    def _loader_load_from_conf(self, conf, return_class=False):
-        if conf is None:
-            return None
-        if return_class:
-            return Loader.from_dict(conf).load_item()
-        return Loader.from_dict(conf).call_item()
-
-    def _prepare_data(self, data, data_name) -> SetupReturn:
-
+    def _prepare_data(self, data, data_name):
         if data is None:
             return None
         if isinstance(data, DataFrame) and self.dataset_conf is None:
             logger.info(
-                'Input data {} is FATE DataFrame and dataset conf is None, will automatically handle the input data'.format(data_name))
+                "Input data {} is FATE DataFrame and dataset conf is None, will automatically handle the input data".format(
+                    data_name
+                )
+            )
             if self.task_type == consts.MULTI:
-                dataset = TableDataset(
-                    flatten_label=True,
-                    label_dtype='long',
-                    to_tensor=True)
+                dataset = TableDataset(flatten_label=True, label_dtype="long", to_tensor=True)
             else:
                 dataset = TableDataset(to_tensor=True)
             dataset.load(data)
         else:
-            dataset = self._loader_load_from_conf(self.dataset_conf)
-            if hasattr(dataset, 'load'):
-                dataset.load(data)
+            dataset = loader_load_from_conf(self.dataset_conf)
+            if hasattr(dataset, "load"):
+                logger.info("load path is {}".format(data))
+                load_output = dataset.load(data)
+                if load_output is not None:
+                    dataset = load_output
+                    return dataset
             else:
                 raise ValueError(
                     f"The dataset {dataset} lacks a load() method, which is required for data parsing in the DefaultRunner. \
                                 Please implement this method in your dataset class. You can refer to the base class 'Dataset' in 'fate.ml.nn.dataset.base' \
-                                for the necessary interfaces to implement.")
-        if dataset is not None and not issubclass(
-                type(dataset), data_utils.Dataset):
+                                for the necessary interfaces to implement."
+                )
+        if dataset is not None and not issubclass(type(dataset), Dataset):
             raise TypeError(
-                f"SetupReturn Error: {data_name}_set must be a subclass of torch.utils.data.Dataset but got {type(dataset)}")
+                f"SetupReturn Error: {data_name}_set must be a subclass of fate built-in Dataset but got {type(dataset)}, \n"
+                f"You can get the class via: from fate.ml.nn.dataset.table import Dataset"
+            )
 
         return dataset
 
-    def client_setup(
-            self,
-            train_set=None,
-            validate_set=None,
-            output_dir=None,
-            saved_model=None,
-            stage='train'):
-
-        if stage == 'predict':
+    def client_setup(self, train_set=None, validate_set=None, output_dir=None, saved_model=None, stage="train"):
+        if stage == "predict":
             self.local_mode = True
 
-        if self.algo == 'fedavg':
-            client_class: FedAVGCLient = FedAVG.client
+        if self.algo == "fedavg":
+            client_class: FedAVGClient = FedAVG.client
         else:
             raise ValueError(f"algo {self.algo} not supported")
 
         ctx = self.get_context()
-        print(self.model_conf)
-        model = self._loader_load_from_conf(self.model_conf)
+        model = loader_load_from_conf(self.model_conf)
         if model is None:
-            raise ValueError(
-                f"model is None, cannot load model from conf {self.model_conf}")
+            raise ValueError(f"model is None, cannot load model from conf {self.model_conf}")
 
         if output_dir is None:
-            output_dir = './'
+            output_dir = "./"
 
         resume_path = None
         if saved_model is not None:
             model_dict = load_model_dict_from_path(saved_model)
             model.load_state_dict(model_dict)
             logger.info(f"loading model dict from {saved_model} to model done")
             if get_last_checkpoint(saved_model) is not None:
                 resume_path = saved_model
-                logger.info(
-                    f"checkpoint detected, resume_path set to {resume_path}")
+                logger.info(f"checkpoint detected, resume_path set to {resume_path}")
         # load optimizer
-        optimizer_loader = Loader.from_dict(self.optimizer_conf)
-        optimizer_ = optimizer_loader.load_item()
-        optimizer_params = optimizer_loader.kwargs
-        optimizer = optimizer_(model.parameters(), **optimizer_params)
+        if self.optimizer_conf:
+            optimizer_loader = Loader.from_dict(self.optimizer_conf)
+            optimizer_ = optimizer_loader.load_item()
+            optimizer_params = optimizer_loader.kwargs
+            optimizer = optimizer_(model.parameters(), **optimizer_params)
+        else:
+            optimizer = None
         # load loss
-        loss = self._loader_load_from_conf(self.loss_conf)
+        loss = loader_load_from_conf(self.loss_conf) if self.loss_conf else None
         # load collator func
-        data_collator = self._loader_load_from_conf(self.data_collator_conf)
+        data_collator = loader_load_from_conf(self.data_collator_conf)
         # load tokenizer if import conf provided
-        tokenizer = self._loader_load_from_conf(self.tokenizer_conf)
+        tokenizer = loader_load_from_conf(self.tokenizer_conf)
         # args
         dir_warning(self.training_args_conf)
         training_args = TrainingArguments(**self.training_args_conf)
+        self.training_args = training_args
         # reset to default, saving to arbitrary path is not allowed in
         # DefaultRunner
         training_args.output_dir = output_dir
         training_args.resume_from_checkpoint = resume_path  # resume path
         fed_args = FedAVGArguments(**self.fed_args_conf)
-
+        if fed_args.aggregate_strategy == 'steps':
+            raise ValueError('aggregate_strategy "steps" is not supported in FATE-pipeline which will be used in production.')
         # prepare trainer
         trainer = client_class(
             ctx=ctx,
             model=model,
             loss_fn=loss,
             optimizer=optimizer,
             training_args=training_args,
             fed_args=fed_args,
             data_collator=data_collator,
             tokenizer=tokenizer,
             train_set=train_set,
             val_set=validate_set,
-            local_mode=self.local_mode)
+            local_mode=self.local_mode,
+        )
 
-        return SetupReturn(
+        _check_instances(
             trainer=trainer,
             model=model,
             optimizer=optimizer,
             loss=loss,
             train_args=training_args,
             fed_args=fed_args,
-            data_collator=data_collator)
-
-    def server_setup(self, stage='train'):
+            data_collator=data_collator,
+        )
+        return trainer
 
-        if stage == 'predict':
+    def server_setup(self, stage="train"):
+        if stage == "predict":
             self.local_mode = True
-        if self.algo == 'fedavg':
+        if self.algo == "fedavg":
             server_class: FedAVGServer = FedAVG.server
         else:
             raise ValueError(f"algo {self.algo} not supported")
         ctx = self.get_context()
         trainer = server_class(ctx=ctx, local_mode=self.local_mode)
-        return SetupReturn(trainer=trainer)
-
-    def train(self,
-              train_data: Optional[Union[str,
-                                         DataFrame]] = None,
-              validate_data: Optional[Union[str,
-                                            DataFrame]] = None,
-              output_dir: str = None,
-              saved_model_path: str = None):
+        _check_instances(trainer)
+        return trainer
 
+    def train(
+        self,
+        train_data: Optional[Union[str, DataFrame]] = None,
+        validate_data: Optional[Union[str, DataFrame]] = None,
+        output_dir: str = None,
+        saved_model_path: str = None,
+    ):
         if self.is_client():
-            train_set = self._prepare_data(train_data, 'train_data')
-            validate_set = self._prepare_data(validate_data, 'val_data')
-            setup = self.client_setup(
-                train_set=train_set,
-                validate_set=validate_set,
-                output_dir=output_dir,
-                saved_model=saved_model_path)
-            trainer = setup['trainer']
+            train_set = self._prepare_data(train_data, "train_data")
+            validate_set = self._prepare_data(validate_data, "val_data")
+            trainer = self.client_setup(
+                train_set=train_set, validate_set=validate_set, output_dir=output_dir, saved_model=saved_model_path
+            )
             self.trainer = trainer
             trainer.train()
             if output_dir is not None:
-                trainer.save_model(output_dir)
+                if self.training_args.deepspeed and self.training_args.local_rank != 0:
+                    pass
+                else:
+                    trainer.save_model(output_dir)
         elif self.is_server():
-            setup = self.server_setup()
-            trainer = setup['trainer']
+            trainer = self.server_setup()
             trainer.train()
 
-    def _run_dataset_func(self, dataset, func_name):
-
-        if hasattr(dataset, func_name):
-            output = getattr(dataset, func_name)()
-            if output is None:
-                logger.info(
-                    f'dataset {type(dataset)}: {func_name} returns None, this will influence the output of predict')
-            return output
-        else:
-            logger.info(
-                f'dataset {type(dataset)} not implemented {func_name}, classes set to None, this will influence the output of predict')
-            return None
-
-    def predict(self,
-                test_data: Union[str,
-                                 DataFrame],
-                saved_model_path: str = None) -> Union[DataFrame,
-                                                       None]:
-
+    def predict(self, test_data: Union[str, DataFrame], saved_model_path: str = None) -> Union[DataFrame, None]:
+        
         if self.is_client():
-            test_set = self._prepare_data(test_data, 'test_data')
+            test_set = self._prepare_data(test_data, "test_data")
             if self.trainer is not None:
                 trainer = self.trainer
-                logger.info('trainer found, skip setting up')
+                logger.info("trainer found, skip setting up")
             else:
-                setup = self.client_setup(
-                    saved_model=saved_model_path, stage='predict')
-                trainer = setup['trainer']
-
-            classes = self._run_dataset_func(test_set, 'get_classes')
-            match_ids = self._run_dataset_func(test_set, 'get_match_ids')
-            sample_ids = self._run_dataset_func(test_set, 'get_sample_ids')
-            match_id_name = self._run_dataset_func(
-                test_set, 'get_match_id_name')
-            sample_id_name = self._run_dataset_func(
-                test_set, 'get_sample_id_name')
+                trainer = self.client_setup(saved_model=saved_model_path, stage="predict")
+
+            classes = run_dataset_func(test_set, "get_classes")
+            match_ids = run_dataset_func(test_set, "get_match_ids")
+            sample_ids = run_dataset_func(test_set, "get_sample_ids")
+            match_id_name = run_dataset_func(test_set, "get_match_id_name")
+            sample_id_name = run_dataset_func(test_set, "get_sample_id_name")
             pred_rs = trainer.predict(test_set)
+
+            if self.training_args and self.training_args.deepspeed and self.training_args.local_rank != 0:
+                return
+
             rs_df = self.get_nn_output_dataframe(
                 self.get_context(),
                 pred_rs.predictions,
-                pred_rs.label_ids if hasattr(pred_rs, 'label_ids') else None,
+                pred_rs.label_ids if hasattr(pred_rs, "label_ids") else None,
                 match_ids,
                 sample_ids,
                 match_id_name=match_id_name,
                 sample_id_name=sample_id_name,
-                dataframe_format='fate_std',
+                dataframe_format="dist_df",
                 task_type=self.task_type,
-                classes=classes)
+                classes=classes,
+            )
             return rs_df
         else:
             # server not predict
             return
```

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/torch/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,8 +8,7 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-#
```

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/torch/nn.py` & `pyfate-2.1.0/fate/components/components/nn/torch/nn.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,2430 +1,2024 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from torch import nn
 from fate.components.components.nn.torch.base import TorchModule
 
 
 class Bilinear(nn.modules.linear.Bilinear, TorchModule):
-
-    def __init__(
-            self,
-            in1_features,
-            in2_features,
-            out_features,
-            bias=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in1_features'] = in1_features
-        self.param_dict['in2_features'] = in2_features
-        self.param_dict['out_features'] = out_features
+    def __init__(self, in1_features, in2_features, out_features, bias=True, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in1_features"] = in1_features
+        self.param_dict["in2_features"] = in2_features
+        self.param_dict["out_features"] = out_features
         self.param_dict.update(kwargs)
         nn.modules.linear.Bilinear.__init__(self, **self.param_dict)
 
 
 class Identity(nn.modules.linear.Identity, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.linear.Identity.__init__(self, **self.param_dict)
 
 
 class LazyLinear(nn.modules.linear.LazyLinear, TorchModule):
-
-    def __init__(
-            self,
-            out_features,
-            bias=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_features'] = out_features
+    def __init__(self, out_features, bias=True, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_features"] = out_features
         self.param_dict.update(kwargs)
         nn.modules.linear.LazyLinear.__init__(self, **self.param_dict)
 
 
 class Linear(nn.modules.linear.Linear, TorchModule):
-
-    def __init__(
-            self,
-            in_features,
-            out_features,
-            bias=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_features'] = in_features
-        self.param_dict['out_features'] = out_features
+    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_features"] = in_features
+        self.param_dict["out_features"] = out_features
         self.param_dict.update(kwargs)
         nn.modules.linear.Linear.__init__(self, **self.param_dict)
 
 
-class NonDynamicallyQuantizableLinear(
-        nn.modules.linear.NonDynamicallyQuantizableLinear,
-        TorchModule):
-
-    def __init__(
-            self,
-            in_features,
-            out_features,
-            bias=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_features'] = in_features
-        self.param_dict['out_features'] = out_features
+class NonDynamicallyQuantizableLinear(nn.modules.linear.NonDynamicallyQuantizableLinear, TorchModule):
+    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_features"] = in_features
+        self.param_dict["out_features"] = out_features
         self.param_dict.update(kwargs)
-        nn.modules.linear.NonDynamicallyQuantizableLinear.__init__(
-            self, **self.param_dict)
+        nn.modules.linear.NonDynamicallyQuantizableLinear.__init__(self, **self.param_dict)
 
 
 class GRU(nn.modules.rnn.GRU, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.rnn.GRU.__init__(self, **self.param_dict)
 
 
 class GRUCell(nn.modules.rnn.GRUCell, TorchModule):
-
-    def __init__(
-            self,
-            input_size,
-            hidden_size,
-            bias=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['input_size'] = input_size
-        self.param_dict['hidden_size'] = hidden_size
+    def __init__(self, input_size, hidden_size, bias=True, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["input_size"] = input_size
+        self.param_dict["hidden_size"] = hidden_size
         self.param_dict.update(kwargs)
         nn.modules.rnn.GRUCell.__init__(self, **self.param_dict)
 
 
 class LSTM(nn.modules.rnn.LSTM, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.rnn.LSTM.__init__(self, **self.param_dict)
 
 
 class LSTMCell(nn.modules.rnn.LSTMCell, TorchModule):
-
-    def __init__(
-            self,
-            input_size,
-            hidden_size,
-            bias=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['input_size'] = input_size
-        self.param_dict['hidden_size'] = hidden_size
+    def __init__(self, input_size, hidden_size, bias=True, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["input_size"] = input_size
+        self.param_dict["hidden_size"] = hidden_size
         self.param_dict.update(kwargs)
         nn.modules.rnn.LSTMCell.__init__(self, **self.param_dict)
 
 
 class RNN(nn.modules.rnn.RNN, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.rnn.RNN.__init__(self, **self.param_dict)
 
 
 class RNNBase(nn.modules.rnn.RNNBase, TorchModule):
-
     def __init__(
-            self,
-            mode,
-            input_size,
-            hidden_size,
-            num_layers=1,
-            bias=True,
-            batch_first=False,
-            dropout=0.0,
-            bidirectional=False,
-            proj_size=0,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['num_layers'] = num_layers
-        self.param_dict['bias'] = bias
-        self.param_dict['batch_first'] = batch_first
-        self.param_dict['dropout'] = dropout
-        self.param_dict['bidirectional'] = bidirectional
-        self.param_dict['proj_size'] = proj_size
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['mode'] = mode
-        self.param_dict['input_size'] = input_size
-        self.param_dict['hidden_size'] = hidden_size
+        self,
+        mode,
+        input_size,
+        hidden_size,
+        num_layers=1,
+        bias=True,
+        batch_first=False,
+        dropout=0.0,
+        bidirectional=False,
+        proj_size=0,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["num_layers"] = num_layers
+        self.param_dict["bias"] = bias
+        self.param_dict["batch_first"] = batch_first
+        self.param_dict["dropout"] = dropout
+        self.param_dict["bidirectional"] = bidirectional
+        self.param_dict["proj_size"] = proj_size
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["mode"] = mode
+        self.param_dict["input_size"] = input_size
+        self.param_dict["hidden_size"] = hidden_size
         self.param_dict.update(kwargs)
         nn.modules.rnn.RNNBase.__init__(self, **self.param_dict)
 
 
 class RNNCell(nn.modules.rnn.RNNCell, TorchModule):
-
-    def __init__(
-            self,
-            input_size,
-            hidden_size,
-            bias=True,
-            nonlinearity='tanh',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['bias'] = bias
-        self.param_dict['nonlinearity'] = nonlinearity
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['input_size'] = input_size
-        self.param_dict['hidden_size'] = hidden_size
+    def __init__(self, input_size, hidden_size, bias=True, nonlinearity="tanh", device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["bias"] = bias
+        self.param_dict["nonlinearity"] = nonlinearity
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["input_size"] = input_size
+        self.param_dict["hidden_size"] = hidden_size
         self.param_dict.update(kwargs)
         nn.modules.rnn.RNNCell.__init__(self, **self.param_dict)
 
 
 class RNNCellBase(nn.modules.rnn.RNNCellBase, TorchModule):
-
-    def __init__(
-            self,
-            input_size,
-            hidden_size,
-            bias,
-            num_chunks,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['input_size'] = input_size
-        self.param_dict['hidden_size'] = hidden_size
-        self.param_dict['bias'] = bias
-        self.param_dict['num_chunks'] = num_chunks
+    def __init__(self, input_size, hidden_size, bias, num_chunks, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["input_size"] = input_size
+        self.param_dict["hidden_size"] = hidden_size
+        self.param_dict["bias"] = bias
+        self.param_dict["num_chunks"] = num_chunks
         self.param_dict.update(kwargs)
         nn.modules.rnn.RNNCellBase.__init__(self, **self.param_dict)
 
 
 class Embedding(nn.modules.sparse.Embedding, TorchModule):
-
     def __init__(
-            self,
-            num_embeddings,
-            embedding_dim,
-            padding_idx=None,
-            max_norm=None,
-            norm_type=2.0,
-            scale_grad_by_freq=False,
-            sparse=False,
-            _weight=None,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['padding_idx'] = padding_idx
-        self.param_dict['max_norm'] = max_norm
-        self.param_dict['norm_type'] = norm_type
-        self.param_dict['scale_grad_by_freq'] = scale_grad_by_freq
-        self.param_dict['sparse'] = sparse
-        self.param_dict['_weight'] = _weight
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_embeddings'] = num_embeddings
-        self.param_dict['embedding_dim'] = embedding_dim
+        self,
+        num_embeddings,
+        embedding_dim,
+        padding_idx=None,
+        max_norm=None,
+        norm_type=2.0,
+        scale_grad_by_freq=False,
+        sparse=False,
+        _weight=None,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["padding_idx"] = padding_idx
+        self.param_dict["max_norm"] = max_norm
+        self.param_dict["norm_type"] = norm_type
+        self.param_dict["scale_grad_by_freq"] = scale_grad_by_freq
+        self.param_dict["sparse"] = sparse
+        self.param_dict["_weight"] = _weight
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_embeddings"] = num_embeddings
+        self.param_dict["embedding_dim"] = embedding_dim
         self.param_dict.update(kwargs)
         nn.modules.sparse.Embedding.__init__(self, **self.param_dict)
 
 
 class EmbeddingBag(nn.modules.sparse.EmbeddingBag, TorchModule):
-
     def __init__(
-            self,
-            num_embeddings,
-            embedding_dim,
-            max_norm=None,
-            norm_type=2.0,
-            scale_grad_by_freq=False,
-            mode='mean',
-            sparse=False,
-            _weight=None,
-            include_last_offset=False,
-            padding_idx=None,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['max_norm'] = max_norm
-        self.param_dict['norm_type'] = norm_type
-        self.param_dict['scale_grad_by_freq'] = scale_grad_by_freq
-        self.param_dict['mode'] = mode
-        self.param_dict['sparse'] = sparse
-        self.param_dict['_weight'] = _weight
-        self.param_dict['include_last_offset'] = include_last_offset
-        self.param_dict['padding_idx'] = padding_idx
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_embeddings'] = num_embeddings
-        self.param_dict['embedding_dim'] = embedding_dim
+        self,
+        num_embeddings,
+        embedding_dim,
+        max_norm=None,
+        norm_type=2.0,
+        scale_grad_by_freq=False,
+        mode="mean",
+        sparse=False,
+        _weight=None,
+        include_last_offset=False,
+        padding_idx=None,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["max_norm"] = max_norm
+        self.param_dict["norm_type"] = norm_type
+        self.param_dict["scale_grad_by_freq"] = scale_grad_by_freq
+        self.param_dict["mode"] = mode
+        self.param_dict["sparse"] = sparse
+        self.param_dict["_weight"] = _weight
+        self.param_dict["include_last_offset"] = include_last_offset
+        self.param_dict["padding_idx"] = padding_idx
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_embeddings"] = num_embeddings
+        self.param_dict["embedding_dim"] = embedding_dim
         self.param_dict.update(kwargs)
         nn.modules.sparse.EmbeddingBag.__init__(self, **self.param_dict)
 
 
 class AlphaDropout(nn.modules.dropout.AlphaDropout, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.dropout.AlphaDropout.__init__(self, **self.param_dict)
 
 
 class Dropout(nn.modules.dropout.Dropout, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.dropout.Dropout.__init__(self, **self.param_dict)
 
 
 class Dropout1d(nn.modules.dropout.Dropout1d, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.dropout.Dropout1d.__init__(self, **self.param_dict)
 
 
 class Dropout2d(nn.modules.dropout.Dropout2d, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.dropout.Dropout2d.__init__(self, **self.param_dict)
 
 
 class Dropout3d(nn.modules.dropout.Dropout3d, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.dropout.Dropout3d.__init__(self, **self.param_dict)
 
 
 class FeatureAlphaDropout(nn.modules.dropout.FeatureAlphaDropout, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
-        nn.modules.dropout.FeatureAlphaDropout.__init__(
-            self, **self.param_dict)
+        nn.modules.dropout.FeatureAlphaDropout.__init__(self, **self.param_dict)
 
 
 class _DropoutNd(nn.modules.dropout._DropoutNd, TorchModule):
-
     def __init__(self, p=0.5, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['inplace'] = inplace
+        self.param_dict["p"] = p
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.dropout._DropoutNd.__init__(self, **self.param_dict)
 
 
 class CELU(nn.modules.activation.CELU, TorchModule):
-
     def __init__(self, alpha=1.0, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['alpha'] = alpha
-        self.param_dict['inplace'] = inplace
+        self.param_dict["alpha"] = alpha
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.CELU.__init__(self, **self.param_dict)
 
 
 class ELU(nn.modules.activation.ELU, TorchModule):
-
     def __init__(self, alpha=1.0, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['alpha'] = alpha
-        self.param_dict['inplace'] = inplace
+        self.param_dict["alpha"] = alpha
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.ELU.__init__(self, **self.param_dict)
 
 
 class GELU(nn.modules.activation.GELU, TorchModule):
-
-    def __init__(self, approximate='none', **kwargs):
+    def __init__(self, approximate="none", **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['approximate'] = approximate
+        self.param_dict["approximate"] = approximate
         self.param_dict.update(kwargs)
         nn.modules.activation.GELU.__init__(self, **self.param_dict)
 
 
 class GLU(nn.modules.activation.GLU, TorchModule):
-
     def __init__(self, dim=-1, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['dim'] = dim
+        self.param_dict["dim"] = dim
         self.param_dict.update(kwargs)
         nn.modules.activation.GLU.__init__(self, **self.param_dict)
 
 
 class Hardshrink(nn.modules.activation.Hardshrink, TorchModule):
-
     def __init__(self, lambd=0.5, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['lambd'] = lambd
+        self.param_dict["lambd"] = lambd
         self.param_dict.update(kwargs)
         nn.modules.activation.Hardshrink.__init__(self, **self.param_dict)
 
 
 class Hardsigmoid(nn.modules.activation.Hardsigmoid, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.Hardsigmoid.__init__(self, **self.param_dict)
 
 
 class Hardswish(nn.modules.activation.Hardswish, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.Hardswish.__init__(self, **self.param_dict)
 
 
 class Hardtanh(nn.modules.activation.Hardtanh, TorchModule):
-
-    def __init__(
-            self,
-            min_val=-1.0,
-            max_val=1.0,
-            inplace=False,
-            min_value=None,
-            max_value=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['min_val'] = min_val
-        self.param_dict['max_val'] = max_val
-        self.param_dict['inplace'] = inplace
-        self.param_dict['min_value'] = min_value
-        self.param_dict['max_value'] = max_value
+    def __init__(self, min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["min_val"] = min_val
+        self.param_dict["max_val"] = max_val
+        self.param_dict["inplace"] = inplace
+        self.param_dict["min_value"] = min_value
+        self.param_dict["max_value"] = max_value
         self.param_dict.update(kwargs)
         nn.modules.activation.Hardtanh.__init__(self, **self.param_dict)
 
 
 class LeakyReLU(nn.modules.activation.LeakyReLU, TorchModule):
-
     def __init__(self, negative_slope=0.01, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['negative_slope'] = negative_slope
-        self.param_dict['inplace'] = inplace
+        self.param_dict["negative_slope"] = negative_slope
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.LeakyReLU.__init__(self, **self.param_dict)
 
 
 class LogSigmoid(nn.modules.activation.LogSigmoid, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.activation.LogSigmoid.__init__(self, **self.param_dict)
 
 
 class LogSoftmax(nn.modules.activation.LogSoftmax, TorchModule):
-
     def __init__(self, dim=None, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['dim'] = dim
+        self.param_dict["dim"] = dim
         self.param_dict.update(kwargs)
         nn.modules.activation.LogSoftmax.__init__(self, **self.param_dict)
 
 
 class Mish(nn.modules.activation.Mish, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.Mish.__init__(self, **self.param_dict)
 
 
 class MultiheadAttention(nn.modules.activation.MultiheadAttention, TorchModule):
-
     def __init__(
-            self,
-            embed_dim,
-            num_heads,
-            dropout=0.0,
-            bias=True,
-            add_bias_kv=False,
-            add_zero_attn=False,
-            kdim=None,
-            vdim=None,
-            batch_first=False,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['dropout'] = dropout
-        self.param_dict['bias'] = bias
-        self.param_dict['add_bias_kv'] = add_bias_kv
-        self.param_dict['add_zero_attn'] = add_zero_attn
-        self.param_dict['kdim'] = kdim
-        self.param_dict['vdim'] = vdim
-        self.param_dict['batch_first'] = batch_first
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['embed_dim'] = embed_dim
-        self.param_dict['num_heads'] = num_heads
+        self,
+        embed_dim,
+        num_heads,
+        dropout=0.0,
+        bias=True,
+        add_bias_kv=False,
+        add_zero_attn=False,
+        kdim=None,
+        vdim=None,
+        batch_first=False,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["dropout"] = dropout
+        self.param_dict["bias"] = bias
+        self.param_dict["add_bias_kv"] = add_bias_kv
+        self.param_dict["add_zero_attn"] = add_zero_attn
+        self.param_dict["kdim"] = kdim
+        self.param_dict["vdim"] = vdim
+        self.param_dict["batch_first"] = batch_first
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["embed_dim"] = embed_dim
+        self.param_dict["num_heads"] = num_heads
         self.param_dict.update(kwargs)
-        nn.modules.activation.MultiheadAttention.__init__(
-            self, **self.param_dict)
+        nn.modules.activation.MultiheadAttention.__init__(self, **self.param_dict)
 
 
 class PReLU(nn.modules.activation.PReLU, TorchModule):
-
-    def __init__(
-            self,
-            num_parameters=1,
-            init=0.25,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['num_parameters'] = num_parameters
-        self.param_dict['init'] = init
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
+    def __init__(self, num_parameters=1, init=0.25, device=None, dtype=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["num_parameters"] = num_parameters
+        self.param_dict["init"] = init
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
         self.param_dict.update(kwargs)
         nn.modules.activation.PReLU.__init__(self, **self.param_dict)
 
 
 class RReLU(nn.modules.activation.RReLU, TorchModule):
-
-    def __init__(
-            self,
-            lower=0.125,
-            upper=0.3333333333333333,
-            inplace=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['lower'] = lower
-        self.param_dict['upper'] = upper
-        self.param_dict['inplace'] = inplace
+    def __init__(self, lower=0.125, upper=0.3333333333333333, inplace=False, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["lower"] = lower
+        self.param_dict["upper"] = upper
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.RReLU.__init__(self, **self.param_dict)
 
 
 class ReLU(nn.modules.activation.ReLU, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.ReLU.__init__(self, **self.param_dict)
 
 
 class ReLU6(nn.modules.activation.ReLU6, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.ReLU6.__init__(self, **self.param_dict)
 
 
 class SELU(nn.modules.activation.SELU, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.SELU.__init__(self, **self.param_dict)
 
 
 class SiLU(nn.modules.activation.SiLU, TorchModule):
-
     def __init__(self, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
+        self.param_dict["inplace"] = inplace
         self.param_dict.update(kwargs)
         nn.modules.activation.SiLU.__init__(self, **self.param_dict)
 
 
 class Sigmoid(nn.modules.activation.Sigmoid, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.activation.Sigmoid.__init__(self, **self.param_dict)
 
 
 class Softmax(nn.modules.activation.Softmax, TorchModule):
-
     def __init__(self, dim=None, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['dim'] = dim
+        self.param_dict["dim"] = dim
         self.param_dict.update(kwargs)
         nn.modules.activation.Softmax.__init__(self, **self.param_dict)
 
 
 class Softmax2d(nn.modules.activation.Softmax2d, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.activation.Softmax2d.__init__(self, **self.param_dict)
 
 
 class Softmin(nn.modules.activation.Softmin, TorchModule):
-
     def __init__(self, dim=None, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['dim'] = dim
+        self.param_dict["dim"] = dim
         self.param_dict.update(kwargs)
         nn.modules.activation.Softmin.__init__(self, **self.param_dict)
 
 
 class Softplus(nn.modules.activation.Softplus, TorchModule):
-
     def __init__(self, beta=1, threshold=20, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['beta'] = beta
-        self.param_dict['threshold'] = threshold
+        self.param_dict["beta"] = beta
+        self.param_dict["threshold"] = threshold
         self.param_dict.update(kwargs)
         nn.modules.activation.Softplus.__init__(self, **self.param_dict)
 
 
 class Softshrink(nn.modules.activation.Softshrink, TorchModule):
-
     def __init__(self, lambd=0.5, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['lambd'] = lambd
+        self.param_dict["lambd"] = lambd
         self.param_dict.update(kwargs)
         nn.modules.activation.Softshrink.__init__(self, **self.param_dict)
 
 
 class Softsign(nn.modules.activation.Softsign, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.activation.Softsign.__init__(self, **self.param_dict)
 
 
 class Tanh(nn.modules.activation.Tanh, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.activation.Tanh.__init__(self, **self.param_dict)
 
 
 class Tanhshrink(nn.modules.activation.Tanhshrink, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.activation.Tanhshrink.__init__(self, **self.param_dict)
 
 
 class Threshold(nn.modules.activation.Threshold, TorchModule):
-
     def __init__(self, threshold, value, inplace=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['inplace'] = inplace
-        self.param_dict['threshold'] = threshold
-        self.param_dict['value'] = value
+        self.param_dict["inplace"] = inplace
+        self.param_dict["threshold"] = threshold
+        self.param_dict["value"] = value
         self.param_dict.update(kwargs)
         nn.modules.activation.Threshold.__init__(self, **self.param_dict)
 
 
 class Conv1d(nn.modules.conv.Conv1d, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            dilation=1,
-            groups=1,
-            bias=True,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1,
+        bias=True,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.Conv1d.__init__(self, **self.param_dict)
 
 
 class Conv2d(nn.modules.conv.Conv2d, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            dilation=1,
-            groups=1,
-            bias=True,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1,
+        bias=True,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.Conv2d.__init__(self, **self.param_dict)
 
 
 class Conv3d(nn.modules.conv.Conv3d, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            dilation=1,
-            groups=1,
-            bias=True,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1,
+        bias=True,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.Conv3d.__init__(self, **self.param_dict)
 
 
 class ConvTranspose1d(nn.modules.conv.ConvTranspose1d, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            output_padding=0,
-            groups=1,
-            bias=True,
-            dilation=1,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['dilation'] = dilation
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        output_padding=0,
+        groups=1,
+        bias=True,
+        dilation=1,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["dilation"] = dilation
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.ConvTranspose1d.__init__(self, **self.param_dict)
 
 
 class ConvTranspose2d(nn.modules.conv.ConvTranspose2d, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            output_padding=0,
-            groups=1,
-            bias=True,
-            dilation=1,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['dilation'] = dilation
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        output_padding=0,
+        groups=1,
+        bias=True,
+        dilation=1,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["dilation"] = dilation
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.ConvTranspose2d.__init__(self, **self.param_dict)
 
 
 class ConvTranspose3d(nn.modules.conv.ConvTranspose3d, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            output_padding=0,
-            groups=1,
-            bias=True,
-            dilation=1,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['dilation'] = dilation
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        output_padding=0,
+        groups=1,
+        bias=True,
+        dilation=1,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["dilation"] = dilation
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.ConvTranspose3d.__init__(self, **self.param_dict)
 
 
 class LazyConv1d(nn.modules.conv.LazyConv1d, TorchModule):
-
     def __init__(
-            self,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            dilation=1,
-            groups=1,
-            bias=True,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1,
+        bias=True,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.LazyConv1d.__init__(self, **self.param_dict)
 
 
 class LazyConv2d(nn.modules.conv.LazyConv2d, TorchModule):
-
     def __init__(
-            self,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            dilation=1,
-            groups=1,
-            bias=True,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1,
+        bias=True,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.LazyConv2d.__init__(self, **self.param_dict)
 
 
 class LazyConv3d(nn.modules.conv.LazyConv3d, TorchModule):
-
     def __init__(
-            self,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            dilation=1,
-            groups=1,
-            bias=True,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1,
+        bias=True,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.LazyConv3d.__init__(self, **self.param_dict)
 
 
 class LazyConvTranspose1d(nn.modules.conv.LazyConvTranspose1d, TorchModule):
-
     def __init__(
-            self,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            output_padding=0,
-            groups=1,
-            bias=True,
-            dilation=1,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['dilation'] = dilation
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        output_padding=0,
+        groups=1,
+        bias=True,
+        dilation=1,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["dilation"] = dilation
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.LazyConvTranspose1d.__init__(self, **self.param_dict)
 
 
 class LazyConvTranspose2d(nn.modules.conv.LazyConvTranspose2d, TorchModule):
-
     def __init__(
-            self,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            output_padding=0,
-            groups=1,
-            bias=True,
-            dilation=1,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['dilation'] = dilation
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        output_padding=0,
+        groups=1,
+        bias=True,
+        dilation=1,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["dilation"] = dilation
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.LazyConvTranspose2d.__init__(self, **self.param_dict)
 
 
 class LazyConvTranspose3d(nn.modules.conv.LazyConvTranspose3d, TorchModule):
-
     def __init__(
-            self,
-            out_channels,
-            kernel_size,
-            stride=1,
-            padding=0,
-            output_padding=0,
-            groups=1,
-            bias=True,
-            dilation=1,
-            padding_mode='zeros',
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['dilation'] = dilation
-        self.param_dict['padding_mode'] = padding_mode
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        out_channels,
+        kernel_size,
+        stride=1,
+        padding=0,
+        output_padding=0,
+        groups=1,
+        bias=True,
+        dilation=1,
+        padding_mode="zeros",
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["dilation"] = dilation
+        self.param_dict["padding_mode"] = padding_mode
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.conv.LazyConvTranspose3d.__init__(self, **self.param_dict)
 
 
 class _ConvNd(nn.modules.conv._ConvNd, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride,
-            padding,
-            dilation,
-            transposed,
-            output_padding,
-            groups,
-            bias,
-            padding_mode,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['transposed'] = transposed
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride,
+        padding,
+        dilation,
+        transposed,
+        output_padding,
+        groups,
+        bias,
+        padding_mode,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["transposed"] = transposed
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
         self.param_dict.update(kwargs)
         nn.modules.conv._ConvNd.__init__(self, **self.param_dict)
 
 
 class _ConvTransposeMixin(nn.modules.conv._ConvTransposeMixin, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.conv._ConvTransposeMixin.__init__(self, **self.param_dict)
 
 
 class _ConvTransposeNd(nn.modules.conv._ConvTransposeNd, TorchModule):
-
     def __init__(
-            self,
-            in_channels,
-            out_channels,
-            kernel_size,
-            stride,
-            padding,
-            dilation,
-            transposed,
-            output_padding,
-            groups,
-            bias,
-            padding_mode,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['in_channels'] = in_channels
-        self.param_dict['out_channels'] = out_channels
-        self.param_dict['kernel_size'] = kernel_size
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['transposed'] = transposed
-        self.param_dict['output_padding'] = output_padding
-        self.param_dict['groups'] = groups
-        self.param_dict['bias'] = bias
-        self.param_dict['padding_mode'] = padding_mode
+        self,
+        in_channels,
+        out_channels,
+        kernel_size,
+        stride,
+        padding,
+        dilation,
+        transposed,
+        output_padding,
+        groups,
+        bias,
+        padding_mode,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["in_channels"] = in_channels
+        self.param_dict["out_channels"] = out_channels
+        self.param_dict["kernel_size"] = kernel_size
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["transposed"] = transposed
+        self.param_dict["output_padding"] = output_padding
+        self.param_dict["groups"] = groups
+        self.param_dict["bias"] = bias
+        self.param_dict["padding_mode"] = padding_mode
         self.param_dict.update(kwargs)
         nn.modules.conv._ConvTransposeNd.__init__(self, **self.param_dict)
 
 
 class _LazyConvXdMixin(nn.modules.conv._LazyConvXdMixin, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.conv._LazyConvXdMixin.__init__(self, **self.param_dict)
 
 
 class Transformer(nn.modules.transformer.Transformer, TorchModule):
-
     def __init__(
-            self,
-            d_model=512,
-            nhead=8,
-            num_encoder_layers=6,
-            num_decoder_layers=6,
-            dim_feedforward=2048,
-            dropout=0.1,
-            custom_encoder=None,
-            custom_decoder=None,
-            layer_norm_eps=1e-05,
-            batch_first=False,
-            norm_first=False,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['d_model'] = d_model
-        self.param_dict['nhead'] = nhead
-        self.param_dict['num_encoder_layers'] = num_encoder_layers
-        self.param_dict['num_decoder_layers'] = num_decoder_layers
-        self.param_dict['dim_feedforward'] = dim_feedforward
-        self.param_dict['dropout'] = dropout
-        self.param_dict['custom_encoder'] = custom_encoder
-        self.param_dict['custom_decoder'] = custom_decoder
-        self.param_dict['layer_norm_eps'] = layer_norm_eps
-        self.param_dict['batch_first'] = batch_first
-        self.param_dict['norm_first'] = norm_first
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
+        self,
+        d_model=512,
+        nhead=8,
+        num_encoder_layers=6,
+        num_decoder_layers=6,
+        dim_feedforward=2048,
+        dropout=0.1,
+        custom_encoder=None,
+        custom_decoder=None,
+        layer_norm_eps=1e-05,
+        batch_first=False,
+        norm_first=False,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["d_model"] = d_model
+        self.param_dict["nhead"] = nhead
+        self.param_dict["num_encoder_layers"] = num_encoder_layers
+        self.param_dict["num_decoder_layers"] = num_decoder_layers
+        self.param_dict["dim_feedforward"] = dim_feedforward
+        self.param_dict["dropout"] = dropout
+        self.param_dict["custom_encoder"] = custom_encoder
+        self.param_dict["custom_decoder"] = custom_decoder
+        self.param_dict["layer_norm_eps"] = layer_norm_eps
+        self.param_dict["batch_first"] = batch_first
+        self.param_dict["norm_first"] = norm_first
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
         self.param_dict.update(kwargs)
         nn.modules.transformer.Transformer.__init__(self, **self.param_dict)
 
 
 class TransformerDecoder(nn.modules.transformer.TransformerDecoder, TorchModule):
-
     def __init__(self, decoder_layer, num_layers, norm=None, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['norm'] = norm
-        self.param_dict['decoder_layer'] = decoder_layer
-        self.param_dict['num_layers'] = num_layers
-        self.param_dict.update(kwargs)
-        nn.modules.transformer.TransformerDecoder.__init__(
-            self, **self.param_dict)
-
-
-class TransformerDecoderLayer(
-        nn.modules.transformer.TransformerDecoderLayer,
-        TorchModule):
-
-    def __init__(
-            self,
-            d_model,
-            nhead,
-            dim_feedforward=2048,
-            dropout=0.1,
-            layer_norm_eps=1e-05,
-            batch_first=False,
-            norm_first=False,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['dim_feedforward'] = dim_feedforward
-        self.param_dict['dropout'] = dropout
-        self.param_dict['layer_norm_eps'] = layer_norm_eps
-        self.param_dict['batch_first'] = batch_first
-        self.param_dict['norm_first'] = norm_first
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['d_model'] = d_model
-        self.param_dict['nhead'] = nhead
+        self.param_dict["norm"] = norm
+        self.param_dict["decoder_layer"] = decoder_layer
+        self.param_dict["num_layers"] = num_layers
+        self.param_dict.update(kwargs)
+        nn.modules.transformer.TransformerDecoder.__init__(self, **self.param_dict)
+
+
+class TransformerDecoderLayer(nn.modules.transformer.TransformerDecoderLayer, TorchModule):
+    def __init__(
+        self,
+        d_model,
+        nhead,
+        dim_feedforward=2048,
+        dropout=0.1,
+        layer_norm_eps=1e-05,
+        batch_first=False,
+        norm_first=False,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["dim_feedforward"] = dim_feedforward
+        self.param_dict["dropout"] = dropout
+        self.param_dict["layer_norm_eps"] = layer_norm_eps
+        self.param_dict["batch_first"] = batch_first
+        self.param_dict["norm_first"] = norm_first
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["d_model"] = d_model
+        self.param_dict["nhead"] = nhead
         self.param_dict.update(kwargs)
-        nn.modules.transformer.TransformerDecoderLayer.__init__(
-            self, **self.param_dict)
+        nn.modules.transformer.TransformerDecoderLayer.__init__(self, **self.param_dict)
 
 
 class TransformerEncoder(nn.modules.transformer.TransformerEncoder, TorchModule):
-
-    def __init__(
-            self,
-            encoder_layer,
-            num_layers,
-            norm=None,
-            enable_nested_tensor=True,
-            mask_check=True,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['norm'] = norm
-        self.param_dict['enable_nested_tensor'] = enable_nested_tensor
-        self.param_dict['mask_check'] = mask_check
-        self.param_dict['encoder_layer'] = encoder_layer
-        self.param_dict['num_layers'] = num_layers
-        self.param_dict.update(kwargs)
-        nn.modules.transformer.TransformerEncoder.__init__(
-            self, **self.param_dict)
-
-
-class TransformerEncoderLayer(
-        nn.modules.transformer.TransformerEncoderLayer,
-        TorchModule):
-
-    def __init__(
-            self,
-            d_model,
-            nhead,
-            dim_feedforward=2048,
-            dropout=0.1,
-            layer_norm_eps=1e-05,
-            batch_first=False,
-            norm_first=False,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['dim_feedforward'] = dim_feedforward
-        self.param_dict['dropout'] = dropout
-        self.param_dict['layer_norm_eps'] = layer_norm_eps
-        self.param_dict['batch_first'] = batch_first
-        self.param_dict['norm_first'] = norm_first
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['d_model'] = d_model
-        self.param_dict['nhead'] = nhead
+    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True, mask_check=True, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["norm"] = norm
+        self.param_dict["enable_nested_tensor"] = enable_nested_tensor
+        self.param_dict["mask_check"] = mask_check
+        self.param_dict["encoder_layer"] = encoder_layer
+        self.param_dict["num_layers"] = num_layers
+        self.param_dict.update(kwargs)
+        nn.modules.transformer.TransformerEncoder.__init__(self, **self.param_dict)
+
+
+class TransformerEncoderLayer(nn.modules.transformer.TransformerEncoderLayer, TorchModule):
+    def __init__(
+        self,
+        d_model,
+        nhead,
+        dim_feedforward=2048,
+        dropout=0.1,
+        layer_norm_eps=1e-05,
+        batch_first=False,
+        norm_first=False,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["dim_feedforward"] = dim_feedforward
+        self.param_dict["dropout"] = dropout
+        self.param_dict["layer_norm_eps"] = layer_norm_eps
+        self.param_dict["batch_first"] = batch_first
+        self.param_dict["norm_first"] = norm_first
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["d_model"] = d_model
+        self.param_dict["nhead"] = nhead
         self.param_dict.update(kwargs)
-        nn.modules.transformer.TransformerEncoderLayer.__init__(
-            self, **self.param_dict)
+        nn.modules.transformer.TransformerEncoderLayer.__init__(self, **self.param_dict)
 
 
 class AdaptiveAvgPool1d(nn.modules.pooling.AdaptiveAvgPool1d, TorchModule):
-
     def __init__(self, output_size, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['output_size'] = output_size
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AdaptiveAvgPool1d.__init__(self, **self.param_dict)
 
 
 class AdaptiveAvgPool2d(nn.modules.pooling.AdaptiveAvgPool2d, TorchModule):
-
     def __init__(self, output_size, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['output_size'] = output_size
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AdaptiveAvgPool2d.__init__(self, **self.param_dict)
 
 
 class AdaptiveAvgPool3d(nn.modules.pooling.AdaptiveAvgPool3d, TorchModule):
-
     def __init__(self, output_size, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['output_size'] = output_size
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AdaptiveAvgPool3d.__init__(self, **self.param_dict)
 
 
 class AdaptiveMaxPool1d(nn.modules.pooling.AdaptiveMaxPool1d, TorchModule):
-
     def __init__(self, output_size, return_indices=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['output_size'] = output_size
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AdaptiveMaxPool1d.__init__(self, **self.param_dict)
 
 
 class AdaptiveMaxPool2d(nn.modules.pooling.AdaptiveMaxPool2d, TorchModule):
-
     def __init__(self, output_size, return_indices=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['output_size'] = output_size
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AdaptiveMaxPool2d.__init__(self, **self.param_dict)
 
 
 class AdaptiveMaxPool3d(nn.modules.pooling.AdaptiveMaxPool3d, TorchModule):
-
     def __init__(self, output_size, return_indices=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['output_size'] = output_size
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AdaptiveMaxPool3d.__init__(self, **self.param_dict)
 
 
 class AvgPool1d(nn.modules.pooling.AvgPool1d, TorchModule):
-
-    def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            ceil_mode=False,
-            count_include_pad=True,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['count_include_pad'] = count_include_pad
-        self.param_dict['kernel_size'] = kernel_size
+    def __init__(self, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["count_include_pad"] = count_include_pad
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AvgPool1d.__init__(self, **self.param_dict)
 
 
 class AvgPool2d(nn.modules.pooling.AvgPool2d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            ceil_mode=False,
-            count_include_pad=True,
-            divisor_override=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['count_include_pad'] = count_include_pad
-        self.param_dict['divisor_override'] = divisor_override
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        kernel_size,
+        stride=None,
+        padding=0,
+        ceil_mode=False,
+        count_include_pad=True,
+        divisor_override=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["count_include_pad"] = count_include_pad
+        self.param_dict["divisor_override"] = divisor_override
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AvgPool2d.__init__(self, **self.param_dict)
 
 
 class AvgPool3d(nn.modules.pooling.AvgPool3d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            ceil_mode=False,
-            count_include_pad=True,
-            divisor_override=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['count_include_pad'] = count_include_pad
-        self.param_dict['divisor_override'] = divisor_override
-        self.param_dict['kernel_size'] = kernel_size
+        self,
+        kernel_size,
+        stride=None,
+        padding=0,
+        ceil_mode=False,
+        count_include_pad=True,
+        divisor_override=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["count_include_pad"] = count_include_pad
+        self.param_dict["divisor_override"] = divisor_override
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.AvgPool3d.__init__(self, **self.param_dict)
 
 
 class FractionalMaxPool2d(nn.modules.pooling.FractionalMaxPool2d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            output_size=None,
-            output_ratio=None,
-            return_indices=False,
-            _random_samples=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['output_size'] = output_size
-        self.param_dict['output_ratio'] = output_ratio
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['_random_samples'] = _random_samples
-        self.param_dict['kernel_size'] = kernel_size
+        self, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["output_size"] = output_size
+        self.param_dict["output_ratio"] = output_ratio
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["_random_samples"] = _random_samples
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
-        nn.modules.pooling.FractionalMaxPool2d.__init__(
-            self, **self.param_dict)
+        nn.modules.pooling.FractionalMaxPool2d.__init__(self, **self.param_dict)
 
 
 class FractionalMaxPool3d(nn.modules.pooling.FractionalMaxPool3d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            output_size=None,
-            output_ratio=None,
-            return_indices=False,
-            _random_samples=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['output_size'] = output_size
-        self.param_dict['output_ratio'] = output_ratio
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['_random_samples'] = _random_samples
-        self.param_dict['kernel_size'] = kernel_size
+        self, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["output_size"] = output_size
+        self.param_dict["output_ratio"] = output_ratio
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["_random_samples"] = _random_samples
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
-        nn.modules.pooling.FractionalMaxPool3d.__init__(
-            self, **self.param_dict)
+        nn.modules.pooling.FractionalMaxPool3d.__init__(self, **self.param_dict)
 
 
 class LPPool1d(nn.modules.pooling.LPPool1d, TorchModule):
-
-    def __init__(
-            self,
-            norm_type,
-            kernel_size,
-            stride=None,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['norm_type'] = norm_type
-        self.param_dict['kernel_size'] = kernel_size
+    def __init__(self, norm_type, kernel_size, stride=None, ceil_mode=False, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["norm_type"] = norm_type
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.LPPool1d.__init__(self, **self.param_dict)
 
 
 class LPPool2d(nn.modules.pooling.LPPool2d, TorchModule):
-
-    def __init__(
-            self,
-            norm_type,
-            kernel_size,
-            stride=None,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['norm_type'] = norm_type
-        self.param_dict['kernel_size'] = kernel_size
+    def __init__(self, norm_type, kernel_size, stride=None, ceil_mode=False, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["norm_type"] = norm_type
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.LPPool2d.__init__(self, **self.param_dict)
 
 
 class MaxPool1d(nn.modules.pooling.MaxPool1d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            dilation=1,
-            return_indices=False,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['kernel_size'] = kernel_size
+        self, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.MaxPool1d.__init__(self, **self.param_dict)
 
 
 class MaxPool2d(nn.modules.pooling.MaxPool2d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            dilation=1,
-            return_indices=False,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['kernel_size'] = kernel_size
+        self, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.MaxPool2d.__init__(self, **self.param_dict)
 
 
 class MaxPool3d(nn.modules.pooling.MaxPool3d, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            dilation=1,
-            return_indices=False,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['kernel_size'] = kernel_size
+        self, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.MaxPool3d.__init__(self, **self.param_dict)
 
 
 class MaxUnpool1d(nn.modules.pooling.MaxUnpool1d, TorchModule):
-
     def __init__(self, kernel_size, stride=None, padding=0, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['kernel_size'] = kernel_size
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.MaxUnpool1d.__init__(self, **self.param_dict)
 
 
 class MaxUnpool2d(nn.modules.pooling.MaxUnpool2d, TorchModule):
-
     def __init__(self, kernel_size, stride=None, padding=0, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['kernel_size'] = kernel_size
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.MaxUnpool2d.__init__(self, **self.param_dict)
 
 
 class MaxUnpool3d(nn.modules.pooling.MaxUnpool3d, TorchModule):
-
     def __init__(self, kernel_size, stride=None, padding=0, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['kernel_size'] = kernel_size
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling.MaxUnpool3d.__init__(self, **self.param_dict)
 
 
 class _AdaptiveAvgPoolNd(nn.modules.pooling._AdaptiveAvgPoolNd, TorchModule):
-
     def __init__(self, output_size, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['output_size'] = output_size
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling._AdaptiveAvgPoolNd.__init__(self, **self.param_dict)
 
 
 class _AdaptiveMaxPoolNd(nn.modules.pooling._AdaptiveMaxPoolNd, TorchModule):
-
     def __init__(self, output_size, return_indices=False, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['output_size'] = output_size
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["output_size"] = output_size
         self.param_dict.update(kwargs)
         nn.modules.pooling._AdaptiveMaxPoolNd.__init__(self, **self.param_dict)
 
 
 class _AvgPoolNd(nn.modules.pooling._AvgPoolNd, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.pooling._AvgPoolNd.__init__(self, **self.param_dict)
 
 
 class _LPPoolNd(nn.modules.pooling._LPPoolNd, TorchModule):
-
-    def __init__(
-            self,
-            norm_type,
-            kernel_size,
-            stride=None,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['norm_type'] = norm_type
-        self.param_dict['kernel_size'] = kernel_size
+    def __init__(self, norm_type, kernel_size, stride=None, ceil_mode=False, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["norm_type"] = norm_type
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling._LPPoolNd.__init__(self, **self.param_dict)
 
 
 class _MaxPoolNd(nn.modules.pooling._MaxPoolNd, TorchModule):
-
     def __init__(
-            self,
-            kernel_size,
-            stride=None,
-            padding=0,
-            dilation=1,
-            return_indices=False,
-            ceil_mode=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['stride'] = stride
-        self.param_dict['padding'] = padding
-        self.param_dict['dilation'] = dilation
-        self.param_dict['return_indices'] = return_indices
-        self.param_dict['ceil_mode'] = ceil_mode
-        self.param_dict['kernel_size'] = kernel_size
+        self, kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["stride"] = stride
+        self.param_dict["padding"] = padding
+        self.param_dict["dilation"] = dilation
+        self.param_dict["return_indices"] = return_indices
+        self.param_dict["ceil_mode"] = ceil_mode
+        self.param_dict["kernel_size"] = kernel_size
         self.param_dict.update(kwargs)
         nn.modules.pooling._MaxPoolNd.__init__(self, **self.param_dict)
 
 
 class _MaxUnpoolNd(nn.modules.pooling._MaxUnpoolNd, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.pooling._MaxUnpoolNd.__init__(self, **self.param_dict)
 
 
 class BatchNorm1d(nn.modules.batchnorm.BatchNorm1d, TorchModule):
-
     def __init__(
-            self,
-            num_features,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_features'] = num_features
+        self,
+        num_features,
+        eps=1e-05,
+        momentum=0.1,
+        affine=True,
+        track_running_stats=True,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_features"] = num_features
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.BatchNorm1d.__init__(self, **self.param_dict)
 
 
 class BatchNorm2d(nn.modules.batchnorm.BatchNorm2d, TorchModule):
-
     def __init__(
-            self,
-            num_features,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_features'] = num_features
+        self,
+        num_features,
+        eps=1e-05,
+        momentum=0.1,
+        affine=True,
+        track_running_stats=True,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_features"] = num_features
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.BatchNorm2d.__init__(self, **self.param_dict)
 
 
 class BatchNorm3d(nn.modules.batchnorm.BatchNorm3d, TorchModule):
-
     def __init__(
-            self,
-            num_features,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_features'] = num_features
+        self,
+        num_features,
+        eps=1e-05,
+        momentum=0.1,
+        affine=True,
+        track_running_stats=True,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_features"] = num_features
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.BatchNorm3d.__init__(self, **self.param_dict)
 
 
 class LazyBatchNorm1d(nn.modules.batchnorm.LazyBatchNorm1d, TorchModule):
-
     def __init__(
-            self,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
+        self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.LazyBatchNorm1d.__init__(self, **self.param_dict)
 
 
 class LazyBatchNorm2d(nn.modules.batchnorm.LazyBatchNorm2d, TorchModule):
-
     def __init__(
-            self,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
+        self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.LazyBatchNorm2d.__init__(self, **self.param_dict)
 
 
 class LazyBatchNorm3d(nn.modules.batchnorm.LazyBatchNorm3d, TorchModule):
-
     def __init__(
-            self,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
+        self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.LazyBatchNorm3d.__init__(self, **self.param_dict)
 
 
 class SyncBatchNorm(nn.modules.batchnorm.SyncBatchNorm, TorchModule):
-
     def __init__(
-            self,
-            num_features,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            process_group=None,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['process_group'] = process_group
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_features'] = num_features
+        self,
+        num_features,
+        eps=1e-05,
+        momentum=0.1,
+        affine=True,
+        track_running_stats=True,
+        process_group=None,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["process_group"] = process_group
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_features"] = num_features
         self.param_dict.update(kwargs)
         nn.modules.batchnorm.SyncBatchNorm.__init__(self, **self.param_dict)
 
 
 class _BatchNorm(nn.modules.batchnorm._BatchNorm, TorchModule):
-
     def __init__(
-            self,
-            num_features,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_features'] = num_features
+        self,
+        num_features,
+        eps=1e-05,
+        momentum=0.1,
+        affine=True,
+        track_running_stats=True,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_features"] = num_features
         self.param_dict.update(kwargs)
         nn.modules.batchnorm._BatchNorm.__init__(self, **self.param_dict)
 
 
 class _LazyNormBase(nn.modules.batchnorm._LazyNormBase, TorchModule):
-
     def __init__(
-            self,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
+        self, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None, **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
         self.param_dict.update(kwargs)
         nn.modules.batchnorm._LazyNormBase.__init__(self, **self.param_dict)
 
 
 class _NormBase(nn.modules.batchnorm._NormBase, TorchModule):
-
     def __init__(
-            self,
-            num_features,
-            eps=1e-05,
-            momentum=0.1,
-            affine=True,
-            track_running_stats=True,
-            device=None,
-            dtype=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['eps'] = eps
-        self.param_dict['momentum'] = momentum
-        self.param_dict['affine'] = affine
-        self.param_dict['track_running_stats'] = track_running_stats
-        self.param_dict['device'] = device
-        self.param_dict['dtype'] = dtype
-        self.param_dict['num_features'] = num_features
+        self,
+        num_features,
+        eps=1e-05,
+        momentum=0.1,
+        affine=True,
+        track_running_stats=True,
+        device=None,
+        dtype=None,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["eps"] = eps
+        self.param_dict["momentum"] = momentum
+        self.param_dict["affine"] = affine
+        self.param_dict["track_running_stats"] = track_running_stats
+        self.param_dict["device"] = device
+        self.param_dict["dtype"] = dtype
+        self.param_dict["num_features"] = num_features
         self.param_dict.update(kwargs)
         nn.modules.batchnorm._NormBase.__init__(self, **self.param_dict)
 
 
 class ConstantPad1d(nn.modules.padding.ConstantPad1d, TorchModule):
-
     def __init__(self, padding, value, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
-        self.param_dict['value'] = value
+        self.param_dict["padding"] = padding
+        self.param_dict["value"] = value
         self.param_dict.update(kwargs)
         nn.modules.padding.ConstantPad1d.__init__(self, **self.param_dict)
 
 
 class ConstantPad2d(nn.modules.padding.ConstantPad2d, TorchModule):
-
     def __init__(self, padding, value, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
-        self.param_dict['value'] = value
+        self.param_dict["padding"] = padding
+        self.param_dict["value"] = value
         self.param_dict.update(kwargs)
         nn.modules.padding.ConstantPad2d.__init__(self, **self.param_dict)
 
 
 class ConstantPad3d(nn.modules.padding.ConstantPad3d, TorchModule):
-
     def __init__(self, padding, value, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
-        self.param_dict['value'] = value
+        self.param_dict["padding"] = padding
+        self.param_dict["value"] = value
         self.param_dict.update(kwargs)
         nn.modules.padding.ConstantPad3d.__init__(self, **self.param_dict)
 
 
 class ReflectionPad1d(nn.modules.padding.ReflectionPad1d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ReflectionPad1d.__init__(self, **self.param_dict)
 
 
 class ReflectionPad2d(nn.modules.padding.ReflectionPad2d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ReflectionPad2d.__init__(self, **self.param_dict)
 
 
 class ReflectionPad3d(nn.modules.padding.ReflectionPad3d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ReflectionPad3d.__init__(self, **self.param_dict)
 
 
 class ReplicationPad1d(nn.modules.padding.ReplicationPad1d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ReplicationPad1d.__init__(self, **self.param_dict)
 
 
 class ReplicationPad2d(nn.modules.padding.ReplicationPad2d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ReplicationPad2d.__init__(self, **self.param_dict)
 
 
 class ReplicationPad3d(nn.modules.padding.ReplicationPad3d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ReplicationPad3d.__init__(self, **self.param_dict)
 
 
 class ZeroPad2d(nn.modules.padding.ZeroPad2d, TorchModule):
-
     def __init__(self, padding, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['padding'] = padding
+        self.param_dict["padding"] = padding
         self.param_dict.update(kwargs)
         nn.modules.padding.ZeroPad2d.__init__(self, **self.param_dict)
 
 
 class _ConstantPadNd(nn.modules.padding._ConstantPadNd, TorchModule):
-
     def __init__(self, value, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['value'] = value
+        self.param_dict["value"] = value
         self.param_dict.update(kwargs)
         nn.modules.padding._ConstantPadNd.__init__(self, **self.param_dict)
 
 
 class _ReflectionPadNd(nn.modules.padding._ReflectionPadNd, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.padding._ReflectionPadNd.__init__(self, **self.param_dict)
 
 
 class _ReplicationPadNd(nn.modules.padding._ReplicationPadNd, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.padding._ReplicationPadNd.__init__(self, **self.param_dict)
 
 
 class BCELoss(nn.modules.loss.BCELoss, TorchModule):
-
-    def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, weight=None, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.BCELoss.__init__(self, **self.param_dict)
 
 
 class BCEWithLogitsLoss(nn.modules.loss.BCEWithLogitsLoss, TorchModule):
-
-    def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            pos_weight=None,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
-        self.param_dict['pos_weight'] = pos_weight
+    def __init__(self, weight=None, size_average=None, reduce=None, reduction="mean", pos_weight=None, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
+        self.param_dict["pos_weight"] = pos_weight
         self.param_dict.update(kwargs)
         nn.modules.loss.BCEWithLogitsLoss.__init__(self, **self.param_dict)
 
 
 class CTCLoss(nn.modules.loss.CTCLoss, TorchModule):
-
-    def __init__(
-            self,
-            blank=0,
-            reduction='mean',
-            zero_infinity=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['blank'] = blank
-        self.param_dict['reduction'] = reduction
-        self.param_dict['zero_infinity'] = zero_infinity
+    def __init__(self, blank=0, reduction="mean", zero_infinity=False, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["blank"] = blank
+        self.param_dict["reduction"] = reduction
+        self.param_dict["zero_infinity"] = zero_infinity
         self.param_dict.update(kwargs)
         nn.modules.loss.CTCLoss.__init__(self, **self.param_dict)
 
 
 class CosineEmbeddingLoss(nn.modules.loss.CosineEmbeddingLoss, TorchModule):
-
-    def __init__(
-            self,
-            margin=0.0,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['margin'] = margin
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, margin=0.0, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["margin"] = margin
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.CosineEmbeddingLoss.__init__(self, **self.param_dict)
 
 
 class CrossEntropyLoss(nn.modules.loss.CrossEntropyLoss, TorchModule):
-
     def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            ignore_index=-100,
-            reduce=None,
-            reduction='mean',
-            label_smoothing=0.0,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['ignore_index'] = ignore_index
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
-        self.param_dict['label_smoothing'] = label_smoothing
+        self,
+        weight=None,
+        size_average=None,
+        ignore_index=-100,
+        reduce=None,
+        reduction="mean",
+        label_smoothing=0.0,
+        **kwargs,
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["ignore_index"] = ignore_index
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
+        self.param_dict["label_smoothing"] = label_smoothing
         self.param_dict.update(kwargs)
         nn.modules.loss.CrossEntropyLoss.__init__(self, **self.param_dict)
 
 
 class GaussianNLLLoss(nn.modules.loss.GaussianNLLLoss, TorchModule):
-
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
         nn.modules.loss.GaussianNLLLoss.__init__(self, **self.param_dict)
 
 
 class HingeEmbeddingLoss(nn.modules.loss.HingeEmbeddingLoss, TorchModule):
-
-    def __init__(
-            self,
-            margin=1.0,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['margin'] = margin
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, margin=1.0, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["margin"] = margin
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.HingeEmbeddingLoss.__init__(self, **self.param_dict)
 
 
 class HuberLoss(nn.modules.loss.HuberLoss, TorchModule):
-
-    def __init__(self, reduction='mean', delta=1.0, **kwargs):
+    def __init__(self, reduction="mean", delta=1.0, **kwargs):
         TorchModule.__init__(self)
-        self.param_dict['reduction'] = reduction
-        self.param_dict['delta'] = delta
+        self.param_dict["reduction"] = reduction
+        self.param_dict["delta"] = delta
         self.param_dict.update(kwargs)
         nn.modules.loss.HuberLoss.__init__(self, **self.param_dict)
 
 
 class KLDivLoss(nn.modules.loss.KLDivLoss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            log_target=False,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
-        self.param_dict['log_target'] = log_target
+    def __init__(self, size_average=None, reduce=None, reduction="mean", log_target=False, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
+        self.param_dict["log_target"] = log_target
         self.param_dict.update(kwargs)
         nn.modules.loss.KLDivLoss.__init__(self, **self.param_dict)
 
 
 class L1Loss(nn.modules.loss.L1Loss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.L1Loss.__init__(self, **self.param_dict)
 
 
 class MSELoss(nn.modules.loss.MSELoss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.MSELoss.__init__(self, **self.param_dict)
 
 
 class MarginRankingLoss(nn.modules.loss.MarginRankingLoss, TorchModule):
-
-    def __init__(
-            self,
-            margin=0.0,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['margin'] = margin
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, margin=0.0, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["margin"] = margin
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.MarginRankingLoss.__init__(self, **self.param_dict)
 
 
 class MultiLabelMarginLoss(nn.modules.loss.MultiLabelMarginLoss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.MultiLabelMarginLoss.__init__(self, **self.param_dict)
 
 
-class MultiLabelSoftMarginLoss(
-        nn.modules.loss.MultiLabelSoftMarginLoss,
-        TorchModule):
-
-    def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+class MultiLabelSoftMarginLoss(nn.modules.loss.MultiLabelSoftMarginLoss, TorchModule):
+    def __init__(self, weight=None, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
-        nn.modules.loss.MultiLabelSoftMarginLoss.__init__(
-            self, **self.param_dict)
+        nn.modules.loss.MultiLabelSoftMarginLoss.__init__(self, **self.param_dict)
 
 
 class MultiMarginLoss(nn.modules.loss.MultiMarginLoss, TorchModule):
-
-    def __init__(
-            self,
-            p=1,
-            margin=1.0,
-            weight=None,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['p'] = p
-        self.param_dict['margin'] = margin
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["p"] = p
+        self.param_dict["margin"] = margin
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.MultiMarginLoss.__init__(self, **self.param_dict)
 
 
 class NLLLoss(nn.modules.loss.NLLLoss, TorchModule):
-
-    def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            ignore_index=-100,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['ignore_index'] = ignore_index
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["ignore_index"] = ignore_index
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.NLLLoss.__init__(self, **self.param_dict)
 
 
 class NLLLoss2d(nn.modules.loss.NLLLoss2d, TorchModule):
-
-    def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            ignore_index=-100,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['ignore_index'] = ignore_index
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["ignore_index"] = ignore_index
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.NLLLoss2d.__init__(self, **self.param_dict)
 
 
 class PoissonNLLLoss(nn.modules.loss.PoissonNLLLoss, TorchModule):
-
     def __init__(
-            self,
-            log_input=True,
-            full=False,
-            size_average=None,
-            eps=1e-08,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['log_input'] = log_input
-        self.param_dict['full'] = full
-        self.param_dict['size_average'] = size_average
-        self.param_dict['eps'] = eps
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+        self, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction="mean", **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["log_input"] = log_input
+        self.param_dict["full"] = full
+        self.param_dict["size_average"] = size_average
+        self.param_dict["eps"] = eps
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.PoissonNLLLoss.__init__(self, **self.param_dict)
 
 
 class SmoothL1Loss(nn.modules.loss.SmoothL1Loss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            beta=1.0,
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
-        self.param_dict['beta'] = beta
+    def __init__(self, size_average=None, reduce=None, reduction="mean", beta=1.0, **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
+        self.param_dict["beta"] = beta
         self.param_dict.update(kwargs)
         nn.modules.loss.SmoothL1Loss.__init__(self, **self.param_dict)
 
 
 class SoftMarginLoss(nn.modules.loss.SoftMarginLoss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.SoftMarginLoss.__init__(self, **self.param_dict)
 
 
 class TripletMarginLoss(nn.modules.loss.TripletMarginLoss, TorchModule):
-
     def __init__(
-            self,
-            margin=1.0,
-            p=2.0,
-            eps=1e-06,
-            swap=False,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['margin'] = margin
-        self.param_dict['p'] = p
-        self.param_dict['eps'] = eps
-        self.param_dict['swap'] = swap
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+        self, margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction="mean", **kwargs
+    ):
+        TorchModule.__init__(self)
+        self.param_dict["margin"] = margin
+        self.param_dict["p"] = p
+        self.param_dict["eps"] = eps
+        self.param_dict["swap"] = swap
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss.TripletMarginLoss.__init__(self, **self.param_dict)
 
 
-class TripletMarginWithDistanceLoss(
-        nn.modules.loss.TripletMarginWithDistanceLoss,
-        TorchModule):
-
+class TripletMarginWithDistanceLoss(nn.modules.loss.TripletMarginWithDistanceLoss, TorchModule):
     def __init__(self, **kwargs):
         TorchModule.__init__(self)
         self.param_dict.update(kwargs)
-        nn.modules.loss.TripletMarginWithDistanceLoss.__init__(
-            self, **self.param_dict)
+        nn.modules.loss.TripletMarginWithDistanceLoss.__init__(self, **self.param_dict)
 
 
 class _Loss(nn.modules.loss._Loss, TorchModule):
-
-    def __init__(
-            self,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss._Loss.__init__(self, **self.param_dict)
 
 
 class _WeightedLoss(nn.modules.loss._WeightedLoss, TorchModule):
-
-    def __init__(
-            self,
-            weight=None,
-            size_average=None,
-            reduce=None,
-            reduction='mean',
-            **kwargs):
-        TorchModule.__init__(self)
-        self.param_dict['weight'] = weight
-        self.param_dict['size_average'] = size_average
-        self.param_dict['reduce'] = reduce
-        self.param_dict['reduction'] = reduction
+    def __init__(self, weight=None, size_average=None, reduce=None, reduction="mean", **kwargs):
+        TorchModule.__init__(self)
+        self.param_dict["weight"] = weight
+        self.param_dict["size_average"] = size_average
+        self.param_dict["reduce"] = reduce
+        self.param_dict["reduction"] = reduction
         self.param_dict.update(kwargs)
         nn.modules.loss._WeightedLoss.__init__(self, **self.param_dict)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/utils/__init__.py` & `pyfate-2.1.0/fate/arch/dataframe/ops/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/components/nn/utils/extract_torch_modules.py` & `pyfate-2.1.0/fate/components/components/nn/hook_code/extract_torch_modules.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,32 +1,46 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import inspect
 from torch.nn.modules import linear, activation, rnn, dropout, sparse, pooling, conv, transformer, batchnorm
 from torch.nn.modules import padding, pixelshuffle
 from torch.nn.modules import loss
 
 
 class Required(object):
-
     def __init__(self):
         pass
 
     def __repr__(self):
-        return '(Required Parameter)'
+        return "(Required Parameter)"
 
 
-def get_all_class_obj(module, key_word=''):
+def get_all_class_obj(module, key_word=""):
     members = inspect.getmembers(module)
     rs = []
     module_name = None
     for name, obj in members:
         if inspect.isclass(obj):
-            if 'modules.' + key_word in obj.__module__:
+            if "modules." + key_word in obj.__module__:
                 rs.append(obj)
                 # print(obj)
-                module_name = obj.__module__.split('.')[-1]
+                module_name = obj.__module__.split(".")[-1]
 
     return rs, module_name
 
 
 def extract_init_param(class_):
     args = inspect.getfullargspec(class_.__init__)
     print(class_)
@@ -35,89 +49,91 @@
     if len(keys) == 0:
         return {}
     defaults = args[3]
     args_map = {}
     print(keys)
     print(defaults)
     if defaults is not None:
-        for idx, i in enumerate(keys[-len(defaults):]):
+        for idx, i in enumerate(keys[-len(defaults) :]):
             print(args_map)
             print(defaults)
             args_map[i] = defaults[idx]
 
     for i in keys:
         if i not in args_map:
             args_map[i] = Required()
 
     return args_map
 
 
 def code_assembly(param, nn_class, module_name):
-    if module_name == 'loss':
-        parent_class = 'FateTorch'
+    if module_name == "loss":
+        parent_class = "FateTorch"
     else:
-        parent_class = 'FateTorch'
+        parent_class = "FateTorch"
 
     para_str = ""
     non_default_param = ""
     init_str = """"""
     for k, v in param.items():
-
         new_para = "\n        self.param_dict['{}'] = {}".format(k, k)
         init_str += new_para
         if isinstance(v, Required):
             non_default_param += str(k)
-            non_default_param += ', '
+            non_default_param += ", "
             continue
 
         para_str += str(k)
         if isinstance(v, str):
             para_str += "='{}'".format(v)
         else:
             para_str += "={}".format(str(v))
-        para_str += ', '
+        para_str += ", "
 
     para_str = non_default_param + para_str
 
     init_ = """
     def __init__(self, {}**kwargs):
         {}.__init__(self){}
         self.param_dict.update(kwargs)
         nn.modules.{}.{}.__init__(self, **self.param_dict)
-    """.format(para_str, parent_class, init_str, module_name, nn_class)
+    """.format(
+        para_str, parent_class, init_str, module_name, nn_class
+    )
 
     code = """
 class {}({}, {}):
         {}
-    """.format(nn_class, 'nn.modules.{}.{}'.format(module_name, nn_class), parent_class, init_)
+    """.format(
+        nn_class, "nn.modules.{}.{}".format(module_name, nn_class), parent_class, init_
+    )
 
     return code
 
 
-if __name__ == '__main__':
-
-    rs1 = get_all_class_obj(linear, 'linear')
-    rs2 = get_all_class_obj(rnn, 'rnn')
-    rs3 = get_all_class_obj(sparse, 'sparse')
-    rs4 = get_all_class_obj(dropout, 'dropout')
-    rs5 = get_all_class_obj(activation, 'activation')
-    rs6 = get_all_class_obj(conv, 'conv')
-    rs7 = get_all_class_obj(transformer, 'transformer')
-    rs8 = get_all_class_obj(pooling, 'pooling')
-    rs9 = get_all_class_obj(batchnorm, 'batchnorm')
-    rs10 = get_all_class_obj(padding, 'padding')
-    rs11 = get_all_class_obj(pixelshuffle, 'pixielshuffle')
-    rs12 = get_all_class_obj(loss, 'loss')
+if __name__ == "__main__":
+    rs1 = get_all_class_obj(linear, "linear")
+    rs2 = get_all_class_obj(rnn, "rnn")
+    rs3 = get_all_class_obj(sparse, "sparse")
+    rs4 = get_all_class_obj(dropout, "dropout")
+    rs5 = get_all_class_obj(activation, "activation")
+    rs6 = get_all_class_obj(conv, "conv")
+    rs7 = get_all_class_obj(transformer, "transformer")
+    rs8 = get_all_class_obj(pooling, "pooling")
+    rs9 = get_all_class_obj(batchnorm, "batchnorm")
+    rs10 = get_all_class_obj(padding, "padding")
+    rs11 = get_all_class_obj(pixelshuffle, "pixielshuffle")
+    rs12 = get_all_class_obj(loss, "loss")
 
     module_str = """"""
     module_str += "from torch import nn\n\n"
     for rs in [rs1, rs2, rs3, rs4, rs5, rs6, rs7, rs8, rs9, rs10, rs11, rs12]:
         module_name = rs[1]
         for i in rs[0]:
             # print(i)
             param = extract_init_param(i)
             class_str = code_assembly(param, i.__name__, module_name)
             module_str += class_str
 
     module_str = module_str
 
-    open('../torch/nn.py', 'w').write(module_str)
+    open("../torch/nn.py", "w").write(module_str)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/psi.py` & `pyfate-2.1.0/fate/components/components/psi.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,23 +8,31 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from fate.components.core import GUEST, HOST, Role, cpn
 from fate.arch.protocol.psi import psi_run
+from fate.components.core import GUEST, HOST, Role, cpn
 
 
 @cpn.component(roles=[GUEST, HOST], provider="fate")
 def psi(
     ctx,
     role: Role,
     input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
     protocol: cpn.parameter(type=str, default="ecdh_psi", optional=True),
     curve_type: cpn.parameter(type=str, default="curve25519", optional=True),
     output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
 ):
-
-    intersect_data = psi_run(ctx, input_data.read(), protocol, curve_type)
+    input_data = input_data.read()
+    input_data_count = input_data.shape[0]
+    intersect_data = psi_run(ctx, input_data, protocol, curve_type)
+    summary = {
+        "input_count": input_data_count,
+        "intersect_count": intersect_data.shape[0],
+        "intersect_rate": intersect_data.shape[0] / input_data_count,
+        "method": "curve25519",
+    }
+    ctx.metrics.log_metrics(summary, "summary")
     output_data.write(intersect_data)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/reader.py` & `pyfate-2.1.0/fate/components/components/hetero_nn.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,45 +8,59 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+import logging
+from fate.arch import Context
 from fate.components.core import GUEST, HOST, Role, cpn
+from fate.components.components.nn.component_utils import train_procedure, predict_procedure
+
+
+logger = logging.getLogger(__name__)
 
 
 @cpn.component(roles=[GUEST, HOST])
-def reader(
-    ctx,
+def hetero_nn(ctx, role):
+    ...
+
+
+@hetero_nn.train()
+def train(
+    ctx: Context,
     role: Role,
-    path: cpn.parameter(type=str, default=None, optional=False),
-    format: cpn.parameter(type=str, default="csv", optional=False),
-    sample_id_name: cpn.parameter(type=str, default=None, optional=True),
-    match_id_name: cpn.parameter(type=str, default=None, optional=True),
-    delimiter: cpn.parameter(type=str, default=",", optional=True),
-    label_name: cpn.parameter(type=str, default=None, optional=True),
-    label_type: cpn.parameter(type=str, default="float32", optional=True),
-    dtype: cpn.parameter(type=str, default="float32", optional=True),
-    output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    train_data: cpn.dataframe_input(roles=[GUEST, HOST]) | cpn.data_directory_input(),
+    validate_data: cpn.dataframe_input(roles=[GUEST, HOST], optional=True) | cpn.data_directory_input(optional=True),
+    runner_module: cpn.parameter(type=str, default="hetero_default_runner", desc="name of your runner script"),
+    runner_class: cpn.parameter(type=str, default="DefaultRunner", desc="class name of your runner class"),
+    runner_conf: cpn.parameter(type=dict, default={}, desc="the parameter dict of the NN runner class"),
+    source: cpn.parameter(type=str, default=None, desc="path to your runner script folder"),
+    train_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
+    output_model: cpn.model_directory_output(roles=[GUEST, HOST], optional=True),
+    warm_start_model: cpn.model_directory_input(roles=[GUEST, HOST], optional=True),
 ):
-    if format == "csv":
-        data_meta = DataframeArtifact(
-            uri=path,
-            name="data",
-            metadata=dict(
-                format=format,
-                sample_id_name=sample_id_name,
-                match_id_name=match_id_name,
-                delimiter=delimiter,
-                label_name=label_name,
-                label_type=label_type,
-                dtype=dtype,
-            ),
-        )
-    elif format == "raw_table":
-        data_meta = DataframeArtifact(uri=path, name="data", metadata=dict(format=format))
-    else:
-        raise ValueError(f"Reader does not support format={format}")
+    train_procedure(
+        ctx,
+        role,
+        train_data,
+        validate_data,
+        runner_module,
+        runner_class,
+        runner_conf,
+        source,
+        train_output_data,
+        output_model,
+        warm_start_model
+    )
 
-    data = ctx.reader(data_meta).read_dataframe()
-    ctx.writer(output_data).write_dataframe(data)
+
+@hetero_nn.predict()
+def predict(
+    ctx: Context,
+    role: Role,
+    test_data: cpn.dataframe_input(roles=[GUEST, HOST]) | cpn.data_directory_input(),
+    input_model: cpn.model_directory_input(roles=[GUEST, HOST]),
+    test_output_data: cpn.dataframe_output(roles=[GUEST, HOST], optional=True),
+):
+    predict_procedure(ctx, role, test_data, input_model, test_output_data)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/sample.py` & `pyfate-2.1.0/fate/components/components/sample.py`

 * *Files 25% similar despite different names*

```diff
@@ -18,36 +18,44 @@
 from fate.arch import Context
 from fate.components.core import GUEST, HOST, Role, cpn, params
 from fate.ml.model_selection.sample import SampleModuleGuest, SampleModuleHost
 
 
 @cpn.component(roles=[GUEST, HOST], provider="fate")
 def sample(
-        ctx: Context,
-        role: Role,
-        input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        replace: cpn.parameter(type=bool, default=False,
-                               desc="whether allow sampling with replacement, default False"),
-        frac: cpn.parameter(type=Union[params.confloat(gt=0.0),
-        Mapping[Union[params.conint(), params.confloat()], params.confloat(gt=0.0)]],
-                            default=None, optional=True,
-                            desc="if mode equals to random, it should be a float number greater than 0,"
-                                 "otherwise a dict of pairs like [label_i, sample_rate_i],"
-                                 "e.g. {0: 0.5, 1: 0.8, 2: 0.3}, any label unspecified in dict will not be sampled,"
-                                 "default: 1.0, cannot be used with n"),
-        n: cpn.parameter(type=params.conint(gt=0), default=None, optional=True,
-                         desc="exact sample size, it should be an int greater than 0, "
-                              "default: None, cannot be used with frac"),
-        random_state: cpn.parameter(type=params.conint(ge=0), default=None,
-                                    desc="random state"),
-        hetero_sync: cpn.parameter(type=bool, default=True,
-                                   desc="whether guest sync sampled data sids with host, "
-                                        "default True for hetero scenario, "
-                                        "should set to False for local and homo scenario"),
-        output_data: cpn.dataframe_output(roles=[GUEST, HOST])
+    ctx: Context,
+    role: Role,
+    input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    replace: cpn.parameter(type=bool, default=False, desc="whether allow sampling with replacement, default False"),
+    frac: cpn.parameter(
+        type=Union[
+            params.confloat(gt=0.0), Mapping[Union[params.conint(), params.confloat()], params.confloat(gt=0.0)]
+        ],
+        default=None,
+        optional=True,
+        desc="if mode equals to random, it should be a float number greater than 0,"
+        "otherwise a dict of pairs like [label_i, sample_rate_i],"
+        "e.g. {0: 0.5, 1: 0.8, 2: 0.3}, any label unspecified in dict will not be sampled,"
+        "default: 1.0, cannot be used with n",
+    ),
+    n: cpn.parameter(
+        type=params.conint(gt=0),
+        default=None,
+        optional=True,
+        desc="exact sample size, it should be an int greater than 0, " "default: None, cannot be used with frac",
+    ),
+    random_state: cpn.parameter(type=params.conint(ge=0), default=None, desc="random state"),
+    hetero_sync: cpn.parameter(
+        type=bool,
+        default=True,
+        desc="whether guest sync sampled data sids with host, "
+        "default True for hetero scenario, "
+        "should set to False for local and homo scenario",
+    ),
+    output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
 ):
     if frac is not None and n is not None:
         raise ValueError(f"n and frac cannot be used at the same time")
     if frac is not None:
         if isinstance(frac, float):
             if frac > 1 and not replace:
                 raise ValueError(f"replace has to be set to True when sampling frac greater than 1.")
@@ -58,19 +66,39 @@
     if n is None and frac is None:
         frac = 1.0
     # check if local but federated sample
     if hetero_sync and len(ctx.parties.ranks) < 2:
         raise ValueError(f"federated sample can only be called when both 'guest' and 'host' present. Please check")
     sub_ctx = ctx.sub_ctx("train")
     if role.is_guest:
-        module = SampleModuleGuest(replace=replace, frac=frac, n=n,
-                                   random_state=random_state, hetero_sync=hetero_sync)
+        module = SampleModuleGuest(replace=replace, frac=frac, n=n, random_state=random_state, hetero_sync=hetero_sync)
     elif role.is_host:
-        module = SampleModuleHost(replace=replace, frac=frac, n=n,
-                                  random_state=random_state, hetero_sync=hetero_sync)
+        module = SampleModuleHost(replace=replace, frac=frac, n=n, random_state=random_state, hetero_sync=hetero_sync)
     else:
         raise ValueError(f"unknown role")
     input_data = input_data.read()
+    original_count = {}
+    if input_data.label is not None:
+        binarized_label = input_data.label.get_dummies()
+        for label_name in binarized_label.schema.columns:
+            label_count = binarized_label[label_name].sum().to_list()[0]
+            true_label_name = int(label_name.split("_")[1])
+            original_count[true_label_name] = label_count
+            if isinstance(frac, dict):
+                if true_label_name not in frac.keys():
+                    frac[true_label_name] = 1.0
+        module.frac = frac
 
     sampled_data = module.fit(sub_ctx, input_data)
+    sample_result_summary = {"total": {"original_count": input_data.shape[0], "sampled_count": sampled_data.shape[0]}}
+    if input_data.label is not None:
+        original_binzied_label = input_data.label.get_dummies()
+        sampled_binarized_label = sampled_data.label.get_dummies()
+        for label_name in binarized_label.schema.columns:
+            original_label_count = original_binzied_label[label_name].sum().to_list()[0]
+            sampled_label_count = sampled_binarized_label[label_name].sum().to_list()[0]
+            label_summary = {"original_count": original_label_count, "sampled_count": sampled_label_count}
+            sample_result_summary[label_name.split("_")[1]] = label_summary
+
+    ctx.metrics.log_metrics(sample_result_summary, "summary")
 
     output_data.write(sampled_data)
```

### Comparing `pyfate-2.0.0b0/fate/components/components/statistics.py` & `pyfate-2.1.0/fate/components/components/statistics.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,45 +17,50 @@
 
 from fate.arch import Context
 from fate.components.core import GUEST, HOST, Role, cpn, params
 
 
 @cpn.component(roles=[GUEST, HOST], provider="fate")
 def statistics(
-        ctx: Context,
-        role: Role,
-        input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
-        metrics: cpn.parameter(
-            type=Union[List[Union[params.statistic_metrics_param(), params.legal_percentile()]],
-            params.statistic_metrics_param(), params.legal_percentile()],
-            default=["mean", "std", "min", "max"],
-            desc="metrics to be computed, default ['count', 'mean', 'std', 'min', 'max']",
-        ),
-        ddof: cpn.parameter(
-            type=params.conint(ge=0), default=1, desc="Delta Degrees of Freedom for std and var, default 1"
-        ),
-        bias: cpn.parameter(
-            type=bool,
-            default=True,
-            desc="If False, the calculations of skewness and kurtosis are corrected for statistical bias.",
-        ),
-        relative_error: cpn.parameter(type=params.confloat(gt=0, le=1), default=1e-3,
-                                      desc="float, error rate for quantile"),
-        skip_col: cpn.parameter(
-            type=List[str],
-            default=None,
-            optional=True,
-            desc="columns to be skipped, default None; if None, statistics will be computed over all columns",
-        ),
-        use_anonymous: cpn.parameter(
-            type=bool, default=False, desc="bool, whether interpret `skip_col` as anonymous column names"
-        ),
-        output_model: cpn.json_model_output(roles=[GUEST, HOST]),
+    ctx: Context,
+    role: Role,
+    input_data: cpn.dataframe_input(roles=[GUEST, HOST]),
+    metrics: cpn.parameter(
+        type=Union[
+            List[Union[params.statistic_metrics_param(), params.legal_percentile()]],
+            params.statistic_metrics_param(),
+            params.legal_percentile(),
+        ],
+        default=["mean", "std", "min", "max"],
+        desc="metrics to be computed, default ['count', 'mean', 'std', 'min', 'max']",
+    ),
+    ddof: cpn.parameter(
+        type=params.conint(ge=0), default=1, desc="Delta Degrees of Freedom for std and var, default 1"
+    ),
+    bias: cpn.parameter(
+        type=bool,
+        default=True,
+        desc="If False, the calculations of skewness and kurtosis are corrected for statistical bias.",
+    ),
+    relative_error: cpn.parameter(
+        type=params.confloat(gt=0, le=1), default=1e-3, desc="float, error rate for quantile"
+    ),
+    skip_col: cpn.parameter(
+        type=List[str],
+        default=None,
+        optional=True,
+        desc="columns to be skipped, default None; if None, statistics will be computed over all columns",
+    ),
+    use_anonymous: cpn.parameter(
+        type=bool, default=False, desc="bool, whether interpret `skip_col` as anonymous column names"
+    ),
+    output_model: cpn.json_model_output(roles=[GUEST, HOST]),
 ):
     from fate.ml.statistics.statistics import FeatureStatistics
+
     sub_ctx = ctx.sub_ctx("train")
     input_data = input_data.read()
     select_cols = get_to_compute_cols(
         input_data.schema.columns, input_data.schema.anonymous_columns, skip_col, use_anonymous
     )
     if isinstance(metrics, str):
         metrics = [metrics]
```

### Comparing `pyfate-2.0.0b0/fate/components/components/toy_example.py` & `pyfate-2.1.0/fate/components/components/toy_example.py`

 * *Files 9% similar despite different names*

```diff
@@ -18,32 +18,32 @@
 from fate.arch import Context
 from fate.arch.dataframe import PandasReader
 from fate.components.core import GUEST, HOST, Role, cpn, params
 
 
 @cpn.component(roles=[GUEST, HOST])
 def toy_example(
-        ctx: Context,
-        role: Role,
-        output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
-        json_model_output: cpn.json_model_output(roles=[GUEST, HOST]),
-        data_num: cpn.parameter(type=params.conint(gt=1), desc="data_num", optional=False),
-        partition: cpn.parameter(type=params.conint(gt=1), desc="data_partition", optional=False),
+    ctx: Context,
+    role: Role,
+    output_data: cpn.dataframe_output(roles=[GUEST, HOST]),
+    json_model_output: cpn.json_model_output(roles=[GUEST, HOST]),
+    data_num: cpn.parameter(type=params.conint(gt=0), desc="data_num", optional=False),
+    partition: cpn.parameter(type=params.conint(gt=0), desc="data_partition", optional=False),
 ):
     pd_df = pd.DataFrame([[str(i), str(i), i] for i in range(data_num)], columns=["sample_id", "match_id", "x0"])
     reader = PandasReader(sample_id_name="sample_id", match_id_name="match_id", dtype="float64", partition=partition)
     df = reader.to_frame(ctx, pd_df)
 
     if role == "guest":
         ctx.hosts.put("guest_index", df.get_indexer(target="sample_id"))
         host_indexes = ctx.hosts[0].get("host_index")
         final_df = df.loc(host_indexes, preserve_order=True)
     else:
         guest_indexes = ctx.guest.get("guest_index")
         final_df = df.loc(guest_indexes)
         ctx.guest.put("host_index", final_df.get_indexer(target="sample_id"))
 
-    assert final_df.shape[0] == data_num, f"data num should be {data_num} instead of {final_df}"
+    assert final_df.shape[0] == data_num, f"data num should be {data_num} instead of {len(final_df)}"
 
     output_data.write(final_df)
 
     json_model_output.write({"test_role": role})
```

### Comparing `pyfate-2.0.0b0/fate/components/components/utils/__init__.py` & `pyfate-2.1.0/fate/arch/federation/backends/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/_cpn_reexport.py` & `pyfate-2.1.0/fate/components/core/_cpn_reexport.py`

 * *Files 12% similar despite different names*

```diff
@@ -21,25 +21,29 @@
     data_directory_inputs,
     data_directory_output,
     data_directory_outputs,
     dataframe_input,
     dataframe_inputs,
     dataframe_output,
     dataframe_outputs,
+    data_unresolved_output,
+    data_unresolved_outputs,
     json_model_input,
     json_model_inputs,
     json_model_output,
     json_model_outputs,
     model_directory_input,
     model_directory_inputs,
     model_directory_output,
     model_directory_outputs,
     parameter,
     table_input,
     table_inputs,
+    model_unresolved_output,
+    model_unresolved_outputs,
 )
 from .essential import Role
 
 T1 = TypeVar("T1")
 T2 = TypeVar("T2")
 
 
@@ -59,16 +63,20 @@
     "dataframe_outputs",
     "table_input",
     "table_inputs",
     "data_directory_input",
     "data_directory_output",
     "data_directory_outputs",
     "data_directory_inputs",
+    "data_unresolved_output",
+    "data_unresolved_outputs",
     "json_model_output",
     "json_model_outputs",
     "json_model_input",
     "json_model_inputs",
     "model_directory_inputs",
     "model_directory_outputs",
     "model_directory_output",
     "model_directory_input",
+    "model_unresolved_output",
+    "model_unresolved_outputs",
 ]
```

### Comparing `pyfate-2.0.0b0/fate/components/core/_cpn_search.py` & `pyfate-2.1.0/fate/components/core/_cpn_search.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/_load_computing.py` & `pyfate-2.1.0/fate/components/core/_load_computing.py`

 * *Files 12% similar despite different names*

```diff
@@ -14,25 +14,30 @@
 #  limitations under the License.
 def load_computing(computing, logger_config=None):
     from fate.components.core.spec.computing import (
         EggrollComputingSpec,
         SparkComputingSpec,
         StandaloneComputingSpec,
     )
+    from fate.arch.computing import ComputingBuilder
 
-    if isinstance(computing, StandaloneComputingSpec):
-        from fate.arch.computing.standalone import CSession
+    builder = ComputingBuilder(computing.metadata.computing_id)
 
-        return CSession(
-            computing.metadata.computing_id, logger_config=logger_config, options=computing.metadata.options
+    if isinstance(computing, StandaloneComputingSpec):
+        return builder.build_standalone(
+            data_dir=computing.metadata.options.get("data_dir", None),
+            logger_config=logger_config,
+            options=computing.metadata.options,
         )
     if isinstance(computing, EggrollComputingSpec):
-        from fate.arch.computing.eggroll import CSession
-
-        return CSession(computing.metadata.computing_id, options=computing.metadata.options)
+        return builder.build_eggroll(
+            host=computing.metadata.host,
+            port=computing.metadata.port,
+            options=computing.metadata.options,
+            config_options=computing.metadata.config_options,
+            config_properties_file=computing.metadata.config_properties_file,
+        )
     if isinstance(computing, SparkComputingSpec):
-        from fate.arch.computing.spark import CSession
-
-        return CSession(computing.metadata.computing_id)
+        return builder.build_spark()
 
     # TODO: load from plugin
     raise ValueError(f"conf.computing={computing} not support")
```

### Comparing `pyfate-2.0.0b0/fate/components/core/_load_device.py` & `pyfate-2.1.0/fate/components/core/_load_device.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/_load_federation.py` & `pyfate-2.1.0/fate/components/core/_load_federation.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,102 +9,93 @@
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 def load_federation(federation, computing):
+    from fate.arch.federation import FederationBuilder, FederationMode
     from fate.components.core.spec.federation import (
         OSXFederationSpec,
         PulsarFederationSpec,
         RabbitMQFederationSpec,
         RollSiteFederationSpec,
         StandaloneFederationSpec,
     )
 
-    if isinstance(federation, StandaloneFederationSpec):
-        from fate.arch.federation.standalone import StandaloneFederation
+    builder = FederationBuilder(
+        federation_session_id=federation.metadata.federation_id,
+        party=federation.metadata.parties.local.tuple(),
+        parties=[p.tuple() for p in federation.metadata.parties.parties],
+    )
 
-        return StandaloneFederation(
-            computing,
-            federation.metadata.federation_id,
-            federation.metadata.parties.local.tuple(),
-            [p.tuple() for p in federation.metadata.parties.parties],
+    if isinstance(federation, StandaloneFederationSpec):
+        return builder.build_standalone(
+            computing_session=computing,
         )
 
-    if isinstance(federation, RollSiteFederationSpec):
-        from fate.arch.computing.eggroll import CSession
-        from fate.arch.federation.eggroll import EggrollFederation
-
-        if not isinstance(computing, CSession):
-            raise RuntimeError(f"Eggroll federation type requires Eggroll computing type, `{type(computing)}` found")
-
-        return EggrollFederation(
-            rp_ctx=computing.get_rpc(),
-            rs_session_id=federation.metadata.federation_id,
-            party=federation.metadata.parties.local.tuple(),
-            parties=[p.tuple() for p in federation.metadata.parties.parties],
-            proxy_endpoint=f"{federation.metadata.rollsite_config.host}:{federation.metadata.rollsite_config.port}",
+    if isinstance(federation, (OSXFederationSpec, RollSiteFederationSpec)):
+        if isinstance(federation, OSXFederationSpec):
+            mode = FederationMode.from_str(federation.metadata.osx_config.mode)
+            host = federation.metadata.osx_config.host
+            port = federation.metadata.osx_config.port
+            options = dict(max_message_size=federation.metadata.osx_config.max_message_size)
+        else:
+            mode = FederationMode.STREAM
+            host = federation.metadata.rollsite_config.host
+            port = federation.metadata.rollsite_config.port
+            options = {}
+        return builder.build_osx(
+            computing_session=computing,
+            host=host,
+            port=port,
+            mode=mode,
+            options=options,
         )
-
     if isinstance(federation, RabbitMQFederationSpec):
-        from fate.arch.federation.rabbitmq import RabbitmqFederation
-
-        return RabbitmqFederation.from_conf(
-            federation_session_id=federation.metadata.federation_id,
+        return builder.build_rabbitmq(
             computing_session=computing,
-            party=federation.metadata.parties.local.tuple(),
-            parties=[p.tuple() for p in federation.metadata.parties.parties],
-            route_table={k: v.dict() for k, v in federation.metadata.route_table.items()},
             host=federation.metadata.rabbitmq_config.host,
             port=federation.metadata.rabbitmq_config.port,
-            mng_port=federation.metadata.rabbitmq_config.mng_port,
-            base_user=federation.metadata.rabbitmq_config.user,
-            base_password=federation.metadata.rabbitmq_config.password,
-            mode=federation.metadata.rabbitmq_config.mode,
-            max_message_size=federation.metadata.rabbitmq_config.max_message_size,
-            rabbitmq_run=federation.metadata.rabbitmq_run,
-            connection=federation.metadata.connection,
+            options=dict(
+                route_table={k: v.dict() for k, v in federation.metadata.route_table.items()},
+                host=federation.metadata.rabbitmq_config.host,
+                port=federation.metadata.rabbitmq_config.port,
+                mng_port=federation.metadata.rabbitmq_config.mng_port,
+                base_user=federation.metadata.rabbitmq_config.user,
+                base_password=federation.metadata.rabbitmq_config.password,
+                mode=federation.metadata.rabbitmq_config.mode,
+                max_message_size=federation.metadata.rabbitmq_config.max_message_size,
+                rabbitmq_run=federation.metadata.rabbitmq_run,
+                connection=federation.metadata.connection,
+            ),
         )
 
     if isinstance(federation, PulsarFederationSpec):
-        from fate.arch.federation.pulsar import PulsarFederation
-
         route_table = {}
         for k, v in federation.metadata.route_table.route.items():
             route_table.update({k: v.dict()})
         if (default := federation.metadata.route_table.default) is not None:
             route_table.update({"default": default.dict()})
-        return PulsarFederation.from_conf(
-            federation_session_id=federation.metadata.federation_id,
+        return builder.build_pulsar(
             computing_session=computing,
-            party=federation.metadata.parties.local.tuple(),
-            parties=[p.tuple() for p in federation.metadata.parties.parties],
-            route_table=route_table,
-            mode=federation.metadata.pulsar_config.mode,
             host=federation.metadata.pulsar_config.host,
             port=federation.metadata.pulsar_config.port,
-            mng_port=federation.metadata.pulsar_config.mng_port,
-            base_user=federation.metadata.pulsar_config.user,
-            base_password=federation.metadata.pulsar_config.password,
-            max_message_size=federation.metadata.pulsar_config.max_message_size,
-            topic_ttl=federation.metadata.pulsar_config.topic_ttl,
-            cluster=federation.metadata.pulsar_config.cluster,
-            tenant=federation.metadata.pulsar_config.tenant,
-            pulsar_run=federation.metadata.pulsar_run,
-            connection=federation.metadata.connection,
+            options=dict(
+                route_table=route_table,
+                mode=federation.metadata.pulsar_config.mode,
+                host=federation.metadata.pulsar_config.host,
+                port=federation.metadata.pulsar_config.port,
+                mng_port=federation.metadata.pulsar_config.mng_port,
+                base_user=federation.metadata.pulsar_config.user,
+                base_password=federation.metadata.pulsar_config.password,
+                max_message_size=federation.metadata.pulsar_config.max_message_size,
+                topic_ttl=federation.metadata.pulsar_config.topic_ttl,
+                cluster=federation.metadata.pulsar_config.cluster,
+                tenant=federation.metadata.pulsar_config.tenant,
+                pulsar_run=federation.metadata.pulsar_run,
+                connection=federation.metadata.connection,
+            ),
         )
 
-    if isinstance(federation, OSXFederationSpec):
-        from fate.arch.federation.osx import OSXFederation
-
-        return OSXFederation.from_conf(
-            federation_session_id=federation.metadata.federation_id,
-            computing_session=computing,
-            party=federation.metadata.parties.local.tuple(),
-            parties=[p.tuple() for p in federation.metadata.parties.parties],
-            host=federation.metadata.osx_config.host,
-            port=federation.metadata.osx_config.port,
-            max_message_size=federation.metadata.osx_config.max_message_size,
-        )
     # TODO: load from plugin
     raise ValueError(f"conf.federation={federation} not support")
```

### Comparing `pyfate-2.0.0b0/fate/components/core/_load_metric_handler.py` & `pyfate-2.1.0/fate/components/core/_load_metric_handler.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/__init__.py` & `pyfate-2.1.0/fate/components/core/component_desc/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -18,14 +18,16 @@
 from ._component_io import ComponentExecutionIO
 from ._parameter import parameter
 from .artifacts import (
     data_directory_input,
     data_directory_inputs,
     data_directory_output,
     data_directory_outputs,
+    data_unresolved_output,
+    data_unresolved_outputs,
     dataframe_input,
     dataframe_inputs,
     dataframe_output,
     dataframe_outputs,
     json_metric_output,
     json_metric_outputs,
     json_model_input,
@@ -34,14 +36,16 @@
     json_model_outputs,
     model_directory_input,
     model_directory_inputs,
     model_directory_output,
     model_directory_outputs,
     table_input,
     table_inputs,
+    model_unresolved_output,
+    model_unresolved_outputs,
 )
 
 __all__ = [
     "component",
     "Component",
     "ComponentExecutionIO",
     "parameter",
@@ -51,18 +55,22 @@
     "dataframe_outputs",
     "table_input",
     "table_inputs",
     "data_directory_input",
     "data_directory_output",
     "data_directory_outputs",
     "data_directory_inputs",
+    "data_unresolved_output",
+    "data_unresolved_outputs",
     "json_model_output",
     "json_model_outputs",
     "json_model_input",
     "json_model_inputs",
     "model_directory_inputs",
     "model_directory_outputs",
     "model_directory_output",
     "model_directory_input",
     "json_metric_output",
     "json_metric_outputs",
+    "model_unresolved_output",
+    "model_unresolved_outputs",
 ]
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/_component.py` & `pyfate-2.1.0/fate/components/core/component_desc/_component.py`

 * *Files 0% similar despite different names*

```diff
@@ -228,33 +228,30 @@
         yaml.dump(spec.dict(), stream=stream)
         if inefficient:
             return stream.getvalue()
 
     def predict(
         self, roles: List = None, provider: Optional[str] = None, version: Optional[str] = None, description=None
     ):
-
         if roles is None:
             roles = []
 
         return self.stage(roles=roles, name=PREDICT.name, provider=provider, version=version, description=description)
 
     def train(
         self, roles: List = None, provider: Optional[str] = None, version: Optional[str] = None, description=None
     ):
-
         if roles is None:
             roles = []
 
         return self.stage(roles=roles, name=TRAIN.name, provider=provider, version=version, description=description)
 
     def cross_validation(
         self, roles: List = None, provider: Optional[str] = None, version: Optional[str] = None, description=None
     ):
-
         if roles is None:
             roles = []
 
         return self.stage(
             roles=roles, name=CROSS_VALIDATION.name, provider=provider, version=version, description=description
         )
 
@@ -322,15 +319,14 @@
         description=description,
         is_subcomponent=False,
     )
 
 
 def _component(name, roles, provider, version, description, is_subcomponent):
     def decorator(f):
-
         cpn_name = name or f.__name__.lower()
         if isinstance(f, Component):
             raise TypeError("Attempted to convert a callback into a component_desc twice.")
         parameters = ComponentParameterDescribes()
         artifacts = ComponentArtifactDescribes()
         signatures = list(inspect.signature(f).parameters.items())
         # first two arguments are ctx and role
@@ -348,15 +344,15 @@
                     name=k,
                     type=annotation.type,
                     default=annotation.default,
                     desc=annotation.desc,
                     optional=annotation.optional,
                 )
             else:
-                raise ComponentDeclareError(f"bad component_desc definition, argument {k} is not annotated")
+                raise ComponentDeclareError(f"bad component_desc definition, argument {v}{k} is not annotated")
 
         if is_subcomponent:
             artifacts.update_roles_and_stages(stages=[Stage.from_str(cpn_name)], roles=roles)
         else:
             artifacts.update_roles_and_stages(stages=[DEFAULT], roles=roles)
         desc = description
         if desc is None:
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/_component_artifact.py` & `pyfate-2.1.0/fate/components/core/component_desc/_component_artifact.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import typing
 from typing import Dict, List, Type, Union
 
 if typing.TYPE_CHECKING:
     from fate.components.core import Role, Stage
 
     from .artifacts import ArtifactDescribe
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/_component_io.py` & `pyfate-2.1.0/fate/components/core/component_desc/_component_io.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 import typing
 from typing import Dict, Generic, List, Optional, Union
 
 from fate.components.core.essential import Role, Stage
 
 from .artifacts._base_type import (
@@ -130,15 +145,14 @@
         from fate.arch import URI
 
         for output_pair_dict, artifacts in [
             (self.output_data, component.artifacts.data_outputs),
             (self.output_model, component.artifacts.model_outputs),
             (self.output_metric, component.artifacts.metric_outputs),
         ]:
-
             if allowed_artifacts := artifacts.get(arg):
                 if allowed_artifacts.is_active_for(stage, role):
                     apply_spec: ArtifactOutputApplySpec = config.output_artifacts.get(arg)
                     if apply_spec is not None:
                         try:
                             if allowed_artifacts.is_multi:
                                 if not apply_spec.is_template():
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/__init__.py` & `pyfate-2.1.0/fate/components/core/component_desc/artifacts/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from ._base_type import (
     ArtifactDescribe,
     DataArtifactDescribe,
     MetricArtifactDescribe,
     ModelArtifactDescribe,
     _ArtifactType,
 )
@@ -12,25 +27,29 @@
     data_directory_outputs,
     dataframe_input,
     dataframe_inputs,
     dataframe_output,
     dataframe_outputs,
     table_input,
     table_inputs,
+    data_unresolved_output,
+    data_unresolved_outputs,
 )
 from .metric import json_metric_output, json_metric_outputs
 from .model import (
     json_model_input,
     json_model_inputs,
     json_model_output,
     json_model_outputs,
     model_directory_input,
     model_directory_inputs,
     model_directory_output,
     model_directory_outputs,
+    model_unresolved_output,
+    model_unresolved_outputs,
 )
 
 __all__ = [
     "_ArtifactType",
     "ArtifactDescribe",
     "json_model_input",
     "json_model_inputs",
@@ -46,10 +65,14 @@
     "dataframe_outputs",
     "table_input",
     "table_inputs",
     "data_directory_input",
     "data_directory_inputs",
     "data_directory_output",
     "data_directory_outputs",
+    "data_unresolved_output",
+    "data_unresolved_outputs",
     "json_metric_output",
     "json_metric_outputs",
+    "model_unresolved_output",
+    "model_unresolved_outputs",
 ]
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/_base_type.py` & `pyfate-2.1.0/fate/components/core/component_desc/artifacts/_base_type.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import inspect
 import typing
 from typing import Generic, List, Optional, Type, TypeVar, Union
 
 from fate.arch import URI
 from fate.components.core.essential import Role, Stage
 from fate.components.core.spec.artifact import (
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/data/__init__.py` & `pyfate-2.1.0/fate/components/core/component_desc/artifacts/model/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,62 +1,73 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import Iterator, List, Optional, Type
 
-from .._base_type import Role, _create_artifact_annotation
-from ._dataframe import DataframeArtifactDescribe, DataframeReader, DataframeWriter
 from ._directory import (
-    DataDirectoryArtifactDescribe,
-    DataDirectoryReader,
-    DataDirectoryWriter,
+    ModelDirectoryArtifactDescribe,
+    ModelDirectoryReader,
+    ModelDirectoryWriter,
 )
-from ._table import TableArtifactDescribe, TableReader, TableWriter
-
-
-def dataframe_input(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[DataframeReader]:
-    return _create_artifact_annotation(True, False, DataframeArtifactDescribe, "data")(roles, desc, optional)
-
-
-def dataframe_inputs(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[List[DataframeReader]]:
-    return _create_artifact_annotation(True, True, DataframeArtifactDescribe, "data")(roles, desc, optional)
+from ._json import JsonModelArtifactDescribe, JsonModelReader, JsonModelWriter
+from ._unresolved import ModelUnresolvedArtifactDescribe, ModelUnresolvedReader, ModelUnresolvedWriter
+from .._base_type import Role, _create_artifact_annotation
 
 
-def dataframe_output(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[DataframeWriter]:
-    return _create_artifact_annotation(False, False, DataframeArtifactDescribe, "data")(roles, desc, optional)
+def json_model_input(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[JsonModelReader]:
+    return _create_artifact_annotation(True, False, JsonModelArtifactDescribe, "model")(roles, desc, optional)
 
 
-def dataframe_outputs(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[Iterator[DataframeWriter]]:
-    return _create_artifact_annotation(False, True, DataframeArtifactDescribe, "data")(roles, desc, optional)
+def json_model_inputs(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[List[JsonModelReader]]:
+    return _create_artifact_annotation(True, True, JsonModelArtifactDescribe, "model")(roles, desc, optional)
 
 
-def table_input(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[TableReader]:
-    return _create_artifact_annotation(True, False, TableArtifactDescribe, "data")(roles, desc, optional)
+def json_model_output(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[JsonModelWriter]:
+    return _create_artifact_annotation(False, False, JsonModelArtifactDescribe, "model")(roles, desc, optional)
 
 
-def table_inputs(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[List[TableReader]]:
-    return _create_artifact_annotation(True, True, TableArtifactDescribe, "data")(roles, desc, optional)
+def json_model_outputs(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[Iterator[JsonModelWriter]]:
+    return _create_artifact_annotation(False, True, JsonModelArtifactDescribe, "model")(roles, desc, optional)
 
 
-def table_output(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[TableWriter]:
-    return _create_artifact_annotation(False, False, TableArtifactDescribe, "data")(roles, desc, optional)
+def model_directory_input(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[ModelDirectoryReader]:
+    return _create_artifact_annotation(True, False, ModelDirectoryArtifactDescribe, "model")(roles, desc, optional)
 
 
-def table_outputs(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[Iterator[TableWriter]]:
-    return _create_artifact_annotation(False, True, TableArtifactDescribe, "data")(roles, desc, optional)
+def model_directory_inputs(
+    roles: Optional[List[Role]] = None, desc="", optional=False
+) -> Type[List[ModelDirectoryReader]]:
+    return _create_artifact_annotation(True, True, ModelDirectoryArtifactDescribe, "model")(roles, desc, optional)
 
 
-def data_directory_input(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[DataDirectoryReader]:
-    return _create_artifact_annotation(True, False, DataDirectoryArtifactDescribe, "data")(roles, desc, optional)
+def model_directory_output(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[ModelDirectoryWriter]:
+    return _create_artifact_annotation(False, False, ModelDirectoryArtifactDescribe, "model")(roles, desc, optional)
 
 
-def data_directory_inputs(
+def model_directory_outputs(
     roles: Optional[List[Role]] = None, desc="", optional=False
-) -> Type[List[DataDirectoryReader]]:
-    return _create_artifact_annotation(True, True, DataDirectoryArtifactDescribe, "data")(roles, desc, optional)
+) -> Type[Iterator[ModelDirectoryWriter]]:
+    return _create_artifact_annotation(False, True, ModelDirectoryArtifactDescribe, "model")(roles, desc, optional)
 
 
-def data_directory_output(roles: Optional[List[Role]] = None, desc="", optional=False) -> Type[DataDirectoryWriter]:
-    return _create_artifact_annotation(False, False, DataDirectoryArtifactDescribe, "data")(roles, desc, optional)
+def model_unresolved_output(
+    roles: Optional[List[Role]] = None, desc="", optional=False
+) -> Type[ModelUnresolvedWriter]:
+    return _create_artifact_annotation(False, False, ModelUnresolvedArtifactDescribe, "model")(roles, desc, optional)
 
 
-def data_directory_outputs(
+def model_unresolved_outputs(
     roles: Optional[List[Role]] = None, desc="", optional=False
-) -> Type[Iterator[DataDirectoryWriter]]:
-    return _create_artifact_annotation(False, True, DataDirectoryArtifactDescribe, "data")(roles, desc, optional)
+) -> Type[Iterator[ModelUnresolvedWriter]]:
+    return _create_artifact_annotation(False, True, ModelUnresolvedArtifactDescribe, "model")(roles, desc, optional)
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/metric/_json.py` & `pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/_table.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,74 +1,65 @@
-import json
-import logging
-import typing
-from pathlib import Path
-from typing import Dict, Optional, Union
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
 
-import requests
-from fate.components.core.essential import JsonMetricArtifactType
+import typing
 
-logger = logging.getLogger(__name__)
+from fate.components.core.essential import TableArtifactType
 
 from .._base_type import (
     URI,
     ArtifactDescribe,
+    DataOutputMetadata,
     Metadata,
-    MetricOutputMetadata,
     _ArtifactType,
+    _ArtifactTypeReader,
     _ArtifactTypeWriter,
 )
 
 if typing.TYPE_CHECKING:
     from fate.arch import Context
 
 
-class JsonMetricFileWriter(_ArtifactTypeWriter[MetricOutputMetadata]):
-    def write(self, data, metadata: Optional[Dict] = None):
+class TableWriter(_ArtifactTypeWriter[DataOutputMetadata]):
+    def write(self, table):
         self.artifact.consumed()
-        path = Path(self.artifact.uri.path)
-        path.parent.mkdir(parents=True, exist_ok=True)
-        with path.open("w") as fw:
-            json.dump(data, fw)
-
-        if metadata is not None:
-            self.artifact.metadata.metadata = metadata
+        if "schema" not in self.artifact.metadata.metadata:
+            self.artifact.metadata.metadata["schema"] = {}
+        table.save(
+            uri=self.artifact.uri,
+            schema=self.artifact.metadata.metadata["schema"],
+            options=self.artifact.metadata.metadata.get("options", None),
+        )
 
 
-class JsonMetricRestfulWriter(_ArtifactTypeWriter[MetricOutputMetadata]):
-    def write(self, data):
+class TableReader(_ArtifactTypeReader):
+    def read(self):
         self.artifact.consumed()
-        try:
-            output = requests.post(url=self.artifact.uri.original_uri, json=dict(data=[data]))
-        except Exception as e:
-            logger.error(f"write data `{data}` to {self.artifact.uri.original_uri} failed, error: {e}")
-        else:
-            logger.debug(f"write data `{data}` to {self.artifact.uri.original_uri} success, output: {output}")
-
-    def write_metadata(self, metadata: Dict):
-        self.artifact.metadata.metadata = metadata
-
-    def close(self):
-        pass
+        return self.ctx.computing.load(
+            uri=self.artifact.uri,
+            schema=self.artifact.metadata.metadata.get("schema", {}),
+            options=self.artifact.metadata.metadata.get("options", None),
+        )
 
 
-class JsonMetricArtifactDescribe(ArtifactDescribe[JsonMetricArtifactType, MetricOutputMetadata]):
+class TableArtifactDescribe(ArtifactDescribe[TableArtifactType, DataOutputMetadata]):
     @classmethod
     def get_type(cls):
-        return JsonMetricArtifactType
+        return TableArtifactType
 
-    def get_writer(
-        self, config, ctx: "Context", uri: URI, type_name: str
-    ) -> Union[JsonMetricFileWriter, JsonMetricRestfulWriter]:
-        if uri.scheme == "http" or uri.scheme == "https":
-            return JsonMetricRestfulWriter(
-                ctx, _ArtifactType(uri=uri, metadata=MetricOutputMetadata(), type_name=type_name)
-            )
-        elif uri.scheme == "file":
-            return JsonMetricFileWriter(
-                ctx, _ArtifactType(uri=uri, metadata=MetricOutputMetadata(), type_name=type_name)
-            )
-        else:
-            raise ValueError(f"unsupported uri scheme: {uri.scheme}")
+    def get_writer(self, config, ctx: "Context", uri: URI, type_name: str) -> TableWriter:
+        return TableWriter(ctx, _ArtifactType(uri, DataOutputMetadata(), type_name))
 
-    def get_reader(self, ctx: "Context", uri: URI, metadata: Metadata, type_name: str):
-        raise NotImplementedError()
+    def get_reader(self, ctx: "Context", uri: "URI", metadata: "Metadata", type_name: str) -> TableReader:
+        return TableReader(ctx, _ArtifactType(uri, metadata, type_name))
```

### Comparing `pyfate-2.0.0b0/fate/components/core/component_desc/artifacts/model/_directory.py` & `pyfate-2.1.0/fate/components/core/component_desc/artifacts/data/_directory.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,64 +1,65 @@
-import datetime
-import typing
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from pathlib import Path
 
-from fate.components.core.essential import ModelDirectoryArtifactType
+from fate.components.core.essential import DataDirectoryArtifactType
 
 from .._base_type import (
     URI,
     ArtifactDescribe,
+    DataOutputMetadata,
     Metadata,
-    ModelOutputMetadata,
     _ArtifactType,
     _ArtifactTypeReader,
     _ArtifactTypeWriter,
 )
 
-if typing.TYPE_CHECKING:
-    from fate.arch import Context
-
 
-class ModelDirectoryWriter(_ArtifactTypeWriter[ModelOutputMetadata]):
-    def get_directory(self):
+class DataDirectoryWriter(_ArtifactTypeWriter[DataDirectoryArtifactType]):
+    def get_directory(self) -> Path:
         self.artifact.consumed()
         path = Path(self.artifact.uri.path)
         path.mkdir(parents=True, exist_ok=True)
+        return path
 
-        # update model overview
-        from fate.components.core.spec.model import MLModelModelSpec
-
-        model_overview = self.artifact.metadata.model_overview
-        model_overview.party.models.append(
-            MLModelModelSpec(
-                name="",
-                created_time=datetime.datetime.now().isoformat(),
-                file_format=ModelDirectoryArtifactType.type_name,
-                metadata={},
-            )
-        )
-        return self.artifact.uri.path
-
-    def write_metadata(self, metadata: dict):
-        self.artifact.metadata.metadata = metadata
+    def write_metadata(self, metadata: dict, name=None, namespace=None):
+        self.artifact.metadata.metadata.update(metadata)
+        if name is not None:
+            self.artifact.metadata.name = name
+        if namespace is not None:
+            self.artifact.metadata.namespace = namespace
 
 
-class ModelDirectoryReader(_ArtifactTypeReader):
-    def get_directory(self):
+class DataDirectoryReader(_ArtifactTypeReader):
+    def get_directory(self) -> Path:
         self.artifact.consumed()
         path = Path(self.artifact.uri.path)
         return path
 
     def get_metadata(self):
         return self.artifact.metadata.metadata
 
 
-class ModelDirectoryArtifactDescribe(ArtifactDescribe[ModelDirectoryArtifactType, ModelOutputMetadata]):
+class DataDirectoryArtifactDescribe(ArtifactDescribe[DataDirectoryArtifactType, DataOutputMetadata]):
     @classmethod
     def get_type(cls):
-        return ModelDirectoryArtifactType
+        return DataDirectoryArtifactType
 
-    def get_writer(self, config, ctx: "Context", uri: URI, type_name: str) -> ModelDirectoryWriter:
-        return ModelDirectoryWriter(ctx, _ArtifactType(uri=uri, metadata=ModelOutputMetadata(), type_name=type_name))
+    def get_writer(self, config, ctx, uri: URI, type_name: str) -> DataDirectoryWriter:
+        return DataDirectoryWriter(ctx, _ArtifactType(uri=uri, metadata=DataOutputMetadata(), type_name=type_name))
 
-    def get_reader(self, ctx: "Context", uri: URI, metadata: Metadata, type_name: str) -> ModelDirectoryReader:
-        return ModelDirectoryReader(ctx, _ArtifactType(uri=uri, metadata=metadata, type_name=type_name))
+    def get_reader(self, ctx, uri: "URI", metadata: "Metadata", type_name: str) -> DataDirectoryReader:
+        return DataDirectoryReader(ctx, _ArtifactType(uri=uri, metadata=metadata, type_name=type_name))
```

### Comparing `pyfate-2.0.0b0/fate/components/core/essential/_label.py` & `pyfate-2.1.0/fate/components/core/essential/_label.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/essential/_role.py` & `pyfate-2.1.0/fate/components/core/essential/_role.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/essential/_stage.py` & `pyfate-2.1.0/fate/components/core/essential/_stage.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/params/__init__.py` & `pyfate-2.1.0/fate/components/core/params/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/params/_fields.py` & `pyfate-2.1.0/fate/components/core/params/_fields.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import typing
 from typing import Any, Optional, Type, TypeVar
 
 import pydantic
 
 
 class Parameter:
```

### Comparing `pyfate-2.0.0b0/fate/components/core/params/_filter_param.py` & `pyfate-2.1.0/fate/components/core/params/_filter_param.py`

 * *Files 16% similar despite different names*

```diff
@@ -20,72 +20,76 @@
 from ._fields import string_choice, Parameter, conint, confloat
 from ._metrics import statistic_metrics_param, legal_percentile
 
 
 class StandardFilterParam(pydantic.BaseModel, Parameter):
     metrics: List[str]
 
-    filter_type: List[string_choice({'threshold', 'top_k', 'top_percentile'})] = ['threshold']
+    filter_type: List[string_choice({"threshold", "top_k", "top_percentile"})] = ["threshold"]
     threshold: List[Union[confloat(ge=0.0, le=1.0), conint(ge=1)]] = [1.0]
     take_high: List[bool] = [True]
 
-    @pydantic.validator('metrics', 'filter_type', 'threshold', 'take_high', pre=True, allow_reuse=True)
+    @pydantic.validator("metrics", "filter_type", "threshold", "take_high", pre=True, allow_reuse=True)
     def to_list(cls, v):
         return v if isinstance(v, list) else [v]
 
     @pydantic.root_validator(pre=False)
     def check_filter_param_length(cls, values):
         max_length = max([len(x) for k, x in values.items()])
         for k, v in values.items():
             if len(v) == 1:
                 v *= max_length
-            assert len(v) == max_length, f"Length of {k}: {v} does not match " \
-                                         f"max length {max_length} of (metrics, filter_type, threshold, take_high)."
+            assert len(v) == max_length, (
+                f"Length of {k}: {v} does not match "
+                f"max length {max_length} of (metrics, filter_type, threshold, take_high)."
+            )
         return values
 
 
 class FederatedStandardFilterParam(StandardFilterParam, Parameter):
-    host_filter_type: List[string_choice({'threshold', 'top_k', 'top_percentile'})] = ['threshold']
+    host_filter_type: List[string_choice({"threshold", "top_k", "top_percentile"})] = ["threshold"]
     host_threshold: List[Union[confloat(ge=0.0, le=1.0), conint(ge=1)]] = [1.0]
     host_take_high: List[bool] = [True]
 
     select_federated: bool = True
 
-    @pydantic.validator('host_filter_type', 'host_threshold', 'host_take_high', pre=True, allow_reuse=True)
+    @pydantic.validator("host_filter_type", "host_threshold", "host_take_high", pre=True, allow_reuse=True)
     def to_list(cls, v):
         return v if isinstance(v, list) else [v]
 
     @pydantic.root_validator(pre=False)
     def check_filter_param_length(cls, values):
-        select_values = {k: v for k, v in values.items() if k != 'select_federated'}
+        select_values = {k: v for k, v in values.items() if k != "select_federated"}
         max_length = max([len(x) for k, x in select_values.items()])
         for k, v in select_values.items():
             if len(v) == 1:
                 v *= max_length
-            assert len(v) == max_length, f"Length of {k}: {v} does not match " \
-                                         f"max length {max_length} of (metrics, filter_type, threshold, take_high)."
+            assert len(v) == max_length, (
+                f"Length of {k}: {v} does not match "
+                f"max length {max_length} of (metrics, filter_type, threshold, take_high)."
+            )
         return values
 
 
 class IVFilterParam(FederatedStandardFilterParam, Parameter):
-    metrics: List[string_choice({'iv'})] = ['iv']
+    metrics: List[string_choice({"iv"})] = ["iv"]
 
 
 class StatisticFilterParam(StandardFilterParam, Parameter):
     metrics: List[Union[statistic_metrics_param(), legal_percentile()]] = ["mean"]
 
 
 class ManualFilterParam(pydantic.BaseModel, Parameter):
     keep_col: List[str] = []
     filter_out_col: List[str] = []
 
     @pydantic.root_validator(pre=False)
     def no_intersection(cls, values):
-        filter_out_col = values.get('filter_out_col', [])
-        keep_col = values.get('keep_col', [])
+        filter_out_col = values.get("filter_out_col", [])
+        keep_col = values.get("keep_col", [])
         intersection = set(filter_out_col).intersection(set(keep_col))
         if intersection:
             raise ValueError(f"`keep_col` and `filter_out_col` share common elements: {intersection}")
         return values
 
 
 def iv_filter_param():
```

### Comparing `pyfate-2.0.0b0/fate/components/core/params/_he_param.py` & `pyfate-2.1.0/fate/components/core/spec/device.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,29 +1,27 @@
 #
-#  Copyright 2023 The FATE Authors. All Rights Reserved.
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
 #
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-#
+from typing import Literal
 
 import pydantic
 
-from ._fields import string_choice
-
 
-class HEParam(pydantic.BaseModel):
-    kind: string_choice(["paillier", "ou", "mock"])
-    key_length: int = 1024
+class CPUSpec(pydantic.BaseModel):
+    type: Literal["CPU"]
+    metadata: dict = {}
 
 
-def he_param():
-    namespace = {}
-    return type("HEParam", (HEParam,), namespace)
+class GPUSpec(pydantic.BaseModel):
+    type: Literal["GPU"]
+    metadata: dict = {}
```

### Comparing `pyfate-2.0.0b0/fate/components/core/params/_metrics.py` & `pyfate-2.1.0/fate/components/core/params/_metrics.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,15 +36,15 @@
     median=True,
     std=True,
     var=True,
     coe=True,
     missing_count=True,
     missing_ratio=True,
     skewness=True,
-    kurtosis=True
+    kurtosis=True,
 ) -> Type[str]:
     choice = {
         "count": count,
         "sum": sum,
         "max": max,
         "min": min,
         "mean": mean,
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/__init__.py` & `pyfate-2.1.0/fate/arch/launchers/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/artifact.py` & `pyfate-2.1.0/fate/components/core/spec/artifact.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,26 +8,20 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-import datetime
 import re
 from typing import Dict, List, Optional, Union
 
 import pydantic
 
 from .model import (
-    MLModelComponentSpec,
-    MLModelFederatedSpec,
-    MLModelModelSpec,
-    MLModelPartiesSpec,
-    MLModelPartySpec,
     MLModelSpec,
 )
 
 # see https://www.rfc-editor.org/rfc/rfc3986#appendix-B
 # scheme    = $2
 # authority = $4
 # path      = $5
@@ -45,14 +39,20 @@
     task_id: str
     party_task_id: str
     task_name: str
     component: str
     output_artifact_key: str
     output_index: Optional[int] = None
 
+    def unique_key(self):
+        key = f"{self.task_id}_{self.task_name}_{self.output_artifact_key}"
+        if self.output_index is not None:
+            key = f"{key}_index_{self.output_index}"
+        return key
+
 
 class Metadata(pydantic.BaseModel):
     metadata: dict = pydantic.Field(default_factory=dict)
     name: Optional[str] = None
     namespace: Optional[str] = None
     source: Optional[ArtifactSource] = None
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/component.py` & `pyfate-2.1.0/fate/components/core/spec/component.py`

 * *Files 0% similar despite different names*

```diff
@@ -80,15 +80,15 @@
     component: ComponentSpec
     schema_version: str = "v1"
 
 
 class ArtifactTypeSpec(BaseModel):
     type_name: str
     uri_types: List[str]
-    path_type: Literal["file", "directory", "distributed"]
+    path_type: Literal["file", "directory", "distributed", "unresolved"]
 
 
 class ComponentIOArtifactTypeSpec(BaseModel):
     name: str
     is_multi: bool
     optional: bool
     types: List[ArtifactTypeSpec]
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/computing.py` & `pyfate-2.1.0/fate/components/core/spec/computing.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+import typing
 from typing import Literal
 
 import pydantic
 
 
 class StandaloneComputingSpec(pydantic.BaseModel):
     class MetadataSpec(pydantic.BaseModel):
@@ -25,14 +26,18 @@
     type: Literal["standalone"]
     metadata: MetadataSpec
 
 
 class EggrollComputingSpec(pydantic.BaseModel):
     class MetadataSpec(pydantic.BaseModel):
         computing_id: str
+        host: typing.Optional[str] = None
+        port: typing.Optional[int] = None
+        config_options: typing.Optional[dict] = None
+        config_properties_file: typing.Optional[str] = None
         options: dict = {}
 
     type: Literal["eggroll"]
     metadata: MetadataSpec
 
 
 class SparkComputingSpec(pydantic.BaseModel):
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/device.py` & `pyfate-2.1.0/fate/components/core/params/_cipher.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,20 +8,23 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-from typing import Literal
+
+from typing import Literal, Union
 
 import pydantic
 
 
-class CPUSpec(pydantic.BaseModel):
-    type: Literal["CPU"]
-    metadata: dict = {}
+class PaillierCipherParam(pydantic.BaseModel):
+    method: Literal["paillier"] = "paillier"
+    key_length: pydantic.conint(gt=1024) = 1024
+
+
+class NoopCipher(pydantic.BaseModel):
+    method: Literal[None]
 
 
-class GPUSpec(pydantic.BaseModel):
-    type: Literal["GPU"]
-    metadata: dict = {}
+CipherParamType = Union[PaillierCipherParam, NoopCipher]
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/federation.py` & `pyfate-2.1.0/fate/components/core/spec/federation.py`

 * *Files 1% similar despite different names*

```diff
@@ -121,14 +121,15 @@
 
 
 class OSXFederationSpec(pydantic.BaseModel):
     class MetadataSpec(pydantic.BaseModel):
         class OSXConfig(pydantic.BaseModel):
             host: str
             port: int
+            mode: str = "message_queue"
             max_message_size: Optional[int] = None
 
         federation_id: str
         parties: FederationPartiesSpec
         osx_config: OSXConfig
 
     type: Literal["osx"]
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/logger.py` & `pyfate-2.1.0/fate/components/core/spec/logger.py`

 * *Files 12% similar despite different names*

```diff
@@ -58,8 +58,12 @@
                 formatters=logging_formatters,
                 handlers=handlers,
                 filters={},
                 loggers={},
                 root=dict(handlers=["console"], level="DEBUG"),
                 disable_existing_loggers=False,
             )
+
+        for _name, _conf in self.config.get("handlers", {}).items():
+            if _conf.get("filename"):
+                os.makedirs(os.path.dirname(_conf.get("filename")), exist_ok=True)
         logging.config.dictConfig(self.config)
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/model.py` & `pyfate-2.1.0/fate/components/core/spec/model.py`

 * *Files 9% similar despite different names*

```diff
@@ -28,32 +28,29 @@
 class MLModelPartiesSpec(pydantic.BaseModel):
     guest: List[str]
     host: List[str]
     arbiter: List[str]
 
 
 class MLModelFederatedSpec(pydantic.BaseModel):
-
     task_id: str
     parties: MLModelPartiesSpec
     component: MLModelComponentSpec
 
 
 class MLModelModelSpec(pydantic.BaseModel):
     name: str
     created_time: str
     file_format: str
     metadata: dict
 
 
 class MLModelPartySpec(pydantic.BaseModel):
-
     party_task_id: str
     role: str
     partyid: str
     models: List[MLModelModelSpec]
 
 
 class MLModelSpec(pydantic.BaseModel):
-
     federated: MLModelFederatedSpec
     party: MLModelPartySpec
```

### Comparing `pyfate-2.0.0b0/fate/components/core/spec/task.py` & `pyfate-2.1.0/fate/components/core/spec/task.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/entrypoint/__init__.py` & `pyfate-2.1.0/fate/arch/protocol/phe/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/components/entrypoint/cli/component/execute_cli.py` & `pyfate-2.1.0/fate/components/entrypoint/cli/component/execute_cli.py`

 * *Files 5% similar despite different names*

```diff
@@ -75,44 +75,54 @@
     # install logger
     task_config.conf.logger.install(debug=debug)
     logger = logging.getLogger(__name__)
     logger.debug("logger installed")
     logger.debug(f"task config: {task_config}")
 
     os.makedirs(os.path.dirname(execution_final_meta_path), exist_ok=True)
+
+    from fate.arch.config import cfg
+
+    if task_config.component in cfg.components.blacklist:
+        raise RuntimeError(f"component `{task_config.component}` is in blacklist, do not use it")
+
     execute_component_from_config(task_config, execution_final_meta_path)
 
 
 def execute_component_from_config(config: "TaskConfigSpec", output_path):
     import json
     import logging
     import traceback
 
     from fate.arch import CipherKit, Context
-    from fate.arch.computing import profile_ends, profile_start
+    from fate.arch.trace import profile_ends, profile_start
     from fate.components.core import (
         ComponentExecutionIO,
         Role,
         Stage,
         load_component,
         load_computing,
         load_device,
         load_federation,
         load_metric_handler,
+        is_root_worker,
     )
 
     logger = logging.getLogger(__name__)
     logger.debug(f"logging final status to  `{output_path}`")
     try:
         party_task_id = config.party_task_id
         device = load_device(config.conf.device)
         computing = load_computing(config.conf.computing, config.conf.logger.config)
-        federation = load_federation(config.conf.federation, computing)
+        if is_root_worker():
+            federation = load_federation(config.conf.federation, computing)
+        else:
+            federation = None
+            logger.info("skip federation initialization for non-root worker")
         cipher = CipherKit(device=device)
-
         ctx = Context(
             device=device,
             computing=computing,
             federation=federation,
             cipher=cipher,
         )
         role = Role.from_str(config.role)
```

### Comparing `pyfate-2.0.0b0/fate/ml/__init__.py` & `pyfate-2.1.0/fate/components/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/abc/__init__.py` & `pyfate-2.1.0/fate/components/components/nn/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/abc/module.py` & `pyfate-2.1.0/fate/ml/abc/module.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,28 +12,33 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 from typing import Optional, Union
 
 from fate.arch import Context
 from fate.arch.dataframe import DataFrame
+import typing
 
 
 class Model:
     ...
 
 
 class Module:
     mode: str
 
+    @typing.overload
+    def fit(self, ctx: Context, input_data):
+        ...
+
     def fit(
         self,
         ctx: Context,
-        train_data: DataFrame,
-        validate_data: Optional[DataFrame] = None,
+        *args,
+        **kwargs,
     ) -> None:
         ...
 
     def transform(self, ctx: Context, transform_data: DataFrame) -> DataFrame:
         ...
 
     def predict(self, ctx: Context, predict_data: DataFrame) -> DataFrame:
```

### Comparing `pyfate-2.0.0b0/fate/ml/aggregator/base.py` & `pyfate-2.1.0/fate/ml/aggregator/base.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import logging
 from typing import Optional
 
 import numpy as np
 import torch as t
 from fate.arch import Context
 from fate.arch.protocol.secure_aggregation._secure_aggregation import (
@@ -32,15 +47,14 @@
         concat_suffix = self.suffix_str + "_" + str(self._count)
         self._count += 1
         return concat_suffix
 
 
 class Aggregator:
     def __init__(self, ctx: Context, aggregator_name: Optional[str] = None):
-
         if aggregator_name is not None:
             agg_name = "_" + aggregator_name
         else:
             agg_name = ""
         self.suffix = {
             "local_loss": AutoSuffix("local_loss" + agg_name),
             "agg_loss": AutoSuffix("agg_loss" + agg_name),
@@ -65,15 +79,14 @@
         aggregator_name: str = None,
         aggregate_type="mean",
         sample_num=1,
         is_mock=True,
         require_grad=True,
         float_p="float64",
     ) -> None:
-
         super().__init__(ctx, aggregator_name)
         self._weight = 1.0
         self.aggregator_name = "default" if aggregator_name is None else aggregator_name
         self.require_grad = require_grad
 
         assert float_p in TORCH_TENSOR_PRECISION, "float_p should be one of {}".format(TORCH_TENSOR_PRECISION)
         self.float_p = float_p
@@ -100,15 +113,14 @@
 
         self.model_aggregator = sa_client(prefix=self.aggregator_name + "_model", is_mock=is_mock)
         self.model_aggregator.dh_exchange(ctx, [ctx.guest.rank, *ctx.hosts.ranks])
         self.loss_aggregator = sa_client(prefix=self.aggregator_name + "_loss", is_mock=is_mock)
         self.loss_aggregator.dh_exchange(ctx, [ctx.guest.rank, *ctx.hosts.ranks])
 
     def _convert_type(self, data, dtype="float32"):
-
         if isinstance(data, t.Tensor):
             if dtype == "float32":
                 data = data.float()
             elif dtype == "float64":
                 data = data.double()
             else:
                 raise ValueError("Invalid dtype. Choose either 'float32' or 'float64'")
@@ -124,15 +136,14 @@
                 raise ValueError("Invalid dtype. Choose either 'float32' or 'float64'")
         else:
             raise ValueError("Invalid data type. Only numpy ndarray and PyTorch tensor are supported.")
 
         return numpy_array
 
     def _process_model(self, model):
-
         to_agg = None
         if isinstance(model, np.ndarray) or isinstance(model, t.Tensor):
             to_agg = self._convert_type(model, self.float_p)
             return [to_agg]
 
         if isinstance(model, t.nn.Module):
             parameters = list(model.parameters())
@@ -145,18 +156,20 @@
 
         elif isinstance(model, list):
             to_agg = []
             for p in model:
                 to_agg.append(self._convert_type(p, self.float_p))
             agg_list = to_agg
 
+        else:
+            return None
+
         return agg_list
 
     def _recover_model(self, model, agg_model):
-
         if isinstance(model, np.ndarray) or isinstance(model, t.Tensor):
             return agg_model
         elif isinstance(model, t.nn.Module):
             if self.require_grad:
                 for agg_p, p in zip(agg_model, [p for p in model.parameters() if p.requires_grad]):
                     p.data.copy_(t.Tensor(agg_p))
             else:
@@ -167,15 +180,14 @@
             return agg_model
 
     """
     User API
     """
 
     def model_aggregation(self, ctx, model):
-
         to_send = self._process_model(model)
         agg_model = self.model_aggregator.secure_aggregate(ctx, to_send, self._weight)
         return self._recover_model(model, agg_model)
 
     def loss_aggregation(self, ctx, loss):
         if isinstance(loss, t.Tensor):
             loss = loss.detach.cpu().numpy()
@@ -184,15 +196,14 @@
         loss = [loss]
         agg_loss = self.loss_aggregator.secure_aggregate(ctx, loss, self._weight)
         return agg_loss
 
 
 class BaseAggregatorServer(Aggregator):
     def __init__(self, ctx: Context, aggregator_name: str = None, is_mock=True) -> None:
-
         super().__init__(ctx, aggregator_name)
 
         weight_list = self._collect(ctx, self.suffix["local_weight"]())
         weight_sum = sum(weight_list)
         ret_weight = []
         for w in weight_list:
             ret_weight.append(w / weight_sum)
@@ -236,8 +247,8 @@
     User API
     """
 
     def model_aggregation(self, ctx, ranks=None):
         self.model_aggregator.secure_aggregate(ctx, ranks=ranks)
 
     def loss_aggregation(self, ctx, ranks=None):
-        self.loss_aggregator.secure_aggregate(ctx, ranks=ranks)
+        return float(self.loss_aggregator.secure_aggregate(ctx, ranks=ranks)[0])
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/__init__.py` & `pyfate-2.1.0/fate/components/components/nn/hook_code/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/__init__.py` & `pyfate-2.1.0/fate/components/components/nn/runner/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/common/__init__.py` & `pyfate-2.1.0/fate/components/components/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/common/predict.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/common/predict.py`

 * *Files 2% similar despite different names*

```diff
@@ -91,16 +91,17 @@
 def _merge_pos_arr(s: pd.Series):
     arr_1 = s["sample_pos"]
     arr_2 = s["host_sample_pos"]
     arr_1 = np.array(arr_1)
     arr_2 = np.array(arr_2)
     assert len(arr_1) == len(arr_2)
     merge_rs = np.copy(arr_1)
+    already_on_leaf = arr_1 < 0
     on_leaf = arr_2 < 0
-    updated = on_leaf | (arr_2 > arr_1)
+    updated = ~already_on_leaf & (on_leaf | (arr_2 > arr_1))
     merge_rs[updated] = arr_2[updated]
     return [merge_rs]
 
 
 def _merge_pos(guest_pos: DataFrame, host_pos: List[DataFrame]):
     for host_df in host_pos:
         # assert alignment
@@ -128,14 +129,15 @@
     comm_round = 0
 
     while True:
         sub_ctx = ctx.sub_ctx("predict_round").indexed_ctx(comm_round)
 
         if comm_round:
             predict_data = predict_data.loc(indexer=sample_pos.get_indexer(target="sample_id"), preserve_order=True)
+
         sample_with_pos = DataFrame.hstack([predict_data, sample_pos])
         logger.info("predict round {} has {} samples to predict".format(comm_round, len(sample_with_pos)))
         map_func = functools.partial(traverse_tree, trees=tree_list, sitename=sitename)
         new_pos = sample_with_pos.create_frame()
         new_pos["sample_pos"] = sample_with_pos.apply_row(map_func)
         done_sample_idx = new_pos.apply_row(
             lambda x: all_reach_leaf(x["sample_pos"])
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/__init__.py` & `pyfate-2.1.0/fate/components/core/spec/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/_base.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/_base.py`

 * *Files 27% similar despite different names*

```diff
@@ -23,36 +23,45 @@
 
 class HeteroBoostingTree(HeteroModule):
     def __init__(self) -> None:
         super().__init__()
         self._global_feature_importance = {}
         self._trees = []
         self._saved_tree = []
+        self._fid_name_mapping = {}
 
     def _update_feature_importance(self, fi_dict: Dict[int, FeatureImportance]):
         for fid, fi in fi_dict.items():
             if fid not in self._global_feature_importance:
                 self._global_feature_importance[fid] = fi
             else:
                 self._global_feature_importance[fid] = self._global_feature_importance[fid] + fi
 
-    def _sum_leaf_weights(self, leaf_pos: DataFrame, trees, learing_rate: float, loss_func):
-        def _compute_score(leaf_pos: np.array, trees: List[List[Node]], learning_rate: float):
-            score = 0
-            leaf_pos = leaf_pos["sample_pos"]
-            for node_idx, tree in zip(leaf_pos, trees):
+    def _sum_leaf_weights(self, leaf_pos: DataFrame, trees, learing_rate: float, num_dim=1):
+        def _compute_score(leaf_pos_: np.array, trees_: List[List[Node]], learning_rate: float, num_dim_=1):
+            score = np.zeros(num_dim_)
+            leaf_pos_ = leaf_pos_["sample_pos"]
+            tree_idx = 0
+            for node_idx, tree in zip(leaf_pos_, trees_):
                 recovered_idx = -(node_idx + 1)
-                score += tree[recovered_idx].weight * learning_rate
-            return score
+                score[tree_idx % num_dim_] += tree[recovered_idx].weight * learning_rate
+                tree_idx += 1
+
+            return float(score[0]) if num_dim_ == 1 else [score]
 
         tree_list = [tree.get_nodes() for tree in trees]
-        apply_func = functools.partial(_compute_score, trees=tree_list, learning_rate=learing_rate)
+        apply_func = functools.partial(_compute_score, trees_=tree_list, learning_rate=learing_rate, num_dim_=num_dim)
         predict_score = leaf_pos.create_frame()
         predict_score["score"] = leaf_pos.apply_row(apply_func)
-        return loss_func.predict(predict_score)
+        return predict_score
+
+    def _get_fid_name_mapping(self, data_instances: DataFrame):
+        columns = data_instances.schema.columns
+        for idx, col in enumerate(columns):
+            self._fid_name_mapping[idx] = col
 
     def get_trees(self):
         return self._trees
 
     def get_feature_importance(self):
         return self._global_feature_importance
 
@@ -63,15 +72,20 @@
             idx += 1
             tree.print_tree()
             print()
 
     def _get_hyper_param(self) -> dict:
         pass
 
+    def _load_feature_importance(self, feature_importance: dict):
+        self._global_feature_importance = {k: FeatureImportance.from_dict(v) for k, v in feature_importance.items()}
+
     def get_model(self) -> dict:
         import copy
 
         hyper_param = self._get_hyper_param()
         result = {}
         result["hyper_param"] = hyper_param
         result["trees"] = copy.deepcopy(self._saved_tree)
+        result["fid_name_mapping"] = self._fid_name_mapping
+        result["feature_importance"] = {k: v.to_dict() for k, v in self._global_feature_importance.items()}
         return result
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/guest.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/guest.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,106 +9,159 @@
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 import numpy as np
-from typing import Optional
+import pandas as pd
+from typing import Optional, Union
 from fate.arch import Context
 from fate.arch.dataframe import DataFrame
 from fate.ml.ensemble.algo.secureboost.hetero._base import HeteroBoostingTree
 from fate.ml.ensemble.learner.decision_tree.hetero.guest import HeteroDecisionTreeGuest
 from fate.ml.ensemble.utils.binning import binning
-from fate.ml.ensemble.learner.decision_tree.tree_core.loss import OBJECTIVE, get_task_info, MULTI_CE
+from fate.ml.ensemble.learner.decision_tree.tree_core.loss import (
+    OBJECTIVE,
+    get_task_info,
+    MULTI_CE,
+    BINARY_BCE,
+    REGRESSION_L2,
+)
 from fate.ml.ensemble.algo.secureboost.common.predict import predict_leaf_guest
-from fate.ml.utils.predict_tools import compute_predict_details, PREDICT_SCORE, LABEL, BINARY, MULTI, REGRESSION
+from fate.ml.utils.predict_tools import compute_predict_details, PREDICT_SCORE, BINARY, MULTI, REGRESSION
+from fate.ml.ensemble.learner.decision_tree.tree_core.decision_tree import GUEST_FEAT_ONLY, ALL_FEAT
+from fate.ml.ensemble.utils.sample import goss_sample
 import logging
 
 
 logger = logging.getLogger(__name__)
 
 
+def _compute_gh(data: DataFrame, scores: DataFrame, loss_func):
+    label = data.label
+    predict = loss_func.predict(scores)
+    gh = data.create_frame()
+    loss_func.compute_grad(gh, label, predict)
+    loss_func.compute_hess(gh, label, predict)
+    return gh
+
+
+def _get_loss_func(objective: str, class_num=None):
+    # to lowercase
+    objective = objective.lower()
+    assert objective in OBJECTIVE, f"objective {objective} not found, supported objective: {list(OBJECTIVE.keys())}"
+
+    obj_class = OBJECTIVE[objective]
+    if objective == MULTI_CE:
+        assert (
+            class_num is not None and class_num >= 3
+        ), "class_num should be set and greater than 2 for multi:ce objective, but got {}".format(class_num)
+        loss_func = obj_class(class_num=class_num)
+    else:
+        loss_func = obj_class()
+    return loss_func
+
+
+def _select_gh_by_tree_dim(gh: DataFrame, tree_idx: int):
+    def select_func(s: pd.Series, idx):
+        new_s = pd.Series()
+        new_s["g"] = s["g"][idx]
+        new_s["h"] = s["h"][idx]
+        return new_s
+
+    target_gh = gh.apply_row(lambda s: select_func(s, tree_idx), columns=["g", "h"])
+    return target_gh
+
+
+def _accumulate_scores(
+    acc_scores: DataFrame, new_scores: DataFrame, learning_rate: float, multi_class=False, class_num=None, dim=0
+):
+    def _extend_score(s: pd.Series, class_num, dim):
+        new_s = pd.Series()
+        new_s["score"] = np.zeros(class_num)
+        new_s["score"][dim] = s["score"]
+        return new_s
+
+    new_scores = new_scores.loc(acc_scores.get_indexer(target="sample_id"), preserve_order=True)
+    if not multi_class:
+        acc_scores = acc_scores + new_scores * learning_rate
+    else:
+        extend_scores = new_scores.apply_row(lambda s: _extend_score(s, class_num, dim), columns=["score"])
+        acc_scores = acc_scores + extend_scores * learning_rate
+    return acc_scores
+
+
 class HeteroSecureBoostGuest(HeteroBoostingTree):
     def __init__(
         self,
         num_trees=3,
-        learning_rate=0.3,
         max_depth=3,
+        complete_secure=0,
+        learning_rate=0.3,
         objective="binary:bce",
-        num_class=3,
+        num_class=1,
         max_bin=32,
         l2=0.1,
         l1=0,
         min_impurity_split=1e-2,
         min_sample_split=2,
         min_leaf_node=1,
         min_child_weight=1,
+        goss=False,
+        goss_start_iter=0,
+        top_rate=0.2,
+        other_rate=0.1,
         gh_pack=True,
         split_info_pack=True,
         hist_sub=True,
+        random_seed=42,
     ):
         super().__init__()
         self.num_trees = num_trees
         self.learning_rate = learning_rate
         self.max_depth = max_depth
         self.objective = objective
         self.max_bin = max_bin
+        self.goss = goss
+        self.goss_start_iter = goss_start_iter
+        self.top_rate = top_rate
+        self.other_rate = other_rate
+        self.random_seed = random_seed
 
         # regularization
         self.l2 = l2
         self.l1 = l1
         self.min_impurity_split = min_impurity_split
         self.min_sample_split = min_sample_split
         self.min_leaf_node = min_leaf_node
         self.min_child_weight = min_child_weight
 
         # running var
         self.num_class = num_class
         self._accumulate_scores = None
-        self._tree_dim = 1  # tree dimension, if is multilcass task, tree dim > 1
-        self._loss_func = None
+        self._tree_dim = None  # tree dimension, if is multilcass task, tree dim > 1
+        self._loss_func: Union[BINARY_BCE, MULTI_CE, REGRESSION_L2] = None
         self._train_predict = None
         self._hist_sub = hist_sub
+        self._complete_secure = complete_secure
 
         # encryption
         self._encrypt_kit = None
         self._gh_pack = gh_pack
         self._split_info_pack = split_info_pack
 
         # reg score
         self._init_score = None
 
         # model loaded
         self._model_loaded = False
 
-    def _prepare_parameter(self):
-        self._tree_dim = self.num_class if self.objective == "multiclass:ce" else 1
-
-    def _get_loss_func(self, objective: str) -> Optional[object]:
-        # to lowercase
-        objective = objective.lower()
-        if objective == MULTI_CE:
-            raise ValueError(
-                "multi:ce objective is not supported in the beta version, will be added in the next version"
-            )
-        assert (
-            objective in OBJECTIVE
-        ), f"objective {objective} not found, supported objective: {list(OBJECTIVE.keys())}"
-        obj_class = OBJECTIVE[objective]
-        loss_func = obj_class()
-        return loss_func
-
-    def _compute_gh(self, data: DataFrame, scores: DataFrame, loss_func):
-        label = data.label
-        predict = loss_func.predict(scores)
-        gh = data.create_frame()
-        loss_func.compute_grad(gh, label, predict)
-        loss_func.compute_hess(gh, label, predict)
-        return gh
+        # loss history
+        self._loss_history = []
 
     def _check_encrypt_kit(self, ctx: Context):
         if self._encrypt_kit is None:
             # make sure cipher is initialized
             kit = ctx.cipher.phe.setup()
             self._encrypt_kit = kit
 
@@ -131,61 +184,73 @@
         return self._trees[idx]
 
     def _init_sample_scores(self, ctx: Context, label, train_data: DataFrame):
         task_type = self.objective.split(":")[0]
         pred_ctx = ctx.sub_ctx("warmstart_predict")
         if self._model_loaded:
             logger.info("prepare warmstarting score")
-            self._accumulate_scores = self.predict(pred_ctx, train_data, ret_std_format=False)
+            self._accumulate_scores = self.predict(pred_ctx, train_data, ret_raw_scores=True)
             self._accumulate_scores = self._accumulate_scores.loc(
                 train_data.get_indexer(target="sample_id"), preserve_order=True
             )
         else:
             if task_type == REGRESSION:
                 self._accumulate_scores, avg_score = self._loss_func.initialize(label)
                 if self._init_score is None:
                     self._init_score = avg_score
-            elif task_type == MULTI:
-                self._accumulate_scores = self._loss_func.initialize(label, self.num_class)
             else:
                 self._accumulate_scores = self._loss_func.initialize(label)
 
     def _check_label(self, label: DataFrame):
-        label_df = label.as_pd_df()[label.schema.label_name]
-        if self.objective == "multi:ce":
-            if self.num_class is None or self.num_class <= 2:
-                raise ValueError(
-                    f"num_class should be set and greater than 2 for multi:ce objective, but got {self.num_class}"
-                )
-            label_set = set(np.unique(label_df))
-            if len(label_set) > self.num_class:
-                raise ValueError(
-                    f"num_class should be greater than or equal to the number of unique label in provided train data, but got {self.num_class} and {len(label_set)}"
-                )
-            if max(label_set) - 1 > self.num_class:
-                raise ValueError(
-                    f"the max label index in the provided train data should be less than or equal to num_class - 1, but got index {max(label_set)} which is > {self.num_class}"
-                )
-
-        elif self.objective == "binary:bce":
-            label_set = set(np.unique(label_df))
-            assert len(label_set) == 2, f"binary classification task should have 2 unique label, but got {label_set}"
-            assert (
-                0 in label_set and 1 in label_set
-            ), f"binary classification task should have label 0 and 1, but got {label_set}"
-            self.num_class = 2
+        if self.objective != REGRESSION_L2:
+            train_data_binarized_label = label.get_dummies()
+            labels = [int(label_name.split("_")[1]) for label_name in train_data_binarized_label.columns]
+            label_set = set(labels)
+            if self.objective == MULTI_CE:
+                if self.num_class is None or self.num_class <= 2:
+                    raise ValueError(
+                        f"num_class should be set and greater than 2 for multi:ce objective, but got {self.num_class}"
+                    )
+
+                if len(label_set) > self.num_class:
+                    raise ValueError(
+                        f"num_class should be greater than or equal to the number of unique label in provided train data, but got {self.num_class} and {len(label_set)}"
+                    )
+                if max(label_set) - 1 > self.num_class:
+                    raise ValueError(
+                        f"the max label index in the provided train data should be less than or equal to num_class - 1, but got index {max(label_set)} which is > {self.num_class}"
+                    )
+
+            elif self.objective == BINARY_BCE:
+                assert (
+                    len(label_set) == 2
+                ), f"binary classification task should have 2 unique label, but got {label_set}"
+                assert (
+                    0 in label_set and 1 in label_set
+                ), f"binary classification task should have label 0 and 1, but got {label_set}"
+                self.num_class = 2
         else:
             self.num_class = None
 
+    def _set_tree_dim(self, ctx: Context):
+        if not self._model_loaded:
+            self._tree_dim = self.num_class if self.objective == MULTI_CE else 1
+        assert self._tree_dim >= 1
+        ctx.hosts.put("tree_dim", self._tree_dim)
+
     def get_task_info(self):
         task_type = get_task_info(self.objective)
         if task_type == BINARY:
             classes = [0, 1]
         elif task_type == REGRESSION:
             classes = None
+        elif task_type == MULTI:
+            classes = [i for i in range(self.num_class)]
+        else:
+            raise RuntimeError(f"unknown task type {task_type}")
         return task_type, classes
 
     def fit(self, ctx: Context, train_data: DataFrame, validate_data: DataFrame = None) -> None:
         """
         Train model with train data and validate data.
 
         Parameters
@@ -198,88 +263,128 @@
             Validate data used to evaluate model performance during training process.
         """
 
         # data binning
         bin_info = binning(train_data, max_bin=self.max_bin)
         bin_data: DataFrame = train_data.bucketize(boundaries=bin_info)
         self._check_label(bin_data.label)
+        self._get_fid_name_mapping(train_data)
+
+        # tree dimension
+        self._set_tree_dim(ctx)
 
         # init loss func & scores
-        self._loss_func = self._get_loss_func(self.objective)
+        self._loss_func = _get_loss_func(self.objective, self.num_class)
         label = bin_data.label
         self._init_sample_scores(ctx, label, train_data)
 
         # init encryption kit
         self._encrypt_kit = self._check_encrypt_kit(ctx)
 
-        # start tree fittingf
-        for tree_idx, tree_ctx in ctx.on_iterations.ctxs_range(len(self._trees), len(self._trees) + self.num_trees):
+        # start tree fitting
+        for iter_dix, tree_ctx in ctx.on_iterations.ctxs_range(len(self._trees), len(self._trees) + self.num_trees):
             # compute gh of current iter
-            logger.info("start to fit a guest tree")
-            gh = self._compute_gh(bin_data, self._accumulate_scores, self._loss_func)
-            tree = HeteroDecisionTreeGuest(
-                max_depth=self.max_depth,
-                l2=self.l2,
-                l1=self.l1,
-                min_impurity_split=self.min_impurity_split,
-                min_sample_split=self.min_sample_split,
-                min_leaf_node=self.min_leaf_node,
-                min_child_weight=self.min_child_weight,
-                objective=self.objective,
-                gh_pack=self._gh_pack,
-                split_info_pack=self._split_info_pack,
-                hist_sub=self._hist_sub,
+            gh = _compute_gh(bin_data, self._accumulate_scores, self._loss_func)
+            tree_mode = ALL_FEAT
+            if iter_dix < self._complete_secure:
+                tree_mode = GUEST_FEAT_ONLY
+            for tree_dim, tree_ctx_ in tree_ctx.on_iterations.ctxs_range(self._tree_dim):
+                logger.info("start to fit a guest tree")
+                if self.objective == MULTI_CE:
+                    target_gh = _select_gh_by_tree_dim(gh, tree_dim)
+                else:
+                    target_gh = gh
+                tree = HeteroDecisionTreeGuest(
+                    max_depth=self.max_depth,
+                    l2=self.l2,
+                    l1=self.l1,
+                    min_impurity_split=self.min_impurity_split,
+                    min_sample_split=self.min_sample_split,
+                    min_leaf_node=self.min_leaf_node,
+                    min_child_weight=self.min_child_weight,
+                    objective=self.objective,
+                    gh_pack=self._gh_pack,
+                    split_info_pack=self._split_info_pack,
+                    hist_sub=self._hist_sub,
+                    tree_mode=tree_mode,
+                )
+                tree.set_encrypt_kit(self._encrypt_kit)
+
+                if self.goss:
+                    if iter_dix >= self.goss_start_iter:
+                        target_gh = goss_sample(target_gh, self.top_rate, self.other_rate, self.random_seed)
+                        logger.debug("goss sample done, got {} samples".format(len(target_gh)))
+
+                tree.booster_fit(tree_ctx_, bin_data, target_gh, bin_info)
+                # accumulate scores of cur boosting round
+                scores = tree.get_sample_predict_weights()
+                assert len(scores) == len(
+                    self._accumulate_scores
+                ), f"tree predict scores length {len(scores)} not equal to accumulate scores length {len(self._accumulate_scores)}."
+                self._accumulate_scores = _accumulate_scores(
+                    self._accumulate_scores,
+                    scores,
+                    self.learning_rate,
+                    self.objective == MULTI_CE,
+                    class_num=self.num_class,
+                    dim=tree_dim,
+                )
+                self._trees.append(tree)
+                self._saved_tree.append(tree.get_model())
+                self._update_feature_importance(tree.get_feature_importance())
+                logger.info("fitting guest decision tree iter {}, dim {} done".format(iter_dix, tree_dim))
+
+            # compute loss
+            iter_loss = self._loss_func.compute_loss(
+                train_data.label, self._loss_func.predict(self._accumulate_scores)
             )
-            tree.set_encrypt_kit(self._encrypt_kit)
-            tree.booster_fit(tree_ctx, bin_data, gh, bin_info)
-            # accumulate scores of cur boosting round
-            scores = tree.get_sample_predict_weights()
-            assert len(scores) == len(
-                self._accumulate_scores
-            ), f"tree predict scores length {len(scores)} not equal to accumulate scores length {len(self._accumulate_scores)}."
-            scores = scores.loc(self._accumulate_scores.get_indexer(target="sample_id"), preserve_order=True)
-            self._accumulate_scores = self._accumulate_scores + scores * self.learning_rate
-            self._trees.append(tree)
-            self._saved_tree.append(tree.get_model())
-            self._update_feature_importance(tree.get_feature_importance())
-            logger.info("fitting guest decision tree {} done".format(tree_idx))
+            iter_loss = float(iter_loss.iloc[0])
+            self._loss_history.append(iter_loss)
+            tree_ctx.metrics.log_loss("sbt_loss", iter_loss)
 
         # compute train predict using cache scores
         train_predict: DataFrame = self._loss_func.predict(self._accumulate_scores)
         train_predict = train_predict.loc(train_data.get_indexer(target="sample_id"), preserve_order=True)
         train_predict.label = train_data.label
         task_type, classes = self.get_task_info()
         train_predict.rename(columns={"score": PREDICT_SCORE})
         self._train_predict = compute_predict_details(train_predict, task_type, classes)
 
-    def predict(self, ctx: Context, predict_data: DataFrame, predict_leaf=False, ret_std_format=True) -> DataFrame:
+    def predict(
+        self, ctx: Context, predict_data: DataFrame, predict_leaf=False, ret_std_format=True, ret_raw_scores=False
+    ) -> DataFrame:
         """
         predict function
 
         Parameters
         ----------
         ctx: Context
             FATE Context object
         predict_data: DataFrame
             Data used to predict.
         predict_leaf: bool, optional
             Whether to predict and return leaf index.
         ret_std_format: bool, optional
             Whether to return result in a FATE standard format which contains more details.
+        ret_raw_scores:
+            Whether to return raw scores.
         """
 
         task_type, classes = self.get_task_info()
         leaf_pos = predict_leaf_guest(ctx, self._trees, predict_data)
         if predict_leaf:
             return leaf_pos
-        result = self._sum_leaf_weights(leaf_pos, self._trees, self.learning_rate, self._loss_func)
-
+        raw_scores = self._sum_leaf_weights(leaf_pos, self._trees, self.learning_rate, num_dim=self._tree_dim)
         if task_type == REGRESSION:
             logger.debug("regression task, add init score")
-            result = result + self._init_score
+            raw_scores = self._init_score + raw_scores
+
+        if ret_raw_scores:
+            return raw_scores
+        result = self._loss_func.predict(raw_scores)
 
         if ret_std_format:
             # align table
             result: DataFrame = result.loc(predict_data.get_indexer(target="sample_id"), preserve_order=True)
             ret_frame = result.create_frame()
             if predict_data.schema.label_name is not None:
                 ret_frame.label = predict_data.label
@@ -294,32 +399,38 @@
             "num_trees": self.num_trees,
             "learning_rate": self.learning_rate,
             "max_depth": self.max_depth,
             "objective": self.objective,
             "max_bin": self.max_bin,
             "l2": self.l2,
             "num_class": self.num_class,
+            "complete_secure": self._complete_secure,
         }
 
     def get_model(self) -> dict:
         ret_dict = super().get_model()
         ret_dict["init_score"] = self._init_score
+        ret_dict["loss_history"] = self._loss_history
         return ret_dict
 
     def from_model(self, model: dict):
         trees = model["trees"]
         self._saved_tree = trees
         self._trees = [HeteroDecisionTreeGuest.from_model(tree) for tree in trees]
         hyper_parameter = model["hyper_param"]
 
-        # these parameter are related to predict
+        # these parameters are related to predict
         self.learning_rate = hyper_parameter["learning_rate"]
         self.num_class = hyper_parameter["num_class"]
         self.objective = hyper_parameter["objective"]
         self._init_score = float(model["init_score"]) if model["init_score"] is not None else None
         # initialize
-        self._prepare_parameter()
-        self._loss_func = self._get_loss_func(self.objective)
+        self._tree_dim = self.num_class if self.objective == MULTI_CE else 1
+        self._loss_func = _get_loss_func(self.objective, class_num=self.num_class)
         # for warmstart
         self._model_loaded = True
+        # load loss
+        self._loss_history.extend(model["loss_history"])
+        # load feature importances
+        self._load_feature_importance(model["feature_importance"])
 
         return self
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/algo/secureboost/hetero/host.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/host.py`

 * *Files 25% similar despite different names*

```diff
@@ -16,28 +16,30 @@
 import os
 from fate.ml.ensemble.learner.decision_tree.hetero.host import HeteroDecisionTreeHost
 from fate.ml.ensemble.algo.secureboost.hetero._base import HeteroBoostingTree
 from fate.arch import Context
 from fate.arch.dataframe import DataFrame
 from fate.ml.ensemble.utils.binning import binning
 from fate.ml.ensemble.algo.secureboost.common.predict import predict_leaf_host
+from fate.ml.ensemble.learner.decision_tree.tree_core.decision_tree import GUEST_FEAT_ONLY, ALL_FEAT
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 class HeteroSecureBoostHost(HeteroBoostingTree):
-    def __init__(self, num_trees=3, learning_rate=0.3, max_depth=3, max_bin=32, hist_sub=True) -> None:
+    def __init__(self, num_trees=3, max_depth=3, complete_secure=0, max_bin=32, hist_sub=True) -> None:
         super().__init__()
         self.num_trees = num_trees
-        self.learning_rate = learning_rate
         self.max_depth = max_depth
         self.max_bin = max_bin
         self._model_loaded = False
         self._hist_sub = hist_sub
+        self._tree_dim = None
+        self._complete_secure = complete_secure
 
     def get_tree(self, idx):
         return self._trees[idx]
 
     def _get_seeds(self, ctx: Context):
         if ctx.cipher.allow_custom_random_seed:
             seed = ctx.cipher.get_custom_random_seed()
@@ -46,50 +48,66 @@
             while True:
                 yield random_state.getrandbits(64)
         else:
             while True:
                 random_seed = os.urandom(8)
                 yield int.from_bytes(random_seed, byteorder="big")
 
+    def _set_tree_dim(self, ctx: Context):
+        self._tree_dim = ctx.guest.get("tree_dim")
+        logger.info("tree dimension is {}".format(self._tree_dim))
+
     def fit(self, ctx: Context, train_data: DataFrame, validate_data: DataFrame = None) -> None:
         # data binning
         bin_info = binning(train_data, max_bin=self.max_bin)
         bin_data: DataFrame = train_data.bucketize(boundaries=bin_info)
+        self._get_fid_name_mapping(train_data)
         logger.info("data binning done")
+
+        # tree dimension
+        self._set_tree_dim(ctx)
+
         # predict to help guest to get the warmstart scores
         if self._model_loaded:
             pred_ctx = ctx.sub_ctx("warmstart_predict")
             self.predict(pred_ctx, train_data)
 
         random_seeds = self._get_seeds(ctx)
         global_random_seed = next(random_seeds)
-        for tree_idx, tree_ctx in ctx.on_iterations.ctxs_range(len(self._trees), len(self._trees) + self.num_trees):
-            logger.info("start to fit a host tree")
-            tree = HeteroDecisionTreeHost(
-                max_depth=self.max_depth,
-                hist_sub=self._hist_sub,
-                global_random_seed=global_random_seed,
-                random_seed=next(random_seeds),
-            )
-            tree.booster_fit(tree_ctx, bin_data, bin_info)
-            self._trees.append(tree)
-            self._saved_tree.append(tree.get_model())
-            self._update_feature_importance(tree.get_feature_importance())
-            logger.info("fitting host decision tree {} done".format(tree_idx))
+        for iter_idx, tree_ctx in ctx.on_iterations.ctxs_range(len(self._trees), len(self._trees) + self.num_trees):
+            tree_mode = ALL_FEAT
+            if iter_idx < self._complete_secure:
+                tree_mode = GUEST_FEAT_ONLY
+            for tree_dim, tree_ctx_ in tree_ctx.on_iterations.ctxs_range(self._tree_dim):
+                tree = HeteroDecisionTreeHost(
+                    max_depth=self.max_depth,
+                    hist_sub=self._hist_sub,
+                    global_random_seed=global_random_seed,
+                    random_seed=next(random_seeds),
+                    tree_mode=tree_mode,
+                )
+                tree.booster_fit(tree_ctx_, bin_data, bin_info)
+                self._trees.append(tree)
+                self._saved_tree.append(tree.get_model())
+                self._update_feature_importance(tree.get_feature_importance())
+                logger.info("fitting host decision tree {}, dim {} done".format(iter_idx, tree_dim))
 
     def predict(self, ctx: Context, predict_data: DataFrame) -> None:
         predict_leaf_host(ctx, self._trees, predict_data)
 
     def _get_hyper_param(self) -> dict:
         return {
             "num_trees": self.num_trees,
-            "learning_rate": self.learning_rate,
             "max_depth": self.max_depth,
             "max_bin": self.max_bin,
+            "complete_secure": self._complete_secure,
         }
 
     def from_model(self, model: dict):
         trees = model["trees"]
         self._saved_tree = trees
         self._trees = [HeteroDecisionTreeHost.from_model(tree) for tree in trees]
         self._model_loaded = True
+        # load feature importances
+        self._load_feature_importance(model["feature_importance"])
+
         return self
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/__init__.py` & `pyfate-2.1.0/fate/components/entrypoint/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/__init__.py` & `pyfate-2.1.0/fate/components/entrypoint/cli/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/guest.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/guest.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,25 +15,25 @@
 from fate.ml.ensemble.learner.decision_tree.tree_core.decision_tree import (
     DecisionTree,
     Node,
     _update_sample_pos_on_local_nodes,
     _merge_sample_pos,
 )
 from fate.ml.ensemble.learner.decision_tree.tree_core.hist import SBTHistogramBuilder
-from fate.ml.ensemble.learner.decision_tree.tree_core.splitter import FedSBTSplitter
+from fate.ml.ensemble.learner.decision_tree.tree_core.splitter import SBTSplitter
 from fate.ml.ensemble.learner.decision_tree.tree_core.loss import get_task_info
+from fate.ml.ensemble.learner.decision_tree.tree_core.decision_tree import ALL_FEAT, GUEST_FEAT_ONLY
 from fate.ml.utils.predict_tools import BINARY, MULTI, REGRESSION
 from fate.arch import Context
 from fate.arch.dataframe import DataFrame
 from typing import List
 import functools
 import logging
 import pandas as pd
 import torch as t
-import numpy as np
 import math
 
 
 logger = logging.getLogger(__name__)
 
 FIX_POINT_PRECISION = 52
 
@@ -52,23 +52,33 @@
         min_sample_split=2,
         min_leaf_node=1,
         min_child_weight=1,
         objective=None,
         gh_pack=True,
         split_info_pack=True,
         hist_sub=True,
+        tree_mode=ALL_FEAT,
     ):
         super().__init__(
             max_depth, use_missing=use_missing, zero_as_missing=zero_as_missing, valid_features=valid_features
         )
         self.host_sitenames = None
         self._tree_node_num = 0
         self.hist_builder = None
         self.splitter = None
 
+        # feature control
+        self._tree_mode = tree_mode
+        assert self._tree_mode in [ALL_FEAT, GUEST_FEAT_ONLY], "tree mode {} not supported".format(self._tree_mode)
+        # is local tree
+        if self._tree_mode == GUEST_FEAT_ONLY:
+            self._is_local_tree = True
+        elif self._tree_mode == ALL_FEAT:
+            self._is_local_tree = False
+
         # regularization
         self.l1 = l1
         self.l2 = l2
         self.min_impurity_split = min_impurity_split
         self.min_sample_split = min_sample_split
         self.min_leaf_node = min_leaf_node
         self.min_child_weight = min_child_weight
@@ -98,14 +108,17 @@
         self._h_abs_max = 0
         self._objective = objective
         if gh_pack:
             if objective is None:
                 raise ValueError("objective must be specified when gh_pack is True")
         self._pack_info = {}
 
+        # param checking
+        assert l1 >= 0 and l2 >= 0, "l1 and l2 should be non-negative, got l1: {}, l2: {}".format(l1, l2)
+
     def set_encrypt_kit(self, kit):
         self._encrypt_kit = kit
         self._en_key_length = kit.key_size
         self._sk, self._pk, self._coder, self._evaluator, self._encryptor = (
             kit.sk,
             kit.pk,
             kit.coder,
@@ -136,22 +149,33 @@
             bin_len[column] = bin_num
 
         max_max_value = max(bin_len.values())
 
         return bin_len, max_max_value
 
     def _update_sample_pos(
-        self, ctx: Context, cur_layer_nodes: List[Node], sample_pos: DataFrame, data: DataFrame, node_map: dict
+        self,
+        ctx: Context,
+        cur_layer_nodes: List[Node],
+        sample_pos: DataFrame,
+        data: DataFrame,
+        node_map: dict,
+        local_update=False,
     ):
         sitename = ctx.local.name
         data_with_pos = DataFrame.hstack([data, sample_pos])
         map_func = functools.partial(
             _update_sample_pos_on_local_nodes, cur_layer_node=cur_layer_nodes, node_map=node_map, sitename=sitename
         )
-        updated_sample_pos = data_with_pos.apply_row(map_func, columns=["g_on_local", "g_node_idx"])
+
+        if local_update:
+            updated_sample_pos = data_with_pos.apply_row(map_func, columns=["g_on_local", "node_idx"])
+            return updated_sample_pos["node_idx"]
+        else:
+            updated_sample_pos = data_with_pos.apply_row(map_func, columns=["g_on_local", "g_node_idx"])
 
         # synchronize sample pos
         host_update_sample_pos = ctx.hosts.get("updated_data")
 
         merge_func = functools.partial(_merge_sample_pos)
         for host_data in host_update_sample_pos:
             updated_sample_pos = DataFrame.hstack([updated_sample_pos, host_data]).apply_row(
@@ -221,15 +245,14 @@
             en_grad_hess["g"] = self._encryptor.encrypt_tensor(grad_and_hess["g"].as_tensor())
             en_grad_hess["h"] = self._encryptor.encrypt_tensor(grad_and_hess["h"].as_tensor())
             logger.info("not using gh pack")
 
         return en_grad_hess
 
     def _send_gh(self, ctx: Context, grad_and_hess: DataFrame):
-        # encrypt g & h
         en_grad_hess = self._g_h_process(grad_and_hess)
         ctx.hosts.put("en_gh", en_grad_hess)
         ctx.hosts.put("en_kit", [self._pk, self._evaluator])
 
     def _mask_node(self, ctx: Context, nodes: List[Node]):
         new_nodes = []
         for n in nodes:
@@ -270,95 +293,99 @@
 
     def _sync_nodes(self, ctx: Context, cur_layer_nodes: List[Node], next_layer_nodes: List[Node]):
         mask_cur_layer = self._mask_node(ctx, cur_layer_nodes)
         mask_next_layer = self._mask_node(ctx, next_layer_nodes)
         ctx.hosts.put("sync_nodes", [mask_cur_layer, mask_next_layer])
 
     def booster_fit(self, ctx: Context, bin_train_data: DataFrame, grad_and_hess: DataFrame, binning_dict: dict):
-        logger.info
         # Initialization
         train_df = bin_train_data
         sample_pos = self._init_sample_pos(train_df)
         self._sample_on_leaves = sample_pos.empty_frame()
         root_node = self._initialize_root_node(ctx, train_df, grad_and_hess)
 
-        # initialize homographic encryption
-        if self._encrypt_kit is None:
-            self._init_encrypt_kit(ctx)
-        # Send Encrypted Grad and Hess
-        self._send_gh(ctx, grad_and_hess)
-
-        # send pack info
-        send_pack_info = (
-            {
-                "total_pack_num": self._pack_info["total_pack_num"],
-                "split_point_shift_bit": self._pack_info["split_point_shift_bit"],
-                "split_info_pack": self._split_info_pack,
-            }
-            if self._gh_pack
-            else {}
-        )
-        ctx.hosts.put("pack_info", send_pack_info)
+        # federated tree
+        if not self._is_local_tree:
+            # initialize homographic encryption
+            if self._encrypt_kit is None:
+                self._init_encrypt_kit(ctx)
+            # Send Encrypted Grad and Hess
+            self._send_gh(ctx, grad_and_hess)
+
+            # send pack info
+            send_pack_info = (
+                {
+                    "total_pack_num": self._pack_info["total_pack_num"],
+                    "split_point_shift_bit": self._pack_info["split_point_shift_bit"],
+                    "split_info_pack": self._split_info_pack,
+                }
+                if self._gh_pack
+                else {}
+            )
+            ctx.hosts.put("pack_info", send_pack_info)
 
         # init histogram builder
         self.hist_builder = SBTHistogramBuilder(bin_train_data, binning_dict, None, None, hist_sub=self._hist_sub)
 
         # init splitter
-        self.splitter = FedSBTSplitter(
+        self.splitter = SBTSplitter(
             bin_train_data,
             binning_dict,
             l2=self.l2,
             l1=self.l1,
             min_sample_split=self.min_sample_split,
             min_impurity_split=self.min_impurity_split,
             min_child_weight=self.min_child_weight,
             min_leaf_node=self.min_leaf_node,
         )
 
         # Prepare for training
-        node_map = {}
         cur_layer_node = [root_node]
         grad_and_hess["cnt"] = 1
 
         for cur_depth, sub_ctx in ctx.on_iterations.ctxs_range(self.max_depth):
             if len(cur_layer_node) == 0:
                 logger.info("no nodes to split, stop training")
                 break
 
             assert len(sample_pos) == len(train_df), "sample pos len not match train data len, {} vs {}".format(
                 len(sample_pos), len(train_df)
             )
-
             # debug checking code
             # self._check_assign_result(sample_pos, cur_layer_node)
             # initialize node map
             node_map = {n.nid: idx for idx, n in enumerate(cur_layer_node)}
             # compute histogram
             hist_inst, statistic_result = self.hist_builder.compute_hist(
                 sub_ctx, cur_layer_node, train_df, grad_and_hess, sample_pos, node_map
             )
             # compute best splits
+            use_local_feat_only = True if self._tree_mode == GUEST_FEAT_ONLY else False
             split_info = self.splitter.split(
                 sub_ctx,
                 statistic_result,
                 cur_layer_node,
                 node_map,
-                self._sk,
-                self._coder,
-                self._gh_pack,
-                self._pack_info,
+                local_split=use_local_feat_only,
+                sk=self._sk,
+                coder=self._coder,
+                gh_pack=self._gh_pack,
+                pack_info=self._pack_info,
             )
             # update tree with best splits
             next_layer_nodes = self._update_tree(sub_ctx, cur_layer_node, split_info, train_df)
             # update feature importance
             self._update_feature_importance(sub_ctx, split_info, train_df)
             # sync nodes
-            self._sync_nodes(sub_ctx, cur_layer_node, next_layer_nodes)
+            if not self._is_local_tree:
+                self._sync_nodes(sub_ctx, cur_layer_node, next_layer_nodes)
             # update sample positions
-            sample_pos = self._update_sample_pos(sub_ctx, cur_layer_node, sample_pos, train_df, node_map)
+            sample_pos = self._update_sample_pos(
+                sub_ctx, cur_layer_node, sample_pos, train_df, node_map, local_update=self._is_local_tree
+            )
             # if sample reaches leaf nodes, drop them
             sample_on_leaves = self._get_samples_on_leaves(sample_pos)
             train_df, sample_pos, grad_and_hess = self._drop_samples_on_leaves(sample_pos, train_df, grad_and_hess)
             self._sample_on_leaves = DataFrame.vstack([self._sample_on_leaves, sample_on_leaves])
             # next layer nodes
             cur_layer_node = next_layer_nodes
             logger.info(
@@ -381,14 +408,18 @@
             len(self._sample_on_leaves), len(bin_train_data)
         )
         # convert sample pos to weights
         self._sample_weights = self._convert_sample_pos_to_weight(self._sample_on_leaves, self._nodes)
         # convert bid to split value
         self._nodes = self._convert_bin_idx_to_split_val(ctx, self._nodes, binning_dict, bin_train_data.schema)
 
+        # if is local tree, send mask nodes to host
+        if self._is_local_tree:
+            self._sync_nodes(ctx, self._nodes, [])
+
     def get_hyper_param(self):
         param = {
             "max_depth": self.max_depth,
             "valid_features": self._valid_features,
             "l1": self.l1,
             "l2": self.l2,
             "use_missing": self.use_missing,
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/hetero/host.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/host.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,15 +15,16 @@
 from fate.ml.ensemble.learner.decision_tree.tree_core.decision_tree import (
     DecisionTree,
     Node,
     _update_sample_pos_on_local_nodes,
     FeatureImportance,
 )
 from fate.ml.ensemble.learner.decision_tree.tree_core.hist import SBTHistogramBuilder, DistributedHistogram
-from fate.ml.ensemble.learner.decision_tree.tree_core.splitter import FedSBTSplitter
+from fate.ml.ensemble.learner.decision_tree.tree_core.splitter import SBTSplitter
+from fate.ml.ensemble.learner.decision_tree.tree_core.decision_tree import ALL_FEAT, GUEST_FEAT_ONLY
 from fate.arch import Context
 from fate.arch.dataframe import DataFrame
 from typing import List
 import functools
 import logging
 
 
@@ -36,14 +37,15 @@
         max_depth=3,
         valid_features=None,
         use_missing=False,
         zero_as_missing=False,
         random_seed=None,
         global_random_seed=None,
         hist_sub=True,
+        tree_mode=ALL_FEAT,
     ):
         super().__init__(
             max_depth, use_missing=use_missing, zero_as_missing=zero_as_missing, valid_features=valid_features
         )
         self._tree_node_num = 0
         self.hist_builder = None
         self.splitter = None
@@ -52,22 +54,26 @@
         self._global_random_seed = global_random_seed
         self._pk = None
         self._evaluator = None
         self._gh_pack = True
         self._pack_info = None
         self._hist_sub = hist_sub
 
+        # feature control
+        self._tree_mode = tree_mode
+        assert self._tree_mode in [ALL_FEAT, GUEST_FEAT_ONLY], "tree mode {} not supported".format(self._tree_mode)
+
     def _convert_split_id(
         self,
         ctx: Context,
         cur_layer_nodes: List[Node],
         node_map: dict,
         hist_builder: SBTHistogramBuilder,
         statistic_histogram: DistributedHistogram,
-        splitter: FedSBTSplitter,
+        splitter: SBTSplitter,
         data: DataFrame,
     ):
         sitename = ctx.local.party[0] + "_" + ctx.local.party[1]
         to_recover = {}
         for idx, n in enumerate(cur_layer_nodes):
             if (not n.is_leaf) and n.sitename == sitename:
                 node_id = n.nid
@@ -126,14 +132,20 @@
 
     def _sync_nodes(self, ctx: Context):
         nodes = ctx.guest.get("sync_nodes")
         cur_layer_nodes, next_layer_nodes = nodes
         return cur_layer_nodes, next_layer_nodes
 
     def booster_fit(self, ctx: Context, bin_train_data: DataFrame, binning_dict: dict):
+        if self._tree_mode == GUEST_FEAT_ONLY:
+            logger.info("this tree is a guest feat only tree, skip computation")
+            masked_tree, _ = self._sync_nodes(ctx)
+            self._nodes = masked_tree
+            return
+
         train_df = bin_train_data
         feat_max_bin, max_bin = self._get_column_max_bin(binning_dict)
         sample_pos = self._init_sample_pos(train_df)
 
         # Get Encrypted Grad And Hess
         ret = self._get_gh(ctx)
         en_grad_and_hess: DataFrame = ret[0]
@@ -148,15 +160,15 @@
             bin_train_data,
             binning_dict,
             random_seed=self._random_seed,
             global_random_seed=self._global_random_seed,
             hist_sub=self._hist_sub,
         )
         # splitter
-        self.splitter = FedSBTSplitter(bin_train_data, binning_dict)
+        self.splitter = SBTSplitter(bin_train_data, binning_dict)
 
         node_map = {}
         cur_layer_node = [root_node]
         en_grad_and_hess["cnt"] = 1
         for cur_depth, sub_ctx in ctx.on_iterations.ctxs_range(self.max_depth):
             if len(cur_layer_node) == 0:
                 logger.info("no nodes to split, stop training")
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/__init__.py` & `pyfate-2.1.0/fate/components/entrypoint/cli/component/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/decision_tree.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/decision_tree.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,18 +21,25 @@
 import pandas as pd
 from fate.arch import Context
 from fate.arch.dataframe import DataFrame
 from fate.ml.ensemble.learner.decision_tree.tree_core.splitter import SplitInfo
 from typing import List
 import logging
 
-
+# PRECISION
 FLOAT_ZERO = 1e-8
+
+# LEAF IDX
 LEAF_IDX = -1
 
+# TREE MODES
+GUEST_FEAT_ONLY = "guest"
+ALL_FEAT = "all"
+# HOST_FEAT_ONLY = 'host'
+
 
 logger = logging.getLogger(__name__)
 
 
 class FeatureImportance(object):
     def __init__(self, gain=0):
         self.gain = gain
@@ -100,15 +107,15 @@
 
     def __init__(
         self,
         nid=None,
         sitename=None,
         fid=None,
         bid=None,
-        weight=0,
+        weight=0.0,
         is_leaf=False,
         grad=None,
         hess=None,
         l=-1,
         r=-1,
         missing_dir=1,
         sample_num=0,
@@ -256,14 +263,15 @@
         self._predict_weights = None
         self._g_tensor, self._h_tensor = None, None
         self._sample_pos = None
         self._leaf_node_map = {}
         self._valid_feature = valid_features
         self._sample_on_leaves = None
         self._sample_weights = None
+        self._splitter = None
 
     def _init_sample_pos(self, train_data: DataFrame):
         sample_pos = train_data.create_frame()
         sample_pos["node_idx"] = 0  # position of current sample
         return sample_pos
 
     def _init_leaves_sample_table(self, sample_pos: DataFrame):
@@ -320,15 +328,17 @@
                     )
 
     def _fid_to_feature_name(self, fid: int, dataframe: DataFrame):
         if fid is None:
             return None
         return dataframe.schema.columns[fid]
 
-    def _update_tree(self, ctx: Context, cur_layer_nodes: List[Node], split_info: List[SplitInfo], data: DataFrame):
+    def _update_tree(
+        self, ctx: Context, cur_layer_nodes: List[Node], split_info: List[SplitInfo], data: DataFrame
+    ) -> object:
         assert len(cur_layer_nodes) == len(
             split_info
         ), "node num not match split info num, got {} node vs {} split info".format(
             len(cur_layer_nodes), len(split_info)
         )
 
         next_layer_node = []
@@ -336,15 +346,14 @@
         for idx in range(len(split_info)):
             node: Node = cur_layer_nodes[idx]
 
             if split_info[idx] is None:
                 node.is_leaf = True
                 node.sitename = ctx.guest.name  # leaf always belongs to guest
                 self._nodes.append(node)
-                logger.info("set node {} to leaf".format(node))
                 continue
 
             sum_grad = node.grad
             sum_hess = node.hess
             sum_cnt = node.sample_num
 
             feat_name = self._fid_to_feature_name(split_info[idx].best_fid, data)
@@ -372,15 +381,15 @@
                 weight=float(self.splitter.node_weight(l_g, l_h)),
                 parent_nodeid=p_id,
                 sibling_nodeid=r_id,
                 is_left_node=True,
                 sample_num=l_cnt,
             )
 
-            # not gonna happen
+            # this is not going to happen
             assert sum_cnt > l_cnt, "sum cnt {} not greater than l cnt {}".format(sum_cnt, l_cnt)
 
             r_g = float(sum_grad - l_g)
             r_h = float(sum_hess - l_h)
             r_cnt = sum_cnt - l_cnt
 
             right_node = Node(
@@ -401,29 +410,34 @@
         return next_layer_node
 
     def _drop_samples_on_leaves(self, new_sample_pos: DataFrame, data: DataFrame, grad_and_hess: DataFrame):
         assert len(new_sample_pos) == len(
             data
         ), "sample pos num not match data num, got {} sample pos vs {} data".format(len(new_sample_pos), len(data))
         x = new_sample_pos >= 0
-        pack_data = DataFrame.hstack([data, new_sample_pos, grad_and_hess]).iloc(x)
-        new_data = pack_data.create_frame(columns=data.schema.columns)
-        update_pos = pack_data.create_frame(columns=new_sample_pos.schema.columns)
-        grad_and_hess = pack_data.create_frame(columns=grad_and_hess.schema.columns)
-        """
-        new_data = data.iloc(x)
-        update_pos = new_sample_pos.iloc(x)
-        grad_and_hess = grad_and_hess.iloc(x)
-        """
-        logger.info(
-            "drop leaf samples, new sample count is {}, {} samples dropped".format(
-                len(new_sample_pos), len(data) - len(new_data)
+        if len(new_sample_pos) == len(grad_and_hess):
+            pack_data = DataFrame.hstack([data, new_sample_pos, grad_and_hess]).iloc(x)
+            new_data = pack_data.create_frame(columns=data.schema.columns)
+            update_pos = pack_data.create_frame(columns=new_sample_pos.schema.columns)
+            grad_and_hess = pack_data.create_frame(columns=grad_and_hess.schema.columns)
+            logger.info(
+                "drop leaf samples, new sample count is {}, {} samples dropped".format(
+                    len(new_sample_pos), len(data) - len(new_data)
+                )
             )
-        )
-        return new_data, update_pos, grad_and_hess
+            return new_data, update_pos, grad_and_hess
+        else:
+            # Goss
+            pack_data = DataFrame.hstack([data, new_sample_pos]).iloc(x)
+            new_data = pack_data.create_frame(columns=data.schema.columns)
+            update_pos = pack_data.create_frame(columns=new_sample_pos.schema.columns)
+            x = x.loc(grad_and_hess.get_indexer(target="sample_id"), preserve_order=True)
+            grad_and_hess = grad_and_hess.iloc(x)
+
+            return new_data, update_pos, grad_and_hess
 
     def _get_samples_on_leaves(self, sample_pos: DataFrame):
         x = sample_pos < 0
         samples_on_leaves = sample_pos.iloc(x)
         return samples_on_leaves
 
     def _get_column_max_bin(self, result_dict):
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/hist.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/hist.py`

 * *Files 4% similar despite different names*

```diff
@@ -200,15 +200,22 @@
             feature_bin_sizes=self.feat_bin_num,
             value_schemas=schema,
             global_seed=self.global_random_seed,
             seed=self.random_seed,
             node_mapping=node_mapping,
         )
 
-        map_sample_pos = sample_pos.apply_row(lambda x: node_map[x["node_idx"]])
+        # if goss is enabled
+        if len(sample_pos) > len(gh):
+            sample_pos = sample_pos.loc(gh.get_indexer(target="sample_id"), preserve_order=True)
+            map_sample_pos = sample_pos.apply_row(lambda x: node_map[x["node_idx"]])
+            bin_train_data = bin_train_data.loc(gh.get_indexer(target="sample_id"), preserve_order=True)
+        else:
+            map_sample_pos = sample_pos.apply_row(lambda x: node_map[x["node_idx"]])
+
         stat_obj = bin_train_data.distributed_hist_stat(hist, map_sample_pos, gh)
 
         if need_hist_sub_process:
             stat_obj = self._cache_parent_hist.compute_child(stat_obj, mapping)
 
         if self._hist_sub:
             self._cache_parent_hist = stat_obj
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/loss.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/loss.py`

 * *Files 12% similar despite different names*

```diff
@@ -25,15 +25,19 @@
 REGRESSION_L2 = "regression:l2"
 
 
 def apply_weight(loss: DataFrame, weight: DataFrame):
     return loss["loss"] * weight["weight"]
 
 
-class BCELoss(object):
+class Loss(object):
+    pass
+
+
+class BCELoss(Loss):
     @staticmethod
     def initialize(label: DataFrame):
         init_score = label.create_frame()
         init_score["score"] = 0.0
         return init_score
 
     @staticmethod
@@ -58,54 +62,67 @@
         gh["g"] = predict_score - label
 
     @staticmethod
     def compute_hess(gh: DataFrame, label: DataFrame, predict_score: DataFrame):
         gh["h"] = predict_score * (1 - predict_score)
 
 
-class CELoss(object):
-    @staticmethod
-    def initialize(label, class_num=3):
+class CELoss(Loss):
+    def __init__(self, class_num) -> None:
+        super().__init__()
+        self.class_num = class_num
+
+    def initialize(self, label):
         init_score = label.create_frame()
-        init_score["score"] = [0.0 for i in range(class_num)]
+        init_score["score"] = [0.0 for i in range(self.class_num)]
         return init_score
 
     @staticmethod
     def predict(score: DataFrame):
         def softmax(s):
             s = np.array(s["score"]).astype(np.float64)
             ret = (np.exp(s) / np.exp(s).sum()).tolist()
             return [ret]
 
         pred_rs = score.create_frame()
         pred_rs["score"] = score.apply_row(lambda s: softmax(s))
         return pred_rs
 
     @staticmethod
-    def compute_loss(label: DataFrame, pred: DataFrame, weight: DataFrame):
+    def compute_loss(label: DataFrame, pred: DataFrame):
         loss_col = label.create_frame()
-        label_pred = label.hstack(pred)
+        label_pred = DataFrame.hstack([label, pred])
         sample_num = len(label)
-        loss_col["loss"] = label_pred.apply_row(lambda s: np.log(s[1:][int(s[0])]))
+        loss_col["loss"] = label_pred.apply_row(lambda s: -np.log(s[1][int(s[0])]), with_label=True)
         loss_col["loss"].fillna(1)
-        if weight:
-            loss_col["loss"] = apply_weight(loss_col, weight)
         reduce_loss = loss_col["loss"].sum() / sample_num
         return reduce_loss
 
     @staticmethod
     def compute_grad(gh: DataFrame, label: DataFrame, score: DataFrame):
-        gh["g"] = score.apply_row(lambda s: [[i - 1 for i in s["score"]]])
+        label_name = label.schema.label_name
+        label = label.loc(score.get_indexer("sample_id"), preserve_order=True)
+        new_label = label.create_frame()
+        new_label[label_name] = label.label
+        stack_df = DataFrame.hstack([score, new_label])
+        stack_df = stack_df.loc(gh.get_indexer("sample_id"), preserve_order=True)
+
+        def grad(s):
+            grads = [i for i in s["score"]]
+            grads[s[label_name]] -= 1
+            return [grads]
+
+        gh["g"] = stack_df.apply_row(lambda s: grad(s))
 
     @staticmethod
     def compute_hess(gh: DataFrame, y, score):
         gh["h"] = score.apply_row(lambda s: [[2 * i * (1 - i) for i in s["score"]]])
 
 
-class L2Loss(object):
+class L2Loss(Loss):
     @staticmethod
     def initialize(label):
         init_score = label.create_frame()
         mean_score = float(label.mean())
         init_score["score"] = mean_score
         return init_score, mean_score
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/splitter.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/splitter.py`

 * *Files 22% similar despite different names*

```diff
@@ -19,14 +19,18 @@
 from fate.arch import Context
 from fate.arch.histogram import DistributedHistogram
 
 
 logger = logging.getLogger(__name__)
 
 
+# tree decimal round to prevent float error
+TREE_DECIMAL_ROUND = 10
+
+
 class SplitInfo(object):
     def __init__(
         self,
         best_fid=None,
         best_bid=None,
         sum_grad=0,
         sum_hess=0,
@@ -67,289 +71,15 @@
 
 
 class Splitter(object):
     def __init__(self) -> None:
         pass
 
 
-class SklearnSplitter(Splitter):
-    def __init__(
-        self,
-        feature_binning_dict,
-        min_impurity_split=1e-2,
-        min_sample_split=2,
-        min_leaf_node=1,
-        min_child_weight=1,
-        l1=0,
-        l2=0.1,
-        valid_features=None,
-    ) -> None:
-        super().__init__()
-        self.min_impurity_split = min_impurity_split
-        self.min_sample_split = min_sample_split
-        self.min_leaf_node = min_leaf_node
-        self.min_child_weight = min_child_weight
-        self.feature_binning_dict = feature_binning_dict
-        self.hist_mask = self.generate_mask(feature_binning_dict)
-        self.l1, self.l2 = l1, l2
-
-    def generate_mask(self, feature_dict):
-        split_counts = [len(split_point) for split_point in feature_dict.values()]
-        max_bin = max(split_counts)
-        mask = np.zeros((len(feature_dict), max_bin)).astype(np.bool8)
-        for i, bucket_count in enumerate(split_counts):
-            mask[i, :bucket_count] = True  # valid split point
-
-        return ~mask
-
-    def node_gain(self, g, h):
-        if isinstance(h, np.ndarray):
-            h[h == 0] = np.nan
-        score = g * g / (h + self.l2)
-        return score
-
-    def node_weight(self, sum_grad, sum_hess):
-        weight = -(sum_grad / (sum_hess + self.l2))
-        return weight
-
-    def _compute_min_leaf_mask(self, l_cnt, r_cnt):
-        min_leaf_node_mask_l = l_cnt < self.min_leaf_node
-        min_leaf_node_mask_r = r_cnt < self.min_leaf_node
-        union_mask_0 = np.logical_or(min_leaf_node_mask_l, min_leaf_node_mask_r)
-        return union_mask_0
-
-    def _compute_gains(self, g, h, cnt, g_sum, h_sum, cnt_sum, hist_mask=None):
-        l_g, l_h, l_cnt = g, h, cnt
-
-        if cnt_sum < self.min_sample_split:
-            return None
-
-        r_g, r_h = g_sum - l_g, h_sum - l_h
-        r_cnt = cnt_sum - l_cnt
-
-        # filter split
-        # leaf count
-        union_mask_0 = self._compute_min_leaf_mask(l_cnt, r_cnt)
-        # min child weight
-        min_child_weight_mask_l = l_h < self.min_child_weight
-        min_child_weight_mask_r = r_h < self.min_child_weight
-        union_mask_1 = np.logical_or(min_child_weight_mask_l, min_child_weight_mask_r)
-        if hist_mask is not None:
-            mask = np.logical_or(union_mask_0, hist_mask)
-        else:
-            mask = union_mask_0
-        mask = np.logical_or(mask, union_mask_1)
-
-        rs = self.node_gain(l_g, l_h) + self.node_gain(r_g, r_h) - self.node_gain(g_sum, h_sum)
-
-        rs[np.isnan(rs)] = -np.inf
-        rs[rs < self.min_impurity_split] = -np.inf
-        rs[mask] = -np.inf
-
-        return rs
-
-    def _find_guest_best_splits(self, node_hist, sitename, ret_sum=False):
-        l_g, l_h, l_cnt = node_hist
-        cnt_sum = l_cnt[::, -1][0]
-        g_sum = l_g[::, -1][0]
-        h_sum = l_h[::, -1][0]
-
-        rs = self._compute_gains(l_g, l_h, l_cnt, g_sum, h_sum, cnt_sum, hist_mask=self.hist_mask)
-
-        # reduce
-        feat_best_split = rs.argmax(axis=1)
-        feat_best_gain = rs.max(axis=1)
-
-        logger.debug("best gain {}".format(feat_best_gain))
-        # best split
-        best_split_idx = feat_best_gain.argmax()
-        best_gain = feat_best_gain.max()
-
-        if best_gain == -np.inf:
-            # can not split
-            logger.info("this node cannot be further split")
-            if ret_sum:
-                return None, g_sum, h_sum, cnt_sum
-            else:
-                return None
-
-        feat_id = best_split_idx
-        bin_id = feat_best_split[best_split_idx]
-
-        split_info = SplitInfo(
-            best_fid=feat_id,
-            best_bid=bin_id,
-            gain=best_gain,
-            sum_grad=l_g[feat_id][bin_id],
-            sum_hess=l_h[feat_id][bin_id],
-            sample_count=l_cnt[feat_id][bin_id],
-            sitename=sitename,
-        )
-
-        if ret_sum:
-            return split_info, g_sum, h_sum, cnt_sum
-        else:
-            return split_info
-
-    def _split(self, ctx: Context, histogram: list, cur_layer_node):
-        splits = []
-        logger.info("got {} hist".format(len(histogram)))
-        for node_hist in histogram:
-            split_info = self._find_guest_best_splits(node_hist, self.hist_mask, sitename=ctx.guest.name)
-            splits.append(split_info)
-        logger.info("split info is {}".format(split_info))
-        assert len(splits) == len(cur_layer_node), "split info length {} != node length {}".format(
-            len(splits), len(cur_layer_node)
-        )
-        return splits
-
-    def split(self, ctx: Context, histogram: list, cur_layer_node):
-        return self._split(ctx, histogram, cur_layer_node)
-
-
-class FedSklearnSplitter(SklearnSplitter):
-    def __init__(
-        self,
-        feature_binning_dict,
-        min_impurity_split=1e-2,
-        min_sample_split=2,
-        min_leaf_node=1,
-        min_child_weight=1,
-        l1=0,
-        l2=0,
-        valid_features=None,
-        random_seed=42,
-    ) -> None:
-        super().__init__(
-            feature_binning_dict,
-            min_impurity_split,
-            min_sample_split,
-            min_leaf_node,
-            min_child_weight,
-            l1,
-            l2,
-            valid_features,
-        )
-        self.random_seed = random_seed
-        np.random.seed(self.random_seed)
-
-    def _get_host_splits(self, ctx):
-        host_splits = ctx.hosts.get("host_splits")
-        return host_splits
-
-    def _find_host_best_splits(self, split, g_sum, h_sum, cnt_sum, sitename):
-        g, h, cnt = split
-        rs = self._compute_gains(g, h, cnt, g_sum, h_sum, cnt_sum)
-        best_splits_id = rs.argmax()
-        best_gain = rs.max()
-        split_info = SplitInfo(
-            gain=best_gain,
-            split_id=best_splits_id,
-            sitename=sitename,
-            sum_grad=g[best_splits_id],
-            sum_hess=h[best_splits_id],
-            sample_count=cnt[best_splits_id],
-        )
-
-        return split_info
-
-    def _merge_splits(self, guest_splits, host_splits_list):
-        splits = []
-        for node_idx in range(len(guest_splits)):
-            best_gain = -np.inf
-            best_splitinfo = None
-            guest_splitinfo: SplitInfo = guest_splits[node_idx]
-            if guest_splitinfo is not None and guest_splitinfo.gain > best_gain:
-                best_gain = guest_splitinfo.gain
-                best_splitinfo = guest_splitinfo
-
-            for host_idx in range(len(host_splits_list)):
-                host_splits = host_splits_list[host_idx]
-                host_splitinfo: SplitInfo = host_splits[node_idx]
-                if host_splitinfo is not None and host_splitinfo.gain > best_gain:
-                    best_gain = host_splitinfo.gain
-                    best_splitinfo = host_splitinfo
-            splits.append(best_splitinfo)
-
-        return splits
-
-    def _guest_split(self, ctx: Context, histogram, cur_layer_node):
-        sitename = ctx.guest.name
-        guest_best_splits = []
-        gh_sum = []
-        logger.info("got {} hist".format(len(histogram)))
-        for node_hist in histogram:
-            split_info, g_sum, h_sum, cnt_sum = self._find_guest_best_splits(
-                node_hist, ret_sum=True, sitename=sitename
-            )
-            guest_best_splits.append(split_info)
-            gh_sum.append((g_sum, h_sum, cnt_sum))
-
-        assert len(guest_best_splits) == len(cur_layer_node), "split info length {} != node length {}".format(
-            len(guest_best_splits), len(cur_layer_node)
-        )
-
-        host_splits_list = self._get_host_splits(ctx)
-        all_host_splits = []
-        for host_idx in range(len(host_splits_list)):
-            host_sitename = ctx.hosts[host_idx].name
-            host_splits = host_splits_list[host_idx]
-            assert len(host_splits) == len(cur_layer_node)
-            best_split = []
-            for node_idx, node_splits in enumerate(host_splits):
-                g_sum, h_sum, cnt_sum = gh_sum[node_idx]
-                node_best = self._find_host_best_splits(node_splits, g_sum, h_sum, cnt_sum, host_sitename)
-                best_split.append(node_best)
-            all_host_splits.append(best_split)
-
-        logger.info("guest split info is {}".format(guest_best_splits))
-        logger.info("host split info is {}".format(all_host_splits))
-        final_best_split = self._merge_splits(guest_best_splits, all_host_splits)
-        logger.info("final split info is {}".format(final_best_split))
-        return host_splits[0]
-
-    def _host_prepare(self, histogram):
-        to_send_hist = []
-        pos_map = []
-        # prepare host split points
-        for node_hist in histogram:
-            g, h, cnt = node_hist
-            shape = g.shape
-            pos_map_ = {}
-            g[self.hist_mask] = np.nan
-            h[self.hist_mask] = np.nan
-            # cnt is int, cannot use np.nan as mask
-            cnt[self.hist_mask] = 0
-            g, h, cnt = g.flatten(), h.flatten(), cnt.flatten()
-            random_shuffle_idx = np.random.permutation(len(g))
-            # random_shuffle_idx = np.array([i for i in range(len(g))])
-            g = g[random_shuffle_idx]
-            h = h[random_shuffle_idx]
-            cnt = cnt[random_shuffle_idx]
-            to_send_hist.append([g, h, cnt])
-            for split_idx, real_idx in enumerate(random_shuffle_idx):
-                pos_map_[split_idx] = (real_idx // shape[1], real_idx % shape[1])
-            pos_map.append(pos_map_)
-        return to_send_hist, pos_map
-
-    def _host_split(self, ctx, histogram, cur_layer_node):
-        to_send_hist, pos_map = self._host_prepare(histogram)
-        ctx.guest.put("host_splits", to_send_hist)
-        return pos_map
-
-    def split(self, ctx: Context, histogram, cur_layer_node):
-        if ctx.is_on_guest:
-            return self._guest_split(ctx, histogram, cur_layer_node)
-        elif ctx.is_on_host:
-            return self._host_split(ctx, histogram, cur_layer_node)
-        else:
-            raise ValueError("illegal role {}".format(ctx.role))
-
-
-class FedSBTSplitter(object):
+class SBTSplitter(Splitter):
     def __init__(
         self,
         bin_train_data: DataFrame,
         bin_info: dict,
         min_impurity_split=1e-2,
         min_sample_split=2,
         min_leaf_node=1,
@@ -377,23 +107,46 @@
             if idx < cumulative_buckets[i]:
                 fid = i - 1
                 bid = idx - cumulative_buckets[i - 1]
                 return fid, bid
 
         raise ValueError("idx is out of range")
 
+    @staticmethod
+    def truncate(f, n=TREE_DECIMAL_ROUND):
+        return np.floor(f * 10**n) / 10**n
+
+    def _l1_reg(self, g):
+        if self.l1 == 0:
+            return g
+        if isinstance(g, torch.Tensor):
+            g[g < -self.l1] += self.l1
+            g[g > self.l1] -= self.l1
+            g[(g <= self.l1) & (g >= -self.l1)] = 0
+        else:
+            if g < -self.l1:
+                return g + self.l1
+            elif g > self.l1:
+                return g - self.l1
+            else:
+                return 0
+        return g
+
     def node_gain(self, g, h):
+        g, h = self.truncate(g), self.truncate(h)
+        g = self._l1_reg(g)
         if isinstance(h, np.ndarray):
             h[h == 0] = np.nan
-        score = g * g / (h + self.l2)
+        score = (g * g) / (h + self.l2)
         return score
 
     def node_weight(self, sum_grad, sum_hess):
+        sum_grad = self._l1_reg(sum_grad)
         weight = -(sum_grad / (sum_hess + self.l2))
-        return weight
+        return self.truncate(weight)
 
     def _extract_hist(self, histogram, pack_info=None):
         tensor_hist: dict = histogram.extract_data()
         g_all, h_all, cnt_all = None, None, None
         for k, v in tensor_hist.items():
             cnt = v["cnt"].reshape((1, -1))
 
@@ -440,15 +193,14 @@
         min_leaf_node_mask_l = l_cnt < self.min_leaf_node
         min_leaf_node_mask_r = r_cnt < self.min_leaf_node
         union_mask_0 = torch.logical_or(min_leaf_node_mask_l, min_leaf_node_mask_r)
         return union_mask_0
 
     def _compute_gains(self, g, h, cnt, g_sum, h_sum, cnt_sum, hist_mask=None):
         l_g, l_h, l_cnt = g, h, cnt
-
         r_g, r_h = g_sum - l_g, h_sum - l_h
         r_cnt = cnt_sum - l_cnt
 
         # filter split
         # leaf count
         union_mask_0 = self._compute_min_leaf_mask(l_cnt, r_cnt)
         # min child weight
@@ -457,18 +209,18 @@
         union_mask_1 = torch.logical_or(min_child_weight_mask_l, min_child_weight_mask_r)
         if hist_mask is not None:
             mask = torch.logical_or(union_mask_0, hist_mask)
         else:
             mask = union_mask_0
         mask = torch.logical_or(mask, union_mask_1)
         rs = self.node_gain(l_g, l_h) + self.node_gain(r_g, r_h) - self.node_gain(g_sum, h_sum)
+        rs = self.truncate(rs)
         rs[torch.isnan(rs)] = float("-inf")
         rs[rs < self.min_impurity_split] = float("-inf")
         rs[mask] = float("-inf")
-
         return rs
 
     def _find_best_splits(
         self, node_hist, sitename, cur_layer_nodes, reverse_node_map, recover_bucket=True, pack_info=None
     ):
         """
         recover_bucket: if node_hist is guest hist, can get the fid and bid of the split info
@@ -532,14 +284,24 @@
 
         return splits
 
     def _recover_pack_split(self, hist: DistributedHistogram, schema, decode_schema=None):
         host_hist = hist.decrypt(schema[0], schema[1], decode_schema)
         return host_hist
 
+    def _local_split(self, ctx: Context, stat_rs, node_map, cur_layer_node):
+        histogram = stat_rs.decrypt({}, {}, None)
+        sitename = ctx.local.name
+        reverse_node_map = {v: k for k, v in node_map.items()}
+        local_best_splits = self._find_best_splits(
+            histogram, sitename, cur_layer_node, reverse_node_map, recover_bucket=True
+        )
+
+        return local_best_splits
+
     def _guest_split(self, ctx: Context, stat_rs, cur_layer_node, node_map, sk, coder, gh_pack, pack_info):
         if sk is None or coder is None:
             raise ValueError("sk or coder is None, not able to decode host split points")
 
         histogram = stat_rs.decrypt({}, {}, None)
         sitename = ctx.local.name
         reverse_node_map = {v: k for k, v in node_map.items()}
@@ -591,27 +353,34 @@
 
     def split(
         self,
         ctx: Context,
         histogram_statistic_result,
         cur_layer_node,
         node_map,
+        local_split=False,
         sk=None,
         coder=None,
         gh_pack=None,
         pack_info=None,
     ):
-        if ctx.is_on_guest:
-            if sk is None or coder is None:
-                raise ValueError("sk or coder is None, not able to decode host split points")
-            assert gh_pack is not None and isinstance(
-                gh_pack, bool
-            ), "gh_pack should be bool, indicating if the gh is packed"
-            if not gh_pack:
-                logger.info("not using gh pack to split")
-            return self._guest_split(
-                ctx, histogram_statistic_result, cur_layer_node, node_map, sk, coder, gh_pack, pack_info
-            )
-        elif ctx.is_on_host:
-            return self._host_split(ctx, histogram_statistic_result, cur_layer_node)
+        if local_split:
+            # Use local features only
+            best_slits = self._local_split(ctx, histogram_statistic_result, node_map, cur_layer_node)
+            return best_slits
         else:
-            raise ValueError("illegal role {}".format(ctx.role))
+            # For hetero-secureboost
+            if ctx.is_on_guest:
+                if sk is None or coder is None:
+                    raise ValueError("sk or coder is None, not able to decode host split points")
+                assert gh_pack is not None and isinstance(
+                    gh_pack, bool
+                ), "gh_pack should be bool, indicating if the gh is packed"
+                if not gh_pack:
+                    logger.info("not using gh pack to split")
+                return self._guest_split(
+                    ctx, histogram_statistic_result, cur_layer_node, node_map, sk, coder, gh_pack, pack_info
+                )
+            elif ctx.is_on_host:
+                return self._host_split(ctx, histogram_statistic_result, cur_layer_node)
+            else:
+                raise ValueError("illegal role {}".format(ctx.role))
```

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/learner/decision_tree/tree_core/subsample.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/subsample.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/utils/__init__.py` & `pyfate-2.1.0/fate/components/entrypoint/cli/test/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/ensemble/utils/binning.py` & `pyfate-2.1.0/fate/ml/ensemble/utils/binning.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/evaluation/__init__.py` & `pyfate-2.1.0/fate/ml/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,8 +8,7 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
```

### Comparing `pyfate-2.0.0b0/fate/ml/evaluation/classification.py` & `pyfate-2.1.0/fate/ml/evaluation/classification.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,137 +7,136 @@
 from fate.ml.evaluation.metric_base import Metric
 from sklearn.metrics import roc_auc_score
 from sklearn.metrics import accuracy_score
 from sklearn.metrics import recall_score, precision_score, f1_score
 from fate.ml.evaluation.metric_base import EvalResult
 
 
-
 """
 Single Value Metrics
 """
 
+
 class AUC(Metric):
 
-    metric_name = 'auc'
+    metric_name = "auc"
 
     def __init__(self):
         super().__init__()
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
         auc_score = roc_auc_score(label, predict)
         return EvalResult(self.metric_name, auc_score)
 
 
 class BinaryMetricWithThreshold(Metric):
-
     def __init__(self, threshold=0.5):
         super().__init__()
         self.threshold = threshold
 
 
 class MultiAccuracy(Metric):
 
-    metric_name = 'multi_accuracy'
+    metric_name = "multi_accuracy"
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict, flatten=False)
         label = self.to_np_format(label).astype(np.int32)
         if predict.shape != label.shape:
             predict = predict.argmax(axis=-1).astype(np.int32)
         acc = accuracy_score(label, predict)
         return EvalResult(self.metric_name, acc)
 
 
 class MultiRecall(Metric):
 
-    metric_name = 'multi_recall'
+    metric_name = "multi_recall"
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict, flatten=False)
         label = self.to_np_format(label)
         if predict.shape != label.shape:
             predict = predict.argmax(axis=-1)
-        recall = recall_score(label, predict, average='macro')
+        recall = recall_score(label, predict, average="macro")
         return EvalResult(self.metric_name, recall)
 
 
 class MultiPrecision(Metric):
 
-    metric_name = 'multi_precision'
+    metric_name = "multi_precision"
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict, flatten=False)
         label = self.to_np_format(label)
         if predict.shape != label.shape:
             predict = predict.argmax(axis=-1)
-        precision = precision_score(label, predict, average='macro')
+        precision = precision_score(label, predict, average="macro")
         return EvalResult(self.metric_name, precision)
 
 
 class BinaryAccuracy(MultiAccuracy, BinaryMetricWithThreshold):
 
-    metric_name = 'binary_accuracy'
+    metric_name = "binary_accuracy"
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         predict = (predict > self.threshold).astype(int)
         label = self.to_np_format(label)
         acc = accuracy_score(label, predict)
         return EvalResult(self.metric_name, acc)
 
 
 class BinaryRecall(MultiRecall, BinaryMetricWithThreshold):
 
-    metric_name = 'binary_recall'
+    metric_name = "binary_recall"
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         predict = (predict > self.threshold).astype(int)
         label = self.to_np_format(label)
         recall = recall_score(label, predict)
         return EvalResult(self.metric_name, recall)
 
 
 class BinaryPrecision(MultiPrecision, BinaryMetricWithThreshold):
 
-    metric_name = 'binary_precision'
+    metric_name = "binary_precision"
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         predict = (predict > self.threshold).astype(int)
         label = self.to_np_format(label)
-        precision = precision_score(label, predict)       
+        precision = precision_score(label, predict)
         return EvalResult(self.metric_name, precision)
 
 
 class MultiF1Score(Metric):
 
-    metric_name = 'multi_f1_score'
+    metric_name = "multi_f1_score"
 
-    def __init__(self, average='micro'):
+    def __init__(self, average="micro"):
         super().__init__()
         self.average = average
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict, flatten=False)
         label = self.to_np_format(label)
         if predict.shape != label.shape:
             predict = predict.argmax(axis=-1)
-        f1 = f1_score(label, predict, average=self.average)  
+        f1 = f1_score(label, predict, average=self.average)
         return EvalResult(self.metric_name, f1)
 
 
 class BinaryF1Score(MultiF1Score, BinaryMetricWithThreshold):
 
-    metric_name = 'binary_f1_score'
+    metric_name = "binary_f1_score"
 
-    def __init__(self, threshold=0.5, average='binary'):
+    def __init__(self, threshold=0.5, average="binary"):
         super().__init__(average)
         self.threshold = threshold
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         predict = (predict > self.threshold).astype(int)
         label = self.to_np_format(label)
@@ -166,52 +165,50 @@
     sorted_labels = labels[sort_idx]
     sorted_scores = pred_scores[sort_idx]
 
     return sorted_labels, sorted_scores
 
 
 class _ConfusionMatrix(object):
-
     @staticmethod
     def compute(sorted_labels: list, sorted_pred_scores: list, score_thresholds: list, ret: list, pos_label=1):
 
         for ret_type in ret:
-            assert ret_type in ['tp', 'tn', 'fp', 'fn']
+            assert ret_type in ["tp", "tn", "fp", "fn"]
 
         sorted_labels = np.array(sorted_labels)
         sorted_scores = np.array(sorted_pred_scores)
         sorted_labels[sorted_labels != pos_label] = 0
         sorted_labels[sorted_labels == pos_label] = 1
         score_thresholds = np.array([score_thresholds]).transpose()
         pred_labels = (sorted_scores > score_thresholds) + 0
 
         ret_dict = {}
-        if 'tp' in ret or 'tn' in ret:
-            match_arr = (pred_labels + sorted_labels)
-            if 'tp' in ret:
+        if "tp" in ret or "tn" in ret:
+            match_arr = pred_labels + sorted_labels
+            if "tp" in ret:
                 tp_num = (match_arr == 2).sum(axis=-1)
-                ret_dict['tp'] = tp_num
-            if 'tn' in ret:
+                ret_dict["tp"] = tp_num
+            if "tn" in ret:
                 tn_num = (match_arr == 0).sum(axis=-1)
-                ret_dict['tn'] = tn_num
+                ret_dict["tn"] = tn_num
 
-        if 'fp' in ret or 'fn' in ret:
-            match_arr = (sorted_labels - pred_labels)
-            if 'fp' in ret:
+        if "fp" in ret or "fn" in ret:
+            match_arr = sorted_labels - pred_labels
+            if "fp" in ret:
                 fp_num = (match_arr == -1).sum(axis=-1)
-                ret_dict['fp'] = fp_num
-            if 'fn' in ret:
+                ret_dict["fp"] = fp_num
+            if "fn" in ret:
                 fn_num = (match_arr == 1).sum(axis=-1)
-                ret_dict['fn'] = fn_num
+                ret_dict["fn"] = fn_num
 
         return ret_dict
 
 
 class ThresholdCutter(object):
-
     @staticmethod
     def cut_by_step(sorted_scores, steps=0.01):
         assert isinstance(steps, float) and (0 < steps < 1)
         thresholds = list(set(sorted_scores))
         thresholds, cuts = ThresholdCutter.__filt_threshold(thresholds, 0.01)
         score_threshold = thresholds
 
@@ -238,146 +235,161 @@
         thresholds.sort(reverse=True)
         index_list = [int(size * cut) for cut in cuts]
         new_thresholds = [thresholds[idx] for idx in index_list]
 
         return new_thresholds, cuts
 
     @staticmethod
-    def cut_by_quantile(scores, quantile_list=None, interpolation='nearest', remove_duplicate=True):
+    def cut_by_quantile(scores, quantile_list=None, interpolation="nearest", remove_duplicate=True):
 
         if quantile_list is None:  # default is 20 intervals
             quantile_list = [round(i * 0.05, 3) for i in range(20)] + [1.0]
         quantile_val = np.quantile(scores, quantile_list, interpolation=interpolation)
         if remove_duplicate:
             quantile_val = sorted(list(set(quantile_val)))
         else:
             quantile_val = sorted(list(quantile_val))
 
         if len(quantile_val) == 1:
             quantile_val = [np.min(scores), np.max(scores)]
 
         return quantile_val
 
-class BiClassMetric(object):
 
-    def __init__(self, cut_method='step', remove_duplicate=False, pos_label=1):
-        assert cut_method in ['step', 'quantile']
+class BiClassMetric(object):
+    def __init__(self, cut_method="step", remove_duplicate=False, pos_label=1):
+        assert cut_method in ["step", "quantile"]
         self.cut_method = cut_method
         self.remove_duplicate = remove_duplicate  # available when cut_method is quantile
         self.pos_label = pos_label
 
-    def prepare_confusion_mat(self, labels, scores, add_to_end=True, ):
-        import logging
-        logger = logging.getLogger(__name__)
-        logger.info('labels are {}, scores are {}'.format(labels, scores))
+    def prepare_confusion_mat(
+        self,
+        labels,
+        scores,
+        add_to_end=True,
+    ):
         sorted_labels, sorted_scores = sort_score_and_label(labels, scores)
 
         score_threshold, cuts = None, None
 
-        if self.cut_method == 'step':
+        if self.cut_method == "step":
             score_threshold, cuts = ThresholdCutter.cut_by_step(sorted_scores, steps=0.01)
             if add_to_end:
                 score_threshold.append(min(score_threshold) - 0.001)
                 cuts.append(1)
 
-        elif self.cut_method == 'quantile':
+        elif self.cut_method == "quantile":
             score_threshold = ThresholdCutter.cut_by_quantile(sorted_scores, remove_duplicate=self.remove_duplicate)
             score_threshold = list(np.flip(score_threshold))
 
-        confusion_mat = _ConfusionMatrix.compute(sorted_labels, sorted_scores, score_threshold,
-                                                ret=['tp', 'fp', 'fn', 'tn'], pos_label=self.pos_label)
+        confusion_mat = _ConfusionMatrix.compute(
+            sorted_labels, sorted_scores, score_threshold, ret=["tp", "fp", "fn", "tn"], pos_label=self.pos_label
+        )
 
         return confusion_mat, score_threshold, cuts
 
-    def compute(self, labels, scores, ):
-        confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(labels, scores, )
+    def compute(
+        self,
+        labels,
+        scores,
+    ):
+        confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(
+            labels,
+            scores,
+        )
         metric_scores = self.compute_metric_from_confusion_mat(confusion_mat)
         return list(metric_scores), score_threshold, cuts
 
     def compute_metric_from_confusion_mat(self, *args):
         raise NotImplementedError()
 
 
 """
 Metrics with Cruve/Table Results
 """
 
+
 class KS(Metric):
 
-    metric_name = 'ks'
+    metric_name = "ks"
 
     def __init__(self):
         super().__init__()
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
-        
+
         sorted_labels, sorted_scores = sort_score_and_label(label, predict)
         threshold, cuts = ThresholdCutter.cut_by_index(sorted_scores)
-        confusion_mat = _ConfusionMatrix.compute(sorted_labels, sorted_scores, threshold, ret=['tp', 'fp'],
-                                                pos_label=1)
+        confusion_mat = _ConfusionMatrix.compute(
+            sorted_labels, sorted_scores, threshold, ret=["tp", "fp"], pos_label=1
+        )
         pos_num, neg_num = neg_pos_count(sorted_labels, pos_label=1)
 
-        assert pos_num > 0 and neg_num > 0, "error when computing KS metric, pos sample number and neg sample number" \
-                                            "must be larger than 0"
+        assert pos_num > 0 and neg_num > 0, (
+            "error when computing KS metric, pos sample number and neg sample number" "must be larger than 0"
+        )
 
-        tpr_arr = confusion_mat['tp'] / pos_num
-        fpr_arr = confusion_mat['fp'] / neg_num
+        tpr_arr = confusion_mat["tp"] / pos_num
+        fpr_arr = confusion_mat["fp"] / neg_num
 
         tpr = np.append(tpr_arr, np.array([1.0]))
         fpr = np.append(fpr_arr, np.array([1.0]))
         cuts = np.append(cuts, np.array([1.0]))
         threshold.append(0.0)
         ks_curve = tpr[:-1] - fpr[:-1]
         ks_val = np.max(ks_curve)
 
-        return EvalResult(self.metric_name, ks_val), \
-            EvalResult(self.metric_name + '_table', pd.DataFrame({'tpr': tpr, 'fpr': fpr, 'threshold': threshold, 'cuts': cuts}))
-    
+        return EvalResult(self.metric_name, ks_val), EvalResult(
+            self.metric_name + "_table", pd.DataFrame({"tpr": tpr, "fpr": fpr, "threshold": threshold, "cuts": cuts})
+        )
+
 
 class ConfusionMatrix(Metric):
 
-    metric_name = 'confusion_matrix'
+    metric_name = "confusion_matrix"
 
     def __init__(self):
         super().__init__()
 
     def __call__(self, predict, label, **kwargs):
 
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
-        
+
         sorted_labels, sorted_scores = sort_score_and_label(label, predict)
         threshold, cuts = ThresholdCutter.cut_by_index(sorted_scores)
-        confusion_mat = _ConfusionMatrix.compute(sorted_labels, sorted_scores, threshold, ret=['tp', 'tn', 'fp', 'fn'],
-                                                pos_label=1)
-        confusion_mat['cuts'] = cuts
-        confusion_mat['threshold'] = threshold
+        confusion_mat = _ConfusionMatrix.compute(
+            sorted_labels, sorted_scores, threshold, ret=["tp", "tn", "fp", "fn"], pos_label=1
+        )
+        confusion_mat["cuts"] = cuts
+        confusion_mat["threshold"] = threshold
         return EvalResult(self.metric_name, pd.DataFrame(confusion_mat))
 
 
 class Lift(Metric, BiClassMetric):
 
-    metric_name = 'lift'
+    metric_name = "lift"
 
     def __init__(self, *args, **kwargs):
         Metric.__init__(self)
-        BiClassMetric.__init__(self, cut_method='step', remove_duplicate=False, pos_label=1)
-    
+        BiClassMetric.__init__(self, cut_method="step", remove_duplicate=False, pos_label=1)
+
     @staticmethod
     def _lift_helper(val):
 
         tp, fp, fn, tn, labels_num = val[0], val[1], val[2], val[3], val[4]
 
         lift_x_type, lift_y_type = [], []
 
-        for label_type in ['1', '0']:
+        for label_type in ["1", "0"]:
 
-            if label_type == '0':
+            if label_type == "0":
                 tp, tn = tn, tp
                 fp, fn = fn, fp
 
             if labels_num == 0:
                 lift_x = 1
                 denominator = 1
             else:
@@ -395,62 +407,71 @@
                 lift_y = numerator / denominator
 
             lift_x_type.insert(0, lift_x)
             lift_y_type.insert(0, lift_y)
 
         return lift_x_type, lift_y_type
 
-    def compute_metric_from_confusion_mat(self, confusion_mat, labels_len, ):
-
-        labels_nums = np.zeros(len(confusion_mat['tp'])) + labels_len
-
-        rs = map(self._lift_helper, zip(confusion_mat['tp'], confusion_mat['fp'],
-                                        confusion_mat['fn'], confusion_mat['tn'], labels_nums))
+    def compute_metric_from_confusion_mat(
+        self,
+        confusion_mat,
+        labels_len,
+    ):
+
+        labels_nums = np.zeros(len(confusion_mat["tp"])) + labels_len
+
+        rs = map(
+            self._lift_helper,
+            zip(confusion_mat["tp"], confusion_mat["fp"], confusion_mat["fn"], confusion_mat["tn"], labels_nums),
+        )
 
         rs = list(rs)
 
         lifts_x, lifts_y = [i[0] for i in rs], [i[1] for i in rs]
 
         return lifts_y, lifts_x
 
     def __call__(self, predict, label, **kwargs):
-        
+
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
-        confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(label, predict, add_to_end=False, )
+        confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(
+            label,
+            predict,
+            add_to_end=False,
+        )
 
-        lifts_y, lifts_x = self.compute_metric_from_confusion_mat(confusion_mat, len(label), )
+        lifts_y, lifts_x = self.compute_metric_from_confusion_mat(
+            confusion_mat,
+            len(label),
+        )
 
-        return EvalResult(self.metric_name,
-            pd.DataFrame({
-                'liftx': lifts_x,
-                'lifty': lifts_y,
-                'threshold': list(score_threshold)
-            })
+        return EvalResult(
+            self.metric_name, pd.DataFrame({"liftx": lifts_x, "lifty": lifts_y, "threshold": list(score_threshold)})
         )
 
 
 class Gain(Metric, BiClassMetric):
 
-    metric_name = 'gain'
+    metric_name = "gain"
 
     def __init__(self, *args, **kwargs):
         Metric.__init__(self)
-        BiClassMetric.__init__(self, cut_method='step', remove_duplicate=False, pos_label=1)
-    
+        BiClassMetric.__init__(self, cut_method="step", remove_duplicate=False, pos_label=1)
+
     @staticmethod
     def _gain_helper(val):
 
         tp, fp, fn, tn, num_label = val[0], val[1], val[2], val[3], val[4]
 
         gain_x_type, gain_y_type = [], []
 
-        for pos_label in ['1', '0']:
+        for pos_label in ["1", "0"]:
 
-            if pos_label == '0':
+            if pos_label == "0":
                 tp, tn = tn, tp
                 fp, fn = fn, fp
 
             if num_label == 0:
                 gain_x = 1
             else:
                 gain_x = float((tp + fp) / num_label)
@@ -464,261 +485,274 @@
             gain_x_type.insert(0, gain_x)
             gain_y_type.insert(0, gain_y)
 
         return gain_x_type, gain_y_type
 
     def compute_metric_from_confusion_mat(self, confusion_mat, labels_len):
 
-        labels_nums = np.zeros(len(confusion_mat['tp'])) + labels_len
+        labels_nums = np.zeros(len(confusion_mat["tp"])) + labels_len
 
-        rs = map(self._gain_helper, zip(confusion_mat['tp'], confusion_mat['fp'],
-                                        confusion_mat['fn'], confusion_mat['tn'], labels_nums))
+        rs = map(
+            self._gain_helper,
+            zip(confusion_mat["tp"], confusion_mat["fp"], confusion_mat["fn"], confusion_mat["tn"], labels_nums),
+        )
 
         rs = list(rs)
 
         gain_x, gain_y = [i[0] for i in rs], [i[1] for i in rs]
 
         return gain_y, gain_x
 
     def __call__(self, predict, label, **kwargs):
 
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
-        confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(label, predict, add_to_end=False, )
+        confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(
+            label,
+            predict,
+            add_to_end=False,
+        )
 
         gain_y, gain_x = self.compute_metric_from_confusion_mat(confusion_mat, len(label))
 
-        return EvalResult(self.metric_name,
-            pd.DataFrame({
-                'gainx': gain_x,
-                'gainy': gain_y,
-                'threshold': list(score_threshold)
-            })
+        return EvalResult(
+            self.metric_name, pd.DataFrame({"gainx": gain_x, "gainy": gain_y, "threshold": list(score_threshold)})
         )
-    
 
 
 class BiClassPrecisionTable(Metric, BiClassMetric):
     """
     Compute binary classification precision using multiple thresholds
     """
-    metric_name = 'biclass_precision_table'
+
+    metric_name = "biclass_precision_table"
 
     def __init__(self, *args, **kwargs):
         Metric.__init__(self)
-        BiClassMetric.__init__(self, cut_method='step', remove_duplicate=False, pos_label=1)
+        BiClassMetric.__init__(self, cut_method="step", remove_duplicate=False, pos_label=1)
 
     def compute_metric_from_confusion_mat(self, confusion_mat, impute_val=1.0):
-        numerator = confusion_mat['tp']
-        denominator = (confusion_mat['tp'] + confusion_mat['fp'])
-        zero_indexes = (denominator == 0)
+        numerator = confusion_mat["tp"]
+        denominator = confusion_mat["tp"] + confusion_mat["fp"]
+        zero_indexes = denominator == 0
         denominator[zero_indexes] = 1
         precision_scores = numerator / denominator
         precision_scores[zero_indexes] = impute_val  # impute_val is for prettifying when drawing pr curves
 
         return precision_scores
-    
+
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
         p, threshold, cuts = self.compute(label, predict)
-        return EvalResult(self.metric_name, pd.DataFrame({
-            'p': p,
-            'threshold': threshold,
-            'cuts': cuts
-        }))
-    
+        return EvalResult(self.metric_name, pd.DataFrame({"p": p, "threshold": threshold, "cuts": cuts}))
 
 
 class BiClassRecallTable(Metric, BiClassMetric):
     """
     Compute binary classification recall using multiple thresholds
     """
-    metric_name = 'biclass_recall_table'
+
+    metric_name = "biclass_recall_table"
 
     def __init__(self, *args, **kwargs):
         Metric.__init__(self)
-        BiClassMetric.__init__(self, cut_method='step', remove_duplicate=False, pos_label=1)
+        BiClassMetric.__init__(self, cut_method="step", remove_duplicate=False, pos_label=1)
 
     def compute_metric_from_confusion_mat(self, confusion_mat, formatted=True):
-        recall_scores = confusion_mat['tp'] / (confusion_mat['tp'] + confusion_mat['fn'])
+        recall_scores = confusion_mat["tp"] / (confusion_mat["tp"] + confusion_mat["fn"])
         return recall_scores
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
         r, threshold, cuts = self.compute(label, predict)
-        return EvalResult(self.metric_name, pd.DataFrame({
-            'r': r,
-            'threshold': threshold,
-            'cuts': cuts
-        }))
+        return EvalResult(self.metric_name, pd.DataFrame({"r": r, "threshold": threshold, "cuts": cuts}))
 
 
 class BiClassAccuracyTable(Metric, BiClassMetric):
     """
     Compute binary classification accuracy using multiple thresholds
     """
-    metric_name = 'biclass_accuracy_table'
+
+    metric_name = "biclass_accuracy_table"
 
     def __init__(self, *args, **kwargs):
         Metric.__init__(self)
-        BiClassMetric.__init__(self, cut_method='step', remove_duplicate=False, pos_label=1)
+        BiClassMetric.__init__(self, cut_method="step", remove_duplicate=False, pos_label=1)
 
     def compute(self, labels, scores, normalize=True):
         confusion_mat, score_threshold, cuts = self.prepare_confusion_mat(labels, scores)
         metric_scores = self.compute_metric_from_confusion_mat(confusion_mat, normalize=normalize)
         return list(metric_scores), score_threshold[: len(metric_scores)], cuts[: len(metric_scores)]
 
     def compute_metric_from_confusion_mat(self, confusion_mat, normalize=True):
-        rs = (confusion_mat['tp'] + confusion_mat['tn']) / \
-             (confusion_mat['tp'] + confusion_mat['tn'] + confusion_mat['fn'] + confusion_mat['fp']) if normalize \
-            else (confusion_mat['tp'] + confusion_mat['tn'])
+        rs = (
+            (confusion_mat["tp"] + confusion_mat["tn"])
+            / (confusion_mat["tp"] + confusion_mat["tn"] + confusion_mat["fn"] + confusion_mat["fp"])
+            if normalize
+            else (confusion_mat["tp"] + confusion_mat["tn"])
+        )
         return rs[:-1]
 
     def __call__(self, predict, label, **kwargs) -> Dict:
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
         accuracy, threshold, cuts = self.compute(label, predict)
-        return EvalResult(self.metric_name, pd.DataFrame({
-            'accuracy': accuracy,
-            'threshold': threshold,
-            'cuts': cuts
-        }))
+        return EvalResult(self.metric_name, pd.DataFrame({"accuracy": accuracy, "threshold": threshold, "cuts": cuts}))
 
 
 class FScoreTable(Metric):
     """
     Compute F score from bi-class confusion mat
     """
 
-    metric_name = 'fscore_table'
+    metric_name = "fscore_table"
 
     def __call__(self, predict, label, beta=1):
 
         predict = self.to_np_format(predict)
         label = self.to_np_format(label)
 
         sorted_labels, sorted_scores = sort_score_and_label(label, predict)
         _, cuts = ThresholdCutter.cut_by_step(sorted_scores, steps=0.01)
         fixed_interval_threshold = ThresholdCutter.fixed_interval_threshold()
-        confusion_mat = _ConfusionMatrix.compute(sorted_labels, sorted_scores,
-                                                fixed_interval_threshold,
-                                                ret=['tp', 'fp', 'fn', 'tn'])
+        confusion_mat = _ConfusionMatrix.compute(
+            sorted_labels, sorted_scores, fixed_interval_threshold, ret=["tp", "fp", "fn", "tn"]
+        )
         precision_computer = BiClassPrecisionTable()
         recall_computer = BiClassRecallTable()
         p_score = precision_computer.compute_metric_from_confusion_mat(confusion_mat)
         r_score = recall_computer.compute_metric_from_confusion_mat(confusion_mat)
         beta_2 = beta * beta
-        denominator = (beta_2 * p_score + r_score)
+        denominator = beta_2 * p_score + r_score
         denominator[denominator == 0] = 1e-6  # in case denominator is 0
         numerator = (1 + beta_2) * (p_score * r_score)
         f_score = numerator / denominator
 
-        return EvalResult(self.metric_name, pd.DataFrame({
-            'f_score': f_score,
-            'threshold': fixed_interval_threshold,
-            'cuts': cuts
-        }))
+        return EvalResult(
+            self.metric_name, pd.DataFrame({"f_score": f_score, "threshold": fixed_interval_threshold, "cuts": cuts})
+        )
 
 
 class PSI(Metric):
 
-    metric_name = 'psi'
+    metric_name = "psi"
 
     def __call__(self, predict: dict, label: dict, **kwargs) -> Dict:
 
         """
         train/validate scores: predicted scores on train/validate set
         train/validate labels: true labels
         debug: print debug message
         if train&validate labels are not None, count positive sample percentage in every interval
         """
 
-        str_intervals=False
-        round_num=3
-        pos_label=1
+        str_intervals = False
+        round_num = 3
+        pos_label = 1
 
         if not isinstance(predict, dict) or (label is not None and not isinstance(label, dict)):
             raise ValueError("Input 'predict' must be a dictionary, and 'label' must be either None or a dictionary.")
 
-        train_scores = predict.get('train_scores')
-        validate_scores = predict.get('validate_scores')
+        train_scores = predict.get("train_scores")
+        validate_scores = predict.get("validate_scores")
 
         if train_scores is None or validate_scores is None:
             raise ValueError(
                 "Input 'predict' should contain the following keys: 'train_scores', 'validate_scores'. "
                 "Please make sure both keys are present."
             )
 
-        train_labels = label.get('train_labels') if label is not None else None
-        validate_labels = label.get('validate_labels') if label is not None else None
+        train_labels = label.get("train_labels") if label is not None else None
+        validate_labels = label.get("validate_labels") if label is not None else None
 
         train_scores = np.array(train_scores)
         validate_scores = np.array(validate_scores)
         quantile_points = ThresholdCutter().cut_by_quantile(train_scores)
 
         train_count = self.quantile_binning_and_count(train_scores, quantile_points)
         validate_count = self.quantile_binning_and_count(validate_scores, quantile_points)
 
         train_pos_perc, validate_pos_perc = None, None
 
         if train_labels is not None and validate_labels is not None:
             assert len(train_labels) == len(train_scores) and len(validate_labels) == len(validate_scores)
             train_labels, validate_labels = np.array(train_labels), np.array(validate_labels)
             train_pos_count = self.quantile_binning_and_count(train_scores[train_labels == pos_label], quantile_points)
-            validate_pos_count = self.quantile_binning_and_count(validate_scores[validate_labels == pos_label],
-                                                                 quantile_points)
+            validate_pos_count = self.quantile_binning_and_count(
+                validate_scores[validate_labels == pos_label], quantile_points
+            )
 
-            train_pos_perc = np.array(train_pos_count['count']) / np.array(train_count['count'])
-            validate_pos_perc = np.array(validate_pos_count['count']) / np.array(validate_count['count'])
+            train_pos_perc = np.array(train_pos_count["count"]) / np.array(train_count["count"])
+            validate_pos_perc = np.array(validate_pos_count["count"]) / np.array(validate_count["count"])
 
             # handle special cases
             train_pos_perc[train_pos_perc == np.inf] = -1
             validate_pos_perc[validate_pos_perc == np.inf] = -1
             train_pos_perc[np.isnan(train_pos_perc)] = 0
             validate_pos_perc[np.isnan(validate_pos_perc)] = 0
 
-        assert (train_count['interval'] == validate_count['interval']), 'train count interval is not equal to ' \
-                                                                        'validate count interval'
+        assert train_count["interval"] == validate_count["interval"], (
+            "train count interval is not equal to " "validate count interval"
+        )
 
-        expected_interval = np.array(train_count['count'])
-        actual_interval = np.array(validate_count['count'])
+        expected_interval = np.array(train_count["count"])
+        actual_interval = np.array(validate_count["count"])
 
         expected_interval = expected_interval.astype(np.float)
         actual_interval = actual_interval.astype(np.float)
 
-        psi_scores, total_psi, expected_interval, actual_interval, expected_percentage, actual_percentage \
-            = self.psi_score(expected_interval, actual_interval, len(train_scores), len(validate_scores))
-
-        intervals = train_count['interval'] if not str_intervals else PSI.intervals_to_str(train_count['interval'],
-                                                                                           round_num=round_num)
+        (
+            psi_scores,
+            total_psi,
+            expected_interval,
+            actual_interval,
+            expected_percentage,
+            actual_percentage,
+        ) = self.psi_score(expected_interval, actual_interval, len(train_scores), len(validate_scores))
+
+        intervals = (
+            train_count["interval"]
+            if not str_intervals
+            else PSI.intervals_to_str(train_count["interval"], round_num=round_num)
+        )
 
-        total_psi = EvalResult('total_psi', total_psi)
+        total_psi = EvalResult("total_psi", total_psi)
 
         if train_labels is None and validate_labels is None:
-            psi_table = EvalResult('psi_table', pd.DataFrame({
-                'psi_scores': psi_scores,
-                'expected_interval': expected_interval,
-                'actual_interval': actual_interval,
-                'expected_percentage': expected_percentage,
-                'actual_percentage': actual_percentage,
-                'interval': intervals
-            }))
+            psi_table = EvalResult(
+                "psi_table",
+                pd.DataFrame(
+                    {
+                        "psi_scores": psi_scores,
+                        "expected_interval": expected_interval,
+                        "actual_interval": actual_interval,
+                        "expected_percentage": expected_percentage,
+                        "actual_percentage": actual_percentage,
+                        "interval": intervals,
+                    }
+                ),
+            )
         else:
-            psi_table = EvalResult('psi_table', pd.DataFrame({
-                'psi_scores': psi_scores,
-                'expected_interval': expected_interval,
-                'actual_interval': actual_interval,
-                'expected_percentage': expected_percentage,
-                'actual_percentage': actual_percentage,
-                'train_pos_perc': train_pos_perc,
-                'validate_pos_perc': validate_pos_perc,
-                'interval': intervals
-            }))
+            psi_table = EvalResult(
+                "psi_table",
+                pd.DataFrame(
+                    {
+                        "psi_scores": psi_scores,
+                        "expected_interval": expected_interval,
+                        "actual_interval": actual_interval,
+                        "expected_percentage": expected_percentage,
+                        "actual_percentage": actual_percentage,
+                        "train_pos_perc": train_pos_perc,
+                        "validate_pos_perc": validate_pos_perc,
+                        "interval": intervals,
+                    }
+                ),
+            )
 
         return psi_table, total_psi
 
     @staticmethod
     def quantile_binning_and_count(scores, quantile_points):
         """
         left edge and right edge of last interval are closed
@@ -731,51 +765,56 @@
 
         last_interval_left = left_bounds.pop()
         last_interval_right = right_bounds.pop()
 
         bin_result_1, bin_result_2 = None, None
 
         if len(left_bounds) != 0 and len(right_bounds) != 0:
-            bin_result_1 = pd.cut(scores, pd.IntervalIndex.from_arrays(left_bounds, right_bounds, closed='left'))
+            bin_result_1 = pd.cut(scores, pd.IntervalIndex.from_arrays(left_bounds, right_bounds, closed="left"))
 
-        bin_result_2 = pd.cut(scores, pd.IntervalIndex.from_arrays([last_interval_left], [last_interval_right],
-                                                                   closed='both'))
+        bin_result_2 = pd.cut(
+            scores, pd.IntervalIndex.from_arrays([last_interval_left], [last_interval_right], closed="both")
+        )
 
         count1 = None if bin_result_1 is None else bin_result_1.value_counts().reset_index()
         count2 = bin_result_2.value_counts().reset_index()
 
         # if predict scores are the same, count1 will be None, only one interval exists
-        final_interval = list(count1['index']) + list(count2['index']) if count1 is not None else list(count2['index'])
+        final_interval = list(count1["index"]) + list(count2["index"]) if count1 is not None else list(count2["index"])
         final_count = list(count1[0]) + list(count2[0]) if count1 is not None else list(count2[0])
-        rs = {'interval': final_interval, 'count': final_count}
+        rs = {"interval": final_interval, "count": final_count}
 
         return rs
 
     @staticmethod
     def interval_psi_score(val):
         expected, actual = val[0], val[1]
         return (actual - expected) * np.log(actual / expected)
 
     @staticmethod
     def intervals_to_str(intervals, round_num=3):
         str_intervals = []
         for interval in intervals:
-            left_bound, right_bound = '[', ']'
-            if interval.closed == 'left':
-                right_bound = ')'
-            elif interval.closed == 'right':
-                left_bound = '('
-            str_intervals.append("{}{}, {}{}".format(left_bound, round(interval.left, round_num),
-                                                     round(interval.right, round_num), right_bound))
+            left_bound, right_bound = "[", "]"
+            if interval.closed == "left":
+                right_bound = ")"
+            elif interval.closed == "right":
+                left_bound = "("
+            str_intervals.append(
+                "{}{}, {}{}".format(
+                    left_bound, round(interval.left, round_num), round(interval.right, round_num), right_bound
+                )
+            )
 
         return str_intervals
 
     @staticmethod
-    def psi_score(expected_interval: np.ndarray, actual_interval: np.ndarray, expect_total_num, actual_total_num,
-                  debug=False):
+    def psi_score(
+        expected_interval: np.ndarray, actual_interval: np.ndarray, expect_total_num, actual_total_num, debug=False
+    ):
 
         expected_interval[expected_interval == 0] = 1e-6  # in case no overlap samples
 
         actual_interval[actual_interval == 0] = 1e-6  # in case no overlap samples
 
         expected_percentage = expected_interval / expect_total_num
         actual_percentage = actual_interval / actual_total_num
@@ -785,8 +824,8 @@
             print(actual_interval)
             print(expected_percentage)
             print(actual_percentage)
 
         psi_scores = list(map(PSI.interval_psi_score, zip(expected_percentage, actual_percentage)))
         psi_scores = np.array(psi_scores)
         total_psi = psi_scores.sum()
-        return psi_scores, total_psi, expected_interval, actual_interval, expected_percentage, actual_percentage
+        return psi_scores, total_psi, expected_interval, actual_interval, expected_percentage, actual_percentage
```

### Comparing `pyfate-2.0.0b0/fate/ml/evaluation/metric_base.py` & `pyfate-2.1.0/fate/ml/evaluation/metric_base.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,25 +1,40 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from typing import Dict
 from transformers import EvalPrediction
+from transformers.trainer_utils import PredictionOutput
 import pandas as pd
 import torch
 import numpy as np
 from typing import Union
 import json
 import logging
 
 logger = logging.getLogger(__name__)
 
 
-SINGLE_VALUE = 'single_value'
-TABLE_VALUE = 'table_value'
+SINGLE_VALUE = "single_value"
+TABLE_VALUE = "table_value"
 
 
 class EvalResult(object):
-
     def __init__(self, metric_name: str, result: Union[int, float, pd.DataFrame]):
         self.metric_name = metric_name
         assert isinstance(self.metric_name, str), "metric_name must be a string."
         if isinstance(result, (int, float)):
             self.result = float(result)
             self.result_type = SINGLE_VALUE
         elif isinstance(result, pd.DataFrame):
@@ -34,101 +49,102 @@
     def __repr__(self) -> str:
         return self.result.__repr__()
 
     def to_dict(self):
         return {
             "metric": self.metric_name,
             # "result_type": self.result_type,
-            "val": self.result.to_dict(orient='list') if self.result_type == TABLE_VALUE else self.result
+            "val": self.result.to_dict(orient="list") if self.result_type == TABLE_VALUE else self.result,
         }
 
     def to_json(self):
         if self.result_type == TABLE_VALUE:
-            return self.result.to_json(orient='split')
+            return self.result.to_json(orient="split")
         else:
             return json.dumps(self.to_dict())
-        
+
     def get_raw_data(self):
         return self.result
 
     def __dict__(self):
         return self.to_dict()
-    
 
-class Metric(object):
 
+class Metric(object):
     metric_name = None
 
     def __init__(self, *args, **kwargs):
         pass
 
     def __call__(self, predict, label, **kwargs) -> EvalResult:
         pass
 
     def to_np_format(self, data, flatten=True):
-
         if isinstance(data, list):
             ret = np.array(data)
         elif isinstance(data, torch.Tensor):
             ret = data.detach().cpu().numpy()
         elif isinstance(data, pd.Series) or isinstance(data, pd.DataFrame):
             ret = np.array(data.values.tolist())
         else:
             ret = data
-        
+
         if flatten:
             ret = ret.flatten()
-        
+
         return ret.astype(np.float64)
 
 
 class MetricEnsemble(object):
-
     def __init__(self, to_dict=True) -> None:
         self._metrics = []
         self._metric_suffix = set()
         self._to_dict = to_dict
 
     def add_metric(self, metric: Metric):
         self._metrics.append(metric)
-        return self        
-    
+        return self
+
     def _parse_input(self, eval_rs):
         if isinstance(eval_rs, EvalPrediction):
             # parse hugging face format
             predict = eval_rs.predictions
             label = eval_rs.label_ids
             input_ = eval_rs.inputs
 
+        elif isinstance(eval_rs, PredictionOutput):
+            predict = eval_rs.predictions
+            label = eval_rs.label_ids
+            input_ = None
+
         elif isinstance(eval_rs, tuple) and len(eval_rs) == 2:
             # conventional format
             predict, label = eval_rs
             input_ = None
         else:
-            raise ValueError('Unknown eval_rs format: {}. Expected input formats are either '
-                             'an instance of EvalPrediction or a 2-tuple (predict, label).'.format(type(eval_rs)))
+            raise ValueError(
+                "Unknown eval_rs format: {}. Expected input formats are either "
+                "an instance of EvalPrediction or a 2-tuple (predict, label).".format(type(eval_rs))
+            )
 
         return predict, label, input_
 
-    def __call__(self, eval_rs=None, predict=None, label=None, **kwargs) -> Dict:
-
+    def __call__(self, eval_rs=None, predict=None, label=None, **kwargs):
         metric_result = []
-        
+
         if eval_rs is not None:
             predict, label, input_ = self._parse_input(eval_rs)
 
         for metric in self._metrics:
             rs = metric(predict, label)
             if isinstance(rs, tuple):
                 new_rs = [r.to_dict() for r in rs]
                 rs = new_rs
             elif isinstance(rs, EvalResult):
                 rs = rs.to_dict()
             else:
-                raise ValueError('cannot parse metric result: {}'.format(rs))
+                raise ValueError("cannot parse metric result: {}".format(rs))
             metric_result.append(rs)
         return metric_result
 
-    def fit(self, eval_rs=None, predict=None, label=None, **kwargs) -> Dict:
+    def fit(self, eval_rs=None, predict=None, label=None, **kwargs):
         return self.__call__(eval_rs, predict, label, **kwargs)
-
-
```

### Comparing `pyfate-2.0.0b0/fate/ml/evaluation/tool.py` & `pyfate-2.1.0/fate/ml/evaluation/tool.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,59 +1,76 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import inspect
 from fate.ml.evaluation import classification as classi
 from fate.ml.evaluation import regression as reg
 from fate.ml.evaluation.metric_base import Metric, MetricEnsemble
 
 
 def get_metric_names(modules):
     result = {}
 
     for module in modules:
         for name, obj in inspect.getmembers(module):
             if inspect.isclass(obj):
-                if hasattr(obj, 'metric_name') and issubclass(obj, Metric):
-                    metric_name = getattr(obj, 'metric_name')
+                if hasattr(obj, "metric_name") and issubclass(obj, Metric):
+                    metric_name = getattr(obj, "metric_name")
                     if metric_name is not None:
                         result[metric_name] = obj
 
     return result
 
 
 def all_available_metrics():
     return get_metric_names([classi, reg])
 
 
 def get_single_val_binary_metrics(threshold=0.5):
-
     binary_ensembles = MetricEnsemble()
-    binary_ensembles.add_metric(classi.AUC()).add_metric(classi.BinaryAccuracy(threshold=threshold)).add_metric(classi.BinaryF1Score(threshold=threshold))
-    binary_ensembles.add_metric(classi.BinaryPrecision(threshold=threshold)).add_metric(classi.BinaryRecall(threshold=threshold))
+    binary_ensembles.add_metric(classi.AUC()).add_metric(classi.BinaryAccuracy(threshold=threshold)).add_metric(
+        classi.BinaryF1Score(threshold=threshold)
+    )
+    binary_ensembles.add_metric(classi.BinaryPrecision(threshold=threshold)).add_metric(
+        classi.BinaryRecall(threshold=threshold)
+    )
     return binary_ensembles
 
 
 def get_binary_metrics():
-
     binary_ensembles = MetricEnsemble()
     binary_ensembles.add_metric(classi.AUC()).add_metric(classi.KS()).add_metric(classi.ConfusionMatrix())
     binary_ensembles.add_metric(classi.Gain()).add_metric(classi.Lift())
     binary_ensembles.add_metric(classi.BiClassPrecisionTable()).add_metric(classi.BiClassRecallTable())
     binary_ensembles.add_metric(classi.BiClassAccuracyTable()).add_metric(classi.FScoreTable())
     return binary_ensembles
 
 
 def get_multi_metrics():
-    
     multi_ensembles = MetricEnsemble()
-    multi_ensembles.add_metric(classi.MultiAccuracy()).add_metric(classi.MultiPrecision()).add_metric(classi.MultiRecall())
-    
+    multi_ensembles.add_metric(classi.MultiAccuracy()).add_metric(classi.MultiPrecision()).add_metric(
+        classi.MultiRecall()
+    )
+
     return multi_ensembles
 
 
 def get_regression_metrics():
-    
     regression_ensembles = MetricEnsemble()
     regression_ensembles.add_metric(reg.RMSE()).add_metric(reg.MAE()).add_metric(reg.MSE()).add_metric(reg.R2Score())
     return regression_ensembles
 
 
 def get_special_metrics():
     # metrics that need special input format like PSI
@@ -65,9 +82,11 @@
 def get_specified_metrics(metric_names: list):
     ensembles = MetricEnsemble()
     available_metrics = get_metric_names([classi, reg])
     for metric_name in metric_names:
         if metric_name in available_metrics:
             ensembles.add_metric(get_metric_names([classi, reg])[metric_name]())
         else:
-            raise ValueError(f"metric {metric_name} is not supported yet, supported metrics are \n {list(available_metrics.keys())}")
-    return ensembles
+            raise ValueError(
+                f"metric {metric_name} is not supported yet, supported metrics are \n {list(available_metrics.keys())}"
+            )
+    return ensembles
```

### Comparing `pyfate-2.0.0b0/fate/ml/feature_binning/__init__.py` & `pyfate-2.1.0/fate/ml/utils/label_alignment.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,21 @@
 #
-#  Copyright 2023 The FATE Authors. All Rights Reserved.
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
 #
 #  Licensed under the Apache License, Version 2.0 (the "License");
 #  you may not use this file except in compliance with the License.
 #  You may obtain a copy of the License at
 #
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-from .hetero_feature_binning import HeteroBinningModuleGuest, HeteroBinningModuleHost
+from fate.arch import Context
+
+
+class LabelAilignment(object):
+    def __init__(self) -> None:
+        pass
```

### Comparing `pyfate-2.0.0b0/fate/ml/feature_binning/hetero_feature_binning.py` & `pyfate-2.1.0/fate/ml/feature_binning/hetero_feature_binning.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,45 +23,46 @@
 from ..abc.module import HeteroModule, Module
 
 logger = logging.getLogger(__name__)
 
 
 class HeteroBinningModuleGuest(HeteroModule):
     def __init__(
-            self,
-            method="quantile",
-            n_bins=10,
-            split_pt_dict=None,
-            bin_col=None,
-            transform_method=None,
-            category_col=None,
-            local_only=False,
-            error_rate=1e-6,
-            adjustment_factor=0.5,
+        self,
+        method="quantile",
+        n_bins=10,
+        split_pt_dict=None,
+        bin_col=None,
+        transform_method=None,
+        category_col=None,
+        local_only=False,
+        error_rate=1e-6,
+        adjustment_factor=0.5,
     ):
         self.method = method
         self.bin_col = bin_col
         self.category_col = category_col
         self.n_bins = n_bins
         self._federation_bin_obj = None
         # param check
         if self.method in ["quantile", "bucket", "manual"]:
             self._bin_obj = StandardBinning(
                 method, n_bins, split_pt_dict, bin_col, transform_method, category_col, error_rate, adjustment_factor
             )
         else:
             raise ValueError(f"{self.method} binning method not supported, please check")
         self.local_only = local_only
+        self.column_anonymous_map = None
 
     def set_transform_method(self, transform_method):
         self._bin_obj.transform_method = transform_method
 
     def fit(self, ctx: Context, train_data, validate_data=None) -> None:
         logger.info("Enter HeteroBinning fit.")
-
+        self.column_anonymous_map = dict(zip(train_data.schema.columns, train_data.schema.anonymous_columns))
         train_data_binarized_label = train_data.label.get_dummies()
         label_count = train_data_binarized_label.shape[1]
         if label_count > 2:
             raise ValueError(
                 f"More than 2 classes found in label column. "
                 f"HeteroBinning currently only supports binary data. Please check."
             )
@@ -84,27 +85,27 @@
         ctx.hosts.put("enc_y", encryptor.encrypt_tensor(label_tensor))
         ctx.hosts.put("pk", pk)
         ctx.hosts.put("evaluator", evaluator)
         ctx.hosts.put("coder", coder)
         host_col_bin = ctx.hosts.get("anonymous_col_bin")
         host_event_non_event_count = ctx.hosts.get("event_non_event_count")
         host_bin_sizes = ctx.hosts.get("feature_bin_sizes")
-        for i, (col_bin_list, bin_sizes, en_host_count_res) in enumerate(zip(host_col_bin,
-                                                                             host_bin_sizes,
-                                                                             host_event_non_event_count)):
-            host_event_non_event_count_hist = en_host_count_res.decrypt({"event_count": sk,
-                                                                         "non_event_count": sk},
-                                                                        {"event_count": (coder, None),
-                                                                         "non_event_count": (coder, None)})
+        for i, (col_bin_list, bin_sizes, en_host_count_res) in enumerate(
+            zip(host_col_bin, host_bin_sizes, host_event_non_event_count)
+        ):
+            host_event_non_event_count_hist = en_host_count_res.decrypt(
+                {"event_count": sk, "non_event_count": sk},
+                {"event_count": (coder, None), "non_event_count": (coder, None)},
+            )
             host_event_non_event_count_hist = host_event_non_event_count_hist.reshape(bin_sizes)
-            summary_metrics, _ = self._bin_obj.compute_all_col_metrics(host_event_non_event_count_hist,
-                                                                       col_bin_list)
+            summary_metrics, _ = self._bin_obj.compute_all_col_metrics(host_event_non_event_count_hist, col_bin_list)
             self._bin_obj.set_host_metrics(ctx.hosts[i], summary_metrics)
 
     def transform(self, ctx: Context, test_data):
+        self.column_anonymous_map = dict(zip(test_data.schema.columns, test_data.schema.anonymous_columns))
         transformed_data = self._bin_obj.transform(ctx, test_data)
         return transformed_data
 
     def get_model(self):
         model_info = self._bin_obj.to_model()
         model = {
             "data": model_info,
@@ -112,14 +113,15 @@
                 "method": self.method,
                 "metrics": ["iv"] if model_info.get("metrics_summary") else [],
                 "local_only": self.local_only,
                 "bin_col": self.bin_col,
                 "category_col": self.category_col,
                 "model_type": "binning",
                 "n_bins": self.n_bins,
+                "column_anonymous_map": self.column_anonymous_map,
             },
         }
         return model
 
     def restore(self, model):
         self._bin_obj.restore(model)
 
@@ -133,41 +135,44 @@
         )
         bin_obj.restore(model["data"])
         return bin_obj
 
 
 class HeteroBinningModuleHost(HeteroModule):
     def __init__(
-            self,
-            method="quantile",
-            n_bins=10,
-            split_pt_dict=None,
-            bin_col=None,
-            transform_method=None,
-            category_col=None,
-            local_only=False,
-            error_rate=1e-6,
-            adjustment_factor=0.5,
+        self,
+        method="quantile",
+        n_bins=10,
+        split_pt_dict=None,
+        bin_col=None,
+        transform_method=None,
+        category_col=None,
+        local_only=False,
+        error_rate=1e-6,
+        adjustment_factor=0.5,
     ):
         self.method = method
         self.n_bins = n_bins
         self._federation_bin_obj = None
         if self.method in ["quantile", "bucket", "manual"]:
             self._bin_obj = StandardBinning(
                 method, n_bins, split_pt_dict, bin_col, transform_method, category_col, error_rate, adjustment_factor
             )
         self.local_only = local_only
         self.bin_col = bin_col
         self.category_col = category_col
+        self.anonymous_col_bin = None
+        self.column_anonymous_map = None
 
     def set_transform_method(self, new_transform_method):
         self._bin_obj.transform_method = new_transform_method
 
     def fit(self, ctx: Context, train_data, validate_data=None) -> None:
         logger.info("Enter HeteroBinning fit.")
+        self.column_anonymous_map = dict(zip(train_data.schema.columns, train_data.schema.anonymous_columns))
         self._bin_obj.fit(ctx, train_data)
 
     def compute_metrics(self, ctx: Context, binned_data):
         if not self.local_only:
             self.compute_federated_metrics(ctx, binned_data)
 
     def compute_federated_metrics(self, ctx: Context, binned_data):
@@ -193,48 +198,50 @@
             columns=dict(zip(to_compute_data.schema.columns, to_compute_data.schema.anonymous_columns))
         )
         hist_targets = binned_data.create_frame()
         hist_targets["event_count"] = encrypt_y
         hist_targets["non_event_count"] = 1
         dtypes = hist_targets.dtypes
 
-        hist_schema = {"event_count": {"type": "ciphertext",
-                                       "stride": 1,
-                                       "pk": pk,
-                                       "evaluator": evaluator,
-                                       "coder": coder,
-                                       "dtype": dtypes["event_count"],
-                                       },
-                       "non_event_count": {"type": "plaintext",
-                                           "stride": 1,
-                                           "dtype": dtypes["non_event_count"]}
-                       }
-        hist = HistogramBuilder(num_node=1,
-                                feature_bin_sizes=feature_bin_sizes,
-                                value_schemas=hist_schema,
-                                enable_cumsum=False)
-        event_non_event_count_hist = to_compute_data.distributed_hist_stat(histogram_builder=hist,
-                                                                           targets=hist_targets)
+        hist_schema = {
+            "event_count": {
+                "type": "ciphertext",
+                "stride": 1,
+                "pk": pk,
+                "evaluator": evaluator,
+                "coder": coder,
+                "dtype": dtypes["event_count"],
+            },
+            "non_event_count": {"type": "plaintext", "stride": 1, "dtype": dtypes["non_event_count"]},
+        }
+        hist = HistogramBuilder(
+            num_node=1, feature_bin_sizes=feature_bin_sizes, value_schemas=hist_schema, enable_cumsum=False
+        )
+        event_non_event_count_hist = to_compute_data.distributed_hist_stat(
+            histogram_builder=hist, targets=hist_targets
+        )
         event_non_event_count_hist.i_sub_on_key("non_event_count", "event_count")
         ctx.guest.put("event_non_event_count", (event_non_event_count_hist))
         ctx.guest.put("feature_bin_sizes", feature_bin_sizes)
 
     def transform(self, ctx: Context, test_data):
+        self.column_anonymous_map = dict(zip(test_data.schema.columns, test_data.schema.anonymous_columns))
         return self._bin_obj.transform(ctx, test_data)
 
     def get_model(self):
         model_info = self._bin_obj.to_model()
         model = {
             "data": model_info,
             "meta": {
                 "method": self.method,
                 "bin_col": self.bin_col,
                 "category_col": self.category_col,
                 "n_bins": self.n_bins,
                 "model_type": "binning",
+                "column_anonymous_map": self.column_anonymous_map,
             },
         }
         return model
 
     def restore(self, model):
         self._bin_obj.restore(model)
 
@@ -370,31 +377,29 @@
             for col in self.category_col:
                 category_bin_size = binned_data[col].get_dummies().shape[1]
                 feature_bin_sizes.append(category_bin_size)
         hist_targets = binned_data.create_frame()
         hist_targets["event_count"] = binned_data.label
         hist_targets["non_event_count"] = 1
         dtypes = hist_targets.dtypes
-        hist_schema = {"event_count": {"type": "plaintext",
-                                       "stride": 1,
-                                       "dtype": dtypes["event_count"]},
-                       "non_event_count": {"type": "plaintext",
-                                           "stride": 1,
-                                           "dtype": dtypes["non_event_count"]}
-                       }
-        hist = HistogramBuilder(num_node=1,
-                                feature_bin_sizes=feature_bin_sizes,
-                                value_schemas=hist_schema,
-                                enable_cumsum=False)
-        event_non_event_count_hist = to_compute_data.distributed_hist_stat(histogram_builder=hist,
-                                                                           targets=hist_targets)
+        hist_schema = {
+            "event_count": {"type": "plaintext", "stride": 1, "dtype": dtypes["event_count"]},
+            "non_event_count": {"type": "plaintext", "stride": 1, "dtype": dtypes["non_event_count"]},
+        }
+        hist = HistogramBuilder(
+            num_node=1, feature_bin_sizes=feature_bin_sizes, value_schemas=hist_schema, enable_cumsum=False
+        )
+        event_non_event_count_hist = to_compute_data.distributed_hist_stat(
+            histogram_builder=hist, targets=hist_targets
+        )
         event_non_event_count_hist.i_sub_on_key("non_event_count", "event_count")
         event_non_event_count_hist = event_non_event_count_hist.decrypt({}, {}).reshape(feature_bin_sizes)
-        self._metrics_summary, self._woe_dict = self.compute_all_col_metrics(event_non_event_count_hist,
-                                                                             to_compute_col)
+        self._metrics_summary, self._woe_dict = self.compute_all_col_metrics(
+            event_non_event_count_hist, to_compute_col
+        )
 
     def transform(self, ctx: Context, binned_data):
         logger.debug(f"Given transform method: {self.transform_method}.")
         if self.transform_method == "bin_idx" and self._bin_idx_dict:
             return binned_data
         elif self.transform_method == "woe":
             if ctx.is_on_host:
```

### Comparing `pyfate-2.0.0b0/fate/ml/feature_selection/__init__.py` & `pyfate-2.1.0/fate/ml/feature_selection/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/feature_selection/hetero_feature_selection.py` & `pyfate-2.1.0/fate/ml/feature_selection/hetero_feature_selection.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 import copy
-import json
 import logging
 import math
 import random
 
 import numpy as np
 import pandas as pd
 
@@ -117,21 +116,21 @@
         return transformed_data
 
     def get_model(self):
         # all selection obj need to be recorded for display of cascade order
         selection_obj_list = []
         for selection_obj in self._selection_obj:
             selection_obj_list.append(selection_obj.to_model())
-        data = {"selection_obj_list": json.dumps(selection_obj_list), "inner_method": self._inner_method}
+        data = {"selection_obj_list": selection_obj_list, "inner_method": self._inner_method}
         meta = {"method": self.method, "select_col": self.select_col, "keep_one": self.keep_one}
         return {"data": data, "meta": meta}
 
     def restore(self, model):
         selection_obj_list = []
-        selection_obj_model_list = json.loads(model["selection_obj_list"])
+        selection_obj_model_list = model["selection_obj_list"]
         for i, selection_model in enumerate(selection_obj_model_list):
             if selection_model["method"] in ["manual"]:
                 selection_obj = ManualSelection(method=self._inner_method[i])
             else:
                 selection_obj = StandardSelection(method=self._inner_method[i])
             selection_obj.restore(selection_model)
             selection_obj_list.append(selection_obj)
@@ -238,21 +237,21 @@
 
     def get_model(self):
         # all selection history need to be recorded for display
         selection_obj_list = []
         for selection_obj in self._selection_obj:
             selection_obj_list.append(selection_obj.to_model())
 
-        data = {"selection_obj_list": json.dumps(selection_obj_list), "inner_method": self._inner_method}
+        data = {"selection_obj_list": selection_obj_list, "inner_method": self._inner_method}
         meta = {"method": self.method, "select_col": self.select_col, "keep_one": self.keep_one}
         return {"data": data, "meta": meta}
 
     def restore(self, model):
         selection_obj_list = []
-        selection_obj_model_list = json.loads(model["selection_obj_list"])
+        selection_obj_model_list = model["selection_obj_list"]
         for i, selection_model in enumerate(selection_obj_model_list):
             if selection_model["method"] in ["manual"]:
                 selection_obj = ManualSelection(method=self._inner_method[i])
             else:
                 selection_obj = StandardSelection(method=self._inner_method[i])
             selection_obj.restore(selection_model)
             selection_obj_list.append(selection_obj)
```

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/__init__.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/arbiter.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/arbiter.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/guest.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/guest.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,16 +27,23 @@
 )
 from fate.ml.utils._optimizer import LRScheduler, Optimizer
 
 logger = logging.getLogger(__name__)
 
 
 class CoordinatedLinRModuleGuest(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer_param=None, learning_rate_param=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer_param=None,
+        learning_rate_param=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.batch_size = batch_size
         self.optimizer_param = optimizer_param
         self.learning_rate_param = learning_rate_param
         self.init_param = init_param
         self.floating_point_precision = floating_point_precision
 
@@ -63,15 +70,15 @@
             )
             estimator = CoordinatedLinREstimatorGuest(
                 epochs=self.epochs,
                 batch_size=self.batch_size,
                 optimizer=optimizer,
                 learning_rate_scheduler=lr_scheduler,
                 init_param=self.init_param,
-                floating_point_precision=self.floating_point_precision
+                floating_point_precision=self.floating_point_precision,
             )
             self.estimator = estimator
         encryptor = ctx.arbiter("encryptor").get()
         self.estimator.fit_model(ctx, encryptor, train_data, validate_data)
 
     def predict(self, ctx, test_data):
         prob = self.estimator.predict(ctx, test_data)
@@ -93,43 +100,51 @@
     @classmethod
     def from_model(cls, model) -> "CoordinatedLinRModuleGuest":
         linr = CoordinatedLinRModuleGuest(
             optimizer_param=model["meta"]["optimizer_param"],
             learning_rate_param=model["meta"]["learning_rate_param"],
             batch_size=model["meta"]["batch_size"],
             init_param=model["meta"]["init_param"],
-            floating_point_precision=model["meta"]["floating_point_precision"]
+            floating_point_precision=model["meta"]["floating_point_precision"],
         )
         estimator = CoordinatedLinREstimatorGuest(
             epochs=model["meta"]["epochs"],
             batch_size=model["meta"]["batch_size"],
             init_param=model["meta"]["init_param"],
-            floating_point_precision=model["meta"]["floating_point_precision"]
+            floating_point_precision=model["meta"]["floating_point_precision"],
         )
         estimator.restore(model["data"]["estimator"])
         linr.estimator = estimator
 
         return linr
 
 
 class CoordinatedLinREstimatorGuest(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer=None, learning_rate_scheduler=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer=None,
+        learning_rate_scheduler=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.batch_size = batch_size
         self.optimizer = optimizer
         self.lr_scheduler = learning_rate_scheduler
         self.init_param = init_param
         self.floating_point_precision = floating_point_precision
-        self._fixpoint_precision = 2 ** floating_point_precision
+        self._fixpoint_precision = 2**floating_point_precision
 
         self.w = None
         self.start_epoch = 0
         self.end_epoch = -1
         self.is_converged = False
+        self.header = None
 
     def asynchronous_compute_gradient(self, batch_ctx, encryptor, w, X, Y, weight):
         h = X.shape[0]
         Xw = torch.matmul(X, w.detach())
         half_d = Xw - Y
         if weight:
             half_d = half_d * weight
@@ -182,15 +197,16 @@
             g = 1 / (self._fixpoint_precision * h) * g
         else:
             g = 1 / h * torch.matmul(X.T, d)
         return g
 
     def fit_model(self, ctx, encryptor, train_data, validate_data=None):
         coef_count = train_data.shape[1]
-        logger.debug(f"init param: {self.init_param}")
+        self.header = train_data.schema.columns.to_list()
+        # logger.debug(f"init param: {self.init_param}")
         if self.init_param.get("fit_intercept"):
             logger.debug(f"add intercept to train data")
             train_data["intercept"] = 1.0
         w = self.w
         if self.w is None:
             w = initialize_param(coef_count, **self.init_param)
             self.optimizer.init_optimizer(model_parameter_length=w.size()[0])
@@ -253,14 +269,15 @@
             "param": param,
             # "intercept": intercept,
             "optimizer": self.optimizer.state_dict(),
             "lr_scheduler": self.lr_scheduler.state_dict(),
             "end_epoch": self.end_epoch,
             "is_converged": self.is_converged,
             "fit_intercept": self.init_param.get("fit_intercept"),
+            "header": self.header,
         }
 
     def restore(self, model):
         """w = model["w"]
         if model["fit_intercept"]:
             w.append(model["intercept"])
         self.w = torch.tensor(w)
```

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_linr/host.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_linr/host.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,23 @@
 )
 from fate.ml.utils._optimizer import LRScheduler, Optimizer
 
 logger = logging.getLogger(__name__)
 
 
 class CoordinatedLinRModuleHost(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer_param=None, learning_rate_param=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer_param=None,
+        learning_rate_param=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.optimizer_param = optimizer_param
         self.learning_rate_param = learning_rate_param
         self.batch_size = batch_size
         self.init_param = init_param or {}
         self.init_param["fit_intercept"] = False
         self.floating_point_precision = 23
@@ -64,15 +71,15 @@
             )
             estimator = CoordinatedLinREstimatorHost(
                 epochs=self.epochs,
                 batch_size=self.batch_size,
                 optimizer=optimizer,
                 learning_rate_scheduler=lr_scheduler,
                 init_param=self.init_param,
-                floating_point_precision=self.floating_point_precision
+                floating_point_precision=self.floating_point_precision,
             )
             self.estimator = estimator
 
         self.estimator.fit_model(ctx, encryptor, train_data, validate_data)
 
     def predict(self, ctx, test_data):
         self.estimator.predict(ctx, test_data)
@@ -94,43 +101,51 @@
     def from_model(cls, model) -> "CoordinatedLinRModuleHost":
         linr = CoordinatedLinRModuleHost(
             optimizer_param=model["meta"]["optimizer_param"],
             learning_rate_param=model["meta"]["learning_rate_param"],
             epochs=model["meta"]["epochs"],
             batch_size=model["meta"]["batch_size"],
             init_param=model["meta"]["init_param"],
-            floating_point_precision=model["meta"]["floating_point_precision"]
+            floating_point_precision=model["meta"]["floating_point_precision"],
         )
         estimator = CoordinatedLinREstimatorHost(
             epochs=model["meta"]["epochs"],
             batch_size=model["meta"]["batch_size"],
             init_param=model["meta"]["init_param"],
-            floating_point_precision=model["meta"]["floating_point_precision"]
+            floating_point_precision=model["meta"]["floating_point_precision"],
         )
         estimator.restore(model["data"]["estimator"])
         linr.estimator = estimator
 
         return linr
 
 
 class CoordinatedLinREstimatorHost(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer=None, learning_rate_scheduler=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer=None,
+        learning_rate_scheduler=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.optimizer = optimizer
         self.lr_scheduler = learning_rate_scheduler
         self.batch_size = batch_size
         self.init_param = init_param
         self.floating_point_precision = floating_point_precision
-        self._fixpoint_precision = 2 ** floating_point_precision
+        self._fixpoint_precision = 2**floating_point_precision
 
         self.w = None
         self.start_epoch = 0
         self.end_epoch = -1
         self.is_converged = False
+        self.header = None
 
     def asynchronous_compute_gradient(self, batch_ctx, encryptor, w, X):
         h = X.shape[0]
         Xw_h = torch.matmul(X, w.detach())
         batch_ctx.guest.put("Xw_h", encryptor.encrypt_tensor(Xw_h, obfuscate=True))
         half_g = torch.matmul(X.T, Xw_h)
         guest_half_d = batch_ctx.guest.get("half_d")
@@ -160,14 +175,15 @@
             g = torch.matmul(torch.encode_as_int_f(X.T, self.floating_point_precision), d)
             g = 1 / (self._fixpoint_precision * h) * g
         else:
             g = 1 / h * torch.matmul(X.T, d)
         return g
 
     def fit_model(self, ctx: Context, encryptor, train_data, validate_data=None) -> None:
+        self.header = train_data.schema.columns.to_list()
         batch_loader = DataLoader(train_data, ctx=ctx, batch_size=self.batch_size, mode="hetero", role="host")
 
         coef_count = train_data.shape[1]
         w = self.w
         if self.w is None:
             w = initialize_param(coef_count, **self.init_param)
             self.optimizer.init_optimizer(model_parameter_length=w.size()[0])
@@ -217,14 +233,15 @@
         param = serialize_param(self.w, False)
         return {
             "param": param,
             "optimizer": self.optimizer.state_dict(),
             "lr_scheduler": self.lr_scheduler.state_dict(),
             "end_epoch": self.end_epoch,
             "is_converged": self.is_converged,
+            "header": self.header,
         }
 
     def restore(self, model):
         # self.w = torch.tensor(model["w"])
         self.w = deserialize_param(model["param"], False)
         self.optimizer = Optimizer()
         self.lr_scheduler = LRScheduler()
```

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/__init__.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/arbiter.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/arbiter.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/guest.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/guest.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,15 +37,15 @@
         self,
         epochs=None,
         batch_size=None,
         optimizer_param=None,
         learning_rate_param=None,
         init_param=None,
         threshold=0.5,
-            floating_point_precision=23
+        floating_point_precision=23,
     ):
         self.epochs = epochs
         self.batch_size = batch_size
         self.learning_rate_param = learning_rate_param
         self.optimizer_param = optimizer_param
         self.init_param = init_param
         self.threshold = threshold
@@ -103,15 +103,15 @@
                     )
                     single_estimator = CoordinatedLREstimatorGuest(
                         epochs=self.epochs,
                         batch_size=self.batch_size,
                         optimizer=optimizer,
                         learning_rate_scheduler=lr_scheduler,
                         init_param=self.init_param,
-                        floating_point_precision=self.floating_point_precision
+                        floating_point_precision=self.floating_point_precision,
                     )
                 else:
                     # warm start
                     logger.info("estimator is not none, will train with warm start")
                     # single_estimator = self.estimator[self.labels.index(labels[i])]
                     single_estimator = self.estimator[i]
                     single_estimator.epochs = self.epochs
@@ -137,15 +137,15 @@
                 )
                 single_estimator = CoordinatedLREstimatorGuest(
                     epochs=self.epochs,
                     batch_size=self.batch_size,
                     optimizer=optimizer,
                     learning_rate_scheduler=lr_scheduler,
                     init_param=self.init_param,
-                    floating_point_precision=self.floating_point_precision
+                    floating_point_precision=self.floating_point_precision,
                 )
             else:
                 logger.info("estimator is not none, will train with warm start")
                 single_estimator = self.estimator
                 single_estimator.epochs = self.epochs
                 single_estimator.batch_size = self.batch_size
             train_data_fit = train_data.copy()
@@ -234,28 +234,36 @@
             estimator.restore(all_estimator)
             lr.estimator = estimator
 
         return lr
 
 
 class CoordinatedLREstimatorGuest(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer=None, learning_rate_scheduler=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer=None,
+        learning_rate_scheduler=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.batch_size = batch_size
         self.optimizer = optimizer
         self.lr_scheduler = learning_rate_scheduler
         self.init_param = init_param
         self.floating_point_precision = floating_point_precision
-        self._fixpoint_precision = 2 ** floating_point_precision
+        self._fixpoint_precision = 2**floating_point_precision
 
         self.w = None
         self.start_epoch = 0
         self.end_epoch = -1
         self.is_converged = False
+        self.header = None
 
     def asynchronous_compute_gradient(self, batch_ctx, encryptor, w, X, Y, weight):
         h = X.shape[0]
         # logger.info(f"h: {h}")
         Xw = torch.matmul(X, w.detach())
         half_d = 0.25 * Xw - 0.5 * Y
         if weight:
@@ -278,15 +286,15 @@
         if self.optimizer.l1_penalty or self.optimizer.l2_penalty:
             loss_norm = self.optimizer.loss_norm(w)
             loss += loss_norm
 
         loss += torch.matmul((1 / h * Xw).T, Xw_h) - torch.matmul((2 / h * Y).T, Xw_h)
 
         for Xw2_h in batch_ctx.hosts.get("Xw2_h"):
-            loss += 0.125 / h * Xw2_h
+            loss += 2 / h * Xw2_h
         h_loss_list = batch_ctx.hosts.get("h_loss")
         for h_loss in h_loss_list:
             if h_loss is not None:
                 loss += h_loss
 
         batch_ctx.arbiter.put(loss=loss)
         # gradient
@@ -321,14 +329,16 @@
         """
         l(w) = 1/h * (log(2) - 0.5 * y * xw + 0.125 * (wx)^2)
         l(w) = 1/h * (0.25 * xw - 0.5 * y)x = 1/h * dx
         where d = 0.25(xw - 2y)
         loss = log2 - (1/N)*0.5*ywx + (1/N)*0.125*[(Wg*Xg)^2 + (Wh*Xh)^2 + 2 * (Wg*Xg * Wh*Xh)]
         """
         coef_count = train_data.shape[1]
+        self.header = train_data.schema.columns.to_list()
+
         if self.init_param.get("fit_intercept"):
             train_data["intercept"] = 1.0
 
         w = self.w
         if w is None:
             w = initialize_param(coef_count, **self.init_param)
 
@@ -397,14 +407,15 @@
             # "intercept": intercept,
             "param": param,
             "optimizer": self.optimizer.state_dict(),
             "lr_scheduler": self.lr_scheduler.state_dict(),
             "end_epoch": self.end_epoch,
             "is_converged": self.is_converged,
             "fit_intercept": self.init_param.get("fit_intercept"),
+            "header": self.header,
         }
 
     def restore(self, model):
         self.w = deserialize_param(model["param"], model["fit_intercept"])
         self.optimizer = Optimizer()
         self.lr_scheduler = LRScheduler()
         self.optimizer.load_state_dict(model["optimizer"])
```

### Comparing `pyfate-2.0.0b0/fate/ml/glm/hetero/coordinated_lr/host.py` & `pyfate-2.1.0/fate/ml/glm/hetero/coordinated_lr/host.py`

 * *Files 6% similar despite different names*

```diff
@@ -27,16 +27,23 @@
 )
 from fate.ml.utils._optimizer import LRScheduler, Optimizer
 
 logger = logging.getLogger(__name__)
 
 
 class CoordinatedLRModuleHost(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer_param=None, learning_rate_param=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer_param=None,
+        learning_rate_param=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.learning_rate_param = learning_rate_param
         self.optimizer_param = optimizer_param
         self.batch_size = batch_size
         self.init_param = init_param
         self.floating_point_precision = floating_point_precision
 
@@ -195,28 +202,36 @@
             lr.estimator = estimator
         logger.info(f"finish from model")
 
         return lr
 
 
 class CoordinatedLREstimatorHost(HeteroModule):
-    def __init__(self, epochs=None, batch_size=None, optimizer=None, learning_rate_scheduler=None, init_param=None,
-                 floating_point_precision=23):
+    def __init__(
+        self,
+        epochs=None,
+        batch_size=None,
+        optimizer=None,
+        learning_rate_scheduler=None,
+        init_param=None,
+        floating_point_precision=23,
+    ):
         self.epochs = epochs
         self.optimizer = optimizer
         self.lr_scheduler = learning_rate_scheduler
         self.batch_size = batch_size
         self.init_param = init_param
         self.floating_point_precision = floating_point_precision
-        self._fixpoint_precision = 2 ** floating_point_precision
+        self._fixpoint_precision = 2**floating_point_precision
 
         self.w = None
         self.start_epoch = 0
         self.end_epoch = -1
         self.is_converged = False
+        self.header = None
 
     def asynchronous_compute_gradient(self, batch_ctx, encryptor, w, X):
         h = X.shape[0]
         Xw_h = 0.25 * torch.matmul(X, w.detach())
         batch_ctx.guest.put("Xw_h", encryptor.encrypt_tensor(Xw_h, obfuscate=True))
 
         half_g = torch.matmul(X.T, Xw_h)
@@ -252,14 +267,16 @@
             g = 1 / (h * self._fixpoint_precision) * g
         else:
             g = 1 / h * torch.matmul(X.T, d)
         return g
 
     def fit_single_model(self, ctx: Context, encryptor, train_data, validate_data=None) -> None:
         coef_count = train_data.shape[1]
+        self.header = train_data.schema.columns.to_list()
+
         w = self.w
         if self.w is None:
             w = initialize_param(coef_count, **self.init_param)
             self.optimizer.init_optimizer(model_parameter_length=w.size()[0])
             self.lr_scheduler.init_scheduler(optimizer=self.optimizer.optimizer)
         batch_loader = DataLoader(train_data, ctx=ctx, batch_size=self.batch_size, mode="hetero", role="host")
         # if self.end_epoch >= 0:
@@ -302,14 +319,15 @@
         param = serialize_param(self.w, False)
         return {
             "param": param,
             "optimizer": self.optimizer.state_dict(),
             "lr_scheduler": self.lr_scheduler.state_dict(),
             "end_epoch": self.end_epoch,
             "is_converged": self.is_converged,
+            "header": self.header,
         }
 
     def restore(self, model):
         # self.w = torch.tensor(model["w"])
         self.w = deserialize_param(model["param"], False)
         self.optimizer = Optimizer()
         self.lr_scheduler = LRScheduler()
```

### Comparing `pyfate-2.0.0b0/fate/ml/glm/homo/__init__.py` & `pyfate-2.1.0/fate/ml/abc/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/glm/homo/lr/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/glm/homo/lr/client.py` & `pyfate-2.1.0/fate/ml/glm/homo/lr/client.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,30 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import torch.nn as nn
-from fate.arch import Context
 from fate.arch.dataframe import DataFrame
 from fate.ml.abc.module import HomoModule
 from fate.ml.utils.model_io import ModelIO
 from fate.arch import Context
 import logging
 import torch as t
-from fate.ml.nn.algo.homo.fedavg import FedAVGCLient, TrainingArguments, FedAVGArguments
+from fate.ml.nn.homo.fedavg import FedAVGClient, TrainingArguments, FedAVGArguments
 from transformers import default_data_collator
 import functools
 import tempfile
 from fate.ml.utils.predict_tools import array_to_predict_df
 from fate.ml.utils.predict_tools import MULTI, BINARY
 from fate.ml.nn.dataset.table import TableDataset
 from fate.ml.utils._optimizer import optimizer_factory, lr_scheduler_factory
@@ -40,213 +54,180 @@
     # normalize loss by the number of classes
     loss /= dim
 
     return loss
 
 
 class HomoLRModel(t.nn.Module):
-
     def __init__(self, feature_num, label_num=2, l1=0, bias=True) -> None:
         super().__init__()
-        assert feature_num >= 2 and isinstance(
-            feature_num, int), "feature_num must be int greater than 2"
-        assert label_num >= 1 and isinstance(
-            label_num, int), "label_num must be int greater than 1"
+        assert feature_num >= 2 and isinstance(feature_num, int), "feature_num must be int greater than 2"
+        assert label_num >= 1 and isinstance(label_num, int), "label_num must be int greater than 1"
         self.models = t.nn.ModuleList()
 
         if 2 >= label_num > 0:
-            self.models.append(
-                t.nn.Linear(feature_num, 1, bias=bias)
-            )
+            self.models.append(t.nn.Linear(feature_num, 1, bias=bias))
         else:
             # OVR Setting
             for i in range(label_num):
-                self.models.append(
-                    t.nn.Linear(feature_num, 1, bias=bias)
-                )
+                self.models.append(t.nn.Linear(feature_num, 1, bias=bias))
         self.sigmoid = t.nn.Sigmoid()
         self.softmax = t.nn.Softmax(dim=1)
         self.l1 = l1
 
     def forward(self, x, labels=None):
-
         if len(self.models) == 1:
             linear_out = self.models[0](x)
         else:
             linear_out = t.cat([model(x) for model in self.models], dim=1)
 
         ret_dict = {}
         linear_out = self.sigmoid(linear_out).reshape((-1, len(self.models)))
 
         if not self.training:
             if len(self.models) > 1:
                 linear_out = self.softmax(linear_out)
 
-        ret_dict['pred'] = linear_out
+        ret_dict["pred"] = linear_out
 
         if labels is not None:
             loss = homo_lr_loss(linear_out, labels, dim=len(self.models))
             if self.l1 != 0:
-                l1_regularization = t.tensor(0.)
+                l1_regularization = t.tensor(0.0)
                 for param in self.models.parameters():
                     l1_regularization += t.norm(param, 1)
                 loss += self.l1 * l1_regularization
-            ret_dict['loss'] = loss
+            ret_dict["loss"] = loss
 
         return ret_dict
 
     def to_dict(self):
         model_dict = {
             "feature_num": self.models[0].in_features,
             "label_num": len(self.models),
             # convert tensor to list
-            "state_dict": {k: v.tolist() for k, v in self.state_dict().items()}
+            "state_dict": {k: v.tolist() for k, v in self.state_dict().items()},
         }
         return model_dict
 
     @classmethod
     def from_dict(cls, model_dict):
         model = cls(model_dict["feature_num"], model_dict["label_num"])
-        model_state_dict = {
-            k: t.tensor(v) for k,
-            v in model_dict["state_dict"].items()}  # convert list back to tensor
+        model_state_dict = {k: t.tensor(v) for k, v in model_dict["state_dict"].items()}  # convert list back to tensor
         model.load_state_dict(model_state_dict)
         return model
 
 
-def init_model(model, method='random', fill_val=1.0):
-    if method == 'zeros':
+def init_model(model, method="random", fill_val=1.0):
+    if method == "zeros":
         init_fn = nn.init.zeros_
-    elif method == 'ones':
+    elif method == "ones":
         init_fn = nn.init.ones_
-    elif method == 'consts':
-        def init_fn(x): return nn.init.constant_(x, fill_val)
-    elif method == 'random':
+    elif method == "consts":
+
+        def init_fn(x):
+            return nn.init.constant_(x, fill_val)
+
+    elif method == "random":
         init_fn = nn.init.normal_
     else:
-        raise ValueError(
-            "Invalid method. Options are: 'zeros', 'ones', 'consts', 'random'")
+        raise ValueError("Invalid method. Options are: 'zeros', 'ones', 'consts', 'random'")
 
     for name, param in model.named_parameters():
-        if 'bias' in name:
+        if "bias" in name:
             # usually it's good practice to initialize biases to zero
             nn.init.zeros_(param)
         else:
             init_fn(param)
 
 
 # read model from model bytes
 def recover_torch_bytes(model_bytes):
-
     with tempfile.TemporaryFile() as f:
         f.write(model_bytes)
         f.seek(0)
         model_dict = t.load(f)
 
     return model_dict
 
 
 def get_torch_bytes(model_dict):
-
     with tempfile.TemporaryFile() as f:
         t.save(model_dict, f)
         f.seek(0)
         model_saved_bytes = f.read()
 
         return model_saved_bytes
 
 
-def update_params(new_params, default, name='optimizer'):
+def update_params(new_params, default, name="optimizer"):
     import copy
+
     params = copy.deepcopy(default)
     if not isinstance(new_params, dict):
-        raise ValueError(
-            "{} param dict must be a dict but got {}".format(
-                name, new_params))
+        raise ValueError("{} param dict must be a dict but got {}".format(name, new_params))
 
     def _update(default, new):
         for key in new.keys():
             if key in default:
                 default[key] = new[key]
 
     _update(params, new_params)
 
     return params
 
 
 DEFAULT_OPT_PARAM = {
-    'method': 'sgd',
-    'penalty': 'l2',
-    'alpha': 0.0,
-    'optimizer_params': {
-        'lr': 0.01,
-        'weight_decay': 0}}
-DEFAULT_INIT_PARAM = {
-    "method": "random",
-    "fill_val": 1.0,
-    "fit_intercept": True}
-DEFAULT_LR_SCHEDULER_PARAM = {
-    'method': 'constant',
-    'scheduler_params': {
-        'factor': 1.0}}
+    "method": "sgd",
+    "penalty": "l2",
+    "alpha": 0.0,
+    "optimizer_params": {"lr": 0.01, "weight_decay": 0},
+}
+DEFAULT_INIT_PARAM = {"method": "random", "fill_val": 1.0, "fit_intercept": True}
+DEFAULT_LR_SCHEDULER_PARAM = {"method": "constant", "scheduler_params": {"factor": 1.0}}
 
 
 class HomoLRClient(HomoModule):
-
     def __init__(
-            self,
-            epochs: int = 5,
-            batch_size: int = None,
-            optimizer_param={
-                'method': 'sgd',
-                'optimizer_params': {
-                    'lr': 0.01,
-                    'weight_decay': 0}},
-        learning_rate_scheduler={
-                'method': 'constant',
-                'scheduler_params': {
-                    'factor': 1.0}},
-            init_param={
-                "method": "random",
-                "fill_val": 1.0,
-                "fit_intercept": True},
-            threshold: float = 0.5,
-            ovr=False,
-            label_num=None,
+        self,
+        epochs: int = 5,
+        batch_size: int = None,
+        optimizer_param={"method": "sgd", "optimizer_params": {"lr": 0.01, "weight_decay": 0}},
+        learning_rate_scheduler={"method": "constant", "scheduler_params": {"factor": 1.0}},
+        init_param={"method": "random", "fill_val": 1.0, "fit_intercept": True},
+        threshold: float = 0.5,
+        ovr=False,
+        label_num=None,
     ) -> None:
-
         super().__init__()
         self.df_schema = None
         self.train_set = None
         self.validate_set = None
         self.predict_set = None
 
         # set vars
         self.max_iter = epochs
         self.batch_size = batch_size
-        self.optimizer_param = update_params(
-            optimizer_param, DEFAULT_OPT_PARAM, name='optimizer')
+        self.optimizer_param = update_params(optimizer_param, DEFAULT_OPT_PARAM, name="optimizer")
         self.learning_rate_param = update_params(
-            learning_rate_scheduler,
-            DEFAULT_LR_SCHEDULER_PARAM,
-            name='learning_rate_scheduler')
-        self.init_param = update_params(
-            init_param, DEFAULT_INIT_PARAM, name='init_param')
+            learning_rate_scheduler, DEFAULT_LR_SCHEDULER_PARAM, name="learning_rate_scheduler"
+        )
+        self.init_param = update_params(init_param, DEFAULT_INIT_PARAM, name="init_param")
         self.threshold = threshold
         self.run_ovr = False
         self.train_feature_num = None
         self.validate_feature_num = None
         self.ovr = ovr
         self.label_num = label_num
 
         if self.ovr:
             if self.label_num is None or self.label_num < 2:
                 raise ValueError(
-                    "label_num must be greater than 2 when ovr is True, but got {}".format(
-                        self.label_num))
+                    "label_num must be greater than 2 when ovr is True, but got {}".format(self.label_num)
+                )
 
         # models & optimizer & schduler
         self.model = None
         self.optimizer = None
         self.scheduler = None
         self.optimizer_state_dict = None
         self.trainer = None
@@ -258,32 +239,27 @@
         self.l1 = 0
         self.l2 = 0
 
         # for testing
         self.local_mode = False
 
         # checkping param
-        assert self.max_iter > 0 and isinstance(
-            self.max_iter, int), "max_iter must be int greater than 0"
+        assert self.max_iter > 0 and isinstance(self.max_iter, int), "max_iter must be int greater than 0"
         if self.batch_size is not None:
             assert self.batch_size > 0 and isinstance(
-                self.batch_size, int), "batch_size must be int greater than 0 or None"
+                self.batch_size, int
+            ), "batch_size must be int greater than 0 or None"
         assert self.threshold > 0 and self.threshold < 1, "threshold must be float between 0 and 1"
 
     def _make_dataset(self, data) -> TableDataset:
         ds = TableDataset(return_dict=True, to_tensor=True)
         ds.load(data)
         return ds
 
-    def _make_output_df(
-            self,
-            ctx,
-            predict_rs,
-            data: TableDataset,
-            threshold: float):
+    def _make_output_df(self, ctx, predict_rs, data: TableDataset, threshold: float):
         classes = [i for i in range(len(self.model.models))]
         if len(classes) == 1:  # binary:
             classes = [0, 1]
         task_type = BINARY if len(classes) == 2 else MULTI
 
         out_df = array_to_predict_df(
             ctx,
@@ -291,202 +267,187 @@
             predict_rs.predictions,
             match_ids=data.get_match_ids(),
             sample_ids=data.get_sample_ids(),
             match_id_name=data.get_match_id_name(),
             sample_id_name=data.get_sample_id_name(),
             label=predict_rs.label_ids,
             threshold=threshold,
-            classes=classes
+            classes=classes,
         )
-        
+
         return out_df
 
     def _check_labels(self, label_set, has_validate=False):
-
-        dataset_descrb = 'train dataset' if not has_validate else 'train and validate dataset'
+        dataset_descrb = "train dataset" if not has_validate else "train and validate dataset"
         if not self.ovr and len(label_set) > 2:
             raise ValueError(
                 "please set ovr=True to enable multi-label classification, multiple labels found in {}: {}".format(
-                    dataset_descrb, label_set))
+                    dataset_descrb, label_set
+                )
+            )
         if not self.ovr and len(label_set) == 2:
             # 0, 1 is required
             if 0 not in label_set or 1 not in label_set:
                 # ask for label 0, 1 when running binary classification
                 raise ValueError(
                     "when doing binary classification, lables must be 0, 1, but found in {}'s label set is {}".format(
-                        label_set, dataset_descrb))
+                        label_set, dataset_descrb
+                    )
+                )
         if self.ovr:
             if max(label_set) > self.label_num - 1:
                 # make sure labels start from 0 and not the label indices not
                 # exceed the label num parameter
                 raise ValueError(
                     "when doing multi-label classification, labels must start from 0 and not exceed the label num parameter, \
                                  but {}'s label set is {}, while label num is {}".format(
-                        label_set, dataset_descrb, self.label_num))
-
-    def fit(self, ctx: Context, train_data: DataFrame,
-            validate_data: DataFrame = None) -> None:
+                        label_set, dataset_descrb, self.label_num
+                    )
+                )
 
+    def fit(self, ctx: Context, train_data: DataFrame, validate_data: DataFrame = None) -> None:
         # check data, must be fate Dataframe
-        assert isinstance(
-            train_data, DataFrame), "train_data must be a fate DataFrame"
+        assert isinstance(train_data, DataFrame), "train_data must be a fate DataFrame"
         if validate_data is not None:
-            assert isinstance(
-                validate_data, DataFrame), "validate_data must be a fate DataFrame"
+            assert isinstance(validate_data, DataFrame), "validate_data must be a fate DataFrame"
 
         self.train_set = self._make_dataset(train_data)
         if not self.train_set.has_label():
             raise RuntimeError("train data must have label column")
         self.train_feature_num = self.train_set.features.shape[1]
         unique_label_set = set(self.train_set.get_classes())
 
         if validate_data is not None:
             self.validate_set = self._make_dataset(validate_data)
             if not self.validate_set.has_label():
                 raise RuntimeError("validate data must have label column")
             self.validate_feature_num = self.validate_set.features.shape[1]
-            assert self.train_feature_num == self.validate_feature_num, "train and validate feature num not match: {} vs {}".format(
-                self.train_feature_num, self.validate_feature_num)
-            unique_label_set = unique_label_set.union(
-                set(self.validate_set.get_classes()))
+            assert (
+                self.train_feature_num == self.validate_feature_num
+            ), "train and validate feature num not match: {} vs {}".format(
+                self.train_feature_num, self.validate_feature_num
+            )
+            unique_label_set = unique_label_set.union(set(self.validate_set.get_classes()))
 
         self._check_labels(unique_label_set, validate_data is not None)
 
         if self.batch_size is None:
             self.batch_size = len(self.train_set)
 
         # prepare loss function
         loss_fn = functools.partial(homo_lr_loss, dim=len(unique_label_set))
-        optimizer_params = self.optimizer_param['optimizer_params']
-        opt_method = self.optimizer_param['method']
-        if self.optimizer_param['penalty'] == 'l2':
-            self.l2 = self.optimizer_param['alpha']
-            optimizer_params['weight_decay'] = self.l2
-        elif self.optimizer_param['penalty'] == 'l1':
-            self.l1 = self.optimizer_param['alpha']
+        optimizer_params = self.optimizer_param["optimizer_params"]
+        opt_method = self.optimizer_param["method"]
+        if self.optimizer_param["penalty"] == "l2":
+            self.l2 = self.optimizer_param["alpha"]
+            optimizer_params["weight_decay"] = self.l2
+        elif self.optimizer_param["penalty"] == "l1":
+            self.l1 = self.optimizer_param["alpha"]
 
         # initialize model
         if self.model is None:
             fit_intercept = self.init_param["fit_intercept"]
             self.model = HomoLRModel(
-                self.train_feature_num,
-                label_num=len(unique_label_set),
-                l1=self.l1,
-                bias=fit_intercept)
+                self.train_feature_num, label_num=len(unique_label_set), l1=self.l1, bias=fit_intercept
+            )
             # init model here
-            init_model(
-                self.model,
-                method=self.init_param["method"],
-                fill_val=self.init_param["fill_val"])
-            logger.info('model initialized')
-            logger.info(
-                'model parameters are {}'.format(
-                    list(
-                        self.model.parameters())))
+            init_model(self.model, method=self.init_param["method"], fill_val=self.init_param["fill_val"])
+            logger.info("model initialized")
+            logger.info("model parameters are {}".format(list(self.model.parameters())))
         else:
-            logger.info('model is loaded, warm start training')
-        logger.info('model structure is {}'.format(self.model))
+            logger.info("model is loaded, warm start training")
+        logger.info("model structure is {}".format(self.model))
 
-        self.optimizer = optimizer_factory(
-            self.model.parameters(), opt_method, optimizer_params)
+        self.optimizer = optimizer_factory(self.model.parameters(), opt_method, optimizer_params)
         self.lr_scheduler = lr_scheduler_factory(
-            self.optimizer,
-            self.learning_rate_param['method'],
-            self.learning_rate_param['scheduler_params'])
+            self.optimizer, self.learning_rate_param["method"], self.learning_rate_param["scheduler_params"]
+        )
 
         if self.optimizer_state_dict is not None:
             optimizer_state_dict = {
-                "state": {
-                    k: t.tensor(v) for k,
-                    v in self.optimizer_state_dict['state'].items()},
-                "param_groups": self.optimizer_state_dict['param_groups'],
+                "state": {k: t.tensor(v) for k, v in self.optimizer_state_dict["state"].items()},
+                "param_groups": self.optimizer_state_dict["param_groups"],
             }
             self.optimizer.load_state_dict(optimizer_state_dict)
-            logger.info('load warmstart optimizer state dict')
+            logger.info("load warmstart optimizer state dict")
 
         # training
         fed_arg = FedAVGArguments()
         train_arg = TrainingArguments(
             num_train_epochs=self.max_iter,
             per_device_train_batch_size=self.batch_size,
-            per_device_eval_batch_size=self.batch_size)
-        self.trainer = FedAVGCLient(
+            per_device_eval_batch_size=self.batch_size,
+        )
+        self.trainer = FedAVGClient(
             ctx,
             model=self.model,
             loss_fn=loss_fn,
             optimizer=self.optimizer,
             train_set=self.train_set,
             val_set=self.validate_set,
             training_args=train_arg,
             fed_args=fed_arg,
             data_collator=default_data_collator,
-            scheduler=self.lr_scheduler)
+            scheduler=self.lr_scheduler,
+        )
         if self.local_mode:  # for debugging
             self.trainer.set_local_mode()
         self.trainer.train()
 
-        logger.info('homo lr fit done')
+        logger.info("homo lr fit done")
 
     def predict(self, ctx: Context, predict_data: DataFrame) -> DataFrame:
-
         if self.model is None:
             raise ValueError("model is not initialized")
         self.predict_set = self._make_dataset(predict_data)
         if self.trainer is None:
-            batch_size = len(
-                self.predict_set) if self.batch_size is None else self.batch_size
-            train_arg = TrainingArguments(
-                num_train_epochs=self.max_iter,
-                per_device_eval_batch_size=batch_size)
-            trainer = FedAVGCLient(
+            batch_size = len(self.predict_set) if self.batch_size is None else self.batch_size
+            train_arg = TrainingArguments(num_train_epochs=self.max_iter, per_device_eval_batch_size=batch_size)
+            trainer = FedAVGClient(
                 ctx,
                 train_set=self.predict_set,
                 model=self.model,
                 training_args=train_arg,
                 fed_args=FedAVGArguments(),
-                data_collator=default_data_collator)
+                data_collator=default_data_collator,
+            )
             trainer.set_local_mode()
         else:
             trainer = self.trainer
         predict_rs = trainer.predict(self.predict_set)
-        predict_out_df = self._make_output_df(
-            ctx, predict_rs, self.predict_set, self.threshold)
+        predict_out_df = self._make_output_df(ctx, predict_rs, self.predict_set, self.threshold)
         return predict_out_df
 
     def get_model(self) -> ModelIO:
         param = {}
         if self.model is not None:
-            param['model'] = self.model.to_dict()
+            param["model"] = self.model.to_dict()
         if self.optimizer is not None:
-            param['optimizer'] = str(
-                get_torch_bytes(
-                    self.optimizer.state_dict()))
+            param["optimizer"] = str(get_torch_bytes(self.optimizer.state_dict()))
 
         meta = {
-            'batch_size': self.batch_size,
-            'max_iter': self.max_iter,
-            'threshold': self.threshold,
-            'optimizer_param': self.optimizer_param,
-            'learning_rate_param': self.learning_rate_param,
-            'init_param': self.init_param,
-            'ovr': self.ovr,
-            'label_num': self.label_num}
+            "batch_size": self.batch_size,
+            "max_iter": self.max_iter,
+            "threshold": self.threshold,
+            "optimizer_param": self.optimizer_param,
+            "learning_rate_param": self.learning_rate_param,
+            "init_param": self.init_param,
+            "ovr": self.ovr,
+            "label_num": self.label_num,
+        }
 
-        return {'param': param, 'meta': meta}
+        return {"param": param, "meta": meta}
 
     def from_model(self, model: dict):
-
-        if 'param' not in model:
+        if "param" not in model:
             raise ('key "data" is not found in the input model dict')
 
-        model_param = model['param']
-        if 'model' not in model_param:
-            raise ValueError(
-                "param dict must have key 'model' that contains the model parameter and structure info")
-        self.model = HomoLRModel.from_dict(model_param['model'])
+        model_param = model["param"]
+        if "model" not in model_param:
+            raise ValueError("param dict must have key 'model' that contains the model parameter and structure info")
+        self.model = HomoLRModel.from_dict(model_param["model"])
         if self.ovr:
-            assert len(self.model.models) == self.label_num, ''
+            assert len(self.model.models) == self.label_num, ""
         self.model.l1 = self.l1
-        if hasattr(model_param, 'optimizer'):
-            self.optimizer_state_dict = recover_torch_bytes(
-                bytes(model_param['optimizer'], 'utf-8'))
-        self.loaded_meta = model['meta']
+        if hasattr(model_param, "optimizer"):
+            self.optimizer_state_dict = recover_torch_bytes(bytes(model_param["optimizer"], "utf-8"))
+        self.loaded_meta = model["meta"]
```

### Comparing `pyfate-2.0.0b0/fate/ml/model_selection/data_split.py` & `pyfate-2.1.0/fate/ml/model_selection/data_split.py`

 * *Files 17% similar despite different names*

```diff
@@ -21,49 +21,50 @@
 from ..abc.module import Module
 
 logger = logging.getLogger(__name__)
 
 
 class DataSplitModuleGuest(Module):
     def __init__(
-            self,
-            train_size=0.8,
-            validate_size=0.2,
-            test_size=0.0,
-            stratified=False,
-            random_state=None,
-            hetero_sync=True
+        self, train_size=0.8, validate_size=0.2, test_size=0.0, stratified=False, random_state=None, hetero_sync=True
     ):
         self.train_size = train_size
         self.validate_size = validate_size
         self.test_size = test_size
         self.stratified = stratified
         self.random_state = random_state
         self.hetero_sync = hetero_sync
 
     def fit(self, ctx: Context, train_data, validate_data=None):
         data_count = train_data.shape[0]
-        train_size, validate_size, test_size = get_split_data_size(self.train_size,
-                                                                   self.validate_size,
-                                                                   self.test_size,
-                                                                   data_count)
+        train_size, validate_size, test_size = get_split_data_size(
+            self.train_size, self.validate_size, self.test_size, data_count
+        )
+
         if self.stratified:
-            train_data_set = sample_per_label(train_data, sample_count=train_size, random_state=self.random_state)
+            train_data_set, train_sample_n_per_label, labels = sample_per_label(
+                train_data, sample_count=train_size, random_state=self.random_state
+            )
+            if len(train_sample_n_per_label) == 0:
+                train_sample_n_per_label = {label: 0 for label in labels}
         else:
             train_data_set = sample_data(df=train_data, n=train_size, random_state=self.random_state)
         if train_data_set is not None:
             train_sid = train_data_set.get_indexer(target="sample_id")
             validate_test_data_set = train_data.drop(train_data_set)
         else:
             train_sid = None
             validate_test_data_set = train_data
 
         if self.stratified:
-            validate_data_set = sample_per_label(validate_test_data_set, sample_count=validate_size,
-                                                 random_state=self.random_state)
+            validate_data_set, valid_sample_n_per_label, _ = sample_per_label(
+                validate_test_data_set, sample_count=validate_size, random_state=self.random_state
+            )
+            if len(valid_sample_n_per_label) == 0:
+                valid_sample_n_per_label = {label: 0 for label in labels}
         else:
             validate_data_set = sample_data(df=validate_test_data_set, n=validate_size, random_state=self.random_state)
         if validate_data_set is not None:
             validate_sid = validate_data_set.get_indexer(target="sample_id")
             test_data_set = validate_test_data_set.drop(validate_data_set)
             if test_data_set.shape[0] == 0:
                 test_sid = None
@@ -80,26 +81,36 @@
                 test_sid = validate_test_data_set.get_indexer(target="sample_id")
 
         if self.hetero_sync:
             ctx.hosts.put("train_data_sid", train_sid)
             ctx.hosts.put("validate_data_sid", validate_sid)
             ctx.hosts.put("test_data_sid", test_sid)
 
+        if self.stratified:
+            if test_data_set:
+                test_sample_n_per_label = {}
+                for label in labels:
+                    test_sample_n_per_label[label] = int((test_data_set.label == label).sum().values[0])
+            else:
+                test_sample_n_per_label = {label: 0 for label in labels}
+            for label in labels:
+                label_summary = {}
+                label_summary["original_count"] = int((train_data.label == label).sum().values[0])
+                label_summary["train_count"] = train_sample_n_per_label[label]
+                label_summary["validate_count"] = valid_sample_n_per_label[label]
+                label_summary["test_count"] = test_sample_n_per_label[label]
+
+                ctx.metrics.log_metrics(label_summary, name=f"{label}_summary", type="data_split")
+
         return train_data_set, validate_data_set, test_data_set
 
 
 class DataSplitModuleHost(Module):
     def __init__(
-            self,
-            train_size=0.8,
-            validate_size=0.2,
-            test_size=0.0,
-            stratified=False,
-            random_state=None,
-            hetero_sync=True
+        self, train_size=0.8, validate_size=0.2, test_size=0.0, stratified=False, random_state=None, hetero_sync=True
     ):
         self.train_size = train_size
         self.validate_size = validate_size
         self.test_size = test_size
         self.stratified = stratified
         self.random_state = random_state
         self.hetero_sync = hetero_sync
@@ -114,77 +125,104 @@
                 train_data_set = train_data.loc(train_data_sid, preserve_order=True)
             if validate_data_sid:
                 validate_data_set = train_data.loc(validate_data_sid, preserve_order=True)
             if test_data_sid:
                 test_data_set = train_data.loc(test_data_sid, preserve_order=True)
         else:
             data_count = train_data.shape[0]
-            train_size, validate_size, test_size = get_split_data_size(self.train_size,
-                                                                       self.validate_size,
-                                                                       self.test_size,
-                                                                       data_count)
+            train_size, validate_size, test_size = get_split_data_size(
+                self.train_size, self.validate_size, self.test_size, data_count
+            )
 
             if self.stratified:
-                train_data_set = sample_per_label(train_data, sample_count=train_size, random_state=self.random_state)
+                train_data_set, train_sample_n_per_label, labels = sample_per_label(
+                    train_data, sample_count=train_size, random_state=self.random_state
+                )
+                if len(train_sample_n_per_label) == 0:
+                    train_sample_n_per_label = {label: 0 for label in labels}
             else:
                 train_data_set = sample_data(df=train_data, n=train_size, random_state=self.random_state)
             if train_data_set is not None:
                 # train_sid = train_data_set.get_indexer(target="sample_id")
                 validate_test_data_set = train_data.drop(train_data_set)
             else:
                 validate_test_data_set = train_data
 
             if self.stratified:
-                validate_data_set = sample_per_label(validate_test_data_set, sample_count=validate_size,
-                                                     random_state=self.random_state)
-            else:
-                validate_data_set = sample_data(df=validate_test_data_set, n=validate_size,
-                                                random_state=self.random_state)
+                validate_data_set, valid_sample_n_per_label, _ = sample_per_label(
+                    validate_test_data_set, sample_count=validate_size, random_state=self.random_state
+                )
+                if len(valid_sample_n_per_label) == 0:
+                    valid_sample_n_per_label = {label: 0 for label in labels}
+            else:
+                validate_data_set = sample_data(
+                    df=validate_test_data_set, n=validate_size, random_state=self.random_state
+                )
             if validate_data_set is not None:
                 # validate_sid = validate_data_set.get_indexer(target="sample_id")
                 test_data_set = validate_test_data_set.drop(validate_data_set)
                 if test_data_set.shape[0] == 0:
                     test_data_set = None
             else:
                 if validate_test_data_set.shape[0] == 0:
                     test_data_set = None
                 else:
                     test_data_set = validate_test_data_set
+            if self.stratified:
+                if test_data_set:
+                    test_sample_n_per_label = {}
+                    for label in labels:
+                        test_sample_n_per_label[label] = int((test_data_set.label == label).sum().values[0])
+                else:
+                    test_sample_n_per_label = {label: 0 for label in labels}
+                for label in labels:
+                    label_summary = {}
+                    label_summary["original_count"] = int((train_data.label == label).sum().values[0])
+                    label_summary["train_count"] = train_sample_n_per_label[label]
+                    label_summary["validate_count"] = valid_sample_n_per_label[label]
+                    label_summary["test_count"] = test_sample_n_per_label[label]
+
+                    ctx.metrics.log_metrics(label_summary, name=f"{label}_summary", type="data_split")
 
         return train_data_set, validate_data_set, test_data_set
 
 
 def sample_data(df, n, random_state):
     if n == 0:
         return
     else:
         return df.sample(n=n, random_state=random_state)
 
 
 def sample_per_label(train_data, sample_count=None, random_state=None):
     train_data_binarized_label = train_data.label.get_dummies()
-    labels = [label_name.split("_")[1] for label_name in train_data_binarized_label.columns]
+    labels = [int(label_name.split("_")[1]) for label_name in train_data_binarized_label.columns]
     sampled_data_df = []
     sampled_n = 0
     data_n = train_data.shape[0]
+    sample_n_per_label = {}
     for i, label in enumerate(labels):
-        label_data = train_data.iloc(train_data.label == int(label))
+        label_data = train_data.iloc(train_data.label == label)
         if i == len(labels) - 1:
             # last label:
             to_sample_n = sample_count - sampled_n
         else:
             to_sample_n = round(label_data.shape[0] / data_n * sample_count)
         label_sampled_data = sample_data(df=label_data, n=to_sample_n, random_state=random_state)
+        if label_sampled_data:
+            sample_n_per_label[label] = label_sampled_data.shape[0]
+        else:
+            sample_n_per_label[label] = 0
         if label_sampled_data is not None:
             sampled_data_df.append(label_sampled_data)
             sampled_n += label_sampled_data.shape[0]
     sampled_data = None
     if sampled_data_df:
         sampled_data = DataFrame.vstack(sampled_data_df)
-    return sampled_data
+    return sampled_data, sample_n_per_label, labels
 
 
 def get_split_data_size(train_size, validate_size, test_size, data_count):
     """
     Validate & transform param inputs into all int
     """
     # check & transform data set sizes
```

### Comparing `pyfate-2.0.0b0/fate/ml/model_selection/sample.py` & `pyfate-2.1.0/fate/ml/model_selection/sample.py`

 * *Files 18% similar despite different names*

```diff
@@ -20,89 +20,69 @@
 from fate.arch.dataframe import utils
 from ..abc.module import Module
 
 logger = logging.getLogger(__name__)
 
 
 class SampleModuleGuest(Module):
-    def __init__(
-            self,
-            replace=False,
-            frac=1.0,
-            n=None,
-            random_state=None,
-            hetero_sync=True
-    ):
+    def __init__(self, replace=False, frac=1.0, n=None, random_state=None, hetero_sync=True):
         self.replace = replace
         self.frac = frac
         self.n = n
         self.random_state = random_state
         self.hetero_sync = hetero_sync
 
         self._sample_obj = None
 
     def fit(self, ctx: Context, train_data, validate_data=None) -> None:
         logger.info(f"enter sample fit")
         if self.hetero_sync:
             logger.info(f"hetero sync")
             logger.info(f"role: {ctx.local.role}")
 
-            sampled_data = utils.federated_sample(ctx,
-                                                  train_data,
-                                                  n=self.n,
-                                                  frac=self.frac,
-                                                  replace=self.replace,
-                                                  role=ctx.local.role,
-                                                  random_state=self.random_state)
+            sampled_data = utils.federated_sample(
+                ctx,
+                train_data,
+                n=self.n,
+                frac=self.frac,
+                replace=self.replace,
+                role=ctx.local.role,
+                random_state=self.random_state,
+            )
         else:
             logger.info(f"local sample")
             # local sample
-            sampled_data = utils.local_sample(ctx,
-                                              train_data,
-                                              n=self.n,
-                                              frac=self.frac,
-                                              replace=self.replace,
-                                              random_state=self.random_state)
+            sampled_data = utils.local_sample(
+                ctx, train_data, n=self.n, frac=self.frac, replace=self.replace, random_state=self.random_state
+            )
 
         return sampled_data
 
 
 class SampleModuleHost(Module):
-    def __init__(
-            self,
-            replace=False,
-            frac=1.0,
-            n=None,
-            random_state=None,
-            hetero_sync=True
-    ):
+    def __init__(self, replace=False, frac=1.0, n=None, random_state=None, hetero_sync=True):
         self.replace = replace
         self.frac = frac
         self.n = n
         self.random_state = random_state
         self.hetero_sync = hetero_sync
 
     def fit(self, ctx: Context, train_data, validate_data=None) -> None:
         logger.info(f"enter sample fit")
         if self.hetero_sync:
             logger.info(f"hetero sync")
             logger.info(f"role: {ctx.local.role}")
 
-            sampled_data = utils.federated_sample(ctx,
-                                                  train_data,
-                                                  role=ctx.local.role)
+            sampled_data = utils.federated_sample(ctx, train_data, role=ctx.local.role)
         else:
             # local sample
             logger.info(f"local sample")
-            sampled_data = utils.local_sample(ctx,
-                                              train_data,
-                                              n=self.n,
-                                              frac=self.frac,
-                                              replace=self.replace,
-                                              random_state=self.random_state)
+            sampled_data = utils.local_sample(
+                ctx, train_data, n=self.n, frac=self.frac, replace=self.replace, random_state=self.random_state
+            )
             """elif self.mode == "weight":
                 if self.n is not None:
                     sampled_data = train_data.sample(n=self.n,
                                                      replace=self.replace,
                                                      weight=train_data.weight,
                                                      random_state=self.random_state)
                 else:
```

### Comparing `pyfate-2.0.0b0/fate/ml/nn/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/nn/algo/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/common/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/nn/algo/homo/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/algo/secureboost/hetero/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/nn/dataset/table.py` & `pyfate-2.1.0/fate/ml/nn/dataset/table.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import numpy as np
 import pandas as pd
 from fate.arch.dataframe import DataFrame
 from fate.ml.nn.dataset.base import Dataset
 import logging
 import torch as t
 
@@ -150,16 +165,17 @@
                         break
                 if label is None:
                     logger.info('found no "y"/"label"/"target" in input table, no label will be set')
             else:
                 if label not in self.origin_table:
                     raise ValueError("label column {} not found in input table".format(label))
 
-            self.label = self.origin_table[[label]].values
-            self.origin_table = self.origin_table.drop(columns=[label])
+            if label is not None:
+                self.label = self.origin_table[[label]].values
+                self.origin_table = self.origin_table.drop(columns=[label])
             self.features = self.origin_table.values
 
         elif isinstance(data_or_path, DataFrame):
             schema = data_or_path.schema
             sample_id = schema.sample_id_name
             match_id = schema.match_id_name
             label = schema.label_name
```

### Comparing `pyfate-2.0.0b0/fate/ml/nn/model_zoo/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/nn/trainer/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/hetero/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/nn/trainer/trainer_base.py` & `pyfate-2.1.0/fate/ml/nn/trainer/trainer_base.py`

 * *Files 15% similar despite different names*

```diff
@@ -13,39 +13,39 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 #
 
 import os
 import re
 import torch
+import torch.distributed as dist
 import math
 import sys
 from torch import nn
-import numpy as np
-import logging
-from dataclasses import dataclass, field
-from typing import Any, Dict, List, Optional, Tuple, Union, Callable
+from typing import Any, Dict, List, Tuple, Union, Callable
 from enum import Enum
-from transformers.training_args import TrainingArguments
 from fate.arch import Context
 from torch.optim import Optimizer
 from torch.utils.data import DataLoader, Dataset
 from transformers import TrainingArguments as _hf_TrainingArguments, PreTrainedTokenizer
-from transformers import Trainer, TrainerState, TrainerControl, EvalPrediction
+from transformers import Trainer, EvalPrediction
 from transformers.trainer_utils import has_length
-from torch.optim.lr_scheduler import _LRScheduler, LambdaLR
+from torch.optim.lr_scheduler import _LRScheduler
 from torch.utils.data import _utils
 from fate.ml.aggregator.base import Aggregator
 import logging
 from transformers import logging as transformers_logging
 from transformers.trainer_callback import TrainerCallback, TrainerControl, TrainerState
 from typing import Optional
 from dataclasses import dataclass, field, fields
-from transformers.trainer_callback import PrinterCallback
 from fate.ml.aggregator import AggregatorType
+from fate.ml.nn.model_zoo.hetero_nn_model import HeteroNNModelGuest, HeteroNNModelHost
+from transformers.trainer import logger as logger_
+from fate.ml.evaluation.metric_base import MetricEnsemble
+from transformers import IntervalStrategy, DefaultFlowCallback
 
 
 # Reset the logger to redirect logs output
 transformers_logging.disable_default_handler()
 transformers_logging.enable_propagation()
 logger = logging.getLogger(__name__)
 
@@ -74,25 +74,25 @@
 
 """
 Fed Arguments
 """
 
 
 class AggregateStrategy(Enum):
-    EPOCH = "epochs"
+    EPOCH = "epoch"
     STEP = "steps"
 
 
 @dataclass
 class FedArguments(object):
     """
     The argument for Fed algorithm
     """
 
-    aggregate_strategy: AggregateStrategy = field(default=AggregateStrategy.EPOCH.value)
+    aggregate_strategy: str = field(default=AggregateStrategy.EPOCH.value)
     aggregate_freq: int = field(default=1)
     aggregator: str = field(default=AggregatorType.SECURE_AGGREGATE.value)
 
     def to_dict(self):
         """
         Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates
         the token values by removing their value.
@@ -107,49 +107,55 @@
                 d[k] = [x.value for x in v]
             if k.endswith("_token"):
                 d[k] = f"<{k.upper()}>"
         return d
 
 
 @dataclass
-class TrainingArguments(_hf_TrainingArguments):
+class _TrainingArguments(_hf_TrainingArguments):
     # in fate-2.0, we will control the output dir when using pipeline
     output_dir: str = field(default="./")
     disable_tqdm: bool = field(default=True)
     save_strategy: str = field(default="no")
     logging_strategy: str = field(default="epoch")
+    logging_steps: int = field(default=1)
     evaluation_strategy: str = field(default="no")
     logging_dir: str = field(default=None)
     checkpoint_idx: int = field(default=None)
-    # by default we use constant learning rate, the same as FATE-1.X
+    # by default, we use constant learning rate, the same as FATE-1.X
     lr_scheduler_type: str = field(default="constant")
+    log_level: str = field(default="info")
+    deepspeed: Optional[str] = field(default=None)
+    save_safetensors: bool = field(default=False)
+    use_cpu: bool = field(default=True)
 
     def __post_init__(self):
-        # Always use default values for hub-related attributes
         self.push_to_hub = False
         self.hub_model_id = None
         self.hub_strategy = "every_save"
         self.hub_token = None
         self.hub_private_repo = False
         self.push_to_hub_model_id = None
         self.push_to_hub_organization = None
         self.push_to_hub_token = None
 
         super().__post_init__()
 
+
+@dataclass
+class TrainingArguments(_TrainingArguments):
+    # To simplify the to dict result(to_dict only return non-default args)
+
     def to_dict(self):
         # Call the superclass's to_dict method
         all_args = super().to_dict()
-
         # Get a dict with default values for all fields
-        default_args = _hf_TrainingArguments(output_dir="./").to_dict()
-
+        default_args = _TrainingArguments().to_dict()
         # Filter out args that are equal to their default values
         set_args = {name: value for name, value in all_args.items() if value != default_args.get(name)}
-
         return set_args
 
 
 """
 Fed Callback Related Classes
 """
 
@@ -268,14 +274,17 @@
         state: Optional[TrainerState] = None,
         **kwargs,
     ):
         pass
 
 
 class FedCallbackInterface(object):
+    def __init__(self):
+        pass
+
     def on_federation(
         self,
         ctx: Context,
         aggregator: Aggregator,
         fed_args: FedArguments,
         args: TrainingArguments,
         model: Optional[nn.Module] = None,
@@ -291,22 +300,23 @@
     def init_aggregator(self, fed_arg: FedArguments):
         raise NotImplementedError("init_aggregator() must be implemented in subclass, init aggregator here")
 
 
 # I dont like huggingface logging
 class LogSuppressFilter(logging.Filter):
     def filter(self, record):
-        suppress_list = set(
-            ["\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n"]
-        )
+        suppress_list = {"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n"}
         if record.getMessage() in suppress_list:
             return False
         return True
 
 
+logger_.addFilter(LogSuppressFilter())
+
+
 def compute_max_aggregation(
     fed_args: FedArguments, max_epoch: int, max_steps: int, epochs_trained: int, steps_trained: int
 ) -> int:
     assert (
         max_epoch > epochs_trained and max_epoch > 0
     ), "max_epoch must be greater than epochs_trained: {} and greater than 0".format(epochs_trained)
     assert (
@@ -325,47 +335,69 @@
         raise ValueError("aggregate_freq must be a positive integer or a float between 0 and 1")
 
     if fed_args.aggregate_strategy == AggregateStrategy.EPOCH.value:
         max_aggregation = int((max_epoch - epochs_trained) / aggregate_freq)
     elif fed_args.aggregate_strategy == AggregateStrategy.STEP.value:
         max_aggregation = int((max_steps - steps_trained) / aggregate_freq)
     else:
-        raise ValueError('aggregate_strategy must be either "epochs" or "steps"')
+        raise ValueError('aggregate_strategy must be either "epoch" or "steps"')
 
     return max_aggregation, aggregate_freq
 
 
+def can_aggregate_loss(args: TrainingArguments, fed_args: FedArguments):
+    # check if logging strategy is aligned with aggregation strategy
+    # make sure that the aggregated loss is correct
+    can_aggregate = False
+    if args.logging_strategy == fed_args.aggregate_strategy:
+        if fed_args.aggregate_strategy == "epoch":
+            if args.logging_steps == fed_args.aggregate_freq:
+                can_aggregate = True
+
+    if not can_aggregate:
+        logger.warning(
+            "Not able to aggregate loss, "
+            'Trainer is only able to aggregate loss on server when aggregate strategy and logging strategy are "epoch",'
+            " and aggregate_freq and logging_freq are the same"
+        )
+
+    return can_aggregate
+
+
 class AggregationChecker:
     def __init__(
         self,
         fed_args,
         max_aggregation,
         aggregate_freq,
         max_epoch: int,
         max_steps: int,
         epochs_trained: int,
         steps_trained: int,
+        can_aggregate_loss: bool,
     ):
         self.fed_args = fed_args
         self.max_epoch = max_epoch
         self.max_steps = max_steps
         self.epochs_trained = epochs_trained
         self.steps_trained = steps_trained
-        self.aggregation_count = 0
+        self.model_aggregation_count = 0
+        self.loss_aggregation_count = 0
         self.aggregate_freq = aggregate_freq
         self.max_aggregation = max_aggregation
+        self.can_aggregate_loss = can_aggregate_loss
 
     def report(self):
-        logger.info(f"Aggregation count: {self.aggregation_count} / {self.max_aggregation}")
+        logger.info(f"Aggregation count: {self.model_aggregation_count} / {self.max_aggregation}")
 
     def should_aggregate(self, state: TrainerState) -> bool:
         cur_epoch = int(state.epoch)
         cur_step = int(state.global_step)
 
-        if self.aggregation_count >= self.max_aggregation:
+        if self.model_aggregation_count >= self.max_aggregation:
             return False
 
         if cur_epoch > self.max_epoch:
             return False
 
         strategy = self.fed_args.aggregate_strategy
 
@@ -374,18 +406,40 @@
                 return True
         elif strategy == AggregateStrategy.STEP.value:
             if cur_step > self.steps_trained and (cur_step - self.steps_trained) % self.aggregate_freq == 0:
                 return True
 
         return False
 
-    def inc_aggregation_count(self):
-        self.aggregation_count += 1
+    def inc_model_agg_count(self):
+        self.model_aggregation_count += 1
         self.report()
 
+    def inc_loss_agg_count(self):
+        self.loss_aggregation_count += 1
+
+
+class LossLoggingCallback(TrainerCallback):
+    def __init__(self, ctx: Context):
+        self.ctx = ctx
+        self.sub_ctx = self.ctx.sub_ctx("nn_loss")
+
+    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
+        latest_log = state.log_history[-1]
+        logger.info(latest_log)
+        if "loss" in latest_log:
+            loss = latest_log["loss"]
+            if args.logging_strategy == IntervalStrategy.EPOCH:
+                idx = int(state.log_history[-1]["epoch"])
+            elif args.logging_strategy == IntervalStrategy.STEPS:
+                idx = int(state.log_history[-1]["step"])
+            else:
+                raise RuntimeError("unknown logging strategy")
+            self.sub_ctx.indexed_ctx(idx).metrics.log_loss("loss", loss)
+
 
 class FedParameterAlignCallback(TrainerCallback):
     def __init__(
         self,
         trainer_class,
         ctx: Context,
         training_args: TrainingArguments,
@@ -393,14 +447,15 @@
         is_server: bool = False,
     ) -> None:
         super().__init__()
         self.trainer_class = trainer_class
         self.ctx = ctx
         self.is_server = is_server
         self.training_args = training_args
+        self.can_aggregate_loss = False
         self.fed_args = fed_args
         self._suffix = "fed_para"
         self._send_count = 0
         self._parameters = None
         self._aggregation_checker = None
 
     def get_aggregation_checker(self):
@@ -442,39 +497,57 @@
             steps_trained_in_current_epoch = 0
 
         max_aggregation, aggregate_freq = compute_max_aggregation(
             self.fed_args, num_train_epochs, max_steps, epochs_trained, state.global_step
         )
         logger.info("computed max_aggregation is {}".format(max_aggregation))
 
+        # if able to aggregate loss
+        if can_aggregate_loss(args, self.fed_args):
+            self.can_aggregate_loss = True
+
         # send parameters
         parameters = {
             "num_train_epochs": num_train_epochs,
             "max_steps": max_steps,
             "num_update_steps_per_epoch": num_update_steps_per_epoch,
             "epochs_trained": epochs_trained,
             "steps_trained_in_current_epoch": steps_trained_in_current_epoch,
             "max_aggregation": max_aggregation,
             "aggregate_freq": aggregate_freq,
             "aggregation_strategy": self.fed_args.aggregate_strategy,
+            "can_aggregate_loss": self.can_aggregate_loss,
         }
 
         logger.info("parameters is {}".format(parameters))
+        self._parameters = parameters
+
+        if args.world_size <= 1 or args.local_rank == 0:
+            self.ctx.arbiter.put(self._suffix + "_" + str(self._send_count), parameters)
+            self.can_aggregate_loss = self.ctx.arbiter.get("agg_loss_" + str(self._send_count))
+
+            if args.world_size > 1:
+                can_agg_loss_t = torch.tensor([self.can_aggregate_loss], dtype=torch.bool).cuda(args.device)
+                can_agg_loss_array = [can_agg_loss_t for _ in range(args.world_size)]
+                dist.scatter(can_agg_loss_t, can_agg_loss_array, async_op=False)
+        else:
+            can_agg_loss_t = torch.tensor([self.can_aggregate_loss], dtype=torch.bool).cuda(args.device)
+            dist.scatter(can_agg_loss_t, src=0, async_op=False)
+            self.can_aggregate_loss = can_agg_loss_t.item()
 
-        self.ctx.arbiter.put(self._suffix + "_" + str(self._send_count), parameters)
         self._send_count += 1
-        self._parameters = parameters
         self.trainer_class.aggregation_checker = AggregationChecker(
             self.fed_args,
             max_aggregation,
             aggregate_freq,
             num_train_epochs,
             max_steps,
             epochs_trained,
             state.global_step,
+            self.can_aggregate_loss,
         )
 
     def get_parameters(self):
         return self._parameters
 
     def _startegy_type(self, strategy):
         # by step or by epoch
@@ -510,120 +583,138 @@
                 "federation round not match, all clients has to have the same aggregation round,\n \
                               please check: {}".format(
                     agg_round
                 )
             )
         return agg_round[0]
 
+    def _check_aggregate_loss(self, parameters):
+        aggregate_loss = True
+        for p in parameters:
+            flag = p["can_aggregate_loss"]
+            if not flag:
+                return False
+        return aggregate_loss
+
     def _server_check_parameters(self):
         # check if all clients parameters of aggregation match
         para_1 = self.ctx.hosts.get(self._suffix + "_" + str(self._send_count))
         para_2 = self.ctx.guest.get(self._suffix + "_" + str(self._send_count))
-        self._send_count += 1
         para_1.append(para_2)
         para = para_1
         # strategy = self._check_fed_strategy(para)
         agg_round = self._check_federation_round(para)
+        self.can_aggregate_loss = self._check_aggregate_loss(para)
         self._parameters = {"max_aggregation": agg_round}
+        self._parameters["can_aggregate_loss"] = self.can_aggregate_loss
+        self.ctx.guest.put("agg_loss_" + str(self._send_count), self.can_aggregate_loss)
+        self.ctx.hosts.put("agg_loss_" + str(self._send_count), self.can_aggregate_loss)
+        self._send_count += 1
 
     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
         if self.trainer_class.local_mode:
             logger.info("FedParameterAlignCallback: local model, skipping federated parameter checking")
             return
         else:
             if self.is_server:
                 self._server_check_parameters()
             else:
                 train_dataloader = kwargs["train_dataloader"]
                 self._client_send_parameters(state, args, train_dataloader)
 
 
-class FatePrinterCallback(TrainerCallback):
-    def on_log(self, args, state, control, logs=None, **kwargs):
-        if state.is_local_process_zero:
-            _ = logs.pop("total_flos", None)
-            logger.info(str(logs))
-
-
 class CallbackWrapper(TrainerCallback):
-    def __init__(self, ctx: Context, wrapped_trainer: "StdFedTrainerMixin"):
+    def __init__(self, ctx: Context, wrapped_trainer: "HomoTrainerMixin"):
         self.ctx = ctx
         self.wrapped_trainer = wrapped_trainer
         self.fed_arg = self.wrapped_trainer._fed_args
 
     def _call_wrapped(self, ctx, aggregator, fed_arg, event_name: str, **kwargs):
         event = getattr(self.wrapped_trainer, event_name)
         kwargs["scheduler"] = kwargs.pop("lr_scheduler", None)
-
         train_dataloader = kwargs.pop("train_dataloader", None)
         eval_dataloader = kwargs.pop("eval_dataloader", None)
         dataloaders = tuple(filter(None, (train_dataloader, eval_dataloader)))
         kwargs["dataloader"] = dataloaders
         return event(ctx, aggregator, fed_arg, **kwargs)
 
 
 class WrappedFedCallback(CallbackWrapper):
-    def __init__(self, ctx: Context, wrapped_trainer: "StdFedTrainerMixin"):
+    def __init__(self, ctx: Context, wrapped_trainer: "HomoTrainerMixin"):
         super().__init__(ctx, wrapped_trainer)
 
     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
         # initialize aggregator
         # doesnot call wrapper here, make sure aggregator is not called before
         # it is initialized
         if self.wrapped_trainer.local_mode:
             logger.info("local mode, skip federation aggregator initialization, aggregator will be None")
         else:
             self.wrapped_trainer.aggregator = self.wrapped_trainer.init_aggregator(self.ctx, self.fed_arg)
 
+    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
+        if self.wrapped_trainer.local_mode:
+            return
+        # aggregate loss
+        if self.fed_arg.aggregate_strategy == AggregateStrategy.EPOCH.value:
+            if self.wrapped_trainer.aggregation_checker.can_aggregate_loss:
+                if "loss" not in state.log_history[-1]:  # only process train loss
+                    return
+                loss = state.log_history[-1]["loss"]
+                agg_round = self.wrapped_trainer.aggregation_checker.loss_aggregation_count
+                aggregator = self.wrapped_trainer.aggregator
+                sub_ctx = self.ctx.sub_ctx("loss_aggregation").indexed_ctx(agg_round)
+                aggregator.loss_aggregation(sub_ctx, loss)
+                self.wrapped_trainer.aggregation_checker.inc_loss_agg_count()
+
     def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
         if self.wrapped_trainer.local_mode:
             return
         if self.fed_arg.aggregate_strategy == AggregateStrategy.EPOCH.value:
             if self.wrapped_trainer.aggregation_checker.should_aggregate(state):
                 logger.info("aggregation on epoch end")
-                agg_round = self.wrapped_trainer.aggregation_checker.aggregation_count
+                agg_round = self.wrapped_trainer.aggregation_checker.model_aggregation_count
                 sub_ctx = self.ctx.sub_ctx("aggregation").indexed_ctx(agg_round)
                 ret = self._call_wrapped(
                     sub_ctx,
                     self.wrapped_trainer.aggregator,
                     self.fed_arg,
                     "on_federation",
                     args=args,
                     state=state,
                     control=control,
                     **kwargs,
                 )
-                self.wrapped_trainer.aggregation_checker.inc_aggregation_count()
+                self.wrapped_trainer.aggregation_checker.inc_model_agg_count()
                 return ret
 
     def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
         if self.wrapped_trainer.local_mode:
             return
         if self.fed_arg.aggregate_strategy == AggregateStrategy.STEP.value:
             if self.wrapped_trainer.aggregation_checker.should_aggregate(state):
-                logger.info("state is {}".format(state))
                 logger.info("aggregation on step end")
-                agg_round = self.wrapped_trainer.aggregation_checker.aggregation_count
+                agg_round = self.wrapped_trainer.aggregation_checker.model_aggregation_count
                 sub_ctx = self.ctx.sub_ctx("aggregation").indexed_ctx(agg_round)
                 ret = self._call_wrapped(
                     sub_ctx,
                     self.wrapped_trainer.aggregator,
                     self.fed_arg,
                     "on_federation",
                     args=args,
                     state=state,
                     control=control,
                     **kwargs,
                 )
-                self.wrapped_trainer.aggregation_checker.inc_aggregation_count()
+                self.wrapped_trainer.aggregation_checker.inc_model_agg_count()
                 return ret
 
 
 class WrappedShortcutCallback(CallbackWrapper):
-    def __init__(self, ctx: Context, wrapped_trainer: "StdFedTrainerMixin"):
+    def __init__(self, ctx: Context, wrapped_trainer: "HomoTrainerMixin"):
         super().__init__(ctx, wrapped_trainer)
 
     def on_init_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
         return self._call_wrapped(
             self.ctx,
             self.wrapped_trainer.aggregator,
             self.fed_arg,
@@ -703,23 +794,71 @@
             args=args,
             state=state,
             control=control,
             **kwargs,
         )
 
 
-logger.addFilter(LogSuppressFilter())
-
-
 """
 Mixin Class For Federation Trainer
 """
 
 
-class StdFedTrainerMixin(FedCallbackInterface, ShortcutCallBackInterFace):
+def _parse_metrics_ensemble_result(metrics_ensemble_result):
+    rs_dict = {}
+    for i in metrics_ensemble_result:
+        if isinstance(i, dict):
+            rs_dict[i["metric"]] = i["val"]
+        elif isinstance(i, list):
+            for k in i:
+                if isinstance(k, dict):
+                    rs_dict[k["metric"]] = k["val"]
+
+    return rs_dict
+
+
+class HeteroTrainerMixin(ShortcutCallBackInterFace):
+    def __init__(
+        self,
+        ctx: Context,
+        model: nn.Module,
+        training_args: TrainingArguments,
+        train_set: Dataset,
+        val_set: Dataset = None,
+        loss_fn: nn.Module = None,
+        optimizer: torch.optim.Optimizer = None,
+        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
+        tokenizer: Optional[PreTrainedTokenizer] = None,
+        callbacks: Optional[List[TrainerCallback]] = [],
+        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
+    ):
+        super().__init__()
+
+        self.ctx: Context = ctx
+        self._callbacks = callbacks
+        self._args = training_args
+        self._user_compute_metric_func = compute_metrics
+        self.train_dataset = train_set
+        self.eval_dataset = val_set
+        self.loss_func = loss_fn
+
+    def _compute_metrics_warp_func(self, *args, **kwargs):
+        if self._user_compute_metric_func is None:
+            return {}
+        else:
+            eval_result = self._user_compute_metric_func(*args, **kwargs)
+            if isinstance(self._user_compute_metric_func, MetricEnsemble):
+                return _parse_metrics_ensemble_result(eval_result)
+            return eval_result
+
+    def _set_ctx_to_model(self, model: Union[HeteroNNModelGuest, HeteroNNModelHost]):
+        model.set_context(self.ctx)
+
+
+class HomoTrainerMixin(FedCallbackInterface, ShortcutCallBackInterFace):
     def __init__(
         self,
         ctx: Context,
         model: nn.Module,
         training_args: TrainingArguments,
         fed_args: FedArguments,
         train_set: Dataset,
@@ -728,15 +867,18 @@
         optimizer: torch.optim.Optimizer = None,
         scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
         tokenizer: Optional[PreTrainedTokenizer] = None,
         callbacks: Optional[List[TrainerCallback]] = [],
         use_hf_default_behavior: bool = False,
         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
         local_mode: bool = False,
+        save_trainable_weights_only: bool = False,
     ):
+        super().__init__()
+
         assert isinstance(callbacks, list), "callback must be a list containing Callback objects, but got {}".format(
             callbacks
         )
 
         self.ctx: Context = ctx
         self.local_mode = local_mode
         self._callbacks = callbacks
@@ -748,53 +890,47 @@
         self.loss_func = loss_fn
         self._use_hf_default_behavior = use_hf_default_behavior
         self._aggregator = None
 
         # for callback class to check if aggregation is needed
         self.aggregation_checker: AggregationChecker = None
 
+        self._save_trainable_weights_only = save_trainable_weights_only
+
     def _compute_metrics_warp_func(self, *args, **kwargs):
         if self._user_compute_metric_func is None:
             return {}
         else:
             eval_result = self._user_compute_metric_func(*args, **kwargs)
-            # Do some FATEBoard Callback here
+            if isinstance(self._user_compute_metric_func, MetricEnsemble):
+                return _parse_metrics_ensemble_result(eval_result)
             return eval_result
 
     def _handle_callback(self, callback_handler, new_callbacks):
-        # remove default logger.infoer callback, need to use our logging
+        # remove default logger.info callback, need to use our logging
         # strategy
         new_callback_list = []
         for i in callback_handler.callbacks:
-            # if not isinstance(i, logger.infoerCallback):
             new_callback_list.append(i)
         new_callback_list += new_callbacks
         callback_handler.callbacks = new_callback_list
 
     def _add_fate_callback(self, callback_handler):
         # the callback handler is Trainer.callback_handler
         # call order:
         # fed callback aggregator init(once), parameter check(once),
         # on federation of fedcallback
         # callbacks of shortcutcallback
-        new_callback_list = []
-        for i in callback_handler.callbacks:
-            if isinstance(i, PrinterCallback):
-                continue
-            else:
-                new_callback_list.append(i)
-        new_callback_list.append(FatePrinterCallback())
-        callback_handler.callbacks = new_callback_list
+        callback_handler.callbacks.append(LossLoggingCallback(self.ctx))
         callback_handler.callbacks.append(WrappedFedCallback(self.ctx, self))
         callback_handler.callbacks.append(
             FedParameterAlignCallback(
                 self, self.ctx, fed_args=self._fed_args, training_args=self._args, is_server=False
             )
         )
-
         callback_handler.callbacks.append(WrappedShortcutCallback(self.ctx, self))
 
     def _remove_fed_callback(self, callback_class):
         self.callback_handler.callbacks = [
             c for c in self.callback_handler.callbacks if not isinstance(c, callback_class)
         ]
 
@@ -812,19 +948,76 @@
 
     @aggregator.setter
     def aggregator(self, value):
         self._aggregator = value
 
 
 """
-Base Classes of Client/Sever Trainer
+Base Classes of NN Trainer
 """
 
 
-class FedTrainerClient(Trainer, StdFedTrainerMixin):
+class HeteroTrainerBase(Trainer, HeteroTrainerMixin):
+    def __init__(
+        self,
+        ctx: Context,
+        model: Union[HeteroNNModelGuest, HeteroNNModelHost],
+        training_args: TrainingArguments,
+        train_set: Dataset,
+        val_set: Dataset = None,
+        loss_fn: nn.Module = None,
+        optimizer: torch.optim.Optimizer = None,
+        data_collator: Callable = None,
+        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
+        tokenizer: Optional[PreTrainedTokenizer] = None,
+        callbacks: Optional[List[TrainerCallback]] = [],
+        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
+    ):
+        HeteroTrainerMixin.__init__(
+            self,
+            ctx=ctx,
+            model=model,
+            loss_fn=loss_fn,
+            optimizer=optimizer,
+            training_args=training_args,
+            train_set=train_set,
+            val_set=val_set,
+            scheduler=scheduler,
+            callbacks=callbacks,
+            compute_metrics=compute_metrics,
+            tokenizer=tokenizer,
+        )
+
+        if data_collator is None:
+            data_collator = _utils.collate.default_collate
+
+        # concat checkpoint path if checkpoint idx is set
+        if self._args.checkpoint_idx is not None:
+            checkpoint_path = self._args.resume_from_checkpoint
+            if checkpoint_path is not None and os.path.exists(checkpoint_path):
+                checkpoint_folder = get_ith_checkpoint(checkpoint_path, self._args.checkpoint_idx)
+                self._args.resume_from_checkpoint = os.path.join(checkpoint_path, checkpoint_folder)
+
+        Trainer.__init__(
+            self,
+            model=model,
+            args=self._args,
+            train_dataset=train_set,
+            eval_dataset=val_set,
+            data_collator=data_collator,
+            optimizers=(optimizer, scheduler),
+            tokenizer=tokenizer,
+            compute_metrics=self._compute_metrics_warp_func,
+        )
+
+        # update callbacks
+        self.callback_handler.callbacks.append(LossLoggingCallback(ctx))
+
+
+class HomoTrainerClient(Trainer, HomoTrainerMixin):
 
     """
     FedTrainerClient is designed to handle diverse federated training tasks.
 
     By extending the transformers.Trainer class, this class allows customization of the federated training,
     evaluation, and prediction processes to meet the needs of specific federateion training tasks. Users can
     override relevant methods to implement custom functionality.
@@ -848,15 +1041,15 @@
         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
         local_mode: bool = False,
     ):
         # in case you forget to set evaluation_strategy
         if val_set is not None and training_args.evaluation_strategy == "no":
             training_args.evaluation_strategy = "epoch"
 
-        StdFedTrainerMixin.__init__(
+        HomoTrainerMixin.__init__(
             self,
             ctx=ctx,
             model=model,
             loss_fn=loss_fn,
             optimizer=optimizer,
             training_args=training_args,
             fed_args=fed_args,
@@ -882,15 +1075,15 @@
         Trainer.__init__(
             self,
             model=model,
             args=self._args,
             train_dataset=train_set,
             eval_dataset=val_set,
             data_collator=data_collator,
-            optimizers=[optimizer, scheduler],
+            optimizers=(optimizer, scheduler),
             tokenizer=tokenizer,
             compute_metrics=self._compute_metrics_warp_func,
         )
 
         self._add_fate_callback(self.callback_handler)
 
     def init_aggregator(self, ctx: Context, fed_arg: FedArguments):
@@ -917,75 +1110,86 @@
         ignore_keys: Optional[List[str]] = None,
     ):
         if self._use_hf_default_behavior:
             return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)
         else:
             # (features, labels), this format is used in FATE-1.x
             # now the model is in eval status
+            inputs = self._prepare_inputs(inputs)
             if isinstance(inputs, tuple) or isinstance(inputs, list) and len(inputs) == 2:
                 with torch.no_grad():
                     feats, labels = inputs
                     logits = model(feats)
-                    return (None, logits, labels)
+                    return None, logits, labels
             else:
                 return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)
 
 
-class FedTrainerServer(object):
+class HomoTrainerServer(object):
     def __init__(self, ctx: Context, local_mode: bool = False) -> None:
         self.ctx = ctx
         self.local_mode = local_mode
         self._max_steps = None
         self._parameter_check_callback = FedParameterAlignCallback(self, self.ctx, None, None, is_server=True)
         self._max_aggregation = None
+        self.aggregator = None
+        self.can_aggregate_loss = True
 
     def set_fed_context(self, ctx: Context):
         assert isinstance(ctx, Context), "ctx must be a Context object, but got {}".format(ctx)
         self.ctx = ctx
 
     def set_local_mode(self):
         self.local_mode = True
         logger.info("trainer set to local mode")
 
     def set_fed_mode(self):
         self.local_mode = False
         logger.info("trainer set to federated mode")
 
-    def init_aggregator(self, ctx: Context):
-        return None
-
     def on_train_end(self, ctx: Context, aggregator: Aggregator):
         pass
 
     def on_train_begin(self, ctx: Context, aggregator: Aggregator):
         pass
 
     def on_init_end(self, ctx: Context, aggregator: Aggregator):
         pass
 
-    def on_federation(self, ctx: Context, aggregator: Aggregator):
+    def on_federation(self, ctx: Context, aggregator: Aggregator, agg_iter_idx: int):
+        pass
+
+    def init_aggregator(self, ctx: Context) -> Aggregator:
+        pass
+
+    def _aggregate_loss(self, ctx):
         pass
 
     def train(self):
         if self.local_mode:
             logger.info("Local model is set, skip initializing fed setting & aggregator")
             return
 
         self.aggregator: Aggregator = self.init_aggregator(self.ctx)
         logger.info("Initialized aggregator Done: {}".format(self.aggregator))
         self._parameter_check_callback.on_train_begin(None, None, None)  # only get parameters from clients and align
         parameters = self._parameter_check_callback.get_parameters()
         self._max_aggregation = parameters["max_aggregation"]
+        self.can_aggregate_loss = parameters["can_aggregate_loss"]
         logger.info("checked parameters are {}".format(parameters))
 
         self.on_init_end(self.ctx, aggregator=self.aggregator)
         self.on_train_begin(self.ctx, aggregator=self.aggregator)
 
         ctx = self.ctx
         for i in range(self._max_aggregation):
             sub_ctx = ctx.sub_ctx("aggregation").indexed_ctx(i)
-            self.on_federation(sub_ctx, aggregator=self.aggregator)
+            self.on_federation(sub_ctx, aggregator=self.aggregator, agg_iter_idx=i)
+            if self.can_aggregate_loss:
+                loss_sub_ctx = ctx.sub_ctx("loss_aggregation").indexed_ctx(i)
+                loss = self.aggregator.loss_aggregation(loss_sub_ctx)
+                sub_ctx.metrics.log_loss("loss", loss)
 
         self.on_train_end(self.ctx, aggregator=self.aggregator)
 
     def predict(self):
         pass
```

### Comparing `pyfate-2.0.0b0/fate/ml/preprocessing/__init__.py` & `pyfate-2.1.0/fate/arch/unify/_uuid.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,10 +8,12 @@
 #      http://www.apache.org/licenses/LICENSE-2.0
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+from uuid import uuid1
 
-from .feature_scale import FeatureScale
-from .union import Union
+
+def uuid():
+    return uuid1().hex
```

### Comparing `pyfate-2.0.0b0/fate/ml/preprocessing/feature_scale.py` & `pyfate-2.1.0/fate/ml/preprocessing/feature_scale.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,16 +38,15 @@
 
     def transform(self, ctx: Context, test_data):
         return self._scaler.transform(ctx, test_data)
 
     def get_model(self):
         scaler_info = self._scaler.to_model()
         model_data = dict(scaler_info=scaler_info)
-        return {"data": model_data, "meta": {"method": self.method,
-                                             "model_type": "feature_scale"}}
+        return {"data": model_data, "meta": {"method": self.method, "model_type": "feature_scale"}}
 
     def restore(self, model):
         self._scaler.from_model(model)
 
     @classmethod
     def from_model(cls, model) -> "FeatureScale":
         scaler = FeatureScale(model["meta"]["method"])
@@ -130,15 +129,15 @@
         Transformation is given by:
             X_scaled = (X * scale - scale_min) + feature_range_min
         where scale = feature_range / (X_train.max() - X_train.min()) and scale_min = X_train.min() * scale
 
         """
         test_data_select = test_data[self.select_col]
 
-        data_scaled = test_data_select * self._scale - (self._scale_min + self._range_min)
+        data_scaled = test_data_select * self._scale - self._scale_min + self._range_min
         if self.strict_range:
             # restrict feature output within given feature value range
             data_scaled = data_scaled[data_scaled >= self._range_min].fillna(self._range_min)
             data_scaled = data_scaled[data_scaled <= self._range_max].fillna(self._range_max)
         test_data[self.select_col] = data_scaled
         return test_data
```

### Comparing `pyfate-2.0.0b0/fate/ml/preprocessing/union.py` & `pyfate-2.1.0/fate/ml/preprocessing/union.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/statistics/__init__.py` & `pyfate-2.1.0/fate/ml/statistics/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,8 +9,9 @@
 #
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
+from .pearson_correlation import PearsonCorrelation
 from .statistics import FeatureStatistics
```

### Comparing `pyfate-2.0.0b0/fate/ml/statistics/statistics.py` & `pyfate-2.1.0/fate/ml/statistics/statistics.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,17 +31,15 @@
         self.summary = StatisticsSummary(ddof, bias, relative_error)
 
     def fit(self, ctx: Context, input_data, validate_data=None) -> None:
         self.summary.compute_metrics(input_data, self.metrics)
 
     def get_model(self):
         model = self.summary.to_model()
-        output_model = {"data": model,
-                        "meta": {"metrics": self.metrics,
-                                 "model_type": "statistics"}}
+        output_model = {"data": model, "meta": {"metrics": self.metrics, "model_type": "statistics"}}
         return output_model
 
     def restore(self, model):
         self.summary.restore(model)
 
     def from_model(cls, model) -> "FeatureStatistics":
         stat = FeatureStatistics(model["meta"]["metrics"])
```

### Comparing `pyfate-2.0.0b0/fate/ml/utils/__init__.py` & `pyfate-2.1.0/fate/ml/ensemble/learner/decision_tree/tree_core/__init__.py`

 * *Files identical despite different names*

### Comparing `pyfate-2.0.0b0/fate/ml/utils/_convergence.py` & `pyfate-2.1.0/fate/ml/utils/_convergence.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,31 +21,30 @@
 logger = logging.getLogger(__name__)
 
 
 class _ConvergeFunction:
     def __init__(self, eps):
         self.eps = eps
 
-    def is_converge(self, loss): pass
+    def is_converge(self, loss):
+        pass
 
 
 class _DiffConverge(_ConvergeFunction):
     """
     Judge convergence by the difference between two iterations.
     If the difference is smaller than eps, converge flag will be provided.
     """
 
     def __init__(self, eps):
         super().__init__(eps=eps)
         self.pre_loss = None
 
     def is_converge(self, loss):
-        logger.debug(
-            "In diff converge function, pre_loss: {}, current_loss: {}".format(
-                self.pre_loss, loss))
+        logger.debug("In diff converge function, pre_loss: {}, current_loss: {}".format(self.pre_loss, loss))
 
         converge_flag = False
         if self.pre_loss is None:
             pass
         elif abs(self.pre_loss - loss) < self.eps:
             converge_flag = True
         self.pre_loss = loss
@@ -71,16 +70,24 @@
     Use 2-norm of gradient to judge whether converge or not.
     """
 
     def __init__(self, eps):
         super().__init__(eps=eps)
         self.pre_weight = None
 
-    def is_converge(self, delta_weight, weight=None):
+    def set_pre_weight(self, weight):
+        self.pre_weight = weight
+
+    @staticmethod
+    def compute_weight_diff(delta_weight):
         weight_diff = torch.linalg.norm(delta_weight, 2)
+        return weight_diff
+
+    def is_converge(self, delta_weight, weight=None):
+        weight_diff = self.compute_weight_diff(delta_weight)
         if weight is None:
             # avoid tensor[bool]
             if weight_diff < self.eps:
                 return True
             return False
         if self.pre_weight is None:
             self.pre_weight = weight
@@ -93,16 +100,15 @@
 def converge_func_factory(early_stop, tol):
     # try:
     #     converge_func = param.converge_func
     #     eps = param.eps
     # except AttributeError:
     #     raise AttributeError("Converge Function parameters has not been totally set")
 
-    if early_stop == 'diff':
+    if early_stop == "diff":
         return _DiffConverge(tol)
-    elif early_stop == 'weight_diff':
+    elif early_stop == "weight_diff":
         return _WeightDiffConverge(tol)
-    elif early_stop == 'abs':
+    elif early_stop == "abs":
         return _AbsConverge(tol)
     else:
-        raise NotImplementedError(
-            "Converge Function method cannot be recognized: {}".format(early_stop))
+        raise NotImplementedError("Converge Function method cannot be recognized: {}".format(early_stop))
```

### Comparing `pyfate-2.0.0b0/fate/ml/utils/_model_param.py` & `pyfate-2.1.0/fate/ml/utils/_model_param.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,35 +12,55 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 import torch
 
 
+def get_initialize_func(**kwargs):
+    method = kwargs["method"]
+    random_state = kwargs.get("random_state", None)
+
+    if method == "zeros":
+        return lambda shape: torch.zeros(shape)
+    elif method == "ones":
+        return lambda shape: torch.ones(shape)
+    elif method == "random":
+        if random_state is not None:
+            generator = torch.Generator().manual_seed(random_state)
+            return lambda shape: torch.randn(shape, generator=generator)
+        return lambda shape: torch.randn(shape)
+    elif method == "random_uniform":
+        if random_state is not None:
+            generator = torch.Generator().manual_seed(random_state)
+            return lambda shape: torch.rand(shape, generator=generator)
+        return lambda shape: torch.rand(shape)
+    else:
+        raise NotImplementedError(f"Unknown initialization method: {method}")
+
+
 def initialize_param(coef_len, **kwargs):
     param_len = coef_len
     method = kwargs["method"]
     fit_intercept = kwargs["fit_intercept"]
     random_state = kwargs.get("random_state", None)
     if fit_intercept:
         param_len = param_len + 1
-    if method == 'zeros':
+    if method == "zeros":
         return torch.zeros((param_len, 1), requires_grad=True)
-    elif method == 'ones':
+    elif method == "ones":
         return torch.ones((param_len, 1), requires_grad=True)
-    elif method == 'consts':
-        return torch.full(
-            (param_len, 1), float(
-                kwargs["fill_val"]), requires_grad=True)
-    elif method == 'random':
+    elif method == "consts":
+        return torch.full((param_len, 1), float(kwargs["fill_val"]), requires_grad=True)
+    elif method == "random":
         if random_state is not None:
             generator = torch.Generator().manual_seed(random_state)
             return torch.randn((param_len, 1), generator=generator, requires_grad=True)
         return torch.randn((param_len, 1), requires_grad=True)
-    elif method == 'random_uniform':
+    elif method == "random_uniform":
         if random_state is not None:
             generator = torch.Generator().manual_seed(random_state)
             return torch.rand((param_len, 1), generator=generator, requires_grad=True)
         return torch.rand((param_len, 1), requires_grad=True)
     else:
         raise NotImplementedError(f"Unknown initialization method: {method}")
```

### Comparing `pyfate-2.0.0b0/fate/ml/utils/_optimizer.py` & `pyfate-2.1.0/fate/ml/utils/_optimizer.py`

 * *Files 7% similar despite different names*

```diff
@@ -49,44 +49,34 @@
         self.lr_scheduler.load_state_dict(dict["lr_scheduler"])
 
     def get_last_lr(self):
         return self.get_last_lr()
 
 
 class Optimizer(object):
-    def __init__(
-            self,
-            method=None,
-            penalty=None,
-            alpha=None,
-            optim_param: dict = None,
-            iters: int = 0):
+    def __init__(self, method=None, penalty=None, alpha=None, optim_param: dict = None, iters: int = 0):
         self.method = method
         self.optim_param = optim_param
         self.iters = iters
         self.l2_penalty = True if penalty == "l2" else False
         self.l1_penalty = True if penalty == "l1" else False
         self.alpha = alpha
 
         self.model_parameter = None
         self.prev_model_parameter = None
         self.optimizer = None
 
-    def init_optimizer(
-            self,
-            model_parameter_length=None,
-            model_parameter=None,
-            dtype=torch.float32):
+    def init_optimizer(self, model_parameter_length=None, model_parameter=None, dtype=torch.float32):
         # allow group of parameter in future
         if model_parameter_length is not None:
-            model_parameter = torch.nn.parameter.Parameter(torch.zeros(
-                (model_parameter_length, 1), requires_grad=True, dtype=dtype))
+            model_parameter = torch.nn.parameter.Parameter(
+                torch.zeros((model_parameter_length, 1), requires_grad=True, dtype=dtype)
+            )
         self.model_parameter = model_parameter
-        self.optimizer = optimizer_factory(
-            [model_parameter], self.method, self.optim_param)
+        self.optimizer = optimizer_factory([model_parameter], self.method, self.optim_param)
         # for regularization
         # self.alpha = self.optimizer.state_dict()['param_groups'][0]['alpha']
 
     def step(self, gradient):
         # logger.info(f"before copy, model parameter: {self.model_parameter}")
         self.prev_model_parameter = self.model_parameter.data.clone()
         self.model_parameter.grad = gradient
@@ -172,24 +162,17 @@
             else:
                 new_grad = grad + self.alpha * model_weights
         else:
             new_grad = grad
 
         return new_grad
 
-    def regularization_update(
-            self,
-            model_weights,
-            grad,
-            fit_intercept,
-            lr,
-            prev_round_weights=None):
+    def regularization_update(self, model_weights, grad, fit_intercept, lr, prev_round_weights=None):
         if self.l1_penalty:
-            model_weights = self._l1_updator(
-                model_weights, grad, fit_intercept, lr)
+            model_weights = self._l1_updator(model_weights, grad, fit_intercept, lr)
         else:
             model_weights = model_weights - grad
         """elif self.l2_penalty:
                     model_weights = self._l2_updator(model_weights, grad)
                 """
         """if prev_round_weights is not None:  # additional proximal term for homo
             coef_ = model_weights.unboxed
@@ -212,16 +195,15 @@
         return model_weights
 
     def __l1_loss_norm(self, model_weights):
         loss_norm = torch.sum(self.alpha * model_weights)
         return loss_norm.reshape((1, 1))
 
     def __l2_loss_norm(self, model_weights):
-        loss_norm = 0.5 * self.alpha * \
-            torch.matmul(model_weights.T, model_weights)
+        loss_norm = 0.5 * self.alpha * torch.matmul(model_weights.T, model_weights)
         return loss_norm
 
     """def __add_proximal(self, model_weights, prev_round_weights):
         prev_round_coef_ = prev_round_weights.coef_
         coef_ = model_weights.coef_
         diff = coef_ - prev_round_coef_
         loss_norm = self.mu * 0.5 * np.dot(diff, diff)
@@ -260,31 +242,23 @@
                                       raise_overflow_error=delta_s.raise_overflow_error)
         else:
             return LinearModelWeights(np.zeros_like(delta_s.unboxed),
                                       fit_intercept=delta_s.fit_intercept,
                                       raise_overflow_error=delta_s.raise_overflow_error)
     """
 
-    def update_weights(
-            self,
-            model_weights,
-            grad,
-            fit_intercept,
-            lr,
-            prev_round_weights=None,
-            has_applied=True):
+    def update_weights(self, model_weights, grad, fit_intercept, lr, prev_round_weights=None, has_applied=True):
         """if not has_applied:
             grad = self.add_regular_to_grad(grad, model_weights)
             delta_grad = self.apply_gradients(grad)
         else:"""
         # logger.info(
         #     f"before update, model weights: {model_weights}, delta_grad: {grad}")
         delta_grad = grad
-        model_weights = self.regularization_update(
-            model_weights, delta_grad, fit_intercept, lr, prev_round_weights)
+        model_weights = self.regularization_update(model_weights, delta_grad, fit_intercept, lr, prev_round_weights)
         # (f"after update, model weights: {model_weights}")
 
         return model_weights
 
 
 def separate(value, size_list):
     """
@@ -328,22 +302,20 @@
     elif optimizer_type == "rmsprop":
         return torch.optim.RMSprop(model_parameter, **optimizer_params)
     elif optimizer_type == "rprop":
         return torch.optim.Rprop(model_parameter, **optimizer_params)
     elif optimizer_type == "sgd":
         return torch.optim.SGD(model_parameter, **optimizer_params)
     else:
-        raise NotImplementedError(
-            "Optimize method cannot be recognized: {}".format(optimizer_type))
+        raise NotImplementedError("Optimize method cannot be recognized: {}".format(optimizer_type))
 
 
 def lr_scheduler_factory(optimizer, method, scheduler_param):
     scheduler_method = method
     if scheduler_method == "constant":
         return torch.optim.lr_scheduler.ConstantLR(optimizer, **scheduler_param)
     elif scheduler_method == "step":
         return torch.optim.lr_scheduler.StepLR(optimizer, **scheduler_param)
     elif scheduler_method == "linear":
         return torch.optim.lr_scheduler.LinearLR(optimizer, **scheduler_param)
     else:
-        raise NotImplementedError(
-            f"Learning rate method cannot be recognized: {scheduler_method}")
+        raise NotImplementedError(f"Learning rate method cannot be recognized: {scheduler_method}")
```

### Comparing `pyfate-2.0.0b0/fate/ml/utils/callbacks.py` & `pyfate-2.1.0/fate/ml/utils/callbacks.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,56 +1,72 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 from fate.ml.evaluation.tool import get_metric_names, get_specified_metrics
 from fate.ml.abc.module import Module
 
 
 class CallbackParam(object):
-
-    def __init__(self, 
-                 callback_types: list, 
-                 metrics: list, 
-                 evaluation_freq: int = None, 
-                 early_stopping_rounds: int = None,
-                 checkpoint_freq: int = None,
-                 use_first_metric: bool = False) -> None:
-
+    def __init__(
+        self,
+        callback_types: list,
+        metrics: list,
+        evaluation_freq: int = None,
+        early_stopping_rounds: int = None,
+        checkpoint_freq: int = None,
+        use_first_metric: bool = False,
+    ) -> None:
         if not isinstance(callback_types, list) or len(callback_types) == 0:
             raise ValueError("callback_types must be a list with at least one type.")
 
         if not isinstance(metrics, list) or len(metrics) == 0:
             raise ValueError("metrics must be a list with at least one metric.")
-            
-        for param, param_name in [(evaluation_freq, "evaluation_freq"), 
-                                  (early_stopping_rounds, "early_stopping_rounds"), 
-                                  (checkpoint_freq, "checkpoint_freq")]:
+
+        for param, param_name in [
+            (evaluation_freq, "evaluation_freq"),
+            (early_stopping_rounds, "early_stopping_rounds"),
+            (checkpoint_freq, "checkpoint_freq"),
+        ]:
             if param is not None and (not isinstance(param, int) or param <= 0):
                 raise ValueError(f"{param_name} must be a positive integer or None.")
-            
+
         if not isinstance(use_first_metric, bool):
             raise ValueError("use_first_metric must be a boolean.")
 
         self.callback_types = callback_types
         self.metrics = metrics
         self.evaluation_freq = evaluation_freq
         self.early_stopping_rounds = early_stopping_rounds
         self.checkpoint_freq = checkpoint_freq
         self.use_first_metric = use_first_metric
 
     def __str__(self) -> str:
-        return (f'Callback types: {self.callback_types}, '
-                f'Metrics: {self.metrics}, '
-                f'Evaluation frequency: {self.evaluation_freq}, '
-                f'Early stopping rounds: {self.early_stopping_rounds}, '
-                f'Use first metric for early stopping: {self.use_first_metric}, '
-                f'Checkpoint frequency: {self.checkpoint_freq}')
-
-
+        return (
+            f"Callback types: {self.callback_types}, "
+            f"Metrics: {self.metrics}, "
+            f"Evaluation frequency: {self.evaluation_freq}, "
+            f"Early stopping rounds: {self.early_stopping_rounds}, "
+            f"Use first metric for early stopping: {self.use_first_metric}, "
+            f"Checkpoint frequency: {self.checkpoint_freq}"
+        )
 
 
 class Callbacks(object):
-
     def __init__(self, model: Module, callback_params) -> None:
         pass
 
     def on_train_begin(self, ctx):
         pass
 
     def on_train_end(self, ctx):
@@ -68,8 +84,8 @@
     def on_batch_end(self, ctx, batch_index):
         pass
 
     def need_stop(self, ctx):
         pass
 
     def get_best_model(self):
-        pass
+        pass
```

### Comparing `pyfate-2.0.0b0/fate/ml/utils/model_serdes.py` & `pyfate-2.1.0/fate/ml/utils/model_serdes.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,16 +19,15 @@
     from google.protobuf import json_format
 
     serialized_models: Dict[str, Tuple[str, bytes, dict]] = {}
 
     for model_name, buffer_object in models.items():
         serialized_string = buffer_object.SerializeToString()
         pb_name = type(buffer_object).__name__
-        json_format_dict = json_format.MessageToDict(
-            buffer_object, including_default_value_fields=True)
+        json_format_dict = json_format.MessageToDict(buffer_object, including_default_value_fields=True)
 
         serialized_models[model_name] = (
             pb_name,
             serialized_string,
             json_format_dict,
         )
```

### Comparing `pyfate-2.0.0b0/fate/ml/utils/predict_tools.py` & `pyfate-2.1.0/fate/ml/utils/predict_tools.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,135 +1,160 @@
+#
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import json
 from typing import Literal
 
 import numpy as np
 import pandas as pd
 
 from fate.arch.dataframe import DataFrame
 from fate.arch.dataframe import PandasReader
 
 # DATA SET COLUMNS
-TRAIN_SET = 'train_set'
-VALIDATE_SET = 'validate_set'
-TEST_SET = 'test_set'
+TRAIN_SET = "train_set"
+VALIDATE_SET = "validate_set"
+TEST_SET = "test_set"
 
 # PREDICT RESULT COLUMNS
 PREDICT_RESULT = "predict_result"
 PREDICT_SCORE = "predict_score"
 PREDICT_DETAIL = "predict_detail"
 LABEL = "label"
 
+# PREDICT RESULT OF CausalLanguageModeling
+PREDICT_TEXT = "predict_text"
+
 # TASK TYPE
-BINARY = 'binary'
-MULTI = 'multi'
-REGRESSION = 'regression'
-OTHER = 'other'
+BINARY = "binary"
+MULTI = "multi"
+REGRESSION = "regression"
+CAUSAL_LM = "causal_lm"
+OTHER = "other"
 
 
 def predict_detail_dict_to_str(result_dict):
-    return "\"" + json.dumps(result_dict).replace("\"", "\'") + "\""
+    return '"' + json.dumps(result_dict).replace('"', "'") + '"'
 
 
 def add_ids(df: pd.DataFrame, match_id: pd.DataFrame, sample_id: pd.DataFrame):
     df = pd.concat([df, match_id, sample_id], axis=1)
     return df
 
 
 def to_dist_df(ctx, sample_id_name, match_id_name, result_df: pd.DataFrame):
-
     if LABEL in result_df:
         reader = PandasReader(
-            sample_id_name=sample_id_name,
-            match_id_name=match_id_name,
-            label_name=LABEL,
-            dtype="object")
+            sample_id_name=sample_id_name, match_id_name=match_id_name, label_name=LABEL, dtype="object"
+        )
     else:
-        reader = PandasReader(
-            sample_id_name=sample_id_name,
-            match_id_name=match_id_name,
-            dtype="object")
+        reader = PandasReader(sample_id_name=sample_id_name, match_id_name=match_id_name, dtype="object")
     data = reader.to_frame(ctx, result_df)
     return data
 
 
-def compute_predict_details(dataframe: DataFrame, task_type: Literal['binary', 'multi', 'regression'], classes: list = None, threshold=0.5):
+def compute_predict_details(
+    dataframe: DataFrame, task_type: Literal["binary", "multi", "regression"], classes: list = None, threshold=0.5
+):
+    assert task_type in [
+        BINARY,
+        MULTI,
+        REGRESSION,
+        OTHER,
+    ], "task_type must be one of {} as a std task, but got {}".format([BINARY, MULTI, REGRESSION, OTHER], task_type)
 
-    assert task_type in [BINARY, MULTI, REGRESSION,
-                         OTHER], 'task_type must be one of {} as a std task, but got {}'.format(
-        [BINARY, MULTI, REGRESSION, OTHER], task_type)
-
-    assert threshold >= 0 and threshold <= 1, 'threshold must be float in [0, 1], but got {}'.format(threshold)
+    assert threshold >= 0 and threshold <= 1, "threshold must be float in [0, 1], but got {}".format(threshold)
 
     if not isinstance(dataframe, DataFrame):
-        raise ValueError('dataframe must be a fate DataFrame, but got {}'.format(type(dataframe)))
+        raise ValueError("dataframe must be a fate DataFrame, but got {}".format(type(dataframe)))
     if dataframe.schema.label_name is not None and dataframe.schema.label_name != LABEL:
         dataframe.rename(label_name=LABEL)
-    assert PREDICT_SCORE in dataframe.schema.columns, 'column {} is not found in input dataframe'.format(PREDICT_SCORE)
+    assert PREDICT_SCORE in dataframe.schema.columns, "column {} is not found in input dataframe".format(PREDICT_SCORE)
 
     if task_type == BINARY and task_type == MULTI:
         if classes is None or (not isinstance(classes, list) and len(classes) < 2):
-            raise ValueError('task_type is binary or multi, but classes is None, or classes length is less than 2')
+            raise ValueError("task_type is binary or multi, but classes is None, or classes length is less than 2")
 
     if task_type == BINARY:
         if len(classes) == 2:
             neg_class, pos_class = classes[0], classes[1]
-            dataframe[[PREDICT_RESULT, PREDICT_DETAIL]] = dataframe.apply_row( \
-                lambda v: [int(v[PREDICT_SCORE] > threshold),
-                           predict_detail_dict_to_str({neg_class: 1 - float(v[PREDICT_SCORE]),
-                                                       pos_class: float(v[PREDICT_SCORE])})],
-                enable_type_align_checking=False)
+            dataframe[[PREDICT_RESULT, PREDICT_DETAIL]] = dataframe.apply_row(
+                lambda v: [
+                    int(v[PREDICT_SCORE] > threshold),
+                    predict_detail_dict_to_str(
+                        {neg_class: 1 - float(v[PREDICT_SCORE]), pos_class: float(v[PREDICT_SCORE])}
+                    ),
+                ],
+                enable_type_align_checking=False,
+            )
         else:
-            raise ValueError(
-                'task_type is binary, but classes length is not 2: {}'.format(classes))
-        
+            raise ValueError("task_type is binary, but classes length is not 2: {}".format(classes))
+
     elif task_type == REGRESSION:
-        dataframe[[PREDICT_RESULT, PREDICT_DETAIL]] = dataframe.apply_row( \
+        dataframe[[PREDICT_RESULT, PREDICT_DETAIL]] = dataframe.apply_row(
             lambda v: [v[PREDICT_SCORE], predict_detail_dict_to_str({PREDICT_SCORE: float(v[PREDICT_SCORE])})],
-            enable_type_align_checking=False)
+            enable_type_align_checking=False,
+        )
 
     elif task_type == MULTI:
 
         def handle_multi(v: pd.Series):
             predict_result = np.argmax(v[PREDICT_SCORE])
-            assert len(v[PREDICT_SCORE]) == len(classes), 'predict score length is not equal to classes length,\
-                predict score is {}, but classes are {}, please check the data you provided'.format(v[PREDICT_SCORE], classes)
+            assert len(v[PREDICT_SCORE]) == len(
+                classes
+            ), "predict score length is not equal to classes length,\
+                predict score is {}, but classes are {}, please check the data you provided".format(
+                v[PREDICT_SCORE], classes
+            )
             predict_details = {classes[j]: float(v[PREDICT_SCORE][j]) for j in range(len(classes))}
             return [predict_result, predict_detail_dict_to_str(predict_details)]
 
-        dataframe[[PREDICT_RESULT, PREDICT_DETAIL]] = dataframe.apply_row(handle_multi, enable_type_align_checking=False)
+        dataframe[[PREDICT_RESULT, PREDICT_DETAIL]] = dataframe.apply_row(
+            handle_multi, enable_type_align_checking=False
+        )
         predict_score = dataframe[PREDICT_SCORE].apply_row(lambda v: max(v[PREDICT_SCORE]))
         dataframe[PREDICT_SCORE] = predict_score
 
     return dataframe
 
 
 def array_to_predict_df(
-        ctx,
-        task_type: Literal['binary', 'multi', 'regression'],
-        pred: np.ndarray,
-        match_ids: np.ndarray,
-        sample_ids: np.ndarray,
-        match_id_name: str,
-        sample_id_name: str,
-        label: np.array = None,
-        threshold=0.5,
-        classes: list = None):
-
+    ctx,
+    task_type: Literal["binary", "multi", "regression"],
+    pred: np.ndarray,
+    match_ids: np.ndarray,
+    sample_ids: np.ndarray,
+    match_id_name: str,
+    sample_id_name: str,
+    label: np.array = None,
+    threshold=0.5,
+    classes: list = None,
+):
     df = pd.DataFrame()
     if len(pred.shape) == 1:
         df[PREDICT_SCORE] = np.array(pred)
     elif len(pred.shape) == 2:
         if pred.shape[1] == 1:
             df[PREDICT_SCORE] = np.array(pred).flatten()
         else:
             df[PREDICT_SCORE] = np.array(pred).tolist()
     else:
-        raise ValueError(
-            'This is not a FATE std task, pred scores shape are {}'.format(
-                pred.shape))
+        raise ValueError("This is not a FATE std task, pred scores shape are {}".format(pred.shape))
 
     if label is not None:
         if len(label.shape) == 1:
             label = label.flatten()
         elif len(label.shape) == 2 and label.shape[1] == 1:
             label = label.flatten()
         else:
```

### Comparing `pyfate-2.0.0b0/pyfate.egg-info/PKG-INFO` & `pyfate-2.1.0/pyfate.egg-info/PKG-INFO`

 * *Files 15% similar despite different names*

```diff
@@ -1,50 +1,49 @@
 Metadata-Version: 2.1
 Name: pyfate
-Version: 2.0.0b0
+Version: 2.1.0
 Home-page: https://fate.fedai.org/
 Author: FederatedAI
 Author-email: contact@FedAI.org
 License: Apache-2.0 License
 Keywords: federated learning
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Requires-Dist: lmdb==1.3.0
 Requires-Dist: torch==1.13.1
 Requires-Dist: fate_utils
 Requires-Dist: pydantic==1.10.12
 Requires-Dist: cloudpickle==2.1.0
 Requires-Dist: click
 Requires-Dist: ruamel.yaml==0.16
-Requires-Dist: scikit-learn==1.2.1; sys_platform == "darwin" and platform_machine == "arm64"
-Requires-Dist: scikit-learn==1.0.1; sys_platform != "darwin" or platform_machine != "arm64"
+Requires-Dist: scikit-learn
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: transformers
 Requires-Dist: accelerate
 Requires-Dist: beautifultable
 Requires-Dist: requests
 Requires-Dist: grpcio
 Requires-Dist: protobuf
+Requires-Dist: rich
+Requires-Dist: omegaconf
+Requires-Dist: opentelemetry-api
+Requires-Dist: opentelemetry-sdk
+Requires-Dist: mmh3==3.0.0
+Requires-Dist: safetensors
 Provides-Extra: rabbitmq
-Requires-Dist: pika==1.2.1; extra == "rabbitmq"
+Requires-Dist: pika; extra == "rabbitmq"
 Provides-Extra: pulsar
 Requires-Dist: pulsar-client==2.10.2; sys_platform != "darwin" and extra == "pulsar"
-Requires-Dist: pulsar-client==2.10.1; sys_platform == "darwin" and extra == "pulsar"
-Requires-Dist: urllib3==1.26.5; extra == "pulsar"
+Requires-Dist: pulsar-client; sys_platform == "darwin" and extra == "pulsar"
+Requires-Dist: urllib3; extra == "pulsar"
 Provides-Extra: spark
 Requires-Dist: pyspark; extra == "spark"
 Provides-Extra: eggroll
-Requires-Dist: grpcio==1.46.3; extra == "eggroll"
-Requires-Dist: grpcio-tools==1.46.3; extra == "eggroll"
-Requires-Dist: numba==0.56.4; extra == "eggroll"
-Requires-Dist: protobuf==3.19.6; extra == "eggroll"
-Requires-Dist: mmh3==3.0.0; extra == "eggroll"
-Requires-Dist: cachetools>=3.0.0; extra == "eggroll"
-Requires-Dist: cloudpickle==2.1.0; extra == "eggroll"
+Requires-Dist: grpcio-tools; extra == "eggroll"
 Requires-Dist: psutil>=5.7.0; extra == "eggroll"
 Provides-Extra: all
 Requires-Dist: pyfate[eggroll,pulsar,rabbitmq,spark]; extra == "all"
 
 [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![CodeStyle](https://img.shields.io/badge/Check%20Style-Google-brightgreen)](https://checkstyle.sourceforge.io/google_style.html) [![Style](https://img.shields.io/badge/Check%20Style-Black-black)](https://checkstyle.sourceforge.io/google_style.html) [![Build Status](https://travis-ci.org/FederatedAI/FATE.svg?branch=master)](https://travis-ci.org/FederatedAI/FATE)
 [![codecov](https://codecov.io/gh/FederatedAI/FATE/branch/master/graph/badge.svg)](https://codecov.io/gh/FederatedAI/FATE)
 [![Documentation Status](https://readthedocs.org/projects/fate/badge/?version=latest)](https://fate.readthedocs.io/en/latest/?badge=latest)
@@ -52,55 +51,70 @@
 [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6308/badge)](https://bestpractices.coreinfrastructure.org/projects/6308)
 
 
 <div align="center">
   <img src="./doc/images/FATE_logo.png">
 </div>
 
-[DOCS](./doc) | [](./README_zh.md)
 
 FATE (Federated AI Technology Enabler) is the world's first industrial grade federated learning open source framework to enable enterprises and institutions to collaborate on data while protecting data security and privacy. 
 It implements secure computation protocols based on homomorphic encryption and multi-party computation (MPC). 
 Supporting various federated learning scenarios, FATE now provides a host of federated learning algorithms, including logistic regression, tree-based algorithms, deep learning and transfer learning.
 
 
 FATE is an open source project hosted by Linux Foundation. The [Technical Charter](https://github.com/FederatedAI/FATE-Community/blob/master/FATE_Project_Technical_Charter.pdf) sets forth the responsibilities and procedures for technical contribution to, and oversight of, the FATE (Federated AI Technology Enabler) Project. 
 
 <https://fate.readthedocs.io/en/latest>
 
+
 ## Getting Started
+FATE can be deployed on a single node or on multiple nodes. Choose the deployment approach which matches your environment.
+[Release version can be downloaded here.](https://github.com/FederatedAI/FATE/wiki/Download)
+
 
-### Version < 2.0
-Releases history can be found in [releases](https://github.com/FederatedAI/FATE/releases), deployment resources can be found on [wiki](https://github.com/FederatedAI/FATE/wiki/Download)
+### Version >= 2.0
+### Standalone deployment
 
-### Version == 2.0.0-beta
-#### Standalone deployment
 - Deploying FATE on a single node via PyPI, pre-built docker images or installers. It is for simple testing purposes. Refer to this [guide](./deploy/standalone-deploy/).
 
 ### Cluster deployment
 Deploying FATE to multiple nodes to achieve scalability, reliability and manageability.
-
 - [Cluster deployment by CLI](./deploy/cluster-deploy): Using CLI to deploy a FATE cluster.
 
 ### Quick Start
-- [Training Demo With Installing FATE AND FATE-Flow From Pypi](doc/2.0/quick_start.md)
 - [Training Demo With Installing FATE Only From Pypi](doc/2.0/fate/ml)
+- [Training Demo With Installing FATE AND FATE-Flow From Pypi](doc/2.0/fate/quick_start.md)
+
+### More examples
+- [ML examples](examples/launchers)
+- [PipeLine examples](examples/pipeline)
+
+## Documentation
+
+### FATE Design
+- [Architecture](./doc/architecture/README.md): Building Unified and Standardized API for Heterogeneous Computing Engines Interconnection
+- [FATE Algorithm Components](./doc/2.0/fate/components/README.md): Building Standardized Algorithm Components for different Scheduling Engines
+- [OSX (Open Site Exchange)](./doc/2.0/osx/osx.md): Building Open Platform for Cross-Site Communication Interconnection
+- [FATE-Flow](https://github.com/FederatedAI/FATE-Flow/blob/main/doc/fate_flow.md): Building Open and Standardized Scheduling Platform for Scheduling Interconnection 
+- [PipeLine Design](https://github.com/FederatedAI/FATE-Client/blob/main/doc/pipeline.md): Building Scalable Federated DSL for Application Layer Interconnection And Providing Tools For Fast Federated Modeling
+- [RoadMap](./doc/images/roadmap.png)
+- [Paper & Conference](./doc/resources/README.md)
 
 ## Related Repositories (Projects)
 - [KubeFATE](https://github.com/FederatedAI/KubeFATE): An operational tool for the FATE platform using cloud native technologies such as containers and Kubernetes.
 - [FATE-Flow](https://github.com/FederatedAI/FATE-Flow): A multi-party secure task scheduling platform for federated learning pipeline.
 - [FATE-Board](https://github.com/FederatedAI/FATE-Board): A suite of visualization tools to explore and understand federated models easily and effectively.
 - [FATE-Serving](https://github.com/FederatedAI/FATE-Serving): A high-performance and production-ready serving system for federated learning models.
 - [FATE-Cloud](https://github.com/FederatedAI/FATE-Cloud): An infrastructure for building and managing industrial-grade federated learning cloud services.
 - [EggRoll](https://github.com/WeBankFinTech/eggroll): A simple high-performance computing framework for (federated) machine learning.
 - [AnsibleFATE](https://github.com/FederatedAI/AnsibleFATE): A tool to optimize and automate the configuration and deployment operations via Ansible.
 - [FATE-Builder](https://github.com/FederatedAI/FATE-Builder): A tool to build package and docker image for FATE and KubeFATE.
 - [FATE-Client](https://github.com/FederatedAI/FATE-Client): A tool to enable fast federated modeling tasks for FATE.
 - [FATE-Test](https://github.com/FederatedAI/FATE-Test): An automated testing tool for FATE, including tests and benchmark comparisons.
-
+- [FATE-LLM](https://github.com/FederatedAI/FATE-LLM/blob/main/README.md) : A framework to support federated learning for large language models(LLMs).
 ## Governance 
 
 [FATE-Community](https://github.com/FederatedAI/FATE-Community) contains all the documents about how the community members coopearte with each other. 
 
 - [GOVERNANCE.md](https://github.com/FederatedAI/FATE-Community/blob/master/GOVERNANCE.md) documents the governance model of the project. 
 - [Minutes](https://github.com/FederatedAI/FATE-Community/blob/master/meeting-minutes) of working meetings
 - [Development Process Guidelines](https://github.com/FederatedAI/FATE-Community/blob/master/FederatedAI_PROJECT_PROCESS_GUIDELINE.md)
```

### Comparing `pyfate-2.0.0b0/pyfate.egg-info/SOURCES.txt` & `pyfate-2.1.0/pyfate.egg-info/SOURCES.txt`

 * *Files 24% similar despite different names*

```diff
@@ -1,37 +1,52 @@
 setup.py
 fate/__init__.py
 fate/_info.py
 fate/arch/__init__.py
-fate/arch/_standalone.py
-fate/arch/abc/__init__.py
-fate/arch/abc/_federation.py
-fate/arch/abc/_party.py
-fate/arch/abc/_table.py
 fate/arch/computing/__init__.py
-fate/arch/computing/_profile.py
-fate/arch/computing/_type.py
-fate/arch/computing/eggroll/__init__.py
-fate/arch/computing/eggroll/_csession.py
-fate/arch/computing/eggroll/_table.py
-fate/arch/computing/eggroll/_type.py
-fate/arch/computing/spark/__init__.py
-fate/arch/computing/spark/_csession.py
-fate/arch/computing/spark/_materialize.py
-fate/arch/computing/spark/_table.py
-fate/arch/computing/standalone/__init__.py
-fate/arch/computing/standalone/_csession.py
-fate/arch/computing/standalone/_table.py
-fate/arch/computing/standalone/_type.py
+fate/arch/computing/_builder.py
+fate/arch/computing/api/__init__.py
+fate/arch/computing/api/_table.py
+fate/arch/computing/api/_type.py
+fate/arch/computing/api/_uuid.py
+fate/arch/computing/backends/__init__.py
+fate/arch/computing/backends/eggroll/__init__.py
+fate/arch/computing/backends/eggroll/_csession.py
+fate/arch/computing/backends/eggroll/_table.py
+fate/arch/computing/backends/eggroll/_type.py
+fate/arch/computing/backends/spark/__init__.py
+fate/arch/computing/backends/spark/_csession.py
+fate/arch/computing/backends/spark/_materialize.py
+fate/arch/computing/backends/spark/_table.py
+fate/arch/computing/backends/standalone/__init__.py
+fate/arch/computing/backends/standalone/_csession.py
+fate/arch/computing/backends/standalone/_standalone.py
+fate/arch/computing/backends/standalone/_table.py
+fate/arch/computing/backends/standalone/_type.py
+fate/arch/computing/partitioners/__init__.py
+fate/arch/computing/partitioners/_integer_partitioner.py
+fate/arch/computing/partitioners/_java_string_like_partitioner.py
+fate/arch/computing/partitioners/_mmh3_partitioner.py
+fate/arch/computing/serdes/__init__.py
+fate/arch/computing/serdes/_integer_serdes.py
+fate/arch/computing/serdes/_restricted_serdes.py
+fate/arch/computing/serdes/_safe_serdes.py
+fate/arch/computing/serdes/_serdes_base.py
+fate/arch/computing/serdes/_unrestricted_serdes.py
+fate/arch/config/__init__.py
+fate/arch/config/_config.py
+fate/arch/config/default.yaml
 fate/arch/context/__init__.py
 fate/arch/context/_cipher.py
 fate/arch/context/_context.py
-fate/arch/context/_federation.py
+fate/arch/context/_context_helper.py
 fate/arch/context/_metrics.py
+fate/arch/context/_mpc.py
 fate/arch/context/_namespace.py
+fate/arch/context/_parties.py
 fate/arch/dataframe/__init__.py
 fate/arch/dataframe/_dataframe.py
 fate/arch/dataframe/_frame_reader.py
 fate/arch/dataframe/conf/__init__.py
 fate/arch/dataframe/conf/default_config.py
 fate/arch/dataframe/entity/__init__.py
 fate/arch/dataframe/entity/types.py
@@ -59,95 +74,164 @@
 fate/arch/dataframe/ops/_indexer.py
 fate/arch/dataframe/ops/_isin.py
 fate/arch/dataframe/ops/_missing.py
 fate/arch/dataframe/ops/_promote_types.py
 fate/arch/dataframe/ops/_quantile.py
 fate/arch/dataframe/ops/_replace.py
 fate/arch/dataframe/ops/_set_item.py
+fate/arch/dataframe/ops/_sort.py
 fate/arch/dataframe/ops/_stat.py
 fate/arch/dataframe/ops/_transformer.py
 fate/arch/dataframe/ops/_unary_operator.py
 fate/arch/dataframe/ops/_where.py
 fate/arch/dataframe/ops/utils/__init__.py
 fate/arch/dataframe/ops/utils/operators.py
 fate/arch/dataframe/ops/utils/series_align.py
 fate/arch/dataframe/utils/__init__.py
 fate/arch/dataframe/utils/_auto_column_name_generated.py
 fate/arch/dataframe/utils/_dataloader.py
 fate/arch/dataframe/utils/_id_generator.py
 fate/arch/dataframe/utils/_k_fold.py
 fate/arch/dataframe/utils/_sample.py
 fate/arch/federation/__init__.py
-fate/arch/federation/_datastream.py
-fate/arch/federation/_federation.py
-fate/arch/federation/_gc.py
-fate/arch/federation/_nretry.py
-fate/arch/federation/_parties.py
-fate/arch/federation/_type.py
-fate/arch/federation/eggroll/__init__.py
-fate/arch/federation/eggroll/_federation.py
-fate/arch/federation/osx/__init__.py
-fate/arch/federation/osx/_federation.py
-fate/arch/federation/osx/_mq_channel.py
-fate/arch/federation/osx/osx_pb2.py
-fate/arch/federation/osx/osx_pb2_grpc.py
-fate/arch/federation/pulsar/__init__.py
-fate/arch/federation/pulsar/_federation.py
-fate/arch/federation/pulsar/_mq_channel.py
-fate/arch/federation/pulsar/_pulsar_manager.py
-fate/arch/federation/rabbitmq/__init__.py
-fate/arch/federation/rabbitmq/_federation.py
-fate/arch/federation/rabbitmq/_mq_channel.py
-fate/arch/federation/rabbitmq/_rabbit_manager.py
-fate/arch/federation/standalone/__init__.py
-fate/arch/federation/standalone/_federation.py
+fate/arch/federation/_builder.py
+fate/arch/federation/api/__init__.py
+fate/arch/federation/api/_federation.py
+fate/arch/federation/api/_serdes.py
+fate/arch/federation/api/_table_meta.py
+fate/arch/federation/api/_type.py
+fate/arch/federation/backends/__init__.py
+fate/arch/federation/backends/eggroll/__init__.py
+fate/arch/federation/backends/eggroll/_federation.py
+fate/arch/federation/backends/osx/__init__.py
+fate/arch/federation/backends/osx/_federation.py
+fate/arch/federation/backends/osx/_mq_channel.py
+fate/arch/federation/backends/osx/osx_pb2.py
+fate/arch/federation/backends/osx/osx_pb2_grpc.py
+fate/arch/federation/backends/pulsar/__init__.py
+fate/arch/federation/backends/pulsar/_federation.py
+fate/arch/federation/backends/pulsar/_mq_channel.py
+fate/arch/federation/backends/pulsar/_pulsar_manager.py
+fate/arch/federation/backends/rabbitmq/__init__.py
+fate/arch/federation/backends/rabbitmq/_federation.py
+fate/arch/federation/backends/rabbitmq/_mq_channel.py
+fate/arch/federation/backends/rabbitmq/_rabbit_manager.py
+fate/arch/federation/backends/standalone/__init__.py
+fate/arch/federation/backends/standalone/_federation.py
+fate/arch/federation/message_queue/__init__.py
+fate/arch/federation/message_queue/_datastream.py
+fate/arch/federation/message_queue/_federation.py
+fate/arch/federation/message_queue/_nretry.py
+fate/arch/federation/message_queue/_parties.py
 fate/arch/histogram/__init__.py
 fate/arch/histogram/_histogram_distributed.py
 fate/arch/histogram/_histogram_local.py
 fate/arch/histogram/_histogram_sbt.py
 fate/arch/histogram/_histogram_splits.py
 fate/arch/histogram/indexer/__init__.py
 fate/arch/histogram/indexer/_indexer.py
 fate/arch/histogram/values/__init__.py
 fate/arch/histogram/values/_cipher.py
 fate/arch/histogram/values/_encoded.py
 fate/arch/histogram/values/_plain.py
 fate/arch/histogram/values/_value.py
 fate/arch/histogram/values/_values.py
+fate/arch/launchers/__init__.py
+fate/arch/launchers/argparser.py
+fate/arch/launchers/context_helper.py
+fate/arch/launchers/logger.py
+fate/arch/launchers/multiprocess_launcher.py
+fate/arch/launchers/paths.py
 fate/arch/protocol/__init__.py
 fate/arch/protocol/diffie_hellman/__init__.py
+fate/arch/protocol/mpc/__init__.py
+fate/arch/protocol/mpc/cryptensor.py
+fate/arch/protocol/mpc/encoder.py
+fate/arch/protocol/mpc/gradients.py
+fate/arch/protocol/mpc/mpc.py
+fate/arch/protocol/mpc/ptype.py
+fate/arch/protocol/mpc/common/__init__.py
+fate/arch/protocol/mpc/common/encoding.py
+fate/arch/protocol/mpc/common/rng.py
+fate/arch/protocol/mpc/common/serial.py
+fate/arch/protocol/mpc/common/tensor_types.py
+fate/arch/protocol/mpc/common/util.py
+fate/arch/protocol/mpc/communicator/__init__.py
+fate/arch/protocol/mpc/communicator/communicator.py
+fate/arch/protocol/mpc/config/__init__.py
+fate/arch/protocol/mpc/cuda/__init__.py
+fate/arch/protocol/mpc/cuda/cuda_tensor.py
+fate/arch/protocol/mpc/debug/__init__.py
+fate/arch/protocol/mpc/debug/debug.py
+fate/arch/protocol/mpc/functions/__init__.py
+fate/arch/protocol/mpc/functions/approximations.py
+fate/arch/protocol/mpc/functions/dropout.py
+fate/arch/protocol/mpc/functions/logic.py
+fate/arch/protocol/mpc/functions/maximum.py
+fate/arch/protocol/mpc/functions/pooling.py
+fate/arch/protocol/mpc/functions/power.py
+fate/arch/protocol/mpc/functions/regular.py
+fate/arch/protocol/mpc/functions/sampling.py
+fate/arch/protocol/mpc/nn/__init__.py
+fate/arch/protocol/mpc/nn/privacy/__init__.py
+fate/arch/protocol/mpc/nn/privacy/dp_split.py
+fate/arch/protocol/mpc/nn/sshe/__init__.py
+fate/arch/protocol/mpc/nn/sshe/linr_layer.py
+fate/arch/protocol/mpc/nn/sshe/lr_layer.py
+fate/arch/protocol/mpc/nn/sshe/nn_layer.py
+fate/arch/protocol/mpc/primitives/__init__.py
+fate/arch/protocol/mpc/primitives/arithmetic.py
+fate/arch/protocol/mpc/primitives/beaver.py
+fate/arch/protocol/mpc/primitives/binary.py
+fate/arch/protocol/mpc/primitives/circuit.py
+fate/arch/protocol/mpc/primitives/converters.py
+fate/arch/protocol/mpc/primitives/replicated.py
+fate/arch/protocol/mpc/primitives/sshe.py
+fate/arch/protocol/mpc/primitives/ot/__init__.py
+fate/arch/protocol/mpc/primitives/ot/baseOT.py
+fate/arch/protocol/mpc/provider/__init__.py
+fate/arch/protocol/mpc/provider/homomorphic_provider.py
+fate/arch/protocol/mpc/provider/provider.py
+fate/arch/protocol/mpc/provider/tfp_provider.py
+fate/arch/protocol/mpc/provider/ttp_provider.py
+fate/arch/protocol/mpc/provider/tuple_cache/__init__.py
 fate/arch/protocol/phe/__init__.py
 fate/arch/protocol/phe/mock.py
 fate/arch/protocol/phe/ou.py
 fate/arch/protocol/phe/paillier.py
 fate/arch/protocol/phe/type.py
 fate/arch/protocol/psi/__init__.py
 fate/arch/protocol/psi/_psi_run.py
 fate/arch/protocol/psi/ecdh/__init__.py
 fate/arch/protocol/psi/ecdh/_run.py
 fate/arch/protocol/secure_aggregation/__init__.py
 fate/arch/protocol/secure_aggregation/_secure_aggregation.py
 fate/arch/tensor/__init__.py
 fate/arch/tensor/_custom_ops.py
 fate/arch/tensor/distributed/__init__.py
+fate/arch/tensor/distributed/_op_broadcast.py
 fate/arch/tensor/distributed/_op_matmul.py
 fate/arch/tensor/distributed/_op_slice.py
+fate/arch/tensor/distributed/_op_stack.py
 fate/arch/tensor/distributed/_op_transpose.py
 fate/arch/tensor/distributed/_ops_agg.py
 fate/arch/tensor/distributed/_ops_binary.py
 fate/arch/tensor/distributed/_ops_cipher.py
 fate/arch/tensor/distributed/_ops_others.py
 fate/arch/tensor/distributed/_ops_unary.py
 fate/arch/tensor/distributed/_tensor.py
 fate/arch/tensor/inside/__init__.py
 fate/arch/tensor/inside/_op_quantile.py
 fate/arch/tensor/phe/__init__.py
 fate/arch/tensor/phe/_keypair.py
 fate/arch/tensor/phe/_ops.py
 fate/arch/tensor/phe/_tensor.py
+fate/arch/trace/__init__.py
+fate/arch/trace/_profile.py
+fate/arch/trace/_trace.py
 fate/arch/unify/__init__.py
 fate/arch/unify/_infra_def.py
 fate/arch/unify/_io.py
 fate/arch/unify/_uuid.py
 fate/components/__init__.py
 fate/components/__main__.py
 fate/components/components/__init__.py
@@ -155,45 +239,53 @@
 fate/components/components/coordinated_linr.py
 fate/components/components/coordinated_lr.py
 fate/components/components/cross_validation_test.py
 fate/components/components/data_split.py
 fate/components/components/dataframe_io_test.py
 fate/components/components/dataframe_transformer.py
 fate/components/components/evaluation.py
+fate/components/components/feature_correlation.py
 fate/components/components/feature_scale.py
 fate/components/components/hetero_feature_binning.py
 fate/components/components/hetero_feature_selection.py
-fate/components/components/hetero_sbt.py
+fate/components/components/hetero_nn.py
+fate/components/components/hetero_secureboost.py
 fate/components/components/homo_lr.py
 fate/components/components/homo_nn.py
 fate/components/components/multi_model_test.py
 fate/components/components/psi.py
 fate/components/components/reader.py
 fate/components/components/sample.py
+fate/components/components/sshe_linr.py
+fate/components/components/sshe_lr.py
 fate/components/components/statistics.py
 fate/components/components/toy_example.py
 fate/components/components/union.py
 fate/components/components/nn/__init__.py
+fate/components/components/nn/component_utils.py
 fate/components/components/nn/loader.py
 fate/components/components/nn/nn_runner.py
+fate/components/components/nn/source.yaml
+fate/components/components/nn/hook_code/__init__.py
+fate/components/components/nn/hook_code/extract_pytorch_optim.py
+fate/components/components/nn/hook_code/extract_torch_modules.py
 fate/components/components/nn/runner/__init__.py
-fate/components/components/nn/runner/default_runner.py
+fate/components/components/nn/runner/hetero_default_runner.py
+fate/components/components/nn/runner/homo_default_runner.py
 fate/components/components/nn/torch/__init__.py
 fate/components/components/nn/torch/base.py
 fate/components/components/nn/torch/nn.py
 fate/components/components/nn/torch/optim.py
-fate/components/components/nn/utils/__init__.py
-fate/components/components/nn/utils/extract_pytorch_optim.py
-fate/components/components/nn/utils/extract_torch_modules.py
 fate/components/components/utils/__init__.py
 fate/components/components/utils/consts.py
 fate/components/components/utils/tools.py
 fate/components/core/__init__.py
 fate/components/core/_cpn_reexport.py
 fate/components/core/_cpn_search.py
+fate/components/core/_cpn_task_mode.py
 fate/components/core/_load_computing.py
 fate/components/core/_load_device.py
 fate/components/core/_load_federation.py
 fate/components/core/_load_metric_handler.py
 fate/components/core/component_desc/__init__.py
 fate/components/core/component_desc/_component.py
 fate/components/core/component_desc/_component_artifact.py
@@ -202,19 +294,21 @@
 fate/components/core/component_desc/_parameter.py
 fate/components/core/component_desc/artifacts/__init__.py
 fate/components/core/component_desc/artifacts/_base_type.py
 fate/components/core/component_desc/artifacts/data/__init__.py
 fate/components/core/component_desc/artifacts/data/_dataframe.py
 fate/components/core/component_desc/artifacts/data/_directory.py
 fate/components/core/component_desc/artifacts/data/_table.py
+fate/components/core/component_desc/artifacts/data/_unresolved.py
 fate/components/core/component_desc/artifacts/metric/__init__.py
 fate/components/core/component_desc/artifacts/metric/_json.py
 fate/components/core/component_desc/artifacts/model/__init__.py
 fate/components/core/component_desc/artifacts/model/_directory.py
 fate/components/core/component_desc/artifacts/model/_json.py
+fate/components/core/component_desc/artifacts/model/_unresolved.py
 fate/components/core/essential/__init__.py
 fate/components/core/essential/_artifact_type.py
 fate/components/core/essential/_label.py
 fate/components/core/essential/_role.py
 fate/components/core/essential/_stage.py
 fate/components/core/params/__init__.py
 fate/components/core/params/_cipher.py
@@ -251,14 +345,15 @@
 fate/components/entrypoint/cli/test/__init__.py
 fate/components/entrypoint/cli/test/__main__.py
 fate/components/entrypoint/cli/test/execute.py
 fate/ml/__init__.py
 fate/ml/abc/__init__.py
 fate/ml/abc/module.py
 fate/ml/aggregator/__init__.py
+fate/ml/aggregator/aggregator_wrapper.py
 fate/ml/aggregator/base.py
 fate/ml/aggregator/plaintext_aggregator.py
 fate/ml/aggregator/secure_aggregator.py
 fate/ml/ensemble/__init__.py
 fate/ml/ensemble/algo/__init__.py
 fate/ml/ensemble/algo/secureboost/__init__.py
 fate/ml/ensemble/algo/secureboost/common/__init__.py
@@ -276,14 +371,15 @@
 fate/ml/ensemble/learner/decision_tree/tree_core/decision_tree.py
 fate/ml/ensemble/learner/decision_tree/tree_core/hist.py
 fate/ml/ensemble/learner/decision_tree/tree_core/loss.py
 fate/ml/ensemble/learner/decision_tree/tree_core/splitter.py
 fate/ml/ensemble/learner/decision_tree/tree_core/subsample.py
 fate/ml/ensemble/utils/__init__.py
 fate/ml/ensemble/utils/binning.py
+fate/ml/ensemble/utils/sample.py
 fate/ml/evaluation/__init__.py
 fate/ml/evaluation/classification.py
 fate/ml/evaluation/metric_base.py
 fate/ml/evaluation/regression.py
 fate/ml/evaluation/tool.py
 fate/ml/feature_binning/__init__.py
 fate/ml/feature_binning/hetero_feature_binning.py
@@ -295,36 +391,60 @@
 fate/ml/glm/hetero/coordinated_linr/arbiter.py
 fate/ml/glm/hetero/coordinated_linr/guest.py
 fate/ml/glm/hetero/coordinated_linr/host.py
 fate/ml/glm/hetero/coordinated_lr/__init__.py
 fate/ml/glm/hetero/coordinated_lr/arbiter.py
 fate/ml/glm/hetero/coordinated_lr/guest.py
 fate/ml/glm/hetero/coordinated_lr/host.py
+fate/ml/glm/hetero/sshe/__init__.py
+fate/ml/glm/hetero/sshe/sshe_linr.py
+fate/ml/glm/hetero/sshe/sshe_lr.py
 fate/ml/glm/homo/__init__.py
 fate/ml/glm/homo/lr/__init__.py
 fate/ml/glm/homo/lr/client.py
 fate/ml/glm/homo/lr/server.py
 fate/ml/model_selection/__init__.py
 fate/ml/model_selection/data_split.py
 fate/ml/model_selection/sample.py
 fate/ml/nn/__init__.py
-fate/ml/nn/algo/__init__.py
-fate/ml/nn/algo/homo/__init__.py
-fate/ml/nn/algo/homo/fedavg.py
 fate/ml/nn/dataset/__init__.py
 fate/ml/nn/dataset/base.py
 fate/ml/nn/dataset/table.py
+fate/ml/nn/hetero/__init__.py
+fate/ml/nn/hetero/hetero_nn.py
+fate/ml/nn/homo/__init__.py
+fate/ml/nn/homo/fedavg.py
 fate/ml/nn/model_zoo/__init__.py
+fate/ml/nn/model_zoo/hetero_nn_model.py
 fate/ml/nn/model_zoo/multi_model.py
+fate/ml/nn/model_zoo/agg_layer/__init__.py
+fate/ml/nn/model_zoo/agg_layer/agg_layer.py
+fate/ml/nn/model_zoo/agg_layer/fedpass/__init__.py
+fate/ml/nn/model_zoo/agg_layer/fedpass/_passport_block.py
+fate/ml/nn/model_zoo/agg_layer/fedpass/agg_layer.py
+fate/ml/nn/model_zoo/agg_layer/sshe/__init__.py
+fate/ml/nn/model_zoo/agg_layer/sshe/agg_layer.py
+fate/ml/nn/test/__init__.py
+fate/ml/nn/test/test_agglayer.py
+fate/ml/nn/test/test_fedpass_alexnet.py
+fate/ml/nn/test/test_fedpass_lenet.py
+fate/ml/nn/test/test_fedpass_tabular.py
+fate/ml/nn/test/test_hetero_nn_algo.py
+fate/ml/nn/test/test_hetero_nn_algo_no_guest.py
+fate/ml/nn/test/test_hetero_nn_algo_val.py
+fate/ml/nn/test/test_hetero_nn_sshe.py
+fate/ml/nn/test/test_homo_nn_binary.py
+fate/ml/nn/test/test_sshe_nn_layer.py
 fate/ml/nn/trainer/__init__.py
 fate/ml/nn/trainer/trainer_base.py
 fate/ml/preprocessing/__init__.py
 fate/ml/preprocessing/feature_scale.py
 fate/ml/preprocessing/union.py
 fate/ml/statistics/__init__.py
+fate/ml/statistics/pearson_correlation.py
 fate/ml/statistics/statistics.py
 fate/ml/utils/__init__.py
 fate/ml/utils/_convergence.py
 fate/ml/utils/_model_param.py
 fate/ml/utils/_optimizer.py
 fate/ml/utils/callbacks.py
 fate/ml/utils/label_alignment.py
```

### Comparing `pyfate-2.0.0b0/setup.py` & `pyfate-2.1.0/setup.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,7 +1,21 @@
+#  Copyright 2019 The FATE Authors. All Rights Reserved.
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
 import os
 
 from setuptools import find_packages, setup
 
 import fate
 
 # Base requirements
@@ -9,43 +23,42 @@
     "lmdb==1.3.0",
     "torch==1.13.1",
     "fate_utils",
     "pydantic==1.10.12",
     "cloudpickle==2.1.0",
     "click",
     "ruamel.yaml==0.16",
-    "scikit-learn==1.2.1; sys_platform == 'darwin' and platform_machine == 'arm64'",
-    "scikit-learn==1.0.1; sys_platform != 'darwin' or platform_machine != 'arm64'",
+    "scikit-learn",
     "numpy",
     "pandas",
     "transformers",
     "accelerate",
     "beautifultable",
     "requests",
     "grpcio",
     "protobuf",
+    "rich",
+    "omegaconf",
+    "opentelemetry-api",
+    "opentelemetry-sdk",
+    "mmh3==3.0.0",
+    "safetensors",
 ]
 
 # Extra requirements
 extras_require = {
-    "rabbitmq": ["pika==1.2.1"],
+    "rabbitmq": ["pika"],
     "pulsar": [
         "pulsar-client==2.10.2; sys_platform != 'darwin'",
-        "pulsar-client==2.10.1; sys_platform == 'darwin'",
-        "urllib3==1.26.5"
+        "pulsar-client; sys_platform == 'darwin'",
+        "urllib3",
     ],
     "spark": ["pyspark"],
     "eggroll": [
-        "grpcio==1.46.3",
-        "grpcio-tools==1.46.3",
-        "numba==0.56.4",
-        "protobuf==3.19.6",
-        "mmh3==3.0.0",
-        "cachetools>=3.0.0",
-        "cloudpickle==2.1.0",
+        "grpcio-tools",
         "psutil>=5.7.0",
     ],
     "all": ["pyfate[rabbitmq,pulsar,spark,eggroll]"],
 }
 
 # Long description from README.md
 readme_path = os.path.abspath(os.path.join(__file__, os.path.pardir, os.path.pardir, "README.md"))
@@ -64,10 +77,12 @@
     author_email="contact@FedAI.org",
     long_description_content_type="text/markdown",
     long_description=long_description,
     license="Apache-2.0 License",
     url="https://fate.fedai.org/",
     packages=find_packages("."),
     install_requires=install_requires,
+    include_package_data=True,
+    package_data={"": ["*.yaml"]},
     extras_require=extras_require,
     python_requires=">=3.8",
 )
```

