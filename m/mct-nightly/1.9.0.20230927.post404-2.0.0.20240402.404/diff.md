# Comparing `tmp/mct-nightly-1.9.0.20230927.post404.tar.gz` & `tmp/mct-nightly-2.0.0.20240402.404.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "mct-nightly-1.9.0.20230927.post404.tar", last modified: Wed Sep 27 00:04:05 2023, max compression
+gzip compressed data, was "mct-nightly-2.0.0.20240402.404.tar", last modified: Tue Apr  2 00:04:05 2024, max compression
```

## Comparing `mct-nightly-1.9.0.20230927.post404.tar` & `mct-nightly-2.0.0.20240402.404.tar`

### file list

```diff
@@ -1,538 +1,593 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.476225 mct-nightly-1.9.0.20230927.post404/
--rw-r--r--   0 runner    (1001) docker     (127)    10174 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/LICENSE.md
--rw-r--r--   0 runner    (1001) docker     (127)    16618 2023-09-27 00:04:05.476225 mct-nightly-1.9.0.20230927.post404/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    14929 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.404224 mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    16618 2023-09-27 00:04:05.000000 mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    33228 2023-09-27 00:04:05.000000 mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2023-09-27 00:04:05.000000 mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)      137 2023-09-27 00:04:05.000000 mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       26 2023-09-27 00:04:05.000000 mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.404224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/
--rw-r--r--   0 runner    (1001) docker     (127)     3662 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3817 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.404224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/
--rw-r--r--   0 runner    (1001) docker     (127)     2008 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2975 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/analyzer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.408224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/
--rw-r--r--   0 runner    (1001) docker     (127)     1447 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.408224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/back2framework/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2023 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/back2framework/base_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1666 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/base_substitutions.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.408224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2576 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/base_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     6864 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/histogram_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     3888 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/mean_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     5207 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     7929 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/statistics_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     2102 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/statistics_collector_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4017 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (127)     2281 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/defaultdict.py
--rw-r--r--   0 runner    (1001) docker     (127)    21343 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/framework_implementation.py
--rw-r--r--   0 runner    (1001) docker     (127)     6424 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.408224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/fusion/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/fusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5479 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/fusion/layer_fusing.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.408224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/
--rw-r--r--   0 runner    (1001) docker     (127)      773 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    28860 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/base_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)    20579 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/base_node.py
--rw-r--r--   0 runner    (1001) docker     (127)     3733 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/edge.py
--rw-r--r--   0 runner    (1001) docker     (127)     2922 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/functional_node.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4732 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/graph_matchers.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5128 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/graph_searches.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.412224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3880 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     2612 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
--rw-r--r--   0 runner    (1001) docker     (127)     2470 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/cut.py
--rw-r--r--   0 runner    (1001) docker     (127)    17045 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
--rw-r--r--   0 runner    (1001) docker     (127)     3961 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
--rw-r--r--   0 runner    (1001) docker     (127)     7175 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     9265 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.412224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/
--rwxr-xr-x   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3091 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/base_graph_filter.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2210 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/base_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     3706 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/edge_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1773 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/function.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     2745 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/node_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     1111 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/walk_matcher.py
--rw-r--r--   0 runner    (1001) docker     (127)     1205 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/memory_computation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.412224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6704 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
--rw-r--r--   0 runner    (1001) docker     (127)      882 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/configurable_quant_id.py
--rw-r--r--   0 runner    (1001) docker     (127)     3871 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/configurable_quantizer_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2222 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.416224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4297 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py
--rw-r--r--   0 runner    (1001) docker     (127)     3920 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py
--rw-r--r--   0 runner    (1001) docker     (127)     6866 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     1602 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)    19323 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py
--rw-r--r--   0 runner    (1001) docker     (127)     8041 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     6822 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
--rw-r--r--   0 runner    (1001) docker     (127)    34622 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.416224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/search_methods/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15453 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
--rw-r--r--   0 runner    (1001) docker     (127)    25244 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2846 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py
--rw-r--r--   0 runner    (1001) docker     (127)     6326 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
--rw-r--r--   0 runner    (1001) docker     (127)     1324 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/model_builder_mode.py
--rw-r--r--   0 runner    (1001) docker     (127)     5005 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/model_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     1214 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/model_validation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.416224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/
--rw-r--r--   0 runner    (1001) docker     (127)     1307 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17994 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/actions.py
--rw-r--r--   0 runner    (1001) docker     (127)     1748 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/edit_network.py
--rw-r--r--   0 runner    (1001) docker     (127)     3149 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/node_filters.py
--rw-r--r--   0 runner    (1001) docker     (127)     2832 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/node_prior_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.420224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3022 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2022 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/core_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     1482 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/debug_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     6137 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
--rw-r--r--   0 runner    (1001) docker     (127)    16271 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3462 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_analyzer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7247 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2352 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4090 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.420224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/
--rw-r--r--   0 runner    (1001) docker     (127)     1608 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16505 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2917 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py
--rw-r--r--   0 runner    (1001) docker     (127)     7405 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
--rw-r--r--   0 runner    (1001) docker     (127)     1772 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)     8484 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     4558 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4315 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
--rw-r--r--   0 runner    (1001) docker     (127)    41685 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
--rw-r--r--   0 runner    (1001) docker     (127)     5090 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
--rw-r--r--   0 runner    (1001) docker     (127)     9689 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     7879 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
--rw-r--r--   0 runner    (1001) docker     (127)     2939 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
--rw-r--r--   0 runner    (1001) docker     (127)     3614 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantize_node.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.420224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2325 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2761 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    14190 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     5486 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)    10683 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     8398 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/similarity_analyzer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.420224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3434 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     5956 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)    10228 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     5584 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.424224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1390 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/apply_substitutions.py
--rw-r--r--   0 runner    (1001) docker     (127)    13483 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (127)     5892 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (127)     9962 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (127)     9093 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (127)     2250 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
--rw-r--r--   0 runner    (1001) docker     (127)     4866 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (127)    10978 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (127)    28749 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2625 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (127)     3400 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (127)     4228 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (127)     1631 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/user_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.424224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6371 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/final_config_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5955 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/nn_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    20099 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/tensorboard_writer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4233 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     9995 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/graph_prep_runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.424224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.428224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/
--rw-r--r--   0 runner    (1001) docker     (127)      808 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2226 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     2444 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/float_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     4508 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/instance_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    15916 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    14601 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    15428 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/model_gradients.py
--rw-r--r--   0 runner    (1001) docker     (127)     2481 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     2664 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     1192 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/custom_layer_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4994 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/default_framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.428224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3940 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (127)     8145 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (127)     3168 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (127)     2478 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (127)     5598 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
--rw-r--r--   0 runner    (1001) docker     (127)     5925 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (127)    26759 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (127)     3872 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
--rw-r--r--   0 runner    (1001) docker     (127)     2387 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
--rw-r--r--   0 runner    (1001) docker     (127)     3176 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (127)     5542 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     7730 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (127)    11045 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1623 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (127)     1462 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1814 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (127)    27439 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/keras_implementation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1722 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/keras_model_validation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3936 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/keras_node_prior_info.py
--rw-r--r--   0 runner    (1001) docker     (127)     8502 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/kpi_data_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4922 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     6363 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1628 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/base_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     6151 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     4475 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2490 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/common.py
--rw-r--r--   0 runner    (1001) docker     (127)    11303 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/connectivity_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7906 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
--rw-r--r--   0 runner    (1001) docker     (127)     2760 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2107 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py
--rw-r--r--   0 runner    (1001) docker     (127)     2408 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
--rw-r--r--   0 runner    (1001) docker     (127)     6368 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/node_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     8109 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/reader.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3060 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
--rw-r--r--   0 runner    (1001) docker     (127)     2022 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/tf_tensor_numpy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.432224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/visualization/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/visualization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.436224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.436224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/
--rw-r--r--   0 runner    (1001) docker     (127)      813 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2274 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     3419 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1848 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    14033 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)    18329 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
--rw-r--r--   0 runner    (1001) docker     (127)    16443 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.436224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5773 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)     1640 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3456 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     2472 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     4224 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/default_framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.436224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.440225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7659 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2822 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (127)     2162 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (127)     4804 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
--rw-r--r--   0 runner    (1001) docker     (127)     5797 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (127)    38353 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1953 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
--rw-r--r--   0 runner    (1001) docker     (127)     5601 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4148 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
--rw-r--r--   0 runner    (1001) docker     (127)     2899 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (127)     3303 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (127)    10668 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1588 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (127)     1375 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1616 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (127)     8482 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/kpi_data_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.440225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4656 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5961 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    25792 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/pytorch_implementation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3250 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.440225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6464 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     4420 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.440225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12113 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/graph_builders.py
--rw-r--r--   0 runner    (1001) docker     (127)     1789 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/node_holders.py
--rw-r--r--   0 runner    (1001) docker     (127)     5801 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/reader.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.440225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3261 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
--rw-r--r--   0 runner    (1001) docker     (127)     2959 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    17561 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.440225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/
--rw-r--r--   0 runner    (1001) docker     (127)     1148 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.444224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1018 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6397 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/data_generation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4564 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/data_generation_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3522 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/enums.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/image_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     6047 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/model_info_exctractors.py
--rw-r--r--   0 runner    (1001) docker     (127)    19572 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/optimization_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.444224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1088 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     7623 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/image_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)    19433 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/keras_data_generation.py
--rw-r--r--   0 runner    (1001) docker     (127)     8552 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/model_info_exctractors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.444224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1983 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/batchnorm_alignment_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2699 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/bn_layer_weighting_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     4061 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/image_initilization.py
--rw-r--r--   0 runner    (1001) docker     (127)     5379 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/output_loss_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     4913 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/scheduler_step_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    21146 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1179 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6617 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/image_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     9616 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/model_info_exctractors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1968 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/batchnorm_alignment_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2597 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/bn_layer_weighting_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     4719 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py
--rw-r--r--   0 runner    (1001) docker     (127)     4999 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)     2587 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py
--rw-r--r--   0 runner    (1001) docker     (127)    19069 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    18867 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/pytorch_data_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/
--rw-r--r--   0 runner    (1001) docker     (127)     1189 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/
--rw-r--r--   0 runner    (1001) docker     (127)      698 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2017 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      699 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1690 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     1014 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
--rw-r--r--   0 runner    (1001) docker     (127)    11296 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     3161 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     8048 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     5982 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.448224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      699 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4021 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)      967 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
--rw-r--r--   0 runner    (1001) docker     (127)     4942 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     2897 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (127)     6242 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/
--rw-r--r--   0 runner    (1001) docker     (127)     1187 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4253 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     8755 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2077 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)     3767 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3921 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     8724 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2086 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)     3449 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/
--rw-r--r--   0 runner    (1001) docker     (127)     1276 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.452225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9508 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_config.py
--rw-r--r--   0 runner    (1001) docker     (127)      611 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     1266 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_framework_implementation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2826 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)    15167 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_training.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.456225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1248 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py
--rw-r--r--   0 runner    (1001) docker     (127)     6241 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/gptq_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    17405 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/gptq_training.py
--rw-r--r--   0 runner    (1001) docker     (127)     4627 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/graph_info.py
--rw-r--r--   0 runner    (1001) docker     (127)    14835 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.456225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (127)      963 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4770 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5055 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4429 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     2087 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.456225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4016 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py
--rw-r--r--   0 runner    (1001) docker     (127)    12159 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    10377 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.456225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8366 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.456225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2719 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/gptq_loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     1268 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py
--rw-r--r--   0 runner    (1001) docker     (127)    14668 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/gptq_training.py
--rw-r--r--   0 runner    (1001) docker     (127)     3955 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/graph_info.py
--rw-r--r--   0 runner    (1001) docker     (127)    12768 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.456225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (127)      968 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4171 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     3893 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4289 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     2089 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4132 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
--rw-r--r--   0 runner    (1001) docker     (127)    12347 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     9099 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8778 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (127)     5534 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18014 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/legacy/keras_quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (127)    17630 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/legacy/pytorch_quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (127)     4863 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/
--rw-r--r--   0 runner    (1001) docker     (127)      930 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9800 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8580 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/pytorch/quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (127)     2552 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/
--rw-r--r--   0 runner    (1001) docker     (127)     1091 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/common/
--rw-r--r--   0 runner    (1001) docker     (127)      829 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3295 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/common/qat_config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.460224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    16077 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.464224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (127)      856 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2104 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     2123 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5635 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.464224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13517 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (127)    10789 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.464224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12461 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.464224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (127)      859 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2213 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     5491 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (127)     5004 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.464224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9657 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (127)     8778 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.464224 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      920 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     1723 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/immutable.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.468225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/
--rw-r--r--   0 runner    (1001) docker     (127)     1574 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2010 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     2353 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
--rw-r--r--   0 runner    (1001) docker     (127)     8540 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3108 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
--rw-r--r--   0 runner    (1001) docker     (127)      787 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py
--rw-r--r--   0 runner    (1001) docker     (127)     9206 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1392 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.468225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/
--rw-r--r--   0 runner    (1001) docker     (127)     1513 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8759 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
--rw-r--r--   0 runner    (1001) docker     (127)     2046 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
--rw-r--r--   0 runner    (1001) docker     (127)     4019 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
--rw-r--r--   0 runner    (1001) docker     (127)     6040 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     8713 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
--rw-r--r--   0 runner    (1001) docker     (127)     1030 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.468225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3337 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.468225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.468225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (127)     1522 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2959 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (127)      717 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8140 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     5439 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (127)     4721 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/
--rw-r--r--   0 runner    (1001) docker     (127)      721 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8443 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     5446 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (127)     4729 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/
--rw-r--r--   0 runner    (1001) docker     (127)      721 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8147 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     5459 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (127)     4748 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (127)     1510 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2090 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (127)      717 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6204 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     3183 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (127)     2942 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/
--rw-r--r--   0 runner    (1001) docker     (127)      697 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (127)     1505 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2085 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (127)      717 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8048 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     5920 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (127)     5068 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.472225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/
--rw-r--r--   0 runner    (1001) docker     (127)     1224 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.476225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7564 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)      875 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6351 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3451 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1505 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4791 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.476225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4178 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4360 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
--rw-r--r--   0 runner    (1001) docker     (127)     3684 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/load_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     5502 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/quantize_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)     1797 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2023-09-27 00:04:05.476225 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/pytorch/
--rw-r--r--   0 runner    (1001) docker     (127)      696 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3083 2023-09-27 00:03:16.000000 mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (127)      111 2023-09-27 00:04:05.476225 mct-nightly-1.9.0.20230927.post404/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     2026 2023-09-27 00:04:04.000000 mct-nightly-1.9.0.20230927.post404/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.909841 mct-nightly-2.0.0.20240402.404/
+-rw-r--r--   0 runner    (1001) docker     (127)    10174 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/LICENSE.md
+-rw-r--r--   0 runner    (1001) docker     (127)    20343 2024-04-02 00:04:05.909841 mct-nightly-2.0.0.20240402.404/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    18338 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.833840 mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    20343 2024-04-02 00:04:05.000000 mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    36370 2024-04-02 00:04:05.000000 mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-02 00:04:05.000000 mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      137 2024-04-02 00:04:05.000000 mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       26 2024-04-02 00:04:05.000000 mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.833840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/
+-rw-r--r--   0 runner    (1001) docker     (127)     1573 2024-04-02 00:04:04.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3648 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.837840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1982 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2975 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/analyzer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.837840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/
+-rw-r--r--   0 runner    (1001) docker     (127)     1447 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.837840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2023 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/back2framework/base_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1666 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/base_substitutions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.837840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2591 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/base_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6882 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/histogram_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3414 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/mean_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5207 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7930 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/statistics_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21152 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6337 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.841841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/fusion/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/fusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5548 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/fusion/layer_fusing.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.841841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      773 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38178 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/base_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28492 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/base_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3784 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/edge.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3173 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/functional_node.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4744 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/graph_matchers.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5128 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/graph_searches.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.841841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3800 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/cut.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17045 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3961 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7175 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9803 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.841841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/hessian/
+-rw-r--r--   0 runner    (1001) docker     (127)     1021 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/hessian/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9721 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/hessian/hessian_info_service.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1325 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/hessian/hessian_info_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/hessian/trace_hessian_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3321 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/hessian/trace_hessian_request.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.845841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/
+-rwxr-xr-x   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3091 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/base_graph_filter.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2210 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/base_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3706 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/edge_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1773 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/function.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2745 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/node_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1111 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/walk_matcher.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1205 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/memory_computation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.845841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7536 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      882 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/configurable_quant_id.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5177 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/configurable_quantizer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2812 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4573 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7550 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37578 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.845841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4745 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7922 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4217 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_aggregation_methods.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1682 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_functions_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21473 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_methods.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.845841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/search_methods/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16592 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28519 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2846 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7901 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1324 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/model_builder_mode.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8352 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/model_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/model_validation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.849840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/
+-rw-r--r--   0 runner    (1001) docker     (127)     1307 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19594 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/actions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1748 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/edit_network.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3149 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/node_filters.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2832 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.849840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      699 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3892 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/channels_grouping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7928 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/greedy_mask_calculator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.849840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/importance_metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/importance_metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1988 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/importance_metrics/base_importance_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1999 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/importance_metrics/importance_metric_factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14027 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/importance_metrics/lfh_importance_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.849840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/mask/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/mask/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/mask/per_channel_mask.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5958 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/mask/per_simd_group_mask.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19523 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/memory_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3323 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/prune_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7516 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/pruner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3681 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/pruning_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6734 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/pruning_framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3781 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/pruning_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5721 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/pruning_section.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.853840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4900 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2257 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/core_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1482 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/debug_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7136 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26737 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6699 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3847 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.853840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/
+-rw-r--r--   0 runner    (1001) docker     (127)     1486 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18165 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7410 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1772 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8538 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4558 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41524 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3882 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9743 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7933 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2854 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantize_node.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.853840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2761 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14245 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5521 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11503 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8515 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/similarity_analyzer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.853840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4669 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5641 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10145 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5519 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.857841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1390 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/apply_substitutions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13392 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10028 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12367 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2406 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4767 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10966 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29865 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2625 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3477 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4737 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/weights_activation_split.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1648 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/user_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.857841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6371 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/final_config_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5921 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/nn_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21951 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/tensorboard_writer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4115 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10100 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/graph_prep_runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.861841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.861841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)      808 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2233 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2444 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/float_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4517 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/instance_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16001 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15567 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2481 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3163 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1192 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/custom_layer_validation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4961 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/default_framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.861841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5140 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8158 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3168 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2478 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5794 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/dwconv_to_conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5940 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8143 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4112 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/matmul_substitution.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3872 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2387 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3181 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5542 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7941 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11149 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1462 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1814 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/hessian/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/hessian/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9656 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/hessian/activation_trace_hessian_calculator_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3913 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/hessian/trace_hessian_calculator_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10525 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/hessian/weights_trace_hessian_calculator_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29399 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/keras_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1722 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/keras_model_validation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3879 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/keras_node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5137 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6729 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12710 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/pruning/pruning_keras_implementation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1628 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/base_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6274 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4475 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2484 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11303 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/connectivity_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.865841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7906 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2760 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2107 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2408 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9666 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/node_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8109 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4851 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/resource_utilization_data_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.869840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3060 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2491 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/tf_tensor_numpy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.869840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/visualization/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/visualization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.869840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.869840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)      813 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2279 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3419 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1848 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15060 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.869840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5773 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1640 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3456 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4175 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/default_framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.869840 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.873841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8292 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2822 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2162 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3919 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3935 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_batch_norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3799 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5815 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38459 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1953 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5667 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4919 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2902 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3303 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10668 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1588 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1375 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1616 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.873841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/hessian/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/hessian/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8147 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/hessian/activation_trace_hessian_calculator_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3425 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/hessian/trace_hessian_calculator_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6974 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/hessian/weights_trace_hessian_calculator_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.873841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4871 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6337 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.873841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pruning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14648 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pruning/pruning_pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4330 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pytorch_device_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27082 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3250 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.873841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6464 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4420 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.877841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12626 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/graph_builders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/node_holders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6015 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/reader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4983 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/resource_utilization_data_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.877841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3261 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2880 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6153 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/quantization_prep_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11836 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.877841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/
+-rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.877841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1018 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6415 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/data_generation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4564 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/data_generation_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/enums.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6046 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/model_info_exctractors.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19572 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/optimization_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.877841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7623 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21539 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/keras_data_generation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8610 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/model_info_exctractors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1983 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/batchnorm_alignment_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2699 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/bn_layer_weighting_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4119 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/image_initilization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5325 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/output_loss_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4913 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/scheduler_step_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21146 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1179 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6617 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/image_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9690 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/model_info_exctractors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1968 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/batchnorm_alignment_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2597 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/bn_layer_weighting_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4719 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5015 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2587 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19085 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20937 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/pytorch_data_generation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2277 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/defaultdict.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/
+-rw-r--r--   0 runner    (1001) docker     (127)     1301 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2017 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/quantization_format.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.881841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      699 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      963 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11481 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3514 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8048 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5750 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1976 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/mctq_keras_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      699 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4021 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      967 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5137 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2897 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6351 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)     1187 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2297 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/get_inferable_quantizers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5128 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4913 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9348 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3464 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.885841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/
+-rw-r--r--   0 runner    (1001) docker     (127)     1228 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.889841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5288 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)      611 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1266 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3039 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17705 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_training.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.889841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1248 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6241 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/gptq_loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18438 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/gptq_training.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4639 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/graph_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14057 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.889841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      963 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4782 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5055 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.889841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4016 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12159 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10377 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.889841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8356 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.889841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2719 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/gptq_loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1268 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15808 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/gptq_training.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3967 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/graph_info.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12495 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      968 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4137 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3893 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4566 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1908 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4132 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12347 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9099 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8768 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5924 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4567 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/
+-rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      698 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8612 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/keras/pruning_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9398 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/pytorch/pruning_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/
+-rw-r--r--   0 runner    (1001) docker     (127)      904 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9057 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7583 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/pytorch/quantization_facade.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/
+-rw-r--r--   0 runner    (1001) docker     (127)     1143 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      829 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3394 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/common/qat_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.893841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17026 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      996 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2065 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/lsq/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/lsq/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12022 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/lsq/symmetric_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11223 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/lsq/uniform_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2543 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5872 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13517 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10789 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13374 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1003 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/lsq/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/lsq/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10712 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/lsq/symmetric_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10355 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/lsq/uniform_lsq.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5721 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5335 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9657 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8778 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.897841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1519 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1827 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/immutable.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/
+-rw-r--r--   0 runner    (1001) docker     (127)     1724 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3924 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14248 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3108 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9320 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1392 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/
+-rw-r--r--   0 runner    (1001) docker     (127)     1513 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8776 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4019 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6720 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9837 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3337 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2959 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10883 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6484 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5731 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.901841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/
+-rw-r--r--   0 runner    (1001) docker     (127)      721 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10616 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6493 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5739 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/
+-rw-r--r--   0 runner    (1001) docker     (127)      721 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10369 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6505 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (127)     1510 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2090 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8106 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3814 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      697 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (127)     1505 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2085 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9864 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6830 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/
+-rw-r--r--   0 runner    (1001) docker     (127)     1224 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.905841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7706 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      875 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6983 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3428 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1505 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4791 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.909841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4148 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4360 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3656 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/load_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5468 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/quantize_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1797 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-02 00:04:05.909841 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3047 2024-04-02 00:03:32.000000 mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      133 2024-04-02 00:04:05.909841 mct-nightly-2.0.0.20240402.404/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     2026 2024-04-02 00:04:04.000000 mct-nightly-2.0.0.20240402.404/setup.py
```

### Comparing `mct-nightly-1.9.0.20230927.post404/LICENSE.md` & `mct-nightly-2.0.0.20240402.404/LICENSE.md`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/PKG-INFO` & `mct-nightly-2.0.0.20240402.404/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mct-nightly
-Version: 1.9.0.20230927.post404
+Version: 2.0.0.20240402.404
 Summary: A Model Compression Toolkit for neural networks
 Home-page: UNKNOWN
 License: UNKNOWN
 Description: # Model Compression Toolkit (MCT)
         
         Model Compression Toolkit (MCT) is an open-source project for neural network model optimization under efficient, constrained hardware.
         
@@ -19,14 +19,15 @@
         
         
         ## Table of Contents
         
         - [Getting Started](#getting-started)
         - [Supported features](#supported-features)
         - [Results](#results)
+        - [Troubleshooting](#trouble-shooting)
         - [Contributions](#contributions)
         - [License](#license)
         
         
         ## Getting Started
         
         This section provides an installation and a quick starting guide.
@@ -39,54 +40,55 @@
         ```
         
         For installing the nightly version or installing from source, refer to the [installation guide](INSTALLATION.md).
         
         
         ### Quick start & tutorials 
         
-        For an example of how to use MCT with TensorFlow or PyTorch on various models and tasks,
-        check out the [quick-start page](tutorials/quick_start/README.md) and
-        the [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
-        
-        In addition, a set of [notebooks](tutorials/notebooks) are provided for an easy start. For example:
-        * [MobileNet with Tensorflow](tutorials/notebooks/example_keras_mobilenet.py).
-        * [MobileNetV2 with PyTorch](tutorials/notebooks/example_pytorch_mobilenet_v2.py).
-        * [Data Generation for ResNet18 with PyTorch](tutorials/notebooks/example_pytorch_data_generation.ipynb).
-        
+        Explore the Model Compression Toolkit (MCT) through our tutorials, 
+        covering compression techniques for Keras and PyTorch models. Access interactive [notebooks](tutorials/README.md) 
+        for hands-on learning. For example:
+        * [Keras MobileNetV2 post training quantization](tutorials/notebooks/keras/ptq/example_keras_imagenet.ipynb)
+        * [Post training quantization with PyTorch](tutorials/notebooks/pytorch/ptq/example_pytorch_quantization_mnist.ipynb)
+        * [Data Generation for ResNet18 with PyTorch](tutorials/notebooks/pytorch/data_generation/example_pytorch_data_generation.ipynb).
+        
+        Additionally, for quick quantization of a variety of models from well-known collections,
+        visit the [quick-start page](tutorials/quick_start/README.md) and the
+        [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
         
         ### Supported Versions
         
         Currently, MCT is being tested on various Python, Pytorch and TensorFlow versions:
         
-        |             | PyTorch 1.12                                                                                                                                                                                                                               | PyTorch 1.13                                                                                                                                                                                                                               | PyTorch 2.0                                                                                                                                                                                                              |
-        |-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-        | Python 3.8  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch112.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch20.yml)   |
-        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch112.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml)   |
-        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml) |
+        |             |  PyTorch 1.13                                                                                                                                                                                                               | PyTorch 2.0                                                                                                                                                                                                              | PyTorch 2.1                                                                                                                                                                                                              |
+        |-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch21.yml)   |
+        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch21.yml) |
+        | Python 3.11 | | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch20.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch21.yml) |
         
         
-        |             | TensorFlow 2.11                                                                                                                                                                                                                        | TensorFlow 2.12                                                                                                                                                                                                                        | TensorFlow 2.13                                                                                                                                                                                                                        |
-        |-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-        | Python 3.8  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras211.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras213.yml)   |
-        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras211.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml)   |
-        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras211.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml) |
+        |             | TensorFlow 2.12                                                                                                                                                                                                                        | TensorFlow 2.13                                                                                                                                                                                                                        | TensorFlow 2.14                                                                                                                                                                                                                        | TensorFlow 2.15                                                                                                                                                                                                        |
+        |-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras214.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras215.yml)   |
+        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras214.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras215.yml) |
+        | Python 3.11 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras213.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras214.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras215.yml) |
         
         
         ## Supported Features
         MCT offers a range of powerful features to optimize neural network models for efficient deployment. These supported features include:
         
-        ### Data Generation
+        ### Data Generation [*](#experimental-features)
         MCT provides tools for generating synthetic images based on the statistics stored in a model's batch normalization layers. These generated images are valuable for various compression tasks where image data is required, such as quantization and pruning. 
         You can customize data generation configurations to suit your specific needs. [Go to the Data Generation page.](model_compression_toolkit/data_generation/README.md)
         
         ### Quantization
         MCT supports different quantization methods:
         * Post-training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
         * Gradient-based post-training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
-        * Quantization-aware training (QAT)[*](#experimental-features)
+        * Quantization-aware training (QAT) [*](#experimental-features)
         
         
         | Quantization Method                           | Complexity | Computational Cost          |
         |-----------------------------------------------|------------|-----------------------------|
         | PTQ                                           | Low        | Low (order of minutes)      |
         | GPTQ (parameters fine-tuning using gradients) | Mild       | Mild (order of 2-3 hours)   |
         | QAT                                           | High       | High (order of 12-36 hours) |
@@ -105,14 +107,31 @@
           * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
           * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
         * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
         * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
         * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
         * <ins>Target Platform Capabilities:</ins> The Target Platform Capabilities (TPC) describes the target platform (an edge device with dedicated hardware). For more details, please read the [TPC README](model_compression_toolkit/target_platform_capabilities/README.md).   
         
+        ### Enhanced Post-Training Quantization (EPTQ)
+        As part of the GPTQ we provide an advanced optimization algorithm called EPTQ.
+        
+        The specifications of the algorithm are detailed in the paper: _"**EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian**"_ [4].
+        
+        More details on the how to use EPTQ via MCT can be found in the [EPTQ guidelines](model_compression_toolkit/gptq/README.md).
+        
+        
+        ### Structured Pruning [*](#experimental-features)
+        MCT introduces a structured and hardware-aware model pruning.
+        This pruning technique is designed to compress models for specific hardware architectures, 
+        taking into account the target platform's Single Instruction, Multiple Data (SIMD) capabilities. 
+        By pruning groups of channels (SIMD groups), our approach not only reduces model size 
+        and complexity, but ensures that better utilization of channels is in line with the SIMD architecture 
+        for a target Resource Utilization of weights memory footprint.
+        [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_pruning_experimental.html)
+        [Pytorch API](https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/pruning/pytorch/pruning_facade.py#L43) 
         
         #### Experimental features 
         
         Some features are experimental and subject to future changes. 
          
         For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
         
@@ -134,31 +153,52 @@
         |---------------------------|-----------------|-----------------|-------------------------|
         | MobileNet V2 [3]          | 71.886          | 71.444          |71.29|
         | ResNet-18 [3]             | 69.86           | 69.63           |69.53|
         | SqueezeNet 1.1 [3]        | 58.128          | 57.678          ||
         
         For more results, please refer to [quick start](https://github.com/sony/model_optimization/tree/main/tutorials/quick_start).
         
+        
+        #### Pruning Results
+        
+        Results for applying pruning to reduce the parameters of the following models by 50%:
+        
+        | Model           | Dense Model Accuracy | Pruned Model Accuracy |
+        |-----------------|----------------------|-----------------------|
+        | ResNet50 [2]    | 75.1                 | 72.4                  |
+        | DenseNet121 [3] | 74.44                | 71.71                 |
+        
+        
+        ## Trouble Shooting
+        
+        If the accuracy degradation of the quantized model is too large for your application, check out the [Quantization Troubleshooting](https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md)
+        for common pitfalls and some tools to improve quantization accuracy.
+        
+        Check out the [FAQ](https://github.com/sony/model_optimization/tree/main/FAQ.md) for common issues.
+        
+        
         ## Contributions
         MCT aims at keeping a more up-to-date fork and welcomes contributions from anyone.
         
         *You will find more information about contributions in the [Contribution guide](CONTRIBUTING.md).
         
         
         ## License
         [Apache License 2.0](LICENSE.md).
         
         ## References 
         
         [1] Habi, H.V., Peretz, R., Cohen, E., Dikstein, L., Dror, O., Diamant, I., Jennings, R.H. and Netzer, A., 2021. [HPTQ: Hardware-Friendly Post Training Quantization. arXiv preprint](https://arxiv.org/abs/2109.09113).
         
-        [2] [MobilNet](https://keras.io/api/applications/mobilenet/#mobilenet-function) from Keras applications.
+        [2] [Keras Applications](https://keras.io/api/applications/)
         
         [3] [TORCHVISION.MODELS](https://pytorch.org/vision/stable/models.html) 
         
+        [4] Gordon, O., Habi, H. V., & Netzer, A., 2023. [EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. arXiv preprint](https://arxiv.org/abs/2309.11531)
+        
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
```

### Comparing `mct-nightly-1.9.0.20230927.post404/README.md` & `mct-nightly-2.0.0.20240402.404/README.md`

 * *Files 16% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 
 
 ## Table of Contents
 
 - [Getting Started](#getting-started)
 - [Supported features](#supported-features)
 - [Results](#results)
+- [Troubleshooting](#trouble-shooting)
 - [Contributions](#contributions)
 - [License](#license)
 
 
 ## Getting Started
 
 This section provides an installation and a quick starting guide.
@@ -33,54 +34,55 @@
 ```
 
 For installing the nightly version or installing from source, refer to the [installation guide](INSTALLATION.md).
 
 
 ### Quick start & tutorials 
 
-For an example of how to use MCT with TensorFlow or PyTorch on various models and tasks,
-check out the [quick-start page](tutorials/quick_start/README.md) and
-the [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
-
-In addition, a set of [notebooks](tutorials/notebooks) are provided for an easy start. For example:
-* [MobileNet with Tensorflow](tutorials/notebooks/example_keras_mobilenet.py).
-* [MobileNetV2 with PyTorch](tutorials/notebooks/example_pytorch_mobilenet_v2.py).
-* [Data Generation for ResNet18 with PyTorch](tutorials/notebooks/example_pytorch_data_generation.ipynb).
-
+Explore the Model Compression Toolkit (MCT) through our tutorials, 
+covering compression techniques for Keras and PyTorch models. Access interactive [notebooks](tutorials/README.md) 
+for hands-on learning. For example:
+* [Keras MobileNetV2 post training quantization](tutorials/notebooks/keras/ptq/example_keras_imagenet.ipynb)
+* [Post training quantization with PyTorch](tutorials/notebooks/pytorch/ptq/example_pytorch_quantization_mnist.ipynb)
+* [Data Generation for ResNet18 with PyTorch](tutorials/notebooks/pytorch/data_generation/example_pytorch_data_generation.ipynb).
+
+Additionally, for quick quantization of a variety of models from well-known collections,
+visit the [quick-start page](tutorials/quick_start/README.md) and the
+[results CSV](tutorials/quick_start/results/model_quantization_results.csv).
 
 ### Supported Versions
 
 Currently, MCT is being tested on various Python, Pytorch and TensorFlow versions:
 
-|             | PyTorch 1.12                                                                                                                                                                                                                               | PyTorch 1.13                                                                                                                                                                                                                               | PyTorch 2.0                                                                                                                                                                                                              |
-|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| Python 3.8  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch112.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch20.yml)   |
-| Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch112.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml)   |
-| Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml) |
+|             |  PyTorch 1.13                                                                                                                                                                                                               | PyTorch 2.0                                                                                                                                                                                                              | PyTorch 2.1                                                                                                                                                                                                              |
+|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch21.yml)   |
+| Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch21.yml) |
+| Python 3.11 | | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch20.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch21.yml) |
 
 
-|             | TensorFlow 2.11                                                                                                                                                                                                                        | TensorFlow 2.12                                                                                                                                                                                                                        | TensorFlow 2.13                                                                                                                                                                                                                        |
-|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| Python 3.8  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras211.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras213.yml)   |
-| Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras211.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml)   |
-| Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras211.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml) |
+|             | TensorFlow 2.12                                                                                                                                                                                                                        | TensorFlow 2.13                                                                                                                                                                                                                        | TensorFlow 2.14                                                                                                                                                                                                                        | TensorFlow 2.15                                                                                                                                                                                                        |
+|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras214.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras215.yml)   |
+| Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras214.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras215.yml) |
+| Python 3.11 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras213.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras214.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras215.yml) |
 
 
 ## Supported Features
 MCT offers a range of powerful features to optimize neural network models for efficient deployment. These supported features include:
 
-### Data Generation
+### Data Generation [*](#experimental-features)
 MCT provides tools for generating synthetic images based on the statistics stored in a model's batch normalization layers. These generated images are valuable for various compression tasks where image data is required, such as quantization and pruning. 
 You can customize data generation configurations to suit your specific needs. [Go to the Data Generation page.](model_compression_toolkit/data_generation/README.md)
 
 ### Quantization
 MCT supports different quantization methods:
 * Post-training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
 * Gradient-based post-training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
-* Quantization-aware training (QAT)[*](#experimental-features)
+* Quantization-aware training (QAT) [*](#experimental-features)
 
 
 | Quantization Method                           | Complexity | Computational Cost          |
 |-----------------------------------------------|------------|-----------------------------|
 | PTQ                                           | Low        | Low (order of minutes)      |
 | GPTQ (parameters fine-tuning using gradients) | Mild       | Mild (order of 2-3 hours)   |
 | QAT                                           | High       | High (order of 12-36 hours) |
@@ -99,14 +101,31 @@
   * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
   * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
 * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
 * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
 * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
 * <ins>Target Platform Capabilities:</ins> The Target Platform Capabilities (TPC) describes the target platform (an edge device with dedicated hardware). For more details, please read the [TPC README](model_compression_toolkit/target_platform_capabilities/README.md).   
 
+### Enhanced Post-Training Quantization (EPTQ)
+As part of the GPTQ we provide an advanced optimization algorithm called EPTQ.
+
+The specifications of the algorithm are detailed in the paper: _"**EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian**"_ [4].
+
+More details on the how to use EPTQ via MCT can be found in the [EPTQ guidelines](model_compression_toolkit/gptq/README.md).
+
+
+### Structured Pruning [*](#experimental-features)
+MCT introduces a structured and hardware-aware model pruning.
+This pruning technique is designed to compress models for specific hardware architectures, 
+taking into account the target platform's Single Instruction, Multiple Data (SIMD) capabilities. 
+By pruning groups of channels (SIMD groups), our approach not only reduces model size 
+and complexity, but ensures that better utilization of channels is in line with the SIMD architecture 
+for a target Resource Utilization of weights memory footprint.
+[Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_pruning_experimental.html)
+[Pytorch API](https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/pruning/pytorch/pruning_facade.py#L43) 
 
 #### Experimental features 
 
 Some features are experimental and subject to future changes. 
  
 For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
 
@@ -128,23 +147,44 @@
 |---------------------------|-----------------|-----------------|-------------------------|
 | MobileNet V2 [3]          | 71.886          | 71.444          |71.29|
 | ResNet-18 [3]             | 69.86           | 69.63           |69.53|
 | SqueezeNet 1.1 [3]        | 58.128          | 57.678          ||
 
 For more results, please refer to [quick start](https://github.com/sony/model_optimization/tree/main/tutorials/quick_start).
 
+
+#### Pruning Results
+
+Results for applying pruning to reduce the parameters of the following models by 50%:
+
+| Model           | Dense Model Accuracy | Pruned Model Accuracy |
+|-----------------|----------------------|-----------------------|
+| ResNet50 [2]    | 75.1                 | 72.4                  |
+| DenseNet121 [3] | 74.44                | 71.71                 |
+
+
+## Trouble Shooting
+
+If the accuracy degradation of the quantized model is too large for your application, check out the [Quantization Troubleshooting](https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md)
+for common pitfalls and some tools to improve quantization accuracy.
+
+Check out the [FAQ](https://github.com/sony/model_optimization/tree/main/FAQ.md) for common issues.
+
+
 ## Contributions
 MCT aims at keeping a more up-to-date fork and welcomes contributions from anyone.
 
 *You will find more information about contributions in the [Contribution guide](CONTRIBUTING.md).
 
 
 ## License
 [Apache License 2.0](LICENSE.md).
 
 ## References 
 
 [1] Habi, H.V., Peretz, R., Cohen, E., Dikstein, L., Dror, O., Diamant, I., Jennings, R.H. and Netzer, A., 2021. [HPTQ: Hardware-Friendly Post Training Quantization. arXiv preprint](https://arxiv.org/abs/2109.09113).
 
-[2] [MobilNet](https://keras.io/api/applications/mobilenet/#mobilenet-function) from Keras applications.
+[2] [Keras Applications](https://keras.io/api/applications/)
 
 [3] [TORCHVISION.MODELS](https://pytorch.org/vision/stable/models.html) 
+
+[4] Gordon, O., Habi, H. V., & Netzer, A., 2023. [EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. arXiv preprint](https://arxiv.org/abs/2309.11531)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/PKG-INFO` & `mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mct-nightly
-Version: 1.9.0.20230927.post404
+Version: 2.0.0.20240402.404
 Summary: A Model Compression Toolkit for neural networks
 Home-page: UNKNOWN
 License: UNKNOWN
 Description: # Model Compression Toolkit (MCT)
         
         Model Compression Toolkit (MCT) is an open-source project for neural network model optimization under efficient, constrained hardware.
         
@@ -19,14 +19,15 @@
         
         
         ## Table of Contents
         
         - [Getting Started](#getting-started)
         - [Supported features](#supported-features)
         - [Results](#results)
+        - [Troubleshooting](#trouble-shooting)
         - [Contributions](#contributions)
         - [License](#license)
         
         
         ## Getting Started
         
         This section provides an installation and a quick starting guide.
@@ -39,54 +40,55 @@
         ```
         
         For installing the nightly version or installing from source, refer to the [installation guide](INSTALLATION.md).
         
         
         ### Quick start & tutorials 
         
-        For an example of how to use MCT with TensorFlow or PyTorch on various models and tasks,
-        check out the [quick-start page](tutorials/quick_start/README.md) and
-        the [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
-        
-        In addition, a set of [notebooks](tutorials/notebooks) are provided for an easy start. For example:
-        * [MobileNet with Tensorflow](tutorials/notebooks/example_keras_mobilenet.py).
-        * [MobileNetV2 with PyTorch](tutorials/notebooks/example_pytorch_mobilenet_v2.py).
-        * [Data Generation for ResNet18 with PyTorch](tutorials/notebooks/example_pytorch_data_generation.ipynb).
-        
+        Explore the Model Compression Toolkit (MCT) through our tutorials, 
+        covering compression techniques for Keras and PyTorch models. Access interactive [notebooks](tutorials/README.md) 
+        for hands-on learning. For example:
+        * [Keras MobileNetV2 post training quantization](tutorials/notebooks/keras/ptq/example_keras_imagenet.ipynb)
+        * [Post training quantization with PyTorch](tutorials/notebooks/pytorch/ptq/example_pytorch_quantization_mnist.ipynb)
+        * [Data Generation for ResNet18 with PyTorch](tutorials/notebooks/pytorch/data_generation/example_pytorch_data_generation.ipynb).
+        
+        Additionally, for quick quantization of a variety of models from well-known collections,
+        visit the [quick-start page](tutorials/quick_start/README.md) and the
+        [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
         
         ### Supported Versions
         
         Currently, MCT is being tested on various Python, Pytorch and TensorFlow versions:
         
-        |             | PyTorch 1.12                                                                                                                                                                                                                               | PyTorch 1.13                                                                                                                                                                                                                               | PyTorch 2.0                                                                                                                                                                                                              |
-        |-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-        | Python 3.8  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch112.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_pytorch20.yml)   |
-        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch112.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml)   |
-        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml) |
+        |             |  PyTorch 1.13                                                                                                                                                                                                               | PyTorch 2.0                                                                                                                                                                                                              | PyTorch 2.1                                                                                                                                                                                                              |
+        |-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch113.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch20.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_pytorch21.yml)   |
+        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch112.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch113.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch20.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_pytorch21.yml) |
+        | Python 3.11 | | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch20.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch20.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch21.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_pytorch21.yml) |
         
         
-        |             | TensorFlow 2.11                                                                                                                                                                                                                        | TensorFlow 2.12                                                                                                                                                                                                                        | TensorFlow 2.13                                                                                                                                                                                                                        |
-        |-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-        | Python 3.8  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras211.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python38_keras213.yml)   |
-        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras211.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml)   |
-        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras211.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras211.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml) |
+        |             | TensorFlow 2.12                                                                                                                                                                                                                        | TensorFlow 2.13                                                                                                                                                                                                                        | TensorFlow 2.14                                                                                                                                                                                                                        | TensorFlow 2.15                                                                                                                                                                                                        |
+        |-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+        | Python 3.9  | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras212.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras213.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras214.yml)   | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python39_keras215.yml)   |
+        | Python 3.10 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras213.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras214.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python310_keras215.yml) |
+        | Python 3.11 | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras212.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras212.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras213.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras213.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras214.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras214.yml) | [![Run Tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras215.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_python311_keras215.yml) |
         
         
         ## Supported Features
         MCT offers a range of powerful features to optimize neural network models for efficient deployment. These supported features include:
         
-        ### Data Generation
+        ### Data Generation [*](#experimental-features)
         MCT provides tools for generating synthetic images based on the statistics stored in a model's batch normalization layers. These generated images are valuable for various compression tasks where image data is required, such as quantization and pruning. 
         You can customize data generation configurations to suit your specific needs. [Go to the Data Generation page.](model_compression_toolkit/data_generation/README.md)
         
         ### Quantization
         MCT supports different quantization methods:
         * Post-training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
         * Gradient-based post-training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
-        * Quantization-aware training (QAT)[*](#experimental-features)
+        * Quantization-aware training (QAT) [*](#experimental-features)
         
         
         | Quantization Method                           | Complexity | Computational Cost          |
         |-----------------------------------------------|------------|-----------------------------|
         | PTQ                                           | Low        | Low (order of minutes)      |
         | GPTQ (parameters fine-tuning using gradients) | Mild       | Mild (order of 2-3 hours)   |
         | QAT                                           | High       | High (order of 12-36 hours) |
@@ -105,14 +107,31 @@
           * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
           * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
         * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
         * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
         * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
         * <ins>Target Platform Capabilities:</ins> The Target Platform Capabilities (TPC) describes the target platform (an edge device with dedicated hardware). For more details, please read the [TPC README](model_compression_toolkit/target_platform_capabilities/README.md).   
         
+        ### Enhanced Post-Training Quantization (EPTQ)
+        As part of the GPTQ we provide an advanced optimization algorithm called EPTQ.
+        
+        The specifications of the algorithm are detailed in the paper: _"**EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian**"_ [4].
+        
+        More details on the how to use EPTQ via MCT can be found in the [EPTQ guidelines](model_compression_toolkit/gptq/README.md).
+        
+        
+        ### Structured Pruning [*](#experimental-features)
+        MCT introduces a structured and hardware-aware model pruning.
+        This pruning technique is designed to compress models for specific hardware architectures, 
+        taking into account the target platform's Single Instruction, Multiple Data (SIMD) capabilities. 
+        By pruning groups of channels (SIMD groups), our approach not only reduces model size 
+        and complexity, but ensures that better utilization of channels is in line with the SIMD architecture 
+        for a target Resource Utilization of weights memory footprint.
+        [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_pruning_experimental.html)
+        [Pytorch API](https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/pruning/pytorch/pruning_facade.py#L43) 
         
         #### Experimental features 
         
         Some features are experimental and subject to future changes. 
          
         For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
         
@@ -134,31 +153,52 @@
         |---------------------------|-----------------|-----------------|-------------------------|
         | MobileNet V2 [3]          | 71.886          | 71.444          |71.29|
         | ResNet-18 [3]             | 69.86           | 69.63           |69.53|
         | SqueezeNet 1.1 [3]        | 58.128          | 57.678          ||
         
         For more results, please refer to [quick start](https://github.com/sony/model_optimization/tree/main/tutorials/quick_start).
         
+        
+        #### Pruning Results
+        
+        Results for applying pruning to reduce the parameters of the following models by 50%:
+        
+        | Model           | Dense Model Accuracy | Pruned Model Accuracy |
+        |-----------------|----------------------|-----------------------|
+        | ResNet50 [2]    | 75.1                 | 72.4                  |
+        | DenseNet121 [3] | 74.44                | 71.71                 |
+        
+        
+        ## Trouble Shooting
+        
+        If the accuracy degradation of the quantized model is too large for your application, check out the [Quantization Troubleshooting](https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md)
+        for common pitfalls and some tools to improve quantization accuracy.
+        
+        Check out the [FAQ](https://github.com/sony/model_optimization/tree/main/FAQ.md) for common issues.
+        
+        
         ## Contributions
         MCT aims at keeping a more up-to-date fork and welcomes contributions from anyone.
         
         *You will find more information about contributions in the [Contribution guide](CONTRIBUTING.md).
         
         
         ## License
         [Apache License 2.0](LICENSE.md).
         
         ## References 
         
         [1] Habi, H.V., Peretz, R., Cohen, E., Dikstein, L., Dror, O., Diamant, I., Jennings, R.H. and Netzer, A., 2021. [HPTQ: Hardware-Friendly Post Training Quantization. arXiv preprint](https://arxiv.org/abs/2109.09113).
         
-        [2] [MobilNet](https://keras.io/api/applications/mobilenet/#mobilenet-function) from Keras applications.
+        [2] [Keras Applications](https://keras.io/api/applications/)
         
         [3] [TORCHVISION.MODELS](https://pytorch.org/vision/stable/models.html) 
         
+        [4] Gordon, O., Habi, H. V., & Netzer, A., 2023. [EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. arXiv preprint](https://arxiv.org/abs/2309.11531)
+        
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
```

### Comparing `mct-nightly-1.9.0.20230927.post404/mct_nightly.egg-info/SOURCES.txt` & `mct-nightly-2.0.0.20240402.404/mct_nightly.egg-info/SOURCES.txt`

 * *Files 3% similar despite different names*

```diff
@@ -5,24 +5,24 @@
 mct_nightly.egg-info/PKG-INFO
 mct_nightly.egg-info/SOURCES.txt
 mct_nightly.egg-info/dependency_links.txt
 mct_nightly.egg-info/requires.txt
 mct_nightly.egg-info/top_level.txt
 model_compression_toolkit/__init__.py
 model_compression_toolkit/constants.py
+model_compression_toolkit/defaultdict.py
 model_compression_toolkit/logger.py
 model_compression_toolkit/core/__init__.py
 model_compression_toolkit/core/analyzer.py
 model_compression_toolkit/core/exporter.py
 model_compression_toolkit/core/graph_prep_runner.py
+model_compression_toolkit/core/quantization_prep_runner.py
 model_compression_toolkit/core/runner.py
 model_compression_toolkit/core/common/__init__.py
 model_compression_toolkit/core/common/base_substitutions.py
-model_compression_toolkit/core/common/data_loader.py
-model_compression_toolkit/core/common/defaultdict.py
 model_compression_toolkit/core/common/framework_implementation.py
 model_compression_toolkit/core/common/framework_info.py
 model_compression_toolkit/core/common/memory_computation.py
 model_compression_toolkit/core/common/model_builder_mode.py
 model_compression_toolkit/core/common/model_collector.py
 model_compression_toolkit/core/common/model_validation.py
 model_compression_toolkit/core/common/node_prior_info.py
@@ -32,15 +32,14 @@
 model_compression_toolkit/core/common/back2framework/base_model_builder.py
 model_compression_toolkit/core/common/collectors/__init__.py
 model_compression_toolkit/core/common/collectors/base_collector.py
 model_compression_toolkit/core/common/collectors/histogram_collector.py
 model_compression_toolkit/core/common/collectors/mean_collector.py
 model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
 model_compression_toolkit/core/common/collectors/statistics_collector.py
-model_compression_toolkit/core/common/collectors/statistics_collector_generator.py
 model_compression_toolkit/core/common/fusion/__init__.py
 model_compression_toolkit/core/common/fusion/layer_fusing.py
 model_compression_toolkit/core/common/graph/__init__.py
 model_compression_toolkit/core/common/graph/base_graph.py
 model_compression_toolkit/core/common/graph/base_node.py
 model_compression_toolkit/core/common/graph/edge.py
 model_compression_toolkit/core/common/graph/functional_node.py
@@ -50,14 +49,19 @@
 model_compression_toolkit/core/common/graph/memory_graph/__init__.py
 model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
 model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
 model_compression_toolkit/core/common/graph/memory_graph/cut.py
 model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
 model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
 model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
+model_compression_toolkit/core/common/hessian/__init__.py
+model_compression_toolkit/core/common/hessian/hessian_info_service.py
+model_compression_toolkit/core/common/hessian/hessian_info_utils.py
+model_compression_toolkit/core/common/hessian/trace_hessian_calculator.py
+model_compression_toolkit/core/common/hessian/trace_hessian_request.py
 model_compression_toolkit/core/common/matchers/__init__.py
 model_compression_toolkit/core/common/matchers/base_graph_filter.py
 model_compression_toolkit/core/common/matchers/base_matcher.py
 model_compression_toolkit/core/common/matchers/edge_matcher.py
 model_compression_toolkit/core/common/matchers/function.py
 model_compression_toolkit/core/common/matchers/node_matcher.py
 model_compression_toolkit/core/common/matchers/walk_matcher.py
@@ -68,53 +72,67 @@
 model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
 model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
 model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
 model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
 model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
 model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py
 model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py
-model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/__init__.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization_data.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_aggregation_methods.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_functions_mapping.py
+model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_methods.py
 model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
 model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
 model_compression_toolkit/core/common/network_editors/__init__.py
 model_compression_toolkit/core/common/network_editors/actions.py
 model_compression_toolkit/core/common/network_editors/edit_network.py
 model_compression_toolkit/core/common/network_editors/node_filters.py
+model_compression_toolkit/core/common/pruning/__init__.py
+model_compression_toolkit/core/common/pruning/channels_grouping.py
+model_compression_toolkit/core/common/pruning/greedy_mask_calculator.py
+model_compression_toolkit/core/common/pruning/memory_calculator.py
+model_compression_toolkit/core/common/pruning/prune_graph.py
+model_compression_toolkit/core/common/pruning/pruner.py
+model_compression_toolkit/core/common/pruning/pruning_config.py
+model_compression_toolkit/core/common/pruning/pruning_framework_implementation.py
+model_compression_toolkit/core/common/pruning/pruning_info.py
+model_compression_toolkit/core/common/pruning/pruning_section.py
+model_compression_toolkit/core/common/pruning/importance_metrics/__init__.py
+model_compression_toolkit/core/common/pruning/importance_metrics/base_importance_metric.py
+model_compression_toolkit/core/common/pruning/importance_metrics/importance_metric_factory.py
+model_compression_toolkit/core/common/pruning/importance_metrics/lfh_importance_metric.py
+model_compression_toolkit/core/common/pruning/mask/__init__.py
+model_compression_toolkit/core/common/pruning/mask/per_channel_mask.py
+model_compression_toolkit/core/common/pruning/mask/per_simd_group_mask.py
 model_compression_toolkit/core/common/quantization/__init__.py
 model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
 model_compression_toolkit/core/common/quantization/core_config.py
 model_compression_toolkit/core/common/quantization/debug_config.py
 model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
 model_compression_toolkit/core/common/quantization/node_quantization_config.py
-model_compression_toolkit/core/common/quantization/quantization_analyzer.py
 model_compression_toolkit/core/common/quantization/quantization_config.py
 model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
 model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
 model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
 model_compression_toolkit/core/common/quantization/quantize_node.py
 model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
-model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
 model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
 model_compression_toolkit/core/common/quantization/quantizers/__init__.py
-model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py
 model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
 model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
 model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
 model_compression_toolkit/core/common/statistics_correction/__init__.py
 model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
 model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
 model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
@@ -139,45 +157,52 @@
 model_compression_toolkit/core/keras/__init__.py
 model_compression_toolkit/core/keras/constants.py
 model_compression_toolkit/core/keras/custom_layer_validation.py
 model_compression_toolkit/core/keras/default_framework_info.py
 model_compression_toolkit/core/keras/keras_implementation.py
 model_compression_toolkit/core/keras/keras_model_validation.py
 model_compression_toolkit/core/keras/keras_node_prior_info.py
-model_compression_toolkit/core/keras/kpi_data_facade.py
+model_compression_toolkit/core/keras/resource_utilization_data_facade.py
 model_compression_toolkit/core/keras/tf_tensor_numpy.py
 model_compression_toolkit/core/keras/back2framework/__init__.py
 model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
 model_compression_toolkit/core/keras/back2framework/float_model_builder.py
 model_compression_toolkit/core/keras/back2framework/instance_builder.py
 model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
 model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
-model_compression_toolkit/core/keras/back2framework/model_gradients.py
 model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
 model_compression_toolkit/core/keras/graph_substitutions/__init__.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
+model_compression_toolkit/core/keras/graph_substitutions/substitutions/dwconv_to_conv.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
+model_compression_toolkit/core/keras/graph_substitutions/substitutions/matmul_substitution.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
 model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
+model_compression_toolkit/core/keras/hessian/__init__.py
+model_compression_toolkit/core/keras/hessian/activation_trace_hessian_calculator_keras.py
+model_compression_toolkit/core/keras/hessian/trace_hessian_calculator_keras.py
+model_compression_toolkit/core/keras/hessian/weights_trace_hessian_calculator_keras.py
 model_compression_toolkit/core/keras/mixed_precision/__init__.py
 model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py
 model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py
+model_compression_toolkit/core/keras/pruning/__init__.py
+model_compression_toolkit/core/keras/pruning/pruning_keras_implementation.py
 model_compression_toolkit/core/keras/quantizer/__init__.py
 model_compression_toolkit/core/keras/quantizer/base_quantizer.py
 model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
 model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
 model_compression_toolkit/core/keras/reader/__init__.py
 model_compression_toolkit/core/keras/reader/common.py
 model_compression_toolkit/core/keras/reader/connectivity_handler.py
@@ -190,49 +215,57 @@
 model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
 model_compression_toolkit/core/keras/statistics_correction/__init__.py
 model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
 model_compression_toolkit/core/keras/visualization/__init__.py
 model_compression_toolkit/core/pytorch/__init__.py
 model_compression_toolkit/core/pytorch/constants.py
 model_compression_toolkit/core/pytorch/default_framework_info.py
-model_compression_toolkit/core/pytorch/kpi_data_facade.py
+model_compression_toolkit/core/pytorch/pytorch_device_config.py
 model_compression_toolkit/core/pytorch/pytorch_implementation.py
 model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
+model_compression_toolkit/core/pytorch/resource_utilization_data_facade.py
 model_compression_toolkit/core/pytorch/utils.py
 model_compression_toolkit/core/pytorch/back2framework/__init__.py
 model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
 model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
-model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
 model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
 model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
 model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
 model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
+model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_batch_norm.py
+model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/functional_layer_norm.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
 model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
+model_compression_toolkit/core/pytorch/hessian/__init__.py
+model_compression_toolkit/core/pytorch/hessian/activation_trace_hessian_calculator_pytorch.py
+model_compression_toolkit/core/pytorch/hessian/trace_hessian_calculator_pytorch.py
+model_compression_toolkit/core/pytorch/hessian/weights_trace_hessian_calculator_pytorch.py
 model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
 model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py
 model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py
+model_compression_toolkit/core/pytorch/pruning/__init__.py
+model_compression_toolkit/core/pytorch/pruning/pruning_pytorch_implementation.py
 model_compression_toolkit/core/pytorch/quantizer/__init__.py
 model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
 model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
 model_compression_toolkit/core/pytorch/reader/__init__.py
 model_compression_toolkit/core/pytorch/reader/graph_builders.py
 model_compression_toolkit/core/pytorch/reader/node_holders.py
 model_compression_toolkit/core/pytorch/reader/reader.py
@@ -271,40 +304,42 @@
 model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py
 model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py
 model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py
 model_compression_toolkit/exporter/__init__.py
 model_compression_toolkit/exporter/model_exporter/__init__.py
 model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
 model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
+model_compression_toolkit/exporter/model_exporter/fw_agonstic/quantization_format.py
 model_compression_toolkit/exporter/model_exporter/keras/__init__.py
 model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
 model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
+model_compression_toolkit/exporter/model_exporter/keras/mctq_keras_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
 model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
 model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
 model_compression_toolkit/exporter/model_wrapper/__init__.py
+model_compression_toolkit/exporter/model_wrapper/fw_agnostic/__init__.py
+model_compression_toolkit/exporter/model_wrapper/fw_agnostic/get_inferable_quantizers.py
 model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
 model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
-model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
-model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
 model_compression_toolkit/gptq/__init__.py
 model_compression_toolkit/gptq/runner.py
 model_compression_toolkit/gptq/common/__init__.py
 model_compression_toolkit/gptq/common/gptq_config.py
 model_compression_toolkit/gptq/common/gptq_constants.py
 model_compression_toolkit/gptq/common/gptq_framework_implementation.py
 model_compression_toolkit/gptq/common/gptq_graph.py
@@ -339,17 +374,19 @@
 model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
 model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
 model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
-model_compression_toolkit/legacy/__init__.py
-model_compression_toolkit/legacy/keras_quantization_facade.py
-model_compression_toolkit/legacy/pytorch_quantization_facade.py
+model_compression_toolkit/pruning/__init__.py
+model_compression_toolkit/pruning/keras/__init__.py
+model_compression_toolkit/pruning/keras/pruning_facade.py
+model_compression_toolkit/pruning/pytorch/__init__.py
+model_compression_toolkit/pruning/pytorch/pruning_facade.py
 model_compression_toolkit/ptq/__init__.py
 model_compression_toolkit/ptq/runner.py
 model_compression_toolkit/ptq/keras/__init__.py
 model_compression_toolkit/ptq/keras/quantization_facade.py
 model_compression_toolkit/ptq/pytorch/__init__.py
 model_compression_toolkit/ptq/pytorch/quantization_facade.py
 model_compression_toolkit/qat/__init__.py
@@ -357,35 +394,40 @@
 model_compression_toolkit/qat/common/qat_config.py
 model_compression_toolkit/qat/keras/__init__.py
 model_compression_toolkit/qat/keras/quantization_facade.py
 model_compression_toolkit/qat/keras/quantizer/__init__.py
 model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
 model_compression_toolkit/qat/keras/quantizer/quant_utils.py
 model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
+model_compression_toolkit/qat/keras/quantizer/lsq/__init__.py
+model_compression_toolkit/qat/keras/quantizer/lsq/symmetric_lsq.py
+model_compression_toolkit/qat/keras/quantizer/lsq/uniform_lsq.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
 model_compression_toolkit/qat/pytorch/__init__.py
 model_compression_toolkit/qat/pytorch/quantization_facade.py
 model_compression_toolkit/qat/pytorch/quantizer/__init__.py
 model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
 model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
 model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
+model_compression_toolkit/qat/pytorch/quantizer/lsq/__init__.py
+model_compression_toolkit/qat/pytorch/quantizer/lsq/symmetric_lsq.py
+model_compression_toolkit/qat/pytorch/quantizer/lsq/uniform_lsq.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
 model_compression_toolkit/target_platform_capabilities/__init__.py
 model_compression_toolkit/target_platform_capabilities/constants.py
 model_compression_toolkit/target_platform_capabilities/immutable.py
 model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
 model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
 model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
 model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
 model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
-model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py
 model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
 model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
 model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,52 +1,28 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.target_platform_capabilities import target_platform
-from model_compression_toolkit.target_platform_capabilities.tpc_models.get_target_platform_capabilities import get_target_platform_capabilities
-from model_compression_toolkit import core
-from model_compression_toolkit.logger import set_log_folder
-from model_compression_toolkit.legacy.keras_quantization_facade import keras_post_training_quantization, keras_post_training_quantization_mixed_precision
-from model_compression_toolkit.legacy.pytorch_quantization_facade import pytorch_post_training_quantization, pytorch_post_training_quantization_mixed_precision
-from model_compression_toolkit import trainable_infrastructure
-from model_compression_toolkit import ptq
-from model_compression_toolkit import qat
-from model_compression_toolkit import exporter
-from model_compression_toolkit import gptq
-from model_compression_toolkit import data_generation
-from model_compression_toolkit.trainable_infrastructure.keras.load_model import keras_load_quantized_model
-
-
-# Old API (will not be accessible in future releases)
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
 from model_compression_toolkit.core.common import network_editors as network_editor
+from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
 from model_compression_toolkit.core.common.quantization import quantization_config
 from model_compression_toolkit.core.common.mixed_precision import mixed_precision_quantization_config
-from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, QuantizationErrorMethod, DEFAULTCONFIG
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
+from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
-from model_compression_toolkit.logger import set_log_folder
-from model_compression_toolkit.core.common.data_loader import FolderImageLoader
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.legacy.keras_quantization_facade import keras_post_training_quantization, keras_post_training_quantization_mixed_precision
-from model_compression_toolkit.legacy.pytorch_quantization_facade import pytorch_post_training_quantization, pytorch_post_training_quantization_mixed_precision
-from model_compression_toolkit.core.keras.kpi_data_facade import keras_kpi_data
-from model_compression_toolkit.core.pytorch.kpi_data_facade import pytorch_kpi_data
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
-from model_compression_toolkit.gptq.common.gptq_config import RoundingType
-from model_compression_toolkit.gptq.keras.quantization_facade import get_keras_gptq_config
-from model_compression_toolkit.gptq.pytorch.quantization_facade import get_pytorch_gptq_config
+from model_compression_toolkit.core.keras.resource_utilization_data_facade import keras_resource_utilization_data
+from model_compression_toolkit.core.pytorch.resource_utilization_data_facade import pytorch_resource_utilization_data
+from model_compression_toolkit.core.common.mixed_precision.distance_weighting import MpDistanceWeighting
 
-__version__ = "1.9.0"
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/constants.py`

 * *Files 14% similar despite different names*

```diff
@@ -18,20 +18,22 @@
 # Supported frameworks in MCT:
 TENSORFLOW = 'tensorflow'
 PYTORCH = 'pytorch'
 FOUND_TF = importlib.util.find_spec(TENSORFLOW) is not None
 FOUND_TORCH = importlib.util.find_spec("torch") is not None
 FOUND_ONNX = importlib.util.find_spec("onnx") is not None
 FOUND_ONNXRUNTIME = importlib.util.find_spec("onnxruntime") is not None
+FOUND_SONY_CUSTOM_LAYERS = importlib.util.find_spec('sony_custom_layers') is not None
 
 WEIGHTS_SIGNED = True
 # Minimal threshold to use for quantization ranges:
 MIN_THRESHOLD = (2 ** -16)
 EPS = 1e-8
 LUT_VALUES_BITWIDTH = 8
+FP32_BYTES_PER_PARAMETER = 4.
 
 # Quantization attributes:
 OUTPUT_SCALE = 'output_scale'
 THRESHOLD = 'threshold'
 SIGNED = 'is_signed'
 LUT_VALUES = 'lut_values'
 SCALE_PER_CHANNEL = 'scale_per_channel'
@@ -87,36 +89,34 @@
 DEFAULT_DEC_FACTOR = (1.02, 0.98)
 DEFAULT_TOL = 1e-11
 BOTTOM_FACTOR = 0.7
 UPPER_FACTOR = 1.2
 DEC_RANGE_BOTTOM = 0.97
 DEC_RANGE_UPPER = 1.03
 
-# KPI computation parameters
+# Resource utilization computation parameters
 BITS_TO_BYTES = 8.0
 
 # Default threshold for Softmax layer
 SOFTMAX_THRESHOLD = 1
 
 # Substitutions node names
 VIRTUAL_WEIGHTS_SUFFIX = '_v_weights'
 VIRTUAL_ACTIVATION_SUFFIX = '_v_activation'
 VIRTUAL_ACTIVATION_WEIGHTS_NODE_PREFIX = 'virtual'
 
-# Quantization config candidate initialization
-ACTIVATION_QUANTIZATION_CFG = 'activation_quantization_cfg'
-WEIGHTS_QUANTIZATION_CFG = 'weights_quantization_cfg'
-QC = 'qc'
-OP_CFG = 'op_cfg'
-ACTIVATION_QUANTIZATION_FN = 'activation_quantization_fn'
-WEIGHTS_QUANTIZATION_FN = 'weights_quantization_fn'
-ACTIVATION_QUANT_PARAMS_FN = 'activation_quantization_params_fn'
-WEIGHTS_QUANT_PARAMS_FN = 'weights_quantization_params_fn'
-WEIGHTS_CHANNELS_AXIS = 'weights_channels_axis'
-
 # Memory graph constants
 DUMMY_NODE = 'dummy_node'
 DUMMY_TENSOR = 'dummy_tensor'
 
-# Jacobian-weights constants
-MIN_JACOBIANS_ITER = 10
-JACOBIANS_COMP_TOLERANCE = 1e-3
+# Hessian scores constants
+MIN_HESSIAN_ITER = 10
+HESSIAN_COMP_TOLERANCE = 1e-3
+
+
+# Hessian configuration default constants
+HESSIAN_OUTPUT_ALPHA = 0.3
+HESSIAN_NUM_ITERATIONS = 50
+HESSIAN_EPS = 1e-6
+
+# Pruning constants
+PRUNING_NUM_SCORE_APPROXIMATIONS = 32
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,28 +1,23 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from model_compression_toolkit.core.common.quantization import quantization_params_generation
+from model_compression_toolkit.core.common.base_substitutions import BaseSubstitution
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.graph.base_node import BaseNode
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, DEFAULTCONFIG
+from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import max_power_of_two
+from model_compression_toolkit.core.common.collectors.statistics_collector import StatsCollector, NoStatsCollector
 
-from model_compression_toolkit.core.common.data_loader import FolderImageLoader
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common import network_editors as network_editor
-from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
-from model_compression_toolkit.core.common.quantization import quantization_config
-from model_compression_toolkit.core.common.mixed_precision import mixed_precision_quantization_config
-from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, QuantizationErrorMethod, DEFAULTCONFIG
-from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.keras.kpi_data_facade import keras_kpi_data, keras_kpi_data_experimental
-from model_compression_toolkit.core.pytorch.kpi_data_facade import pytorch_kpi_data, pytorch_kpi_data_experimental
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/analyzer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/analyzer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,16 +8,13 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.quantization import quantization_params_generation
-from model_compression_toolkit.core.common.base_substitutions import BaseSubstitution
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.graph.base_graph import Graph
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, DEFAULTCONFIG
-from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import max_power_of_two
-from model_compression_toolkit.core.common.collectors.statistics_collector import StatsCollector, NoStatsCollector
-
+from model_compression_toolkit.core.common.quantization.quantization_params_generation.power_of_two_selection import power_of_two_no_clipping_selection_min_max, \
+    power_of_two_selection_histogram, power_of_two_selection_tensor
+from model_compression_toolkit.core.common.quantization.quantization_params_generation.lut_kmeans_params import lut_kmeans_tensor
+from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import symmetric_no_clipping_selection_min_max
+from model_compression_toolkit.core.common.quantization.quantization_params_generation.uniform_selection import uniform_no_clipping_selection_min_max
+from model_compression_toolkit.core.common.quantization.quantization_params_generation.outlier_filter import z_score_filter
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/back2framework/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/back2framework/base_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/back2framework/base_model_builder.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/base_substitutions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/base_substitutions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/base_collector.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/base_collector.py`

 * *Files 4% similar despite different names*

```diff
@@ -62,9 +62,9 @@
     def validate_data_correctness(self):
         """
         Verify the collector's statistics were manipulated in a granularity they were collected by.
         If the statistics are invalid, an exception is raised.
         """
 
         if not self.is_legal:
-            Logger.exception(f'{self.__class__.__name__} was manipulated per-channel,'
-                             'but collected per-tensor. Data is invalid.')  # pragma: no cover
+            Logger.critical('The data is invalid.'
+                            f'{self.__class__.__name__} was collected per-tensor but received data manipulated per-channel.')  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/histogram_collector.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/histogram_collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -69,15 +69,15 @@
             bins_stack = np.vstack([hist[1] for hist in self.__histogram_per_iteration])
 
             # The combined histogram will be computed between new min/max (which is the min/max of all histograms).
             # The bin width of the merged histogram is the minimal bin width among all histograms (to lose as less
             # information as possible during the merge).
             merged_histogram_min = np.min(bins_stack)
             merged_histogram_max = np.max(bins_stack)
-            merged_bin_width = np.min(bins_stack[:, 1] - bins_stack[:, 0])
+            merged_bin_width = (merged_histogram_max - merged_histogram_min) / self.__n_bins
             merged_histogram_bins = np.arange(merged_histogram_min, merged_histogram_max+merged_bin_width, merged_bin_width)
 
             merged_histogram_counts = None
             for histogram in self.__histogram_per_iteration:  # Iterate all collected histograms and merge them
                 if merged_histogram_counts is None:  # First histogram to consider
                     merged_histogram_counts = interpolate_histogram(merged_histogram_bins, histogram[1], histogram[0])
                 else:  # Merge rest of histograms into existing final histogram
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/mean_collector.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/mean_collector.py`

 * *Files 22% similar despite different names*

```diff
@@ -23,82 +23,75 @@
 class MeanCollector(BaseCollector):
     """
         Class to collect observed per channel mean values of tensors that goes through it (passed to update).
         The mean is calculated using a exponential moving average with bias correction.
     """
 
     def __init__(self,
-                 axis: int,
-                 beta: float = 0.99):
+                 axis: int):
         """
         Instantiate a per channel mean collector using a exponential moving average with bias correction.
 
         Args:
             axis: Compute the mean with regard to this axis.
-            beta: Parameter for mean smoothing by EMA.
         """
         super().__init__()
         self.axis = axis
-        self.__state_internal = np.array([0.0])  # mean per-channel
-        self.__state_internal_correction = None
-        self.beta = beta
-        self.i = 0.0
+        self.current_mean = 0
+        self.current_sum = 0
+        self.i = 0
 
     def scale(self, scale_factor: np.ndarray):
         """
         Scale all statistics in collector by some factor.
         Since mean was collected per-channel, it can be scaled either by a single factor or a factor
         per-channel.
         The scaling is done using the corrected mean.
 
         Args:
             scale_factor: Factor to scale all collector's statistics by.
 
         """
 
-        self.__state_internal_correction *= scale_factor
+        self.current_mean *= scale_factor
 
     def shift(self, shift_value: np.ndarray):
         """
         Shift all statistics in collector by some value.
         Since mean was collected per-channel, it can be shifted either by a single value or a
         shifting value per-channel.
         The shifting is done using the corrected mean.
 
         Args:
             shift_value: Value to shift all collector's statistics by.
 
         """
 
-        self.__state_internal_correction += shift_value
+        self.current_mean += shift_value
 
     @property
     def state(self):
         """
         The mean is kept internal and corrected when accessed from outside the collector.
 
         Returns: Mean of the collector after bias correction.
         """
         self.validate_data_correctness()
-        return self.__state_internal_correction
+        return self.current_mean
 
     def update(self,
                x: np.ndarray):
         """
         Update the mean using a new tensor x to consider.
 
         Args:
             x: Tensor that goes through the mean collector and needs to be considered in the mean computation.
         """
-
         self.i += 1  # Update the iteration index
         axis = (len(x.shape) - 1) if self.axis == LAST_AXIS else self.axis
         n = x.shape[axis]
         transpose_index = [axis, *[i for i in range(len(x.shape)) if i != axis]]
-        mu = np.mean(np.reshape(np.transpose(x, transpose_index), [n, -1]), axis=-1)  # compute mean per channel
-        update_state = self.beta * self.__state_internal + (1 - self.beta) * mu
-        self.__state_internal = update_state
-
-        # Since we use a weighted mean, initial values can be distorted,
-        # so use bias correction to compensate it.
-        bias_correction = 1 - (self.beta ** self.i)
-        self.__state_internal_correction = self.__state_internal / bias_correction
+        mu = np.mean(np.reshape(np.transpose(x, transpose_index), [n, -1]), axis=-1) # mean per channel for a batch
+        self.current_sum += mu # sum of all batches
+        self.current_mean = self.current_sum / self.i # mean of all batches
+
+
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/collectors/statistics_collector.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/collectors/statistics_collector.py`

 * *Files 0% similar despite different names*

```diff
@@ -98,15 +98,15 @@
         """
 
         return self.mc.state
 
     def get_min_max_values(self) -> Tuple[float, float]:
         """
         Get min/max from collector.
-        When its accessed from outside the tensor, the scale and shift come into consideration.
+        When it's accessed from outside the tensor, the scale and shift come into consideration.
 
         Returns: Min/max from collector.
         """
 
         min_value = self.mpcc.min
         max_value = self.mpcc.max
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/defaultdict.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/defaultdict.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,36 +10,37 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
-from typing import Callable, Dict, Any
+from typing import Dict, Any
+from copy import deepcopy
 
 
-class DefaultDict(object):
+class DefaultDict:
     """
     Default dictionary. It wraps a dictionary given at initialization and return its
     values when requested. If the requested key is not presented at initial dictionary,
-    it returns the returned value a default factory (that is passed at initialization) generates.
+    it returns the returned value a default value (that is passed at initialization) generates.
     """
 
     def __init__(self,
-                 known_dict: Dict[Any, Any],
-                 default_factory: Callable = None):
+                 known_dict: Dict[Any, Any] = None,
+                 default_value: Any = None):
         """
 
         Args:
-            known_dict: Dictionary to wrap.
-            default_factory: Callable to get default values when requested key is not in known_dict.
+            known_dict: Dictionary to wrap. If None is provided, initializes an empty dictionary.
+            default_value: default value when requested key is not in known_dict.
         """
 
-        self.default_factory = default_factory
-        self.known_dict = known_dict
+        self.default_value = default_value
+        self.known_dict = known_dict if known_dict is not None else {}
 
     def get(self, key: Any) -> Any:
         """
         Get the value of the inner dictionary by the given key, If key is not in dictionary,
         it uses the default_factory to return a default value.
 
         Args:
@@ -47,19 +48,17 @@
 
         Returns:
             Value of the inner dictionary by the given key, or a default value if not exist.
             If default_factory was not passed at initialization, it returns None.
         """
 
         if key in self.known_dict:
-            return self.known_dict.get(key)
+            return self.known_dict[key]
         else:
-            if self.default_factory is not None:
-                return self.default_factory()
-            return None
+            return deepcopy(self.default_value)
 
     def keys(self):
         """
         Get keys of known_dict
         Returns: keys of known_dict
         """
         return self.known_dict.keys()
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/framework_implementation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/framework_implementation.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,20 +13,22 @@
 # limitations under the License.
 # ==============================================================================
 from abc import ABC, abstractmethod
 from typing import Callable, Any, List, Tuple, Dict
 
 import numpy as np
 
-from model_compression_toolkit.core import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.constants import HESSIAN_NUM_ITERATIONS
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianInfoService
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.user_info import UserInformation
 
@@ -43,14 +45,33 @@
 
         Returns: Module of the framework constants.
 
         """
         raise NotImplemented(f'{self.__class__.__name__} did not supply a constants module.')  # pragma: no cover
 
     @abstractmethod
+    def get_trace_hessian_calculator(self,
+                                     graph: Graph,
+                                     input_images: List[Any],
+                                     trace_hessian_request: TraceHessianRequest,
+                                     num_iterations_for_approximation: int = HESSIAN_NUM_ITERATIONS):
+        """
+        Get framework trace hessian approximations calculator based on the trace hessian request.
+        Args:
+            input_images: Images to use for computation.
+            graph: Float graph to compute the approximation of its different nodes.
+            trace_hessian_request: TraceHessianRequest to search for the desired calculator.
+            num_iterations_for_approximation: Number of iterations to use when approximating the Hessian trace.
+
+        Returns: TraceHessianCalculator to use for the trace hessian approximation computation for this request.
+        """
+        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
+                             f'framework\'s get_trace_hessian_calculator method.')  # pragma: no cover
+
+    @abstractmethod
     def to_numpy(self, tensor: Any) -> np.ndarray:
         """
         Convert framework's tensor to a Numpy array.
         Args:
             tensor: Framework's tensor.
 
         Returns:
@@ -146,30 +167,14 @@
         Returns:
             Graph after SNC.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s apply_shift_negative_correction method.')  # pragma: no cover
 
     @abstractmethod
-    def attach_sc_to_node(self, node: BaseNode, fw_info: FrameworkInfo) -> BaseStatsCollector:
-        """
-        Return a statistics collector that should be attached to a node's output
-        during statistics collection.
-
-        Args:
-            node: Node to return its collector.
-            fw_info: Information relevant to a specific framework about what is out channel axis (for statistics per-channel).
-
-        Returns:
-            Statistics collector for the node.
-        """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s attach_sc_to_node method.')  # pragma: no cover
-
-    @abstractmethod
     def get_substitutions_channel_equalization(self,
                                                quant_config: QuantizationConfig,
                                                fw_info: FrameworkInfo) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used for channel equalization.
 
         Args:
@@ -211,14 +216,22 @@
         """
         Returns: linear collapsing substitution
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_linear_collapsing_substitution method.')  # pragma: no cover
 
     @abstractmethod
+    def get_op2d_add_const_collapsing_substitution(self) -> common.BaseSubstitution:
+        """
+        Returns: conv2d add const collapsing substitution
+        """
+        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
+                             f'framework\'s get_op2d_add_const_collapsing_substitution method.')  # pragma: no cover
+
+    @abstractmethod
     def get_substitutions_statistics_correction(self, quant_config: QuantizationConfig) -> \
             List[common.BaseSubstitution]:
         """
         Returns A list of the framework substitutions used for statistics correction.
 
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
@@ -287,28 +300,30 @@
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_substitutions_after_second_moment_correction '
                              f'method.')  # pragma: no cover
 
     @abstractmethod
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
-                                  quant_config: MixedPrecisionQuantizationConfigV2,
+                                  quant_config: MixedPrecisionQuantizationConfig,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
+                                  hessian_info_service: HessianInfoService = None,
                                   disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
         """
         Creates and returns an object which handles the computation of a sensitivity metric for a mixed-precision
         configuration (comparing to the float model).
 
         Args:
             graph: Graph to build its float and mixed-precision models.
             quant_config: QuantizationConfig of how the model should be quantized.
             representative_data_gen: Dataset to use for retrieving images for the models inputs.
             fw_info: FrameworkInfo object with information about the specific framework's model.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch Hessian traces approximations.
 
         Returns:
             A function that computes the metric.
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_sensitivity_evaluator method.')  # pragma: no cover
@@ -340,81 +355,49 @@
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s count_node_for_mixed_precision_interest_points method.')  # pragma: no cover
 
     def get_node_distance_fn(self, layer_class: type,
                              framework_attrs: Dict[str, Any],
-                             compute_distance_fn: Callable = None) -> Callable:
+                             compute_distance_fn: Callable = None,
+                             axis: int = None) -> Callable:
         """
         A mapping between layers' types and a distance function for computing the distance between
         two tensors (for loss computation purposes). Returns a specific function if node of specific types is
         given, or a default (normalized MSE) function otherwise.
 
         Args:
             layer_class: Class path of a model's layer.
             framework_attrs: Framework attributes the layer had which the graph node holds.
             compute_distance_fn: An optional distance function to use globally for all nodes.
+            axis: The axis on which the operation is preformed (if specified).
 
         Returns: A distance function between two tensors.
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_node_distance_fn method.')  # pragma: no cover
 
-    @abstractmethod
-    def model_grad(self,
-                   graph_float: common.Graph,
-                   model_input_tensors: Dict[BaseNode, np.ndarray],
-                   interest_points: List[BaseNode],
-                   output_list: List[BaseNode],
-                   all_outputs_indices: List[int],
-                   alpha: float = 0.3,
-                   n_iter: int = 50,
-                   norm_weights: bool = True) -> List[float]:
-        """
-        Calls a framework specific model gradient calculation function, which computes the jacobian-based weights of the model's
-        outputs with respect to the feature maps of the set of given interest points.
-
-        Args:
-            graph_float: Graph to build its corresponding Keras model.
-            model_input_tensors: A mapping between model input nodes to an input batch.
-            interest_points: List of nodes which we want to get their feature map as output, to calculate distance metric.
-            output_list: List of nodes that considered as model's output for the purpose of gradients computation.
-            all_outputs_indices: Indices of the model outputs and outputs replacements (if exists),
-                in a topological sorted interest points list.
-            alpha: A tuning parameter to allow calibration between the contribution of the output feature maps returned
-                weights and the other feature maps weights (since the gradient of the output layers does not provide a
-                compatible weight for the distance metric computation).
-            n_iter: The number of random iterations to calculate the approximated jacobian-based weights for each interest point.
-            norm_weights: Whether to normalize the returned weights (to get values between 0 and 1).
-
-        Returns: A list of (possibly normalized) jacobian-based weights to be considered as the relevancy that each interest
-        point's output has on the model's output.
-        """
-
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s model_grad method.')  # pragma: no cover
 
     @abstractmethod
-    def is_node_compatible_for_metric_outputs(self,
-                                                 node: BaseNode) -> bool:
+    def is_output_node_compatible_for_hessian_score_computation(self,
+                                                                node: BaseNode) -> bool:
         """
-        Checks and returns whether the given node is compatible as output for metric computation
-        purposes and gradient-based weights calculation.
+        Checks and returns whether the given node is compatible as output for Hessian-based information computation.
 
         Args:
             node: A BaseNode object.
 
-        Returns: Whether the node is compatible as output for metric computation or not.
+        Returns: Whether the node is compatible as output for Hessian-based information computation.
 
         """
 
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s is_node_compatible_for_metric_outputs method.')  # pragma: no cover
+                             f'framework\'s is_output_node_compatible_for_hessian_score_computation method.')  # pragma: no cover
 
     @abstractmethod
     def get_node_mac_operations(self,
                                 node: BaseNode,
                                 fw_info: FrameworkInfo) -> float:
         """
         Gets the MAC operation count for a given operation.
@@ -462,7 +445,23 @@
             inputs: Input tensors to run inference on.
 
         Returns:
             The output of the model inference on the given input.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s sensitivity_eval_inference method.')  # pragma: no cover
+
+    def get_inferable_quantizers(self, node: BaseNode):
+        """
+        Returns sets of framework compatible weights and activation quantizers for the given node.
+
+        Args:
+           node: Node to get quantizers for.
+
+        Returns:
+            weight_quantizers: A dictionary between a weight's name to its quantizer.
+            activation_quantizers: A list of activations quantization, one for each layer output.
+
+        """
+
+        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
+                             f'framework\'s get_inferable_quantizers method.')  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/framework_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/framework_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,16 +16,15 @@
 
 from collections.abc import Callable
 from enum import Enum
 from typing import Dict, Any, List
 
 
 
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 class ChannelAxis(Enum):
     """
 
     Index of output channels axis:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/fusion/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/fusion/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/fusion/layer_fusing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/fusion/layer_fusing.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,41 +27,43 @@
         fusing_patterns: supported fusings
         node: node to decide if it can be a part of fusion
         idx: index of layer in the fusion
     Returns:
         fusing_patterns after filtering non-relevant fusions
     """
     valid_fusing_patterns = []
-    for i,fusing_pattern in enumerate(fusing_patterns):
+    for i, fusing_pattern in enumerate(fusing_patterns):
         if idx < len(fusing_pattern):
-            if (type(fusing_pattern[idx]) == LayerFilterParams and node.is_match_filter_params(fusing_pattern[idx])) or fusing_pattern[idx] == node.type:
+            if (type(fusing_pattern[idx]) == LayerFilterParams and node.is_match_filter_params(fusing_pattern[idx])) or \
+                    node.is_match_type(fusing_pattern[idx]):
                 valid_fusing_patterns.append(fusing_pattern)
 
     # Return only valid patterns for this node
     return valid_fusing_patterns
 
 
 def is_valid_fusion(fusing_patterns: List[List[Any]], nodes: List[BaseNode]) -> bool:
     """
     Check if the fusion is valid: exist in fusing_patterns
     Args:
-        fusing_patterns: supported fusings
+        fusing_patterns: supported fusing patterns
         nodes: nodes which are participating in fusion
     Returns:
         whether the fusion in valid
     """
     fusion_depth = len(nodes)
     if fusion_depth <= 1:
         return False
     for fusing_pattern in fusing_patterns:
         if fusion_depth != len(fusing_pattern):
             continue
         counter = 0
-        for i,layer in enumerate(fusing_pattern):
-            if (type(layer) == LayerFilterParams and nodes[i].is_match_filter_params(layer)) or layer == nodes[i].type:
+        for i, layer in enumerate(fusing_pattern):
+            if (type(layer) == LayerFilterParams and nodes[i].is_match_filter_params(layer)) or \
+                    nodes[i].is_match_type(layer):
                 counter += 1
         if counter == fusion_depth:
             return True
     return False
 
 
 def disable_nodes_activation_quantization(nodes: List[BaseNode]):
@@ -103,15 +105,15 @@
     nodes = fused_graph.get_topo_sorted_nodes()
     fused_nodes = []  # nodes that are participating in fusing
     for node in nodes:
         # Skip if already in fusing
         if node in fused_nodes:
             continue
         # Start fusing search
-        fusing_nodes = [] # nodes that are candidates for participating in fusing
+        fusing_nodes = []  # nodes that are candidates for participating in fusing
         patterns = copy.deepcopy(fusing_patterns)
         next_nodes = [node]
         for i in range(max_layers_fusing):
             patterns = filter_fusing_patterns(patterns, next_nodes[0], i)
             if len(patterns) == 0: # Give up if no more fusion pattern
                 break
             fusing_nodes.append(next_nodes[0])
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/base_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/base_graph.py`

 * *Files 23% similar despite different names*

```diff
@@ -25,17 +25,19 @@
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX, EDGE_SOURCE_INDEX
 from model_compression_toolkit.core.common.graph.edge import Edge, convert_to_edge
 from model_compression_toolkit.core.common.graph.graph_searches import GraphSearches
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.collectors.statistics_collector import scale_statistics, shift_statistics
+from model_compression_toolkit.core.common.pruning.pruning_section import PruningSection
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
+    TargetPlatformCapabilities, LayerFilterParams
 
 OutTensor = namedtuple('OutTensor', 'node node_out_index')
 
 
 class Graph(nx.MultiDiGraph, GraphSearches):
     """
     Base graph representing a model to be optimized.
@@ -87,14 +89,28 @@
     def set_tpc(self,
                 tpc: TargetPlatformCapabilities):
         """
         Set the graph's TPC.
         Args:
             tpc: TargetPlatformCapabilities object.
         """
+        # validate graph nodes are either from the framework or a custom layer defined in the TPC
+        # Validate graph nodes are either built-in layers from the framework or custom layers defined in the TPC
+        tpc_layers = tpc.op_sets_to_layers.get_layers()
+        tpc_filtered_layers = [layer for layer in tpc_layers if isinstance(layer, LayerFilterParams)]
+        for n in self.nodes:
+            is_node_in_tpc = n.type in tpc_layers or any([n.is_match_filter_params(filtered_layer)
+                                                          for filtered_layer in tpc_filtered_layers])
+            if n.is_custom:
+                if not is_node_in_tpc:
+                    Logger.critical(f'MCT does not support optimizing Keras custom layers. Found a layer of type {n.type}. '
+                                 f' Please add the custom layer to Target Platform Capabilities (TPC), or file a feature request or an issue if you believe this should be supported.')
+                if any([qc.default_weight_attr_config.enable_weights_quantization for qc in n.get_qco(tpc).quantization_config_list]):
+                    Logger.critical(f'Layer identified: {n.type}. MCT does not support weight quantization for Keras custom layers.')
+
         self.tpc = tpc
 
     def get_topo_sorted_nodes(self):
         """
         Returns: a list of toposorted nodes.
         """
 
@@ -194,15 +210,15 @@
                                 n: BaseNode) -> BaseStatsCollector:
         """
         Get the output statistics collector of a node containing output statistics of the node.
         Args:
             n: Node to get its output statistics collector.
 
         Returns:
-            Tensor containing output statistics of the node.
+            BaseStatsCollector object of the node.
         """
         return self.node_to_out_stats_collector.get(n)
 
     def get_in_stats_collector(self,
                                n: BaseNode) -> BaseStatsCollector:
         """
         Get the input statistics collector of a node containing input statistics of the node.
@@ -211,15 +227,15 @@
 
         Returns:
             Statistics collector containing input statistics of the node.
         """
 
         sc = self.node_to_in_stats_collector.get(n)
         if sc is None:
-            Logger.error(f'Input statistics collector of node {n.name} is None')  # pragma: no cover
+            Logger.critical(f'No input statistics collector found for node {n.name}.')  # pragma: no cover
         return sc
 
     def scale_stats_collector(self,
                               node: BaseNode,
                               scale_factor: np.ndarray):
         """
         Scale the output statistics of a node in the graph by a given scaling factor.
@@ -280,27 +296,32 @@
             List of output nodes objects.
 
         """
 
         return [edges_list.sink_node for edges_list in self.out_edges(node_obj)]
 
     def get_prev_nodes(self,
-                       node_obj: BaseNode) -> List[BaseNode]:
+                       node_obj: BaseNode,
+                       sink_index_sorted: bool = False) -> List[BaseNode]:
         """
         Get previous nodes (in a topological order) of a node.
 
         Args:
             node_obj: Node to get its previous nodes.
+            sink_index_sorted: Whether to sort the returned list by the sink_index of the edges.
 
         Returns:
             List of input nodes objects.
 
         """
-
-        return [edges_list.source_node for edges_list in self.incoming_edges(node_obj)]
+        if sink_index_sorted:
+            sort_attr = 'sink_index'
+        else:
+            sort_attr = None
+        return [edges_list.source_node for edges_list in self.incoming_edges(node_obj, sort_by_attr=sort_attr)]
 
     def reconnect_out_edges(self,
                             current_node: BaseNode,
                             new_node: BaseNode):
         """
         Connect all outgoing edges of a node to be outgoing edges of a different node
         (useful when replacing a node during substitutions).
@@ -345,16 +366,15 @@
              are zero
         """
 
         if len(input_nodes_output_index) == 0:
             input_nodes_output_index = [0] * len(input_nodes)
 
         if len(input_nodes_output_index) != len(input_nodes):
-            Logger.error('Graph.add_node_with_in_edges: input_nodes & input_nodes_output_index must be the same '
-                         'length')  # pragma: no cover
+            Logger.critical('The number of input nodes and their corresponding output indices must be equal. Found mismatched lengths.')  # pragma: no cover
 
         self.add_node(new_node)
         for sink_index, (in_node, source_index) in enumerate(zip(input_nodes, input_nodes_output_index)):
             self.add_edge(in_node, new_node, source_index=source_index, sink_index=sink_index)
 
     def replace_output_node(self,
                             current_node: BaseNode,
@@ -389,15 +409,15 @@
 
         Args:
             current_node: Node that (possibly) is an input node.
             new_node: New node to set as an input node if the current node is an input node.
 
         """
         if new_node is None:
-            Logger.error("Graph received a None value as a new input node.")
+            Logger.critical("Cannot replace input node with a None value; new input node is required.")
 
         graph_inputs = self.get_inputs()
         new_graph_inputs = copy(graph_inputs)
         if current_node in graph_inputs:
             new_graph_inputs.remove(current_node)
             new_graph_inputs.append(new_node)
         self.set_inputs(new_graph_inputs)
@@ -417,21 +437,21 @@
 
         """
 
         output_nodes = [ot.node for ot in self.get_outputs()]  # get output nodes from namedtuples
         if node_to_remove in output_nodes:  # If node is in the graph's outputs, the outputs should be updated
             if new_graph_outputs is None:
                 Logger.critical(
-                    f'{node_to_remove.name} is in graph outputs, but new outputs were not given.')  # pragma: no cover
+                    f"{node_to_remove.name} is among the graph outputs; however, it cannot be removed without providing a new output.")  # pragma: no cover
             self.set_outputs(new_graph_outputs)
 
         if node_to_remove in self.get_inputs():  # If node is in the graph's inputs, the inputs should be updated
             if new_graph_inputs is None:
                 Logger.critical(
-                    f'{node_to_remove.name} is in graph inputs, but new inputs were not given.')  # pragma: no cover
+                    f'{node_to_remove.name} s among the graph inputs; however, it cannot be removed without providing a new input.')  # pragma: no cover
             self.set_inputs(new_graph_inputs)
 
         # Make sure there are no connected edges left to the node before removing it.
         assert len(
             self.incoming_edges(node_to_remove)) == 0, f'There are {len(self.incoming_edges(node_to_remove))} ' \
                                                        f'incoming ' \
                                                        f'edges to node {node_to_remove}, and they should be removed ' \
@@ -504,60 +524,69 @@
         """
         memory = 0
         for n in self.nodes:
             memory += n.get_float_memory_bytes(self.fw_info)
         return memory
 
     def get_configurable_sorted_nodes_names(self,
+                                            fw_info: FrameworkInfo,
                                             include_reused_nodes: bool = False) -> List[str]:
         """
         Get a list of nodes' names that can be configured (namely, has one or
         more weight qc candidate). The names are sorted according to the topological
         order of the graph.
 
         Args:
+            fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether or not to include reused nodes (False by default).
 
         Returns: List of nodes' names that can be configured (namely, has one or
         more weight qc candidate) sorted topology.
 
         """
-        sorted_names = [n.name for n in self.get_configurable_sorted_nodes(include_reused_nodes=include_reused_nodes)]
+        sorted_names = [n.name for n in self.get_configurable_sorted_nodes(fw_info=fw_info,
+                                                                           include_reused_nodes=include_reused_nodes)]
         return sorted_names
 
     def get_weights_configurable_nodes(self,
+                                       fw_info: FrameworkInfo,
                                        include_reused_nodes: bool = False) -> List[BaseNode]:
         """
         Get a list of nodes that their weights can be configured (namely, has one or
         more weight qc candidate and their weights should be quantized).
 
         Args:
+            fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether to include reused nodes (False by default).
 
         Returns:
             A list of nodes that their weights can be configured (namely, has one or more weight qc candidate).
         """
-        return list(filter(lambda n: n.is_weights_quantization_enabled()
-                                     and not n.is_all_weights_candidates_equal()
-                                     and (not n.reuse or include_reused_nodes), list(self)))
+        # configurability is only relevant for kernel attribute quantization
+        potential_conf_nodes = [n for n in list(self) if fw_info.is_kernel_op(n.type)]
+        return list(filter(lambda n: n.is_weights_quantization_enabled(fw_info.get_kernel_op_attributes(n.type)[0])
+                                     and not n.is_all_weights_candidates_equal(fw_info.get_kernel_op_attributes(n.type)[0])
+                                     and (not n.reuse or include_reused_nodes), potential_conf_nodes))
 
     def get_sorted_weights_configurable_nodes(self,
+                                              fw_info: FrameworkInfo,
                                               include_reused_nodes: bool = False) -> List[BaseNode]:
         """
         Get a list of sorted nodes that their weights can be configured (namely, has one or
         more weight qc candidate and their weights should be quantized).
 
         Args:
+            fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether to include reused nodes (False by default).
 
         Returns:
             A list of nodes that their weights can be configured (namely, has one or more weight qc candidate)
             sorted topologically.
         """
-        return self._sort_nodes_in_list(self.get_weights_configurable_nodes(include_reused_nodes))
+        return self._sort_nodes_in_list(self.get_weights_configurable_nodes(fw_info, include_reused_nodes))
 
     def get_activation_configurable_nodes(self) -> List[BaseNode]:
         """
         Get a list of nodes that their activation can be configured (namely, has one or
         more activation qc candidate and their activation should be quantized).
 
         Returns:
@@ -574,28 +603,30 @@
         Returns:
             A list of nodes that their activation can be configured (namely, has one or more activation qc candidate)
             sorted topologically.
         """
         return self._sort_nodes_in_list(self.get_activation_configurable_nodes())
 
     def get_configurable_sorted_nodes(self,
+                                      fw_info: FrameworkInfo,
                                       include_reused_nodes: bool = False) -> List[BaseNode]:
         """
         Get a list of nodes that can be configured (namely, has one or
         more qc candidate and their weights or activations should be quantized).
         The nodes are sorted according to the topological order of the graph.
 
         Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
             include_reused_nodes: Whether or not to include reused nodes (False by default).
 
         Returns:
              A list of nodes that can be configured (namely, has one or more qc candidate) sorted topology.
 
         """
-        weights_configurable_nodes = self.get_weights_configurable_nodes(include_reused_nodes)
+        weights_configurable_nodes = self.get_weights_configurable_nodes(fw_info, include_reused_nodes)
         activation_configurable_nodes = self.get_activation_configurable_nodes()
 
         # combine and remove duplications
         configurable_nodes = list(set(weights_configurable_nodes + activation_configurable_nodes))
 
         return self._sort_nodes_in_list(configurable_nodes)
 
@@ -612,59 +643,70 @@
         sorted_configurable_nodes = []
         sorted_nodes = list(topological_sort(self))
         for n in sorted_nodes:
             if n in nodes_list:
                 sorted_configurable_nodes.append(n)
         return sorted_configurable_nodes
 
-    def get_min_candidates_config(self) -> List[int]:
+    def get_min_candidates_config(self, fw_info: FrameworkInfo) -> List[int]:
         """
         Builds a minimal configuration.
         Note: we assume that a minimal configuration exists, i.e., each configurable node has exactly one candidate
             with minimal n_bits (in both weight and activation if both are quantized, or in the relevant one if only
             one of them is quantized)
 
+        Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
+
         Returns: A list of candidate for each node (list on indices)
         """
 
-        conf_sorted_nodes = self.get_configurable_sorted_nodes()
+        conf_sorted_nodes = self.get_configurable_sorted_nodes(fw_info)
         min_cfg_candidates = [n.find_min_candidates_indices() for n in conf_sorted_nodes]  # list of lists of indices
 
         assert all([len(lst) == 1 for lst in min_cfg_candidates]), \
             f"A minimal config candidate must be defined, but some node have multiple potential minimal candidates"
 
         return [lst[0] for lst in min_cfg_candidates]
 
-    def get_max_candidates_config(self) -> List[int]:
+    def get_max_candidates_config(self, fw_info: FrameworkInfo) -> List[int]:
         """
         Builds a maximal configuration.
         Note: we assume that a maximal configuration exists, i.e., each configurable node has exactly one candidate
             with maximal n_bits (in both weight and activation if both are quantized, or in the relevant one if only
             one of them is quantized)
 
+        Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
+
         Returns: A list of candidate for each node (list on indices)
         """
 
-        conf_sorted_nodes = self.get_configurable_sorted_nodes()
+        conf_sorted_nodes = self.get_configurable_sorted_nodes(fw_info)
         max_cfg_candidates = [n.find_max_candidates_indices() for n in conf_sorted_nodes]  # list of lists of indices
 
         assert all([len(lst) == 1 for lst in max_cfg_candidates]), \
             f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
 
         return [lst[0] for lst in max_cfg_candidates]
 
-    def get_final_weights_config(self) -> List[Tuple[BaseNode, int]]:
+    def get_final_weights_config(self, fw_info: FrameworkInfo) -> List[Tuple[BaseNode, int]]:
         """
         Gets the final number of bits for quantization of each weights' configurable layer.
 
+        Args:
+            fw_info: fw_info: FrameworkInfo object with information about the specific framework's model.
+
         Returns: A list of pairs of (node type, node's weights quantization bitwidth).
 
         """
-        sorted_conf_weights = self.get_sorted_weights_configurable_nodes()
-        return [(n, n.final_weights_quantization_cfg.weights_n_bits) for n in sorted_conf_weights]
+        sorted_conf_weights = self.get_sorted_weights_configurable_nodes(fw_info)
+        # a configurable node by definition has a kernel op
+        return [(n, n.final_weights_quantization_cfg.get_attr_config(self.fw_info.get_kernel_op_attributes(n.type)[0]).weights_n_bits)
+                for n in sorted_conf_weights]
 
     def get_final_activation_config(self) -> List[Tuple[BaseNode, int]]:
         """
         Gets the final number of bits for quantization of each activation configurable layer.
 
         Returns: A list of pairs of (node type, nod's activation quantization bitwidth).
 
@@ -686,7 +728,134 @@
         """
         Checks whether all nodes in the graph that have activation quantization are quantized with the same bit-width.
 
         Returns: True if all quantization config candidates of all nodes have the same activation quantization bit-width.
 
         """
         return all([n.is_all_activation_candidates_equal() for n in self.nodes])
+
+    def replace_node(self, node_to_replace: BaseNode, new_node: BaseNode):
+        """
+        Replaces a node in the graph with a new node.
+
+        Args:
+            node_to_replace: The node to replace.
+            new_node: The new node to replace with.
+
+        """
+        self.add_node(new_node)
+        self.reconnect_out_edges(node_to_replace, new_node)
+        self.reconnect_in_edges(node_to_replace, new_node)
+        self.replace_output_node(node_to_replace, new_node)
+        self.replace_input_node(node_to_replace, new_node)
+        self.remove_node(node_to_replace)
+
+    def get_pruning_sections(self,
+                             fw_impl: Any) -> List[PruningSection]:
+        """
+        Constructs pruning sections for a given computational graph.
+        Each section is created starting from an entry node and includes intermediate and exit nodes.
+
+        Args:
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: List of PruningSection in the graph.
+        """
+        entry_nodes = self.get_pruning_sections_entry_nodes(fw_impl)
+        return [self._create_pruning_section(entry_node,  fw_impl) for entry_node in entry_nodes]
+
+    def get_pruning_sections_entry_nodes(self, fw_impl: Any) -> List[BaseNode]:
+        """
+        Identifies entry nodes for pruning sections within the graph.
+        Traverses the graph in a topological order, checking each node for prunability criteria.
+        Returns a list of nodes that mark the beginning of a prunable section in the graph.
+
+        Args:
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: List of nodes that are entry nodes in the pruning sections of the graph.
+
+        """
+        prunable_nodes = []
+        for n in list(topological_sort(self)):
+            if fw_impl.is_node_entry_node(n) and self._is_node_topology_prunable(n, fw_impl):
+                prunable_nodes.append(n)
+        return prunable_nodes
+
+    def _is_node_topology_prunable(self, entry_node: BaseNode, fw_impl: Any) -> bool:
+        """
+        Determines if the topology starting from a given entry node is suitable for pruning.
+        Iteratively examines the graph structure, focusing on node connectivity and pruning criteria.
+        Returns True if the topology is prunable, False otherwise.
+
+        Args:
+            entry_node (BaseNode): The node to start the topology check from.
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: Whether this node is a start of a pruning section according to the graph topology or not.
+
+        """
+        next_node = entry_node
+
+        # Continue iterating until the conditions for prunability are no longer met
+        while len(self.out_edges(next_node)) == 1:
+            next_node = self.out_edges(next_node)[0].sink_node
+
+            # If next_node is an exit node and has only one incoming edge, the topology is prunable.
+            if fw_impl.is_node_exit_node(next_node, entry_node, self.fw_info) and len(self.in_edges(next_node)) == 1:
+                return True
+
+            # If the next node is not an intermediate node or has more than one incoming/outgoing edge,
+            # stop the check.
+            if not fw_impl.is_node_intermediate_pruning_section(next_node) or len(self.in_edges(next_node)) != 1 or len(self.out_edges(next_node)) != 1:
+                return False
+
+        # If the loop exits normally, it implies that the topology is not prunable
+        return False
+
+
+    def _create_pruning_section(self, entry_node: BaseNode, fw_impl: Any) -> PruningSection:
+        """
+        Creates a PruningSection object starting from a given entry node.
+        Includes logic to find intermediate and exit nodes to complete the section.
+        Ensures the provided entry node is a valid starting point for pruning.
+
+        Args:
+            entry_node (BaseNode): The entry node to create the section it starts.
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: The pruning section that starts with node entry_node.
+
+        """
+        if not fw_impl.is_node_entry_node(entry_node):
+            Logger.critical(f"Node {entry_node} is not a valid entry node for creating a pruning section")
+
+        intermediate_nodes, exit_node = self._find_intermediate_and_exit_nodes(entry_node, fw_impl)
+
+        if not fw_impl.is_node_exit_node(exit_node, entry_node, self.fw_info):
+            Logger.critical(f"Node {exit_node} is not a valid exit node for the pruning section starting with {entry_node}.")
+
+        return PruningSection(entry_node=entry_node,
+                              intermediate_nodes=intermediate_nodes,
+                              exit_node=exit_node)
+
+    def _find_intermediate_and_exit_nodes(self, entry_node: BaseNode, fw_impl: Any) -> Tuple[List[BaseNode], BaseNode]:
+        """
+        Identifies intermediate and exit nodes for a pruning section starting from an entry node.
+        Iterates through connected nodes to build the complete structure of the pruning section.
+
+        Args:
+            entry_node (BaseNode): An entry node to find the intermediate and exit nodes of its section.
+            fw_impl (PruningFrameworkImplementation): Implementation of specific framework methods required for pruning.
+
+        Returns: A tuple containing a list of intermediate nodes and the exit node.
+
+        """
+        intermediate_nodes = []
+        next_node = self.out_edges(entry_node)[0].sink_node
+        while not fw_impl.is_node_exit_node(next_node, entry_node, self.fw_info):
+            intermediate_nodes.append(next_node)
+            next_node = self.out_edges(next_node)[0].sink_node
+
+        return intermediate_nodes, next_node
+
+
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/base_node.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/base_node.py`

 * *Files 25% similar despite different names*

```diff
@@ -10,20 +10,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import copy
-from typing import Dict, Any, Tuple, List
+from typing import Dict, Any, Tuple, List, Type
 
 import numpy as np
 
 from model_compression_toolkit.constants import WEIGHTS_NBITS_ATTRIBUTE, CORRECTED_BIAS_ATTRIBUTE, \
-    ACTIVATION_NBITS_ATTRIBUTE
+    ACTIVATION_NBITS_ATTRIBUTE, FP32_BYTES_PER_PARAMETER
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationConfigOptions, \
     TargetPlatformCapabilities, LayerFilterParams
 
 
 class BaseNode:
     """
@@ -36,30 +37,34 @@
                  input_shape: Tuple[Any],
                  output_shape: Tuple[Any],
                  weights: Dict[str, np.ndarray],
                  layer_class: type,
                  reuse: bool = False,
                  reuse_group: str = None,
                  quantization_attr: Dict[str, Any] = None,
-                 has_activation: bool = True
+                 has_activation: bool = True,
+                 is_custom: bool = False
                  ):
         """
         Init a Node object.
 
         Args:
             name: Node's name
             framework_attr: Framework attributes the layer had which the node holds.
             input_shape: Input tensor shape of the node.
             output_shape: Input tensor shape of the node.
             weights: Dictionary from a variable name to the weights with that name in the layer the node represents.
+                     Constant inputs to a node are also saved in the weights (AKA positional weights) dictionary and
+                     their key is their position (an integer) in the node's call_args.
             layer_class: Class path of the layer this node represents.
             reuse: Whether this node was duplicated and represents a reused layer.
             reuse_group: Name of group of nodes from the same reused layer.
             quantization_attr: Attributes the node holds regarding how it should be quantized.
             has_activation: Whether the node has activations that we might want to quantize.
+            is_custom: Whether the node is custom layer or not.
         """
         self.name = name
         self.framework_attr = framework_attr
         self.quantization_attr = quantization_attr if quantization_attr is not None else dict()
         self.input_shape = input_shape
         self.output_shape = output_shape
         self.weights = weights
@@ -67,20 +72,22 @@
         self.reuse = reuse
         self.reuse_group = reuse_group
         self.final_weights_quantization_cfg = None
         self.final_activation_quantization_cfg = None
         self.candidates_quantization_cfg = None
         self.prior_info = None
         self.has_activation = has_activation
+        self.is_custom = is_custom
 
     @property
     def type(self):
         """
         A function to get the node's layer_class op for convenient comparison
-        :return: the node's layer_class
+        Returns:
+            the node's layer_class
         """
         return self.layer_class
 
     def get_has_activation(self):
         """
         Returns has_activation attribute.
 
@@ -100,37 +107,54 @@
             return self.final_activation_quantization_cfg.enable_activation_quantization
 
         for qc in self.candidates_quantization_cfg:
             assert self.candidates_quantization_cfg[0].activation_quantization_cfg.enable_activation_quantization == \
                    qc.activation_quantization_cfg.enable_activation_quantization
         return self.candidates_quantization_cfg[0].activation_quantization_cfg.enable_activation_quantization
 
-    def is_weights_quantization_enabled(self) -> bool:
+    def is_weights_quantization_enabled(self, attr_name: str) -> bool:
         """
+        Checks whether a node's weights attribute quantization is enabled.
+
+        Args:
+            attr_name: An attribute to check if its quantization is enabled.
 
         Returns: Whether node weights quantization is enabled or not.
 
         """
         if self.final_weights_quantization_cfg:
             # if we have a final configuration, then we only care to check if it enables weights quantization
-            return self.final_weights_quantization_cfg.enable_weights_quantization
+            return self.final_weights_quantization_cfg.get_attr_config(attr_name).enable_weights_quantization
 
-        for qc in self.candidates_quantization_cfg:
-            assert self.candidates_quantization_cfg[0].weights_quantization_cfg.enable_weights_quantization == \
-                   qc.weights_quantization_cfg.enable_weights_quantization
-        return self.candidates_quantization_cfg[0].weights_quantization_cfg.enable_weights_quantization
+        attr_candidates = self.get_all_weights_attr_candidates(attr_name)
+        candidates_enable_quantization = [c.enable_weights_quantization for c in attr_candidates]
+        if len(candidates_enable_quantization) > 0 and len(set(candidates_enable_quantization)) > 1:
+            Logger.error(f"Weights attribute {attr_name} in node {self.name} has multiple quantization candidates "
+                         f"configuration with incompatible values.")
+        if all(candidates_enable_quantization):
+            return True
+
+        return False
 
     def __repr__(self):
         """
 
         Returns: String that represents the node.
 
         """
         return f'{self.type.__name__}:{self.name}'
 
+    def is_reused(self) -> bool:
+        """
+        Check whether the node is reused or not
+        Returns:
+            True if node is reused, else False
+        """
+        return self.reuse or self.reuse_group is not None
+
     def get_weights_by_keys(self, name: str) -> np.ndarray:
         """
         Get a node's weight by its name.
         Args:
             name: Name of the variable for a node's weight.
 
         Returns:
@@ -161,18 +185,43 @@
         else:  # Add if not exist
             self.weights[name] = tensor
             self.weights_keys = list(self.weights.keys())  # update keys
 
     def get_weights_list(self):
         """
 
-        Returns: A list of all weights the node holds.
+        Returns: A list of all non-positional weights the node holds.
 
         """
-        return [self.weights[k] for k in self.weights.keys() if self.weights[k] is not None]
+        return [self.weights[k] for k in self.weights.keys() if not isinstance(k, int)]
+
+    def get_node_weights_attributes(self) -> List[str]:
+        """
+
+        Returns: A list of all weights attributes that the node holds.
+
+        """
+        return list(self.weights.keys())
+
+    def insert_positional_weights_to_input_list(self, input_tensors: List) -> List:
+        """
+        Insert node's positional weights to input tensors list. The positional weights are inserted
+        in the node's list of inputs according to their keys in the weights dictionary.
+
+        Args:
+            input_tensors: activation input tensors to node.
+        Returns:
+            Activation input tensors list with positional weights
+        """
+        for pos, weight in sorted((pos, weight) for pos, weight in self.weights.items()
+                                  if isinstance(pos, int)):
+            assert pos <= len(input_tensors), 'Positional weight index mismatch'
+            input_tensors.insert(pos, weight)
+
+        return input_tensors
 
     def get_num_parameters(self, fw_info) -> Tuple[int,int]:
         """
         Compute the number of parameters the node holds.
         It returns a tuple: Number of quantized parameters, number of float parameters.
 
         Args:
@@ -204,116 +253,155 @@
 
         Args:
             fw_info: Framework info to decide which attributes should be quantized.
 
         Returns: Number of bytes the node's memory requires.
 
         """
+        # TODO: this method is used for tensorboard only. If we want to enable logging of other attributes memory
+        #  then it needs to be modified. But, it might be better to remove this method from the BaseNode completely.
+        kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+        if kernel_attr is None:
+            return 0
         q_params, f_params = self.get_num_parameters(fw_info)
         if self.final_weights_quantization_cfg is None:  # float coefficients
-            memory = (f_params+q_params) * 4
+            memory = (f_params+q_params) * FP32_BYTES_PER_PARAMETER
         else:
-            memory = (f_params*4)+ (q_params * self.final_weights_quantization_cfg.weights_n_bits / 8)  # in bytes
+            memory = ((f_params * FP32_BYTES_PER_PARAMETER) +
+                      (q_params * self.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+                       / 8))  # in bytes
 
         return memory
 
     def get_float_memory_bytes(self, fw_info) -> float:
         """
         Compute the number of bytes the node's memory requires.
 
         Args:
             fw_info: Framework info to decide which attributes should be quantized.
 
         Returns: Number of bytes the node's memory requires when in floating point (32 bit).
 
         """
         q_params, f_params = self.get_num_parameters(fw_info)
-        return (f_params + q_params) * 32 / 8 # in bytes
+        return (f_params + q_params) * FP32_BYTES_PER_PARAMETER
 
-    def get_unified_weights_candidates_dict(self):
+    def get_unified_weights_candidates_dict(self, fw_info) -> Dict[str, Any]:
         """
-        In Mixed-Precision, a node can have multiple candidates for weights quantization configuration.
+        In Mixed-Precision, a node's kernel can have multiple candidates for weights quantization configuration.
         In order to display a single view of a node (for example, for logging in TensorBoard) we need a way
         to create a single dictionary from all candidates.
         This method is aimed to build such an unified dictionary for a node.
 
+        Args:
+            fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
+
         Returns: A dictionary containing information from node's weight quantization configuration candidates.
 
         """
-        shared_attributes = [CORRECTED_BIAS_ATTRIBUTE, WEIGHTS_NBITS_ATTRIBUTE]
-        attr = dict()
-        if self.is_weights_quantization_enabled():
-            attr = copy.deepcopy(self.candidates_quantization_cfg[0].weights_quantization_cfg.__dict__)
-            for shared_attr in shared_attributes:
-                if shared_attr in attr:
-                    unified_attr = []
-                    for candidate in self.candidates_quantization_cfg:
-                        unified_attr.append(getattr(candidate.weights_quantization_cfg, shared_attr))
-                    attr[shared_attr] = unified_attr
-        return attr
+        shared_parameters = [CORRECTED_BIAS_ATTRIBUTE, WEIGHTS_NBITS_ATTRIBUTE]
+        parameters_dict = dict()
+        # We assume that only the kernel attribute have more than one candidate, since we only allow to
+        # quantize the kernel using mixed precision
+        # TODO: need to modify if we want to present a unified config for other attributes
+        kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+        if kernel_attr is None:
+            # This node doesn't have a kernel attribute
+            return {}
+
+        if self.is_weights_quantization_enabled(kernel_attr):
+            parameters_dict = copy.deepcopy(self.candidates_quantization_cfg[0].weights_quantization_cfg.
+                                            get_attr_config(kernel_attr).__dict__)
+            for shared_parameter in shared_parameters:
+                if shared_parameter in parameters_dict:
+                    unified_param = []
+                    attr_candidates = self.get_all_weights_attr_candidates(kernel_attr)
+                    for attr_candidate in attr_candidates:
+                        unified_param.append(getattr(attr_candidate, shared_parameter))
+                    parameters_dict[shared_parameter] = unified_param
+        return parameters_dict
 
-    def get_unified_activation_candidates_dict(self):
+    def get_unified_activation_candidates_dict(self) -> Dict[str, Any]:
         """
         In Mixed-Precision, a node can have multiple candidates for activation quantization configuration.
         In order to display a single view of a node (for example, for logging in TensorBoard) we need a way
         to create a single dictionary from all candidates.
         This method is aimed to build such an unified dictionary for a node.
 
         Returns: A dictionary containing information from node's activation quantization configuration candidates.
 
         """
         shared_attributes = [ACTIVATION_NBITS_ATTRIBUTE]
         attr = dict()
-        if self.is_weights_quantization_enabled():
+        if self.is_activation_quantization_enabled():
             attr = copy.deepcopy(self.candidates_quantization_cfg[0].activation_quantization_cfg.__dict__)
             for shared_attr in shared_attributes:
                 if shared_attr in attr:
                     unified_attr = []
                     for candidate in self.candidates_quantization_cfg:
                         unified_attr.append(getattr(candidate.activation_quantization_cfg, shared_attr))
                     attr[shared_attr] = unified_attr
         return attr
 
-    def is_all_activation_candidates_equal(self):
+    def is_all_activation_candidates_equal(self) -> bool:
         """
         Checks whether all candidates' quantization configuration have the same activation configuration,
         using the self-implemented __eq__ method of class NodeActivationQuantizationConfig.
 
         Returns: True if all candidates have same activation configuration, False otherwise.
 
         """
         return all(candidate.activation_quantization_cfg ==
                    self.candidates_quantization_cfg[0].activation_quantization_cfg
                    for candidate in self.candidates_quantization_cfg)
 
-    def is_all_weights_candidates_equal(self):
+    def is_all_weights_candidates_equal(self, attr: str) -> bool:
         """
-        Checks whether all candidates' quantization configuration have the same weights configuration,
+        Checks whether all candidates' quantization configuration of a given weights attribute
+        have the same weights configuration,
         using the self-implemented __eq__ method of class NodeWeightsQuantizationConfig.
 
-        Returns: True if all candidates have same weights configuration, False otherwise.
+        Args:
+            attr: The attribute name to check if all its quantization configuration candidates are equal.
+
+        Returns: True if all the weights attribute candidates have same configuration, False otherwise.
 
         """
-        return all(candidate.weights_quantization_cfg ==
-                   self.candidates_quantization_cfg[0].weights_quantization_cfg
-                   for candidate in self.candidates_quantization_cfg)
+        # note that if the given attribute name does not exist in the node's attributes mapping,
+        # the inner method would log an exception.
+        return all(attr_candidate ==
+                   self.candidates_quantization_cfg[0].weights_quantization_cfg.get_attr_config(attr)
+                   for attr_candidate in self.get_all_weights_attr_candidates(attr))
 
-    def has_weights_to_quantize(self, fw_info):
+    def has_kernel_weight_to_quantize(self, fw_info):
         """
-        Checks whether the node has weights that need to be quantized according to the framework info.
+        Checks whether the node has kernel attribute that need to be quantized according to the framework info.
+
         Args:
             fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
+
         Returns: Whether the node has weights that need to be quantized.
         """
         attrs = fw_info.get_kernel_op_attributes(self.type)
         for attr in attrs:
             if attr and self.get_weights_by_keys(attr) is not None:
                 return True
         return False
 
+    def has_any_weight_attr_to_quantize(self) -> bool:
+        """
+        Checks whether the node has any weights attribute that is supposed to be quantized, based on its provided
+        quantization configuration candidates.
+
+        Returns: True if the is at least one weights attribute in the node that is supposed to be quantized.
+
+        """
+
+        return any([self.is_weights_quantization_enabled(attr) for attr in self.get_node_weights_attributes()])
+
     def get_total_output_params(self) -> float:
         """
         Calculates the output size of the node.
 
         Returns: Output size.
         """
         output_shapes = self.output_shape if isinstance(self.output_shape, List) else [self.output_shape]
@@ -337,15 +425,15 @@
 
         return sum([np.prod([x for x in input_shape if x is not None]) for input_shape in input_shapes])
 
     def find_min_candidates_indices(self) -> List[int]:
         """
         Returns a list with potential minimal candidates.
         A potential minimal candidate is a candidate which its weights_n_bits and activation_n_bits pair is
-        on the Pareto Front, i.e., there is no other candidates that its n_bits pair exceeds in both entries.
+        on the Pareto Front, i.e., there is no other candidate that its n_bits pair exceeds in both entries.
 
         Returns: A list of indices of potential minimal candidates.
 
         """
 
         # We assume that the candidates are sorted according to weights_n_bits first and activation_n_bits second
         # First, we add the last candidate to the set of minimal candidates (candidate, index)
@@ -377,28 +465,37 @@
         # Iterate over all other candidates, and add ones with higher weights_n_bits but smaller activation_n_bits
         for i, c in enumerate(self.candidates_quantization_cfg):
             if c.activation_quantization_cfg.activation_n_bits > first_max[1]:
                 max_candidates.append((i, c))
 
         return [i for i, a_n_bits in max_candidates]
 
-    def get_unique_weights_candidates(self) -> List[Any]:
+    def get_unique_weights_candidates(self, attr: str) -> List[Any]:
         """
-        Returns a list with node's candidates of unique weights bit-width value.
-        If the node have multiple candidates with the same weights bit-width,
+        Returns a list with node's candidates of unique weights bit-width value for the given attribute.
+        If the node have multiple candidates with the same weights bit-width for this attribute,
         the first candidate in the list is returned.
 
-        Returns: A list with node's candidates of unique weights bit-width value.
+        Args:
+            attr: A weights attribute name to get its unique candidates list.
+
+        Returns: A list with node's candidates of unique weights bit-width value for the given attribute.
         """
 
+        if attr is None or len(self.get_all_weights_attr_candidates(attr)) == 0:
+            Logger.warning(f"Trying to retrieve quantization configuration candidates for attribute '{attr}', "
+                           f"but such attribute can't be found in node {self.name}."
+                           f"An empty list of candidates is returned.")
+            return []
+
         unique_candidates = copy.deepcopy(self.candidates_quantization_cfg)
         seen_candidates = set()
         unique_candidates = [candidate for candidate in unique_candidates if
-                             candidate.weights_quantization_cfg not in seen_candidates
-                             and not seen_candidates.add(candidate.weights_quantization_cfg)]
+                             candidate.weights_quantization_cfg.get_attr_config(attr) not in seen_candidates
+                             and not seen_candidates.add(candidate.weights_quantization_cfg.get_attr_config(attr))]
         return unique_candidates
 
     def get_unique_activation_candidates(self) -> List[Any]:
         """
         Returns a list with node's candidates of unique activation bit-width value.
         If the node have multiple candidates with the same activation bit-width,
         the first candidate in the list is returned.
@@ -409,70 +506,90 @@
         unique_candidates = copy.deepcopy(self.candidates_quantization_cfg)
         seen_candidates = set()
         unique_candidates = [candidate for candidate in unique_candidates if
                              candidate.activation_quantization_cfg not in seen_candidates
                              and not seen_candidates.add(candidate.activation_quantization_cfg)]
         return unique_candidates
 
-    def has_weights_quantization_enabled_candidate(self) -> bool:
-        """
-        Checks whether the node has quantization configuration candidates that enable weights quantization.
-
-        Returns: True if the node has at list one quantization configuration candidate with weights quantization enabled.
-        """
-
-        return len(self.candidates_quantization_cfg) > 0 and \
-               any([c.weights_quantization_cfg.enable_weights_quantization for c in self.candidates_quantization_cfg])
-
     def has_activation_quantization_enabled_candidate(self) -> bool:
         """
         Checks whether the node has quantization configuration candidates that enable activation quantization.
 
         Returns: True if the node has at list one quantization configuration candidate with activation quantization enabled.
         """
 
         return len(self.candidates_quantization_cfg) > 0 and \
                any([c.activation_quantization_cfg.enable_activation_quantization for c in self.candidates_quantization_cfg])
 
+    def get_all_weights_attr_candidates(self, attr: str) -> List[WeightsAttrQuantizationConfig]:
+        """
+        Returns all WeightsAttrQuantizationConfig configuration of the given attribute of the node.
+
+        Args:
+            attr: The attribute name to get its configurations.
+
+        Returns: A list of the attribute's quantization configurations.
+
+        """
+        # note that if the given attribute name does not exist in the node's attributes mapping,
+        # the inner method would log an exception.
+        return [c.weights_quantization_cfg.get_attr_config(attr) for c in self.candidates_quantization_cfg]
+
     def get_qco(self, tpc: TargetPlatformCapabilities) -> QuantizationConfigOptions:
         """
         Get the QuantizationConfigOptions of the node according
         to the mappings from layers/LayerFilterParams to the OperatorsSet in the TargetPlatformModel.
 
         Args:
             tpc: TPC to extract the QuantizationConfigOptions for the node
 
         Returns:
             QuantizationConfigOptions of the node.
         """
 
         if tpc is None:
-            Logger.error(f'Can not retrieve QC options for None TPC')  # pragma: no cover
+            Logger.critical(f'Can not retrieve QC options for None TPC')  # pragma: no cover
 
         for fl, qco in tpc.filterlayer2qco.items():
             if self.is_match_filter_params(fl):
                 return qco
         if self.type in tpc.layer2qco:
             return tpc.layer2qco.get(self.type)
         return tpc.tp_model.default_qco
 
+    def is_match_type(self, _type: Type) -> bool:
+        """
+        Check if input type matches the node type, either in instance type or in type name. Checking the
+        name string is required because of function types changes that occurred in TF 2.15.
+
+        Args:
+            _type: other node type
+        Returns:
+            Whether _type matches the self node type
+
+        """
+        return _type == self.type or _type.__name__ == self.type.__name__
 
     def is_match_filter_params(self, layer_filter_params: LayerFilterParams) -> bool:
         """
         Check if the node matches a LayerFilterParams according to its
         layer, conditions and keyword-arguments.
 
         Args:
             layer_filter_params: LayerFilterParams to check if the node matches its properties.
 
         Returns:
             Whether the node matches to the LayerFilterParams properties.
         """
+        # check if provided argument is of type LayerFilterParams
+        if not isinstance(layer_filter_params, LayerFilterParams):
+            return False
+
         # Check the node has the same type as the layer in LayerFilterParams
-        if layer_filter_params.layer != self.type:
+        if not self.is_match_type(layer_filter_params.layer):
             return False
 
         # Get attributes from node to filter
         layer_config = self.framework_attr
         if hasattr(self, "op_call_kwargs"):
             layer_config.update(self.op_call_kwargs)
 
@@ -480,8 +597,52 @@
             if layer_config.get(attr) != value:
                 return False
 
         for c in layer_filter_params.conditions:
             if not c.match(layer_config):
                 return False
 
-        return True
+        return True
+
+    def get_simd(self) -> int:
+        """
+        Retrieves the SIMD size used for this node. It collects the SIMD sizes from all candidate
+        configurations and returns the minimum SIMD size.
+
+        Returns:
+            int: The node's SIMD size.
+
+        """
+        simd_list = [qc.weights_quantization_cfg.simd_size for qc in self.candidates_quantization_cfg]
+        if len(simd_list) > 1:
+            Logger.warning(f"More than one pruning SIMD option is available."
+                           f" Min SIMD is used: {min(simd_list)}")
+        if len(simd_list) == 0:
+            Logger.critical(f"No SIMD option is available for {self}")
+        _simd = min(simd_list)
+        if _simd <= 0 or int(_simd) != _simd:
+            Logger.critical(f"SIMD is expected to be a non-positive integer but found: {_simd}")
+        return _simd
+
+    def sort_node_candidates(self, fw_info):
+        """
+        Sorts the node candidates.
+        We assume that the candidates are ordered in the following way (for mixed precision purposes):
+            - If the node has a kernel attribute, then we use the kernel weights number of bits to sort the candidates
+            (in descending order). We use the candidate activation number of bits as a secondary order.
+            - If the node doesn't have a kernel we only consider the candidate activation number of bits to sort
+            the candidates in descending order.
+        The operation is done inplace.
+
+        Args:
+            fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
+
+        """
+        if self.candidates_quantization_cfg is not None:
+            kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+            if kernel_attr is not None:
+                self.candidates_quantization_cfg.sort(
+                    key=lambda c: (c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits,
+                                   c.activation_quantization_cfg.activation_n_bits), reverse=True)
+            else:
+                self.candidates_quantization_cfg.sort(key=lambda c: c.activation_quantization_cfg.activation_n_bits,
+                                                      reverse=True)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/edge.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/edge.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 from typing import Any, Dict
 
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
+from model_compression_toolkit.logger import Logger
 
 # Edge attributes:
 EDGE_SOURCE_INDEX = 'source_index'
 EDGE_SINK_INDEX = 'sink_index'
 
 
 class Edge(object):
@@ -104,8 +105,8 @@
                     dst_node,
                     edge_data[EDGE_SOURCE_INDEX],
                     edge_data[EDGE_SINK_INDEX])
 
     elif isinstance(edge, Edge):  # it's already an Edge and no change need to be done
         return edge
 
-    raise Exception('Edges list contains an object that is not a known edge format.')
+    Logger.critical('Edge conversion failed: unrecognized edge format encountered.')
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/functional_node.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/functional_node.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,22 +12,23 @@
     def __init__(self,
                  name: str,
                  framework_attr: Dict[str, Any],
                  input_shape: Tuple[Any],
                  output_shape: Tuple[Any],
                  weights: Dict[str, np.ndarray],
                  layer_class: type,
-                 op_call_args: List[Any] = None,
+                 op_call_args: Tuple[Any] = None,
                  op_call_kwargs: Dict[str, Any] = None,
                  reuse: bool = False,
                  reuse_group: str = None,
                  quantization_attr: Dict[str, Any] = None,
                  functional_op: Any = None,
                  inputs_as_list: bool = False,
-                 has_activation: bool = True):
+                 has_activation: bool = True,
+                 tensor_input_indices = None):
         """
         Init a FunctionalNode object.
 
         Args:
             name: Node's name
             framework_attr: Framework attributes the layer had which the node holds.
             input_shape: Input tensor shape of the node.
@@ -38,14 +39,15 @@
             op_call_kwargs: Key-Word Arguments dictionary with values to pass when calling the layer.
             reuse: Whether this node was duplicated and represents a reused layer.
             reuse_group: Name of group of nodes from the same reused layer.
             quantization_attr: Attributes the node holds regarding how it should be quantized.
             functional_op: The op the node implements.
             inputs_as_list: Whether to pass the node its input tensors as a list or not when calling the layer.
             has_activation: Whether the node has activations that we might want to quantize.
+            tensor_input_indices: A list of indices for activation tensors in the node's input tensor list
 
         """
 
         super().__init__(name,
                          framework_attr,
                          input_shape,
                          output_shape,
@@ -56,14 +58,15 @@
                          quantization_attr,
                          has_activation=has_activation)
 
         self.op_call_kwargs = op_call_kwargs
         self.op_call_args = op_call_args
         self.functional_op = functional_op
         self.inputs_as_list = inputs_as_list
+        self.tensor_input_indices = [] if tensor_input_indices is None else tensor_input_indices
 
     @property
     def type(self):
         """
         A function to get the node's function op for convenient comparison (instead of the layer_class)
         :return: the node's functional_op
         """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/graph_matchers.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/graph_matchers.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,27 +31,27 @@
 
         Args:
             operation: Which layer to check if matches.
         """
 
         self.operation = operation
 
-    def apply(self, input_node_object: Any) -> bool:
+    def apply(self, input_node_object: BaseNode) -> bool:
         """
         Check if input_node_object matches the matcher condition.
 
         Args:
             input_node_object: Node object to check the matcher on.
 
         Returns:
             True if input_node_object is the layer the NodeOperationMatcher holds. Otherwise,
             return nothing.
         """
 
-        if input_node_object.type == self.operation:
+        if input_node_object.is_match_type(self.operation):
             return True
 
 
 class NodeFrameworkAttrMatcher(node_matcher.BaseNodeMatcher):
     """
     Class NodeFrameworkAttrMatcher to check if a node's attribute has a specific value.
     """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/graph_searches.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/graph_searches.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py`

 * *Files 5% similar despite different names*

```diff
@@ -71,18 +71,16 @@
         Verifies bipartite correctness of a set of edges to add to the graph.
         If there is an edge in the list that violates the bipartite property - an Exception is raised.
 
         Args:
             edges_list: A list of edges to verify their correction.
         """
         for n1, n2 in edges_list:
-            if n1 in self.a_nodes and n2 in self.a_nodes:
-                Logger.critical(f"Can't add an edge {(n1, n2)} between two nodes in size A of a bipartite graph.")
-            if n1 in self.b_nodes and n2 in self.b_nodes:
-                Logger.critical(f"Can't add an edge {(n1, n2)} between two nodes in size B of a bipartite graph.")
+            if (n1 in self.a_nodes and n2 in self.a_nodes) or (n1 in self.b_nodes and n2 in self.b_nodes):
+                Logger.critical(f"Attempted to add edge {(n1, n2)} between nodes of the same partition in a bipartite graph, violating bipartite properties.")
 
     def add_nodes_to_a(self, new_nodes: List[Any]):
         """
         Add a set of nodes to side A of the bipartite graph.
 
         Args:
             new_nodes: New nodes to add to side A.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/cut.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/cut.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,14 +20,15 @@
     VIRTUAL_WEIGHTS_SUFFIX, VIRTUAL_ACTIVATION_SUFFIX, FLOAT_BITWIDTH
 
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 import numpy as np
 
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
+from model_compression_toolkit.logger import Logger
 
 
 class VirtualSplitNode(BaseNode):
     """
     A class that represents a node that was split from a kernel node (node with weights).
     """
 
@@ -56,27 +57,28 @@
 class VirtualSplitWeightsNode(VirtualSplitNode):
     """
     A class that represents a node that was split from a kernel node (node with weights) and holds the weights of
     the original node. This node contains the original node's weights and the relevant weights candidate quantization
     config.
     """
 
-    def __init__(self, origin_node: BaseNode):
+    def __init__(self, origin_node: BaseNode, kernel_attr: str):
         """
         Init a VirtualSplitWeightsNode object.
 
         Args:
             origin_node: The original node from which the new node was split.
+            kernel_attr: The name of the kernel attribute of the original node.
         """
 
         super().__init__(origin_node)
 
         self.name = origin_node.name + VIRTUAL_WEIGHTS_SUFFIX
 
-        self.candidates_quantization_cfg = origin_node.get_unique_weights_candidates()
+        self.candidates_quantization_cfg = origin_node.get_unique_weights_candidates(kernel_attr)
         for c in self.candidates_quantization_cfg:
             c.activation_quantization_cfg.enable_activation_quantization = False
             c.activation_quantization_cfg.activation_n_bits = FLOAT_BITWIDTH
 
 
 class VirtualSplitActivationNode(VirtualSplitNode):
     """
@@ -107,33 +109,34 @@
             c.weights_quantization_cfg.enable_weights_quantization = False
             c.weights_quantization_cfg.weights_n_bits = FLOAT_BITWIDTH
 
 
 class VirtualActivationWeightsNode(BaseNode):
     """
     A node that represents a composition of pair of sequential activation node and weights (kernel) node.
-    This structure is used for mixed-precision search with bit-operation KPI.
+    This structure is used for mixed-precision search with bit-operation constraint.
     The node's candidates are the cartesian product of both nodes' candidates.
 
     Important: note that not like regular BaseNode or FunctionalNode, in VirtualActivationWeightsNode the activation
     candidates config refer to the quantization config of the activation that precedes the linear operation! instead of
     the output of the linear operation.
     It is ok, since this node is not meant to be used in a graph for creating an actual model, but only a virtual
-    representation of the model's graph only for allowing to compute the bit-operations KPI in mixed-precision.
+    representation of the model's graph only for allowing to compute the bit-operations constraint in mixed-precision.
     """
 
     def __init__(self,
                  act_node: BaseNode,
                  weights_node: BaseNode,
                  name: str,
                  framework_attr: Dict[str, Any],
                  input_shape: Tuple[Any],
                  output_shape: Tuple[Any],
                  weights: Dict[str, np.ndarray],
                  layer_class: type,
+                 fw_info: FrameworkInfo,
                  reuse: bool = False,
                  reuse_group: str = None,
                  quantization_attr: Dict[str, Any] = None,
                  has_activation: bool = True,
                  **kwargs):
         """
         Init a VirtualActivationWeightsNode object.
@@ -143,14 +146,15 @@
             weights_node: The original weights node.
             name: Node's name
             framework_attr: Framework attributes the layer had which the node holds.
             input_shape: Input tensor shape of the node.
             output_shape: Input tensor shape of the node.
             weights: Dictionary from a variable name to the weights with that name in the layer the node represents.
             layer_class: Class path of the layer this node represents.
+            fw_info: A FrameworkInfo object with framework specific information,
             reuse: Whether this node was duplicated and represents a reused layer.
             reuse_group: Name of group of nodes from the same reused layer.
             quantization_attr: Attributes the node holds regarding how it should be quantized.
             has_activation: Whether the node has activations that we might want to quantize.
             **kwargs: Additional arguments that can be passed but are not used (allows to init the object with an
                 existing node's __dict__).
 
@@ -176,15 +180,16 @@
         for c_a in act_node.candidates_quantization_cfg:
             for c_w in weights_node.candidates_quantization_cfg:
                 composed_candidate = CandidateNodeQuantizationConfig(activation_quantization_cfg=c_a.activation_quantization_cfg,
                                                                      weights_quantization_cfg=c_w.weights_quantization_cfg)
                 v_candidates.append(composed_candidate)
 
         # sorting the candidates by weights number of bits first and then by activation number of bits (reversed order)
-        v_candidates.sort(key=lambda c: (c.weights_quantization_cfg.weights_n_bits,
+        kernel_attr = fw_info.get_kernel_op_attributes(self.type)[0]
+        v_candidates.sort(key=lambda c: (c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits,
                                          c.activation_quantization_cfg.activation_n_bits), reverse=True)
 
         self.candidates_quantization_cfg = v_candidates
 
     def get_bops_count(self, fw_impl: Any, fw_info: FrameworkInfo, candidate_idx: int) -> float:
         """
         Computes the composed node's (edge) bit-operation count.
@@ -193,15 +198,17 @@
             fw_impl: A FrameworkImplementation object with framework specific methods.
             fw_info: A FrameworkInfo object with framework specific information,
             candidate_idx: The index of the node's quantization candidate configuration.
 
         Returns: The BOPS count of the composed node.
 
         """
+        kernel_attr = fw_info.get_kernel_op_attributes(self.original_weights_node.type)[0]
         node_mac = fw_impl.get_node_mac_operations(self.original_weights_node, fw_info)
         candidate = self.candidates_quantization_cfg[candidate_idx]
-        weights_bit = candidate.weights_quantization_cfg.weights_n_bits if \
-            candidate.weights_quantization_cfg.enable_weights_quantization else FLOAT_BITWIDTH
+        kernel_attr_cfg = candidate.weights_quantization_cfg.get_attr_config(kernel_attr)
+        weights_bit = kernel_attr_cfg.weights_n_bits if \
+            kernel_attr_cfg.enable_weights_quantization else FLOAT_BITWIDTH
         activation_bit = candidate.activation_quantization_cfg.activation_n_bits if \
             candidate.activation_quantization_cfg.enable_activation_quantization else FLOAT_BITWIDTH
         node_bops = weights_bit * activation_bit * node_mac
         return node_bops
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/base_graph_filter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/base_graph_filter.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/base_matcher.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/base_matcher.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/edge_matcher.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/edge_matcher.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/function.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/function.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/node_matcher.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/node_matcher.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/matchers/walk_matcher.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/matchers/walk_matcher.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/memory_computation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/memory_computation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py`

 * *Files 14% similar despite different names*

```diff
@@ -32,98 +32,118 @@
         mixed_precision_enable: Is mixed precision enabled.
         graph: A prepared for quantization graph to set its bit widths.
         bit_widths_config: MP configuration (a list of indices: one for each node's candidate
         quantization configuration).
 
     """
     if mixed_precision_enable:
-        assert all([len(n.candidates_quantization_cfg) > 0 for n in graph.get_configurable_sorted_nodes()]), \
+        assert all([len(n.candidates_quantization_cfg) > 0
+                    for n in graph.get_configurable_sorted_nodes(graph.fw_info)]), \
             "All configurable nodes in graph should have at least one candidate configuration in mixed precision mode"
 
-        Logger.info(f'Set bit widths from configuration: {bit_widths_config}')
         # Get a list of nodes' names we need to finalize (that they have at least one weight qc candidate).
-        sorted_nodes_names = graph.get_configurable_sorted_nodes_names()
-        for node in graph.nodes:  # set a specific node qc for each node final weights qc
+        sorted_nodes_names = graph.get_configurable_sorted_nodes_names(graph.fw_info)
+
+        for node in graph.nodes:  # set a specific node qc for each node final qc
             # If it's reused, take the configuration that the base node has
             node_name = node.name if not node.reuse else '_'.join(node.name.split('_')[:-2])
             if node_name in sorted_nodes_names:  # only configurable nodes are in this list
                 node_index_in_graph = sorted_nodes_names.index(node_name)
                 _set_node_final_qc(bit_widths_config,
                                    node,
-                                   node_index_in_graph)
-            elif node.is_activation_quantization_enabled():
-                # If we are here, this means that we are in weights-only mixed-precision
-                # (i.e., activations are quantized with fixed bitwidth or not quantized)
-                # and that this node doesn't have weights to quantize
-                assert len(node.candidates_quantization_cfg) > 0, \
-                    "Node need to have at least one quantization configuration in order to quantize its activation"
-                node.final_activation_quantization_cfg = copy.deepcopy(node.candidates_quantization_cfg[0].activation_quantization_cfg)
-            elif node.is_weights_quantization_enabled():
-                # If we are here, this means that we are in activation-only mixed-precision
-                # (i.e., weights are quantized with fixed bitwidth or not quantized)
-                # and that this node doesn't have activations to quantize
-                assert len(node.candidates_quantization_cfg) > 0, \
-                    "Node need to have at least one quantization configuration in order to quantize its activation"
-                node.final_weights_quantization_cfg = copy.deepcopy(node.candidates_quantization_cfg[0].weights_quantization_cfg)
+                                   node_index_in_graph,
+                                   graph.fw_info)
+            else:
+                if node.is_activation_quantization_enabled():
+                    # If we are here, this means that we are in weights-only mixed-precision
+                    # (i.e., activations are quantized with fixed bitwidth or not quantized)
+                    # and that this node doesn't have kernel to quantize
+                    # (since only the kernel is quantized in mixed precision).
+                    assert len(node.candidates_quantization_cfg) > 0, \
+                        "Node need to have at least one quantization configuration in order to quantize its activation"
+                    node.final_activation_quantization_cfg = copy.deepcopy(node.candidates_quantization_cfg[0].activation_quantization_cfg)
+
+                if node.has_any_weight_attr_to_quantize():
+                    # If we are here, this means that we are in activation-only mixed-precision
+                    # (i.e., kernel is quantized with fixed bitwidth or not quantized)
+                    # and that this node doesn't have activations to quantize.
+                    assert len(node.candidates_quantization_cfg) > 0, \
+                        "Node need to have at least one quantization configuration in order to quantize its activation"
+                    node.final_weights_quantization_cfg = (
+                        copy.deepcopy(node.candidates_quantization_cfg[0].weights_quantization_cfg))
 
     # When working in non-mixed-precision mode, there's only one bitwidth, and we simply set the
     # only candidate of the node as its final weight and activation quantization configuration.
     else:
         for n in graph.nodes:
             assert len(n.candidates_quantization_cfg) == 1
             n.final_weights_quantization_cfg = copy.deepcopy(n.candidates_quantization_cfg[0].weights_quantization_cfg)
             n.final_activation_quantization_cfg = copy.deepcopy(n.candidates_quantization_cfg[0].activation_quantization_cfg)
 
     return graph
 
 
 def _get_node_qc_by_bit_widths(node: BaseNode,
                                bit_width_cfg: List[int],
-                               node_index_in_graph: int) -> Any:
+                               node_index_in_graph: int,
+                               fw_info) -> Any:
     """
     Get the node's quantization configuration that
     matches to the bit width index as in the MP configuration bit_width_cfg.
     If it was not found, return None.
 
     Args:
         node: Node to get its quantization configuration candidate.
         bit_width_cfg: Configuration which determines the node's desired bit width.
         node_index_in_graph: Index of the node in the bit_width_cfg.
+        fw_info: Information relevant to a specific framework about how layers should be quantized.
 
     Returns:
         Node quantization configuration if it was found, or None otherwise.
     """
+    # only the weights kernel attribute is quantized in weights mixed precision at the moment
+    kernel_attr = fw_info.get_kernel_op_attributes(node.type)
 
-    if node.is_weights_quantization_enabled() or node.is_activation_quantization_enabled():
+    if node.is_activation_quantization_enabled():
         bit_index_in_cfg = bit_width_cfg[node_index_in_graph]
         qc = node.candidates_quantization_cfg[bit_index_in_cfg]
+
         return qc
 
-    Logger.critical(f'Node {node.name} quantization configuration from configuration file'  # pragma: no cover
-                    f' was not found in candidates configurations.')
+    elif kernel_attr is not None:
+        if node.is_weights_quantization_enabled(kernel_attr[0]):
+            bit_index_in_cfg = bit_width_cfg[node_index_in_graph]
+            qc = node.candidates_quantization_cfg[bit_index_in_cfg]
+
+            return qc
+
+    Logger.critical(f"Quantization configuration for node '{node.name}' not found in candidate configurations.")  # pragma: no cover
 
 
 def _set_node_final_qc(bit_width_cfg: List[int],
                        node: BaseNode,
-                       node_index_in_graph: int):
+                       node_index_in_graph: int,
+                       fw_info):
     """
     Get the node's quantization configuration that
     matches to the bit width index as in the MP configuration bit_width_cfg, and use it to finalize the node's
     weights and activation quantization config.
     If the node quantization config was not found, raise an exception.
 
     Args:
         bit_width_cfg: Configuration which determines the node's desired bit width.
         node: Node to set its node quantization configuration.
         node_index_in_graph: Index of the node in the bit_width_cfg.
+        fw_info: Information relevant to a specific framework about how layers should be quantized.
 
     """
     node_qc = _get_node_qc_by_bit_widths(node,
                                          bit_width_cfg,
-                                         node_index_in_graph)
+                                         node_index_in_graph,
+                                         fw_info)
 
     if node_qc is None:
         Logger.critical(f'Node {node.name} quantization configuration from configuration file'  # pragma: no cover
                         f' was not found in candidates configurations.')
 
     else:
         node.final_weights_quantization_cfg = node_qc.weights_quantization_cfg
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/configurable_quant_id.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/configurable_quant_id.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,23 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from enum import Enum
+from functools import partial
 
 import numpy as np
 
 
 def get_average_weights(distance_matrix: np.ndarray) -> np.ndarray:
     """
     Get weights for weighting the sensitivity among different layers when evaluating MP configurations on
     model's sensitivity. This function returns equal weights for each layer, such that the sensitivity
     is averaged over all layers.
+
     Args:
         distance_matrix: Numpy array at shape (L,M): L -number of interest points, M number of samples.
         The matrix contain the distance for each interest point at each sample.
 
     Returns:
         Numpy array containing equal weights for sensitivity weighting.
     """
@@ -46,7 +49,25 @@
     Returns:
         Numpy array containing weights for sensitivity weighting (all zero but the last one).
     """
     num_nodes = len(distance_matrix)
     w = np.asarray([0 for _ in range(num_nodes)])
     w[-1] = 1
     return w
+
+
+class MpDistanceWeighting(Enum):
+    """
+    Defines mixed precision distance metric weighting methods.
+    The enum values can be used to call a function on a set of arguments and key-arguments.
+
+     AVG - take the average distance on all computed layers.
+
+     LAST_LAYER - take only the distance of the last layer output.
+
+    """
+
+    AVG = partial(get_average_weights)
+    LAST_LAYER = partial(get_last_layer_weights)
+
+    def __call__(self, distance_matrix: np.ndarray) -> np.ndarray:
+        return self.value(distance_matrix)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_aggregation_methods.py`

 * *Files 18% similar despite different names*

```diff
@@ -17,89 +17,89 @@
 from functools import partial
 from typing import List, Any
 import numpy as np
 
 from pulp import lpSum
 
 
-def sum_kpi(kpi_vector: np.ndarray, set_constraints: bool = True) -> List[Any]:
+def sum_ru_values(ru_vector: np.ndarray, set_constraints: bool = True) -> List[Any]:
     """
-    Aggregates KPIs vector to a single KPI measure by summing all values.
+    Aggregates resource utilization vector to a single resource utilization measure by summing all values.
 
     Args:
-        kpi_vector: A vector with nodes' KPI values.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        ru_vector: A vector with nodes' resource utilization values.
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A list with an lpSum object for lp problem definition with the vector's sum.
 
     """
     if not set_constraints:
-        return [0] if len(kpi_vector) == 0 else [sum(kpi_vector)]
-    return [lpSum(kpi_vector)]
+        return [0] if len(ru_vector) == 0 else [sum(ru_vector)]
+    return [lpSum(ru_vector)]
 
 
-def max_kpi(kpi_vector: np.ndarray, set_constraints: bool = True) -> List[float]:
+def max_ru_values(ru_vector: np.ndarray, set_constraints: bool = True) -> List[float]:
     """
-    Aggregates KPIs vector to allow max constraint in the linear programming problem formalization.
-    In order to do so, we need to define a separate constraint on each value in the KPI vector,
-    to be bounded by the target KPI.
+    Aggregates resource utilization vector to allow max constraint in the linear programming problem formalization.
+    In order to do so, we need to define a separate constraint on each value in the resource utilization vector,
+    to be bounded by the target resource utilization.
 
     Args:
-        kpi_vector: A vector with nodes' KPI values.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        ru_vector: A vector with nodes' resource utilization values.
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A list with the vector's values, to be used to define max constraint
     in the linear programming problem formalization.
 
     """
     if not set_constraints:
-        return [0] if len(kpi_vector) == 0 else [max(kpi_vector)]
-    return [kpi for kpi in kpi_vector]
+        return [0] if len(ru_vector) == 0 else [max(ru_vector)]
+    return [ru for ru in ru_vector]
 
 
-def total_kpi(kpi_tensor: np.ndarray, set_constraints: bool = True) -> List[float]:
+def total_ru(ru_tensor: np.ndarray, set_constraints: bool = True) -> List[float]:
     """
-    Aggregates KPIs vector to allow weights and activation total kpi constraint in the linear programming
-    problem formalization. In order to do so, we need to define a separate constraint on each activation value in
-    the KPI vector, combined with the sum weights kpi.
-    Note that the given kpi_tensor should contain weights and activation kpi values in each entry.
+    Aggregates resource utilization vector to allow weights and activation total utilization constraint in the linear programming
+    problem formalization. In order to do so, we need to define a separate constraint on each activation memory utilization value in
+    the resource utilization vector, combined with the sum weights memory utilization.
+    Note that the given ru_tensor should contain weights and activation utilization values in each entry.
 
     Args:
-        kpi_tensor: A tensor with nodes' KPI values for weights and activation.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        ru_tensor: A tensor with nodes' resource utilization values for weights and activation.
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A list with lpSum objects, to be used to define total constraint
     in the linear programming problem formalization.
 
     """
     if not set_constraints:
-        weights_kpi = sum([kpi[0] for kpi in kpi_tensor])
-        activation_kpi = max([kpi[1] for kpi in kpi_tensor])
-        return [weights_kpi + activation_kpi]
+        weights_ru = sum([ru[0] for ru in ru_tensor])
+        activation_ru = max([ru[1] for ru in ru_tensor])
+        return [weights_ru + activation_ru]
 
-    weights_kpi = lpSum([kpi[0] for kpi in kpi_tensor])
-    total_kpis = [weights_kpi + activation_kpi for _, activation_kpi in kpi_tensor]
+    weights_ru = lpSum([ru[0] for ru in ru_tensor])
+    total_ru = [weights_ru + activation_ru for _, activation_ru in ru_tensor]
 
-    return total_kpis
+    return total_ru
 
 
-class MpKpiAggregation(Enum):
+class MpRuAggregation(Enum):
     """
-    Defines kpi aggregation functions that can be used to compute final KPI metric.
+    Defines resource utilization aggregation functions that can be used to compute final resource utilization metric.
     The enum values can be used to call a function on a set of arguments.
 
-     SUM - applies the sum_kpi function
+     SUM - applies the sum_ru_values function
 
-     MAX - applies the max_kpi function
+     MAX - applies the max_ru_values function
 
-     TOTAL - applies the total_kpi function
+     TOTAL - applies the total_ru function
 
     """
-    SUM = partial(sum_kpi)
-    MAX = partial(max_kpi)
-    TOTAL = partial(total_kpi)
+    SUM = partial(sum_ru_values)
+    MAX = partial(max_ru_values)
+    TOTAL = partial(total_ru)
 
     def __call__(self, *args):
         return self.value(*args)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/resource_utilization_data.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,76 +8,78 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable, Any
 import numpy as np
+from typing import Callable, Any
 
-from model_compression_toolkit.core import FrameworkInfo, KPI, CoreConfig
-from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.core import FrameworkInfo, ResourceUtilization, CoreConfig
+from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.graph_prep_runner import graph_preparation_runner
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 
 
-def compute_kpi_data(in_model: Any,
-                     representative_data_gen: Callable,
-                     core_config: CoreConfig,
-                     tpc: TargetPlatformCapabilities,
-                     fw_info: FrameworkInfo,
-                     fw_impl: FrameworkImplementation) -> KPI:
+def compute_resource_utilization_data(in_model: Any,
+                                      representative_data_gen: Callable,
+                                      core_config: CoreConfig,
+                                      tpc: TargetPlatformCapabilities,
+                                      fw_info: FrameworkInfo,
+                                      fw_impl: FrameworkImplementation) -> ResourceUtilization:
     """
-    Compute KPI information that can be relevant for defining target KPI for mixed precision search.
+    Compute Resource Utilization information that can be relevant for defining target ResourceUtilization for mixed precision search.
     Calculates maximal activation tensor, sum of weights' parameters and total (sum of both).
 
     Args:
         in_model:  Model to build graph from (the model that intended to be quantized).
         representative_data_gen: Dataset used for calibration.
         core_config: CoreConfig containing parameters of how the model should be quantized.
         tpc: TargetPlatformCapabilities object that models the inference target platform and
                                               the attached framework operator's information.
         fw_info: Information needed for quantization about the specific framework.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
-    Returns: A KPI object with the results.
+    Returns: A ResourceUtilization object with the results.
 
     """
 
+    # We assume that the resource_utilization_data API is used to compute the model resource utilization for
+    # mixed precision scenario, so we run graph preparation under the assumption of enabled mixed precision.
     transformed_graph = graph_preparation_runner(in_model,
                                                  representative_data_gen,
                                                  core_config.quantization_config,
                                                  fw_info,
                                                  fw_impl,
                                                  tpc,
-                                                 mixed_precision_enable=core_config.mixed_precision_enable)
+                                                 mixed_precision_enable=True)
 
     # Compute parameters sum
     weights_params = compute_nodes_weights_params(graph=transformed_graph, fw_info=fw_info)
     total_weights_params = 0 if len(weights_params) == 0 else sum(weights_params)
 
     # Compute max activation tensor
     activation_output_sizes = compute_activation_output_sizes(graph=transformed_graph)
     max_activation_tensor_size = 0 if len(activation_output_sizes) == 0 else max(activation_output_sizes)
 
-    # Compute total kpi - parameters sum + max activation tensor
+    # Compute total memory utilization - parameters sum + max activation tensor
     total_size = total_weights_params + max_activation_tensor_size
 
-    # Compute BOPS kpi - total count of bit-operations for all configurable layers with kernel
+    # Compute BOPS utilization - total count of bit-operations for all configurable layers with kernel
     bops_count = compute_total_bops(graph=transformed_graph, fw_info=fw_info, fw_impl=fw_impl)
     bops_count = np.inf if len(bops_count) == 0 else sum(bops_count)
 
-    return KPI(weights_memory=total_weights_params,
-               activation_memory=max_activation_tensor_size,
-               total_memory=total_size,
-               bops=bops_count)
+    return ResourceUtilization(weights_memory=total_weights_params,
+                               activation_memory=max_activation_tensor_size,
+                               total_memory=total_size,
+                               bops=bops_count)
 
 
 def compute_nodes_weights_params(graph: Graph, fw_info: FrameworkInfo) -> np.ndarray:
     """
     Computes a vector with the respective weights' parameters size for each node.
 
     Args:
@@ -87,21 +89,28 @@
 
     Returns: A vector of node's weights memory sizes.
 
     """
 
     weights_params = []
     for n in graph.nodes:
-        if n.has_weights_quantization_enabled_candidate() and not n.reuse:
-            node_num_weights_params = 0
-            for attr in fw_info.get_kernel_op_attributes(n.type):
-                if attr is not None:
-                    node_num_weights_params += n.get_weights_by_keys(attr).flatten().shape[0]
+        # TODO: when enabling multiple attribute quantization by default (currently,
+        #  only kernel quantization is enabled) we should include other attributes memory in the sum of all
+        #  weights memory.
+        #  When implementing this, we should just go over all attributes in the node instead of counting only kernels.
+        kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and not n.reuse:
+            kernel_candidates = n.get_all_weights_attr_candidates(kernel_attr)
+            if len(kernel_candidates) > 0 and any([c.enable_weights_quantization for c in kernel_candidates]):
+                node_num_weights_params = 0
+                for attr in fw_info.get_kernel_op_attributes(n.type):
+                    if attr is not None:
+                        node_num_weights_params += n.get_weights_by_keys(attr).flatten().shape[0]
 
-            weights_params.append(node_num_weights_params)
+                weights_params.append(node_num_weights_params)
 
     return np.array(weights_params)
 
 
 def compute_activation_output_sizes(graph: Graph) -> np.ndarray:
     """
     Computes a vector with the respective output tensor size for each node.
@@ -138,15 +147,15 @@
 
     """
 
     bops = []
 
     # Go over all configurable nodes that have kernels.
     for n in graph.get_topo_sorted_nodes():
-        if n.has_weights_to_quantize(fw_info):
+        if n.has_kernel_weight_to_quantize(fw_info):
             # If node doesn't have weights then its MAC count is 0, and we shouldn't consider it in the BOPS count.
             incoming_edges = graph.incoming_edges(n, sort_by_attr=EDGE_SINK_INDEX)
             assert len(incoming_edges) == 1, f"Can't compute BOPS metric for node {n.name} with multiple inputs."
 
             node_mac = fw_impl.get_node_mac_operations(n, fw_info)
 
             node_bops = (FLOAT_BITWIDTH ** 2) * node_mac
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_functions_mapping.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,19 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPITarget
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_methods import MpKpiMetric
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import RUTarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_aggregation_methods import MpRuAggregation
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_methods import MpRuMetric
 
 
-# When adding a KPITarget that we want to consider in our mp search,
-# a matching pair of kpi_tools computation function and a kpi_tools
+# When adding a RUTarget that we want to consider in our mp search,
+# a matching pair of resource_utilization_tools computation function and a resource_utilization_tools
 # aggregation function should be added to this dictionary
-kpi_functions_mapping = {KPITarget.WEIGHTS: (MpKpiMetric.WEIGHTS_SIZE, MpKpiAggregation.SUM),
-                         KPITarget.ACTIVATION: (MpKpiMetric.ACTIVATION_OUTPUT_SIZE, MpKpiAggregation.MAX),
-                         KPITarget.TOTAL: (MpKpiMetric.TOTAL_WEIGHTS_ACTIVATION_SIZE, MpKpiAggregation.TOTAL),
-                         KPITarget.BOPS: (MpKpiMetric.BOPS_COUNT, MpKpiAggregation.SUM)}
+ru_functions_mapping = {RUTarget.WEIGHTS: (MpRuMetric.WEIGHTS_SIZE, MpRuAggregation.SUM),
+                        RUTarget.ACTIVATION: (MpRuMetric.ACTIVATION_OUTPUT_SIZE, MpRuAggregation.MAX),
+                        RUTarget.TOTAL: (MpRuMetric.TOTAL_WEIGHTS_ACTIVATION_SIZE, MpRuAggregation.TOTAL),
+                        RUTarget.BOPS: (MpRuMetric.BOPS_COUNT, MpRuAggregation.SUM)}
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/resource_utilization_tools/ru_methods.py`

 * *Files 19% similar despite different names*

```diff
@@ -24,153 +24,167 @@
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode, \
     VirtualSplitWeightsNode, VirtualSplitActivationNode
 from model_compression_toolkit.logger import Logger
 
 
-def weights_size_kpi(mp_cfg: List[int],
-                     graph: Graph,
-                     fw_info: FrameworkInfo,
-                     fw_impl: FrameworkImplementation) -> np.ndarray:
+def weights_size_utilization(mp_cfg: List[int],
+                             graph: Graph,
+                             fw_info: FrameworkInfo,
+                             fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective weights' memory size for the given weight configurable node,
+    Computes a resource utilization vector with the respective weights' memory size for the given weight configurable node,
     according to the given mixed-precision configuration.
-    If an empty configuration is given, then computes KPI vector for non-configurable nodes.
+    If an empty configuration is given, then computes resource utilization vector for non-configurable nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation (not used in this method).
 
     Returns: A vector of node's weights memory sizes.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
     weights_memory = []
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
-    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes()]
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
+    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes(fw_info)]
 
     if len(mp_cfg) == 0:
-        # Computing non-configurable nodes KPI
+        # Computing non-configurable nodes resource utilization
+        # TODO: when enabling multiple attribute quantization by default (currently,
+        #  only kernel quantization is enabled) we should include other attributes memory in the sum of all
+        #  weights memory (when quantized to their default 8-bit, non-configurable).
+        #  When implementing this, we should just go over all attributes in the node instead of counting only kernels.
         for n in graph.nodes:
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if kernel_attr is None:
+                continue
             non_configurable_node = n.name not in weights_mp_nodes \
-                                    and n.has_weights_quantization_enabled_candidate() \
                                     and not n.reuse \
-                                    and n.is_all_weights_candidates_equal()
+                                    and n.is_all_weights_candidates_equal(kernel_attr)
 
             if non_configurable_node:
-                node_nbits = n.candidates_quantization_cfg[0].weights_quantization_cfg.weights_n_bits
+                node_nbits = (n.candidates_quantization_cfg[0].weights_quantization_cfg
+                              .get_attr_config(kernel_attr).weights_n_bits)
                 node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
                 weights_memory.append(node_weights_memory_in_bytes)
     else:
-        # Go over configurable all nodes that should be taken into consideration when computing the weights KPI.
-        for n in graph.get_sorted_weights_configurable_nodes():
+        # Go over configurable all nodes that should be taken into consideration when computing the weights
+        # resource utilization.
+        for n in graph.get_sorted_weights_configurable_nodes(fw_info):
+            # Only nodes with kernel op can be considered configurable
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
             node_idx = mp_nodes.index(n.name)
             node_qc = n.candidates_quantization_cfg[mp_cfg[node_idx]]
-            node_nbits = node_qc.weights_quantization_cfg.weights_n_bits
+            node_nbits = node_qc.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
 
             node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
 
             weights_memory.append(node_weights_memory_in_bytes)
 
     return np.array(weights_memory)
 
 
-def activation_output_size_kpi(mp_cfg: List[int],
-                               graph: Graph,
-                               fw_info: FrameworkInfo,
-                               fw_impl: FrameworkImplementation) -> np.ndarray:
+def activation_output_size_utilization(mp_cfg: List[int],
+                                       graph: Graph,
+                                       fw_info: FrameworkInfo,
+                                       fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective output memory size for each activation configurable node,
+    Computes a resource utilization vector with the respective output memory size for each activation configurable node,
     according to the given mixed-precision configuration.
-    If an empty configuration is given, then computes KPI vector for non-configurable nodes.
+    If an empty configuration is given, then computes resource utilization vector for non-configurable nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize)
             (not used in this method).
         fw_impl: FrameworkImplementation object with specific framework methods implementation(not used in this method).
 
     Returns: A vector of node's activation memory sizes.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
     activation_memory = []
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
     activation_mp_nodes = [n.name for n in graph.get_sorted_activation_configurable_nodes()]
 
     if len(mp_cfg) == 0:
-        # Computing non-configurable nodes KPI
+        # Computing non-configurable nodes resource utilization
         for n in graph.nodes:
             non_configurable_node = n.name not in activation_mp_nodes \
                                     and n.has_activation_quantization_enabled_candidate() \
                                     and n.is_all_activation_candidates_equal()
 
             if non_configurable_node:
                 node_nbits = n.candidates_quantization_cfg[0].activation_quantization_cfg.activation_n_bits
                 node_activation_memory_in_bytes = _compute_node_activation_memory(n, node_nbits)
                 activation_memory.append(node_activation_memory_in_bytes)
     else:
-        # Go over all nodes that should be taken into consideration when computing the weights KPI.
+        # Go over all nodes that should be taken into consideration when computing the weights memory utilization.
         for n in graph.get_sorted_activation_configurable_nodes():
             node_idx = mp_nodes.index(n.name)
             node_qc = n.candidates_quantization_cfg[mp_cfg[node_idx]]
             node_nbits = node_qc.activation_quantization_cfg.activation_n_bits
 
             node_activation_memory_in_bytes = _compute_node_activation_memory(n, node_nbits)
 
             activation_memory.append(node_activation_memory_in_bytes)
 
     return np.array(activation_memory)
 
 
-def total_weights_activation_kpi(mp_cfg: List[int],
-                                 graph: Graph,
-                                 fw_info: FrameworkInfo,
-                                 fw_impl: FrameworkImplementation) -> np.ndarray:
+def total_weights_activation_utilization(mp_cfg: List[int],
+                                         graph: Graph,
+                                         fw_info: FrameworkInfo,
+                                         fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes KPIs tensor with the respective weights size and output memory size for each activation configurable node,
+    Computes resource utilization tensor with the respective weights size and output memory size for each activation configurable node,
     according to the given mixed-precision configuration.
-    If an empty configuration is given, then computes KPI vector for non-configurable nodes.
+    If an empty configuration is given, then computes resource utilization vector for non-configurable nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize)
             (not used in this method).
         fw_impl: FrameworkImplementation object with specific framework methods implementation(not used in this method).
 
     Returns: A 2D tensor of nodes' weights memory sizes and activation output memory size.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
     weights_activation_memory = []
-    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes()]
+    weights_mp_nodes = [n.name for n in graph.get_sorted_weights_configurable_nodes(fw_info)]
     activation_mp_nodes = [n.name for n in graph.get_sorted_activation_configurable_nodes()]
 
     if len(mp_cfg) == 0:
-        # Computing non-configurable nodes KPI
+        # Computing non-configurable nodes utilization
         for n in graph.nodes:
 
             non_configurable = False
             node_weights_memory_in_bytes, node_activation_memory_in_bytes = 0, 0
 
             # Non-configurable Weights
-            is_non_configurable_weights = n.name not in weights_mp_nodes and \
-                                          n.has_weights_quantization_enabled_candidate() and \
-                                          n.is_all_weights_candidates_equal() and \
-                                          not n.reuse
-
-            if is_non_configurable_weights:
-                node_nbits = n.candidates_quantization_cfg[0].weights_quantization_cfg.weights_n_bits
-                node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
-                non_configurable = True
+            # TODO: currently considering only kernel attributes in weights memory utilization.
+            #  When enabling multi-attribute quantization we need to modify this method to count all attributes.
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if kernel_attr is not None:
+                is_non_configurable_weights = n.name not in weights_mp_nodes and \
+                                              n.is_all_weights_candidates_equal(kernel_attr) and \
+                                              not n.reuse
+
+                if is_non_configurable_weights:
+                    node_nbits = (n.candidates_quantization_cfg[0].weights_quantization_cfg
+                                  .get_attr_config(kernel_attr).weights_n_bits)
+                    node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_nbits, fw_info)
+                    non_configurable = True
 
             # Non-configurable Activation
             is_non_configurable_activation = n.name not in activation_mp_nodes and \
                                              n.has_activation_quantization_enabled_candidate() and \
                                              n.is_all_activation_candidates_equal()
 
             if is_non_configurable_activation:
@@ -179,116 +193,121 @@
                 non_configurable = True
 
             if non_configurable:
                 weights_activation_memory.append(
                     np.array([node_weights_memory_in_bytes, node_activation_memory_in_bytes]))
     else:
         # Go over all nodes that should be taken into consideration when computing the weights or
-        # activation KPI (all configurable nodes).
-        for node_idx, n in enumerate(graph.get_configurable_sorted_nodes()):
+        # activation memory utilization (all configurable nodes).
+        for node_idx, n in enumerate(graph.get_configurable_sorted_nodes(fw_info)):
+            # TODO: currently considering only kernel attributes in weights memory utilization. When enabling multi-attribute
+            #  quantization we need to modify this method to count all attributes.
+
             node_qc = n.candidates_quantization_cfg[mp_cfg[node_idx]]
-            node_weights_nbits = node_qc.weights_quantization_cfg.weights_n_bits
-            node_activation_nbits = node_qc.activation_quantization_cfg.activation_n_bits
 
             # Compute node's weights memory (if no weights to quantize then set to 0)
             node_weights_memory_in_bytes = 0
-            if n.is_weights_quantization_enabled() and not n.is_all_weights_candidates_equal():
-                node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_weights_nbits, fw_info)
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if kernel_attr is not None:
+                if n.is_weights_quantization_enabled(kernel_attr) and not n.is_all_weights_candidates_equal(kernel_attr):
+                    node_weights_nbits = node_qc.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+                    node_weights_memory_in_bytes = _compute_node_weights_memory(n, node_weights_nbits, fw_info)
 
             # Compute node's activation memory (if node's activation are not being quantized then set to 0)
+            node_activation_nbits = node_qc.activation_quantization_cfg.activation_n_bits
             node_activation_memory_in_bytes = 0
             if n.is_activation_quantization_enabled() and not n.is_all_activation_candidates_equal():
                 node_activation_memory_in_bytes = _compute_node_activation_memory(n, node_activation_nbits)
 
             weights_activation_memory.append(np.array([node_weights_memory_in_bytes, node_activation_memory_in_bytes]))
 
     return np.array(weights_activation_memory)
 
 
-def bops_kpi(mp_cfg: List[int],
-             graph: Graph,
-             fw_info: FrameworkInfo,
-             fw_impl: FrameworkImplementation,
-             set_constraints: bool = True) -> np.ndarray:
+def bops_utilization(mp_cfg: List[int],
+                     graph: Graph,
+                     fw_info: FrameworkInfo,
+                     fw_impl: FrameworkImplementation,
+                     set_constraints: bool = True) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective bit-operations (BOPS) count for each configurable node,
+    Computes a resource utilization vector with the respective bit-operations (BOPS) count for each configurable node,
     according to the given mixed-precision configuration of a virtual graph with composed nodes.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation.
-        set_constraints: A flag for utilizing the method for KPI computation of a
+        set_constraints: A flag for utilizing the method for resource utilization computation of a
             given config not for LP formalization purposes.
 
     Returns: A vector of node's BOPS count.
     Note that the vector is not necessarily of the same length as the given config.
 
     """
 
     if not set_constraints:
-        return _bops_kpi(mp_cfg,
-                         graph,
-                         fw_info,
-                         fw_impl)
+        return _bops_utilization(mp_cfg,
+                                 graph,
+                                 fw_info,
+                                 fw_impl)
 
-    # BOPs KPI method considers non-configurable nodes, therefore, it doesn't need separate implementation
+    # BOPs utilization method considers non-configurable nodes, therefore, it doesn't need separate implementation
     # for non-configurable nodes for setting a constraint (no need for separate implementation for len(mp_cfg) = 0).
 
     virtual_bops_nodes = [n for n in graph.get_topo_sorted_nodes() if isinstance(n, VirtualActivationWeightsNode)]
 
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
     bops = [n.get_bops_count(fw_impl, fw_info, candidate_idx=_get_node_cfg_idx(n, mp_cfg, mp_nodes)) for n in virtual_bops_nodes]
 
     return np.array(bops)
 
 
-def _bops_kpi(mp_cfg: List[int],
-              graph: Graph,
-              fw_info: FrameworkInfo,
-              fw_impl: FrameworkImplementation) -> np.ndarray:
+def _bops_utilization(mp_cfg: List[int],
+                      graph: Graph,
+                      fw_info: FrameworkInfo,
+                      fw_impl: FrameworkImplementation) -> np.ndarray:
     """
-    Computes a KPIs vector with the respective bit-operations (BOPS) count for each configurable node,
+    Computes a resource utilization vector with the respective bit-operations (BOPS) count for each configurable node,
     according to the given mixed-precision configuration of an original graph.
 
     Args:
         mp_cfg: A mixed-precision configuration (list of candidates index for each configurable node)
         graph: Graph object.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation.
 
     Returns: A vector of node's BOPS count.
 
     """
 
-    mp_nodes = graph.get_configurable_sorted_nodes_names()
+    mp_nodes = graph.get_configurable_sorted_nodes_names(fw_info)
 
-    # Go over all nodes that should be taken into consideration when computing the BOPS KPI.
+    # Go over all nodes that should be taken into consideration when computing the BOPS utilization.
     bops = []
     for n in graph.get_topo_sorted_nodes():
-        if n.has_weights_to_quantize(fw_info):
+        if n.has_kernel_weight_to_quantize(fw_info):
             # If node doesn't have weights then its MAC count is 0, and we shouldn't consider it in the BOPS count.
             incoming_edges = graph.incoming_edges(n, sort_by_attr=EDGE_SINK_INDEX)
             if len(incoming_edges) != 1:
-                Logger.critical(f"Can't compute BOPS metric for node {n.name} with multiple inputs.")  # pragma: no cover
-
+                Logger.critical(f"Unable to compute BOPS metric for node {n.name} due to multiple inputs.")  # pragma: no cover
             input_activation_node = incoming_edges[0].source_node
             if len(graph.out_edges(input_activation_node)) > 1:
                 # In the case where the activation node has multiple outgoing edges
-                # we don't consider this edge in the BOPS KPI calculation
+                # we don't consider this edge in the BOPS utilization calculation
                 continue
 
             input_activation_node_cfg = input_activation_node.candidates_quantization_cfg[_get_node_cfg_idx(input_activation_node, mp_cfg, mp_nodes)]
 
             node_mac = fw_impl.get_node_mac_operations(n, fw_info)
 
             node_qc = n.candidates_quantization_cfg[_get_node_cfg_idx(n, mp_cfg, mp_nodes)]
-            node_weights_nbits = node_qc.weights_quantization_cfg.weights_n_bits if \
-                node_qc.weights_quantization_cfg.enable_weights_quantization else FLOAT_BITWIDTH
+            kenrel_node_qc = node_qc.weights_quantization_cfg.get_attr_config(fw_info.get_kernel_op_attributes(n.type)[0])
+            node_weights_nbits = kenrel_node_qc.weights_n_bits if \
+                kenrel_node_qc.enable_weights_quantization else FLOAT_BITWIDTH
             input_activation_nbits = input_activation_node_cfg.activation_quantization_cfg.activation_n_bits if \
                 input_activation_node_cfg.activation_quantization_cfg.enable_activation_quantization else FLOAT_BITWIDTH
 
             node_bops = node_weights_nbits * input_activation_nbits * node_mac
             bops.append(node_bops)
 
     return np.array(bops)
@@ -315,15 +334,15 @@
         assert len(node.candidates_quantization_cfg) > 0, \
             "Any node should have at least one candidate configuration."
         return 0
 
 
 def _get_origin_weights_node(n: BaseNode) -> BaseNode:
     """
-    In case we run a KPI computation on a virtual graph,
+    In case we run a resource utilization computation on a virtual graph,
     this method is used to retrieve the original node out of a virtual weights node,
 
     Args:
         n: A possibly virtual node.
 
     Returns: A node from the original (non-virtual) graph which the given node represents.
 
@@ -335,15 +354,15 @@
         return n.origin_node
 
     return n
 
 
 def _get_origin_activation_node(n: BaseNode) -> BaseNode:
     """
-    In case we run a KPI computation on a virtual graph,
+    In case we run a resource utilization computation on a virtual graph,
     this method is used to retrieve the original node out of a virtual activation node,
 
     Args:
         n: A possibly virtual node.
 
     Returns: A node from the original (non-virtual) graph which the given node represents.
 
@@ -394,29 +413,29 @@
 
     origin_node = _get_origin_activation_node(n)
     node_output_size = origin_node.get_total_output_params()
 
     return node_output_size * node_nbits / BITS_TO_BYTES
 
 
-class MpKpiMetric(Enum):
+class MpRuMetric(Enum):
     """
-    Defines kpi computation functions that can be used to compute KPI for a given target for a given mp config.
-    The enum values can be used to call a function on a set of arguments.
+    Defines resource utilization computation functions that can be used to compute bops_utilization for a given target
+    for a given mp config. The enum values can be used to call a function on a set of arguments.
 
-     WEIGHTS_SIZE - applies the weights_size_kpi function
+     WEIGHTS_SIZE - applies the weights_size_utilization function
 
-     ACTIVATION_OUTPUT_SIZE - applies the activation_output_size_kpi function
+     ACTIVATION_OUTPUT_SIZE - applies the activation_output_size_utilization function
 
-     TOTAL_WEIGHTS_ACTIVATION_SIZE - applies the total_weights_activation_kpi function
+     TOTAL_WEIGHTS_ACTIVATION_SIZE - applies the total_weights_activation_utilization function
 
-     BOPS_COUNT - applies the bops_kpi function
+     BOPS_COUNT - applies the bops_utilization function
 
     """
 
-    WEIGHTS_SIZE = partial(weights_size_kpi)
-    ACTIVATION_OUTPUT_SIZE = partial(activation_output_size_kpi)
-    TOTAL_WEIGHTS_ACTIVATION_SIZE = partial(total_weights_activation_kpi)
-    BOPS_COUNT = partial(bops_kpi)
+    WEIGHTS_SIZE = partial(weights_size_utilization)
+    ACTIVATION_OUTPUT_SIZE = partial(activation_output_size_utilization)
+    TOTAL_WEIGHTS_ACTIVATION_SIZE = partial(total_weights_activation_utilization)
+    BOPS_COUNT = partial(bops_utilization)
 
     def __call__(self, *args):
         return self.value(*args)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py`

 * *Files 23% similar despite different names*

```diff
@@ -14,18 +14,19 @@
 # ==============================================================================
 
 import copy
 from enum import Enum
 import numpy as np
 from typing import List, Callable, Dict
 
-from model_compression_toolkit.core import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core.common import Graph
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_functions_mapping import kpi_functions_mapping
+from model_compression_toolkit.core.common.hessian import HessianInfoService
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization, RUTarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_functions_mapping import ru_functions_mapping
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import MixedPrecisionSearchManager
 from model_compression_toolkit.core.common.mixed_precision.search_methods.linear_programming import \
     mp_integer_programming_search
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.solution_refinement_procedure import \
     greedy_solution_refinement_procedure
@@ -42,86 +43,90 @@
 search_methods = {
     BitWidthSearchMethod.INTEGER_PROGRAMMING: mp_integer_programming_search}
 
 
 def search_bit_width(graph_to_search_cfg: Graph,
                      fw_info: FrameworkInfo,
                      fw_impl: FrameworkImplementation,
-                     target_kpi: KPI,
-                     mp_config: MixedPrecisionQuantizationConfigV2,
+                     target_resource_utilization: ResourceUtilization,
+                     mp_config: MixedPrecisionQuantizationConfig,
                      representative_data_gen: Callable,
-                     search_method: BitWidthSearchMethod = BitWidthSearchMethod.INTEGER_PROGRAMMING) -> List[int]:
+                     search_method: BitWidthSearchMethod = BitWidthSearchMethod.INTEGER_PROGRAMMING,
+                     hessian_info_service: HessianInfoService=None) -> List[int]:
     """
     Search for an MP configuration for a given graph. Given a search_method method (by default, it's linear
     programming), we use the sensitivity_evaluator object that provides a function to compute an
     evaluation for the expected sensitivity for a bit-width configuration.
-    Then, and after computing the KPI for each node in the graph for each bit-width in the search space,
-    we search for the optimal solution, given some target_kpi, the solution should fit.
-    target_kpi have to be passed. If it was not passed, the facade is not supposed to get here by now.
+    Then, and after computing the resource utilization for each node in the graph for each bit-width in the search space,
+    we search for the optimal solution, given some target_resource_utilization, the solution should fit.
+    target_resource_utilization have to be passed. If it was not passed, the facade is not supposed to get here by now.
 
     Args:
         graph_to_search_cfg: Graph to search a MP configuration for.
         fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
         fw_impl: FrameworkImplementation object with specific framework methods implementation.
-        target_kpi: Target KPI to bound our feasible solution space s.t the configuration does not violate it.
+        target_resource_utilization: Target Resource Utilization to bound our feasible solution space s.t the configuration does not violate it.
         mp_config: Mixed-precision quantization configuration.
         representative_data_gen: Dataset to use for retrieving images for the models inputs.
         search_method: BitWidthSearchMethod to define which searching method to use.
+        hessian_info_service: HessianInfoService to fetch Hessian traces approximations.
 
     Returns:
         A MP configuration for the graph (list of integers, where the index in the list, is the node's
         index in the graph, when the graph is topology sorted, and the value in this index is the
         bit-width index on the node).
 
     """
 
-    # target_kpi have to be passed. If it was not passed, the facade is not supposed to get here by now.
-    if target_kpi is None:
-        Logger.critical('Target KPI have to be passed for search_methods bit-width configuration')  # pragma: no cover
+    # target_resource_utilization have to be passed. If it was not passed, the facade is not supposed to get here by now.
+    if target_resource_utilization is None:
+        Logger.critical("Target ResourceUtilization is required for the bit-width search method's configuration.")  # pragma: no cover
 
     # Set graph for MP search
     graph = copy.deepcopy(graph_to_search_cfg)  # Copy graph before searching
-    if target_kpi.bops < np.inf:
-        # Since Bit-operations count target KPI is set, we need to reconstruct the graph for the MP search
+    if target_resource_utilization.bops < np.inf:
+        # Since Bit-operations count target resource utilization is set, we need to reconstruct the graph for the MP search
         graph = substitute(graph, fw_impl.get_substitutions_virtual_weights_activation_coupling())
 
     # If we only run weights compression with MP than no need to consider activation quantization when computing the
     # MP metric (it adds noise to the computation)
-    disable_activation_for_metric = (target_kpi.weights_memory < np.inf and
-                                    (target_kpi.activation_memory == np.inf and
-                                     target_kpi.total_memory == np.inf and
-                                     target_kpi.bops == np.inf)) or graph_to_search_cfg.is_single_activation_cfg()
+    disable_activation_for_metric = (target_resource_utilization.weights_memory < np.inf and
+                                    (target_resource_utilization.activation_memory == np.inf and
+                                     target_resource_utilization.total_memory == np.inf and
+                                     target_resource_utilization.bops == np.inf)) or graph_to_search_cfg.is_single_activation_cfg()
 
     # Set Sensitivity Evaluator for MP search. It should always work with the original MP graph,
-    # even if a virtual graph was created (and is used only for BOPS KPI computation purposes)
+    # even if a virtual graph was created (and is used only for BOPS utilization computation purposes)
     se = fw_impl.get_sensitivity_evaluator(
         graph_to_search_cfg,
         mp_config,
         representative_data_gen=representative_data_gen,
         fw_info=fw_info,
-        disable_activation_for_metric=disable_activation_for_metric)
+        disable_activation_for_metric=disable_activation_for_metric,
+        hessian_info_service=hessian_info_service)
 
-    # Each pair of (KPI method, KPI aggregation) should match to a specific provided kpi target
-    kpi_functions = kpi_functions_mapping
+    # Each pair of (resource utilization method, resource utilization aggregation) should match to a specific
+    # provided target resource utilization
+    ru_functions = ru_functions_mapping
 
     # Instantiate a manager object
     search_manager = MixedPrecisionSearchManager(graph,
                                                  fw_info,
                                                  fw_impl,
                                                  se,
-                                                 kpi_functions,
-                                                 target_kpi,
+                                                 ru_functions,
+                                                 target_resource_utilization,
                                                  original_graph=graph_to_search_cfg)
 
     if search_method in search_methods:  # Get a specific search function
         search_method_fn = search_methods.get(search_method)
     else:
         raise NotImplemented  # pragma: no cover
 
     # Search for the desired mixed-precision configuration
     result_bit_cfg = search_method_fn(search_manager,
-                                      target_kpi)
+                                      target_resource_utilization)
 
     if mp_config.refine_mp_solution:
-        result_bit_cfg = greedy_solution_refinement_procedure(result_bit_cfg, search_manager, target_kpi)
+        result_bit_cfg = greedy_solution_refinement_procedure(result_bit_cfg, search_manager, target_resource_utilization)
 
     return result_bit_cfg
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py`

 * *Files 13% similar despite different names*

```diff
@@ -19,78 +19,78 @@
 
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode, \
     VirtualSplitWeightsNode, VirtualSplitActivationNode
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPITarget, KPI
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_methods import MpKpiMetric
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import RUTarget, ResourceUtilization
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_aggregation_methods import MpRuAggregation
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_methods import MpRuMetric
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 
 
 class MixedPrecisionSearchManager:
     """
     Class to wrap and manage the search process of a mixed-precision configuration.
     """
 
     def __init__(self,
                  graph: Graph,
                  fw_info: FrameworkInfo,
                  fw_impl: FrameworkImplementation,
                  sensitivity_evaluator: SensitivityEvaluation,
-                 kpi_functions: Dict[KPITarget, Tuple[MpKpiMetric, MpKpiAggregation]],
-                 target_kpi: KPI,
+                 ru_functions: Dict[RUTarget, Tuple[MpRuMetric, MpRuAggregation]],
+                 target_resource_utilization: ResourceUtilization,
                  original_graph: Graph = None):
         """
 
         Args:
             graph: Graph to search for its MP configuration.
             fw_info: FrameworkInfo object about the specific framework (e.g., attributes of different layers' weights to quantize).
             fw_impl: FrameworkImplementation object with specific framework methods implementation.
             sensitivity_evaluator: A SensitivityEvaluation which provides a function that evaluates the sensitivity of
                 a bit-width configuration for the MP model.
-            kpi_functions: A dictionary with pairs of (MpKpiMethod, MpKpiAggregationMethod) mapping a KPITarget to
-                a couple of kpi metric function and kpi aggregation function.
-            target_kpi: Target KPI to bound our feasible solution space s.t the configuration does not violate it.
-            original_graph: In case we have a search over a virtual graph (if we have BOPS KPI target), then this argument
+            ru_functions: A dictionary with pairs of (MpRuMethod, MpRuAggregationMethod) mapping a RUTarget to
+                a couple of resource utilization metric function and resource utilization aggregation function.
+            target_resource_utilization: Target Resource Utilization to bound our feasible solution space s.t the configuration does not violate it.
+            original_graph: In case we have a search over a virtual graph (if we have BOPS utilization target), then this argument
                 will contain the original graph (for config reconstruction purposes).
         """
 
         self.graph = graph
         self.original_graph = graph if original_graph is None else original_graph
         self.fw_info = fw_info
         self.fw_impl = fw_impl
         self.sensitivity_evaluator = sensitivity_evaluator
         self.layer_to_bitwidth_mapping = self.get_search_space()
         self.compute_metric_fn = self.get_sensitivity_metric()
 
-        self.compute_kpi_functions = kpi_functions
-        self.target_kpi = target_kpi
-        self.min_kpi_config = self.graph.get_min_candidates_config()
-        self.max_kpi_config = self.graph.get_max_candidates_config()
-        self.min_kpi = self.compute_min_kpis()
-        self.non_conf_kpi_dict = self._non_configurable_nodes_kpi()
+        self.compute_ru_functions = ru_functions
+        self.target_resource_utilization = target_resource_utilization
+        self.min_ru_config = self.graph.get_min_candidates_config(fw_info)
+        self.max_ru_config = self.graph.get_max_candidates_config(fw_info)
+        self.min_ru = self.compute_min_ru()
+        self.non_conf_ru_dict = self._non_configurable_nodes_ru()
 
         self.config_reconstruction_helper = ConfigReconstructionHelper(virtual_graph=self.graph,
                                                                        original_graph=self.original_graph)
 
     def get_search_space(self) -> Dict[int, List[int]]:
         """
         The search space is a mapping from a node's index to a list of integers (possible bitwidths candidates indeces
         for the node).
 
         Returns:
             The entire search space of the graph.
         """
 
         indices_mapping = {}
-        nodes_to_configure = self.graph.get_configurable_sorted_nodes()
+        nodes_to_configure = self.graph.get_configurable_sorted_nodes(self.fw_info)
         for idx, n in enumerate(nodes_to_configure):
             # For each node, get all possible bitwidth indices for it
             # (which is a list from 0 to the length of the candidates mp_config list of the node).
             indices_mapping[idx] = list(range(len(n.candidates_quantization_cfg)))  # all search_methods space
         return indices_mapping
 
     def get_sensitivity_metric(self) -> Callable:
@@ -102,120 +102,122 @@
 
         """
         # Get from the framework an evaluation function on how a MP configuration,
         # affects the expected loss.
 
         return self.sensitivity_evaluator.compute_metric
 
-    def compute_min_kpis(self) -> Dict[KPITarget, np.ndarray]:
+    def compute_min_ru(self) -> Dict[RUTarget, np.ndarray]:
         """
-        Computes a KPIs vector with the values matching to the minimal mp configuration
+        Computes a resource utilization vector with the values matching to the minimal mp configuration
         (i.e., each node is configured with the quantization candidate that would give the minimal size of the
-        node's KPI).
-        The method computes the minimal KPIs vector for each kpi target.
+        node's resource utilization).
+        The method computes the minimal resource utilization vector for each target resource utilization.
 
-        Returns: A dictionary mapping each kpi target to its respective minimal KPIs values.
+        Returns: A dictionary mapping each target resource utilization to its respective minimal 
+        resource utilization values.
 
         """
-        min_kpis = {}
-        for kpi_target, kpi_fns in self.compute_kpi_functions.items():
-            # kpi_fns is a pair of kpi computation method and kpi aggregation method (in this method we only need
-            # the first one)
-            min_kpis[kpi_target] = kpi_fns[0](self.min_kpi_config, self.graph, self.fw_info, self.fw_impl)
+        min_ru = {}
+        for ru_target, ru_fns in self.compute_ru_functions.items():
+            # ru_fns is a pair of resource utilization computation method and 
+            # resource utilization aggregation method (in this method we only need the first one)
+            min_ru[ru_target] = ru_fns[0](self.min_ru_config, self.graph, self.fw_info, self.fw_impl)
 
-        return min_kpis
+        return min_ru
 
-    def compute_kpi_matrix(self, target: KPITarget) -> np.ndarray:
+    def compute_resource_utilization_matrix(self, target: RUTarget) -> np.ndarray:
         """
-        Computes and builds a KPIs matrix, to be used for the mixed-precision search problem formalization.
+        Computes and builds a resource utilization matrix, to be used for the mixed-precision search problem formalization.
         The matrix is constructed as follows (for a given target):
-        - Each row represents the set of KPI values for a specific KPI measure (number of rows should be equal to the
-            length of the output of the respective target compute_kpi function).
-        - Each entry in a specific column represents the KPI value of a given configuration (single layer is configured
-            with specific candidate, all other layer are at the minimal KPI configuration) for the KPI measure of the
-            respective row.
+        - Each row represents the set of resource utilization values for a specific resource utilization 
+            measure (number of rows should be equal to the length of the output of the respective target compute_ru function).
+        - Each entry in a specific column represents the resource utilization value of a given configuration 
+            (single layer is configured with specific candidate, all other layer are at the minimal resource 
+            utilization configuration) for the resource utilization measure of the respective row.
 
         Args:
-            target: The target for which the KPI is calculated (a KPITarget value).
+            target: The resource target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: A KPI matrix.
+        Returns: A resource utilization matrix.
 
         """
-        assert isinstance(target, KPITarget), f"{target} is not a valid KPI target"
+        assert isinstance(target, RUTarget), f"{target} is not a valid resource target"
 
-        configurable_sorted_nodes = self.graph.get_configurable_sorted_nodes()
+        configurable_sorted_nodes = self.graph.get_configurable_sorted_nodes(self.fw_info)
 
-        kpi_matrix = []
+        ru_matrix = []
         for c, c_n in enumerate(configurable_sorted_nodes):
             for candidate_idx in range(len(c_n.candidates_quantization_cfg)):
-                if candidate_idx == self.min_kpi_config[c]:
-                    # skip KPI computation for min configuration. Since we compute the difference from min_kpi it'll
+                if candidate_idx == self.min_ru_config[c]:
+                    # skip ru computation for min configuration. Since we compute the difference from min_ru it'll
                     # always be 0 for all entries in the results vector.
-                    candidate_kpis = np.zeros(shape=self.min_kpi[target].shape)
+                    candidate_rus = np.zeros(shape=self.min_ru[target].shape)
                 else:
-                    candidate_kpis = self.compute_candidate_relative_kpis(c, candidate_idx, target)
-                kpi_matrix.append(np.asarray(candidate_kpis))
+                    candidate_rus = self.compute_candidate_relative_ru(c, candidate_idx, target)
+                ru_matrix.append(np.asarray(candidate_rus))
 
-        # We need to transpose the calculated kpi matrix to allow later multiplication with
+        # We need to transpose the calculated ru matrix to allow later multiplication with
         # the indicators' diagonal matrix.
         # We only move the first axis (num of configurations) to be last,
         # the remaining axes include the metric specific nodes (rows dimension of the new tensor)
-        # and the kpi metric values (if they are non-scalars)
-        np_kpi_matrix = np.array(kpi_matrix)
-        return np.moveaxis(np_kpi_matrix, source=0, destination=len(np_kpi_matrix.shape) - 1)
-
-    def compute_candidate_relative_kpis(self,
-                                        conf_node_idx: int,
-                                        candidate_idx: int,
-                                        target: KPITarget) -> np.ndarray:
-        """
-        Computes a KPIs vector for a given candidates of a given configurable node, i.e., the matching KPI vector
-        which is obtained by computing the given target's KPI function on a minimal configuration in which the given
+        # and the ru metric values (if they are non-scalars)
+        np_ru_matrix = np.array(ru_matrix)
+        return np.moveaxis(np_ru_matrix, source=0, destination=len(np_ru_matrix.shape) - 1)
+
+    def compute_candidate_relative_ru(self,
+                                      conf_node_idx: int,
+                                      candidate_idx: int,
+                                      target: RUTarget) -> np.ndarray:
+        """
+        Computes a resource utilization vector for a given candidates of a given configurable node, 
+        i.e., the matching resource utilization vector which is obtained by computing the given target's 
+        resource utilization function on a minimal configuration in which the given
         layer's candidates is changed to the new given one.
-        The result is normalized by subtracting the target's minimal KPIs vector.
+        The result is normalized by subtracting the target's minimal resource utilization vector.
 
         Args:
             conf_node_idx: The index of a node in a sorted configurable nodes list.
             candidate_idx: The index of a node's quantization configuration candidate.
-            target: The target for which the KPI is calculated (a KPITarget value).
+            target: The target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: Normalized node's KPIs vector
+        Returns: Normalized node's resource utilization vector
 
         """
-        return self.compute_node_kpi_for_candidate(conf_node_idx, candidate_idx, target) - \
-               self.get_min_target_kpi(target)
+        return self.compute_node_ru_for_candidate(conf_node_idx, candidate_idx, target) - \
+               self.get_min_target_resource_utilization(target)
 
-    def get_min_target_kpi(self, target: KPITarget) -> np.ndarray:
+    def get_min_target_resource_utilization(self, target: RUTarget) -> np.ndarray:
         """
-        Returns the minimal KPIs vector (pre-calculated on initialization) of a specific target.
+        Returns the minimal resource utilization vector (pre-calculated on initialization) of a specific target.
 
         Args:
-            target: The target for which the KPI is calculated (a KPITarget value).
+            target: The target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: Minimal KPIs vector.
+        Returns: Minimal resource utilization vector.
 
         """
-        return self.min_kpi[target]
+        return self.min_ru[target]
 
-    def compute_node_kpi_for_candidate(self, conf_node_idx: int, candidate_idx: int, target: KPITarget) -> np.ndarray:
+    def compute_node_ru_for_candidate(self, conf_node_idx: int, candidate_idx: int, target: RUTarget) -> np.ndarray:
         """
-        Computes a KPIs vector after replacing the given node's configuration candidate in the minimal
+        Computes a resource utilization vector after replacing the given node's configuration candidate in the minimal
         target configuration with the given candidate index.
 
         Args:
             conf_node_idx: The index of a node in a sorted configurable nodes list.
-            candidate_idx: Quantization config candidate to be used for the node's KPI computation.
-            target: The target for which the KPI is calculated (a KPITarget value).
+            candidate_idx: Quantization config candidate to be used for the node's resource utilization computation.
+            target: The target for which the resource utilization is calculated (a RUTarget value).
 
-        Returns: Node's KPIs vector.
+        Returns: Node's resource utilization vector.
 
         """
-        return self.compute_kpi_functions[target][0](
+        return self.compute_ru_functions[target][0](
             self.replace_config_in_index(
-                self.min_kpi_config,
+                self.min_ru_config,
                 conf_node_idx,
                 candidate_idx),
             self.graph,
             self.fw_info,
             self.fw_impl)
 
     @staticmethod
@@ -232,72 +234,98 @@
         Returns: A new mixed-precision configuration.
 
         """
         updated_cfg = mp_cfg.copy()
         updated_cfg[idx] = value
         return updated_cfg
 
-    def _non_configurable_nodes_kpi(self) -> Dict[KPITarget, np.ndarray]:
+    def _non_configurable_nodes_ru(self) -> Dict[RUTarget, np.ndarray]:
         """
-        Computes a KPI vector of all non-configurable nodes in the given graph for each of the KPI target.
+        Computes a resource utilization vector of all non-configurable nodes in the given graph for each of the 
+        resource utilization targets.
 
-        Returns: A mapping between a KPITarget and its non-configurable nodes' KPI vector.
+        Returns: A mapping between a RUTarget and its non-configurable nodes' resource utilization vector.
         """
 
-        non_conf_kpi_dict = {}
-        for target, kpi_value in self.target_kpi.get_kpi_dict().items():
-            # Call for the KPI method of the given target - empty quantization configuration list is passed since we
+        non_conf_ru_dict = {}
+        for target, ru_value in self.target_resource_utilization.get_resource_utilization_dict().items():
+            # Call for the ru method of the given target - empty quantization configuration list is passed since we
             # compute for non-configurable nodes
-            if target == KPITarget.BOPS:
-                kpi_vector = None
+            if target == RUTarget.BOPS:
+                ru_vector = None
             else:
-                kpi_vector = self.compute_kpi_functions[target][0]([], self.graph, self.fw_info, self.fw_impl)
+                ru_vector = self.compute_ru_functions[target][0]([], self.graph, self.fw_info, self.fw_impl)
 
-            non_conf_kpi_dict[target] = kpi_vector
+            non_conf_ru_dict[target] = ru_vector
 
-        return non_conf_kpi_dict
+        return non_conf_ru_dict
 
-    def compute_kpi_for_config(self, config: List[int]) -> KPI:
+    def compute_resource_utilization_for_config(self, config: List[int]) -> ResourceUtilization:
         """
-        Computes the KPI values for a given mixed-precision configuration.
+        Computes the resource utilization values for a given mixed-precision configuration.
 
         Args:
             config: A mixed-precision configuration (list of candidates indices)
 
-        Returns: A KPI object with the model's KPI values when quantized with the given config.
+        Returns: A ResourceUtilization object with the model's resource utilization values when quantized 
+        with the given config.
 
         """
 
-        kpis_dict = {}
+        ru_dict = {}
 
-        for kpi_target, kpi_fns in self.compute_kpi_functions.items():
-            # Passing False to kpi methods and aggregations to indicates that the computations
+        for ru_target, ru_fns in self.compute_ru_functions.items():
+            # Passing False to ru methods and aggregations to indicates that the computations
             # are not for constraints setting
-            if kpi_target == KPITarget.BOPS:
-                configurable_nodes_kpi_vector = kpi_fns[0](config, self.original_graph, self.fw_info, self.fw_impl, False)
+            if ru_target == RUTarget.BOPS:
+                configurable_nodes_ru_vector = ru_fns[0](config, self.original_graph, self.fw_info, self.fw_impl, False)
             else:
-                configurable_nodes_kpi_vector = kpi_fns[0](config, self.original_graph, self.fw_info, self.fw_impl)
-            non_configurable_nodes_kpi_vector = self.non_conf_kpi_dict.get(kpi_target)
-            if non_configurable_nodes_kpi_vector is None or len(non_configurable_nodes_kpi_vector) == 0:
-                aggr_kpi = self.compute_kpi_functions[kpi_target][1](configurable_nodes_kpi_vector, False)
+                configurable_nodes_ru_vector = ru_fns[0](config, self.original_graph, self.fw_info, self.fw_impl)
+            non_configurable_nodes_ru_vector = self.non_conf_ru_dict.get(ru_target)
+            if non_configurable_nodes_ru_vector is None or len(non_configurable_nodes_ru_vector) == 0:
+                ru_ru = self.compute_ru_functions[ru_target][1](configurable_nodes_ru_vector, False)
             else:
-                aggr_kpi = self.compute_kpi_functions[kpi_target][1](
-                    np.concatenate([configurable_nodes_kpi_vector, non_configurable_nodes_kpi_vector]), False)
+                ru_ru = self.compute_ru_functions[ru_target][1](
+                    np.concatenate([configurable_nodes_ru_vector, non_configurable_nodes_ru_vector]), False)
+
+            ru_dict[ru_target] = ru_ru[0]
+
+        config_ru = ResourceUtilization()
+        config_ru.set_resource_utilization_by_target(ru_dict)
+        return config_ru
+
+    def finalize_distance_metric(self, layer_to_metrics_mapping: Dict[int, Dict[int, float]]):
+        """
+        Finalizing the distance metric building.
+        The method checks to see if the maximal distance value is larger than a given threshold, and if so,
+        it scales all metric values to prevent possible numerical issues.
+        Modification to the dictionary is done inplace.
 
-            kpis_dict[kpi_target] = aggr_kpi[0]
+        Args:
+            layer_to_metrics_mapping: A mapping between a node index to a mapping between
+            a bitwidth index to a distance value.
+
+        """
+        # normalize metric for numerical stability
 
-        config_kpi = KPI()
-        config_kpi.set_kpi_by_target(kpis_dict)
-        return config_kpi
+        max_dist = max([max([d for b, d in dists.items()]) for layer, dists in layer_to_metrics_mapping.items()])
+        if max_dist >= self.sensitivity_evaluator.quant_config.metric_normalization_threshold:
+            Logger.warning(f"The mixed precision distance metric values indicate a large error in the quantized model."
+                           f"this can cause numerical issues."
+                           f"The program will proceed with mixed precision search after scaling the metric values,"
+                           f"which can lead to unstable results.")
+            for layer, dists in layer_to_metrics_mapping.items():
+                for b, d in dists.items():
+                    layer_to_metrics_mapping[layer][b] /= max_dist
 
 
 class ConfigReconstructionHelper:
     """
     A class to help reconstruct an original mixed-precision configuration from a virtual one,
-    when running mixed-precision search with BOPS KPI.
+    when running mixed-precision search with BOPS utilization.
     It provides a reconstruct_config_from_virtual_graph which allows to translate a bit-width config of a virtual graph
     to a config of the original configurable nodes.
     """
 
     def __init__(self, virtual_graph: Graph, original_graph: Graph):
         """
         Init a ConfigReconstructionHelper object.
@@ -308,17 +336,18 @@
         Args:
             virtual_graph: The virtual graph.
             original_graph: The original graph.
         """
 
         self.virtual_graph = virtual_graph
         self.original_graph = original_graph
+        self.fw_info = original_graph.fw_info
 
-        self.virtual_sorted_nodes_names = self.virtual_graph.get_configurable_sorted_nodes_names()
-        self.origin_sorted_conf_nodes_names = self.original_graph.get_configurable_sorted_nodes_names()
+        self.virtual_sorted_nodes_names = self.virtual_graph.get_configurable_sorted_nodes_names(self.fw_info)
+        self.origin_sorted_conf_nodes_names = self.original_graph.get_configurable_sorted_nodes_names(self.fw_info)
 
         self.origin_node_idx_to_cfg = {}
 
     def _clear_reconstruction_dict(self):
         """
         Clears the origin_node_idx_to_cfg data structure.
         """
@@ -346,31 +375,30 @@
 
         Returns: A mixed-precision configuration (list of candidates indices) of the original graph.
 
         """
 
         if changed_virtual_nodes_idx is not None:
             if original_base_config is None:
-                Logger.critical("Must provide a base original config in order to run config reconstruction for partial"
-                                "set of nodes.")  # pragma: no cover
+                Logger.critical("To run config reconstruction for a partial set of nodes, a base original config must be provided.")  # pragma: no cover
 
             updated_virtual_nodes = \
-                [(idx, self.virtual_graph.get_configurable_sorted_nodes()[idx]) for idx in changed_virtual_nodes_idx]
+                [(idx, self.virtual_graph.get_configurable_sorted_nodes(self.fw_info)[idx]) for idx in changed_virtual_nodes_idx]
             # Iterating only over the virtual nodes that have updated config
             for virtual_node_idx, n in updated_virtual_nodes:
                 self.reconstruct_node_config(n, virtual_mp_cfg, virtual_node_idx)
             # Updating reconstructed config for all other nodes based on provided base_config
-            original_sorted_conf_nodes = self.original_graph.get_configurable_sorted_nodes()
+            original_sorted_conf_nodes = self.original_graph.get_configurable_sorted_nodes(self.fw_info)
             for i in range(len(original_base_config)):
                 if i not in list(self.origin_node_idx_to_cfg.keys()):
                     self.update_config_at_original_idx(n=original_sorted_conf_nodes[i],
                                                        origin_cfg_idx=original_base_config[i])
         else:
             # Reconstruct entire config
-            for virtual_node_idx, n in enumerate(self.virtual_graph.get_configurable_sorted_nodes()):
+            for virtual_node_idx, n in enumerate(self.virtual_graph.get_configurable_sorted_nodes(self.fw_info)):
                 self.reconstruct_node_config(n, virtual_mp_cfg, virtual_node_idx)
 
         res_config = [self.origin_node_idx_to_cfg[key] for key in sorted(self.origin_node_idx_to_cfg.keys())]
         self._clear_reconstruction_dict()
         return res_config
 
     def reconstruct_node_config(self,
@@ -389,17 +417,15 @@
         virtual_cfg_idx = virtual_mp_cfg[virtual_node_idx]
 
         if isinstance(n, VirtualActivationWeightsNode):
             weights_node = n.original_weights_node
             if isinstance(weights_node, VirtualSplitWeightsNode):
                 self.get_activation_for_split_weights(weights_node, n, virtual_cfg_idx, virtual_mp_cfg)
             else:
-                Logger.error(f"Virtual graph error - all weights nodes should be split to weights and activation nodes"
-                             f"in order to construct the virtual graph, but node {n.name} is not of type "
-                             f"VirtualSplitWeightsNode")  # pragma: no cover
+                Logger.critical(f"Virtual graph construction error: Expected all weights nodes to be split into weights and activation nodes. Found node '{n.name}' not split as expected. Every weights node should correspond to a VirtualSplitWeightsNode type.")  # pragma: no cover
 
             activation_node = n.original_activation_node
             if isinstance(activation_node, VirtualSplitActivationNode):
                 self.get_weights_for_split_activation(activation_node, n, virtual_cfg_idx, virtual_mp_cfg)
             else:
                 if activation_node.name in self.origin_sorted_conf_nodes_names:
                     # It is possible that the original activation node is not configurable,
@@ -412,23 +438,21 @@
             predecessor = self.virtual_graph.get_prev_nodes(n)
             assert len(predecessor) == 1  # Sanity check
             predecessor = predecessor[0]
             if len(self.virtual_graph.out_edges(predecessor)) > 1:
                 # It's ok, need to find the node's configuration
                 self.get_activation_for_split_weights(n, n, virtual_cfg_idx, virtual_mp_cfg)
             else:
-                Logger.error(f"Virtual graph error - a weights node is not composed with an activation node,"
-                             f"but its predecessor doesn't have multiple outputs.")  # pragma: no cover
+                Logger.critical(f"Virtual graph configuration error: Expected the predecessor of node '{n.name}' to have multiple outputs when not composed with an activation node.")  # pragma: no cover
         elif isinstance(n, VirtualSplitActivationNode):
             self.get_weights_for_split_activation(n, n, virtual_cfg_idx, virtual_mp_cfg)
         else:
             # Node didn't change in virtual graph - candidates list is similar to original
             if n.name not in self.origin_sorted_conf_nodes_names:
-                Logger.error(f"Node {n.name} appears in virtual graph as configurable, "
-                             f"but is not configurable in the original graph.")  # pragma: no cover
+                Logger.critical(f"Configuration mismatch: Node '{n.name}' is configurable in the virtual graph but not in the original graph. Verify node configurations.")  # pragma: no cover
             origin_idx = self.origin_sorted_conf_nodes_names.index(n.name)
             self.origin_node_idx_to_cfg[origin_idx] = virtual_cfg_idx
 
     def retrieve_weights_only_config(self, weights_node: BaseNode, virtual_node: BaseNode, virtual_cfg_idx: int):
         """
         Retrieves the configuration of an original weights configurable node based on a
         virtual weights configurable node's chosen config idx, and updates (inplace) the origin_cfg_idx mapping dict.
@@ -439,18 +463,20 @@
             virtual_node: The virtual weights configurable node.
             virtual_cfg_idx: The virtual node's chosen config index.
         """
 
         if weights_node.name in self.origin_sorted_conf_nodes_names:
             # It is possible that the original weights node is not configurable,
             # in this case we don't need to retrieve its bit-width config
-            weights_bitwidth = virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg.weights_n_bits
+            kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+            weights_bitwidth = (virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg
+                                .get_attr_config(kernel_attr).weights_n_bits)
             origin_cfg_idx = [i for i, c in
                               enumerate(weights_node.candidates_quantization_cfg) if
-                              c.weights_quantization_cfg.weights_n_bits == weights_bitwidth]
+                              c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_bitwidth]
 
             self.update_config_at_original_idx(weights_node, origin_cfg_idx[0])
 
     def retrieve_activation_only_config(self, activation_node: BaseNode, virtual_node: BaseNode, virtual_cfg_idx: int):
         """
         Retrieves the configuration of an original activation configurable node based on a
         virtual activation configurable node's chosen config idx, and updates (inplace) the origin_cfg_idx mapping dict.
@@ -491,19 +517,22 @@
             virtual_cfg_idx: The virtual node's chosen config index.
             virtual_mp_cfg: The virtual graph's chosen mp config.
         """
 
         activation_bitwidth = activation_node.candidates_quantization_cfg[virtual_mp_cfg[
             self.virtual_sorted_nodes_names.index(activation_node.name)]].activation_quantization_cfg.activation_n_bits
 
-        weights_bitwidth = virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg.weights_n_bits
+        kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+
+        weights_bitwidth = (virtual_node.candidates_quantization_cfg[virtual_cfg_idx].weights_quantization_cfg
+                            .get_attr_config(kernel_attr).weights_n_bits)
 
         origin_cfg_idx = [i for i, c in
                           enumerate(weights_node.origin_node.candidates_quantization_cfg) if
-                          c.weights_quantization_cfg.weights_n_bits == weights_bitwidth and
+                          c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_bitwidth and
                           c.activation_quantization_cfg.activation_n_bits == activation_bitwidth]
 
         self.update_config_at_original_idx(weights_node.origin_node, origin_cfg_idx[0])
 
     def retrieve_weights_activation_config(self,
                                            activation_node: BaseNode,
                                            weights_node: BaseNode,
@@ -519,22 +548,25 @@
             activation_node: The virtual node that contains the activation representation of an original node.
             weights_node: The virtual node that contains the weights that matches the activation node in the original graph.
             virtual_node: The virtual node that contains the virtual activation node (either a composed node or a split activation node).
             virtual_cfg_idx: The virtual node's chosen config index.
             virtual_mp_cfg: The virtual graph's chosen mp config.
         """
 
-        weights_bitwidth = weights_node.candidates_quantization_cfg[virtual_mp_cfg[
-            self.virtual_sorted_nodes_names.index(weights_node.name)]].weights_quantization_cfg.weights_n_bits
+        kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+
+        weights_bitwidth = (weights_node.candidates_quantization_cfg[virtual_mp_cfg[
+            self.virtual_sorted_nodes_names.index(weights_node.name)]]
+                            .weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits)
 
         activation_bitwidth = virtual_node.candidates_quantization_cfg[
             virtual_cfg_idx].activation_quantization_cfg.activation_n_bits
 
         origin_cfg_idx = [i for i, c in enumerate(activation_node.origin_node.candidates_quantization_cfg) if
-                          c.weights_quantization_cfg.weights_n_bits == weights_bitwidth and
+                          c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_bitwidth and
                           c.activation_quantization_cfg.activation_n_bits == activation_bitwidth]
 
         self.update_config_at_original_idx(activation_node.origin_node, origin_cfg_idx[0])
 
     def get_activation_for_split_weights(self,
                                          weights_node: BaseNode,
                                          virtual_node: BaseNode,
@@ -596,16 +628,17 @@
         # This is an activation node that was split, means it has a weights node that should come before it,
         # and we need its configuration in order to reconstruct the original node's configuration.
         matching_weights_node = self.virtual_graph.get_prev_nodes(virtual_node)
         assert len(matching_weights_node) == 1
         weights_node = matching_weights_node[0]
 
         if isinstance(weights_node, VirtualActivationWeightsNode):
-            if weights_node.original_weights_node.is_weights_quantization_enabled() and not \
-                    weights_node.original_weights_node.is_all_weights_candidates_equal():
+            kernel_attr = self.fw_info.get_kernel_op_attributes(weights_node.type)[0]
+            if weights_node.original_weights_node.is_weights_quantization_enabled(kernel_attr) and not \
+                    weights_node.original_weights_node.is_all_weights_candidates_equal(kernel_attr):
                 assert weights_node.name in self.virtual_sorted_nodes_names  # Sanity check
                 # The original node is both weights and activation configurable
                 self.retrieve_weights_activation_config(activation_node, weights_node, virtual_node, virtual_cfg_idx, virtual_mp_cfg)
             else:
                 # The original node is only activation configurable
                 # activation_node here is a split activation node therefore must have 'origin_node'
                 self.retrieve_activation_only_config(activation_node.origin_node, virtual_node, virtual_cfg_idx)
@@ -616,16 +649,15 @@
             predecessor = self.virtual_graph.get_prev_nodes(weights_node)
             assert len(predecessor) == 1  # Sanity check
             predecessor = predecessor[0]
             if len(self.virtual_graph.out_edges(predecessor)) > 1:
                 # It's ok, need to find the node's configuration
                 self.retrieve_weights_activation_config(activation_node, weights_node, virtual_node, virtual_cfg_idx, virtual_mp_cfg)
             else:
-                Logger.error(f"Virtual graph error - a weights node is not composed with an activation node,"
-                             f"but its predecessor doesn't have multiple outputs.")  # pragma: no cover
+                Logger.critical(f"Virtual graph configuration error: Expected the predecessor of node '{n.name}' to have multiple outputs when not composed with an activation node.")  # pragma: no cover
 
     def update_config_at_original_idx(self, n: BaseNode, origin_cfg_idx: int):
         """
         Updates (inplace) the origin_node_idx_to_cfg mapping wit hthe given index for a given original node index
         (in the original graph's sorted configurable nodes list).
 
         Args:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py`

 * *Files 26% similar despite different names*

```diff
@@ -15,59 +15,60 @@
 
 import numpy as np
 from pulp import *
 from tqdm import tqdm
 from typing import Dict, List, Tuple, Callable
 
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization, RUTarget
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import MixedPrecisionSearchManager
 
 # Limit ILP solver runtime in seconds
 SOLVER_TIME_LIMIT = 60
 
+
 def mp_integer_programming_search(search_manager: MixedPrecisionSearchManager,
-                                  target_kpi: KPI = None) -> List[int]:
+                                  target_resource_utilization: ResourceUtilization = None) -> List[int]:
     """
     Searching and returning a mixed-precision configuration using an ILP optimization solution.
     It first builds a mapping from each layer's index (in the model) to a dictionary that maps the
     bitwidth index to the observed sensitivity of the model when using that bitwidth for that layer.
     Then, it creates a mapping from each node's index (in the graph) to a dictionary
     that maps the bitwidth index to the contribution of configuring this node with this
-    bitwidth to the minimal possible KPI of the model.
+    bitwidth to the minimal possible resource utilization of the model.
     Then, and using these mappings, it builds an LP problem and finds an optimal solution.
     If a solution could not be found, exception is thrown.
 
     Args:
         search_manager: MixedPrecisionSearchManager object to be used for problem formalization.
-        target_kpi: KPI to constrain our LP problem with some resources limitations (like model' weights memory
+        target_resource_utilization: Target resource utilization to constrain our LP problem with some resources limitations (like model' weights memory
         consumption).
 
     Returns:
         The mixed-precision configuration (list of indices. Each indicates the bitwidth index of a node).
 
     """
 
     # Build a mapping from each layer's index (in the model) to a dictionary that maps the
     # bitwidth index to the observed sensitivity of the model when using that bitwidth for that layer.
 
-    if target_kpi is None or search_manager is None:
-        Logger.critical("Can't run mixed precision search with given target_kpi=None or search_manager=None."
-                        "Please provide a valid target_kpi and check the mixed precision parameters values.")
+    if target_resource_utilization is None or search_manager is None:
+        Logger.critical("Invalid parameters: 'target_resource_utilization' and 'search_manager' must not be 'None' "
+                        "for mixed-precision search. Ensure valid inputs are provided.")
 
-    layer_to_metrics_mapping = _build_layer_to_metrics_mapping(search_manager, target_kpi)
+    layer_to_metrics_mapping = _build_layer_to_metrics_mapping(search_manager, target_resource_utilization)
 
     # Init variables to find their values when solving the lp problem.
     layer_to_indicator_vars_mapping, layer_to_objective_vars_mapping = _init_problem_vars(layer_to_metrics_mapping)
 
     # Add all equations and inequalities that define the problem.
     lp_problem = _formalize_problem(layer_to_indicator_vars_mapping,
                                     layer_to_metrics_mapping,
                                     layer_to_objective_vars_mapping,
-                                    target_kpi,
+                                    target_resource_utilization,
                                     search_manager)
 
     # Use default PULP solver. Limit runtime in seconds
     solver = PULP_CBC_CMD(timeLimit=SOLVER_TIME_LIMIT)
     lp_problem.solve(solver=solver)  # Try to solve the problem.
 
     assert lp_problem.status == LpStatusOptimal, Logger.critical(
@@ -77,15 +78,15 @@
     # Take the bitwidth index only if its corresponding indicator is one.
     config = np.asarray(
         [[nbits for nbits, indicator in nbits_to_indicator.items() if indicator.varValue == 1.0] for
          nbits_to_indicator
          in layer_to_indicator_vars_mapping.values()]
     ).flatten()
 
-    if target_kpi.bops < np.inf:
+    if target_resource_utilization.bops < np.inf:
         return search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(config)
     else:
         return config
 
 
 def _init_problem_vars(layer_to_metrics_mapping: Dict[int, Dict[int, float]]) -> Tuple[
     Dict[int, Dict[int, LpVariable]], Dict[int, LpVariable]]:
@@ -118,28 +119,28 @@
 
     return layer_to_indicator_vars_mapping, layer_to_objective_vars_mapping
 
 
 def _formalize_problem(layer_to_indicator_vars_mapping: Dict[int, Dict[int, LpVariable]],
                        layer_to_metrics_mapping: Dict[int, Dict[int, float]],
                        layer_to_objective_vars_mapping: Dict[int, LpVariable],
-                       target_kpi: KPI,
+                       target_resource_utilization: ResourceUtilization,
                        search_manager: MixedPrecisionSearchManager) -> LpProblem:
     """
     Formalize the LP problem by defining all inequalities that define the solution space.
 
     Args:
         layer_to_indicator_vars_mapping: Dictionary that maps each node's index to a dictionary of bitwidth to
         indicator variable.
         layer_to_metrics_mapping: Dictionary that maps each node's index to a dictionary of bitwidth to sensitivity
         evaluation.
         layer_to_objective_vars_mapping: Dictionary that maps each node's index to a bitwidth variable we find its
         value.
-        target_kpi: KPI to reduce our feasible solution space.
-        search_manager: MixedPrecisionSearchManager object to be used for kpi constraints formalization.
+        target_resource_utilization: Target resource utilization to reduce our feasible solution space.
+        search_manager: MixedPrecisionSearchManager object to be used for resource utilization constraints formalization.
 
     Returns:
         The formalized LP problem.
     """
 
     lp_problem = LpProblem()  # minimization problem by default
     lp_problem += lpSum([layer_to_objective_vars_mapping[layer] for layer in
@@ -151,150 +152,160 @@
                              for nbits, indicator in layer_to_indicator_vars_mapping[layer].items()]) == \
                       layer_to_objective_vars_mapping[layer]
 
         # Constraint of only one indicator==1
         lp_problem += lpSum(
             [v for v in layer_to_indicator_vars_mapping[layer].values()]) == 1
 
-    # Bound the feasible solution space with the desired KPI.
-    # Creates separate constraints for weights KPI and activation KPI.
-    if target_kpi is not None:
+    # Bound the feasible solution space with the desired resource utilization values.
+    # Creates separate constraints for weights utilization and activation utilization.
+    if target_resource_utilization is not None:
         indicators = []
         for layer in layer_to_metrics_mapping.keys():
             for _, indicator in layer_to_indicator_vars_mapping[layer].items():
                 indicators.append(indicator)
 
         indicators_arr = np.array(indicators)
         indicators_matrix = np.diag(indicators_arr)
 
-        for target, kpi_value in target_kpi.get_kpi_dict().items():
-            if not np.isinf(kpi_value):
-                non_conf_kpi_vector = None if search_manager.non_conf_kpi_dict is None \
-                    else search_manager.non_conf_kpi_dict.get(target)
-                _add_set_of_kpi_constraints(search_manager=search_manager,
-                                            target=target,
-                                            target_kpi_value=kpi_value,
-                                            indicators_matrix=indicators_matrix,
-                                            lp_problem=lp_problem,
-                                            non_conf_kpi_vector=non_conf_kpi_vector)
+        for target, ru_value in target_resource_utilization.get_resource_utilization_dict().items():
+            if not np.isinf(ru_value):
+                non_conf_ru_vector = None if search_manager.non_conf_ru_dict is None \
+                    else search_manager.non_conf_ru_dict.get(target)
+                _add_set_of_ru_constraints(search_manager=search_manager,
+                                           target=target,
+                                           target_resource_utilization_value=ru_value,
+                                           indicators_matrix=indicators_matrix,
+                                           lp_problem=lp_problem,
+                                           non_conf_ru_vector=non_conf_ru_vector)
     else:  # pragma: no cover
-        raise Logger.critical("Can't run mixed-precision search with given target_kpi=None."
-                              "Please provide a valid target_kpi.")
+        Logger.critical("Unable to execute mixed-precision search: 'target_resource_utilization' is None. "
+                        "A valid 'target_resource_utilization' is required.")
     return lp_problem
 
 
-def _add_set_of_kpi_constraints(search_manager: MixedPrecisionSearchManager,
-                                target: KPITarget,
-                                target_kpi_value: float,
-                                indicators_matrix: np.ndarray,
-                                lp_problem: LpProblem,
-                                non_conf_kpi_vector: np.ndarray):
+def _add_set_of_ru_constraints(search_manager: MixedPrecisionSearchManager,
+                               target: RUTarget,
+                               target_resource_utilization_value: float,
+                               indicators_matrix: np.ndarray,
+                               lp_problem: LpProblem,
+                               non_conf_ru_vector: np.ndarray):
     """
-    Adding a constraint for the Lp problem for the given KPI target.
+    Adding a constraint for the Lp problem for the given target resource utilization.
     The update to the Lp problem object is done inplace.
 
     Args:
-        search_manager:  MixedPrecisionSearchManager object to be used for kpi constraints formalization.
-        target: A KPITarget.
-        target_kpi_value: Target KPI value of the given KPI target for which the constraint is added.
+        search_manager:  MixedPrecisionSearchManager object to be used for resource utilization constraints formalization.
+        target: A RUTarget.
+        target_resource_utilization_value: Target resource utilization value of the given target resource utilization
+        for which the constraint is added.
         indicators_matrix: A diagonal matrix of the Lp problem's indicators.
         lp_problem: An Lp problem object to add constraint to.
-        non_conf_kpi_vector: A non-configurable nodes' KPI vector.
+        non_conf_ru_vector: A non-configurable nodes' resource utilization vector.
 
     """
 
-    kpi_matrix = search_manager.compute_kpi_matrix(target)
-    indicated_kpi_matrix = np.matmul(kpi_matrix, indicators_matrix)
+    ru_matrix = search_manager.compute_resource_utilization_matrix(target)
+    indicated_ru_matrix = np.matmul(ru_matrix, indicators_matrix)
     # Need to re-organize the tensor such that the configurations' axis will be second,
     # and all metric values' axis will come afterword
-    indicated_kpi_matrix = np.moveaxis(indicated_kpi_matrix, source=len(indicated_kpi_matrix.shape) - 1, destination=1)
+    indicated_ru_matrix = np.moveaxis(indicated_ru_matrix, source=len(indicated_ru_matrix.shape) - 1, destination=1)
 
-    # In order to get the result KPI according to a chosen set of indicators, we sum each row in the result matrix.
-    # Each row represents the KPI values for a specific KPI metric, such that only elements corresponding
-    # to a configuration which implied by the set of indicators will have some positive value different than 0
-    # (and will contribute to the total KPI).
-    kpi_sum_vector = np.array([
-        np.sum(indicated_kpi_matrix[i], axis=0) +  # sum of metric values over all configurations in a row
-        search_manager.min_kpi[target][i] for i in range(indicated_kpi_matrix.shape[0])])
-
-    # search_manager.compute_kpi_functions contains a pair of kpi_metric and kpi_aggregation for each kpi target
-    # get aggregated KPI, considering both configurable and non-configurable nodes
-    if non_conf_kpi_vector is None or len(non_conf_kpi_vector) == 0:
-        aggr_kpi = search_manager.compute_kpi_functions[target][1](kpi_sum_vector)
+    # In order to get the result resource utilization according to a chosen set of indicators, we sum each row in
+    # the result matrix. Each row represents the resource utilization values for a specific resource utilization metric,
+    # such that only elements corresponding to a configuration which implied by the set of indicators will have some
+    # positive value different than 0 (and will contribute to the total resource utilization).
+    ru_sum_vector = np.array([
+        np.sum(indicated_ru_matrix[i], axis=0) +  # sum of metric values over all configurations in a row
+        search_manager.min_ru[target][i] for i in range(indicated_ru_matrix.shape[0])])
+
+    # search_manager.compute_ru_functions contains a pair of ru_metric and ru_aggregation for each ru target
+    # get aggregated ru, considering both configurable and non-configurable nodes
+    if non_conf_ru_vector is None or len(non_conf_ru_vector) == 0:
+        aggr_ru = search_manager.compute_ru_functions[target][1](ru_sum_vector)
     else:
-        aggr_kpi = search_manager.compute_kpi_functions[target][1](np.concatenate([kpi_sum_vector, non_conf_kpi_vector]))
+        aggr_ru = search_manager.compute_ru_functions[target][1](np.concatenate([ru_sum_vector, non_conf_ru_vector]))
 
-    for v in aggr_kpi:
+    for v in aggr_ru:
         if isinstance(v, float):
-            if v > target_kpi_value:
-                Logger.critical(f"The model can't be quantized to satisfy target KPI {target.value} with value {target_kpi_value}")  # pragma: no cover
+            if v > target_resource_utilization_value:
+                Logger.critical(
+                    f"The model cannot be quantized to meet the specified target resource utilization {target.value} "
+                    f"with the value {target_resource_utilization_value}.")  # pragma: no cover
         else:
-            lp_problem += v <= target_kpi_value
+            lp_problem += v <= target_resource_utilization_value
 
 
 def _build_layer_to_metrics_mapping(search_manager: MixedPrecisionSearchManager,
-                                    target_kpi: KPI) -> Dict[int, Dict[int, float]]:
+                                    target_resource_utilization: ResourceUtilization,
+                                    eps: float = EPS) -> Dict[int, Dict[int, float]]:
     """
     This function measures the sensitivity of a change in a bitwidth of a layer on the entire model.
     It builds a mapping from a node's index, to its bitwidht's effect on the model sensitivity.
     For each node and some possible node's bitwidth (according to the given search space), we use
     the framework function compute_metric_fn in order to infer
     a batch of images, and compute (using the inference results) the sensitivity metric of
     the configured mixed-precision model.
 
     Args:
         search_manager: MixedPrecisionSearchManager object to be used for problem formalization.
-        target_kpi: KPI to constrain our LP problem with some resources limitations (like model' weights memory
-        consumption).
+        target_resource_utilization: ResourceUtilization to constrain our LP problem with some resources limitations
+        (like model' weights memory consumption).
+        eps: Epsilon value to manually increase metric value (if necessary) for numerical stability
 
     Returns:
         Mapping from each node's index in a graph, to a dictionary from the bitwidth index (of this node) to
         the sensitivity of the model.
 
     """
 
     Logger.info('Starting to evaluate metrics')
     layer_to_metrics_mapping = {}
 
-    is_bops_target_kpi = target_kpi.bops < np.inf
+    is_bops_target_resource_utilization = target_resource_utilization.bops < np.inf
 
-    if is_bops_target_kpi:
-        origin_max_config = search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(search_manager.max_kpi_config)
+    if is_bops_target_resource_utilization:
+        origin_max_config = search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(search_manager.max_ru_config)
         max_config_value = search_manager.compute_metric_fn(origin_max_config)
     else:
-        max_config_value = search_manager.compute_metric_fn(search_manager.max_kpi_config)
+        max_config_value = search_manager.compute_metric_fn(search_manager.max_ru_config)
 
     for node_idx, layer_possible_bitwidths_indices in tqdm(search_manager.layer_to_bitwidth_mapping.items(),
                                                            total=len(search_manager.layer_to_bitwidth_mapping)):
         layer_to_metrics_mapping[node_idx] = {}
 
         for bitwidth_idx in layer_possible_bitwidths_indices:
-            if search_manager.max_kpi_config[node_idx] == bitwidth_idx:
+            if search_manager.max_ru_config[node_idx] == bitwidth_idx:
                 # This is a computation of the metric for the max configuration, assign pre-calculated value
                 layer_to_metrics_mapping[node_idx][bitwidth_idx] = max_config_value
                 continue
 
             # Create a configuration that differs at one layer only from the baseline model
-            mp_model_configuration = search_manager.max_kpi_config.copy()
+            mp_model_configuration = search_manager.max_ru_config.copy()
             mp_model_configuration[node_idx] = bitwidth_idx
 
             # Build a distance matrix using the function we got from the framework implementation.
-            if is_bops_target_kpi:
+            if is_bops_target_resource_utilization:
                 # Reconstructing original graph's configuration from virtual graph's configuration
                 origin_mp_model_configuration = \
                     search_manager.config_reconstruction_helper.reconstruct_config_from_virtual_graph(
                         mp_model_configuration,
                         changed_virtual_nodes_idx=[node_idx],
                         original_base_config=origin_max_config)
                 origin_changed_nodes_indices = [i for i, c in enumerate(origin_max_config) if
                                                 c != origin_mp_model_configuration[i]]
-                layer_to_metrics_mapping[node_idx][bitwidth_idx] = search_manager.compute_metric_fn(
+                metric_value = search_manager.compute_metric_fn(
                     origin_mp_model_configuration,
                     origin_changed_nodes_indices,
                     origin_max_config)
             else:
-                layer_to_metrics_mapping[node_idx][bitwidth_idx] = search_manager.compute_metric_fn(
+                metric_value = search_manager.compute_metric_fn(
                     mp_model_configuration,
                     [node_idx],
-                    search_manager.max_kpi_config)
+                    search_manager.max_ru_config)
+
+            layer_to_metrics_mapping[node_idx][bitwidth_idx] = max(metric_value, max_config_value + eps)
+
+    # Finalize distance metric mapping
+    search_manager.finalize_distance_metric(layer_to_metrics_mapping)
 
     return layer_to_metrics_mapping
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,38 +11,44 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 
 import numpy as np
-from typing import Callable, Any, List
+from typing import Callable, Any, List, Tuple
 
 from model_compression_toolkit.constants import AXIS
-from model_compression_toolkit.core import FrameworkInfo, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import FrameworkInfo, MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
+from model_compression_toolkit.core.common.similarity_analyzer import compute_kl_divergence
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianMode, \
+    HessianInfoGranularity, HessianInfoService
+from model_compression_toolkit.core.common.hessian import hessian_info_utils as hessian_utils
 
 
 class SensitivityEvaluation:
     """
     Class to wrap and manage the computation on distance metric for Mixed-Precision quantization search.
     It provides a function that evaluates the sensitivity of a bit-width configuration for the MP model.
     """
 
     def __init__(self,
                  graph: Graph,
-                 quant_config: MixedPrecisionQuantizationConfigV2,
+                 quant_config: MixedPrecisionQuantizationConfig,
                  representative_data_gen: Callable,
                  fw_info: FrameworkInfo,
                  fw_impl: Any,
                  set_layer_to_bitwidth: Callable,
-                 disable_activation_for_metric: bool = False):
+                 disable_activation_for_metric: bool = False,
+                 hessian_info_service: HessianInfoService = None
+                 ):
         """
         Initiates all relevant objects to manage a sensitivity evaluation for MP search.
         Create an object that allows to compute the sensitivity metric of an MP model (the sensitivity
         is computed based on the similarity of the interest points' outputs between the MP model
         and the float model).
         First, we initiate a SensitivityEvaluationManager that handles the components which are necessary for
         evaluating the sensitivity. It initializes an MP model (a model where layers that can be configured in
@@ -56,39 +62,50 @@
                 (e.g., attributes of different layers' weights to quantize).
             quant_config: MP Quantization configuration for how the graph should be quantized.
             representative_data_gen: Dataset used for getting batches for inference.
             fw_impl: FrameworkImplementation object with a specific framework methods implementation.
             set_layer_to_bitwidth: A fw-dependent function that allows to configure a configurable MP model
                     with a specific bit-width configuration.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch Hessian traces approximations.
+
         """
         self.graph = graph
         self.quant_config = quant_config
         self.representative_data_gen = representative_data_gen
         self.fw_info = fw_info
         self.fw_impl = fw_impl
         self.set_layer_to_bitwidth = set_layer_to_bitwidth
         self.disable_activation_for_metric = disable_activation_for_metric
+        if self.quant_config.use_hessian_based_scores:
+            if not isinstance(hessian_info_service, HessianInfoService):
+                Logger.critical(f"When using Hessian-based approximations for sensitivity evaluation, a valid HessianInfoService object is required; found {type(hessian_info_service)}.")
+            self.hessian_info_service = hessian_info_service
+
+        self.sorted_configurable_nodes_names = graph.get_configurable_sorted_nodes_names(self.fw_info)
 
-        # Get interest points for distance measurement and a list of sorted configurable nodes names
-        self.sorted_configurable_nodes_names = graph.get_configurable_sorted_nodes_names()
+        # Get interest points and output points set for distance measurement and set other helper datasets
+        # We define a separate set of output nodes of the model for the purpose of sensitivity computation.
         self.interest_points = get_mp_interest_points(graph,
                                                       fw_impl.count_node_for_mixed_precision_interest_points,
                                                       quant_config.num_interest_points_factor)
 
-        self.outputs_replacement_nodes = None
-        self.output_nodes_indices = None
-        if self.quant_config.use_grad_based_weights is True:
-            # Getting output replacement (if needed) - if a model's output layer is not compatible for the task of
-            # gradients computation then we find a predecessor layer which is compatible,
-            # add it to the set of interest points and use it for the gradients' computation.
-            # Note that we need to modify the set of interest points before building the models,
-            # therefore, it is separated from the part where we compute the actual gradient weights.
-            self.outputs_replacement_nodes = get_output_replacement_nodes(graph, fw_impl)
-            self.output_nodes_indices = self._update_ips_with_outputs_replacements()
+        self.ips_distance_fns, self.ips_axis = self._init_metric_points_lists(self.interest_points)
+
+        self.output_points = get_output_nodes_for_metric(graph)
+        self.out_ps_distance_fns, self.out_ps_axis = self._init_metric_points_lists(self.output_points)
+
+        # Setting lists with relative position of the interest points
+        # and output points in the list of all mp model activation tensors
+        graph_sorted_nodes = self.graph.get_topo_sorted_nodes()
+        all_out_tensors_indices = [graph_sorted_nodes.index(n) for n in self.interest_points + self.output_points]
+        global_ipts_indices = [graph_sorted_nodes.index(n) for n in self.interest_points]
+        global_out_pts_indices = [graph_sorted_nodes.index(n) for n in self.output_points]
+        self.ips_act_indices = [all_out_tensors_indices.index(i) for i in global_ipts_indices]
+        self.out_ps_act_indices = [all_out_tensors_indices.index(i) for i in global_out_pts_indices]
 
         # Build a mixed-precision model which can be configured to use different bitwidth in different layers.
         # And a baseline model.
         # Also, returns a mapping between a configurable graph's node and its matching layer(s)
         # in the new built MP model.
         self.baseline_model, self.model_mp, self.conf_node2layers = self._build_models()
 
@@ -100,24 +117,46 @@
 
         # Casting images tensors to the framework tensor type.
         self.images_batches = list(map(lambda in_arr: self.fw_impl.to_tensor(in_arr), self.images_batches))
 
         # Initiating baseline_tensors_list since it is not initiated in SensitivityEvaluationManager init.
         self._init_baseline_tensors_list()
 
-        # Computing gradient-based weights for weighted average distance metric computation (only if requested),
+        # Computing Hessian-based scores for weighted average distance metric computation (only if requested),
         # and assigning distance_weighting method accordingly.
-        self.interest_points_gradients = None
-        if self.quant_config.use_grad_based_weights is True:
-            assert self.outputs_replacement_nodes is not None and self.output_nodes_indices is not None, \
-                f"{self.outputs_replacement_nodes} and {self.output_nodes_indices} " \
-                f"should've been assigned before computing the gradient-based weights."
+        self.interest_points_hessians = None
+        if self.quant_config.use_hessian_based_scores is True:
+            self.interest_points_hessians = self._compute_hessian_based_scores()
+            self.quant_config.distance_weighting_method = lambda d: self.interest_points_hessians
+
+    def _init_metric_points_lists(self, points: List[BaseNode]) -> Tuple[List[Callable], List[int]]:
+        """
+        Initiates required lists for future use when computing the sensitivity metric.
+        Each point on which the metric is computed uses a dedicated distance function based on its type.
+        In addition, all distance functions preform batch computation. Axis is needed only for KL Divergence computation.
 
-            self.interest_points_gradients = self._compute_gradient_based_weights()
-            self.quant_config.distance_weighting_method = lambda d: self.interest_points_gradients
+        Args:
+            points: The set of nodes in the graph for which we need to initiate the lists.
+
+        Returns: A lists with distance functions and an axis list for each node.
+
+        """
+        distance_fns_list = []
+        axis_list = []
+        for n in points:
+            axis = n.framework_attr.get(AXIS) if not isinstance(n, FunctionalNode) else n.op_call_kwargs.get(AXIS)
+            distance_fn = self.fw_impl.get_node_distance_fn(
+                layer_class=n.layer_class,
+                framework_attrs=n.framework_attr,
+                compute_distance_fn=self.quant_config.compute_distance_fn,
+                axis=axis)
+            distance_fns_list.append(distance_fn)
+            # Axis is needed only for KL Divergence calculation, otherwise we use per-tensor computation
+            axis_list.append(axis if distance_fn==compute_kl_divergence else None)
+        return distance_fns_list, axis_list
 
     def compute_metric(self,
                        mp_model_configuration: List[int],
                        node_idx: List[int] = None,
                        baseline_mp_configuration: List[int] = None) -> float:
         """
         Compute the sensitivity metric of the MP model for a given configuration (the sensitivity
@@ -134,23 +173,24 @@
             The sensitivity metric of the MP model for a given configuration.
         """
 
         # Configure MP model with the given configuration.
         self._configure_bitwidths_model(mp_model_configuration,
                                         node_idx)
 
-        # Compute the distance matrix
-        distance_matrix = self._build_distance_matrix()
+        # Compute the distance metric
+        ipts_distances, out_pts_distances = self._compute_distance()
 
         # Configure MP model back to the same configuration as the baseline model if baseline provided
         if baseline_mp_configuration is not None:
             self._configure_bitwidths_model(baseline_mp_configuration,
                                             node_idx)
 
-        return self._compute_mp_distance_measure(distance_matrix, self.quant_config.distance_weighting_method)
+        return self._compute_mp_distance_measure(ipts_distances, out_pts_distances,
+                                                 self.quant_config.distance_weighting_method)
 
     def _init_baseline_tensors_list(self):
         """
         Evaluates the baseline model on all images and saves the obtained lists of tensors in a list for later use.
         Initiates a class variable self.baseline_tensors_list
         """
         self.baseline_tensors_list = [self.fw_impl.to_numpy(self.fw_impl.sensitivity_eval_inference(self.baseline_model,
@@ -171,48 +211,73 @@
         if self.disable_activation_for_metric:
             for n in evaluation_graph.get_topo_sorted_nodes():
                 for c in n.candidates_quantization_cfg:
                     c.activation_quantization_cfg.enable_activation_quantization = False
 
         model_mp, _, conf_node2layers = self.fw_impl.model_builder(evaluation_graph,
                                                                    mode=ModelBuilderMode.MIXEDPRECISION,
-                                                                   append2output=self.interest_points,
+                                                                   append2output=self.interest_points + self.output_points,
                                                                    fw_info=self.fw_info)
 
         # Build a baseline model.
         baseline_model, _ = self.fw_impl.model_builder(evaluation_graph,
                                                        mode=ModelBuilderMode.FLOAT,
-                                                       append2output=self.interest_points)
+                                                       append2output=self.interest_points + self.output_points)
 
         return baseline_model, model_mp, conf_node2layers
 
-    def _compute_gradient_based_weights(self) -> np.ndarray:
+    def _compute_hessian_based_scores(self) -> np.ndarray:
         """
-        Computes the gradient-based weights using the framework's model_grad method per batch of images.
+        Compute Hessian-based scores for each interest point.
+
+        Returns: A vector of scores, one for each interest point,
+         to be used for the distance metric weighted average computation.
 
-        Returns: A vector of weights, one for each interest point,
-        to be used for the distance metric weighted average computation.
         """
+        # Dictionary to store the trace Hessian approximations for each interest point (target node)
+        compare_point_to_trace_hessian_approximations = {}
+
+        # Iterate over each interest point to fetch the trace Hessian approximations
+        for target_node in self.interest_points:
+            # Create a request for trace Hessian approximation with specific configurations
+            # (here we use per-tensor approximation of the Hessian's trace w.r.t the node's activations)
+            trace_hessian_request = TraceHessianRequest(mode=HessianMode.ACTIVATION,
+                                                        granularity=HessianInfoGranularity.PER_TENSOR,
+                                                        target_node=target_node)
+
+            # Fetch the trace Hessian approximations for the current interest point
+            node_approximations = self.hessian_info_service.fetch_hessian(trace_hessian_request=trace_hessian_request,
+                                                                          required_size=self.quant_config.num_of_images)
+            # Store the fetched approximations in the dictionary
+            compare_point_to_trace_hessian_approximations[target_node] = node_approximations
+
+        # List to store the approximations for each image
+        approx_by_image = []
+        # Iterate over each image
+        for image_idx in range(self.quant_config.num_of_images):
+            # List to store approximations for the current image for each interest point
+            approx_by_image_per_interest_point = []
+            # Iterate over each interest point to gather approximations
+            for target_node in self.interest_points:
+                # Ensure the approximation for the current interest point and image is a list
+                assert isinstance(compare_point_to_trace_hessian_approximations[target_node][image_idx], list)
+                # Ensure the approximation list contains only one element (since, granularity is per-tensor)
+                assert len(compare_point_to_trace_hessian_approximations[target_node][image_idx]) == 1
+                # Append the single approximation value to the list for the current image
+                approx_by_image_per_interest_point.append(compare_point_to_trace_hessian_approximations[target_node][image_idx][0])
+
+            if self.quant_config.norm_scores:
+                approx_by_image_per_interest_point = \
+                    hessian_utils.normalize_scores(hessian_approximations=approx_by_image_per_interest_point)
+
+            # Append the approximations for the current image to the main list
+            approx_by_image.append(approx_by_image_per_interest_point)
 
-        grad_per_batch = []
-        for images in self.images_batches:
-            batch_ip_gradients = []
-            for i in range(1, images[0].shape[0] + 1):
-                Logger.info(f"Computing Jacobian-based weights approximation for image sample {i} out of {images[0].shape[0]}...")
-                image_ip_gradients = self.fw_impl.model_grad(self.graph,
-                                                             {inode: images[0][i - 1:i] for inode in
-                                                              self.graph.get_inputs()},
-                                                             self.interest_points,
-                                                             self.outputs_replacement_nodes,
-                                                             self.output_nodes_indices,
-                                                             self.quant_config.output_grad_factor,
-                                                             norm_weights=self.quant_config.norm_weights)
-                batch_ip_gradients.append(image_ip_gradients)
-            grad_per_batch.append(np.mean(batch_ip_gradients, axis=0))
-        return np.mean(grad_per_batch, axis=0)
+        # Return the mean approximation value across all images for each interest point
+        return np.mean(approx_by_image, axis=0)
 
     def _configure_bitwidths_model(self,
                                    mp_model_configuration: List[int],
                                    node_idx: List[int]):
         """
         Configure a dynamic model (namely, model with layers that their weights and activation
         bit-width can be configured) using an MP model configuration mp_model_configuration.
@@ -250,94 +315,114 @@
 
         Returns:
 
         """
         node_name = sorted_configurable_nodes_names[node_idx_to_configure]
         layers_to_config = self.conf_node2layers.get(node_name, None)
         if layers_to_config is None:
-            Logger.error(
-                f"Couldn't find matching layers in the MP model for node {node_name}.")  # pragma: no cover
+            Logger.critical(f"Matching layers for node {node_name} not found in the mixed precision model configuration.")  # pragma: no cover
 
         for current_layer in layers_to_config:
             self.set_layer_to_bitwidth(current_layer, mp_model_configuration[node_idx_to_configure])
 
-    def _compute_distance_matrix(self,
+    def _compute_points_distance(self,
                                  baseline_tensors: List[Any],
-                                 mp_tensors: List[Any]):
+                                 mp_tensors: List[Any],
+                                 points_distance_fns: List[Callable],
+                                 points_axis: List[int]):
         """
-        Compute the distance between the MP model's outputs and the baseline model's outputs
+        Compute the distance on the given set of points outputs between the MP model and the baseline model
         for each image in the batch that was inferred.
+
         Args:
-            baseline_tensors: Baseline model's output tensors.
-            mp_tensors: MP model's output tensors.
+            baseline_tensors: Baseline model's output tensors of the given points.
+            mp_tensors: MP model's output tensors pf the given points.
+            points_distance_fns: A list with distance function to compute the distance between each given
+                point's output tensors.
+            points_axis: A list with the matching axis of each given point's output tensors.
+
         Returns:
-            A distance matrix that maps each node's index to the distance between this node's output
+            A distance vector that maps each node's index in the given nodes list to the distance between this node's output
              and the baseline model's output for all images that were inferred.
         """
 
-        assert len(baseline_tensors) == len(self.interest_points)
-        num_interest_points = len(baseline_tensors)
-        num_samples = len(baseline_tensors[0])
-        distance_matrix = np.ndarray((num_interest_points, num_samples))
-
-        for i in range(num_interest_points):
-            point_node = self.interest_points[i]
-            point_distance_fn = \
-                self.fw_impl.get_node_distance_fn(layer_class=point_node.layer_class,
-                                                  framework_attrs=point_node.framework_attr,
-                                                  compute_distance_fn=self.quant_config.compute_distance_fn)
-
-            axis = point_node.framework_attr.get(AXIS) if not isinstance(point_node, FunctionalNode) \
-                else point_node.op_call_kwargs.get(AXIS)
-
-            distance_matrix[i] = point_distance_fn(baseline_tensors[i], mp_tensors[i], batch=True, axis=axis)
+        distance_v = [fn(x, y, batch=True, axis=axis) for fn, x, y, axis
+                      in zip(points_distance_fns, baseline_tensors, mp_tensors, points_axis)]
 
-        return distance_matrix
+        return np.asarray(distance_v)
 
-    def _build_distance_matrix(self):
+    def _compute_distance(self) -> Tuple[np.ndarray, np.ndarray]:
         """
-        Builds a matrix that contains the distances between the baseline and MP models for each interest point.
-        Returns: A distance matrix.
+        Computing the interest points distance and the output points distance, and using them to build a
+        unified distance vector.
+
+        Returns: A distance vector.
         """
-        # List of distance matrices. We create a distance matrix for each sample from the representative_data_gen
-        # and merge all of them eventually.
-        distance_matrices = []
+
+        ipts_per_batch_distance = []
+        out_pts_per_batch_distance = []
 
         # Compute the distance matrix for num_of_images images.
         for images, baseline_tensors in zip(self.images_batches, self.baseline_tensors_list):
             # when using model.predict(), it does not use the QuantizeWrapper functionality
             mp_tensors = self.fw_impl.sensitivity_eval_inference(self.model_mp, images)
             mp_tensors = self.fw_impl.to_numpy(mp_tensors)
 
-            # Build distance matrix: similarity between the baseline model to the float model
+            # Compute distance: similarity between the baseline model to the float model
             # in every interest point for every image in the batch.
-            distance_matrices.append(self._compute_distance_matrix(baseline_tensors, mp_tensors))
+            ips_distance = self._compute_points_distance([baseline_tensors[i] for i in self.ips_act_indices],
+                                                         [mp_tensors[i] for i in self.ips_act_indices],
+                                                         self.ips_distance_fns,
+                                                         self.ips_axis)
+            outputs_distance = self._compute_points_distance([baseline_tensors[i] for i in self.out_ps_act_indices],
+                                                             [mp_tensors[i] for i in self.out_ps_act_indices],
+                                                             self.out_ps_distance_fns,
+                                                             self.out_ps_axis)
+
+            # Extending the dimensions for the concatenation at the end in case we need to
+            ips_distance = ips_distance if len(ips_distance.shape) > 1 else ips_distance[:, None]
+            outputs_distance = outputs_distance if len(outputs_distance.shape) > 1 else outputs_distance[:, None]
+            ipts_per_batch_distance.append(ips_distance)
+            out_pts_per_batch_distance.append(outputs_distance)
 
         # Merge all distance matrices into a single distance matrix.
-        distance_matrix = np.concatenate(distance_matrices, axis=1)
+        ipts_distances = np.concatenate(ipts_per_batch_distance, axis=1)
+        out_pts_distances = np.concatenate(out_pts_per_batch_distance, axis=1)
 
-        return distance_matrix
+        return ipts_distances, out_pts_distances
 
     @staticmethod
-    def _compute_mp_distance_measure(distance_matrix: np.ndarray, metrics_weights_fn: Callable) -> float:
+    def _compute_mp_distance_measure(ipts_distances: np.ndarray,
+                                     out_pts_distances: np.ndarray,
+                                     metrics_weights_fn: Callable) -> float:
         """
         Computes the final distance value out of a distance matrix.
 
         Args:
-            distance_matrix: A matrix that contains the distances between the baseline and MP models
+            ipts_distances: A matrix that contains the distances between the baseline and MP models
                 for each interest point.
-            metrics_weights_fn:
+            out_pts_distances: A matrix that contains the distances between the baseline and MP models
+                for each output point.
+            metrics_weights_fn: A callable that produces the scores to compute weighted distance for interest points.
 
         Returns: Distance value.
         """
-        # Compute the distance between the baseline model's outputs and the MP model's outputs.
-        # The distance is the mean of distances over all images in the batch that was inferred.
-        mean_distance_per_layer = distance_matrix.mean(axis=1)
-        # Use weights such that every layer's distance is weighted differently (possibly).
-        return np.average(mean_distance_per_layer, weights=metrics_weights_fn(distance_matrix))
+        mean_ipts_distance = 0
+        if len(ipts_distances) > 0:
+            mean_distance_per_layer = ipts_distances.mean(axis=1)
+
+            # Use weights such that every layer's distance is weighted differently (possibly).
+            mean_ipts_distance = np.average(mean_distance_per_layer, weights=metrics_weights_fn(ipts_distances))
+
+        mean_output_distance = 0
+        if len(out_pts_distances) > 0:
+            mean_distance_per_output = out_pts_distances.mean(axis=1)
+            mean_output_distance = np.average(mean_distance_per_output)
+
+        return mean_output_distance + mean_ipts_distance
 
     def _get_images_batches(self, num_of_images: int) -> List[Any]:
         """
         Construct batches of image samples for inference.
 
         Args:
             num_of_images: Num of total images for evaluation.
@@ -363,45 +448,22 @@
             samples_count += batch_size
         else:
             if samples_count < num_of_images:
                 Logger.warning(f'Not enough images in representative dataset to generate {num_of_images} data points, '
                                f'only {samples_count} were generated')
         return images_batches
 
-    def _update_ips_with_outputs_replacements(self):
-        """
-        Updates the list of interest points with the set of pre-calculated replacement outputs.
-        Also, returns the indices of all output nodes (original, replacements and nodes in between them) in a
-        topological sorted interest points list (for later use in gradients computation and normalization).
-
-        Returns: A list of indices of the output nodes in the sorted interest points list.
-
-        """
-
-        assert self.outputs_replacement_nodes is not None, \
-            "Trying to update interest points list with new output nodes but outputs_replacement_nodes list is None."
-
-        replacement_outputs_to_ip = [r_node for r_node in self.outputs_replacement_nodes if
-                                     r_node not in self.interest_points]
-        updated_interest_points = self.interest_points + replacement_outputs_to_ip
-
-        # Re-sort interest points in a topological order according to the graph's sort
-        self.interest_points = [n for n in self.graph.get_topo_sorted_nodes() if n in updated_interest_points]
-
-        output_indices = [self.interest_points.index(n.node) for n in self.graph.get_outputs()]
-        replacement_indices = [self.interest_points.index(n) for n in self.outputs_replacement_nodes]
-        return list(set(output_indices + replacement_indices))
-
 
 def get_mp_interest_points(graph: Graph,
                            interest_points_classifier: Callable,
                            num_ip_factor: float) -> List[BaseNode]:
     """
     Gets a list of interest points for the mixed precision metric computation.
-    The list is constructed from a filtered set of the convolutions nodes in the graph.
+    The list is constructed from a filtered set of nodes in the graph.
+    Note that the output layers are separated from the interest point set for metric computation purposes.
 
     Args:
         graph: Graph to search for its MP configuration.
         interest_points_classifier: A function that indicates whether a given node in considered as a potential
             interest point for mp metric computation purposes.
         num_ip_factor: Percentage out of the total set of interest points that we want to actually use.
 
@@ -409,47 +471,38 @@
 
     """
     sorted_nodes = graph.get_topo_sorted_nodes()
     ip_nodes = list(filter(lambda n: interest_points_classifier(n), sorted_nodes))
 
     interest_points_nodes = bound_num_interest_points(ip_nodes, num_ip_factor)
 
-    # We add output layers of the model to interest points
-    # in order to consider the model's output in the distance metric computation (and also to make sure
-    # all configurable layers are included in the configured mp model for metric computation purposes)
-    output_nodes = [n.node for n in graph.get_outputs() if n.node not in interest_points_nodes]
-    interest_points = interest_points_nodes + output_nodes
+    # We exclude output nodes from the set of interest points since they are used separately in the sensitivity evaluation.
+    output_nodes = [n.node for n in graph.get_outputs()]
+
+    interest_points = [n for n in interest_points_nodes if n not in output_nodes]
 
     return interest_points
 
 
-def get_output_replacement_nodes(graph: Graph,
-                                 fw_impl: Any) -> List[BaseNode]:
+def get_output_nodes_for_metric(graph: Graph) -> List[BaseNode]:
     """
-    If a model's output node is not compatible for the task of gradients computation we need to find a predecessor
-    node in the model's graph representation which is compatible and add it to the set of interest points and use it
-    for the gradients' computation. This method searches for this predecessor node for each output of the model.
+    Returns a list of output nodes that are also quantized (either kernel weights attribute or activation)
+    to be used as a set of output points in the distance metric computation.
 
     Args:
-        graph: Graph to search for replacement output nodes.
-        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        graph: Graph to search for its MP configuration.
 
-    Returns: A list of output replacement nodes.
+    Returns: A list of output nodes.
 
     """
-    replacement_outputs = []
-    for n in graph.get_outputs():
-        prev_node = n.node
-        while not fw_impl.is_node_compatible_for_metric_outputs(prev_node):
-            prev_node = graph.get_prev_nodes(n.node)
-            assert len(prev_node) == 1, "A none MP compatible output node has multiple inputs, " \
-                                        "which is incompatible for metric computation."
-            prev_node = prev_node[0]
-        replacement_outputs.append(prev_node)
-    return replacement_outputs
+
+    return [n.node for n in graph.get_outputs()
+            if (graph.fw_info.is_kernel_op(n.node.type) and
+                n.node.is_weights_quantization_enabled(graph.fw_info.get_kernel_op_attributes(n.node.type)[0])) or
+            n.node.is_activation_quantization_enabled()]
 
 
 def bound_num_interest_points(sorted_ip_list: List[BaseNode], num_ip_factor: float) -> List[BaseNode]:
     """
     Filters the list of interest points and returns a shorter list with number of interest points smaller than some
     default threshold.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/set_layer_to_bitwidth.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,113 +11,136 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import List
 
-from model_compression_toolkit.core import KPI
+from model_compression_toolkit.core import ResourceUtilization
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import \
     MixedPrecisionSearchManager
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 from model_compression_toolkit.logger import Logger
 import numpy as np
 
 
 def greedy_solution_refinement_procedure(mp_solution: List[int],
                                          search_manager: MixedPrecisionSearchManager,
-                                         target_kpi: KPI) -> List[int]:
+                                         target_resource_utilization: ResourceUtilization) -> List[int]:
     """
     A greedy procedure to try and improve a mixed-precision solution that was found by a mixed-precision optimization
     algorithm.
     This procedure tries to increase the bit-width precision of configurable nodes that did not get the maximal
     candidate
     in the found solution.
-    It iteratively goes over all such nodes, computes the KPI values on a modified configuration (with the node's next
-    best candidate), filters out all configs that hold the KPI constraints and chooses one of them as an improvement
+    It iteratively goes over all such nodes, computes the resource utilization values on a modified configuration (with the node's next
+    best candidate), filters out all configs that hold the resource utilization constraints and chooses one of them as an improvement
     step
-    The choice is done in a greedy approach where we take the configuration that modifies the KPI the least.
+    The choice is done in a greedy approach where we take the configuration that modifies the resource utilization the least.
 
     Args:
         mp_solution: A mixed-precision configuration that was found by a mixed-precision optimization algorithm.
         search_manager: A MixedPrecisionSearchManager object.
-        target_kpi: The target KPIs for the mixed-precision search.
+        target_resource_utilization: The target resource utilization for the mixed-precision search.
 
     Returns: A new, possibly updated, mixed-precision bit-width configuration.
 
     """
-    # Refinement is not supported for BOPs KPI for now...
-    if target_kpi.bops < np.inf:
-        Logger.info(f'Target KPI constraint BOPs - Skipping MP greedy solution refinement')
+    # Refinement is not supported for BOPs utilization for now...
+    if target_resource_utilization.bops < np.inf:
+        Logger.info(f'Target resource utilization constraint BOPs - Skipping MP greedy solution refinement')
         return mp_solution
 
     new_solution = mp_solution.copy()
     changed = True
 
     while changed:
         changed = False
-        nodes_kpis = {}
+        nodes_ru = {}
         nodes_next_candidate = {}
 
         for node_idx in range(len(mp_solution)):
             if new_solution[node_idx] == 0:
                 # layer has max config in the given solution, nothing to optimize
                 continue
 
-            node_candidates = search_manager.graph.get_configurable_sorted_nodes()[node_idx].candidates_quantization_cfg
-            valid_candidates = _get_valid_candidates_indices(node_candidates, new_solution[node_idx])
+            current_node = search_manager.graph.get_configurable_sorted_nodes(search_manager.fw_info)[node_idx]
+            node_candidates = current_node.candidates_quantization_cfg
 
-            # Create a list of KPIs for the valid candidates.
-            updated_kpis = []
+            # only weights kernel attribute is quantized with weights mixed precision
+            kernel_attr = search_manager.fw_info.get_kernel_op_attributes(current_node)
+            kernel_attr = None if kernel_attr is None else kernel_attr[0]
+            valid_candidates = _get_valid_candidates_indices(node_candidates, new_solution[node_idx], kernel_attr)
+
+            # Create a list of ru for the valid candidates.
+            updated_ru = []
             for valid_idx in valid_candidates:
-                node_updated_kpis = search_manager.compute_kpi_for_config(
+                node_updated_ru = search_manager.compute_resource_utilization_for_config(
                     config=search_manager.replace_config_in_index(new_solution, node_idx, valid_idx))
-                updated_kpis.append(node_updated_kpis)
+                updated_ru.append(node_updated_ru)
 
-            # filter out new configs that don't hold the KPI restrictions
-            node_filtered_kpis = [(node_idx, kpis) for node_idx, kpis in zip(valid_candidates,updated_kpis) if
-                               target_kpi.holds_constraints(kpis)]
-
-            if len(node_filtered_kpis) > 0:
-                sorted_by_kpi = sorted(node_filtered_kpis, key=lambda node_kpis: (node_kpis[1].total_memory,
-                                                                               node_kpis[1].weights_memory,
-                                                                               node_kpis[1].activation_memory))
-                nodes_kpis[node_idx] = sorted_by_kpi[0][1]
-                nodes_next_candidate[node_idx] = sorted_by_kpi[0][0]
-
-
-        if len(nodes_kpis) > 0:
-            # filter out new configs that don't hold the KPI restrictions
-            node_filtered_kpis = [(node_idx, kpis) for node_idx, kpis in nodes_kpis.items()]
-            sorted_by_kpi = sorted(node_filtered_kpis, key=lambda node_kpis: (node_kpis[1].total_memory,
-                                                                           node_kpis[1].weights_memory,
-                                                                           node_kpis[1].activation_memory))
+            # filter out new configs that don't hold the resource utilization restrictions
+            node_filtered_ru = [(node_idx, ru) for node_idx, ru in zip(valid_candidates, updated_ru) if
+                                target_resource_utilization.holds_constraints(ru)]
+
+            if len(node_filtered_ru) > 0:
+                sorted_by_ru = sorted(node_filtered_ru, key=lambda node_ru: (node_ru[1].total_memory,
+                                                                             node_ru[1].weights_memory,
+                                                                             node_ru[1].activation_memory))
+                nodes_ru[node_idx] = sorted_by_ru[0][1]
+                nodes_next_candidate[node_idx] = sorted_by_ru[0][0]
+
+        if len(nodes_ru) > 0:
+            # filter out new configs that don't hold the ru restrictions
+            node_filtered_ru = [(node_idx, ru) for node_idx, ru in nodes_ru.items()]
+            sorted_by_ru = sorted(node_filtered_ru, key=lambda node_ru: (node_ru[1].total_memory,
+                                                                         node_ru[1].weights_memory,
+                                                                         node_ru[1].activation_memory))
 
-            node_idx_to_upgrade = sorted_by_kpi[0][0]
+            node_idx_to_upgrade = sorted_by_ru[0][0]
             new_solution[node_idx_to_upgrade] = nodes_next_candidate[node_idx_to_upgrade]
             changed = True
 
-    Logger.info(f'Greedy MP algorithm changed configuration from: {mp_solution} to {new_solution}')
+    if any([mp_solution[i] != new_solution[i] for i in range(len(mp_solution))]):
+        Logger.info(f'Greedy MP algorithm changed configuration from (numbers represent indices of the '
+                    f'chosen bit-width candidate for each layer):\n{mp_solution}\nto\n{new_solution}')
+
     return new_solution
 
 
 def _get_valid_candidates_indices(node_candidates: List[CandidateNodeQuantizationConfig],
-                                  current_chosen_index: int) -> List[int]:
+                                  current_chosen_index: int,
+                                  kernel_attr: str = None) -> List[int]:
     """
     Find node's valid candidates to try and improve the node's MP chosen candidate.
-    Valid indices are indices of candidates that have higher number of bits for both weights and activations.
+    Valid indices are indices of candidates that have higher number of bits for both weights and activations
+    (if they are quantized in this node).
 
     Args:
         node_candidates: Candidates of the node.
         current_chosen_index: Current index in MP configuration of the node.
+        kernel_attr: The name of the kernel attribute on the node, otherwise None.
 
     Returns:
         List of indices of valid candidates.
     """
-
     current_candidate = node_candidates[current_chosen_index]
-    weights_num_bits = current_candidate.weights_quantization_cfg.weights_n_bits
-    activation_num_bits = current_candidate.activation_quantization_cfg.activation_n_bits
 
-    # Filter candidates that have higher bit-width for both weights and activations (except for the current index).
-    return [i for i, c in enumerate(node_candidates) if c.activation_quantization_cfg.activation_n_bits >= activation_num_bits and c.weights_quantization_cfg.weights_n_bits >= weights_num_bits and not (c.activation_quantization_cfg.activation_n_bits == activation_num_bits and c.weights_quantization_cfg.weights_n_bits == weights_num_bits)]
+    if kernel_attr is None:
+        # In this node we only quantize activation, so no need to check weights number of bits
+        activation_num_bits = current_candidate.activation_quantization_cfg.activation_n_bits
+
+        # Filter candidates that have higher bit-width for activations
+        return [i for i, c in enumerate(node_candidates) if
+                c.activation_quantization_cfg.activation_n_bits >= activation_num_bits
+                and not (c.activation_quantization_cfg.activation_n_bits == activation_num_bits)]
+    else:
+        weights_num_bits = current_candidate.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+        activation_num_bits = current_candidate.activation_quantization_cfg.activation_n_bits
+
+        # Filter candidates that have higher bit-width for both weights and activations (except for the current index).
+        return [i for i, c in enumerate(node_candidates) if
+                c.activation_quantization_cfg.activation_n_bits >= activation_num_bits
+                and c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits >= weights_num_bits
+                and not (c.activation_quantization_cfg.activation_n_bits == activation_num_bits
+                         and c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits == weights_num_bits)]
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/model_builder_mode.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/model_builder_mode.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/model_validation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/model_validation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/actions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/actions.py`

 * *Files 16% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 from abc import ABC, abstractmethod
 from collections import namedtuple
 from typing import Callable
 
+from mct_quantizers import QuantizationMethod
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.logger import Logger
 
 
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
@@ -33,19 +34,20 @@
 
 class EditRule(_EditRule):
     """
     A tuple of a node filter and an action. The filter matches nodes in the graph which represents the model,
     and the action is applied on these nodes during the quantization process.
 
     Examples:
-        Create an EditRule to quantize all Conv2D wights using 9 bits:
+        Create an EditRule to quantize all Conv2D kernel attribute weights using 9 bits:
 
         >>> import model_compression_toolkit as mct
+        >>> from model_compression_toolkit.core.keras.constants import KERNEL
         >>> from tensorflow.keras.layers import Conv2D
-        >>> er_list = [mct.network_editor.EditRule(filter=mct.network_editor.NodeTypeFilter(Conv2D), action=mct.network_editor.ChangeCandidatesWeightsQuantConfigAttr(weights_n_bits=9))]
+        >>> er_list = [mct.core.network_editor.EditRule(filter=mct.core.network_editor.NodeTypeFilter(Conv2D), action=mct.core.network_editor.ChangeCandidatesWeightsQuantConfigAttr(attr_name=KERNEL, weights_n_bits=9))]
 
         Then the rules list can be passed to :func:`~model_compression_toolkit.keras_post_training_quantization`
         to modify the network during the quantization process.
 
     """
 
     def __repr__(self):
@@ -80,54 +82,61 @@
 
 
 class ChangeCandidatesWeightsQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's weights quantization configuration candidates.
     """
 
-    def __init__(self, **kwargs):
+    def __init__(self, attr_name: str = None, **kwargs):
         """
         Args:
+            attr_name: The weights attribute's name to set the weights quantization params function for.
             kwargs: Dictionary of attr_name and attr_value to change layer's weights quantization configuration candidates.
         """
         self.kwargs = kwargs
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         """
         Change the attribute 'attr_name' in weights quantization config candidates with 'attr_value'.
 
         Args:
             node: Node object to change its quant_config.
             graph: Graph to apply the action on.
             fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
                      groups of layers by how they should be quantized, etc.)
         Returns:
             The node after its weights' quantization config candidates have been modified.
         """
+
         for nqc in node.candidates_quantization_cfg:
-            for attr_name, attr_value in self.kwargs.items():
-                nqc.weights_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                nqc.weights_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value,
+                                                                   attr_name=self.attr_name)
 
 
 class ChangeFinalWeightsQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's final weights quantization config.
     """
 
-    def __init__(self, **kwargs):
+    def __init__(self, attr_name: str = None, **kwargs):
         """
         Args:
+            attr_name: The weights attribute's name to set the weights quantization params function for.
             kwargs: Dictionary of attr_name and attr_value to change layer's final weights quantization config.
         """
         self.kwargs = kwargs
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         if node.final_weights_quantization_cfg is not None:
-            for attr_name, attr_value in self.kwargs.items():
-                node.final_weights_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                node.final_weights_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value,
+                                                                          self.attr_name)
 
 
 class ChangeCandidatesActivationQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's activation quantization configuration candidates.
     """
 
@@ -147,16 +156,16 @@
             graph: Graph to apply the action on.
             fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
                      groups of layers by how they should be quantized, etc.)
         Returns:q
             The node after its activation quantization configuration candidates have been modified.
         """
         for nqc in node.candidates_quantization_cfg:
-            for attr_name, attr_value in self.kwargs.items():
-                nqc.activation_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                nqc.activation_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value)
 
 
 class ChangeFinalActivationQuantConfigAttr(BaseAction):
     """
     Change attributes in a layer's final activation quantization config.
     """
 
@@ -165,33 +174,38 @@
         Args:
             kwargs: Dictionary of attr_name and attr_value to change layer's final activation quantization config.
         """
         self.kwargs = kwargs
 
     def apply(self, node: BaseNode, graph, fw_info):
         if node.final_activation_quantization_cfg is not None:
-            for attr_name, attr_value in self.kwargs.items():
-                node.final_activation_quantization_cfg.set_quant_config_attr(attr_name, attr_value)
+            for parameter_name, parameter_value in self.kwargs.items():
+                node.final_activation_quantization_cfg.set_quant_config_attr(parameter_name, parameter_value)
 
 
 class ChangeQuantizationParamFunction(BaseAction):
     """
     Class ChangeQuantizationParamFunction to change a node's weights/activations quantization params function.
     """
 
-    def __init__(self, activation_quantization_params_fn=None, weights_quantization_params_fn=None):
+    def __init__(self,
+                 attr_name: str = None,
+                 activation_quantization_params_fn: Callable = None,
+                 weights_quantization_params_fn: Callable = None):
         """
         Init a ChangeQuantizationParamFunction object.
 
         Args:
+            attr_name: The weights attribute's name to set the weights quantization params function for (if setting weights params).
             activation_quantization_params_fn: a params function for a node's activations.
             weights_quantization_params_fn: a params function for a node's weights.
         """
         self.activation_quantization_params_fn = activation_quantization_params_fn
         self.weights_quantization_params_fn = weights_quantization_params_fn
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         """
         Change the node's weights/activations quantization params function.
 
         Args:
             node: Node object to change its quantization params function.
@@ -203,15 +217,16 @@
             The node after its quantization params function has been modified.
         """
         for nqc in node.candidates_quantization_cfg:
             if self.activation_quantization_params_fn is not None:
                 nqc.activation_quantization_cfg.set_activation_quantization_params_fn(
                     self.activation_quantization_params_fn)
             if self.weights_quantization_params_fn is not None:
-                nqc.weights_quantization_cfg.set_weights_quantization_params_fn(self.weights_quantization_params_fn)
+                (nqc.weights_quantization_cfg.get_attr_config(self.attr_name)
+                 .set_weights_quantization_params_fn(self.weights_quantization_params_fn))
 
 
 class ChangeFinalActivationQuantizationMethod(BaseAction):
     """
     Class ChangeFinalActivationQuantizationMethod to change a node's weights/activations quantizer function.
     """
 
@@ -286,34 +301,36 @@
                     self.activation_quantization_method)
 
                 qc.activation_quantization_cfg.set_activation_quantization_params_fn(activation_quantization_params_fn)
                 activation_quantization_fn = fw_info.activation_quantizer_mapping.get(
                     self.activation_quantization_method)
 
                 if activation_quantization_fn is None:
-                    raise Exception('Unknown quantization method for activations')  # pragma: no cover
+                    Logger.critical('Unknown activation quantization method specified.')  # pragma: no cover
 
                 qc.activation_quantization_cfg.set_activation_quantization_fn(activation_quantization_fn)
                 qc.activation_quantization_cfg.activation_quantization_method = self.activation_quantization_method
 
 
 class ChangeFinalWeightsQuantizationMethod(BaseAction):
     """
     Class ChangeFinalWeightsQuantizationMethod to change a node's weights/activations quantizer function.
     """
 
-    def __init__(self, weights_quantization_method=None):
+    def __init__(self, attr_name: str, weights_quantization_method=None):
         """
         Init a ChangeFinalWeightsQuantizationMethod object.
 
         Args:
+            attr_name: The weights attribute's name to set the weights quantization method for.
             weights_quantization_method: a quantization method for a node's weights.
         """
 
         self.weights_quantization_method = weights_quantization_method
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph, fw_info):
         """
         Change the node's weights quantization function.
 
         Args:
             node: Node object to change its threshold selection function.
@@ -325,38 +342,43 @@
             The node after its quantization function has been modified.
         """
 
         if self.weights_quantization_method is not None and node.final_weights_quantization_cfg is not None:
 
             weights_quantization_params_fn = get_weights_quantization_params_fn(self.weights_quantization_method)
 
-            node.final_weights_quantization_cfg.set_weights_quantization_params_fn(weights_quantization_params_fn)
+            (node.final_weights_quantization_cfg.get_attr_config(self.attr_name)
+             .set_weights_quantization_params_fn(weights_quantization_params_fn))
 
             weights_quantization_fn = get_weights_quantization_fn(self.weights_quantization_method)
 
             if weights_quantization_fn is None:
-                raise Exception('Unknown quantization method for weights')  # pragma: no cover
+                Logger.critical('Unknown weights quantization method specified.')  # pragma: no cover
 
-            node.final_weights_quantization_cfg.set_weights_quantization_fn(weights_quantization_fn)
-            node.final_weights_quantization_cfg.weights_quantization_method = self.weights_quantization_method
+            (node.final_weights_quantization_cfg.get_attr_config(self.attr_name)
+             .set_weights_quantization_fn(weights_quantization_fn))
+            node.final_weights_quantization_cfg.get_attr_config(self.attr_name).weights_quantization_method = \
+                self.weights_quantization_method
 
 
 class ChangeCandidatesWeightsQuantizationMethod(BaseAction):
     """
     Class ChangeCandidatesWeightsQuantizationMethod to change a node's weights quantizer function.
     """
 
-    def __init__(self, weights_quantization_method=None):
+    def __init__(self, attr_name: str, weights_quantization_method: QuantizationMethod = None):
         """
         Init a ChangeCandidatesWeightsQuantizationMethod object.
 
         Args:
             weights_quantization_method: a quantization method for a node's weights.
+            attr_name: The weights attribute's name to set the weights quantization params function for.
         """
         self.weights_quantization_method = weights_quantization_method
+        self.attr_name = attr_name
 
     def apply(self, node: BaseNode, graph: Graph, fw_info: FrameworkInfo):
         """
         Change the node's weights quantization function.
 
         Args:
             node: Node object to change its threshold selection function.
@@ -369,23 +391,24 @@
         """
 
         if self.weights_quantization_method is not None:
             for qc in node.candidates_quantization_cfg:
 
                 weights_quantization_params_fn = get_weights_quantization_params_fn(self.weights_quantization_method)
 
-                qc.weights_quantization_cfg.set_weights_quantization_params_fn(weights_quantization_params_fn)
+                attr_qc = qc.weights_quantization_cfg.get_attr_config(self.attr_name)
+                attr_qc.set_weights_quantization_params_fn(weights_quantization_params_fn)
 
                 weights_quantization_fn = get_weights_quantization_fn(self.weights_quantization_method)
 
                 if weights_quantization_fn is None:
-                    raise Exception('Unknown quantization method for weights')  # pragma: no cover
+                    Logger.critical('Unknown weights quantization method specified.')  # pragma: no cover
 
-                qc.weights_quantization_cfg.set_weights_quantization_fn(weights_quantization_fn)
-                qc.weights_quantization_cfg.weights_quantization_method = self.weights_quantization_method
+                attr_qc.set_weights_quantization_fn(weights_quantization_fn)
+                attr_qc.weights_quantization_method = self.weights_quantization_method
 
 
 class ReplaceLayer(BaseAction):
 
     def __init__(self, layer_type: type, get_params_and_weights_fn: Callable):
         """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/edit_network.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/edit_network.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/network_editors/node_filters.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/network_editors/node_filters.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/node_prior_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/node_prior_info.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/core_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/core_config.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,37 +8,41 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
 
 
 class CoreConfig:
     """
     A class to hold the configurations classes of the MCT-core.
     """
     def __init__(self,
                  quantization_config: QuantizationConfig = QuantizationConfig(),
-                 mixed_precision_config: MixedPrecisionQuantizationConfigV2 = None,
+                 mixed_precision_config: MixedPrecisionQuantizationConfig = None,
                  debug_config: DebugConfig = DebugConfig()
                  ):
         """
 
         Args:
             quantization_config (QuantizationConfig): Config for quantization.
-            mixed_precision_config (MixedPrecisionQuantizationConfigV2): Config for mixed precision quantization (optional, default=None).
+            mixed_precision_config (MixedPrecisionQuantizationConfig): Config for mixed precision quantization.
+            If None, a default MixedPrecisionQuantizationConfig is used.
             debug_config (DebugConfig): Config for debugging and editing the network quantization process.
         """
         self.quantization_config = quantization_config
-        self.mixed_precision_config = mixed_precision_config
         self.debug_config = debug_config
 
+        if mixed_precision_config is None:
+            self.mixed_precision_config = MixedPrecisionQuantizationConfig()
+        else:
+            self.mixed_precision_config = mixed_precision_config
+
     @property
     def mixed_precision_enable(self):
-        return self.mixed_precision_config is not None
+        return self.mixed_precision_config is not None and self.mixed_precision_config.mixed_precision_enable
+
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/debug_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/debug_config.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py`

 * *Files 16% similar despite different names*

```diff
@@ -31,91 +31,108 @@
     Updating the lists is preformed inplace on the graph object.
 
     Args:
         graph: Graph for which to add quantization info to each node.
     """
     nodes = list(graph.nodes)
     for n in nodes:
-        n.candidates_quantization_cfg = filter_node_candidates(node=n)
+        n.candidates_quantization_cfg = filter_node_candidates(node=n, fw_info=graph.fw_info)
 
     return graph
 
 
-def _filter_bit_method_dups(candidates: List[CandidateNodeQuantizationConfig]) -> List[CandidateNodeQuantizationConfig]:
+def _filter_bit_method_dups(candidates: List[CandidateNodeQuantizationConfig],
+                            kernel_attr: str = None) -> List[CandidateNodeQuantizationConfig]:
     """
     Filters out duplications in candidates configuration list, based on similarity in
     (weights_n_bits, weights_quantization_method, activation_n_bits, activation_quantization_method).
+    Weights quantization configuration considers only kernel attributes.
 
     Args:
         candidates: A list of quantization configuration candidates.
+        kernel_attr: The name of the node's kernel attribute if such exists.
 
     Returns: A filtered list of quantization configuration candidates.
 
     """
     seen_bits_method_combinations = set()
     final_candidates = []
     for c in candidates:
-        comb = (c.weights_quantization_cfg.weights_n_bits,
-                c.weights_quantization_cfg.weights_quantization_method,
+        weight_n_bits = None if kernel_attr is None else (
+            c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits)
+        weights_quantization_method = None if kernel_attr is None else (
+            c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method)
+        comb = (weight_n_bits,
+                weights_quantization_method,
                 c.activation_quantization_cfg.activation_n_bits,
                 c.activation_quantization_cfg.activation_quantization_method)
         if comb not in seen_bits_method_combinations:
             final_candidates.append(c)
             seen_bits_method_combinations.add(comb)
 
     return final_candidates
 
 
-def filter_node_candidates(node: BaseNode) -> List[CandidateNodeQuantizationConfig]:
+def filter_node_candidates(node: BaseNode, fw_info) -> List[CandidateNodeQuantizationConfig]:
     """
     Updates a node's candidates configuration list.
     If the node's weights quantization is disabled (or it only has activations to quantize), then the updated list
     will have a candidate with any of the different original activation bitwidths candidates and a default value
     for its weights bitwidth (that doesn't have any impact on the quantization or the mixed-precision search.
     If the node's activation quantization is disabled, the same filtering applied for the weights bitwidth candidates.
 
     Args:
         node: Node to set its quantization configurations.
+        fw_info: FrameworkInfo object with information about the specific framework's model.
+
     """
 
     filtered_candidates = copy.deepcopy(node.candidates_quantization_cfg)
     final_candidates = copy.deepcopy(node.candidates_quantization_cfg)
+    kernel_attr = fw_info.get_kernel_op_attributes(node.type)[0]
 
-    if not node.is_weights_quantization_enabled() and not node.is_activation_quantization_enabled():
-        # If both weights and activation quantization are disabled, but for some reason the node has multiple candidates
-        # then replace it with a single dummy candidate with default bit-width values.
+    if (kernel_attr is None or not node.is_weights_quantization_enabled(kernel_attr)) and not node.is_activation_quantization_enabled():
+        # If activation quantization is disabled and the node doesn't have a kernel or doesn't quantize the kernel,
+        # but for some reason the node has multiple candidates then replace it with a single dummy candidate with
+        # default bit-width values.
         single_dummy_candidate = filtered_candidates[0]
         single_dummy_candidate.activation_quantization_cfg.activation_n_bits = FLOAT_BITWIDTH
-        single_dummy_candidate.weights_quantization_cfg.weights_n_bits = FLOAT_BITWIDTH
-        single_dummy_candidate.weights_quantization_cfg.weights_quantization_method = QuantizationMethod.POWER_OF_TWO
         single_dummy_candidate.activation_quantization_cfg.activation_quantization_method = QuantizationMethod.POWER_OF_TWO
+
+        if kernel_attr is not None:
+            kernel_config = single_dummy_candidate.weights_quantization_cfg.get_attr_config(kernel_attr)
+            kernel_config.weights_n_bits = FLOAT_BITWIDTH
+            kernel_config.weights_quantization_method = QuantizationMethod.POWER_OF_TWO
+
         final_candidates = [single_dummy_candidate]
 
     elif not node.is_activation_quantization_enabled():
         # Remove candidates that have duplicated weights candidates for node with disabled activation quantization.
         # Replacing the activation n_bits in the remained configurations with default value to prevent confusion.
         seen_candidates = set()
         filtered_candidates = [candidate for candidate in filtered_candidates if
                                candidate.weights_quantization_cfg not in seen_candidates
                                and not seen_candidates.add(candidate.weights_quantization_cfg)]
 
         for c in filtered_candidates:
             c.activation_quantization_cfg.activation_n_bits = FLOAT_BITWIDTH
             c.activation_quantization_cfg.activation_quantization_method = QuantizationMethod.POWER_OF_TWO
 
-        final_candidates = _filter_bit_method_dups(filtered_candidates)
+        final_candidates = _filter_bit_method_dups(filtered_candidates, kernel_attr)
 
-    elif not node.is_weights_quantization_enabled():
+    elif kernel_attr is None or not node.is_weights_quantization_enabled(kernel_attr):
         # Remove candidates that have duplicated activation candidates for node with disabled weights quantization.
         # Replacing the weights n_bits in the remained configurations with default value to prevent confusion.
         seen_candidates = set()
         filtered_candidates = [candidate for candidate in filtered_candidates if
                                candidate.activation_quantization_cfg not in seen_candidates
                                and not seen_candidates.add(candidate.activation_quantization_cfg)]
 
         for c in filtered_candidates:
-            c.weights_quantization_cfg.weights_n_bits = FLOAT_BITWIDTH
-            c.weights_quantization_cfg.weights_quantization_method = QuantizationMethod.POWER_OF_TWO
+            if kernel_attr is not None:
+                kernel_config = c.weights_quantization_cfg.get_attr_config(kernel_attr)
+                kernel_config.weights_n_bits = FLOAT_BITWIDTH
+                kernel_config.weights_quantization_method = QuantizationMethod.POWER_OF_TWO
 
-        final_candidates = _filter_bit_method_dups(filtered_candidates)
+        final_candidates = _filter_bit_method_dups(filtered_candidates, kernel_attr)
 
     return final_candidates
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/node_quantization_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,384 +1,293 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Tuple, Any, Dict, Union, List
 
+from packaging import version
+import tensorflow as tf
+if version.parse(tf.__version__) >= version.parse("2.13"):
+    from keras.src.engine.base_layer import Layer
+else:
+    from keras.engine.base_layer import Layer
+
+from keras.models import Model
+from mct_quantizers import KerasQuantizationWrapper, KerasActivationQuantizationHolder, QuantizationTarget
+from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
+from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.core.common.user_info import UserInformation
+from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
+from model_compression_toolkit.core.keras.mixed_precision.configurable_activation_quantizer import \
+    ConfigurableActivationQuantizer
+from model_compression_toolkit.core.keras.mixed_precision.configurable_weights_quantizer import \
+    ConfigurableWeightsQuantizer
 
-from typing import Callable, Any
-
-import numpy as np
+from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
+    get_inferable_quantizer_kwargs
 
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
-    get_activation_quantization_params_fn, get_weights_quantization_params_fn
-
-from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, \
-    QuantizationErrorMethod
-from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig
-
-
-##########################################
-# Every node holds a quantization configuration
-# for its weights and activations quantization, and a different quantization
-# configuration for its activation quantization configuration.
-##########################################
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 
 
-class BaseNodeQuantizationConfig(object):
+class MixedPrecisionKerasModelBuilder(KerasModelBuilder):
     """
-    Base class for node quantization configuration
+    Builder of mixed-precision Keras models.
     """
 
-    def set_quant_config_attr(self, attr_name, attr_value):
-        """
-        Changes a BaseNodeQuantizationConfig's attribute.
-
-        Args:
-            attr_name: attribute name to change.
-            attr_value: attribute value to change.
-
-        """
-
-        if hasattr(self, attr_name):
-            setattr(self, attr_name, attr_value)
-
-    def __repr__(self) -> str:
-        """
-        Returns: String to display a NodeQuantizationConfig object.
-        """
-        repr_str = ''
-        for k, v in self.__dict__.items():
-            repr_str += f'{k}: {v}\n'
-        return repr_str
-
-
-class NodeActivationQuantizationConfig(BaseNodeQuantizationConfig):
-    """
-    Attributes for configuring the quantization of the activations of a node.
-    """
     def __init__(self,
-                 qc: QuantizationConfig,
-                 op_cfg: OpQuantizationConfig,
-                 activation_quantization_fn: Callable,
-                 activation_quantization_params_fn: Callable
-                 ):
+                 graph: common.Graph,
+                 append2output=None,
+                 fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
+                 return_float_outputs: bool = False):
         """
 
         Args:
-            qc: QuantizationConfig to create the node's config from.
-            op_cfg: OpQuantizationConfig of the node with quantizers types to use when creating node quantization configuration.
-            activation_quantization_fn: Function to use when quantizing the node's activations.
-            activation_quantization_params_fn: Function to use when computing the threshold for quantizing a node's activations.
-        """
-
-        self.activation_quantization_fn = activation_quantization_fn
-        self.activation_quantization_params_fn = activation_quantization_params_fn
-        self.activation_quantization_params = {}
-        self.activation_quantization_method = op_cfg.activation_quantization_method
-        self.activation_error_method = qc.activation_error_method
-        self.activation_n_bits = op_cfg.activation_n_bits
-        self.relu_bound_to_power_of_2 = qc.relu_bound_to_power_of_2
-        self.enable_activation_quantization = op_cfg.enable_activation_quantization
-        self.activation_channel_equalization = qc.activation_channel_equalization
-        self.input_scaling = qc.input_scaling
-        self.min_threshold = qc.min_threshold
-        self.l_p_value = qc.l_p_value
-        self.shift_negative_activation_correction = qc.shift_negative_activation_correction
-        self.z_threshold = qc.z_threshold
-        self.shift_negative_ratio = qc.shift_negative_ratio
-        self.shift_negative_threshold_recalculation = qc.shift_negative_threshold_recalculation
-
-    def quantize_node_output(self,
-                             tensors: Any) -> Any:
+            graph: Graph to build the model from.
+            append2output: Nodes to append to model's output.
+            fw_info: Information about the specific framework of the model that is built.
+            return_float_outputs: Whether the model returns float tensors or not.
         """
 
-        Args:
-            tensors: framework tensor/s
+        self.graph = graph
 
-        Returns:
-            Framework tensor/s after applying fake quantization.
+        super().__init__(graph,
+                         append2output,
+                         fw_info,
+                         return_float_outputs,
+                         wrapper=self.mixed_precision_wrapper,
+                         get_activation_quantizer_holder_fn=self.mixed_precision_activation_holder)
 
+    def mixed_precision_wrapper(self,
+                                n: common.BaseNode,
+                                layer: Layer) -> Union[KerasQuantizationWrapper, Layer]:
         """
-        fake_quant = self.activation_quantization_fn(self.activation_n_bits,
-                                                     self.activation_quantization_params)
+        A function which takes a computational graph node and a keras layer and perform the quantization
+        wrapping for mixed precision.
 
-        if fake_quant is None:
-            Logger.error('Layer is meant to be quantized but fake_quant function is None')  # pragma: no cover
-        return fake_quant(tensors)
+        Args:
+            n: A node of mct graph.
+            layer: A keras layer
 
-    @property
-    def activation_error_method(self) -> QuantizationErrorMethod:
-        """
-        activation_error_method getter.
-        """
-        return self._activation_error_method
+        Returns: Wrapped layer with a configurable quantizer if the layer should quantized in mixed precision,
+        otherwise returns either the layer wrapped with a fixed precision inferable quantizer or the layer as is if it's
+        not supposed to be quantized.
 
-    @activation_error_method.setter
-    def activation_error_method(self, value: QuantizationErrorMethod):
         """
-        activation_error_method setter.
-
-        Args:
-            value: New activation_error_method to set to the node activation configuration.
 
-        """
-        self._activation_error_method = value
-        self.activation_quantization_params_fn = get_activation_quantization_params_fn(activation_quantization_method=self.activation_quantization_method)
+        kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+            weights_conf_nodes_names = [node.name for node in self.graph.get_weights_configurable_nodes(self.fw_info)]
+            if n.name in weights_conf_nodes_names:
+                return KerasQuantizationWrapper(layer,
+                                                weights_quantizers={
+                                                    kernel_attr: ConfigurableWeightsQuantizer(
+                                                        **self._get_weights_configurable_quantizer_kwargs(n,
+                                                                                                          kernel_attr))})
+            else:
+                # TODO: Do we want to include other quantized attributes that are not
+                #  the kernel attribute in the mixed precision model?
+                #  Currently, we only consider kernel attribute quantization (whether it is in mixed precision
+                #  or single precision).
+                node_weights_qc = n.get_unique_weights_candidates(kernel_attr)
+                if not len(node_weights_qc) == 1:
+                    Logger.critical(f"Expected a unique weights configuration for node {n.name}, but found {len(node_weights_qc)} configurations.")# pragma: no cover
 
-    def set_activation_quantization_fn(self, activation_quantization_fn: Callable):
-        """
-        Sets activation quantization function for the node.
+                quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
+                                                                  node_weights_qc[0].weights_quantization_cfg
+                                                                  .get_attr_config(kernel_attr)
+                                                                  .weights_quantization_method,
+                                                                  BaseKerasInferableQuantizer)
+                kwargs = get_inferable_quantizer_kwargs(node_weights_qc[0].weights_quantization_cfg,
+                                                        QuantizationTarget.Weights,
+                                                        kernel_attr)
 
-        Args:
-            activation_quantization_fn: Function for quantazing the activations.
+                return KerasQuantizationWrapper(layer,
+                                                weights_quantizers={kernel_attr: quantier_for_node(**kwargs)})
 
-        """
-        self.activation_quantization_fn = activation_quantization_fn
+        return layer
 
-    def set_activation_quantization_params_fn(self, activation_quantization_params_fn:Callable):
+    def _get_weights_configurable_quantizer_kwargs(self, n: BaseNode, attr: str) -> Dict[str, Any]:
         """
-        Sets activation params function for the node.
+        Get the quantization parameters for a configurable quantizer.
 
         Args:
-            activation_quantization_params_fn: Function for calculating activation params.
+            n: The node for which the quantizer is being created.
+            attr: The name of the weights attribute to be quantized.
 
+        Returns:
+            The quantization parameters as a dictionary.
         """
-        self.activation_quantization_params_fn = activation_quantization_params_fn
 
-    def set_activation_quantization_param(self,
-                                          activation_params: dict):
-        """
-         Set a quantization parameter for the node's activation.
+        assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
+        node_q_cfg_candidates = n.candidates_quantization_cfg
+        # sort by descending bit width so using indices would be easier
+        node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.get_attr_config(attr).weights_n_bits,
+                                                  x.activation_quantization_cfg.activation_n_bits), reverse=True)
 
-        Args:
-            activation_params: Dictionary that contains weight quantization params.
+        float_weights = n.get_weights_by_keys(attr)
 
-        """
-        assert self.enable_activation_quantization
-        for param_name, param_value in activation_params.items():
-            self.activation_quantization_params[param_name] = param_value
+        max_cfg_candidates = n.find_max_candidates_indices()
+        if not len(max_cfg_candidates) == 1:
+            Logger.critical(f"A maximal configuration candidate must be defined; found multiple potential maximal candidates.")# pragma: no cover
 
-    def has_activation_quantization_params(self) -> bool:
-        """
+        max_candidate_idx = max_cfg_candidates[0]
 
-        Returns: Whether NodeQuantizationConfig has a activation quantization params or not.
+        return {'node_q_cfg': node_q_cfg_candidates,
+                'float_weights': float_weights,
+                'max_candidate_idx': max_candidate_idx,
+                'kernel_attr': attr,
+                }
 
+    def mixed_precision_activation_holder(self, n: BaseNode) -> KerasActivationQuantizationHolder:
         """
-        return len(self.activation_quantization_params) > 0
+        Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
+        The layer should hold either a configurable activation quantizer, if it is quantized with mixed precision,
+        or an inferable quantizer for fixed single bit-width quantization.
 
-    def no_quantization(self) -> bool:
-        """
-        Returns: Whether NodeQuantizationConfig does not have activation params.
-        """
-        return (not self.has_activation_quantization_params())
+        Args:
+            n: Node to get KerasActivationQuantizationHolder to attach in its output.
 
-    def __eq__(self, other: Any) -> bool:
+        Returns:
+            A KerasActivationQuantizationHolder layer for the node activation quantization.
         """
-        Compares the object to another object to find if they are equal.
 
-        Args:
-            other: An object to compare to.
+        activation_conf_nodes_names = [n.name for n in self.graph.get_activation_configurable_nodes()]
 
-        Returns: Whether the objects are identical or not.
+        activation_quantizers = []
+        if n.is_activation_quantization_enabled():
+            num_of_outputs = len(n.output_shape) if isinstance(n.output_shape, list) else 1
 
-        """
-        if not isinstance(other, NodeActivationQuantizationConfig):
-            return False
+            if n.name in activation_conf_nodes_names:
+                assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
+                node_q_cfg_candidates = n.candidates_quantization_cfg
 
-        return self.activation_quantization_fn == other.activation_quantization_fn and \
-               self.activation_quantization_params_fn == other.activation_quantization_params_fn and \
-               self.activation_error_method == other.activation_error_method and \
-               self.activation_quantization_method == other.activation_quantization_method and \
-               self.activation_n_bits == other.activation_n_bits and \
-               self.enable_activation_quantization == other.enable_activation_quantization and \
-               self.activation_channel_equalization == other.activation_channel_equalization and \
-               self.input_scaling == other.input_scaling and \
-               self.min_threshold == other.min_threshold and \
-               self.l_p_value == other.l_p_value and \
-               self.shift_negative_activation_correction == other.shift_negative_activation_correction and \
-               self.z_threshold == other.z_threshold and \
-               self.shift_negative_ratio == other.shift_negative_ratio and \
-               self.shift_negative_threshold_recalculation == other.shift_negative_threshold_recalculation
-
-    def __hash__(self):
-        return hash((self.activation_quantization_fn,
-                     self.activation_quantization_params_fn,
-                     self.activation_error_method,
-                     self.activation_quantization_method,
-                     self.activation_n_bits,
-                     self.enable_activation_quantization,
-                     self.activation_channel_equalization,
-                     self.input_scaling,
-                     self.min_threshold,
-                     self.l_p_value,
-                     self.shift_negative_activation_correction,
-                     self.z_threshold,
-                     self.shift_negative_ratio,
-                     self.shift_negative_threshold_recalculation))
+                # sorting the candidates by kernel attribute weights number of bits first and then by
+                # activation number of bits (in reversed order).
+                # since only kernel attribute is quantized in weights mixed precision,
+                # if the node doesn't have a kernel attribute, we only sort by activation_n_bits.
+                n.sort_node_candidates(self.fw_info)
 
+                max_cfg_candidates = n.find_max_candidates_indices()
+                assert len(max_cfg_candidates) == 1, \
+                    f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
+                max_candidate_idx = max_cfg_candidates[0]
 
-class NodeWeightsQuantizationConfig(BaseNodeQuantizationConfig):
-    """
-    Attributes for configuring the quantization of the weights of a node.
-    """
-    def __init__(self,
-                 qc: QuantizationConfig,
-                 op_cfg: OpQuantizationConfig,
-                 weights_quantization_fn: Callable,
-                 weights_quantization_params_fn: Callable,
-                 weights_channels_axis: int):
-        """
+                kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+                activation_quantizers = [ConfigurableActivationQuantizer(**{'node_q_cfg': node_q_cfg_candidates,
+                                                                            'max_candidate_idx': max_candidate_idx,
+                                                                            'kernel_attr': kernel_attr})] \
+                                        * num_of_outputs
+            else:
+                node_act_qc = n.get_unique_activation_candidates()
+                assert len(node_act_qc) == 1, f"Expecting node {n.name} to have a unique activation configuration, " \
+                                              f"but {len(node_act_qc)} different configurations exist."
+                quantizer_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
+                                                                   node_act_qc[0].activation_quantization_cfg.activation_quantization_method,
+                                                                   BaseKerasInferableQuantizer)
+                kwargs = get_inferable_quantizer_kwargs(node_act_qc[0].activation_quantization_cfg,
+                                                        QuantizationTarget.Activation)
 
-        Args:
-            qc: QuantizationConfig to create the node's config from.
-            op_cfg: OpQuantizationConfig of the node with quantizers types to use when creating node quantization configuration.
-            weights_quantization_fn: Function to use when quantizing the node's weights.
-            weights_quantization_params_fn:  Function to use when computing the threshold for quantizing a node's weights.
-            weights_channels_axis: Axis to quantize a node's kernel when quantizing per-channel.
-        """
-
-        self.weights_quantization_fn = weights_quantization_fn
-        self.weights_quantization_params_fn = weights_quantization_params_fn
-        self.weights_channels_axis = weights_channels_axis
-        self.weights_quantization_params = {}
-        self.weights_quantization_method = op_cfg.weights_quantization_method
-        self.weights_error_method = qc.weights_error_method
-        self.weights_n_bits = op_cfg.weights_n_bits
-        self.weights_bias_correction = qc.weights_bias_correction
-        self.weights_second_moment_correction = qc.weights_second_moment_correction
-        self.weights_per_channel_threshold = op_cfg.weights_per_channel_threshold
-        self.enable_weights_quantization = op_cfg.enable_weights_quantization
-        self.min_threshold = qc.min_threshold
-        self.l_p_value = qc.l_p_value
-
-
-    @property
-    def weights_error_method(self) -> QuantizationErrorMethod:
-        """
-        weights_error_method getter.
-        """
-        return self._weights_error_method
+                activation_quantizers = [quantizer_for_node(**kwargs)] * num_of_outputs
 
-    @weights_error_method.setter
-    def weights_error_method(self, value: QuantizationErrorMethod):
-        """
-        weights_error_method setter.
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node with no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return KerasActivationQuantizationHolder(activation_quantizers[0])
 
-        Args:
-            value: New weights_error_method to set to the node weights configuration.
+        Logger.critical(f"'KerasActivationQuantizationHolder' supports only one quantizer, but found {len(activation_quantizers)} for node {n}")# pragma: no cover
 
+    def build_model(self) -> Tuple[Model, UserInformation,
+                                   Dict[str, Union[KerasQuantizationWrapper, KerasActivationQuantizationHolder]]]:
         """
-        self._weights_error_method = value
-        self.weights_quantization_params_fn = get_weights_quantization_params_fn(weights_quantization_method=self.weights_quantization_method)
+        Build a Keras mixed-precision model and return it.
+        Used the basic Keras model builder to build the model, and adding a mapping between each configurable node to
+        a list of layers (from the new model) that are matching to the node (either KerasQuantizationWrapper or
+        KerasActivationQuantizationHolder type layers).
+        This mapping is used during mixed precision metric computation to enforce pairs of weights-activation bit-width
+        candidates when configuring a model.
 
+        Returns: Mixed-precision Keras model.
 
-    def set_weights_quantization_fn(self, weights_quantization_fn: Callable):
         """
-        Sets weights quantization function for the node.
+        model, user_info = super().build_model()
 
-        Args:
-            weights_quantization_fn: Function for quantazing the weights.
+        # creating a mapping between graph nodes and model's layers for mixed precision configurability
+        conf_node2layers = {n.name: self._find_layers_in_model_by_node(n, model.layers)
+                            for n in self.graph.get_configurable_sorted_nodes(self.fw_info)}
 
-        """
-        self.weights_quantization_fn = weights_quantization_fn
+        return model, user_info, conf_node2layers
 
-    def set_weights_quantization_params_fn(self, weights_quantization_params_fn: Callable):
+    @staticmethod
+    def _get_weights_quant_layers(n: BaseNode, layers_list: List[Layer]) -> List[KerasQuantizationWrapper]:
         """
-        Sets weights params function for the node.
+        Filters KerasQuantizationWrapper layers from an MP model that are matching to the given graph node.
 
         Args:
-            weights_quantization_params_fn: Function for calculating the weights params.
+            n: A configurable graph node.
+            layers_list: Mixed precision model layers list.
 
-        """
-        self.weights_quantization_params_fn = weights_quantization_params_fn
+        Returns: A list of layers that responsible for the node's weights quantization.
 
-    def set_weights_quantization_param(self,
-                                       weights_params: dict):
         """
-         Set a quantization parameter for the node's weights.
-
-        Args:
-            weights_params: Dictionary that contains weight quantization params.
+        return [_l for _l in layers_list if isinstance(_l, KerasQuantizationWrapper) and _l.layer.name == n.name]
 
+    @staticmethod
+    def _get_activation_quant_layers(n: BaseNode, layers_list: List[Layer]) -> List[KerasActivationQuantizationHolder]:
         """
-        assert self.enable_weights_quantization
-        for param_name, param_value in weights_params.items():
-            self.weights_quantization_params[param_name] = param_value
+        Filters KerasActivationQuantizationHolder layers from an MP model that are matching to the given graph node.
 
-    def calculate_and_set_weights_params(self, tensor_data: np.ndarray) -> float:
-        """
         Args:
-            tensor_data: Tensor content as Numpy array.
-
-        Returns:
-            Recalculated weights quantization params from the kernel and channel axis.
-
-        """
-        assert self.enable_weights_quantization
-        if self.weights_quantization_params_fn is not None:
-            self.set_weights_quantization_param(self.weights_quantization_params_fn(tensor_data,
-                                                                                    p=self.l_p_value,
-                                                                                    n_bits=self.weights_n_bits,
-                                                                                    per_channel=self.weights_per_channel_threshold and self.weights_channels_axis is not None,
-                                                                                    channel_axis=self.weights_channels_axis,
-                                                                                    min_threshold=self.min_threshold))
-        else:
-            return self.set_weights_quantization_param({})
+            n: A configurable graph node.
+            layers_list: Mixed precision model layers list.
 
-    def has_weights_quantization_params(self) -> bool:
-        """
-
-        Returns: Whether NodeQuantizationConfig has weights quantization params or not.
+        Returns: A list of layers that responsible for the node's activation quantization.
 
         """
-        return len(self.weights_quantization_params) > 0
+        return [_l for _l in layers_list if isinstance(_l, KerasActivationQuantizationHolder)
+                and (_l.inbound_nodes[0].inbound_layers.name == n.name or
+                     (isinstance(_l.inbound_nodes[0].inbound_layers, KerasQuantizationWrapper) and
+                      _l.inbound_nodes[0].inbound_layers.layer.name == n.name))]
 
-    def __eq__(self, other: Any) -> bool:
+    def _find_layers_in_model_by_node(self, n: BaseNode, layers_list: List[Layer]) -> \
+            List[Union[KerasQuantizationWrapper, KerasActivationQuantizationHolder]]:
         """
-        Compares the object to another object to find if they are equal.
+        Retrieves layers from an MP model that are matching to the given graph node, that is, these are either
+        KerasQuantizationWrapper layers or KerasActivationQuantizationHolder layers that are responsible for the graph
+        configurable model quantization.
 
         Args:
-            other: An object to compare to.
+            n: A configurable graph node.
+            layers_list: Mixed precision model layers list.
 
-        Returns: Whether the objects are identical or not.
+        Returns: A list of layers that responsible for the node's quantization.
 
         """
-        if not isinstance(other, NodeWeightsQuantizationConfig):
-            return False
+        # Only layers with kernel op are considered weights configurable
+        kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+        weights_quant = False if kernel_attr is None else n.is_weights_quantization_enabled(kernel_attr)
+        act_quant = n.is_activation_quantization_enabled()
+
+        if weights_quant and not act_quant:
+            return self._get_weights_quant_layers(n, layers_list)
+        elif not weights_quant and act_quant:
+            return self._get_activation_quant_layers(n, layers_list)
+        elif weights_quant and act_quant:
+            return self._get_weights_quant_layers(n, layers_list) + self._get_activation_quant_layers(n, layers_list)
+        else:
+            Logger.critical(f"Expected node {n.name} to have either weights or activation quantization configured, but both are disabled.")# pragma: no cover
 
-        return self.weights_quantization_fn == other.weights_quantization_fn and \
-               self.weights_quantization_params_fn == other.weights_quantization_params_fn and \
-               self.weights_channels_axis == other.weights_channels_axis and \
-               self.weights_error_method == other.weights_error_method and \
-               self.weights_quantization_method == other.weights_quantization_method and \
-               self.weights_n_bits == other.weights_n_bits and \
-               self.weights_bias_correction == other.weights_bias_correction and \
-               self.weights_second_moment_correction == other.weights_second_moment_correction and \
-               self.weights_per_channel_threshold == other.weights_per_channel_threshold and \
-               self.enable_weights_quantization == other.enable_weights_quantization and \
-               self.min_threshold == other.min_threshold and \
-               self.l_p_value == other.l_p_value
-
-    def __hash__(self):
-        return hash((self.weights_quantization_fn,
-                     self.weights_quantization_params_fn,
-                     self.weights_channels_axis,
-                     self.weights_error_method,
-                     self.weights_quantization_method,
-                     self.weights_n_bits,
-                     self.weights_bias_correction,
-                     self.weights_second_moment_correction,
-                     self.weights_per_channel_threshold,
-                     self.enable_weights_quantization,
-                     self.min_threshold,
-                     self.l_p_value))
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_config.py`

 * *Files 8% similar despite different names*

```diff
@@ -46,15 +46,14 @@
 class QuantizationConfig:
 
     def __init__(self,
                  activation_error_method: QuantizationErrorMethod = QuantizationErrorMethod.MSE,
                  weights_error_method: QuantizationErrorMethod = QuantizationErrorMethod.MSE,
                  relu_bound_to_power_of_2: bool = False,
                  weights_bias_correction: bool = True,
-                 weights_per_channel_threshold: bool = True,
                  weights_second_moment_correction: bool = False,
                  input_scaling: bool = False,
                  softmax_shift: bool = False,
                  shift_negative_activation_correction: bool = False,
                  activation_channel_equalization: bool = False,
                  z_threshold: float = math.inf,
                  min_threshold: float = MIN_THRESHOLD,
@@ -69,49 +68,47 @@
 
         Args:
             activation_error_method (QuantizationErrorMethod): Which method to use from QuantizationErrorMethod for activation quantization threshold selection.
             weights_error_method (QuantizationErrorMethod): Which method to use from QuantizationErrorMethod for activation quantization threshold selection.
             relu_bound_to_power_of_2 (bool): Whether to use relu to power of 2 scaling correction or not.
             weights_bias_correction (bool): Whether to use weights bias correction or not.
             weights_second_moment_correction (bool): Whether to use weights second_moment correction or not.
-            weights_per_channel_threshold (bool): Whether to quantize the weights per-channel or not (per-tensor).
             input_scaling (bool): Whether to use input scaling or not.
             softmax_shift (bool): Whether to use softmax shift or not.
             shift_negative_activation_correction (bool): Whether to use shifting negative activation correction or not.
             activation_channel_equalization (bool): Whether to use activation channel equalization correction or not.
             z_threshold (float): Value of z score for outliers removal.
             min_threshold (float): Minimum threshold to use during thresholds selection.
             l_p_value (int): The p value of L_p norm threshold selection.
             block_collapsing (bool): Whether to collapse block one to another in the input network
             shift_negative_ratio (float): Value for the ratio between the minimal negative value of a non-linearity output to its activation threshold, which above it - shifting negative activation should occur if enabled.
             shift_negative_threshold_recalculation (bool): Whether or not to recompute the threshold after shifting negative activation.
-            shift_negative_params_search (bool): Whether to search for optimal shift and threshold in shift negative activation (experimental)
+            shift_negative_params_search (bool): Whether to search for optimal shift and threshold in shift negative activation.
 
         Examples:
             One may create a quantization configuration to quantize a model according to.
             For example, to quantize a model's weights and activation using thresholds, such that
             weights threshold selection is done using MSE, activation threshold selection is done using NOCLIPPING (min/max),
-            enabling relu_bound_to_power_of_2, weights_bias_correction, and quantizing the weights per-channel,
+            enabling relu_bound_to_power_of_2, weights_bias_correction,
             one can instantiate a quantization configuration:
 
             >>> import model_compression_toolkit as mct
-            >>> qc = mct.core.QuantizationConfig(activation_error_method=mct.core.QuantizationErrorMethod.NOCLIPPING,weights_error_method=mct.core.QuantizationErrorMethod.MSE,relu_bound_to_power_of_2=True,weights_bias_correction=True,weights_per_channel_threshold=True)
+            >>> qc = mct.core.QuantizationConfig(activation_error_method=mct.core.QuantizationErrorMethod.NOCLIPPING, weights_error_method=mct.core.QuantizationErrorMethod.MSE, relu_bound_to_power_of_2=True, weights_bias_correction=True)
 
 
             The QuantizationConfig instanse can then be passed to
             :func:`~model_compression_toolkit.ptq.keras_post_training_quantization`
 
         """
 
         self.activation_error_method = activation_error_method
         self.weights_error_method = weights_error_method
         self.relu_bound_to_power_of_2 = relu_bound_to_power_of_2
         self.weights_bias_correction = weights_bias_correction
         self.weights_second_moment_correction = weights_second_moment_correction
-        self.weights_per_channel_threshold = weights_per_channel_threshold
         self.activation_channel_equalization = activation_channel_equalization
         self.input_scaling = input_scaling
         self.softmax_shift = softmax_shift
         self.min_threshold = min_threshold
         self.shift_negative_activation_correction = shift_negative_activation_correction
         self.z_threshold = z_threshold
         self.l_p_value = l_p_value
@@ -122,15 +119,10 @@
         self.shift_negative_params_search = shift_negative_params_search
 
     def __repr__(self):
         return str(self.__dict__)
 
 
 # Default quantization configuration the library use.
-DEFAULTCONFIG = QuantizationConfig(QuantizationErrorMethod.MSE,
-                                   QuantizationErrorMethod.MSE,
-                                   relu_bound_to_power_of_2=False,
-                                   weights_bias_correction=True,
-                                   weights_second_moment_correction=False,
-                                   weights_per_channel_threshold=True,
-                                   input_scaling=False,
-                                   softmax_shift=False)
+DEFAULTCONFIG = QuantizationConfig(QuantizationErrorMethod.MSE, QuantizationErrorMethod.MSE,
+                                   relu_bound_to_power_of_2=False, weights_bias_correction=True,
+                                   weights_second_moment_correction=False, input_scaling=False, softmax_shift=False)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py`

 * *Files 26% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from collections.abc import Callable
 from functools import partial
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.quantization.quantizers.kmeans_quantizer import kmeans_quantizer
 from model_compression_toolkit.core.common.quantization.quantizers.lut_kmeans_quantizer import lut_kmeans_quantizer
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import power_of_two_quantizer, \
     symmetric_quantizer, uniform_quantizer
 
 
 def get_weights_quantization_fn(weights_quantization_method: QuantizationMethod) -> Callable:
     """
@@ -36,15 +36,14 @@
 
     if weights_quantization_method == QuantizationMethod.POWER_OF_TWO:
         quantizer_fn = power_of_two_quantizer
     elif weights_quantization_method == QuantizationMethod.SYMMETRIC:
         quantizer_fn = symmetric_quantizer
     elif weights_quantization_method == QuantizationMethod.UNIFORM:
         quantizer_fn = uniform_quantizer
-    elif weights_quantization_method == QuantizationMethod.KMEANS:
-        quantizer_fn = kmeans_quantizer
     elif weights_quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER, QuantizationMethod.LUT_SYM_QUANTIZER]:
         quantizer_fn = lut_kmeans_quantizer
     else:
-        raise Exception(
-            f'No quantizer function for the configuration of quantization method {weights_quantization_method}')
+        Logger.critical(
+            f"No quantizer function found for the specified quantization method: {weights_quantization_method}")
+
     return quantizer_fn
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,15 +14,14 @@
 # ==============================================================================
 
 from collections.abc import Callable
 from functools import partial
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.quantization.quantization_params_generation.kmeans_params import kmeans_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.lut_kmeans_params import \
     lut_kmeans_tensor, lut_kmeans_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import \
     symmetric_selection_tensor, symmetric_selection_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.uniform_selection import \
     uniform_selection_histogram, uniform_selection_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.power_of_two_selection import \
@@ -44,17 +43,16 @@
     elif activation_quantization_method == QuantizationMethod.SYMMETRIC:
         params_fn = symmetric_selection_histogram
     elif activation_quantization_method == QuantizationMethod.UNIFORM:
         params_fn = uniform_selection_histogram
     elif activation_quantization_method == QuantizationMethod.LUT_POT_QUANTIZER:
         params_fn = lut_kmeans_histogram
     else:
-        Logger.error(
-            f'No params function for the configuration of '
-            f'quantization method {activation_quantization_method}')  # pragma: no cover
+        Logger.critical(
+            f"No parameter function found for the specified quantization method: {activation_quantization_method}")  # pragma: no cover
     return params_fn
 
 
 def get_weights_quantization_params_fn(weights_quantization_method: QuantizationMethod) -> Callable:
     """
     Generate a function for finding weights quantization parameters.
 
@@ -66,18 +64,15 @@
     """
     if weights_quantization_method == QuantizationMethod.POWER_OF_TWO:
         params_fn = power_of_two_selection_tensor
     elif weights_quantization_method == QuantizationMethod.SYMMETRIC:
         params_fn = symmetric_selection_tensor
     elif weights_quantization_method == QuantizationMethod.UNIFORM:
         params_fn = uniform_selection_tensor
-    elif weights_quantization_method == QuantizationMethod.KMEANS:
-        params_fn = kmeans_tensor
     elif weights_quantization_method == QuantizationMethod.LUT_POT_QUANTIZER:
         params_fn = partial(lut_kmeans_tensor, is_symmetric=False)
     elif weights_quantization_method == QuantizationMethod.LUT_SYM_QUANTIZER:
         params_fn = partial(lut_kmeans_tensor, is_symmetric=True)
     else:
-        Logger.error(
-            f'No params function for the configuration of '
-            f'quantization method {weights_quantization_method}')  # pragma: no cover
+        Logger.critical(
+            f"No parameter function found for the specified quantization method: {weights_quantization_method}")  # pragma: no cover
     return params_fn
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py`

 * *Files 4% similar despite different names*

```diff
@@ -83,14 +83,15 @@
 
     Returns:
         The Lp-norm distance between the two histograms.
     """
 
     return np.sum((np.power(np.abs((q_bins - bins)[:-1]), p) * counts)) / np.sum(counts)
 
+
 def _kl_error_function(x: np.ndarray,
                        range_min: float,
                        range_max: float,
                        n_bins: int = 2048,
                        n_bits: int = 8) -> np.float32:
     """
     Compute the error function between a tensor to its quantized version.
@@ -139,14 +140,43 @@
                                bcq,
                                bv,
                                bc,
                                range_min=range_min,
                                range_max=range_max)
 
 
+def _kl_error_function_wrapper(x: np.ndarray,
+                               range_min: np.ndarray,
+                               range_max: np.ndarray,
+                               n_bins: int = 2048,
+                               n_bits: int = 8) -> np.ndarray:
+    """
+    Computes the error function between a tensor and its quantized version for each channel.
+    The error is based on the KL-divergence between the distributions.
+    The function uses a specified number of bins to compute the histogram of the float tensor.
+    It requires the threshold and number of bits used for quantization to determine the histogram's boundaries and the number of quantized bins.
+
+    Args:
+        x: Float tensor.
+        range_min: Array specifying the minimum bound of the quantization range for each channel.
+        range_max: Array specifying the maximum bound of the quantization range for each channel.
+        n_bins: Number of bins for the float histogram.
+        n_bits: Number of bits used for quantization.
+
+    Returns:
+        An array containing the KL-divergence between the float and quantized histograms of the tensor for each channel.
+
+    """
+
+    error_list = []
+    for j in range(x.shape[0]):  # iterate all channels of the tensor.
+        error_list.append(_kl_error_function(x[j], range_min[j], range_max[j], n_bins=n_bins, n_bits=n_bits))
+    return np.asarray(error_list)
+
+
 def _kl_error_histogram(q_bins: np.ndarray,
                         q_count: np.ndarray,
                         bins: np.ndarray,
                         counts: np.ndarray,
                         range_min: float,
                         range_max: float) -> np.float32:
     """
@@ -335,39 +365,41 @@
 
     return bins_subset, counts_subset
 
 
 def get_threshold_selection_tensor_error_function(quantization_method: QuantizationMethod,
                                                   quant_error_method: qc.QuantizationErrorMethod,
                                                   p: int,
+                                                  axis: int = None,
                                                   norm: bool = False,
                                                   n_bits: int = 8,
                                                   signed: bool = True) -> Callable:
     """
     Returns the error function compatible to the provided threshold method,
     to be used in the threshold optimization search for tensor quantization.
     Args:
-        quantization_method: Quantization method for threshold selection
-        quant_error_method: the requested error function type.
-        p: p-norm to use for the Lp-norm distance.
-        norm: whether to normalize the error function result.
-        n_bits: Number of bits to quantize the tensor.
-        signed: signed input
+        quantization_method: Method used for selecting the quantization threshold.
+        quant_error_method: Type of error function requested.
+        p: P-norm to use for calculating the Lp-norm distance.
+        axis: Axis along which the operation has been performed.
+        norm: Indicates whether to normalize the result of the error function.
+        n_bits: Number of bits used to quantize the tensor.
+        signed: Indicates whether the input is signed.
 
     Returns: a Callable method that calculates the error between a tensor and a quantized tensor.
     """
 
     quant_method_error_function_mapping = {
-        qc.QuantizationErrorMethod.MSE: lambda x, y, threshold: compute_mse(x, y, norm=norm),
-        qc.QuantizationErrorMethod.MAE: lambda x, y, threshold: compute_mae(x, y, norm=norm),
-        qc.QuantizationErrorMethod.LP: lambda x, y, threshold: compute_lp_norm(x, y, p=p, norm=norm),
+        qc.QuantizationErrorMethod.MSE: lambda x, y, threshold: compute_mse(x, y, norm=norm, axis=axis),
+        qc.QuantizationErrorMethod.MAE: lambda x, y, threshold: compute_mae(x, y, norm=norm, axis=axis),
+        qc.QuantizationErrorMethod.LP: lambda x, y, threshold: compute_lp_norm(x, y, p=p, norm=norm, axis=axis),
         qc.QuantizationErrorMethod.KL:
-            lambda x, y, threshold: _kl_error_function(x, range_min=threshold[0], range_max=threshold[1],
+            lambda x, y, threshold: _kl_error_function_wrapper(x, range_min=threshold[:,0], range_max=threshold[:,1],
                                                        n_bits=n_bits) if quantization_method == QuantizationMethod.UNIFORM
-            else _kl_error_function(x, range_min=0 if not signed else -threshold, range_max=threshold, n_bits=n_bits)
+            else _kl_error_function_wrapper(x, range_min=0 if not signed else -threshold, range_max=threshold, n_bits=n_bits)
     }
 
     return quant_method_error_function_mapping[quant_error_method]
 
 
 def get_threshold_selection_histogram_error_function(quantization_method: QuantizationMethod,
                                                      quant_error_method: qc.QuantizationErrorMethod,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py`

 * *Files 5% similar despite different names*

```diff
@@ -56,16 +56,15 @@
         is_symmetric (bool): Whether to apply symmetric weight quantization (default is False, meaning power of 2 quantization)
 
     Returns:
         A dictionary containing the cluster assignments according to the k-means algorithm,
         the thresholds per channel and the multiplier num bits.
     """
     if n_bits >= LUT_VALUES_BITWIDTH:
-        Logger.critical(f'Look-Up-Table bit configuration has {n_bits} bits, but must be less than '
-                        f'{LUT_VALUES_BITWIDTH}')  # pragma: no cover
+        Logger.critical(f'Look-Up-Table (LUT) bit configuration exceeds maximum: {n_bits} bits provided, must be less than {LUT_VALUES_BITWIDTH} bits.')  # pragma: no cover
     # TODO: need to set this externally
     if len(np.unique(tensor_data.flatten())) < 2 ** n_bits:
         n_clusters = len(np.unique(tensor_data.flatten()))
     else:
         n_clusters = 2 ** n_bits
     kmeans = KMeans(n_clusters=n_clusters, n_init=10)
 
@@ -117,16 +116,15 @@
 
     Returns:
         A dictionary containing the cluster assignments according to the k-means algorithm and
         the threshold for pre-clustering quantization.
     """
 
     if n_bits >= LUT_VALUES_BITWIDTH:
-        Logger.critical(f'Look-Up-Table bit configuration has {n_bits} bits. It must be less then '
-                        f'{LUT_VALUES_BITWIDTH}')  # pragma: no cover
+        Logger.critical(f'Look-Up-Table (LUT) bit configuration exceeds maximum: {n_bits} bits provided, must be less than {LUT_VALUES_BITWIDTH} bits.')  # pragma: no cover
 
     bins_with_values = np.abs(bins)[1:][counts > 0]
     if len(np.unique(bins_with_values.flatten())) < 2 ** n_bits:
         n_clusters = len(np.unique(bins_with_values.flatten()))
     else:
         n_clusters = 2 ** n_bits
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py`

 * *Files 1% similar despite different names*

```diff
@@ -51,16 +51,17 @@
     """
 
     if quant_error_method == qc.QuantizationErrorMethod.NOCLIPPING:
         tensor_max = get_tensor_max(tensor_data, per_channel, channel_axis, n_bits)
         threshold = max_power_of_two(tensor_max, min_threshold)
     else:
         signed = True  # weights are always signed
+        axis = -1 if per_channel else None
         error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.POWER_OF_TWO,
-                                                                       quant_error_method, p, norm=False, n_bits=n_bits,
+                                                                       quant_error_method, p, axis=axis, norm=False, n_bits=n_bits,
                                                                        signed=signed)
         threshold = qparams_selection_tensor_search(error_function,
                                                     tensor_data,
                                                     n_bits,
                                                     per_channel=per_channel,
                                                     channel_axis=channel_axis,
                                                     n_iter=n_iter,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,61 +8,68 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from tqdm import tqdm
 from typing import List
 
-from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_weights_computation import \
-    get_weights_qparams, get_channels_axis
+    get_weights_qparams
+from model_compression_toolkit.logger import Logger
 
 
 def calculate_quantization_params(graph: Graph,
-                                  fw_info: FrameworkInfo,
                                   nodes: List[BaseNode] = [],
-                                  specific_nodes: bool = False,
-                                  fw_impl: FrameworkImplementation = None):
+                                  specific_nodes: bool = False):
     """
     For a graph, go over its nodes, compute quantization params (for both weights and activations according
     to the given framework info), and create and attach a NodeQuantizationConfig to each node (containing the
     computed params).
     By default, the function goes over all nodes in the graph. However, the specific_nodes flag enables
-    to compute quantization paramss for specific nodes if the default behavior is unnecessary. For that,
-    a list of nodes nodes should be passed as well.
+    to compute quantization params for specific nodes if the default behavior is unnecessary. For that,
+    a list of nodes should be passed as well.
 
     Args:
-        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
         groups of layers by how they should be quantized, etc.)
         graph: Graph to compute its nodes' thresholds.
         nodes: List of nodes to compute their thresholds instead of computing it for all nodes in the graph.
         specific_nodes: Flag to compute thresholds for only specific nodes.
-        fw_impl: FrameworkImplementation with specific framework implementations.
 
     """
 
+    Logger.info(f"Running quantization parameters search. "
+                f"This process might take some time, "
+                f"depending on the model size and the selected quantization methods.\n")
+
     # Create a list of nodes to compute their thresholds
     nodes_list: List[BaseNode] = nodes if specific_nodes else graph.nodes()
 
-    for n in nodes_list:  # iterate only nodes that we should compute their thresholds
+    for n in tqdm(nodes_list, "Calculating quantization params"):  # iterate only nodes that we should compute their thresholds
         for candidate_qc in n.candidates_quantization_cfg:
-            if n.is_weights_quantization_enabled():
-                # If node's weights should be quantized, we compute its weights' quantization parameters
-                output_channels_axis, _ = get_channels_axis(candidate_qc.weights_quantization_cfg, fw_info, n.type)
-                weights_params = get_weights_qparams(n.get_weights_by_keys(fw_impl.constants.KERNEL),
-                                                     candidate_qc.weights_quantization_cfg,
-                                                     output_channels_axis)
-                candidate_qc.weights_quantization_cfg.set_weights_quantization_param(weights_params)
-                candidate_qc.weights_quantization_cfg.weights_channels_axis = output_channels_axis
+            for attr in n.get_node_weights_attributes():
+                if n.is_weights_quantization_enabled(attr):
+                    # If the node's weights attribute should be quantized, we compute its quantization parameters
+                    attr_cfg = candidate_qc.weights_quantization_cfg.get_attr_config(attr)
+                    channels_axis = attr_cfg.weights_channels_axis
+                    if channels_axis is not None:
+                        output_channels_axis = channels_axis[0]
+                    else:
+                        output_channels_axis = None
+                    weights_params = get_weights_qparams(n.get_weights_by_keys(attr),
+                                                         candidate_qc.weights_quantization_cfg,
+                                                         attr_cfg,
+                                                         output_channels_axis)
+                    attr_cfg.set_weights_quantization_param(weights_params)
+
             if n.is_activation_quantization_enabled():
                 # If node's activations should be quantized as well, we compute its activation quantization parameters
                 activation_params = get_activations_qparams(
                     activation_quant_cfg=candidate_qc.activation_quantization_cfg,
                     nodes_prior_info=n.prior_info,
                     out_stats_container=graph.get_out_stats_collector(n))
                 # Create a NodeQuantizationConfig containing all quantization params and attach it to the node
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -722,13 +722,11 @@
 
     Args:
         error_function: error_function: Function to compute the error between the original and quantized tensors.
         float_tensor: Numpy array with float tensor's content.
         q_tensor: Numpy array with quantized tensor's content.
         in_params: Quantization params the tensor is quantized by (used in specific error functions only).
 
-    Returns: A list of error values per-channel for the quantized tensor, according to the error function.
+    Returns: An array of error values for each channel of the quantized tensor, as determined by the specified error function.
     """
-    _error_per_list = []
-    for j in range(float_tensor.shape[0]):  # iterate all channels of the tensor.
-        _error_per_list.append(error_function(float_tensor[j, :], q_tensor[j, :], in_params[j]))
-    return np.asarray(_error_per_list)
+    return error_function(float_tensor, q_tensor, in_params)
+
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,15 +54,16 @@
 
     tensor_max = get_tensor_max(tensor_data, per_channel, channel_axis, n_bits)
 
     if quant_error_method == qc.QuantizationErrorMethod.NOCLIPPING:
         threshold = get_init_threshold(min_threshold, tensor_max, per_channel)
     else:
         signed = True  # weights are always signed
-        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.SYMMETRIC, quant_error_method, p, norm=False, n_bits=n_bits, signed=signed)
+        axis = -1 if per_channel else None
+        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.SYMMETRIC, quant_error_method, p, axis=axis, norm=False, n_bits=n_bits, signed=signed)
         threshold = qparams_symmetric_selection_tensor_search(error_function,
                                                               tensor_data,
                                                               tensor_max,
                                                               n_bits,
                                                               per_channel,
                                                               channel_axis,
                                                               min_threshold=min_threshold,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,16 @@
     """
     tensor_min = get_tensor_min(tensor_data, per_channel, channel_axis)
     tensor_max = get_tensor_max(tensor_data, per_channel, channel_axis, n_bits, is_uniform_quantization=True)
 
     if quant_error_method == qc.QuantizationErrorMethod.NOCLIPPING:
         mm = tensor_min, tensor_max
     else:
-        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.UNIFORM, quant_error_method, p, norm=False)
+        axis = -1 if per_channel else None
+        error_function = get_threshold_selection_tensor_error_function(QuantizationMethod.UNIFORM, quant_error_method, p, axis=axis, norm=False)
         mm = qparams_uniform_selection_tensor_search(error_function,
                                                      tensor_data,
                                                      tensor_min,
                                                      tensor_max,
                                                      n_bits,
                                                      per_channel,
                                                      channel_axis)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,44 +15,40 @@
 
 import copy
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
-from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_kernel_by_weights_qc
+from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_weights_attr_by_qc
 from model_compression_toolkit.logger import Logger
 
 
-def quantize_graph_weights(graph: Graph,
-                           fw_info: FrameworkInfo,
-                           fw_impl: FrameworkImplementation) -> Graph:
+def quantize_graph_weights(graph: Graph) -> Graph:
     """
     Get a graph representing a model, and quantize its nodes' weights.
     Each node is quantized according to the passed framework info and quantization configuration.
     If weights bias correction is enabled in the quantization configuration, a bias correction term
     is calculated and subtracted from the original node's bias. The graph is quantized in-place.
 
     Args:
         graph: Graph to quantize its nodes.
-        fw_info: Framework information needed for quantizing the graph's nodes' weights and activations.
-        fw_impl: FrameworkImplementation with specific framework implementations.
 
     """
     # Iterate over nodes in the graph and quantize each node's weights and activations
     # (according to operators groups in framework info).
     for n in graph.nodes():
+        for attr in n.get_node_weights_attributes():
+            if n.is_weights_quantization_enabled(attr):
+                quantized_attr, io_channels_axes = \
+                    get_quantized_weights_attr_by_qc(attr,
+                                                     n,
+                                                     n.final_weights_quantization_cfg.get_attr_config(attr))
+
+                Logger.debug(
+                    f'Weights attribute: {attr} of node name: {n.name} has the following quantization params: '
+                    f'{str(n.final_weights_quantization_cfg.get_attr_config(attr).weights_quantization_params)}')
 
-        if n.is_weights_quantization_enabled():
-            quantized_kernel, io_channels_axes = get_quantized_kernel_by_weights_qc(fw_info,
-                                                                                    n,
-                                                                                    n.final_weights_quantization_cfg,
-                                                                                    fw_impl=fw_impl)
-
-            Logger.debug(
-                f'Node name: {n.name} has the following quantization params: '
-                f'{str(n.final_weights_quantization_cfg.weights_quantization_params)}')
-
-            # Set the kernel node to be the quantized kernel.
-            n.set_weights_by_keys(fw_impl.constants.KERNEL, quantized_kernel)
+                # Set the attribute to be the quantized attribute.
+                n.set_weights_by_keys(attr, quantized_attr)
 
     return graph
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantize_node.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/exporter.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,55 +10,81 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
-from model_compression_toolkit.core import common
-from model_compression_toolkit.logger import Logger
+from typing import Tuple, Any
+
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeWeightsQuantizationConfig
-from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_weights_computation \
-    import \
-    get_channels_axis
+from model_compression_toolkit.core.common import FrameworkInfo
+from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
+from model_compression_toolkit.core.common.quantization.quantize_graph_weights import quantize_graph_weights
+
+from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
+from model_compression_toolkit.core.common.user_info import UserInformation
+
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
+
+
+def _quantize_model(tg: Graph,
+                    fw_info: FrameworkInfo,
+                    fw_impl: FrameworkImplementation,
+                    tb_w: TensorboardWriter) -> Tuple[Any, UserInformation]:
+    """
+    Quantize graph's weights, and build a quantized framework model from it.
+
+    Args:
+        tg: A prepared for quantization graph.
+        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
+        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        tb_w: TensorBoardWriter object to log events.
+
+    Returns:
+        Quantized model in the input framework, and information the user may need in order to use the quantized model.
+    """
 
+    quantized_tg = quantize_graph_weights(tg)
+    if tb_w is not None:
+        tb_w.add_graph(quantized_tg, 'after_quantization')
+    ######################################
+    # Back2Framework
+    ######################################
+    # Before building a quantized model, first apply some substitutions.
+    quantized_tg = substitute(quantized_tg,
+                              fw_impl.get_substitutions_pre_build())
+
+    quantized_model, user_info = fw_impl.model_builder(quantized_tg,
+                                                       mode=ModelBuilderMode.QUANTIZED,
+                                                       fw_info=fw_info)
+    return quantized_model, user_info
+
+
+def export_model(tg,
+                 fw_info,
+                 fw_impl,
+                 tb_w,
+                 bit_widths_config):
 
-def get_quantized_kernel_by_weights_qc(fw_info: FrameworkInfo,
-                                       n: BaseNode,
-                                       weights_qc: NodeWeightsQuantizationConfig,
-                                       fw_impl: FrameworkImplementation):
     """
-    For a node and weights quantization configuration, compute
-    the quantized kernel of the node and return it and the input/output channels indices.
+    A function for quantizing the graph's weights and build a quantized framework model from it.
 
     Args:
-        fw_info: A FrameworkInfo object Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
-        n: Node to quantize its kernel.
-        weights_qc: Weight quantization configuration to use for the quantization.
-        fw_impl: FrameworkImplementation with specific framework implementations.
+        tg: A prepared for quantization graph.
+        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
+        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        tb_w: TensorBoardWriter object to log events.
+        bit_widths_config: mixed-precision bit configuration to be added to model user_info
 
     Returns:
-        A quantized kernel of the node using a weights quantization configuration.
+        Quantized model in the input framework, and information the user may need in order to use the quantized model.
     """
+    quantized_model, user_info = _quantize_model(tg,
+                                                 fw_info,
+                                                 fw_impl,
+                                                 tb_w)
+    user_info.mixed_precision_cfg = bit_widths_config
 
-    # If weights should be quantized per-channel but a kernel channels mapping is missing.
-    if weights_qc.weights_per_channel_threshold and fw_info.kernel_channels_mapping is \
-            None:
-        Logger.warning(
-            'Weights Per Channel Quantization requires channel mapping function but framework info '
-            'does not contain one')
-    output_channels_axis, input_channels_axis = get_channels_axis(weights_qc,
-                                                                  fw_info,
-                                                                  n.type)
-
-    Logger.debug(f'quantizing {n.name} with {weights_qc.weights_n_bits} bits')
-    quantized_kernel = weights_qc.weights_quantization_fn(n.get_weights_by_keys(fw_impl.constants.KERNEL),
-                                                          n_bits=weights_qc.weights_n_bits,
-                                                          signed=True,
-                                                          quantization_params=weights_qc.weights_quantization_params,
-                                                          per_channel=weights_qc.weights_per_channel_threshold,
-                                                          output_channels_axis=output_channels_axis)
+    return quantized_model, user_info
 
-    return quantized_kernel, (input_channels_axis, output_channels_axis)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py`

 * *Files 13% similar despite different names*

```diff
@@ -9,45 +9,49 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from sklearn.cluster import KMeans
 import numpy as np
 
-from model_compression_toolkit.constants import LUT_VALUES, MIN_THRESHOLD, SCALE_PER_CHANNEL
-from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import kmeans_assign_clusters
+from model_compression_toolkit.constants import LUT_VALUES, SCALE_PER_CHANNEL, \
+    LUT_VALUES_BITWIDTH
+from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import kmeans_assign_clusters, \
+    get_quantized_tensor, int_quantization_with_threshold
 
 
-def kmeans_quantizer(tensor_data: np.ndarray,
+def lut_kmeans_quantizer(tensor_data: np.ndarray,
                         n_bits: int,
                         signed: bool,
                         quantization_params: dict,
                         per_channel: bool,
                         output_channels_axis: int) -> np.ndarray:
     """
-    Quantize a tensor according to k-means algorithm. This function assigns cluster centers
-    to the tensor data values.
+    Quantize a tensor with given cluster centers and thresholds-per-channel vector.
+    1. We divide tensor_data with the scale vector per channel.
+    2. We scale the result to the range [-2^(LUT_VALUES_BITWIDTH-1), 2^(LUT_VALUES_BITWIDTH-1)-1].
+    3. We assign cluster centers to every value, multiply by thresholds_per_channel and divide by 2^(LUT_VALUES_BITWIDTH-1).
+    The result is the quantized tensor.
+
 
     Args:
         tensor_data: Tensor values to quantize.
         n_bits: Number of bits to quantize the tensor.
         signed: Whether the tensor contains negative values or not.
         quantization_params: Dictionary of specific parameters for this quantization function.
         per_channel: Whether to use separate quantization per output channel.
         output_channels_axis: Axis of the output channel.
 
     Returns:
         Quantized data.
     """
-    eps = 1e-8
     lut_values = quantization_params[LUT_VALUES]
-    scales_per_channel = quantization_params[SCALE_PER_CHANNEL]
-    tensor = (tensor_data / (scales_per_channel + eps))
+    thresholds_per_channel = quantization_params[SCALE_PER_CHANNEL]
+    tensor = int_quantization_with_threshold(tensor_data, thresholds_per_channel, LUT_VALUES_BITWIDTH)
     shape_before_kmeans = tensor.shape
     cluster_assignments = kmeans_assign_clusters(lut_values, tensor.reshape(-1, 1))
-    quant_tensor = lut_values[cluster_assignments].reshape(shape_before_kmeans)
-    if per_channel:
-        quant_tensor = (quant_tensor * scales_per_channel)
+    quant_tensor = get_quantized_tensor(lut_values[cluster_assignments].reshape(shape_before_kmeans),
+                                        thresholds_per_channel,
+                                        LUT_VALUES_BITWIDTH)
     return quant_tensor
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py`

 * *Files 1% similar despite different names*

```diff
@@ -234,15 +234,15 @@
         n_bits: number of bits the tensor will be quantized with
         is_uniform_quantization (bool): Whether the tensor will be quantized with uniform quantization (min-max)
 
     Returns: maximal value (or values).
 
     """
     if n_bits < 1:
-        Logger.error("n_bits must be positive")
+        Logger.critical(f"Parameter n_bits must be positive; however 'n_bits'={n_bits} was provided.")
     if is_uniform_quantization:
         expansion_factor = 1.0
     elif n_bits == 1:
         expansion_factor = 0.0
     else:
         expansion_factor = np.power(2.0, n_bits - 1) / (np.power(2.0, n_bits - 1) - 1)
     if per_channel:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py`

 * *Files 3% similar despite different names*

```diff
@@ -48,19 +48,21 @@
         output_channels_axis: Axis of the output channel.
 
     Returns:
         Quantized data.
     """
     threshold = quantization_params.get(THRESHOLD)
     if threshold is None:
-        Logger.error(f"{THRESHOLD} parameter must be defined in 'quantization_params'")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must be defined in 'quantization_params'")  # pragma: no cover
+
     if not threshold_is_power_of_two(threshold, per_channel):
-        Logger.error(f"Expects {THRESHOLD} parameter to be a power of two, but got {threshold}")  # pragma: no cover
+        Logger.critical(f"Expected '{THRESHOLD}' parameter to be a power of two, but received {threshold}.")# pragma: no cover
+
     if (per_channel and (threshold <= 0).any()) or ((not per_channel) and threshold <= 0):
-        Logger.error(f"{THRESHOLD} parameter must positive")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must positive")  # pragma: no cover
 
 
     return quantize_tensor(tensor_data,
                            threshold,
                            n_bits,
                            signed)
 
@@ -84,18 +86,18 @@
         output_channels_axis: Axis of the output channel.
 
     Returns:
         Quantized data.
     """
     threshold = quantization_params.get(THRESHOLD)
     if threshold is None:
-        Logger.error(f"{THRESHOLD} parameter must be defined in 'quantization_params'")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must be defined in 'quantization_params'")  # pragma: no cover
 
     if (per_channel and np.any(threshold <= 0)) or (not per_channel and threshold <= 0):
-        Logger.error(f"{THRESHOLD} parameter must positive")  # pragma: no cover
+        Logger.critical(f"'{THRESHOLD}' parameter must positive")  # pragma: no cover
 
     return quantize_tensor(tensor_data,
                            threshold,
                            n_bits,
                            signed)
 
 
@@ -118,10 +120,10 @@
 
     Returns:
         Quantized data.
     """
     range_min = quantization_params.get(RANGE_MIN)
     range_max = quantization_params.get(RANGE_MAX)
     if range_min is None or range_max is None:
-        Logger.error("'quantization range' parameters must be defined in 'quantization_params'")  # pragma: no cover
+        Logger.critical("'quantization range' parameters must be defined in 'quantization_params'")  # pragma: no cover
 
     return uniform_quantize_tensor(tensor_data, range_min, range_max, n_bits)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
 import copy
-from typing import List
+from typing import List, Tuple
 
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
@@ -67,29 +67,33 @@
     Create and set quantization configurations to a node (for both weights and activation).
 
     Args:
         node: Node to set its quantization configurations.
         quant_config: Quantization configuration to generate the node's configurations from.
         fw_info: Information needed for quantization about the specific framework.
         tpc: TargetPlatformCapabilities to get default OpQuantizationConfig.
-        mixed_precision_enable: is mixed precision enabled
+        mixed_precision_enable: is mixed precision enabled.
     """
     node_qc_options = node.get_qco(tpc)
 
     # Create QC candidates for weights and activation combined
-    weight_channel_axis = fw_info.kernel_channels_mapping.get(node.type)[0]
+    weight_channel_axis = fw_info.kernel_channels_mapping.get(node.type)
     node.candidates_quantization_cfg = _create_node_candidates_qc(quant_config,
                                                                   fw_info,
                                                                   weight_channel_axis,
                                                                   node_qc_options,
+                                                                  node,
                                                                   mixed_precision_enable=mixed_precision_enable)
 
+    # sorting the candidates by kernel attribute weights number of bits first and then by activation number of bits
+    # (in reversed order). since only kernel attribute is quantized in weights mixed precision,
+    # if the node doesn't have a kernel attribute, we only sort by activation_n_bits.
+    node.sort_node_candidates(fw_info)
+
     for candidate_qc in node.candidates_quantization_cfg:
-        candidate_qc.weights_quantization_cfg.enable_weights_quantization = \
-            candidate_qc.weights_quantization_cfg.enable_weights_quantization and node.has_weights_to_quantize(fw_info)
         candidate_qc.activation_quantization_cfg.enable_activation_quantization = \
             candidate_qc.activation_quantization_cfg.enable_activation_quantization and node.get_has_activation()
 
 
 def create_node_activation_qc(qc: QuantizationConfig,
                               fw_info: FrameworkInfo,
                               op_cfg: OpQuantizationConfig) -> NodeActivationQuantizationConfig:
@@ -104,99 +108,106 @@
 
     Returns:
         Activation quantization configuration of a node.
     """
 
     activation_quantization_fn = fw_info.activation_quantizer_mapping.get(op_cfg.activation_quantization_method)
     if activation_quantization_fn is None:
-        Logger.critical('Unknown quantization method for activations')  # pragma: no cover
+        Logger.critical('Unknown activation quantization method specified.')  # pragma: no cover
 
     activation_quantization_params_fn = get_activation_quantization_params_fn(op_cfg.activation_quantization_method)
 
     return NodeActivationQuantizationConfig(qc,
                                             op_cfg,
                                             activation_quantization_fn,
                                             activation_quantization_params_fn)
 
 
-def create_node_qc_candidate(qc: QuantizationConfig,
-                             fw_info: FrameworkInfo,
-                             weight_channel_axis: int,
-                             op_cfg: OpQuantizationConfig) -> CandidateNodeQuantizationConfig:
+def _create_node_single_candidate_qc(qc: QuantizationConfig,
+                                     fw_info: FrameworkInfo,
+                                     weight_channel_axis: Tuple[int, int],
+                                     op_cfg: OpQuantizationConfig,
+                                     node_attrs_list: List[str]) -> CandidateNodeQuantizationConfig:
     """
     Create quantization configuration candidate from a QuantizationConfig object.
     Creates both weights and activation quantization configurations
     and initialize a candidate object that encapsulates both.
 
     Args:
         qc: QuantizationConfig to create the node's config from.
         fw_info: Information about the specific framework the node was created from (e.g., whether its
             weights/activations should be quantized)
-        weight_channel_axis: Output channel index of the node's kernel.
+        weight_channel_axis: (Output, Input) channel index of the node's kernel.
         op_cfg: OpQuantizationConfig of the node with quantizers types to use when creating node quantization configuration.
+        node_attrs_list: A list of the node's weights attributes names.
 
     Returns: a CandidateNodeQuantizationConfig object with both weights and activation quantization config objects.
 
     """
 
-    # get attributes for weights quantization
-    weights_quantization_fn = get_weights_quantization_fn(op_cfg.weights_quantization_method)
-
-    if weights_quantization_fn is None:
-        Logger.critical('Unknown quantization method for weights')  # pragma: no cover
+    # parameters for weights attributes quantization are set within  CandidateNodeQuantizationConfig initialization
 
-    weights_quantization_params_fn = get_weights_quantization_params_fn(op_cfg.weights_quantization_method)
-
-    # get attributes for activation quantization
+    # get parameters for activation quantization
     activation_quantization_fn = fw_info.activation_quantizer_mapping.get(op_cfg.activation_quantization_method)
     if activation_quantization_fn is None:
-        Logger.critical('Unknown quantization method for activations')  # pragma: no cover
+        Logger.critical('Unknown activation quantization method specified.')  # pragma: no cover
 
     activation_quantization_params_fn = get_activation_quantization_params_fn(op_cfg.activation_quantization_method)
 
+    # TODO: remove this validation and warning once enabling all attributes quantization by default
+    attrs_with_enabled_quantization = [attr for attr, cfg in op_cfg.attr_weights_configs_mapping.items()
+                                       if cfg.enable_weights_quantization]
+    if len(attrs_with_enabled_quantization) > 1:
+        Logger.warning(f"Multiple weights attributes quantization is enabled via the provided TPC."
+                       f"Quantizing any attribute other than the kernel is experimental "
+                       f"and may be subject to unstable behavior."
+                       f"Attributes with enabled weights quantization: {attrs_with_enabled_quantization}.")
+
     return CandidateNodeQuantizationConfig(qc=qc,
                                            op_cfg=op_cfg,
                                            activation_quantization_fn=activation_quantization_fn,
                                            activation_quantization_params_fn=activation_quantization_params_fn,
-                                           weights_quantization_fn=weights_quantization_fn,
-                                           weights_quantization_params_fn=weights_quantization_params_fn,
-                                           weight_channel_axis=weight_channel_axis)
+                                           weights_channels_axis=weight_channel_axis,
+                                           node_attrs_list=node_attrs_list)
 
 
 def _create_node_candidates_qc(qc: QuantizationConfig,
                                fw_info: FrameworkInfo,
-                               weight_channel_axis: int,
+                               weight_channel_axis: Tuple[int, int],
                                node_qc_options: QuantizationConfigOptions,
+                               node: BaseNode,
                                mixed_precision_enable: bool = False) -> List[CandidateNodeQuantizationConfig]:
     """
     Create a list of candidates of weights and activation quantization configurations for a node.
 
     Args:
         qc: Quantization configuration the quantization process should follow.
         fw_info: Framework information (e.g., which layers should have their kernels' quantized).
-        weight_channel_axis: Output channel index of the node's kernel.
+        weight_channel_axis: (Output, Input) channel index of the node's kernel.
         node_qc_options: QuantizationConfigOptions for the node with quantization candidates information.
+        node: A node to set quantization configuration candidates to.
         mixed_precision_enable: is mixed precision enabled
 
     Returns:
         List of candidates of weights quantization configurations to set for a node.
     """
 
     candidates = []
+    node_attrs_list = node.get_node_weights_attributes()
+
     if mixed_precision_enable:
         for op_cfg in node_qc_options.quantization_config_list:
-            candidate_nbits_qc = copy.deepcopy(qc)
-            candidates.append(create_node_qc_candidate(candidate_nbits_qc,
-                                                       fw_info,
-                                                       weight_channel_axis,
-                                                       op_cfg))
-        # sorting the candidates by weights number of bits first and then by activation number of bits
-        # (in reversed order)
-        candidates.sort(key=lambda c: (c.weights_quantization_cfg.weights_n_bits,
-                                       c.activation_quantization_cfg.activation_n_bits), reverse=True)
+            candidate_qc = copy.deepcopy(qc)
+            candidates.append(_create_node_single_candidate_qc(candidate_qc,
+                                                               fw_info,
+                                                               weight_channel_axis,
+                                                               op_cfg,
+                                                               node_attrs_list))
+
     else:
-        candidates.append(create_node_qc_candidate(qc,
-                                                   fw_info,
-                                                   weight_channel_axis,
-                                                   node_qc_options.base_config))
+        candidates.append(_create_node_single_candidate_qc(qc,
+                                                           fw_info,
+                                                           weight_channel_axis,
+                                                           node_qc_options.base_config,
+                                                           node_attrs_list))
 
     return candidates
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/similarity_analyzer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/similarity_analyzer.py`

 * *Files 9% similar despite different names*

```diff
@@ -93,23 +93,23 @@
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         norm: whether to normalize the error function result.
         norm_eps: epsilon value for error normalization stability.
         batch: Whether to run batch similarity analysis or not.
-        axis: Axis along which the operator has been computed (not used in this function).
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The MSE distance between the two tensors.
     """
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     error = ((float_flat - fxp_flat) ** 2).mean(axis=-1)
     if norm:
         error /= ((float_flat ** 2).mean(axis=-1) + norm_eps)
 
     return error
 
@@ -125,24 +125,24 @@
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         norm: whether to normalize the error function result.
         norm_eps: epsilon value for error normalization stability.
         batch: Whether to run batch similarity analysis or not.
-        axis: Axis along which the operator has been computed (not used in this function).
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The mean average distance between the two tensors.
     """
 
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     error = np.abs(float_flat - fxp_flat).mean(axis=-1)
     if norm:
         error /= (np.abs(float_flat).mean(axis=-1) + norm_eps)
     return error
 
 
@@ -154,26 +154,26 @@
     the greater similarity there is between the two tensors.
 
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         eps: Small value to avoid zero division.
         batch: Whether to run batch similarity analysis or not.
-        axis: Axis along which the operator has been computed (not used in this function).
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The cosine similarity between two tensors.
     """
 
     validate_before_compute_similarity(float_tensor, fxp_tensor)
     if np.all(fxp_tensor == 0) and np.all(float_tensor == 0):
         return 1.0
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     float_norm = _similarity_tensor_norm(float_flat)
     fxp_norm = _similarity_tensor_norm(fxp_flat)
 
     # -1 <= cs <= 1
     axis = None if not batch else 1
     cs = np.sum(float_flat * fxp_flat, axis=axis) / ((float_norm * fxp_norm) + eps)
@@ -196,23 +196,23 @@
     Args:
         float_tensor: First tensor to compare.
         fxp_tensor: Second tensor to compare.
         p: p-norm to use for the Lp-norm distance.
         norm: whether to normalize the error function result.
         norm_eps: epsilon value for error normalization stability.
         batch: Whether to run batch similarity analysis or not.
-        axis: Axis along which the operator has been computed (not used in this function).
+        axis: Axis along which the operator has been computed.
 
     Returns:
         The Lp-norm distance between the two tensors.
     """
     validate_before_compute_similarity(float_tensor, fxp_tensor)
 
-    float_flat = flatten_tensor(float_tensor, batch)
-    fxp_flat = flatten_tensor(fxp_tensor, batch)
+    float_flat = flatten_tensor(float_tensor, batch, axis)
+    fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     error = (np.abs(float_flat - fxp_flat) ** p).mean(axis=-1)
     if norm:
         error /= ((np.abs(float_flat) ** p).mean(axis=-1) + norm_eps)
     return error
 
 
@@ -237,8 +237,11 @@
 
     float_flat = flatten_tensor(float_tensor, batch, axis)
     fxp_flat = flatten_tensor(fxp_tensor, batch, axis)
 
     non_zero_fxp_tensor = fxp_flat.copy()
     non_zero_fxp_tensor[non_zero_fxp_tensor == 0] = EPS
 
-    return np.mean(np.sum(np.where(float_flat != 0, float_flat * np.log(float_flat / non_zero_fxp_tensor), 0), axis=-1))
+    prob_distance = np.where(float_flat != 0, float_flat * np.log(float_flat / non_zero_fxp_tensor), 0)
+    # The sum is part of the KL-Divergance function.
+    # The mean is to aggregate the distance between each output probability vectors.
+    return np.mean(np.sum(prob_distance, axis=-1), axis=-1)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,17 +10,20 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform import AttributeQuantizationConfig
 
 
 def apply_bias_correction_to_graph(graph_to_apply_bias_correction: Graph,
                                    core_config: CoreConfig,
                                    fw_impl: FrameworkImplementation) -> Graph:
     """
     Get a graph, where each node has a final weights quantization configuration (with a bias
@@ -33,37 +36,49 @@
 
     Returns:
         Graph with bias correction apply to it's nodes.
     """
 
     graph = copy.deepcopy(graph_to_apply_bias_correction)
     for n in graph.nodes:
-        if n.is_weights_quantization_enabled() and core_config.quantization_config.weights_bias_correction \
-                and not n.final_weights_quantization_cfg.weights_second_moment_correction:
+        # bias correction is only relevant for nodes with kernel op
+        kernel_attr = graph.fw_info.get_kernel_op_attributes(n.type)[0]
+        if core_config.quantization_config.weights_bias_correction and kernel_attr is not None and \
+            n.is_weights_quantization_enabled(kernel_attr) and \
+                not n.final_weights_quantization_cfg.weights_second_moment_correction:
             # If a kernel was quantized and weights bias correction is enabled in n.quantization_cfg,
             # a bias correction term was calculated during model preparation, and is used now in the node's bias term.
             if n.final_weights_quantization_cfg.weights_bias_correction:
-                _apply_bias_correction_to_node(n, fw_impl)
+                _apply_bias_correction_to_node(n, fw_impl, core_config.quantization_config)
     return graph
 
 
-def _apply_bias_correction_to_node(node:BaseNode,
-                                   fw_impl: FrameworkImplementation):
+def _apply_bias_correction_to_node(node: BaseNode,
+                                   fw_impl: FrameworkImplementation,
+                                   qc: QuantizationConfig):
     """
     Set new bias to node using the bias-correction term that is stored in the
     final weights quantization configuration.
 
     Args:
         node: Node to set its corrected bias after bias-correction.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        qc: QuantizationConfig containing parameters of how the model should be quantized.
 
     """
     correction = node.final_weights_quantization_cfg.bias_corrected
 
     bias = node.get_weights_by_keys(fw_impl.constants.BIAS)  # get original bias from node's weights
 
     if bias is not None:  # If the layer has bias, we subtract the correction from original bias
         node.set_weights_by_keys(fw_impl.constants.BIAS, node.get_weights_by_keys(fw_impl.constants.BIAS) - correction)
 
-    else:  # It the layer has no bias, we consider it as if it has and its value is 0.
+    else:
+        # If the layer has no bias, we consider it as if it has and its value is 0 and add a "dummy" attribute
+        # configuration with disabled quantization.
         node.set_weights_by_keys(fw_impl.constants.BIAS, - correction)
         node.framework_attr[fw_impl.constants.USE_BIAS] = True  # Mark the use_bias attribute of the node.
+        node.final_weights_quantization_cfg.set_attr_config(fw_impl.constants.BIAS,
+                                                            WeightsAttrQuantizationConfig(
+                                                                qc,
+                                                                AttributeQuantizationConfig(
+                                                                    enable_weights_quantization=False)))
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import FrameworkInfo
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.model_collector import ModelCollector
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common.quantization.quantization_analyzer import analyzer_graph
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantize_graph_weights import quantize_graph_weights
 from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
 
 
 def _collect_and_assign_act_threshold(graph: Graph,
@@ -42,22 +41,18 @@
         representative_data_gen (Callable): Dataset used for calibration.
         core_config (CoreConfig): Configuration object containing parameters of how the model should be
          quantized, including mixed precision parameters.
         fw_info: FrameworkInfo object with information about the specific framework's model.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
      """
 
-    analyzer_graph(fw_impl.attach_sc_to_node,
-                   graph,
-                   fw_info,
-                   core_config.quantization_config)  # Mark points for statistics collection
-
     mi = ModelCollector(graph,
                         fw_impl,
-                        fw_info)
+                        fw_info,
+                        core_config.quantization_config) # Mark points for statistics collection
 
     for _data in tqdm(representative_data_gen()):
         mi.infer(_data)
 
     for n in list(graph.nodes):
         if n.is_activation_quantization_enabled():
             activation_params = get_activations_qparams(
@@ -77,17 +72,15 @@
         graph: Graph to build the from.
         fw_info: FrameworkInfo object with information about the specific framework's model.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     Returns:
         Quantized model for second moment correction.
     """
-    quantized_tg = quantize_graph_weights(graph,
-                                          fw_info=fw_info,
-                                          fw_impl=fw_impl)
+    quantized_tg = quantize_graph_weights(graph)
 
     quantized_model, user_info = fw_impl.model_builder(quantized_tg,
                                                        mode=ModelBuilderMode.FLOAT,
                                                        fw_info=fw_info)
     return quantized_model
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,104 +9,107 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-import copy
 from typing import Any
 
 import numpy as np
 
-from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode, Graph
-from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_kernel_by_weights_qc
+from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_weights_attr_by_qc
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.logger import Logger
 
 
 def compute_bias_correction_of_graph(graph: Graph,
-                                     core_config: CoreConfig,
                                      fw_info: FrameworkInfo,
                                      fw_impl: FrameworkImplementation) -> Graph:
     """
     For each node in a graph, and for each candidate weights quantization configuration,
     compute the bias-correction term, and store it in the candidate weights quantization configuration.
 
     Args:
         graph: Graph with nodes to compute the bias correction for
         each node's weights quantization configuration candidates.
-        core_config: CoreConfig containing parameters of how the model should be quantized.
         fw_info: Framework info like lists of nodes their kernel should quantized.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     Returns:
         Graph with bias correction for each weights quantization configuration candidate
         for each node.
     """
 
     for n in graph.nodes:
-        if n.is_weights_quantization_enabled() and core_config.quantization_config.weights_bias_correction:
-            _compute_bias_correction_per_candidate_qc(n,
-                                                      fw_info,
-                                                      graph.get_in_stats_collector(n),
-                                                      fw_impl=fw_impl)
+        # Bias correction is computed based on the quantized kernel, so we need to get the specific kernel attribute
+        # name out of all the weights attributes of the node.
+        if fw_info.is_kernel_op(n.type):
+            kernel_attr = fw_info.get_kernel_op_attributes(n.type)[0]
+            if n.is_weights_quantization_enabled(kernel_attr):
+                _compute_bias_correction_per_candidate_qc(n,
+                                                          kernel_attr,
+                                                          fw_info,
+                                                          graph.get_in_stats_collector(n),
+                                                          fw_impl=fw_impl)
     return graph
 
 
 def _compute_bias_correction_per_candidate_qc(node: BaseNode,
+                                              kernel_attr: str,
                                               fw_info: FrameworkInfo,
                                               node_in_stats_collector: BaseStatsCollector,
                                               fw_impl: FrameworkImplementation):
     """
     For each candidate weights quantization configuration of a given node,
     compute the bias-correction term, and store it in the candidate weights quantization configuration.
 
     Args:
         node: Node to compute the bias correction for its different candidates.
+        kernel_attr: The name of the kernel attribute of the node.
         fw_info: Framework info like lists of nodes their kernel should quantized.
         node_in_stats_collector: Statistics collector of the node for the mean per-channel.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     """
 
     for candidate_qc in node.candidates_quantization_cfg:
-        if candidate_qc.weights_quantization_cfg.enable_weights_quantization and not \
+        if candidate_qc.weights_quantization_cfg.weights_bias_correction and not \
                 candidate_qc.weights_quantization_cfg.weights_second_moment_correction:
-            quantized_kernel, io_channels_axes = get_quantized_kernel_by_weights_qc(fw_info,
-                                                                                    node,
-                                                                                    candidate_qc.weights_quantization_cfg,
-                                                                                    fw_impl=fw_impl)
-
-            # If a kernel was quantized and weights bias correction is enabled in n.quantization_cfg,
-            # a bias correction term is being calculated and used in the node's bias term.
-            if candidate_qc.weights_quantization_cfg.weights_bias_correction:
-                bias_correction_term = _get_bias_correction_term_of_node(io_channels_axes[0],
-                                                                         node,
-                                                                         node_in_stats_collector,
-                                                                         io_channels_axes[1],
-                                                                         quantized_kernel,
-                                                                         fw_impl=fw_impl)
 
-                # Store the correction term to use it later,
-                candidate_qc.weights_quantization_cfg.bias_corrected = bias_correction_term
+            quantized_kernel, io_channels_axes = get_quantized_weights_attr_by_qc(kernel_attr,
+                                                                                  node,
+                                                                                  candidate_qc.weights_quantization_cfg
+                                                                                  .get_attr_config(kernel_attr))
+
+            bias_correction_term = _get_bias_correction_term_of_node(io_channels_axes[0],
+                                                                     node,
+                                                                     node_in_stats_collector,
+                                                                     io_channels_axes[1],
+                                                                     quantized_kernel,
+                                                                     fw_impl=fw_impl)
+
+            # Store the correction term to use it later,
+            candidate_qc.weights_quantization_cfg.bias_corrected = bias_correction_term
+
 
 def is_non_positive_integer(x: float) -> bool:
     """
     Check if a variable is positive integer or not
     Args:
         x: input float to check
     Returns:
         True if x is non-positive integer
     """
     return x < 1 or int(x) != x
 
+
 def _compute_bias_correction(kernel: np.ndarray,
                              quantized_kernel: np.ndarray,
                              in_statistics_container: BaseStatsCollector,
                              output_channels_axis: int,
                              input_channels_axis: int) -> Any:
     """
     Compute the bias correction term for the bias in the error on the layers output,
@@ -183,21 +186,19 @@
 
 
     Returns:
         Bias-correction term to subtract from the current node's bias.
     """
 
     if output_channels_axis is None:
-        Logger.error(
-            f'Unknown output channel axis for node named: {n.name},'
-            f' please update channel mapping function')
+        Logger.critical(
+            f'Unknown output channel axis for node: {n.name}. Please update the channel mapping function.')
     if input_channels_axis is None:
-        Logger.error(
-            f'Unknown input channel axis for node named: {n.name},'
-            f' please update channel mapping function')
+        Logger.critical(
+            f'Unknown input channel axis for node: {n.name}. Please update the channel mapping function')
     # Compute the bias correction term.
     correction = _compute_bias_correction(n.get_weights_by_keys(fw_impl.constants.KERNEL),
                                           quantized_kernel,
                                           node_in_stats_collector,
                                           output_channels_axis,
                                           input_channels_axis)
     return correction
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,15 +53,14 @@
     tg_with_bias = substitute(transformed_graph, fw_impl.get_substitutions_statistics_correction(
         core_config.quantization_config))
 
     ########################################################
     # Compute bias correction to nodes' config candidates
     ########################################################
     tg_with_bias = compute_bias_correction_of_graph(tg_with_bias,
-                                                    core_config,
                                                     fw_info,
                                                     fw_impl)
 
     if tb_w is not None:
         tb_w.add_graph(tg_with_bias, 'statistics_computation')
 
     return tg_with_bias
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/apply_substitutions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/apply_substitutions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py`

 * *Files 2% similar despite different names*

```diff
@@ -89,15 +89,15 @@
         num_nodes_before_substition = len(graph.nodes)
         num_edges_before_substition = len(graph.edges)
 
         conv_node = edge_nodes[0]
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if conv_node.reuse or conv_node.reuse_group is not None:
+        if conv_node.is_reused():
             return graph
 
         bn_node = edge_nodes[1]
 
         if len(graph.get_next_nodes(conv_node)) > 1 or len(graph.get_prev_nodes(bn_node)) > 1:
             return graph
 
@@ -226,15 +226,15 @@
         num_nodes_before_substition = len(graph.nodes)
         num_edges_before_substition = len(graph.edges)
 
         bn_node, conv_node, _ = edge_nodes
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if conv_node.reuse or conv_node.reuse_group is not None or bn_node.reuse or bn_node.reuse_group is not None:
+        if conv_node.is_reused() or bn_node.is_reused():
             return graph
 
         if len(graph.get_next_nodes(bn_node)) > 1 or len(graph.get_prev_nodes(conv_node)) > 1:
             return graph
         if self.is_group_conv_fn(conv_node):
             return graph
         kernel = conv_node.get_weights_by_keys(self.kernel_str)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py`

 * *Files 24% similar despite different names*

```diff
@@ -15,20 +15,23 @@
 
 
 import copy
 from typing import Callable
 
 import numpy as np
 
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod, \
+    AttributeQuantizationConfig
 
 
 class BatchNormalizationReconstruction(common.BaseSubstitution):
     """
     Reconstruct BatchNormalization after linear layers.
     """
     def __init__(self,
@@ -75,28 +78,33 @@
         """
 
         num_nodes_before_substitution = len(graph.nodes)
         num_edges_before_substitution = len(graph.edges)
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if source_node.reuse or source_node.reuse_group is not None:
+        if source_node.is_reused():
             for qc in source_node.candidates_quantization_cfg:
                 qc.weights_quantization_cfg.weights_second_moment_correction = False
             return graph
 
         # We apply only on nodes with folded BatchNormalization.
         if source_node.prior_info.std_output is None or source_node.prior_info.mean_output is None:
             for qc in source_node.candidates_quantization_cfg:
                 qc.weights_quantization_cfg.weights_second_moment_correction = False
             return graph
 
         # This feature disabled for models with weights quantization method of Power of 2
         for qc in source_node.candidates_quantization_cfg:
-            if qc.weights_quantization_cfg.weights_quantization_method == QuantizationMethod.POWER_OF_TWO:
+            # this feature is relevant only for layers with kernel op
+            kernel_attr = graph.fw_info.get_kernel_op_attributes(source_node.type)
+            if kernel_attr is None:
+                Logger.error(f"Can't preform BatchNorm reconstruction on a node {source_node.name} without a kernel op.")
+            if (qc.weights_quantization_cfg.get_attr_config(kernel_attr[0]).weights_quantization_method
+                    == QuantizationMethod.POWER_OF_TWO):
                 Logger.warning("Second moment statistics correction feature disabled for models with weights "
                                "quantization method of Power of 2")
                 for qc_inner in source_node.candidates_quantization_cfg:
                     qc_inner.weights_quantization_cfg.weights_second_moment_correction = False
                 return graph
 
         eps = self.epsilon_val
@@ -115,16 +123,29 @@
         bn_node = self.create_bn_node(source_node, bn_node_weights)
 
         bn_node.prior_info = copy.deepcopy(source_node.prior_info)
 
         bn_node.candidates_quantization_cfg = copy.deepcopy(source_node.candidates_quantization_cfg)
 
         for qc in bn_node.candidates_quantization_cfg:
-            qc.weights_quantization_cfg.enable_weights_quantization = False
             qc.activation_quantization_cfg.enable_activation_quantization = False
+            for attr in bn_node.get_node_weights_attributes():
+                if qc.weights_quantization_cfg.has_attribute_config(attr):
+                    # we only create a BN layer to collect statistics, so we don't need to quantize anything,
+                    # but we do need to add the BN attributes to the reconstructed node.
+                    qc.weights_quantization_cfg.get_attr_config(attr).enable_weights_quantization = False
+                else:
+                    # setting a "dummy" attribute configuration with disabled quantization.
+                    # TODO: once enabling BN attributes quantization, need to figure out if thie
+                    #  reconstructed node BN attributes need to be quantized and how.
+                    qc.weights_quantization_cfg.set_attr_config(attr,
+                                                                WeightsAttrQuantizationConfig(
+                                                                    QuantizationConfig(),
+                                                                    AttributeQuantizationConfig(
+                                                                        enable_weights_quantization=False)))
 
         graph.reconnect_out_edges(current_node=source_node, new_node=bn_node)
         graph.replace_output_node(current_node=source_node, new_node=bn_node)
         graph.add_node_with_in_edges(bn_node, [source_node])
 
         assert len(graph.nodes) - num_nodes_before_substitution == 1
         assert len(graph.edges) - num_edges_before_substitution == 1
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py`

 * *Files 8% similar despite different names*

```diff
@@ -98,24 +98,22 @@
 
         # We apply only on nodes with reconstructed BatchNormalization.
         if not source_node.final_weights_quantization_cfg.weights_second_moment_correction:
             return graph
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if source_node.reuse or source_node.reuse_group is not None:
-            Logger.exception("If the linear operator is part of a reused group we should skip the the BN folding "
-                             "substitution and SMC feature")  # pragma: no cover
+        if source_node.is_reused():
+            Logger.critical("BN folding substitution cannot proceed if the linear operator is part of a reused group.")  # pragma: no cover
 
         bn_node = edge_nodes[1]
 
         if len(graph.get_next_nodes(source_node)) > 1 or len(graph.get_prev_nodes(bn_node)) > 1:
-            Logger.exception(
-                "If the linear operator has multiple outputs or the bn layer has multiple inputs we should "
-                "skip the the BN folding substitution and SMC feature")  # pragma: no cover
+            Logger.critical(
+                "BN folding substitution cannot proceed if the linear operator has multiple outputs or the BN layer has multiple inputs.")  # pragma: no cover
 
         kernel = source_node.get_weights_by_keys(self.kernel_str)
         bias = source_node.get_weights_by_keys(self.bias_str)
         gamma = bn_node.get_weights_by_keys(self.gamma_str)
         beta = bn_node.get_weights_by_keys(self.beta_str)
         moving_mean = bn_node.get_weights_by_keys(self.moving_mean_str)
         moving_variance = bn_node.get_weights_by_keys(self.moving_variance_str)
@@ -155,44 +153,48 @@
         graph.set_out_stats_collector_to_node(conv_bn, out_stats)
         graph.node_to_in_stats_collector.update({conv_bn: in_stats})
 
         graph.remove_edge(source_node, bn_node)
         graph.remove_node(bn_node)
         graph.remove_node(source_node)
 
-        self._calc_weights_quantization_params(conv_bn, weights_scale)
+        self._calc_weights_quantization_params(conv_bn, weights_scale, graph.fw_info)
 
         assert num_nodes_before_substitution - len(graph.nodes) == 1
         assert num_edges_before_substitution - len(graph.edges) == 1
         return graph
 
     def _calc_weights_quantization_params(self,
                                           conv_bn: BaseNode,
-                                          weights_scale: np.ndarray):
+                                          weights_scale: np.ndarray,
+                                          fw_info):
         """
         Update node weights quantization params.
         Args:
             conv_bn: Convolution node to update the weights quantization params.
             weights_scale: Weight scale factor in which to multiply the conv node's weight.
+            fw_info: FrameworkInfo object with information about the specific framework's model
         """
+        # Conv layer is ensured to have a kernel attribute
+        kernel_attr = fw_info.get_kernel_op_attributes(conv_bn.type)[0]
+        conv_bn_kernel_cfg = conv_bn.final_weights_quantization_cfg.get_attr_config(kernel_attr)
         # In case of SYMMETRIC weight quantization method, we update the threshold by weights_scale
-        if conv_bn.final_weights_quantization_cfg.weights_quantization_method == QuantizationMethod.SYMMETRIC:
-            original_threshold = conv_bn.final_weights_quantization_cfg.weights_quantization_params[THRESHOLD]
-            corr_dict = copy.deepcopy(conv_bn.final_weights_quantization_cfg.weights_quantization_params)
+        if conv_bn_kernel_cfg.weights_quantization_method == QuantizationMethod.SYMMETRIC:
+            original_threshold = conv_bn_kernel_cfg.weights_quantization_params[THRESHOLD]
+            corr_dict = copy.deepcopy(conv_bn_kernel_cfg.weights_quantization_params)
             corr_threshold, _ = self.update_kernel_for_bn_refusing_fn(conv_bn, original_threshold, weights_scale)
             corr_dict[THRESHOLD] = corr_threshold
-            conv_bn.final_weights_quantization_cfg.set_weights_quantization_param(corr_dict)
+            conv_bn_kernel_cfg.set_weights_quantization_param(corr_dict)
 
         # In case of UNIFORM weight quantization method, we update the range_min, range_max by weights_scale
-        elif conv_bn.final_weights_quantization_cfg.weights_quantization_method == QuantizationMethod.UNIFORM:
-            corr_dict = copy.deepcopy(conv_bn.final_weights_quantization_cfg.weights_quantization_params)
-            original_range_min = conv_bn.final_weights_quantization_cfg.weights_quantization_params[RANGE_MIN]
+        elif conv_bn_kernel_cfg.weights_quantization_method == QuantizationMethod.UNIFORM:
+            corr_dict = copy.deepcopy(conv_bn_kernel_cfg.weights_quantization_params)
+            original_range_min = conv_bn_kernel_cfg.weights_quantization_params[RANGE_MIN]
             corr_range_min, _ = self.update_kernel_for_bn_refusing_fn(conv_bn, original_range_min, weights_scale)
-            original_range_max = conv_bn.final_weights_quantization_cfg.weights_quantization_params[RANGE_MAX]
+            original_range_max = conv_bn_kernel_cfg.weights_quantization_params[RANGE_MAX]
             corr_range_max, _ = self.update_kernel_for_bn_refusing_fn(conv_bn, original_range_max, weights_scale)
             corr_dict[RANGE_MIN] = corr_range_min
             corr_dict[RANGE_MAX] = corr_range_max
-            conv_bn.final_weights_quantization_cfg.set_weights_quantization_param(corr_dict)
+            conv_bn_kernel_cfg.set_weights_quantization_param(corr_dict)
 
         else:
-            Logger.exception("Second moment statistics correction feature disabled for models with weights "
-                             "quantization method of Power of 2")  # pragma: no cover
+            Logger.critical("Second moment statistics correction feature is not supported for weights quantization methods other than 'SYMMETRIC' and 'UNIFORM'.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/linear_collapsing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/linear_collapsing.py`

 * *Files 14% similar despite different names*

```diff
@@ -87,22 +87,19 @@
         Args:
             graph: Graph we apply the substitution on.
             edge_nodes: Tuple of two linear nodes
         Returns:
             Graph after applying the substitution.
         """
 
-        first_node = edge_nodes[0]
-        second_node = edge_nodes[1]
+        first_node, second_node, _ = edge_nodes
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if first_node.reuse or first_node.reuse_group is not None:
-            return graph
-        if second_node.reuse or second_node.reuse_group is not None:
+        if first_node.is_reused() or second_node.is_reused():
             return graph
 
         # If there is an extra connection between these two nodes skip the substitution
         if len(graph.get_next_nodes(first_node)) > 1 or len(graph.get_prev_nodes(second_node)) > 1:
             return graph
 
         # Skip if convolution's data format is 'channels_first'
@@ -178,7 +175,87 @@
         graph.remove_node(second_node)
 
         # Sanity check
         assert num_nodes_before_substition - len(graph.nodes) == 1
         assert num_edges_before_substition - len(graph.edges) == 1
 
         return graph
+
+
+class Op2DAddConstCollapsing(common.BaseSubstitution):
+    """
+    Collapse Add-const into preceding Op2D (Not non-linear activation between them)
+    """
+    def __init__(self,
+                 first_node: NodeOperationMatcher,
+                 second_node: NodeOperationMatcher,
+                 op2d_collapsing_fn: Callable,
+                 bias_str: str,
+                 use_bias_str: str,
+                 layer_name_str: str = None):
+        """
+        Collapsing Add-const node (2nd node) to Op2D node (first node).
+        Args:
+            first_node: Node matcher for Op2d type nodes.
+            second_node: Node matcher for add type nodes.
+            op2d_collapsing_fn: Function for updating the convolution kernel and bias
+            bias_str: The framework specific attribute name of the convolution layer's bias.
+            use_bias_str: The framework specific attribute name of the convolution layer's bias flag.
+            layer_name_str: The framework specific attribute name of layer's name.
+        """
+        super().__init__(matcher_instance=EdgeMatcher(first_node, second_node))
+        self.op2d_collapsing_fn = op2d_collapsing_fn
+        self.bias_str = bias_str
+        self.use_bias_str = use_bias_str
+        self.layer_name_str = layer_name_str
+
+    def substitute(self,
+                   graph: Graph,
+                   edge_nodes: Tuple[BaseNode, BaseNode]) -> Graph:
+        """
+        Collapse linear layer into preceding linear layers.
+        Convolution condition:
+        |-------------------------|      |------|
+        | Op2D | ---> | Add-const |  ->  | Op2D |
+        |-------------------------|      |------|
+        Args:
+            graph: Graph we apply the substitution on.
+            edge_nodes: Tuple of linear node and add nodes
+        Returns:
+            Graph after applying the substitution.
+        """
+
+        first_node, second_node, _ = edge_nodes
+
+        # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
+        # we should skip the substitution.
+        if first_node.is_reused() or second_node.is_reused():
+            return graph
+
+        # If there is an extra connection between these two nodes skip the substitution
+        if len(graph.get_next_nodes(first_node)) > 1 or len(graph.get_prev_nodes(second_node)) > 1:
+            return graph
+
+        # New collapsed bias
+        bias = self.op2d_collapsing_fn(first_node, second_node, self.bias_str)
+
+        # New collapsed node
+        op2d_collapsed = copy.deepcopy(first_node)
+        op2d_collapsed_name = first_node.name + '_collapsed'
+        op2d_collapsed.name = op2d_collapsed_name
+        op2d_collapsed.framework_attr[self.use_bias_str] = True
+        op2d_collapsed.set_weights_by_keys(self.bias_str, bias)
+
+        if self.layer_name_str is not None:
+            op2d_collapsed.framework_attr[self.layer_name_str] = op2d_collapsed_name
+
+        # Update graph
+        graph.add_node(op2d_collapsed)
+        graph.reconnect_out_edges(current_node=second_node, new_node=op2d_collapsed)
+        graph.reconnect_in_edges(current_node=first_node, new_node=op2d_collapsed)
+        graph.replace_output_node(current_node=second_node, new_node=op2d_collapsed)
+
+        graph.remove_edge(first_node, second_node)
+        graph.remove_node(first_node)
+        graph.remove_node(second_node)
+
+        return graph
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py`

 * *Files 7% similar despite different names*

```diff
@@ -26,14 +26,17 @@
     the matches are not valid anymore, and we can find new matches.
     Args:
         graph: Graph to transform.
         linear_collapsing_substitution: substitution to apply on the graph.
     Returns:
         Transformed graph after applying all linear collapsing substitutions.
     """
+    # TODO: remove this if after adding Op2d-add_const collapse substitution in PyTorch
+    if linear_collapsing_substitution is None:
+        return graph
     matched_nodes = graph.filter(linear_collapsing_substitution.matcher_instance)
     matched_nodes_list = []
     match_indicator = True
     while len(matched_nodes) > 0 and match_indicator:
         match_indicator = False
         for matched_node in matched_nodes:
             if matched_node not in matched_nodes_list:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/residual_collapsing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/residual_collapsing.py`

 * *Files 2% similar despite different names*

```diff
@@ -59,17 +59,15 @@
         """
 
         first_node = edge_nodes[0]
         second_node = edge_nodes[1]
 
         # If the linear operator is part of a reused group (it is the "base" node, or a reused node),
         # we should skip the substitution.
-        if first_node.reuse or first_node.reuse_group is not None:
-            return graph
-        if second_node.reuse or second_node.reuse_group is not None:
+        if first_node.is_reused() or second_node.is_reused():
             return graph
 
         # Check if convolution and residual satisfy the collapsing conditions, otherwise skip substitution
         if len(graph.get_next_nodes(first_node)) > 1 or len(graph.get_prev_nodes(second_node)) != 2:
             return graph
 
         # Check if Add is residual connection, otherwise skip substitution
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/scale_equalization.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/scale_equalization.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 from typing import List
 
 import numpy as np
 import scipy
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 
 
 # We assume to have Gaussian distribution before the RelU operation
 # Hence, the activations after the RelU operation have Rectified Gaussian distribution
 # We need to calculate the "fixed" mean and std of the "new" activations
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,19 +12,22 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 import numpy as np
 from typing import List, Tuple, Any, Callable
 
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
+from model_compression_toolkit.core.common.quantization.node_quantization_config import WeightsAttrQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import FrameworkInfo, Graph, BaseNode
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, SHIFT_NEGATIVE_NON_LINEAR_NUM_BITS
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod, \
+    AttributeQuantizationConfig
 from model_compression_toolkit.core.common.quantization.set_node_quantization_config import create_node_activation_qc, \
     set_quantization_configs_to_node
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.error_functions import \
     _mse_error_histogram
@@ -59,14 +62,20 @@
         bias_flag_str: The framework specific attribute name of the bias flag.
     """
 
     bias = op2d_node.get_weights_by_keys(bias_str)
     if bias is None:
         bias = 0.0
         op2d_node.framework_attr[bias_flag_str] = True
+        # Add an attribute quantization configuration to the newly added bias attribute, with disabled quantization
+        for qc in op2d_node.candidates_quantization_cfg:
+            qc.weights_quantization_cfg.set_attr_config(bias_flag_str,
+                                                        WeightsAttrQuantizationConfig(QuantizationConfig(),
+                                                                                      AttributeQuantizationConfig(
+                                                                                          enable_weights_quantization=False)))
 
     # Each node adds a different noise due to the shifting. It depends on the
     # dimensions of the kernel, thus the correction term is a function of
     # the layer type.
     kernel = op2d_node.get_weights_by_keys(fw_info.kernel_ops_attributes_mapping.get(op2d_node.type)[0])
     if kernel is not None:
         output_channel_index, input_channel_index = fw_info.kernel_channels_mapping.get(op2d_node.type)
@@ -121,15 +130,15 @@
         node_to_insert: Node to add.
         first_node: Node to insert the new node after it.
 
     """
 
     last_nodes = graph.get_next_nodes(first_node)
     if len(last_nodes) != 1:
-        Logger.error('Can only insert if there is only one input')  # pragma: no cover
+        Logger.critical(f'Insertion requires exactly one successor node; {len(last_nodes)} successors found.')  # pragma: no cover
     last_node = last_nodes[0]
     insert_node_between_two_nodes(graph, node_to_insert, first_node, last_node)
 
 
 def insert_node_before_node(graph: Graph,
                             node_to_insert: BaseNode,
                             last_node: BaseNode):
@@ -143,15 +152,15 @@
         graph: Graph to add the new node to.
         node_to_insert: Node to add.
         last_node: Node to insert the new node after it.
 
     """
     first_nodes = graph.get_prev_nodes(last_node)
     if len(first_nodes) != 1:
-        Logger.error('Can only insert if there is only one input')  # pragma: no cover
+        Logger.critical('Insertion requires exactly one predecessor node; multiple or no predecessors found.')  # pragma: no cover
     first_node = first_nodes[0]
     insert_node_between_two_nodes(graph, node_to_insert, first_node, last_node)
 
 
 def remove_node_between_two_nodes(graph: Graph,
                                   node_to_remove: BaseNode,
                                   first_node: BaseNode,
@@ -222,15 +231,15 @@
     Returns:
         Graph after applying the shifting and correction.
     """
 
     min_to_correct, max_value2compare = graph.get_out_stats_collector(non_linear_node).get_min_max_values()
 
     if not non_linear_node.is_all_activation_candidates_equal():
-        Logger.error("Shift negative correction is not supported for more than one activation quantization "
+        Logger.critical("Shift negative correction is not supported for more than one activation quantization "
                      "configuration candidate")  # pragma: no cover
 
     # all candidates have same activation config, so taking the first candidate for calculations
     non_linear_node_cfg_candidate = non_linear_node.candidates_quantization_cfg[0].activation_quantization_cfg
 
     # get the non-linear activation threshold
     activation_threshold = non_linear_node_cfg_candidate.activation_quantization_params.get(THRESHOLD)
@@ -344,16 +353,17 @@
         set_quantization_configs_to_node(fw_info=fw_info,
                                          node=pad_node,
                                          quant_config=core_config.quantization_config,
                                          tpc=graph.tpc,
                                          mixed_precision_enable=core_config.mixed_precision_enable)
 
         for candidate_qc in pad_node.candidates_quantization_cfg:
-            candidate_qc.weights_quantization_cfg.enable_weights_quantization = False
             candidate_qc.activation_quantization_cfg.enable_activation_quantization = False
+            for attr in pad_node.get_node_weights_attributes():
+                candidate_qc.weights_quantization_cfg.get_attr_config(attr).enable_weights_quantization = False
 
         # Insert a pad node between the add node to the op2d, and create statistics for the pad node
         insert_node_before_node(graph,
                                 node_to_insert=pad_node,
                                 last_node=op2d_node)
 
         graph.set_out_stats_collector_to_node(pad_node,
@@ -378,15 +388,16 @@
             for bypass_candidate_qc in bypass_node.candidates_quantization_cfg:
                 if bypass_candidate_qc.activation_quantization_cfg:
                     bypass_candidate_qc.activation_quantization_cfg.activation_quantization_params[SIGNED] = False
                     graph.shift_stats_collector(bypass_node, np.array(shift_value))
 
     add_node_qco = add_node.get_qco(graph.tpc).quantization_config_list
     for op_qc_idx, candidate_qc in enumerate(add_node.candidates_quantization_cfg):
-        candidate_qc.weights_quantization_cfg.enable_weights_quantization = False
+        for attr in add_node.get_node_weights_attributes():
+            candidate_qc.weights_quantization_cfg.get_attr_config(attr).enable_weights_quantization = False
 
         candidate_qc.activation_quantization_cfg = create_node_activation_qc(core_config.quantization_config,
                                                                              fw_info,
                                                                              add_node_qco[op_qc_idx])
 
         candidate_qc.activation_quantization_cfg.set_activation_quantization_param({THRESHOLD: activation_threshold,
                                                                                     SIGNED: False})
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/softmax_shift.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,23 +44,24 @@
         if len(predecessors) != 1:
             return graph
 
         act_node = predecessors[0]
 
         if len(graph.out_edges(act_node)) > 1:
             Logger.warning(f"Node {act_node.name} has multiple outgoing edges, which is not supported with "
-                           f"mixed-precision bit-operations KPI, thus, edge {act_node.name} --> {weights_node.name} "
+                           f"mixed-precision bit-operations utilization, thus, edge {act_node.name} --> {weights_node.name} "
                            f"would not be counted in the bit-operations calculations.")
             return graph
 
         # Virtual composed activation-weights node
         # we pass a dummy initialization dict to initialize the super BaseNode class,
         # the actual arguments values are irrelevant because they are being overridden or not used
         v_node = VirtualActivationWeightsNode(act_node,
                                               weights_node,
+                                              fw_info=graph.fw_info,
                                               **weights_node.__dict__)
 
         # Update graph
         graph.add_node(v_node)
         graph.reconnect_in_edges(current_node=act_node, new_node=v_node)
         graph.reconnect_out_edges(current_node=weights_node, new_node=v_node)
         graph.replace_input_node(current_node=act_node, new_node=v_node)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/substitutions/weights_activation_split.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/substitutions/weights_activation_split.py`

 * *Files 6% similar despite different names*

```diff
@@ -45,29 +45,35 @@
         Args:
             graph: Graph we apply the substitution on.
             node: Node to split.
 
         Returns:
             Graph after applying the substitution.
         """
-
-        if not node.is_all_weights_candidates_equal() and not node.is_all_activation_candidates_equal():
+        # The decomposition works on linear nodes, that is, nodes with kernel ops
+        kernel_attr = graph.fw_info.get_kernel_op_attributes(node.type)[0]
+        if kernel_attr is None:
+            Logger.error(f"Trying to split node weights and activation, but node "
+                         f"{node.name} doesn't have a kernel attribute.")
+        if not node.is_all_weights_candidates_equal(kernel_attr) and not node.is_all_activation_candidates_equal():
             # Node has both different weights and different activation configuration candidates
-            weights_bits = [c.weights_quantization_cfg.weights_n_bits for c in node.get_unique_weights_candidates()]
+            weights_bits = [c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits
+                            for c in node.get_unique_weights_candidates(kernel_attr)]
             activation_bits = [c.activation_quantization_cfg.activation_n_bits for c in node.get_unique_activation_candidates()]
             expected_candidates = list(itertools.product(weights_bits, activation_bits))
-            all_candidates_bits = [(c.weights_quantization_cfg.weights_n_bits,
-                                    c.activation_quantization_cfg.activation_n_bits) for c in node.candidates_quantization_cfg]
+            all_candidates_bits = [(c.weights_quantization_cfg.get_attr_config(kernel_attr).weights_n_bits,
+                                    c.activation_quantization_cfg.activation_n_bits)
+                                   for c in node.candidates_quantization_cfg]
             if not set(expected_candidates).issubset(all_candidates_bits):
                 # Node is not composite, therefore, can't be split
-                Logger.critical(f"The graph contains a node {node.name} with non composite candidates."
-                                f"In order to run mixed-precision search with BOPS target KPI, "
-                                f"all model layers should be composite.")  # pragma: no cover
+                Logger.critical(f"The node {node.name} cannot be split as it has non-composite candidates. "
+                                f"For mixed-precision search with BOPS target resource utilization, "
+                                f"all model layers must be composite.")  # pragma: no cover
 
-        weights_node = VirtualSplitWeightsNode(node)
+        weights_node = VirtualSplitWeightsNode(node, kernel_attr)
         activation_node = VirtualSplitActivationNode(node, self.activation_layer_type, self.fw_attr)
 
         # Update graph
         graph.add_node(weights_node)
         graph.add_node(activation_node)
         graph.reconnect_in_edges(current_node=node, new_node=weights_node)
         graph.reconnect_out_edges(current_node=node, new_node=activation_node)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/user_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/user_info.py`

 * *Files 9% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     input scaling during the process).
     """
 
     def __init__(self):
         self.input_scale = 1
         self.gptq_info_dict = dict()
         self.mixed_precision_cfg = None
-        self.final_kpi = None
+        self.final_resource_utilization = None
 
     def set_input_scale(self, scale_value: float):
         """
         Set the UserInformation an input scale value.
 
         Args:
             scale_value: Scale factor to store in the UserInformation.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/final_config_visualizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/final_config_visualizer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/nn_visualizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/nn_visualizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -63,15 +63,15 @@
         Initialize a NNVisualizer object.
         Args:
             graph_float: Float version of the graph.
 
         """
 
         self.graph_float = graph_float
-        self.graph_quantized = quantize_graph_weights(graph_float, fw_info=fw_info, fw_impl=fw_impl)
+        self.graph_quantized = quantize_graph_weights(graph_float)
         self.fw_impl = fw_impl
         self.fw_info = fw_info
 
         # Get compare points of two graphs.
         self.compare_points, self.compare_points_name = _get_compare_points(self.graph_quantized)
         self.compare_points_float, self.compare_points_name_float = _get_compare_points(self.graph_float)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/common/visualization/tensorboard_writer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/visualization/tensorboard_writer.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from copy import deepcopy
 
 import io
+import os
 import numpy as np
 from PIL import Image
 from matplotlib.figure import Figure
 from tensorboard.compat.proto.attr_value_pb2 import AttrValue
 from tensorboard.compat.proto.config_pb2 import RunMetadata
 from tensorboard.compat.proto.event_pb2 import Event, TaggedRunMetadata
 from tensorboard.compat.proto.graph_pb2 import GraphDef
@@ -30,14 +31,17 @@
 from tensorboard.compat.proto.tensor_shape_pb2 import TensorShapeProto
 from tensorboard.summary.writer.event_file_writer import EventFileWriter
 from typing import List, Any, Dict
 from networkx import topological_sort
 from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.visualization.final_config_visualizer import \
+    WeightsFinalBitwidthConfigVisualizer, ActivationFinalBitwidthConfigVisualizer
 
 DEVICE_STEP_STATS = "/device:CPU:0"
 
 
 def get_node_properties(node_dict_to_log: dict,
                         output_shapes: List[tuple] = None) -> Dict[str, Any]:
     """
@@ -222,15 +226,15 @@
             # if they exist at all, as we can log the initial graph,
             # which its nodes do not have configurations yet.
             # Log final config or unified candidates, not both
             attr = dict()
             if n.final_weights_quantization_cfg is not None:
                 attr.update(n.final_weights_quantization_cfg.__dict__)
             elif n.candidates_quantization_cfg is not None:
-                attr.update(n.get_unified_weights_candidates_dict())
+                attr.update(n.get_unified_weights_candidates_dict(self.fw_info))
             return attr
 
         def __get_node_attr(n: BaseNode) -> Dict[str, Any]:
             """
             Create a dictionary to display as the node's attributes.
             The dictionary contains information from node's framework attributes and quantization attributes
 
@@ -482,7 +486,49 @@
 
         event = Event(summary=Summary(value=[Summary.Value(tag=figure_tag, image=img_summary)]))
 
         # Get the event writer for this tag name
         er = self.__get_event_writer_by_tag_name(main_tag_name)
         er.add_event(event)
         er.flush()
+
+
+def init_tensorboard_writer(fw_info: FrameworkInfo) -> TensorboardWriter:
+    """
+    Create a TensorBoardWriter object initialized with the logger dir path if it was set,
+    or None otherwise.
+
+    Args:
+        fw_info: FrameworkInfo object.
+
+    Returns:
+        A TensorBoardWriter object.
+    """
+    tb_w = None
+    if Logger.LOG_PATH is not None:
+        tb_log_dir = os.path.join(os.getcwd(), Logger.LOG_PATH, 'tensorboard_logs')
+        Logger.info(f'To use Tensorboard, please run: tensorboard --logdir {tb_log_dir}')
+        tb_w = TensorboardWriter(tb_log_dir, fw_info)
+    return tb_w
+
+
+def finalize_bitwidth_in_tb(tb_w: TensorboardWriter,
+                            weights_conf_nodes_bitwidth: List,
+                            activation_conf_nodes_bitwidth: List):
+    """
+    Set the final bit-width configuration of the quantized model in the provided TensorBoard object.
+
+    Args:
+        tb_w: A TensorBoard object.
+        weights_conf_nodes_bitwidth: Final weights bit-width configuration.
+        activation_conf_nodes_bitwidth: Final activation bit-width configuration.
+
+    """
+
+    if len(weights_conf_nodes_bitwidth) > 0:
+        visual = WeightsFinalBitwidthConfigVisualizer(weights_conf_nodes_bitwidth)
+        figure = visual.plot_config_bitwidth()
+        tb_w.add_figure(figure, f'Weights final bit-width config')
+    if len(activation_conf_nodes_bitwidth) > 0:
+        visual = ActivationFinalBitwidthConfigVisualizer(activation_conf_nodes_bitwidth)
+        figure = visual.plot_config_bitwidth()
+        tb_w.add_figure(figure, f'Activation final bit-width config')
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/graph_prep_runner.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/graph_prep_runner.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -37,31 +37,33 @@
                              quantization_config: QuantizationConfig,
                              fw_info: FrameworkInfo,
                              fw_impl: FrameworkImplementation,
                              tpc: TargetPlatformCapabilities,
                              tb_w: TensorboardWriter = None,
                              mixed_precision_enable: bool = False) -> Graph:
     """
-    Quantize a trained model using post-training quantization.
-    First, the model graph is optimized using several transformations (e.g. folding BatchNormalization to preceding
-    layers).
-    Second, statistics (e.g. min/max, histogram, etc.) are collected for each layer's output
-    (and input, depends on the quantization configuration) using a given representative dataset.
-    Next, quantization parameters are calculated using the collected statistics
-    (both coefficients and activations by default).
+    Runs all required preparations in order to build a quantization graph from the given model,
+    quantization configuration and target platform specifications.
+    This runner include the following steps:
+        - Reading and building a graph from the given model.
+        - Setting quantization config to each relevant node in the graph.
+        - Apply all necessary substitutions to finalize the graph for quantization.
+
     Args:
         in_model: Model to quantize.
         representative_data_gen: Dataset used for calibration.
-        core_config: CoreConfig containing parameters of how the model should be quantized
+        quantization_config: QuantizationConfig containing parameters of how the model should be quantized.
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
-        groups of layers by how they should be quantized, etc.).
+            groups of layers by how they should be quantized, etc.).
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
         tpc: TargetPlatformCapabilities object that models the inference target platform and
-                                              the attached framework operator's information.
-        tb_w: TensorboardWriter object for logging
+            the attached framework operator's information.
+        tb_w: TensorboardWriter object for logging.
+        mixed_precision_enable: is mixed precision enabled.
+
     Returns:
         An internal graph representation of the input model.
     """
 
     graph = read_model_to_graph(in_model,
                                 representative_data_gen,
                                 tpc,
@@ -88,24 +90,26 @@
                         fw_info: FrameworkInfo = None,
                         tb_w: TensorboardWriter = None,
                         fw_impl: FrameworkImplementation = None,
                         mixed_precision_enable: bool = False) -> Graph:
     """
     Applies all edit operation (edit, substitutions, etc.) on the model's graph, to prepare it for the quantization
     process. All future graph substitutions and operations that change the graph should be added to this method.
+
     Args:
         initial_graph (Graph): Graph to apply the changes to.
         tpc (TargetPlatformCapabilities): TargetPlatformCapabilities object that describes the desired inference target platform (includes fusing patterns MCT should handle).
         quant_config (QuantizationConfig): QuantizationConfig containing parameters of how the model should be
-        quantized.
+            quantized.
         fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g.,
-        kernel channels indices, groups of layers by how they should be quantized, etc.)
+            kernel channels indices, groups of layers by how they should be quantized, etc.)
         tb_w (TensorboardWriter): TensorboardWriter object to use for logging events such as graphs, histograms, etc.
         fw_impl (FrameworkImplementation): FrameworkImplementation object with a specific framework methods implementation.
         mixed_precision_enable: is mixed precision enabled.
+
     Returns: Graph object that represents the model, after applying all required modifications to it.
     """
 
     ######################################
     # Graph substitution (prepare graph)
     ######################################
     graph = substitute(initial_graph, fw_impl.get_substitutions_prepare_graph(fw_info))
@@ -122,14 +126,15 @@
                                                       graph=graph)
     ##################################################
     # Graph substitution (pre statistics collection)
     ##################################################
     transformed_graph = substitute(graph, fw_impl.get_substitutions_pre_statistics_collection(quant_config))
     if quant_config.linear_collapsing:
         transformed_graph = linear_collapsing_substitute(transformed_graph, fw_impl.get_linear_collapsing_substitution())
+        transformed_graph = linear_collapsing_substitute(transformed_graph, fw_impl.get_op2d_add_const_collapsing_substitution())
     if quant_config.residual_collapsing:
         transformed_graph = substitute(transformed_graph, fw_impl.get_residual_collapsing_substitution())
 
     if tb_w is not None:
         tb_w.add_graph(transformed_graph, 'pre_statistics_collection_substitutions')
 
     ######################################
@@ -169,22 +174,24 @@
                         representative_data_gen: Callable,
                         tpc: TargetPlatformCapabilities,
                         fw_info: FrameworkInfo = None,
                         fw_impl: FrameworkImplementation = None) -> Graph:
 
     """
     Read a model into a graph object.
+
     Args:
         in_model: Model to optimize and prepare for quantization.
         representative_data_gen: Dataset used for calibration.
         tpc: TargetPlatformCapabilities object that models the inference target platform and
                       the attached framework operator's information.
         fw_info: Information needed for quantization about the specific framework (e.g.,
                 kernel channels indices, groups of layers by how they should be quantized, etc.)
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+
     Returns:
         Graph object that represents the model.
     """
     graph = fw_impl.model_reader(in_model,
                                  representative_data_gen)
     graph.set_fw_info(fw_info)
     graph.set_tpc(tpc)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py`

 * *Files 13% similar despite different names*

```diff
@@ -34,13 +34,13 @@
         mode: Mode of the Keras model builder.
 
     Returns:
         Keras model builder for the given mode.
     """
 
     if not isinstance(mode, ModelBuilderMode):
-        Logger.error(f'get_keras_model_builder expects a mode of type ModelBuilderMode, but {type(mode)} was passed.')
+        Logger.critical(f"Expected a ModelBuilderMode type for 'mode', but received {type(mode)} instead.")
     if mode is None:
-        Logger.error(f'get_keras_model_builder received a mode which is None')
+        Logger.critical(f"get_keras_model_builder received 'mode' is None")
     if mode not in keras_model_builders.keys():
-        Logger.error(f'mode {mode} is not in keras model builders factory')
+        Logger.critical(f"'mode' {mode} is not recognized in the Keras model builders factory.")
     return keras_model_builders.get(mode)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/float_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/float_model_builder.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/instance_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/instance_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -84,16 +84,16 @@
     """
     framework_attr = copy.copy(n.framework_attr)
     _layer_class = n.layer_class
     framework_attr[LAYER_NAME] = n.name  # Overwrite framework name to identical graph node name
     try:
         node_instance = _layer_class.from_config(framework_attr)  # Build layer from node's configuration.
     except Exception as e:
-        print(e) # pragma: no cover
-        Logger.error(
+        Logger.info(e) # pragma: no cover
+        Logger.critical(
             f"Keras can not de-serialize layer {_layer_class} in order to build a static graph representation. This is probably because "
             f"your model contains custom layers which MCT doesn't support. Please provide a model without custom layers.") # pragma: no cover
     with tf.name_scope(n.name):
         # Add layer name to default weight name to avoid name duplications
         node_instance.build(n.input_shape)
     node_instance.set_weights(n.get_weights_list())
     node_instance.trainable = False  # Set all node as not trainable
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -266,14 +266,15 @@
         """
         if len(input_tensors) == 0:  # Placeholder handling
             out_tensors_of_n_float = input_nodes_to_input_tensors[n]
             out_tensors_of_n = self._run_operation_activation_quantization(n,
                                                                            out_tensors_of_n_float)
         else:
             input_tensors = [tensor for tensor_list in input_tensors for tensor in tensor_list]  # flat list of lists
+            input_tensors = n.insert_positional_weights_to_input_list(input_tensors)
             # Build a functional node using its args
             if isinstance(n, FunctionalNode):
                 if n.inputs_as_list:  # If the first argument should be a list of tensors:
                     out_tensors_of_n_float = op_func(input_tensors, *n.op_call_args, **n.op_call_kwargs)
                 else:  # If the input tensors should not be a list but iterated:
                     out_tensors_of_n_float = op_func(*input_tensors, *n.op_call_args, **n.op_call_kwargs)
             else:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,54 +8,49 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Tuple, Any, Dict, Union, List
 
-from packaging import version
-import tensorflow as tf
-if version.parse(tf.__version__) >= version.parse("2.13"):
-    from keras.src.engine.base_layer import Layer
-else:
-    from keras.engine.base_layer import Layer
+from typing import List, Any, Tuple, Union, Dict
 
-from keras.models import Model
-from mct_quantizers import KerasQuantizationWrapper, KerasActivationQuantizationHolder, QuantizationTarget
+import torch
+from mct_quantizers import PytorchQuantizationWrapper, QuantizationTarget, \
+    PytorchActivationQuantizationHolder
+from mct_quantizers.common.constants import ACTIVATION_HOLDER_QUANTIZER
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
-from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
 
+from model_compression_toolkit.core import FrameworkInfo
+from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
-from model_compression_toolkit.core.keras.mixed_precision.configurable_activation_quantizer import \
+from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+
+from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+from model_compression_toolkit.core.pytorch.mixed_precision.configurable_activation_quantizer import \
     ConfigurableActivationQuantizer
-from model_compression_toolkit.core.keras.mixed_precision.configurable_weights_quantizer import \
+from model_compression_toolkit.core.pytorch.mixed_precision.configurable_weights_quantizer import \
     ConfigurableWeightsQuantizer
 
-from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
-    get_inferable_quantizer_kwargs
-
+from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
+    get_weights_inferable_quantizer_kwargs, get_activation_inferable_quantizer_kwargs
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 
 
-class MixedPrecisionKerasModelBuilder(KerasModelBuilder):
+class MixedPrecisionPyTorchModelBuilder(PyTorchModelBuilder):
     """
-    Builder of mixed-precision Keras models.
+    Mixed-precision PyTorch model.
     """
-
     def __init__(self,
                  graph: common.Graph,
                  append2output=None,
-                 fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
+                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
                  return_float_outputs: bool = False):
         """
 
         Args:
             graph: Graph to build the model from.
             append2output: Nodes to append to model's output.
             fw_info: Information about the specific framework of the model that is built.
@@ -69,53 +64,59 @@
                          fw_info,
                          return_float_outputs,
                          wrapper=self.mixed_precision_wrapper,
                          get_activation_quantizer_holder_fn=self.mixed_precision_activation_holder)
 
     def mixed_precision_wrapper(self,
                                 n: common.BaseNode,
-                                layer: Layer) -> Union[KerasQuantizationWrapper, Layer]:
+                                layer: torch.nn.Module) -> Union[PytorchQuantizationWrapper, torch.nn.Module]:
         """
-        A function which takes a computational graph node and a keras layer and perform the quantization
+        A function which takes a computational graph node and a pytorch layer and perform the quantization
         wrapping for mixed precision.
 
         Args:
             n: A node of mct graph.
-            layer: A keras layer
+            layer: A pytorch layer
 
         Returns: Wrapped layer with a configurable quantizer if the layer should quantized in mixed precision,
         otherwise returns either the layer wrapped with a fixed precision inferable quantizer or the layer as is if it's
         not supposed to be quantized.
 
         """
 
-        weights_conf_nodes_names = [n.name for n in self.graph.get_weights_configurable_nodes()]
+        weights_conf_nodes_names = [n.name for n in self.graph.get_weights_configurable_nodes(self.fw_info)]
+        kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
 
-        if n.is_weights_quantization_enabled():
-            kernel_attributes = self.fw_info.get_kernel_op_attributes(n.type)
             if n.name in weights_conf_nodes_names:
-                return KerasQuantizationWrapper(layer,
-                                                weights_quantizers={attr: ConfigurableWeightsQuantizer(
-                                                    **self._get_weights_configurable_quantizer_kwargs(n, attr))
-                                                    for attr in kernel_attributes})
+                return PytorchQuantizationWrapper(layer,
+                                                  weights_quantizers={
+                                                      kernel_attr: ConfigurableWeightsQuantizer(
+                                                          **self._get_weights_configurable_quantizer_kwargs(n,
+                                                                                                            kernel_attr),
+                                                          kernel_attr=kernel_attr)})
             else:
-                node_weights_qc = n.get_unique_weights_candidates()
+                # TODO: Do we want to include other quantized attributes that are not
+                #  the kernel attribute in the mixed precision model?
+                #  Currently, we only consider kernel attribute quantization (whether it is in mixed precision
+                #  or single precision).
+                node_weights_qc = n.get_unique_weights_candidates(kernel_attr)
                 if not len(node_weights_qc) == 1:
-                    Logger.error(f"Expecting node {n.name} to have a unique weights configuration "  # pragma: no cover
-                                 f"but {len(node_weights_qc)} different configurations exist.")
+                    Logger.critical(f"Expected a single weights quantization configuration for node '{n.name}', but found ({len(node_weights_qc)}) configurations.")# pragma: no cover
 
                 quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
-                                                                  node_weights_qc[0].weights_quantization_cfg.weights_quantization_method,
-                                                                  BaseKerasInferableQuantizer)
-                kwargs = get_inferable_quantizer_kwargs(node_weights_qc[0].weights_quantization_cfg,
-                                                        QuantizationTarget.Weights)
-
-                return KerasQuantizationWrapper(layer,
-                                                weights_quantizers={attr: quantier_for_node(**kwargs)
-                                                                    for attr in kernel_attributes})
+                                                                  node_weights_qc[0].weights_quantization_cfg
+                                                                  .get_attr_config(kernel_attr)
+                                                                  .weights_quantization_method,
+                                                                  BasePyTorchInferableQuantizer)
+                kwargs = get_weights_inferable_quantizer_kwargs(node_weights_qc[0].weights_quantization_cfg,
+                                                                kernel_attr)
+
+                return PytorchQuantizationWrapper(layer,
+                                                  weights_quantizers={kernel_attr: quantier_for_node(**kwargs)})
 
         return layer
 
     def _get_weights_configurable_quantizer_kwargs(self, n: BaseNode, attr: str) -> Dict[str, Any]:
         """
         Get the quantization parameters for a configurable quantizer.
 
@@ -126,158 +127,156 @@
         Returns:
             The quantization parameters as a dictionary.
         """
 
         assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
         node_q_cfg_candidates = n.candidates_quantization_cfg
         # sort by descending bit width so using indices would be easier
-        node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.weights_n_bits,
+        node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.get_attr_config(attr).weights_n_bits,
                                                   x.activation_quantization_cfg.activation_n_bits), reverse=True)
 
         float_weights = n.get_weights_by_keys(attr)
 
         max_cfg_candidates = n.find_max_candidates_indices()
         if not len(max_cfg_candidates) == 1:
-            Logger.error(f"A maximal config candidate must be defined, "  # pragma: no cover
-                         f"but some node have multiple potential maximal candidates")
+            Logger.critical(f"A maximal configuration candidate must be uniquely defined; however, multiple potential maximal candidates were found.") # pragma: no cover
 
         max_candidate_idx = max_cfg_candidates[0]
 
         return {'node_q_cfg': node_q_cfg_candidates,
                 'float_weights': float_weights,
                 'max_candidate_idx': max_candidate_idx
                 }
 
-    def mixed_precision_activation_holder(self, n: BaseNode) -> KerasActivationQuantizationHolder:
+    def mixed_precision_activation_holder(self, n: BaseNode) -> PytorchActivationQuantizationHolder:
         """
-        Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
+        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization for a node.
         The layer should hold either a configurable activation quantizer, if it is quantized with mixed precision,
         or an inferable quantizer for fixed single bit-width quantization.
 
         Args:
-            n: Node to get KerasActivationQuantizationHolder to attach in its output.
+            n: Node to get PytorchActivationQuantizationHolder to attach in its output.
 
         Returns:
-            A KerasActivationQuantizationHolder layer for the node activation quantization.
+            A PytorchActivationQuantizationHolder layer for the node activation quantization.
         """
 
         activation_conf_nodes_names = [n.name for n in self.graph.get_activation_configurable_nodes()]
 
         activation_quantizers = []
         if n.is_activation_quantization_enabled():
             num_of_outputs = len(n.output_shape) if isinstance(n.output_shape, list) else 1
             if n.name in activation_conf_nodes_names:
                 assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
                 node_q_cfg_candidates = n.candidates_quantization_cfg
-                # sort by descending bit width so using indices would be easier
-                node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.weights_n_bits,
-                                                          x.activation_quantization_cfg.activation_n_bits),
-                                           reverse=True)
+
+                # sorting the candidates by kernel attribute weights number of bits first and then by
+                # activation number of bits (in reversed order).
+                # since only kernel attribute is quantized in weights mixed precision,
+                # if the node doesn't have a kernel attribute, we only sort by activation_n_bits.
+                n.sort_node_candidates(self.fw_info)
 
                 max_cfg_candidates = n.find_max_candidates_indices()
                 assert len(max_cfg_candidates) == 1, \
-                    f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
+                    f"A maximal configuration candidate must be uniquely defined; however, multiple potential maximal candidates were found."
                 max_candidate_idx = max_cfg_candidates[0]
 
+                kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
                 activation_quantizers = [ConfigurableActivationQuantizer(**{'node_q_cfg': node_q_cfg_candidates,
-                                                                            'max_candidate_idx': max_candidate_idx})] \
+                                                                            'max_candidate_idx': max_candidate_idx,
+                                                                            'kernel_attr': kernel_attr})] \
                                         * num_of_outputs
             else:
                 node_act_qc = n.get_unique_activation_candidates()
-                assert len(node_act_qc) == 1, f"Expecting node {n.name} to have a unique activation configuration, " \
-                                              f"but {len(node_act_qc)} different configurations exist."
+                assert len(node_act_qc) == 1, f"Expected a single activation configuration for node '{n.name}', but found multiple ({len(node_act_qc)}) configurations."
                 quantizer_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
                                                                    node_act_qc[0].activation_quantization_cfg.activation_quantization_method,
-                                                                   BaseKerasInferableQuantizer)
-                kwargs = get_inferable_quantizer_kwargs(node_act_qc[0].activation_quantization_cfg,
-                                                        QuantizationTarget.Activation)
+                                                                   BasePyTorchInferableQuantizer)
+                kwargs = get_activation_inferable_quantizer_kwargs(node_act_qc[0].activation_quantization_cfg)
 
                 activation_quantizers = [quantizer_for_node(**kwargs)] * num_of_outputs
 
         # Holder by definition uses a single quantizer for the activation quantization
         # thus we make sure this is the only possible case (unless it's a node with no activation
         # quantization, which in this case has an empty list).
         if len(activation_quantizers) == 1:
-            return KerasActivationQuantizationHolder(activation_quantizers[0])
+            return PytorchActivationQuantizationHolder(activation_quantizers[0])
 
-        Logger.error(f'KerasActivationQuantizationHolder supports a single quantizer but '  # pragma: no cover
-                     f'{len(activation_quantizers)} quantizers were found for node {n}')
+        Logger.critical(f"PytorchActivationQuantizationHolder expects a single quantizer, but ({len(activation_quantizers)}) quantizers were found for node {n}.")# pragma: no cover
 
-    def build_model(self) -> Tuple[Model, UserInformation,
-                                   Dict[str, Union[KerasQuantizationWrapper, KerasActivationQuantizationHolder]]]:
+    def build_model(self) -> Tuple[torch.nn.Module, UserInformation,
+                                   Dict[str, Union[PytorchQuantizationWrapper, PytorchActivationQuantizationHolder]]]:
         """
-        Build a Keras mixed-precision model and return it.
-        Used the basic Keras model builder to build the model, and adding a mapping between each configurable node to
-        a list of layers (from the new model) that are matching to the node (either KerasQuantizationWrapper or
-        KerasActivationQuantizationHolder type layers).
-        This mapping is used during mixed precision metric computation to enforce pairs of weights-activation bit-width
-        candidates when configuring a model.
-
-        Returns: Mixed-precision Keras model.
+        Build a PyTorch float model and return it.
+        Returns: Float PyTorch model and user information.
 
         """
         model, user_info = super().build_model()
 
         # creating a mapping between graph nodes and model's layers for mixed precision configurability
-        conf_node2layers = {n.name: self._find_layers_in_model_by_node(n, model.layers)
-                            for n in self.graph.get_configurable_sorted_nodes()}
+        model_layers = dict(model.named_children())
+        conf_node2layers = {n.name: self._find_layers_in_model_by_node(n, model_layers)
+                            for n in self.graph.get_configurable_sorted_nodes(self.fw_info)}
 
         return model, user_info, conf_node2layers
 
+
     @staticmethod
-    def _get_weights_quant_layers(n: BaseNode, layers_list: List[Layer]) -> List[KerasQuantizationWrapper]:
+    def _get_weights_quant_layers(n: BaseNode, named_layers: Dict[str, torch.nn.Module]) \
+            -> List[PytorchQuantizationWrapper]:
         """
-        Filters KerasQuantizationWrapper layers from an MP model that are matching to the given graph node.
+        Filters PytorchQuantizationWrapper layers from an MP model that are matching to the given graph node.
 
         Args:
             n: A configurable graph node.
-            layers_list: Mixed precision model layers list.
+            named_layers: Mixed precision model layers list.
 
         Returns: A list of layers that responsible for the node's weights quantization.
 
         """
-        return [_l for _l in layers_list if isinstance(_l, KerasQuantizationWrapper) and _l.layer.name == n.name]
+        return [module for m_name, module in named_layers.items() if isinstance(module, PytorchQuantizationWrapper)
+                and m_name == n.name]
 
     @staticmethod
-    def _get_activation_quant_layers(n: BaseNode, layers_list: List[Layer]) -> List[KerasActivationQuantizationHolder]:
+    def _get_activation_quant_layers(n: BaseNode, named_layers: Dict[str, torch.nn.Module]) \
+            -> List[PytorchActivationQuantizationHolder]:
         """
-        Filters KerasActivationQuantizationHolder layers from an MP model that are matching to the given graph node.
+        Filters PytorchActivationQuantizationHolder layers from an MP model that are matching to the given graph node.
 
         Args:
             n: A configurable graph node.
-            layers_list: Mixed precision model layers list.
+            named_layers: Mixed precision model layers list.
 
         Returns: A list of layers that responsible for the node's activation quantization.
 
         """
-        return [_l for _l in layers_list if isinstance(_l, KerasActivationQuantizationHolder)
-                and (_l.inbound_nodes[0].inbound_layers.name == n.name or
-                     (isinstance(_l.inbound_nodes[0].inbound_layers, KerasQuantizationWrapper) and
-                      _l.inbound_nodes[0].inbound_layers.layer.name == n.name))]
+        return [module for m_name, module in named_layers.items()
+                if isinstance(module, PytorchActivationQuantizationHolder) and
+                m_name.replace(f"_{ACTIVATION_HOLDER_QUANTIZER}", '') == n.name]
 
-    def _find_layers_in_model_by_node(self, n: BaseNode, layers_list: List[Layer]) -> \
-            List[Union[KerasQuantizationWrapper, KerasActivationQuantizationHolder]]:
+    def _find_layers_in_model_by_node(self, n: BaseNode, named_layers: Dict[str, torch.nn.Module]) -> \
+            List[Union[PytorchQuantizationWrapper, PytorchActivationQuantizationHolder]]:
         """
-        Retries layers from an MP model that are matching to the given graph node, that is, these are either
-        KerasQuantizationWrapper layers or KerasActivationQuantizationHolder layers that are responsible for the graph
+        Retries layers from an MP model that are matching to the given graph node, that is, this are either
+        PytorchQuantizationWrapper layers or PytorchActivationQuantizationHolder layers that are responsible for the graph
         configurable model quantization.
 
         Args:
             n: A configurable graph node.
-            layers_list: Mixed precision model layers list.
+            named_layers: Mixed precision model layers list.
 
         Returns: A list of layers that responsible for the node's quantization.
 
         """
-        weights_quant = n.is_weights_quantization_enabled()
+        # Only layers with kernel op are considered weights configurable
+        kernel_attr = self.fw_info.get_kernel_op_attributes(n.type)[0]
+        weights_quant = False if kernel_attr is None else n.is_weights_quantization_enabled(kernel_attr)
         act_quant = n.is_activation_quantization_enabled()
 
         if weights_quant and not act_quant:
-            return self._get_weights_quant_layers(n, layers_list)
+            return self._get_weights_quant_layers(n, named_layers)
         elif not weights_quant and act_quant:
-            return self._get_activation_quant_layers(n, layers_list)
+            return self._get_activation_quant_layers(n, named_layers)
         elif weights_quant and act_quant:
-            return self._get_weights_quant_layers(n, layers_list) + self._get_activation_quant_layers(n, layers_list)
-        else:
-            Logger.error(f"Expects node {n.name} to have at either weights or activation quantization configured,"  # pragma: no cover
-                         f"but both are disabled.")
-
+            return self._get_weights_quant_layers(n, named_layers) + self._get_activation_quant_layers(n, named_layers)
+        else:  # pragma: no cover
+            Logger.critical(f"Expected node {n.name} to have either weights or activation quantization enabled, but both are disabled.")
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/custom_layer_validation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/custom_layer_validation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/default_framework_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/default_framework_info.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,52 +20,52 @@
 from packaging import version
 
 if version.parse(tf.__version__) >= version.parse("2.13"):
     from keras.src.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
 
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.constants import SOFTMAX_THRESHOLD
 from model_compression_toolkit.core.keras.constants import SOFTMAX, LINEAR, RELU, SWISH, SIGMOID, IDENTITY, TANH, SELU, \
     KERNEL, DEPTHWISE_KERNEL
 from model_compression_toolkit.core.keras.quantizer.fake_quant_builder import power_of_two_quantization, symmetric_quantization, uniform_quantization
 
 """
 Map each layer to a list of its' weights attributes that should get quantized.
 If a layer that is not listed here is queried, [None] is returned.
 """
 KERNEL_ATTRIBUTES = DefaultDict({Conv2D: [KERNEL],
                                  DepthwiseConv2D: [DEPTHWISE_KERNEL],
                                  Dense: [KERNEL],
-                                 Conv2DTranspose: [KERNEL]}, lambda: [None])
+                                 Conv2DTranspose: [KERNEL]}, [None])
 
 
 """
 Map a layer to its kernel's output and input channels indices.
 Map's values are tuples of (output_channel_index, input_channel_index).
 Default value is returned for layers that are not included.
 """
 DEFAULT_CHANNEL_AXIS_DICT = DefaultDict({Conv2D: (3, 2),
                                          DepthwiseConv2D: (2, 2),
                                          Dense: (1, 0),
-                                         Conv2DTranspose: (2, 3)}, lambda: (None, None))
+                                         Conv2DTranspose: (2, 3)}, (None, None))
 
 
 """
 Map a layer to its output channel axis. 
 Where axis=-1 is the last axis
 """
 DEFAULT_OUT_CHANNEL_AXIS_DICT = DefaultDict({Conv2D: -1,
                                              DepthwiseConv2D: -1,
                                              Dense: -1,
                                              Conv2DTranspose: -1},
-                                            lambda: -1)
+                                            -1)
 
 
 """
 Map from an activation function to its min/max output values (if known).
 The values are used for tensor min/max values initialization.
 """
 ACTIVATION2MINMAX = {SOFTMAX: (0, SOFTMAX_THRESHOLD),
@@ -85,15 +85,15 @@
 LAYER2MINMAX = {Softmax: (0, SOFTMAX_THRESHOLD),
                 ELU: (-1, None),
                 tf.nn.silu: (-0.279, None),
                 tf.nn.swish: (-0.279, None),
                 tf.nn.sigmoid: (0, 1),
                 tf.nn.tanh: (-1, 1),
                 tf.nn.relu: (0, None),
-                tf.nn.relu6: (0, 6),
+                tf.nn.relu6: (0, None),
                 tf.nn.gelu: (-0.17, None),
                 tf.nn.elu: (-1, None),
                 tf.nn.selu: (-1.76, None),
                 tf.nn.softplus: (0, None),
                 tf.nn.softmax: (0, SOFTMAX_THRESHOLD),
                 }
 """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,25 +8,26 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-
+import keras.layers
 from tensorflow.keras.layers import Dense, DepthwiseConv2D, Conv2D, Conv2DTranspose, Activation, SeparableConv2D
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.constants import FLOAT_32, DATA_TYPE
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.keras.constants import LINEAR, ACTIVATION, TRAINABLE, LAYER_NAME
+from model_compression_toolkit.core.keras.constants import LINEAR, ACTIVATION, TRAINABLE, LAYER_NAME, SOFTMAX, AXIS, \
+    SOFTMAX_AXIS_DEFAULT
 
 
 class ActivationDecomposition(common.BaseSubstitution):
     """
     Replace a linear layer that has an activation function, with two nodes: same linear layer without
     an activation function, and a new activation layer to replace the function the linear node had.
     """
@@ -58,28 +59,44 @@
             graph: Graph we apply the substitution on.
             op2d_node: Node to extract its activation function.
 
         Returns:
             Graph after applying the substitution.
         """
 
+        if ACTIVATION not in op2d_node.framework_attr:
+            Logger.warning(f'Op2d node {op2d_node.name} of type {op2d_node.type} is missing an "{ACTIVATION}"'
+                           f' attribute -> Skipping substitution ActivationDecomposition')  # pragma: no cover
+            return graph
+
         activation_node_name = op2d_node.name + '_post_activation'
 
-        activation_fw_attr = {
-            LAYER_NAME: activation_node_name,
-            TRAINABLE: False,
-            DATA_TYPE: FLOAT_32,
-            ACTIVATION: op2d_node.framework_attr.get(ACTIVATION)}
-
-        activation_node = common.graph.BaseNode(activation_node_name,
-                                                activation_fw_attr,
-                                                op2d_node.output_shape,
-                                                op2d_node.output_shape,
-                                                {},
-                                                Activation)
+        # Softmax is a special case where we need to know the default axis parameter used
+        # and for this reason we create a Softmax layer and not Activation layer.
+        if op2d_node.framework_attr.get(ACTIVATION) == SOFTMAX:
+            activation_fw_attr = {AXIS: SOFTMAX_AXIS_DEFAULT}
+            activation_node = common.graph.BaseNode(activation_node_name,
+                                                    activation_fw_attr,
+                                                    op2d_node.output_shape,
+                                                    op2d_node.output_shape,
+                                                    {},
+                                                    keras.layers.Softmax)
+        else:
+            activation_fw_attr = {
+                LAYER_NAME: activation_node_name,
+                TRAINABLE: False,
+                DATA_TYPE: FLOAT_32,
+                ACTIVATION: op2d_node.framework_attr.get(ACTIVATION)}
+
+            activation_node = common.graph.BaseNode(activation_node_name,
+                                                    activation_fw_attr,
+                                                    op2d_node.output_shape,
+                                                    op2d_node.output_shape,
+                                                    {},
+                                                    Activation)
 
         graph.add_node(activation_node)
         graph.reconnect_out_edges(current_node=op2d_node,
                                   new_node=activation_node)
         graph.add_edge(op2d_node,
                        activation_node,
                        source_index=0,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.substitutions.batchnorm_folding import BatchNormalizationFolding, BatchNormalizationForwardFolding
 from model_compression_toolkit.core.keras.constants import KERNEL, LINEAR, ACTIVATION, DEPTHWISE_KERNEL, BIAS, GAMMA, BETA, \
     MOVING_MEAN, MOVING_VARIANCE, EPSILON, USE_BIAS, LAYER_NAME, GROUPS
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 
 
 def batchnorm_folding_node_matchers() -> [BaseNode, BaseNode]:
     """
     Function generates matchers for matching:
     (DepthwiseConv2D, Conv2D, Conv2DTranspose)[activation=linear] -> BatchNormalization.
 
@@ -72,18 +73,15 @@
     if conv_node.type == DepthwiseConv2D:
         kernel = kernel * weights_scale.reshape((1, 1, kernel.shape[-2], kernel.shape[-1]))
     elif conv_node.type == Conv2DTranspose:
         kernel = kernel * weights_scale.reshape((1, 1, -1, 1))
     else:
         kernel = kernel * weights_scale.reshape((1, 1, 1, -1))
 
-    if conv_node.type == DepthwiseConv2D:
-        kernel_name = DEPTHWISE_KERNEL
-    else:
-        kernel_name = KERNEL
+    kernel_name = DEFAULT_KERAS_INFO.get_kernel_op_attributes(conv_node.type)[0]
 
     return kernel, kernel_name
 
 
 def update_weights_for_bn_forward_folding_fn(conv_node: BaseNode,
                                              kernel: np.ndarray,
                                              bias: np.ndarray,
@@ -106,18 +104,15 @@
     elif conv_node.type == Conv2DTranspose:
         bias_update = (kernel * bias_factor.reshape((1, 1, 1, -1))).sum(3)
         kernel = kernel * weights_scale.reshape((1, 1, 1, -1))
     else:
         bias_update = (kernel * bias_factor.reshape((1, 1, -1, 1))).sum(2)
         kernel = kernel * weights_scale.reshape((1, 1, -1, 1))
 
-    if conv_node.type == DepthwiseConv2D:
-        kernel_name = DEPTHWISE_KERNEL
-    else:
-        kernel_name = KERNEL
+    kernel_name = DEFAULT_KERAS_INFO.get_kernel_op_attributes(conv_node.type)[0]
 
     return kernel, bias + bias_update.flatten(), kernel_name
 
 
 def get_kernel_hw_fn(kernel: np.ndarray) -> [int, int]:
     """
     Args:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, EdgeMatcher, WalkMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.constants import THRESHOLD
 from model_compression_toolkit.core.keras.constants import KERNEL
+from model_compression_toolkit.logger import Logger
 
 input_node = NodeOperationMatcher(InputLayer)
 zeropad_node = NodeOperationMatcher(ZeroPadding2D)
 op2d_node = NodeOperationMatcher(Dense) | \
             NodeOperationMatcher(Conv2D) | \
             NodeOperationMatcher(DepthwiseConv2D) | \
             NodeOperationMatcher(Conv2DTranspose)
@@ -76,38 +77,41 @@
             Graph after applying the substitution.
         """
 
         input_layer = nodes_list[0]
         linear_layer = nodes_list[-1]
 
         if not input_layer.is_all_activation_candidates_equal():
-            raise Exception("Input scaling is not supported for more than one activation quantization configuration "
-                            "candidate")
+            Logger.critical("Input scaling is not supported for nodes with more than one activation quantization configuration "
+                            "candidate.")
 
         # all candidates have same activation config, so taking the first candidate for calculations
         threshold = input_layer.candidates_quantization_cfg[0].activation_quantization_cfg.activation_quantization_params.get(THRESHOLD)
 
         if threshold is None:
             return graph
 
         min_value, max_value = graph.get_out_stats_collector(input_layer).get_min_max_values()
         threshold_float = max(abs(min_value), max_value)
 
         if threshold > threshold_float:
             scale_factor = threshold_float / threshold
             graph.user_info.set_input_scale(1 / scale_factor)
 
-            w1_fixed = linear_layer.get_weights_by_keys(KERNEL) * scale_factor
-            linear_layer.set_weights_by_keys(KERNEL, w1_fixed)
+            kernel_attr = graph.fw_info.get_kernel_op_attributes(linear_layer.type)[0]
+
+            w1_fixed = linear_layer.get_weights_by_keys(kernel_attr) * scale_factor
+            linear_layer.set_weights_by_keys(kernel_attr, w1_fixed)
 
             graph.scale_stats_collector(input_layer, 1 / scale_factor)
 
             # After scaling weights may have different thresholds so it needs to be recalculated
             for nqc in linear_layer.candidates_quantization_cfg:
-                nqc.weights_quantization_cfg.calculate_and_set_weights_params(w1_fixed)
+                nqc.weights_quantization_cfg.get_attr_config(kernel_attr).calculate_and_set_weights_params(w1_fixed,
+                                                                                                           nqc.weights_quantization_cfg.min_threshold)
 
         return graph
 
 
 class InputScaling(BaseInputScaling):
     """
     Substitution extends BaseInputScaling to the case of Input-->Linear
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,18 +11,22 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Tuple
 import numpy as np
 import tensorflow as tf
-from tensorflow.keras.layers import Conv2D
+if tf.__version__ < "2.6":
+    from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense
+else:
+    from keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense
+
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, NodeFrameworkAttrMatcher
-from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing
+from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing, Op2DAddConstCollapsing
 from model_compression_toolkit.core.keras.constants import KERNEL, KERNEL_SIZE, STRIDES, DILATIONS, LINEAR, \
     ACTIVATION, BIAS, USE_BIAS, LAYER_NAME, FILTERS, PADDING, GROUPS, DATA_FORMAT
 from model_compression_toolkit.logger import Logger
 
 
 def linear_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
     """
@@ -96,15 +100,15 @@
             if bias2 is not None:
                 bias_collapsed += bias2
         elif bias2 is not None:
             bias_collapsed = bias2
 
         return kernel_collapsed, bias_collapsed
     else:
-        Logger.error("No supported layer collapsing of {} and {}".format(first_node.type, second_node.type))
+        Logger.critical(f"Layer collapsing unsupported for combination: {first_node.type} and {second_node.type}.")
 
 
 def keras_linear_collapsing() -> Conv2DCollapsing:
     """
     Returns:
         A Conv2DCollapsing initialized for Keras models.
     """
@@ -119,7 +123,62 @@
                             STRIDES,
                             PADDING,
                             DILATIONS,
                             GROUPS,
                             FILTERS,
                             data_format_str=DATA_FORMAT,
                             layer_name_str=LAYER_NAME)
+
+
+def op2d_add_const_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
+    """
+    Function generates matchers for matching:
+    (Op2D, Add(const)) -> Op2D.  (Op2D is one of [DepthwiseConv2D, Conv2D, Conv2DTranspose, Dense)
+    Returns:
+        Matcher for Op2D followed by Add const
+    """
+    first_node = NodeOperationMatcher(DepthwiseConv2D) | \
+                 NodeOperationMatcher(Conv2D) | \
+                 NodeOperationMatcher(Conv2DTranspose) | \
+                 NodeOperationMatcher(Dense)
+    second_node = NodeOperationMatcher(tf.math.add)
+    return first_node, second_node
+
+
+def op2d_add_const_collapsing_fn(op2d_node: BaseNode,
+                                 add_node: BaseNode,
+                                 bias_str: str) -> np.ndarray:
+    """
+    Collapsing Add-Const to previous node's bias
+    Args:
+        op2d_node: Op2d layer node
+        add_node: Add layer to collapse
+        bias_str: The framework specific attribute name of the convolution layer's bias.
+    Returns:
+        The modified conv layer node's bias
+    """
+    bias = op2d_node.get_weights_by_keys(bias_str)
+
+    # read constant from add node (either 1st or 2nd positional weight)
+    const = add_node.weights.get(0, add_node.weights.get(1))
+    if const is None:
+        Logger.critical(f'Failed to read constant from add node: {add_node.name}.')  # pragma: no cover
+
+    # return new bias
+    if bias is None:
+        return const
+    else:
+        return const + bias
+
+
+def keras_op2d_add_const_collapsing() -> Op2DAddConstCollapsing:
+    """
+    Returns:
+        An Op2DCollapsing initialized for Keras models.
+    """
+    first_node, second_node = op2d_add_const_collapsing_node_matchers()
+    return Op2DAddConstCollapsing(first_node,
+                                  second_node,
+                                  op2d_add_const_collapsing_fn,
+                                  BIAS,
+                                  USE_BIAS,
+                                  layer_name_str=LAYER_NAME)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py`

 * *Files 0% similar despite different names*

```diff
@@ -444,15 +444,15 @@
             mha_node: MultiHeadAttention node to replace.
 
         Returns:
             Graph after applying the substitution.
         """
 
         if mha_node.reuse:
-            Logger.error("MCT doesn't support reuse of MultiHeadAttention layer")  # pragma: no cover
+            Logger.critical("Reuse of MultiHeadAttention layers is currently not supported.")  # pragma: no cover
         params = MHAParams(mha_node)
 
         mha_in_edges = graph.in_edges(mha_node)
 
         # input permutation and reshape to standard shape: (batch, iterations, sequence, channels)
         q_permute_node, k_permute_node, v_permute_node, \
         q_reshape_node, k_reshape_node, v_reshape_node = \
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py`

 * *Files 8% similar despite different names*

```diff
@@ -58,15 +58,15 @@
         idxH = (kH - 1) // 2
         idxW = (kW - 1) // 2
         for i in range(Cout):
             kernel[idxH, idxW, i, i] += 1
 
         return kernel
     else:
-        Logger.error("No supported add residual collapsing for {}".format(first_node.type))
+        Logger.critical(f"Residual collapsing is unsupported for {first_node.type} node types.")
 
 
 def keras_residual_collapsing() -> ResidualCollapsing:
     """
     Returns:
         A ResidualCollapsing initialized for Keras models.
     """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py`

 * *Files 6% similar despite different names*

```diff
@@ -71,16 +71,18 @@
         """
 
         dw_kernel = separable_node.get_weights_by_keys(DEPTHWISE_KERNEL)
         pw_kernel = separable_node.get_weights_by_keys(POINTWISE_KERNEL)
         pw_bias = separable_node.get_weights_by_keys(BIAS)
 
         dw_weights_dict = {DEPTHWISE_KERNEL: dw_kernel}
-        pw_weights_dict = {KERNEL: pw_kernel,
-                           BIAS: pw_bias}
+        pw_weights_dict = {KERNEL: pw_kernel}
+
+        if pw_bias is not None:
+            pw_weights_dict[BIAS] = pw_bias
 
         # Split separable node attributes into relevant attributes for each of the new nodes.
         # List of dw attributes that should take from separable as they are.
         dw_attr_list = [KERNEL_SIZE, STRIDES, PADDING, DEPTH_MULTIPLIER, DATA_FORMAT, DILATION_RATE,
                         DEPTHWISE_INITIALIZER, DEPTHWISE_REGULARIZER, DEPTHWISE_CONSTRAINT, TRAINABLE]
 
         dw_framework_attr = {attr: separable_node.framework_attr[attr] for attr in dw_attr_list}
@@ -108,38 +110,40 @@
         # compute input/outpus shapes of new nodes
         dw_output_shape = tuple(dw_layer_class(**dw_framework_attr).compute_output_shape(separable_node.input_shape))
         pw_input_shape = dw_output_shape
 
         # If the SeparableConv2D is reused, we need to keep the depthwise node as reused as well,
         # so we keep the names convention with adding the suffix of "_reuse_X".
         dw_node_name = separable_node.name + '_dw' if not separable_node.reuse else '_'.join(separable_node.name.split('_')[:-2]) + '_dw_' + '_'.join(separable_node.name.split('_')[-2:])
+        reuse_group = separable_node.reuse_group if not separable_node.reuse_group else separable_node.reuse_group + '_dw'
+
 
         # create new nodes
         dw_node = common.graph.BaseNode(dw_node_name,
                                         dw_framework_attr,
                                         separable_node.input_shape,
                                         dw_output_shape,
                                         dw_weights_dict,
                                         dw_layer_class,
                                         reuse=separable_node.reuse,
-                                        reuse_group=separable_node.reuse_group)
+                                        reuse_group=reuse_group)
 
         # If the SeparableConv2D is reused, we need to keep the pointwise node as reused as well,
         # so we keep the names convention with adding the suffix of "_reuse_X".
         pw_node_name = separable_node.name + '_pw' if not separable_node.reuse else '_'.join(separable_node.name.split('_')[:-2]) + '_pw_' + '_'.join(separable_node.name.split('_')[-2:])
+        reuse_group = separable_node.reuse_group if not separable_node.reuse_group else separable_node.reuse_group + '_pw'
 
         pw_node = common.graph.BaseNode(pw_node_name,
                                         pw_framework_attr,
                                         pw_input_shape,
                                         separable_node.output_shape,
                                         pw_weights_dict,
                                         pw_layer_class,
                                         reuse=separable_node.reuse,
-                                        reuse_group=separable_node.reuse_group
-                                        )
+                                        reuse_group=reuse_group)
 
         graph.add_node(dw_node)
         graph.add_node(pw_node)
         graph.add_edge(dw_node,
                        pw_node,
                        source_index=0,
                        sink_index=0)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py`

 * *Files 1% similar despite different names*

```diff
@@ -105,15 +105,16 @@
                               {FUNCTION: ADD},
                               input_shape,
                               input_shape,
                               weights={},
                               quantization_attr={},
                               layer_class=TFOpLambda,
                               op_call_args=[np.array(add_value, dtype=np.float32).reshape([1] * len(input_shape))],
-                              op_call_kwargs={})
+                              op_call_kwargs={},
+                              functional_op=tf.add)
     return add_node
 
 
 def create_pad_node(next_node_name: str,
                     prev_node_name: str,
                     value_to_pad: float,
                     input_shape: tuple,
@@ -153,15 +154,16 @@
                               input_shape,
                               tuple(padded_shape),
                               weights={},
                               quantization_attr={},
                               layer_class=TFOpLambda,
                               op_call_args=[],
                               op_call_kwargs={'paddings': num_elements_to_pad,
-                                              'constant_values': value_to_pad})
+                                              'constant_values': value_to_pad},
+                              functional_op=tf.pad)
 
     return pad_node
 
 
 def compute_op2d_padding(op2d_node: BaseNode) -> Tuple[int, int, int, int]:
     """
     Compute the padding around an input tensor of a linear node.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/keras_implementation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/keras_implementation.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,22 +15,30 @@
 from functools import partial
 from typing import List, Any, Tuple, Callable, Dict
 
 import numpy as np
 import tensorflow as tf
 from mct_quantizers import KerasQuantizationWrapper, KerasActivationQuantizationHolder
 from tensorflow.keras.models import Model
-from tensorflow.python.layers.base import Layer
 
+from model_compression_toolkit.constants import HESSIAN_NUM_ITERATIONS
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianMode, HessianInfoService
+from model_compression_toolkit.core.keras.hessian.activation_trace_hessian_calculator_keras import \
+    ActivationTraceHessianCalculatorKeras
+from model_compression_toolkit.core.keras.hessian.weights_trace_hessian_calculator_keras import WeightsTraceHessianCalculatorKeras
+from model_compression_toolkit.exporter.model_wrapper.fw_agnostic.get_inferable_quantizers import \
+    get_inferable_quantizers
+from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
+    get_weights_quantizer_for_node, get_activations_quantizer_for_node
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 from model_compression_toolkit.core.common.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
 from model_compression_toolkit.core.common.similarity_analyzer import compute_kl_divergence, compute_cs, compute_mse
-from model_compression_toolkit.core.keras.back2framework.model_gradients import \
-    keras_iterative_approx_jacobian_trace
-from model_compression_toolkit.core.keras.constants import ACTIVATION, SOFTMAX, SIGMOID, ARGMAX, LAYER_NAME
+from model_compression_toolkit.core.keras.constants import ACTIVATION, SOFTMAX, SIGMOID, ARGMAX, LAYER_NAME, \
+    COMBINED_NMS
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_reconstruction import \
     keras_batchnorm_reconstruction
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.virtual_activation_weights_composition import \
     VirtualActivationWeightsComposition
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.weights_activation_split import \
     WeightsActivationSplit
 from model_compression_toolkit.core.keras.mixed_precision.configurable_activation_quantizer import \
@@ -46,33 +54,33 @@
         Concatenate, Add
     from keras.src.layers.core import TFOpLambda
 else:
     from keras.layers import Dense, Activation, Conv2D, DepthwiseConv2D, Conv2DTranspose, \
         Concatenate, Add
     from keras.layers.core import TFOpLambda
 
-from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
-from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.activation_decomposition import \
     ActivationDecomposition
+from model_compression_toolkit.core.keras.graph_substitutions.substitutions.matmul_substitution import \
+    MatmulToDenseSubstitution
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.softmax_shift import \
     keras_softmax_shift
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_folding import \
     keras_batchnorm_folding, keras_batchnorm_forward_folding
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_refusing import \
     keras_batchnorm_refusing
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.linear_collapsing import \
-    keras_linear_collapsing
+    keras_linear_collapsing, keras_op2d_add_const_collapsing
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.residual_collapsing import \
     keras_residual_collapsing
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.input_scaling import InputScaling, \
     InputScalingWithPad
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.relu_bound_to_power_of_2 import \
     ReLUBoundToPowerOfTwo
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.remove_relu_upper_bound import \
@@ -81,18 +89,17 @@
     MultiHeadAttentionDecomposition
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.scale_equalization import \
     ScaleEqualization, ScaleEqualizationWithPad, ScaleEqualizationMidActivation, ScaleEqualizationMidActivationWithPad
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.separableconv_decomposition import \
     SeparableConvDecomposition, DEPTH_MULTIPLIER
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.shift_negative_activation import \
     keras_apply_shift_negative_correction
+from model_compression_toolkit.core.keras.graph_substitutions.substitutions.dwconv_to_conv import DwconvToConv
 from model_compression_toolkit.core.keras.keras_node_prior_info import create_node_prior_info
 from model_compression_toolkit.core.keras.reader.reader import model_reader
-from model_compression_toolkit.core.common.collectors.statistics_collector_generator import \
-    create_stats_collector_for_node
 import model_compression_toolkit.core.keras.constants as keras_constants
 from model_compression_toolkit.core.keras.tf_tensor_numpy import tf_tensor_to_numpy, to_tf_tensor
 from model_compression_toolkit.core.keras.back2framework import get_keras_model_builder
 
 
 class KerasImplementation(FrameworkImplementation):
     """
@@ -204,29 +211,14 @@
         Returns:
             Graph after SNC.
         """
         return keras_apply_shift_negative_correction(graph,
                                                      core_config,
                                                      fw_info)
 
-    def attach_sc_to_node(self,
-                          node: BaseNode,
-                          fw_info: FrameworkInfo) -> BaseStatsCollector:
-        """
-        Return a statistics collector that should be attached to a node's output
-        during statistics collection.
-
-        Args:
-            node: Node to return its collector.
-            fw_info: Information relevant to a specific framework about what is out channel axis (for statistics per-channel)
-
-        Returns:
-            Statistics collector for the node.
-        """
-        return create_stats_collector_for_node(node, fw_info)
 
     def get_substitutions_channel_equalization(self,
                                                quant_config: QuantizationConfig,
                                                fw_info: FrameworkInfo) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used for channel equalization.
 
@@ -248,16 +240,18 @@
     def get_substitutions_prepare_graph(self, fw_info: FrameworkInfo = None) -> List[common.BaseSubstitution]:
         """
 
         Returns: A list of the framework substitutions used to prepare the graph.
 
         """
         return [SeparableConvDecomposition(),
+                MatmulToDenseSubstitution(),
                 MultiHeadAttentionDecomposition(),
-                ActivationDecomposition()]
+                ActivationDecomposition(),
+                DwconvToConv()]
 
     def get_substitutions_pre_statistics_collection(self, quant_config: QuantizationConfig) -> \
             List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used before we collect statistics.
 
         Args:
@@ -298,14 +292,20 @@
 
     def get_linear_collapsing_substitution(self) -> common.BaseSubstitution:
         """
         Returns: linear collapsing substitution
         """
         return keras_linear_collapsing()
 
+    def get_op2d_add_const_collapsing_substitution(self) -> common.BaseSubstitution:
+        """
+        Returns: Op2d add-const collapsing substitution
+        """
+        return keras_op2d_add_const_collapsing()
+
     def get_substitutions_post_statistics_collection(self, quant_config: QuantizationConfig) \
             -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used after we collect statistics.
 
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
@@ -352,28 +352,30 @@
         substitutions_list = []
         if quant_config.weights_second_moment_correction:
             substitutions_list.append(keras_batchnorm_refusing())
         return substitutions_list
 
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
-                                  quant_config: MixedPrecisionQuantizationConfigV2,
+                                  quant_config: MixedPrecisionQuantizationConfig,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
-                                  disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
+                                  disable_activation_for_metric: bool = False,
+                                  hessian_info_service: HessianInfoService = None) -> SensitivityEvaluation:
         """
         Creates and returns an object which handles the computation of a sensitivity metric for a mixed-precision
         configuration (comparing to the float model).
 
         Args:
             graph: Graph to build its float and mixed-precision models.
             quant_config: QuantizationConfig of how the model should be quantized.
             representative_data_gen: Dataset to use for retrieving images for the models inputs.
             fw_info: FrameworkInfo object with information about the specific framework's model.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
 
         Returns:
             A SensitivityEvaluation object.
         """
 
         return SensitivityEvaluation(graph=graph,
                                      quant_config=quant_config,
@@ -381,15 +383,16 @@
                                      fw_info=fw_info,
                                      fw_impl=self,
                                      set_layer_to_bitwidth=partial(set_layer_to_bitwidth,
                                                                    weights_quantizer_type=ConfigurableWeightsQuantizer,
                                                                    activation_quantizer_type=ConfigurableActivationQuantizer,
                                                                    weights_quant_layer_type=KerasQuantizationWrapper,
                                                                    activation_quant_layer_type=KerasActivationQuantizationHolder),
-                                     disable_activation_for_metric=disable_activation_for_metric)
+                                     disable_activation_for_metric=disable_activation_for_metric,
+                                     hessian_info_service=hessian_info_service)
 
     def get_node_prior_info(self,
                             node: BaseNode,
                             fw_info: FrameworkInfo,
                             graph: Graph) -> NodePriorInfo:
         """
         Get a NodePriorInfo object for a node that represents a Keras layer.
@@ -422,98 +425,98 @@
                            tf.concat, Add, tf.add]:
             return True
 
         return False
 
     def get_node_distance_fn(self, layer_class: type,
                              framework_attrs: Dict[str, Any],
-                             compute_distance_fn: Callable = None) -> Callable:
+                             compute_distance_fn: Callable = None,
+                             axis: int = None) -> Callable:
         """
         A mapping between layers' types and a distance function for computing the distance between
         two tensors (for loss computation purposes). Returns a specific function if node of specific types is
         given, or a default (normalized MSE) function otherwise.
 
         Args:
             layer_class: Class path of a model's layer.
             framework_attrs: Framework attributes the layer had which the graph node holds.
             compute_distance_fn: An optional distance function to use globally for all nodes.
+            axis: The axis on which the operation is preformed (if specified).
 
         Returns: A distance function between two tensors.
         """
 
         if compute_distance_fn is not None:
             return compute_distance_fn
 
         if layer_class == Activation:
             node_type_name = framework_attrs[ACTIVATION]
-            if node_type_name == SOFTMAX:
+            if node_type_name == SOFTMAX and axis is not None:
                 return compute_kl_divergence
             elif node_type_name == SIGMOID:
                 return compute_cs
-        elif layer_class == tf.nn.softmax or layer_class == tf.keras.layers.Softmax \
-                or (layer_class == TFOpLambda and SOFTMAX in framework_attrs[keras_constants.FUNCTION]):
+        elif axis is not None and (layer_class == tf.nn.softmax or layer_class == tf.keras.layers.Softmax
+                                   or (layer_class == TFOpLambda and
+                                       SOFTMAX in framework_attrs[keras_constants.FUNCTION])):
             return compute_kl_divergence
         elif layer_class == tf.nn.sigmoid:
             return compute_cs
         elif layer_class == Dense:
             return compute_cs
         return compute_mse
 
-    def model_grad(self,
-                   graph_float: common.Graph,
-                   model_input_tensors: Dict[BaseNode, np.ndarray],
-                   interest_points: List[BaseNode],
-                   output_list: List[BaseNode],
-                   all_outputs_indices: List[int],
-                   alpha: float = 0.3,
-                   n_iter: int = 50,
-                   norm_weights: bool = True) -> List[float]:
-        """
-        Calls a Keras model gradient calculation function, which computes the jacobian-based weights of the model's
-        outputs with respect to the feature maps of the set of given interest points.
-
-        Args:
-            graph_float: Graph to build its corresponding Keras model.
-            model_input_tensors: A mapping between model input nodes to an input batch.
-            interest_points: List of nodes which we want to get their feature map as output, to calculate distance metric.
-            output_list: List of nodes that considered as model's output for the purpose of gradients computation.
-            all_outputs_indices: Indices of the model outputs and outputs replacements (if exists),
-                in a topological sorted interest points list
-            alpha:A tuning parameter to allow calibration between the contribution of the output feature maps returned
-                weights and the other feature maps weights (since the gradient of the output layers does not provide a
-                compatible weight for the distance metric computation).
-            n_iter: The number of random iterations to calculate the approximated  jacobian-based weights for each interest point.
-            norm_weights: Whether to normalize the returned weights (to get values between 0 and 1).
-
-        Returns: A list of (possibly normalized) jacobian-based weights to be considered as the relevancy that each interest
-        point's output has on the model's output.
-
-        """
-
-        return keras_iterative_approx_jacobian_trace(graph_float, model_input_tensors, interest_points, output_list,
-                                                     all_outputs_indices, alpha, n_iter, norm_weights=norm_weights)
+    def get_trace_hessian_calculator(self,
+                                     graph: Graph,
+                                     input_images: List[Any],
+                                     trace_hessian_request: TraceHessianRequest,
+                                     num_iterations_for_approximation: int = HESSIAN_NUM_ITERATIONS):
+        """
+        Get Keras trace hessian approximations calculator based on the trace hessian request.
+        Args:
+            input_images: Images to use for computation.
+            graph: Float graph to compute the approximation of its different nodes.
+            trace_hessian_request: TraceHessianRequest to search for the desired calculator.
+            num_iterations_for_approximation: Number of iterations to use when approximating the Hessian trace.
+
+        Returns: TraceHessianCalculatorKeras to use for the trace hessian approximation computation for this request.
+
+        """
+        if trace_hessian_request.mode == HessianMode.ACTIVATION:
+            return ActivationTraceHessianCalculatorKeras(graph=graph,
+                                                         trace_hessian_request=trace_hessian_request,
+                                                         input_images=input_images,
+                                                         fw_impl=self,
+                                                         num_iterations_for_approximation=num_iterations_for_approximation)
+        elif trace_hessian_request.mode == HessianMode.WEIGHTS:
+            return WeightsTraceHessianCalculatorKeras(graph=graph,
+                                                      trace_hessian_request=trace_hessian_request,
+                                                      input_images=input_images,
+                                                      fw_impl=self,
+                                                      num_iterations_for_approximation=num_iterations_for_approximation)
+        else:
+            Logger.critical(f"Unsupported Hessian mode for Keras: {trace_hessian_request.mode}.")
 
-    def is_node_compatible_for_metric_outputs(self,
-                                              node: BaseNode) -> Any:
+    def is_output_node_compatible_for_hessian_score_computation(self,
+                                                                node: BaseNode) -> Any:
         """
-        Checks and returns whether the given node is compatible as output for metric computation
-        purposes and gradient-based weights calculation.
+        Checks and returns whether the given node is compatible as output for Hessian-based information computation.
 
         Args:
             node: A BaseNode object.
 
-        Returns: Whether the node is compatible as output for metric computation or not.
+        Returns: Whether the node is compatible as output for Hessian-based information computation.
 
         """
 
         if node.layer_class == TFOpLambda:
             node_attr = getattr(node, 'framework_attr', None)
-            if node_attr is not None and (ARGMAX in node_attr[LAYER_NAME] or SOFTMAX in node_attr[LAYER_NAME]):
+            if node_attr is not None and (ARGMAX in node_attr[LAYER_NAME]
+                                          or COMBINED_NMS in node_attr[LAYER_NAME]):
                 return False
-        elif node.layer_class in [tf.nn.softmax, tf.keras.layers.Softmax, tf.math.argmax]:
+        elif node.layer_class in [tf.math.argmax]:
             return False
 
         return True
 
     def get_node_mac_operations(self,
                                 node: BaseNode,
                                 fw_info: FrameworkInfo) -> float:
@@ -580,7 +583,43 @@
             inputs: Input tensors to run inference on.
 
         Returns:
             The output of the model inference on the given input.
         """
 
         return model(inputs)
+
+    def get_inferable_quantizers(self, node: BaseNode):
+        """
+        Returns sets of Keras compatible weights and activation quantizers for the given node.
+
+        Args:
+           node: Node to get quantizers for.
+
+        Returns:
+            weight_quantizers: A dictionary between a weight's name to its quantizer.
+            activation_quantizers: A list of activations quantization, one for each layer output.
+
+        """
+
+        def _weight_name(w: str) -> str:
+            """
+            Extracts the weight name from the full TensorFlow variable name.
+
+            For example, returns 'kernel' for 'dense_2/kernel:0'.
+
+            Args:
+              w: TensorFlow variable name.
+
+            Returns:
+              Extracted weight name.
+            """
+
+            return w.split(':')[0].split('/')[-1]
+
+        attribute_names = [_weight_name(wn) for wn in node.get_node_weights_attributes()
+                           if node.is_weights_quantization_enabled(wn)]
+
+        return get_inferable_quantizers(node,
+                                        get_weights_quantizer_for_node,
+                                        get_activations_quantizer_for_node,
+                                        attribute_names)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/keras_model_validation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/keras_model_validation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/keras_node_prior_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/keras_node_prior_info.py`

 * *Files 5% similar despite different names*

```diff
@@ -54,15 +54,14 @@
     Returns:
         Min/max output values if known.
     """
     min_output, max_output = None, None
 
     if node.type == ReLU:
         min_output = node.framework_attr[THRESHOLD] if node.framework_attr[NEGATIVE_SLOPE] == 0 else None
-        max_output = node.framework_attr[RELU_MAX_VALUE]
 
     elif fw_info.layers_has_min_max(node.type):
         min_output, max_output = fw_info.layer_min_max_mapping[node.type]
 
     elif node.type == Activation and fw_info.activation_has_min_max(node.framework_attr[ACTIVATION]):
         min_output, max_output = fw_info.activation_min_max_mapping[node.framework_attr[ACTIVATION]]
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/kpi_data_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/pytorch/quantization_facade.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,158 +8,125 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 from typing import Callable
 
-from model_compression_toolkit.core import MixedPrecisionQuantizationConfig, CoreConfig, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.constants import TENSORFLOW
+from model_compression_toolkit.constants import PYTORCH, FOUND_TORCH
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_data import compute_kpi_data
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    DEFAULT_MIXEDPRECISION_CONFIG
-from model_compression_toolkit.constants import FOUND_TF
+    MixedPrecisionQuantizationConfig
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+from model_compression_toolkit.core.exporter import export_model
+from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 
-if FOUND_TF:
-    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
-    from tensorflow.keras.models import Model
 
+if FOUND_TORCH:
+    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+    from torch.nn import Module
+    from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
     from model_compression_toolkit import get_target_platform_capabilities
 
-    KERAS_DEFAULT_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
-
+    DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
-    def keras_kpi_data(in_model: Model,
-                       representative_data_gen: Callable,
-                       quant_config: MixedPrecisionQuantizationConfig = DEFAULT_MIXEDPRECISION_CONFIG,
-                       fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                       target_platform_capabilities: TargetPlatformCapabilities = KERAS_DEFAULT_TPC) -> KPI:
+    def pytorch_post_training_quantization(in_module: Module,
+                                           representative_data_gen: Callable,
+                                           target_resource_utilization: ResourceUtilization = None,
+                                           core_config: CoreConfig = CoreConfig(),
+                                           target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
         """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and target platform modeling, and uses it to compute the KPI data.
+        Quantize a trained Pytorch module using post-training quantization.
+        By default, the module is quantized using a symmetric constraint quantization thresholds
+        (power of two) as defined in the default TargetPlatformCapabilities.
+        The module is first optimized using several transformations (e.g. BatchNormalization folding to
+        preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+        being collected for each layer's output (and input, depends on the quantization configuration).
+        Thresholds are then being calculated using the collected statistics and the module is quantized
+        (both coefficients and activations by default).
+        If gptq_config is passed, the quantized weights are optimized using gradient based post
+        training quantization by comparing points between the float and quantized modules, and minimizing the
+        observed loss.
 
         Args:
-            in_model (Model): Keras model to quantize.
+            in_module (Module): Pytorch module to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
-            quant_config (MixedPrecisionQuantizationConfig): MixedPrecisionQuantizationConfig containing parameters of how the model should be quantized.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+            target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+            core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
 
         Returns:
-            A KPI object with total weights parameters sum, max activation tensor and total kpi.
+            A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
-            Import a Keras model:
+            Import a Pytorch module:
 
-            >>> from tensorflow.keras.applications.mobilenet import MobileNet
-            >>> model = MobileNet()
+            >>> from torchvision import models
+            >>> module = models.mobilenet_v2()
 
             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
             In this example a random dataset of 10 batches each containing 4 images is used.
 
             >>> import numpy as np
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 224, 224, 3))]
+            >>>         yield [np.random.random((4, 3, 224, 224))]
 
-            Import MCT and call for KPI data calculation:
+            Import MCT and pass the module with the representative dataset generator to get a quantized module
+            Set number of clibration iterations to 1:
 
             >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.keras_kpi_data(model, repr_datagen)
-
+            >>> quantized_module, quantization_info = mct.ptq.pytorch_post_training_quantization(module, repr_datagen)
 
         """
 
-        if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfig object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfig.")
-
-        fw_impl = KerasImplementation()
-
-        quantization_config, mp_config = quant_config.separate_configs()
-        core_config = CoreConfig(quantization_config=quantization_config,
-                                 mixed_precision_config=mp_config)
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
-
-
-    def keras_kpi_data_experimental(in_model: Model,
-                                    representative_data_gen: Callable,
-                                    core_config: CoreConfig,
-                                    fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                                    target_platform_capabilities: TargetPlatformCapabilities = KERAS_DEFAULT_TPC) -> KPI:
-        """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and hw modeling, and uses it to compute the KPI data.
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfig. Please use "
+                             "pytorch_post_training_quantization API, or pass a valid mixed precision "
+                             "configuration.")  # pragma: no cover
+
+        tb_w = init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
+
+        fw_impl = PytorchImplementation()
+
+        # Ignore hessian info service as it is not used here yet.
+        tg, bit_widths_config, _ = core_runner(in_model=in_module,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=DEFAULT_PYTORCH_INFO,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
+
+        tg = ptq_runner(tg, representative_data_gen, core_config, DEFAULT_PYTORCH_INFO, fw_impl, tb_w)
+
+        if core_config.debug_config.analyze_similarity:
+            analyzer_model_quantization(representative_data_gen,
+                                        tb_w,
+                                        tg,
+                                        fw_impl,
+                                        DEFAULT_PYTORCH_INFO)
 
-        Args:
-            in_model (Model): Keras model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            core_config (CoreConfig): CoreConfig containing parameters for quantization and mixed precision of how the model should be quantized.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-
-        Returns:
-
-            A KPI object with total weights parameters sum and max activation tensor.
-
-        Examples:
+        return get_exportable_pytorch_model(tg)
 
-            Import a Keras model:
-
-            >>> from tensorflow.keras.applications.mobilenet import MobileNet
-            >>> model = MobileNet()
-
-            Create a random dataset generator:
-
-            >>> import numpy as np
-            >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
-
-            Import MCT and call for KPI data calculation:
-
-            >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.keras_kpi_data(model, repr_datagen)
-
-        """
-
-        if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfigV2 object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfigV2.")
-
-        fw_impl = KerasImplementation()
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
 
 else:
-    # If tensorflow is not installed,
-    # we raise an exception when trying to use this function.
-    def keras_kpi_data(*args, **kwargs):
-        Logger.critical('Installing tensorflow and is mandatory '
-                        'when using keras_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
-
-
-    def keras_kpi_data_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow is mandatory '
-                        'when using keras_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    # If torch is not installed,
+    # we raise an exception when trying to use these functions.
+    def pytorch_post_training_quantization(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_post_training_quantization_experimental'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/configurable_activation_quantizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -41,34 +41,36 @@
     It holds a set of activation quantizers for each of the given bit-width candidates, provided by the
     node's quantization config. This allows to use different quantized activations on-the-fly, according to the
     "active" quantization configuration index.
     """
 
     def __init__(self,
                  node_q_cfg: List[CandidateNodeQuantizationConfig],
-                 max_candidate_idx: int = 0):
+                 max_candidate_idx: int = 0,
+                 kernel_attr: str = None):
         """
         Initializes a configurable quantizer.
 
         Args:
             node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
                 use this quantizer.
             max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
+            kernel_attr: A kernel attribute name if the node have a kernel attribute (used only for candidates order validation).
         """
 
         super(ConfigurableActivationQuantizer, self).__init__()
 
         self.node_q_cfg = node_q_cfg
 
-        verify_candidates_descending_order(self.node_q_cfg)
+        verify_candidates_descending_order(self.node_q_cfg, kernel_attr)
 
         for qc in node_q_cfg:
             if qc.activation_quantization_cfg.enable_activation_quantization != \
                     node_q_cfg[0].activation_quantization_cfg.enable_activation_quantization:
-                Logger.error("Candidates with different activation enabled properties is currently not supported.")  # pragma: no cover
+                Logger.critical("Unsupported configuration: Mixing candidates with differing activation quantization states (enabled/disabled).")  # pragma: no cover
 
         self.activation_quantizers = init_activation_quantizers(self.node_q_cfg)
         self.active_quantization_config_index = max_candidate_idx  # initialize with first config as default
 
     def set_active_activation_quantizer(self, index: int):
         """
         Set an index to use for the activation quantizer to return when requested.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,124 +8,116 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from functools import partial
 from typing import Dict, Any, List
 
+from model_compression_toolkit.core.common.mixed_precision.configurable_quant_id import ConfigurableQuantizerIdentifier
 from model_compression_toolkit.core.common.mixed_precision.configurable_quantizer_utils import \
     verify_candidates_descending_order, init_quantized_weights
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from mct_quantizers import QuantizationTarget
+
 from mct_quantizers import mark_quantizer
 
-import tensorflow as tf
-from model_compression_toolkit.core.common.mixed_precision.configurable_quant_id import \
-    ConfigurableQuantizerIdentifier
-from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
+import torch
+import torch.nn as nn
+from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC,
                                      QuantizationMethod.UNIFORM, QuantizationMethod.LUT_POT_QUANTIZER,
                                      QuantizationMethod.LUT_SYM_QUANTIZER],
                 identifier=ConfigurableQuantizerIdentifier.CONFIGURABLE_ID)
-class ConfigurableWeightsQuantizer(BaseKerasInferableQuantizer):
+class ConfigurableWeightsQuantizer(BasePyTorchInferableQuantizer):
     """
-    Configurable weights quantizer for Keras mixed precision search.
+    Configurable weights quantizer for Pytorch mixed precision search.
     The quantizer holds a set of quantized layer's weights for each of the given bit-width candidates, provided by the
     node's quantization config. This allows to use different quantized weights on-the-fly.
 
     The general idea behind this kind of quantizer is that it gets the float tensor to quantize
     when initialized, it quantizes the float tensor in different bitwidths, and every time it need to return a
     quantized version of the float weight, it returns only one quantized weight according to an "active"
     index - the index of a candidate weight quantization configuration from a list of candidates that was passed
     to the quantizer when it was initialized.
     """
 
     def __init__(self,
                  node_q_cfg: List[CandidateNodeQuantizationConfig],
-                 float_weights: tf.Tensor,
+                 float_weights: torch.Tensor,
+                 kernel_attr: str,
                  max_candidate_idx: int = 0):
         """
         Initializes a configurable quantizer.
 
         Args:
             node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
                 use this quantizer.
             float_weights: Float weights of the layer.
+            kernel_attr: The kernel attribute name of the node. Only layers with kernel op can be configured.
             max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
-
         """
 
         super(ConfigurableWeightsQuantizer, self).__init__()
 
         self.node_q_cfg = node_q_cfg
         self.float_weights = float_weights
         self.max_candidate_idx = max_candidate_idx
+        self.kernel_attr = kernel_attr
 
-        verify_candidates_descending_order(self.node_q_cfg)
+        verify_candidates_descending_order(self.node_q_cfg, kernel_attr)
 
         for qc in self.node_q_cfg:
-            if qc.weights_quantization_cfg.enable_weights_quantization != \
-                    self.node_q_cfg[0].weights_quantization_cfg.enable_weights_quantization:
-                Logger.error("Candidates with different weights enabled properties is currently not supported.")
+            if qc.weights_quantization_cfg.get_attr_config(self.kernel_attr).enable_weights_quantization != \
+                   self.node_q_cfg[0].weights_quantization_cfg.get_attr_config(self.kernel_attr).enable_weights_quantization:
+                Logger.critical("Unsupported configuration: Mixing candidates with differing weights quantization states (enabled/disabled).")  # pragma: no cover
 
         # Initialize quantized weights for each weight that should be quantized.
         self.quantized_weights = init_quantized_weights(node_q_cfg=self.node_q_cfg,
                                                         float_weights=self.float_weights,
-                                                        fw_tensor_convert_func=partial(tf.convert_to_tensor,
-                                                                                       dtype=tf.float32))
+                                                        fw_tensor_convert_func=to_torch_tensor,
+                                                        kernel_attr=kernel_attr)
 
         self.active_quantization_config_index = self.max_candidate_idx
 
     def set_weights_bit_width_index(self,
                                     index: int):
         """
         Change the "active" bitwidth index the configurable quantizer uses, so a different quantized weight
         will be used.
 
         Args:
             index: Quantization configuration candidate index to use.
 
         """
 
-        if index >= len(self.node_q_cfg):
-            Logger.error(f'Quantizer has {len(self.node_q_cfg)} '  # pragma: no cover
-                         f'possible nbits. Can not set index {index}')
+        assert index < len(self.node_q_cfg), \
+            f'Quantizer has {len(self.node_q_cfg)} ' \
+            f'possible nbits. Can not set index {index}'
         self.active_quantization_config_index = index
 
     def __call__(self,
-                 inputs: tf.Tensor) -> tf.Tensor:
+                 inputs: nn.Parameter) -> torch.Tensor:
         """
         Method to return the quantized weight. This method is called when the framework needs to quantize a
-        float weight, and is expected to return the quantized weight. Since we already quantized the weight in
-        all possible bitwidths, we do not quantize it again, and simply return the quantized weight according
-        to the current active_quantization_config_index.
+            float weight, and is expected to return the quantized weight. Since we already quantized the weight in
+            all possible bitwidths, we do not quantize it again, and simply return the quantized weight according
+            to the current active_quantization_config_index.
 
         Args:
-            inputs: Input tensor (not relevant since the weights are already quantized).
+            inputs: Input tensor (not used in this function since the weights are already quantized).
 
         Returns:
             Quantized weight, that was quantized using number of bits that is in a
-            specific quantization configuration candidate (the candidate's index is the
-            index that is in active_quantization_config_index the quantizer holds).
+                specific quantization configuration candidate (the candidate's index is the
+                index that is in active_quantization_config_index the quantizer holds).
         """
 
         return self.quantized_weights[self.active_quantization_config_index]
-
-    def get_config(self) -> Dict[str, Any]:  # pragma: no cover
-        """
-        Returns: The ConfigurableWeightsQuantizer configuration.
-        """
-
-        return {
-            'float_weights': self.float_weights,
-            'node_q_cfg': self.node_q_cfg,
-            'active_quantization_config_index': self.active_quantization_config_index
-        }
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/base_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/base_quantizer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,89 +1,81 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Callable
+import torch
 
-
-from typing import Tuple, Callable
-
-import tensorflow as tf
-import numpy as np
-from tensorflow.python.util.object_identity import Reference as TFReference
-
-from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import threshold_is_power_of_two
+from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
 
 
-def quantizer_min_max_calculator(threshold: np.ndarray,
-                                 num_bits: int,
-                                 signed: bool) -> Tuple[float, float]:
+def get_symmetric_quantization_range_and_scale(activation_is_signed: bool,
+                                               activation_n_bits: int,
+                                               activation_threshold: float):
     """
-    Compute quantization range's min/max values given a threshold, number of bits,
-     and whether it's signed or not.
+    Calculates lower and upper bounds on the quantization range, along with quantization scale,
+    for symmetric quantization (used for the symmetric and power-of-two quantizers),
+    according to whether the quantization is signed or unsigned.
 
     Args:
-        threshold: Threshold for quantization range values.
-        num_bits: Number of bits to use for quantization.
-        signed: Whether the quantization range should include negative values or not.
+        activation_is_signed: Whether the quantization is signed or not.
+        activation_n_bits: Number of bits to use for quantization.
+        activation_threshold: The quantization threshold.
 
-    Returns:
-        Min and max values for quantization range.
-    """
+    Returns: range lower bound, range upper bound and quantization scale.
 
-    if signed:
-        delta = threshold / (2 ** (num_bits - 1))
-        min_value = -threshold
+    """
+    if activation_is_signed:
+        min_value = -2 ** (activation_n_bits - 1)
+        max_value = 2 ** (activation_n_bits - 1) - 1
+        scale = activation_threshold / 2 ** (activation_n_bits - 1)
     else:
-        delta = threshold / (2 ** (num_bits))
         min_value = 0
+        max_value = (2 ** activation_n_bits) - 1
+        scale = activation_threshold / 2 ** activation_n_bits
 
-    max_value = threshold - delta
-    return min_value, max_value
+    return min_value, max_value, scale
 
 
 def power_of_two_quantization(activation_n_bits: int,
                               quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
-    build and return a fake-quantization node with power-of-two quantization.
-
+    build and return a fake-quantization node, quantized with a power-of-two threshold.
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
-
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None:
-        Logger.error("Activation threshold is None")  # pragma: no cover
-    if activation_is_signed is None:
-        Logger.error("activation_is_signed is None")  # pragma: no cover
+    if activation_threshold is None or activation_is_signed is None:
+        return None # pragma: no cover
     if not threshold_is_power_of_two(activation_threshold, per_channel=False):
-        Logger.error("Activation threshold is not power of two")  # pragma: no cover
+        return None # pragma: no cover
 
-    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
-                                                        activation_n_bits,
-                                                        activation_is_signed)
+    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
+                                                                             activation_n_bits,
+                                                                             activation_threshold)
 
-    return lambda x: q(x, min_value, max_value, activation_n_bits)
+    return lambda x: q(x, min_value, max_value, scale)
 
 
 def symmetric_quantization(activation_n_bits: int,
                            quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a symmetric fake-quantization node.
@@ -94,24 +86,22 @@
 
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None:
-        Logger.error("Activation threshold is None")  # pragma: no cover
-    if activation_is_signed is None:
-        Logger.error("activation_is_signed is None")  # pragma: no cover
-
-    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
-                                                        activation_n_bits,
-                                                        activation_is_signed)
+    if activation_threshold is None or activation_is_signed is None:
+        return None # pragma: no cover
+
+    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
+                                                                             activation_n_bits,
+                                                                             activation_threshold)
 
-    return lambda x: q(x, min_value, max_value, activation_n_bits)
+    return lambda x: q(x, min_value, max_value, scale)
 
 
 def uniform_quantization(activation_n_bits: int,
                          quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a uniform fake-quantization node.
@@ -119,38 +109,47 @@
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
 
     Returns:
         A fake quantization node.
     """
-    min_value, max_value = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
+    a, b = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
 
-    if min_value is None:
-        Logger.error("Min value is None")  # pragma: no cover
-    if max_value is None:
-        Logger.error("Max value is None")  # pragma: no cover
+    if a is None or b is None:
+        return None # pragma: no cover
 
-    return lambda x: q(x, min_value, max_value, activation_n_bits)
+    # fixing quantization range to include 0
+    a = 0 if a > 0 else a
+    b = 0 if b < 0 else b
+    a, b = fix_range_to_include_zero(a, b, activation_n_bits)
 
+    min_value = 0
+    max_value = 2 ** activation_n_bits - 1
+    scale = (b - a) / ((2 ** activation_n_bits) - 1)
+    zero_point = -round(a / scale)  # zp has to be positive, and a <=0, so we multiply by -1
 
-def q(x: TFReference, min_value, max_value, activation_n_bits) -> TFReference:
-    """
-    Fake-quantize the input tensor x, using a tensorflow fake-quantization node.
+    return lambda x: q(x, min_value, max_value, scale, zero_point)
 
-    Args:
-        x: Input tensor to quantize.
-        min_value: quantization range lower bound.
-        max_value: quantization range upper bound.
-        activation_n_bits: Number of bits to use for quantization.
 
+def q(x: torch.Tensor,
+      min_value: int,
+      max_value: int,
+      scale: float,
+      zero_point: int = 0) -> torch.Tensor:
+    """
+    Fake-quantize the input tensor x, using a pytorch fake-quantization node.
+    Args:
+        x: input tensor to quantize.
+        min_value: lower bound of the quantized domain.
+        max_value: upper bound of the quantized domain.
+        scale: quantization scale.
+        zero_point: quantization zero_point
     Returns:
         The fake-quantized input tensor.
     """
-    if x.dtype != tf.float32:
-        x = tf.cast(x, dtype=tf.float32)  # pragma: no cover
 
-    # fake_quant_with_min_max_vars expects to get x of float32
-    return tf.quantization.fake_quant_with_min_max_vars(x,
-                                                        min=min_value,
-                                                        max=max_value,
-                                                        num_bits=activation_n_bits)
+    return torch.fake_quantize_per_tensor_affine(x,
+                                                 scale=scale,
+                                                 zero_point=zero_point,
+                                                 quant_min=min_value,
+                                                 quant_max=max_value)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/common.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/common.py`

 * *Files 6% similar despite different names*

```diff
@@ -43,15 +43,15 @@
         Whether the node represents an input layer or not.
     """
     if isinstance(node, BaseNode):
         return node.type == InputLayer
     elif isinstance(node, KerasNode):
         return isinstance(node.layer, InputLayer)
     else:
-        Logger.error('Node to check has to be either a graph node or a keras node')  # pragma: no cover
+        Logger.critical('Node must be a graph node or a Keras node for input layer check.')  # pragma: no cover
 
 
 def is_node_a_model(node: BaseNode) -> bool:
     """
     Checks if a node represents a Keras model.
     Args:
         node: Node to check if its a Keras model by itself.
@@ -60,9 +60,9 @@
         Whether the node represents a Keras model or not.
     """
     if isinstance(node, BaseNode):
         return node.type in [Functional, Sequential]
     elif isinstance(node, KerasNode):
         return isinstance(node.layer, Functional) or isinstance(node.layer, Sequential)
     else:
-        Logger.error('Node to check has to be either a graph node or a keras node')  # pragma: no cover
+        Logger.critical('Node must be a graph node or a Keras node.')  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/connectivity_handler.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/connectivity_handler.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/reader/reader.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/reader/reader.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/statistics_correction/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/tf_tensor_numpy.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/tf_tensor_numpy.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,17 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Union, List, Tuple
 import tensorflow as tf
 import numpy as np
 
+from model_compression_toolkit.logger import Logger
+
 
 def to_tf_tensor(tensor):
     """
     Convert a Numpy array to a TF tensor.
     Args:
         tensor: Numpy array.
 
@@ -30,29 +33,38 @@
     elif isinstance(tensor, list):
         return [to_tf_tensor(t) for t in tensor]
     elif isinstance(tensor, tuple):
         return (to_tf_tensor(t) for t in tensor)
     elif isinstance(tensor, np.ndarray):
         return tf.convert_to_tensor(tensor.astype(np.float32))
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(tf.Tensor)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to TF tensor: {type(tensor)}.')
 
 
-def tf_tensor_to_numpy(tensor: tf.Tensor) -> np.ndarray:
+def tf_tensor_to_numpy(tensor: Union[List, Tuple, np.ndarray, tf.Tensor],
+                       is_single_tensor=False) -> np.ndarray:
     """
     Convert a TF tensor to a Numpy array.
     Args:
         tensor: TF tensor.
+        is_single_tensor: whether input is a value to be converted to a single tensor.
+                          if False, recurse the lists and tuples
 
     Returns:
         Numpy array converted from the input tensor.
     """
     if isinstance(tensor, np.ndarray):
         return tensor
     elif isinstance(tensor, list):
-        return [tf_tensor_to_numpy(t) for t in tensor]
+        if is_single_tensor:
+            return np.array(tensor)
+        else:
+            return [tf_tensor_to_numpy(t) for t in tensor]
     elif isinstance(tensor, tuple):
-        return (tf_tensor_to_numpy(t) for t in tensor)
+        if is_single_tensor:
+            return np.array(tensor)
+        else:
+            return (tf_tensor_to_numpy(t) for t in tensor)
     elif isinstance(tensor, tf.Tensor):
         return tensor.numpy()
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(np.ndarray)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to Numpy array: {type(tensor)}.')
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/keras/visualization/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/visualization/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -34,13 +34,13 @@
         mode: Mode of the PyTorch model builder.
 
     Returns:
         PyTorch model builder for the given mode.
     """
 
     if not isinstance(mode, ModelBuilderMode):
-        Logger.error(f'get_pytorch_model_builder expects a mode of type ModelBuilderMode, but {type(mode)} was passed.')
+        Logger.critical(f"Expected a ModelBuilderMode type for 'mode' parameter; received {type(mode)} instead.")
     if mode is None:
-        Logger.error(f'get_pytorch_model_builder received a mode which is None')
+        Logger.critical(f"Received 'mode' parameter is None.")
     if mode not in pytorch_model_builders.keys():
-        Logger.error(f'mode {mode} is not in pytorch model builders factory')
+        Logger.critical(f"'mode' parameter {mode} is not supported by the PyTorch model builders factory.")
     return pytorch_model_builders.get(mode)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 
     Returns:
         Pytorch module that was built from the node.
     """
 
     framework_attr = copy.copy(n.framework_attr)
     node_instance = n.type(**framework_attr)
-    node_instance.load_state_dict({k: torch.Tensor(v) for k, v in n.weights.items()}, strict=False)
+    node_instance.load_state_dict({k: torch.tensor(v) for k, v in n.weights.items()}, strict=False)
     set_model(node_instance)
     return node_instance
 
 
 # todo: remove. It is not used anymore
 def identity_wrapper(node: BaseNode,
                      module: Module,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,265 +9,319 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import List, Any, Tuple, Union, Dict
+import copy
+from typing import List, Dict, Union, Any
 
-import torch
-from mct_quantizers import PytorchQuantizationWrapper, QuantizationTarget, \
-    PytorchActivationQuantizationHolder
-from mct_quantizers.common.constants import ACTIVATION_HOLDER_QUANTIZER
-from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
-from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
-
-from model_compression_toolkit.core import FrameworkInfo
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-
-from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-from model_compression_toolkit.core.pytorch.mixed_precision.configurable_activation_quantizer import \
-    ConfigurableActivationQuantizer
-from model_compression_toolkit.core.pytorch.mixed_precision.configurable_weights_quantizer import \
-    ConfigurableWeightsQuantizer
-
-from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
-    get_weights_inferable_quantizer_kwargs, get_activation_inferable_quantizer_kwargs
+from mct_quantizers import QuantizationMethod
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
 from model_compression_toolkit.logger import Logger
 
 
-class MixedPrecisionPyTorchModelBuilder(PyTorchModelBuilder):
+def clone_and_edit_object_params(obj: Any, **kwargs: Dict) -> Any:
+    """
+    Clones the given object and edit some of its parameters.
+
+    Args:
+        obj: An object to clone.
+        **kwargs: Keyword arguments to edit in the cloned object.
+
+    Returns:
+        Edited copy of the given object.
+    """
+
+    obj_copy = copy.deepcopy(obj)
+    for k, v in kwargs.items():
+        assert hasattr(obj_copy,
+                       k), f'Edit parameter is possible only for existing parameters in the given object, ' \
+                           f'but {k} is not a parameter of {obj_copy}.'
+        setattr(obj_copy, k, v)
+    return obj_copy
+
+
+class AttributeQuantizationConfig:
     """
-    Mixed-precision PyTorch model.
+    Hold the quantization configuration of a weight attribute of a layer.
     """
     def __init__(self,
-                 graph: common.Graph,
-                 append2output=None,
-                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                 return_float_outputs: bool = False):
+                 weights_quantization_method: QuantizationMethod = QuantizationMethod.POWER_OF_TWO,
+                 weights_n_bits: int = FLOAT_BITWIDTH,
+                 weights_per_channel_threshold: bool = False,
+                 enable_weights_quantization: bool = False,
+                 lut_values_bitwidth: Union[int, None] = None,  # If None - set 8 in hptq, o.w use it
+                 ):
         """
+        Initializes an attribute quantization config.
 
         Args:
-            graph: Graph to build the model from.
-            append2output: Nodes to append to model's output.
-            fw_info: Information about the specific framework of the model that is built.
-            return_float_outputs: Whether the model returns float tensors or not.
-        """
-
-        self.graph = graph
-
-        super().__init__(graph,
-                         append2output,
-                         fw_info,
-                         return_float_outputs,
-                         wrapper=self.mixed_precision_wrapper,
-                         get_activation_quantizer_holder_fn=self.mixed_precision_activation_holder)
-
-    def mixed_precision_wrapper(self,
-                                n: common.BaseNode,
-                                layer: torch.nn.Module) -> Union[PytorchQuantizationWrapper, torch.nn.Module]:
+            weights_quantization_method (QuantizationMethod): Which method to use from QuantizationMethod for weights quantization.
+            weights_n_bits (int): Number of bits to quantize the coefficients.
+            weights_per_channel_threshold (bool): Whether to quantize the weights per-channel or not (per-tensor).
+            enable_weights_quantization (bool): Whether to quantize the model weights or not.
+            lut_values_bitwidth (int): Number of bits to use when quantizing in look-up-table.
+
+        """
+
+        self.weights_quantization_method = weights_quantization_method
+        self.weights_n_bits = weights_n_bits
+        self.weights_per_channel_threshold = weights_per_channel_threshold
+        self.enable_weights_quantization = enable_weights_quantization
+        self.lut_values_bitwidth = lut_values_bitwidth
+
+    def clone_and_edit(self, **kwargs):
         """
-        A function which takes a computational graph node and a pytorch layer and perform the quantization
-        wrapping for mixed precision.
+        Clone the quantization config and edit some of its attributes.
 
         Args:
-            n: A node of mct graph.
-            layer: A pytorch layer
+            **kwargs: Keyword arguments to edit the configuration to clone.
 
-        Returns: Wrapped layer with a configurable quantizer if the layer should quantized in mixed precision,
-        otherwise returns either the layer wrapped with a fixed precision inferable quantizer or the layer as is if it's
-        not supposed to be quantized.
+        Returns:
+            Edited quantization configuration.
+        """
+
+        return clone_and_edit_object_params(self, **kwargs)
 
+    def __eq__(self, other):
         """
+        Is this configuration equal to another object.
 
-        weights_conf_nodes_names = [n.name for n in self.graph.get_weights_configurable_nodes()]
+        Args:
+            other: Object to compare.
 
-        if n.is_weights_quantization_enabled():
-            kernel_attributes = self.fw_info.get_kernel_op_attributes(n.type)
-            if n.name in weights_conf_nodes_names:
-                return PytorchQuantizationWrapper(layer,
-                                                  weights_quantizers={attr: ConfigurableWeightsQuantizer(
-                                                      **self._get_weights_configurable_quantizer_kwargs(n, attr))
-                                                      for attr in kernel_attributes})
-            else:
-                node_weights_qc = n.get_unique_weights_candidates()
-                if not len(node_weights_qc) == 1:
-                    Logger.error(f"Expecting node {n.name} to have a unique weights configuration "  # pragma: no cover
-                                 f"but {len(node_weights_qc)} different configurations exist.")
+        Returns:
 
-                quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
-                                                                  node_weights_qc[0].weights_quantization_cfg.weights_quantization_method,
-                                                                  BasePyTorchInferableQuantizer)
-                kwargs = get_weights_inferable_quantizer_kwargs(node_weights_qc[0].weights_quantization_cfg)
+            Whether this configuration is equal to another object or not.
+        """
+        if not isinstance(other, AttributeQuantizationConfig):
+            return False
+        return self.weights_quantization_method == other.weights_quantization_method and \
+            self.weights_n_bits == other.weights_n_bits and \
+            self.weights_per_channel_threshold == other.weights_per_channel_threshold and \
+            self.enable_weights_quantization == other.enable_weights_quantization and \
+            self.lut_values_bitwidth == other.lut_values_bitwidth
 
-                return PytorchQuantizationWrapper(layer,
-                                                  weights_quantizers={attr: quantier_for_node(**kwargs)
-                                                                      for attr in kernel_attributes})
 
-        return layer
+class OpQuantizationConfig:
+    """
+    OpQuantizationConfig is a class to configure the quantization parameters of an operator.
+    """
 
-    def _get_weights_configurable_quantizer_kwargs(self, n: BaseNode, attr: str) -> Dict[str, Any]:
+    def __init__(self,
+                 default_weight_attr_config: AttributeQuantizationConfig,
+                 attr_weights_configs_mapping: Dict[str, AttributeQuantizationConfig],
+                 activation_quantization_method: QuantizationMethod,
+                 activation_n_bits: int,
+                 enable_activation_quantization: bool,
+                 quantization_preserving: bool,
+                 fixed_scale: float,
+                 fixed_zero_point: int,
+                 simd_size: int
+                 ):
         """
-        Get the quantization parameters for a configurable quantizer.
 
         Args:
-            n: The node for which the quantizer is being created.
-            attr: The name of the weights attribute to be quantized.
+            default_weight_attr_config (AttributeQuantizationConfig): A default attribute quantization configuration for the operation.
+            attr_weights_configs_mapping (Dict[str, AttributeQuantizationConfig]): A mapping between an op attribute name and its quantization configuration.
+            activation_quantization_method (QuantizationMethod): Which method to use from QuantizationMethod for activation quantization.
+            activation_n_bits (int): Number of bits to quantize the activations.
+            enable_activation_quantization (bool): Whether to quantize the model activations or not.
+            quantization_preserving (bool): Whether quantization parameters should be the same for an operator's input and output.
+            fixed_scale (float): Scale to use for an operator quantization parameters.
+            fixed_zero_point (int): Zero-point to use for an operator quantization parameters.
+            simd_size (int): Per op integer representing the Single Instruction, Multiple Data (SIMD) width of an operator. It indicates the number of data elements that can be fetched and processed simultaneously in a single instruction.
 
-        Returns:
-            The quantization parameters as a dictionary.
         """
 
-        assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
-        node_q_cfg_candidates = n.candidates_quantization_cfg
-        # sort by descending bit width so using indices would be easier
-        node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.weights_n_bits,
-                                                  x.activation_quantization_cfg.activation_n_bits), reverse=True)
-
-        float_weights = n.get_weights_by_keys(attr)
+        self.default_weight_attr_config = default_weight_attr_config
+        self.attr_weights_configs_mapping = attr_weights_configs_mapping
 
-        max_cfg_candidates = n.find_max_candidates_indices()
-        if not len(max_cfg_candidates) == 1:
-            Logger.error(f"A maximal config candidate must be defined, "  # pragma: no cover
-                         f"but some node have multiple potential maximal candidates")
+        self.activation_quantization_method = activation_quantization_method
+        self.activation_n_bits = activation_n_bits
+        self.enable_activation_quantization = enable_activation_quantization
+        self.quantization_preserving = quantization_preserving
+        self.fixed_scale = fixed_scale
+        self.fixed_zero_point = fixed_zero_point
+        self.simd_size = simd_size
 
-        max_candidate_idx = max_cfg_candidates[0]
+    def get_info(self):
+        """
 
-        return {'node_q_cfg': node_q_cfg_candidates,
-                'float_weights': float_weights,
-                'max_candidate_idx': max_candidate_idx
-                }
+        Returns: Info about the quantization configuration as a dictionary.
 
-    def mixed_precision_activation_holder(self, n: BaseNode) -> PytorchActivationQuantizationHolder:
         """
-        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization for a node.
-        The layer should hold either a configurable activation quantizer, if it is quantized with mixed precision,
-        or an inferable quantizer for fixed single bit-width quantization.
+        return self.__dict__
 
+    def clone_and_edit(self, attr_to_edit: Dict[str, Dict[str, Any]] = {}, **kwargs):
+        """
+        Clone the quantization config and edit some of its attributes.
         Args:
-            n: Node to get PytorchActivationQuantizationHolder to attach in its output.
+            attr_to_edit: A mapping between attributes names to edit and their parameters that
+            should be edited to a new value.
+            **kwargs: Keyword arguments to edit the configuration to clone.
 
         Returns:
-            A PytorchActivationQuantizationHolder layer for the node activation quantization.
+            Edited quantization configuration.
         """
 
-        activation_conf_nodes_names = [n.name for n in self.graph.get_activation_configurable_nodes()]
-
-        activation_quantizers = []
-        if n.is_activation_quantization_enabled():
-            num_of_outputs = len(n.output_shape) if isinstance(n.output_shape, list) else 1
-            if n.name in activation_conf_nodes_names:
-                assert n.candidates_quantization_cfg is not None, f"Node {n.name} candidates_quantization_cfg is None"
-                node_q_cfg_candidates = n.candidates_quantization_cfg
-                # sort by descending bit width so using indices would be easier
-                node_q_cfg_candidates.sort(key=lambda x: (x.weights_quantization_cfg.weights_n_bits,
-                                                          x.activation_quantization_cfg.activation_n_bits),
-                                           reverse=True)
-
-                max_cfg_candidates = n.find_max_candidates_indices()
-                assert len(max_cfg_candidates) == 1, \
-                    f"A maximal config candidate must be defined, but some node have multiple potential maximal candidates"
-                max_candidate_idx = max_cfg_candidates[0]
-
-                activation_quantizers = [ConfigurableActivationQuantizer(**{'node_q_cfg': node_q_cfg_candidates,
-                                                                            'max_candidate_idx': max_candidate_idx})] \
-                                        * num_of_outputs
-            else:
-                node_act_qc = n.get_unique_activation_candidates()
-                assert len(node_act_qc) == 1, f"Expecting node {n.name} to have a unique activation configuration, " \
-                                              f"but {len(node_act_qc)} different configurations exist."
-                quantizer_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
-                                                                   node_act_qc[0].activation_quantization_cfg.activation_quantization_method,
-                                                                   BasePyTorchInferableQuantizer)
-                kwargs = get_activation_inferable_quantizer_kwargs(node_act_qc[0].activation_quantization_cfg)
+        qc = clone_and_edit_object_params(self, **kwargs)
 
-                activation_quantizers = [quantizer_for_node(**kwargs)] * num_of_outputs
+        # optionally: editing specific parameters in the config of specified attributes
+        edited_attrs = copy.deepcopy(qc.attr_weights_configs_mapping)
+        for attr_name, attr_cfg in qc.attr_weights_configs_mapping.items():
+            if attr_name in attr_to_edit:
+                edited_attrs[attr_name] = attr_cfg.clone_and_edit(**attr_to_edit[attr_name])
 
-        # Holder by definition uses a single quantizer for the activation quantization
-        # thus we make sure this is the only possible case (unless it's a node with no activation
-        # quantization, which in this case has an empty list).
-        if len(activation_quantizers) == 1:
-            return PytorchActivationQuantizationHolder(activation_quantizers[0])
+        qc.attr_weights_configs_mapping = edited_attrs
 
-        Logger.error(f'PytorchActivationQuantizationHolder supports a single quantizer but '  # pragma: no cover
-                     f'{len(activation_quantizers)} quantizers were found for node {n}')
+        return qc
 
-    def build_model(self) -> Tuple[torch.nn.Module, UserInformation,
-                                   Dict[str, Union[PytorchQuantizationWrapper, PytorchActivationQuantizationHolder]]]:
+    def __eq__(self, other):
         """
-        Build a PyTorch float model and return it.
-        Returns: Float PyTorch model and user information.
+        Is this configuration equal to another object.
+        Args:
+            other: Object to compare.
 
+        Returns:
+            Whether this configuration is equal to another object or not.
         """
-        model, user_info = super().build_model()
+        if not isinstance(other, OpQuantizationConfig):
+            return False
+        return self.default_weight_attr_config == other.default_weight_attr_config and \
+            self.attr_weights_configs_mapping == other.attr_weights_configs_mapping and \
+            self.activation_quantization_method == other.activation_quantization_method and \
+            self.activation_n_bits == other.activation_n_bits and \
+            self.enable_activation_quantization == other.enable_activation_quantization and \
+            self.simd_size == other.simd_size
 
-        # creating a mapping between graph nodes and model's layers for mixed precision configurability
-        model_layers = dict(model.named_children())
-        conf_node2layers = {n.name: self._find_layers_in_model_by_node(n, model_layers)
-                            for n in self.graph.get_configurable_sorted_nodes()}
 
-        return model, user_info, conf_node2layers
+class QuantizationConfigOptions(object):
+    """
 
+    Wrap a set of quantization configurations to consider during the quantization
+    of an operator.
 
-    @staticmethod
-    def _get_weights_quant_layers(n: BaseNode, named_layers: Dict[str, torch.nn.Module]) \
-            -> List[PytorchQuantizationWrapper]:
+    """
+    def __init__(self,
+                 quantization_config_list: List[OpQuantizationConfig],
+                 base_config: OpQuantizationConfig = None):
         """
-        Filters PytorchQuantizationWrapper layers from an MP model that are matching to the given graph node.
 
         Args:
-            n: A configurable graph node.
-            named_layers: Mixed precision model layers list.
+            quantization_config_list (List[OpQuantizationConfig]): List of possible OpQuantizationConfig to gather.
+            base_config (OpQuantizationConfig): Fallback OpQuantizationConfig to use when optimizing the model in a non mixed-precision manner.
+        """
 
-        Returns: A list of layers that responsible for the node's weights quantization.
+        assert isinstance(quantization_config_list,
+                          list), f'\'QuantizationConfigOptions\' options list must be a list, but received: {type(quantization_config_list)}.'
+        assert len(quantization_config_list) > 0, f'Options list can not be empty.'
+        for cfg in quantization_config_list:
+            assert isinstance(cfg, OpQuantizationConfig), f'Each option must be an instance of \'OpQuantizationConfig\', but found an object of type: {type(cfg)}.'
+        self.quantization_config_list = quantization_config_list
+        if len(quantization_config_list) > 1:
+            assert base_config is not None, f'For multiple configurations, a \'base_config\' is required for non-mixed-precision optimization.'
+            assert base_config in quantization_config_list, f"\'base_config\' must be included in the quantization config options list."
+            self.base_config = base_config
+        elif len(quantization_config_list) == 1:
+            self.base_config = quantization_config_list[0]
+        else:
+            Logger.critical("\'QuantizationConfigOptions\' requires at least one \'OpQuantizationConfig\'; the provided list is empty.")
 
+    def __eq__(self, other):
         """
-        return [module for m_name, module in named_layers.items() if isinstance(module, PytorchQuantizationWrapper)
-                and m_name == n.name]
+        Is this QCOptions equal to another object.
+        Args:
+            other: Object to compare.
 
-    @staticmethod
-    def _get_activation_quant_layers(n: BaseNode, named_layers: Dict[str, torch.nn.Module]) \
-            -> List[PytorchActivationQuantizationHolder]:
+        Returns:
+            Whether this QCOptions equal to another object or not.
         """
-        Filters PytorchActivationQuantizationHolder layers from an MP model that are matching to the given graph node.
+
+        if not isinstance(other, QuantizationConfigOptions):
+            return False
+        if len(self.quantization_config_list) != len(other.quantization_config_list):
+            return False
+        for qc, other_qc in zip(self.quantization_config_list, other.quantization_config_list):
+            if qc != other_qc:
+                return False
+        return True
+
+    def clone_and_edit(self, **kwargs):
+        qc_options = copy.deepcopy(self)
+        for qc in qc_options.quantization_config_list:
+            self.__edit_quantization_configuration(qc, kwargs)
+        return qc_options
+
+    def clone_and_edit_weight_attribute(self, attrs: List[str] = None, **kwargs):
+        """
+        Clones the quantization configurations and edits some of their attributes' parameters.
 
         Args:
-            n: A configurable graph node.
-            named_layers: Mixed precision model layers list.
+            attrs: attributes names to clone their configurations. If None is provided, updating the configurations
+                of all attributes in the operation attributes config mapping.
+            **kwargs: Keyword arguments to edit in the attributes configuration.
 
-        Returns: A list of layers that responsible for the node's activation quantization.
+        Returns:
+            QuantizationConfigOptions with edited attributes configurations.
 
         """
-        return [module for m_name, module in named_layers.items()
-                if isinstance(module, PytorchActivationQuantizationHolder) and
-                m_name.replace(f"_{ACTIVATION_HOLDER_QUANTIZER}", '') == n.name]
 
-    def _find_layers_in_model_by_node(self, n: BaseNode, named_layers: Dict[str, torch.nn.Module]) -> \
-            List[Union[PytorchQuantizationWrapper, PytorchActivationQuantizationHolder]]:
+        qc_options = copy.deepcopy(self)
+
+        for qc in qc_options.quantization_config_list:
+            if attrs is None:
+                attrs_to_update = list(qc.attr_weights_configs_mapping.keys())
+            else:
+                if not isinstance(attrs, List):
+                    Logger.critical(f"Expected a list of attributes but received {type(attrs)}.")
+                attrs_to_update = attrs
+
+            for attr in attrs_to_update:
+                if qc.attr_weights_configs_mapping.get(attr) is None:
+                    Logger.critical(f'Editing attributes is only possible for existing attributes in the configuration\'s '
+                                    f'weights config mapping; {attr} does not exist in {qc}.')
+                self.__edit_quantization_configuration(qc.attr_weights_configs_mapping[attr], kwargs)
+        return qc_options
+
+    def clone_and_map_weights_attr_keys(self, layer_attrs_mapping: Union[Dict[str, str], None]):
         """
-        Retries layers from an MP model that are matching to the given graph node, that is, this are either
-        PytorchQuantizationWrapper layers or PytorchActivationQuantizationHolder layers that are responsible for the graph
-        configurable model quantization.
+       Clones the quantization configuration options and edits the keys in each configuration attributes config mapping,
+       based on the given attributes names mapping.
 
         Args:
-            n: A configurable graph node.
-            named_layers: Mixed precision model layers list.
+            layer_attrs_mapping: A mapping between attributes names.
 
-        Returns: A list of layers that responsible for the node's quantization.
+        Returns:
+            QuantizationConfigOptions with edited attributes names.
 
         """
-        weights_quant = n.is_weights_quantization_enabled()
-        act_quant = n.is_activation_quantization_enabled()
+        qc_options = copy.deepcopy(self)
+
+        for qc in qc_options.quantization_config_list:
+            if layer_attrs_mapping is None:
+                qc.attr_weights_configs_mapping = {}
+            else:
+                new_attr_mapping = {}
+                for attr in list(qc.attr_weights_configs_mapping.keys()):
+                    new_key = layer_attrs_mapping.get(attr)
+                    if new_key is None:
+                        Logger.critical(f"Attribute \'{attr}\' does not exist in the provided attribute mapping.")
+
+                    new_attr_mapping[new_key] = qc.attr_weights_configs_mapping.pop(attr)
+
+                qc.attr_weights_configs_mapping.update(new_attr_mapping)
+
+        return qc_options
+
+    def __edit_quantization_configuration(self, qc, kwargs):
+        for k, v in kwargs.items():
+            assert hasattr(qc,
+                           k), (f'Editing is only possible for existing attributes in the configuration; '
+                                f'{k} is not an attribute of {qc}.')
+            setattr(qc, k, v)
+
+    def get_info(self):
+        return {f'option {i}': cfg.get_info() for i, cfg in enumerate(self.quantization_config_list)}
 
-        if weights_quant and not act_quant:
-            return self._get_weights_quant_layers(n, named_layers)
-        elif not weights_quant and act_quant:
-            return self._get_activation_quant_layers(n, named_layers)
-        elif weights_quant and act_quant:
-            return self._get_weights_quant_layers(n, named_layers) + self._get_activation_quant_layers(n, named_layers)
-        else:  # pragma: no cover
-            Logger.error(f"Expects node {n.name} to have at either weights or activation quantization configured,"
-                         f"but both are disabled.")
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,28 +13,29 @@
 # limitations under the License.
 # ==============================================================================
 from abc import abstractmethod
 from functools import partial
 from typing import Tuple, Any, Dict, List, Union, Callable
 
 import torch
+import numpy as np
 from networkx import topological_sort
 
 from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.back2framework.base_model_builder import BaseModelBuilder
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder, identity_wrapper
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, BufferHolder
-from model_compression_toolkit.core.pytorch.utils import get_working_device
-from model_compression_toolkit.core.pytorch.constants import BUFFER
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
+from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from mct_quantizers.common.constants import ACTIVATION_HOLDER_QUANTIZER
 
 
 def _build_input_tensors_list(node: BaseNode,
                               graph: Graph,
                               inputs: Tuple[Any],
                               node_to_output_tensors_dict: Dict[BaseNode, List]) -> List[List]:
@@ -57,17 +58,44 @@
         input_tensors = []
         # Go over a sorted list of the node's incoming edges, and for each source node get its output tensors.
         # Append them in a result list.
         for ie in graph.incoming_edges(node, sort_by_attr=EDGE_SINK_INDEX):
             _input_tensors = node_to_output_tensors_dict[ie.source_node]
             input_tensors.append(_input_tensors)
         input_tensors = [tensor for tensor_list in input_tensors for tensor in tensor_list]  # flat list of lists
+        input_tensors = node.insert_positional_weights_to_input_list(input_tensors)
+        # convert inputs from positional weights (numpy arrays) to tensors. Must handle each element in the
+        # list separately, because in FX the tensors are FX objects and fail to_torch_tensor
+        input_tensors = [to_torch_tensor(t) if isinstance(t, np.ndarray) else t
+                         for t in input_tensors]
     return input_tensors
 
 
+def _merge_inputs(_node, input_tensors: List, op_call_args: List) -> List:
+    """
+    Merge input tensors list with op_call_args, according to correct order
+
+    Args:
+        _node: The node the inputs are for
+        input_tensors: activation input tensors to node.
+        op_call_args: framework node call args
+    Returns:
+        Combined list of input_tensors and op_call_args
+    """
+    if isinstance(_node, FunctionalNode) and _node.tensor_input_indices:
+        assert len(_node.tensor_input_indices) == len(input_tensors), 'Mismatch between input tensors and indices'
+        _input_list = op_call_args.copy()
+        for i, t in zip(_node.tensor_input_indices, input_tensors):
+            _input_list.insert(i, t)
+    else:
+        _input_list = input_tensors + op_call_args
+
+    return _input_list
+
+
 def _run_operation(n: BaseNode,
                    input_tensors: List,
                    op_func: Any,
                    quantize_node_activation_fn,
                    use_activation_quantization: bool) -> Tuple[Union[List, torch.Tensor], Union[List, torch.Tensor]]:
     """
     Applying the layer (op_func) to the input tensors (input_tensors).
@@ -86,15 +114,15 @@
     """
 
     op_call_args = n.op_call_args if isinstance(n, FunctionalNode) else []
     functional_kwargs = n.op_call_kwargs if isinstance(n, FunctionalNode) else {}
     if isinstance(n, FunctionalNode) and n.inputs_as_list:
         out_tensors_of_n_float = op_func(input_tensors, *op_call_args, **functional_kwargs)
     else:
-        out_tensors_of_n_float = op_func(*input_tensors + op_call_args, **functional_kwargs)
+        out_tensors_of_n_float = op_func(*_merge_inputs(n, input_tensors, op_call_args), **functional_kwargs)
 
     # Add a fake quant node if the node has an activation threshold.
     out_tensors_of_n = out_tensors_of_n_float
     if use_activation_quantization:
         if isinstance(out_tensors_of_n_float, list):
             out_tensors_of_n_float = torch.cat(out_tensors_of_n_float, dim=0)
         out_tensors_of_n = quantize_node_activation_fn(out_tensors_of_n_float)
@@ -209,15 +237,15 @@
         """
         if isinstance(node, FunctionalNode):
             if self.wrapper is None:
                 node_op = node.type
             else:
                 node_op = self.wrapper(node, node.type)
         else:
-            if self.wrapper is None or node.type == BufferHolder:
+            if self.wrapper is None:
                 node_op = node_builder(node)
             else:
                 node_op = self.wrapper(node, node_builder(node))
         return node_op
 
     def _add_modules(self):
         """
@@ -226,18 +254,14 @@
         for node in self.node_sort:
             node_op = self.wrap(node)
             if isinstance(node, FunctionalNode):
                 # for functional layers
                 setattr(self, node.name, node_op)
             else:
                 self.add_module(node.name, node_op)
-                if node.type == BufferHolder:
-                    self.get_submodule(node.name). \
-                        register_buffer(node.name,
-                                        torch.Tensor(node.get_weights_by_keys(BUFFER)).to(get_working_device()))
 
             # Add activation quantization modules if an activation holder is configured for this node
             if node.is_activation_quantization_enabled() and self.get_activation_quantizer_holder is not None:
                 activation_quantizer_holder = self.get_activation_quantizer_holder(node)
                 if activation_quantizer_holder is not None:
                     self.add_module(node.name + '_' + ACTIVATION_HOLDER_QUANTIZER, activation_quantizer_holder)
                     self.node_to_activation_quantization_holder.update(
@@ -249,15 +273,15 @@
         Args:
             args: argument input tensors to model.
         Returns:
             torch Tensor/s which is/are the output of the model logic.
         """
         node_to_output_tensors_dict = dict()
         node_to_output_tensors_dict_float = dict()
-        configurable_nodes = self.graph.get_configurable_sorted_nodes_names()
+        configurable_nodes = self.graph.get_configurable_sorted_nodes_names(DEFAULT_PYTORCH_INFO)
         for node in self.node_sort:
             input_tensors = _build_input_tensors_list(node,
                                                       self.graph,
                                                       args,
                                                       node_to_output_tensors_dict)
 
             op_func = self._get_op_func(node, configurable_nodes)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/constants.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,16 +13,14 @@
 # limitations under the License.
 # ==============================================================================
 
 
 # # Layer type constants:
 PLACEHOLDER = 'placeholder'
 OUTPUT = 'output'
-CONSTANT = 'constant'
-BUFFER = 'buffer'
 
 # # Module operation type
 CALL_FUNCTION = 'call_function'
 CALL_METHOD = 'call_method'
 GET_ATTR = 'get_attr'
 
 # # Layers attributes constants:
@@ -38,33 +36,39 @@
 TYPE = 'type'
 PAD = 'pad'
 VALUE = 'value'
 FUNCTIONAL_OP = 'functional_op'
 OP_CALL_ARGS = 'op_call_args'
 OP_CALL_KWARGS = 'op_call_kwargs'
 INPUTS_AS_LIST = 'inputs_as_list'
+TENSOR_INPUT_INDICES = 'tensor_input_indices'
 INPLACE = 'inplace'
 HARDTANH_MIN_VAL = 'min_val'
 HARDTANH_MAX_VAL = 'max_val'
 
 # # Layers variables names:
 KERNEL = 'weight'
 BIAS = 'bias'
 GAMMA = 'weight'
 BETA = 'bias'
+WEIGHT = 'weight'
 MOVING_MEAN = 'running_mean'
 MOVING_VARIANCE = 'running_var'
 EPSILON = 'eps'
 EPSILON_VAL = 1e-5
 MOMENTUM = 'momentum'
 MOMENTUM_VAL = 0.1
+NORMALIZED_SHAPE = 'normalized_shape'
 DIM = 'dim'
 IN_CHANNELS = 'in_channels'
 OUT_CHANNELS = 'out_channels'
 NUM_FEATURES = 'num_features'
+NUM_PARAMETERS = 'num_parameters'
+IN_FEATURES = 'in_features'
+OUT_FEATURES = 'out_features'
 
 # torch devices
 CUDA = 'cuda'
 CPU = 'cpu'
 
 # ReLU bound constants
 RELU_POT_BOUND = 8.0
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/default_framework_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/default_framework_info.py`

 * *Files 9% similar despite different names*

```diff
@@ -13,50 +13,50 @@
 # limitations under the License.
 # ==============================================================================
 from torch.nn import Hardsigmoid, ReLU, ReLU6, Softmax, Sigmoid
 from torch.nn.functional import hardsigmoid, relu, relu6, softmax
 from torch.nn import Conv2d, ConvTranspose2d, Linear
 from torch import sigmoid
 
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.constants import SOFTMAX_THRESHOLD
 from model_compression_toolkit.core.pytorch.constants import KERNEL
 from model_compression_toolkit.core.pytorch.quantizer.fake_quant_builder import power_of_two_quantization, \
     symmetric_quantization, uniform_quantization
 from model_compression_toolkit.core.pytorch.quantizer.lut_fake_quant import activation_lut_kmean_quantizer
 
 """
 Map each layer to a list of its' weights attributes that should get quantized.
 If a layer that is not listed here is queried, [None] is returned.
 """
 KERNEL_ATTRIBUTES = DefaultDict({Conv2d: [KERNEL],
                                  ConvTranspose2d: [KERNEL],
                                  Linear: [KERNEL]},
-                                lambda: [None])
+                                [None])
 
 """
 Map a layer to its kernel's output and input channels indices.
 Map's values are tuples of (output_channel_index, input_channel_index).
 Default value is returned for layers that are not included.
 """
 DEFAULT_CHANNEL_AXIS_DICT = DefaultDict({Conv2d: (0, 1),
                                          Linear: (0, 1),
                                          ConvTranspose2d: (1, 0)},
-                                        lambda: (None, None))
+                                        (None, None))
 
 """
 Map a layer to its output channel axis.
 Where axis=-1 is the last axis
 """
 DEFAULT_OUT_CHANNEL_AXIS_DICT = DefaultDict({Conv2d: 1,
                                              Linear: -1,
                                              ConvTranspose2d: 1},
-                                            lambda: 1)
+                                            1)
 
 
 """
 Map from an activation function to its min/max output values (if known).
 The values are used for tensor min/max values initialization.
 """
 ACTIVATION2MINMAX = {}  # should be an empty dict in Pytorch
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 import numpy as np
 from torch.nn import BatchNorm2d, Conv2d, ConvTranspose2d
 
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.substitutions.batchnorm_folding import BatchNormalizationFolding, BatchNormalizationForwardFolding
 from model_compression_toolkit.core.pytorch.constants import KERNEL, BIAS, GAMMA, BETA, MOVING_MEAN, MOVING_VARIANCE, \
-    EPSILON, USE_BIAS, GROUPS, IN_CHANNELS
+    EPSILON, USE_BIAS, GROUPS, IN_CHANNELS, OUT_CHANNELS
 
 
 def batchnorm_folding_node_matchers() -> [BaseNode, BaseNode]:
     """
     Function generates matchers for matching:
     (Conv2d, ConvTranspose2d)-> BatchNorm2d.
 
@@ -62,15 +62,23 @@
     Returns:
         The modified convolution node's weight/kernel/
     """
     if conv_node.type == ConvTranspose2d:
         _scale = weights_scale[None, :, None, None]
     else:
         _scale = weights_scale[:, None, None, None]
-    return kernel * _scale, KERNEL
+    if conv_node.type == ConvTranspose2d and conv_node.framework_attr[GROUPS] > 1:
+        # PyTorch ConvTranspose2d kernel with groups stacks groups on in_channels axis, so need to reshape the kernel
+        # so the groups are stacked on the out_channels axis to match the scale vector (then reshape back to original
+        # shape)
+        _in_channels = int(conv_node.framework_attr[IN_CHANNELS]/conv_node.framework_attr[GROUPS])
+        _out_channels = conv_node.framework_attr[OUT_CHANNELS]
+        return (kernel.reshape((_in_channels, _out_channels, -1, 1)) * _scale).reshape(kernel.shape), KERNEL
+    else:
+        return kernel * _scale, KERNEL
 
 
 def update_weights_for_bn_forward_folding_fn(conv_node: BaseNode,
                                              kernel: np.ndarray,
                                              bias: np.ndarray,
                                              weights_scale: np.ndarray,
                                              bias_factor: np.ndarray) -> [np.ndarray, np.ndarray, str]:
@@ -81,15 +89,15 @@
         bias: The Convolution node's bias
         weights_scale: Weight scale factor in which to multiply the conv node's weight.
         bias_factor: factor for kernel to update the bias with: (beta - moving_mean * weights_scale)
 
     Returns:
         The modified convolution node's weight/kernel/
     """
-    if conv_node.type == Conv2d and conv_node.framework_attr['groups'] > 1:
+    if conv_node.type == Conv2d and conv_node.framework_attr[GROUPS] > 1:
         bias_update = (kernel * bias_factor[:, None, None, None]).flatten()
         _scale = weights_scale[:, None, None, None]
     elif conv_node.type == ConvTranspose2d:
         bias_update = (kernel * bias_factor[:, None, None, None]).sum(axis=0).flatten()
         _scale = weights_scale[:, None, None, None]
     else:
         bias_update = (kernel * bias_factor[None, :, None, None]).sum(axis=1).flatten()
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py`

 * *Files 2% similar despite different names*

```diff
@@ -97,15 +97,15 @@
             if bias2 is not None:
                 bias_collapsed += bias2
         elif bias2 is not None:
             bias_collapsed = bias2
 
         return kernel_collapsed, bias_collapsed
     else:
-        Logger.error("No supported layer collapsing of {} and {}".format(first_node.type, second_node.type))
+        Logger.critical(f"Layer collapsing is not supported for the combination of {first_node.type} and {second_node.type}.")
 
 
 def pytorch_linear_collapsing() -> Conv2DCollapsing:
     """
     Returns:
         A Conv2DCollapsing initialized for pytorch models.
     """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,27 +43,27 @@
         Args:
             mha_node: MHA node
         """
 
         # Only batch first network is supported
         if BATCH_FIRST in mha_node.framework_attr.keys():
             if mha_node.framework_attr[BATCH_FIRST] is not True:
-                Logger.error('Only batch first network is supported')  # pragma: no cover
+                Logger.critical('Only networks with batch first cofiguration are supported.')  # pragma: no cover
         else:
-            Logger.error('Only batch first network is supported')  # pragma: no cover
+            Logger.critical('Only networks with batch first cofiguration are supported.')  # pragma: no cover
 
         # Add Zero Attn feature is Not Implemented
         if ADD_ZERO_ATTN in mha_node.framework_attr.keys():
             if mha_node.framework_attr[ADD_ZERO_ATTN] is not False:
-                Logger.error('Add Zero Attn feature is Not Implemented')  # pragma: no cover
+                Logger.critical('Add Zero Attention (Add Zero Attn) feature is not implemented.')  # pragma: no cover
 
         # Check if Add Bias KV feature is Active
         if BIAS_K and BIAS_V in mha_node.weights.keys():
             if mha_node.weights[BIAS_K] is not None and mha_node.weights[BIAS_V] is not None:
-                Logger.error('Add BIAS_KV feature is Not Implemented')  # pragma: no cover
+                Logger.critical('Add Bias to Key/Value (BIAS_KV) feature is not implemented.')  # pragma: no cover
 
         self.embed_dim = mha_node.framework_attr[EMBED_DIM]
         self.num_heads = mha_node.framework_attr[NUM_HEADS]
 
         self.kdim = mha_node.framework_attr[KEY_DIM]
 
         self.vdim = mha_node.framework_attr[VALUE_DIM]
@@ -698,15 +698,15 @@
             graph: input graph
             mha_node: MHA node to substitute inputs and outputs with
         Returns:
             Graph after applying the substitution.
         """
 
         if mha_node.reuse:
-            raise Exception("MCT doesn't support reuse of MultiHeadAttention layer")  # pragma: no cover
+            Logger.critical("Reuse of MultiHeadAttention layers is currently not supported.")  # pragma: no cover
         params = MHAParams(mha_node)
 
         # project
         # (B, q_seq, q_dim*n_h) --> (B, q_dim*n_h, q_seq)
         # (B, kv_seq, k_dim) --> (B, q_dim*n_h, kv_seq)
         # (B, kv_seq, v_dim) --> (B, q_dim*n_h, kv_seq)
         q_transpose_node, k_transpose_node, v_transpose_node, \
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py`

 * *Files 2% similar despite different names*

```diff
@@ -99,15 +99,15 @@
                     (np.log2(non_linear_node.framework_attr[HARDTANH_MAX_VAL]).astype(int) -
                      np.log2(non_linear_node.framework_attr[HARDTANH_MAX_VAL]) == 0):
                 scale_factor = non_linear_node.framework_attr[HARDTANH_MAX_VAL] / self.threshold
                 non_linear_node.functional_op.__defaults__ = (0.0, self.threshold, non_linear_node.framework_attr[INPLACE])
             else:
                 return graph
         else:
-            Logger.error(f"In substitution with wrong matched pattern")
+            Logger.critical(f"Encountered an unexpected non-linearity type not supported for this substitution: {non_linear_node.type}.")
         Logger.debug(
             f"Node named:{non_linear_node.name} changed "
             f"to:{non_linear_node.type}")
 
         w2_fixed = scale_factor * second_op2d_node.get_weights_by_keys(KERNEL)
         w1_fixed = first_op2d_node.get_weights_by_keys(KERNEL) / scale_factor
         b1_fixed = first_op2d_node.get_weights_by_keys(BIAS) / scale_factor
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py`

 * *Files 15% similar despite different names*

```diff
@@ -35,34 +35,45 @@
         nodes = NodeOperationMatcher(reshape) | NodeOperationMatcher(torch.Tensor.view)
         super().__init__(matcher_instance=nodes)
 
     def substitute(self,
                    graph: Graph,
                    node: BaseNode) -> Graph:
         """
-        Replaces the 'size' attribute to 'reshape' or 'view' operators to be a list of integers,
+        Replaces the 'size' attribute for 'reshape' or 'view' operators to be a list of integers,
         determined by their intended output shape. This replaces 'size' attributes that come from
         nodes in the graph. We delete nodes for which that was their sole purpose.
 
         Args:
             graph: Graph we apply the substitution on.
             node: node that match the pattern in the substitution init.
 
         Returns:
             Graph after applying the substitution.
         """
+        # skip substitution if the reshape is applied to weights (not activations). In this case, the node is the first source and doesn't have an input shape.
+        if len(node.input_shape) == 0:
+            return graph
+
         # we want the batch size value to infer from the length of the array and remaining dimensions
         if len(node.output_shape) == 1:
             node.output_shape[0][0] = BATCH_DIM_VALUE
         else:
-            Logger.error('Reshape or view nodes should have a single output shape')  # pragma: no cover
+            Logger.critical("This substitution handles 'reshape' or 'view' nodes with a single output shape.")  # pragma: no cover
 
         # configure the new static output shape attribute
         node.op_call_args = node.output_shape
 
+        # When a "reshape" is called with multiple arguments (e.g. x.reshape(-1, channels, height, width)
+        # this substitution converts it x.reshape((-1, channels, height, width)), so need to update the
+        # tensor_input_indices attribute.
+        # scalar argument's shape is [1] so remove those indices from tensor_input_indices
+        # node.input_shape example: [[1, 32, 4, 32], [1], [1], [1]]
+        node.tensor_input_indices = node.tensor_input_indices[:sum([i != [1] for i in node.input_shape])]
+
         # modify the node input info
         node.input_shape = [node.input_shape[0]]
 
         # the first input is the tensor to be reshaped, we want his batch size value to infer
         # from the length of the array and remaining dimensions
         node.input_shape[0][0] = BATCH_DIM_VALUE
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py`

 * *Files 3% similar despite different names*

```diff
@@ -54,15 +54,15 @@
         # Collapsing residual by adding "1" to kernel diagonal
         idxH = (kH - 1) // 2
         idxW = (kW - 1) // 2
         for i in range(Cout):
             kernel[i, i, idxH, idxW] += 1
         return kernel
     else:
-        Logger.error("No supported add residual collapsing for {}".format(first_node.type))
+        Logger.critical(f"Residual collapsing not supported for node type: {first_node.type}")
 
 
 def pytorch_residual_collapsing() -> ResidualCollapsing:
     """
     Returns:
         A ResidualCollapsing initialized for pytorch models.
     """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/kpi_data_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/keras/quantization_facade.py`

 * *Files 15% similar despite different names*

```diff
@@ -11,152 +11,150 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.core.analyzer import analyzer_model_quantization
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.constants import PYTORCH
-from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_data import compute_kpi_data
-from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
+from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfig, DEFAULT_MIXEDPRECISION_CONFIG, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.constants import FOUND_TORCH
-
-if FOUND_TORCH:
-    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
+    MixedPrecisionQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.core.exporter import export_model
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+
+if FOUND_TF:
+    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
+    from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
+    from tensorflow.keras.models import Model
     from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from torch.nn import Module
+    from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
 
     from model_compression_toolkit import get_target_platform_capabilities
-
-    PYTORCH_DEFAULT_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
+    DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
-    def pytorch_kpi_data(in_model: Module,
-                         representative_data_gen: Callable,
-                         quant_config: MixedPrecisionQuantizationConfig = DEFAULT_MIXEDPRECISION_CONFIG,
-                         fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                         target_platform_capabilities: TargetPlatformCapabilities = PYTORCH_DEFAULT_TPC) -> KPI:
+    def keras_post_training_quantization(in_model: Model,
+                                         representative_data_gen: Callable,
+                                         target_resource_utilization: ResourceUtilization = None,
+                                         core_config: CoreConfig = CoreConfig(),
+                                         target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC):
         """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and target platform capabilities, and uses it to compute the KPI data.
+         Quantize a trained Keras model using post-training quantization. The model is quantized using a
+         symmetric constraint quantization thresholds (power of two).
+         The model is first optimized using several transformations (e.g. BatchNormalization folding to
+         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+         being collected for each layer's output (and input, depends on the quantization configuration).
+         For each possible bit width (per layer) a threshold is then being calculated using the collected
+         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
+         a mixed-precision configuration, and set a bit-width for each layer. The model is then quantized
+         (both coefficients and activations by default).
+         In order to limit the maximal model's size, a target ResourceUtilization need to be passed after weights_memory
+         is set (in bytes).
+
+         Args:
+             in_model (Model): Keras model to quantize.
+             representative_data_gen (Callable): Dataset used for calibration.
+             target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+
+         Returns:
+
+             A quantized model and information the user may need to handle the quantized model.
 
-        Args:
-            in_model (Model): PyTorch model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            quant_config (MixedPrecisionQuantizationConfig): MixedPrecisionQuantizationConfig containing parameters of how the model should be quantized.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+         Examples:
 
-        Returns:
-            A KPI object with total weights parameters sum, max activation tensor and total kpi.
+            Import MCT:
 
-        Examples:
+            >>> import model_compression_toolkit as mct
 
-            Import a Pytorch model:
+            Import a Keras model:
 
-            >>> from torchvision import models
-            >>> module = models.mobilenet_v2()
+            >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
+            >>> model = MobileNetV2()
 
             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
             In this example a random dataset of 10 batches each containing 4 images is used.
 
             >>> import numpy as np
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 3, 224, 224))]
+            >>>         yield [np.random.random((4, 224, 224, 3))]
 
-            Import mct and call for KPI data calculation:
+            Create a MCT core config, containing the quantization configuration:
 
-            >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.pytorch_kpi_data(module, repr_datagen)
+            >>> config = mct.core.CoreConfig()
 
-        """
+            If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
+            The candidates bitwidth for quantization should be defined in the target platform model.
+            In this example we use 1 image to search mixed-precision configuration:
 
-        if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfig object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfig.")
-
-        fw_impl = PytorchImplementation()
-
-        quantization_config, mp_config = quant_config.separate_configs()
-        core_config = CoreConfig(quantization_config=quantization_config,
-                                 mixed_precision_config=mp_config)
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
-
-
-    def pytorch_kpi_data_experimental(in_model: Module,
-                                      representative_data_gen: Callable,
-                                      core_config: CoreConfig = CoreConfig(),
-                                      fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                                      target_platform_capabilities: TargetPlatformCapabilities = PYTORCH_DEFAULT_TPC) -> KPI:
-        """
-        Computes KPI data that can be used to calculate the desired target KPI for mixed-precision quantization.
-        Builds the computation graph from the given model and target platform capabilities, and uses it to compute the KPI data.
+            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=1))
 
-        Args:
-            in_model (Model): PyTorch model to quantize.
-            representative_data_gen (Callable): Dataset used for calibration.
-            core_config (CoreConfig): CoreConfig containing parameters for quantization and mixed precision
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
+            For mixed-precision set a target ResourceUtilization object:
+            Create a ResourceUtilization object to limit our returned model's size. Note that this value affects only coefficients
+            that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
+            while the bias will not):
 
-        Returns:
+            >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
-            A KPI object with total weights parameters sum and max activation tensor.
+            Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+            quantized model:
 
-        Examples:
+            >>> quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(model, repr_datagen, ru, core_config=config)
 
-            Import a Pytorch model:
+            For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
 
-            >>> from torchvision import models
-            >>> module = models.mobilenet_v2()
+         """
 
-            Create a random dataset generator:
+        fw_info = DEFAULT_KERAS_INFO
 
-            >>> import numpy as np
-            >>> def repr_datagen(): yield [np.random.random((1, 3, 224, 224))]
+        KerasModelValidation(model=in_model,
+                             fw_info=fw_info).validate()
 
-            Import mct and call for KPI data calculation:
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                                    "MixedPrecisionQuantizationConfig. Please use keras_post_training_quantization "
+                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
 
-            >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.core.pytorch_kpi_data(module, repr_datagen)
+        tb_w = init_tensorboard_writer(fw_info)
 
-        """
+        fw_impl = KerasImplementation()
 
-        if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-            Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfigV2 object."
-                         "Given quant_config is not of type MixedPrecisionQuantizationConfigV2.")
-
-        fw_impl = PytorchImplementation()
-
-        return compute_kpi_data(in_model,
-                                representative_data_gen,
-                                core_config,
-                                target_platform_capabilities,
-                                fw_info,
-                                fw_impl)
+        # Ignore returned hessian service as PTQ does not use it
+        tg, bit_widths_config, _ = core_runner(in_model=in_model,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=fw_info,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
 
-else:
-    # If torch is not installed,
-    # we raise an exception when trying to use this function.
-    def pytorch_kpi_data(*args, **kwargs):
-        Logger.critical('Installing torch is mandatory when using pytorch_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
 
+        if core_config.debug_config.analyze_similarity:
+            analyzer_model_quantization(representative_data_gen,
+                                        tb_w, tg,
+                                        fw_impl,
+                                        fw_info)
 
-    def pytorch_kpi_data_experimental(*args, **kwargs):
-        Logger.critical('Installing torch is mandatory when using pytorch_kpi_data. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        return get_exportable_keras_model(tg)
+
+
+
+else:
+    # If tensorflow is not installed,
+    # we raise an exception when trying to use these functions.
+    def keras_post_training_quantization(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_post_training_quantization. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_activation_quantizer.py`

 * *Files 18% similar despite different names*

```diff
@@ -40,34 +40,36 @@
     It holds a set of activation quantizers for each of the given bit-width candidates, provided by the
     node's quantization config. This allows to use different quantized activations on-the-fly, according to the
     "active" quantization configuration index.
     """
 
     def __init__(self,
                  node_q_cfg: List[CandidateNodeQuantizationConfig],
-                 max_candidate_idx: int = 0):
+                 max_candidate_idx: int = 0,
+                 kernel_attr: str = None):
         """
         Initializes a configurable quantizer.
 
         Args:
             node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
                 use this quantizer.
             max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
+            kernel_attr: A kernel attribute name if the node have a kernel attribute (used only for candidates order validation).
         """
 
         super(ConfigurableActivationQuantizer, self).__init__()
 
         self.node_q_cfg = node_q_cfg
 
-        verify_candidates_descending_order(self.node_q_cfg)
+        verify_candidates_descending_order(self.node_q_cfg, kernel_attr)
 
         for qc in self.node_q_cfg:
             if qc.activation_quantization_cfg.enable_activation_quantization != \
                    self.node_q_cfg[0].activation_quantization_cfg.enable_activation_quantization:
-                Logger.error("Candidates with different activation enabled properties is currently not supported.")  # pragma: no cover
+                Logger.critical("Unsupported configuration: Mixing candidates with differing activation quantization states (enabled/disabled).")  # pragma: no cover
 
         # Setting layer's activation
         self.activation_quantizers = init_activation_quantizers(self.node_q_cfg)
         self.active_quantization_config_index = max_candidate_idx  # initialize with first config as default
 
     def set_active_activation_quantizer(self,
                                         index: int):
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/mixed_precision/configurable_weights_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/mixed_precision/configurable_weights_quantizer.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,112 +8,127 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from functools import partial
 from typing import Dict, Any, List
 
-from model_compression_toolkit.core.common.mixed_precision.configurable_quant_id import ConfigurableQuantizerIdentifier
 from model_compression_toolkit.core.common.mixed_precision.configurable_quantizer_utils import \
     verify_candidates_descending_order, init_quantized_weights
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from mct_quantizers import QuantizationTarget
-
 from mct_quantizers import mark_quantizer
 
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
-import torch
-import torch.nn as nn
-from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
+import tensorflow as tf
+from model_compression_toolkit.core.common.mixed_precision.configurable_quant_id import \
+    ConfigurableQuantizerIdentifier
+from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
 
 
 @mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC,
                                      QuantizationMethod.UNIFORM, QuantizationMethod.LUT_POT_QUANTIZER,
                                      QuantizationMethod.LUT_SYM_QUANTIZER],
                 identifier=ConfigurableQuantizerIdentifier.CONFIGURABLE_ID)
-class ConfigurableWeightsQuantizer(BasePyTorchInferableQuantizer):
+class ConfigurableWeightsQuantizer(BaseKerasInferableQuantizer):
     """
-    Configurable weights quantizer for Pytorch mixed precision search.
+    Configurable weights quantizer for Keras mixed precision search.
     The quantizer holds a set of quantized layer's weights for each of the given bit-width candidates, provided by the
     node's quantization config. This allows to use different quantized weights on-the-fly.
 
     The general idea behind this kind of quantizer is that it gets the float tensor to quantize
     when initialized, it quantizes the float tensor in different bitwidths, and every time it need to return a
     quantized version of the float weight, it returns only one quantized weight according to an "active"
     index - the index of a candidate weight quantization configuration from a list of candidates that was passed
     to the quantizer when it was initialized.
     """
 
     def __init__(self,
                  node_q_cfg: List[CandidateNodeQuantizationConfig],
-                 float_weights: torch.Tensor,
+                 float_weights: tf.Tensor,
+                 kernel_attr: str,
                  max_candidate_idx: int = 0):
         """
         Initializes a configurable quantizer.
 
         Args:
             node_q_cfg: Quantization configuration candidates of the node that generated the layer that will
                 use this quantizer.
             float_weights: Float weights of the layer.
+            kernel_attr: The kernel attribute name of the node. Only layers with kernel op can be configured.
             max_candidate_idx: Index of the node's candidate that has the maximal bitwidth (must exist absolute max).
+
         """
 
         super(ConfigurableWeightsQuantizer, self).__init__()
 
         self.node_q_cfg = node_q_cfg
         self.float_weights = float_weights
         self.max_candidate_idx = max_candidate_idx
+        self.kernel_attr = kernel_attr
 
-        verify_candidates_descending_order(self.node_q_cfg)
+        verify_candidates_descending_order(self.node_q_cfg, kernel_attr)
 
         for qc in self.node_q_cfg:
-            if qc.weights_quantization_cfg.enable_weights_quantization != \
-                   self.node_q_cfg[0].weights_quantization_cfg.enable_weights_quantization:
-                Logger.error("Candidates with different weights enabled properties is currently not supported.")  # pragma: no cover
+            if qc.weights_quantization_cfg.get_attr_config(self.kernel_attr).enable_weights_quantization != \
+                    self.node_q_cfg[0].weights_quantization_cfg.get_attr_config(self.kernel_attr).enable_weights_quantization:
+                Logger.critical("Mixing candidates with varying weights quantization states (enabled/disabled) is not supported.")
 
         # Initialize quantized weights for each weight that should be quantized.
         self.quantized_weights = init_quantized_weights(node_q_cfg=self.node_q_cfg,
                                                         float_weights=self.float_weights,
-                                                        fw_tensor_convert_func=to_torch_tensor)
+                                                        fw_tensor_convert_func=partial(tf.convert_to_tensor,
+                                                                                       dtype=tf.float32),
+                                                        kernel_attr=self.kernel_attr)
 
         self.active_quantization_config_index = self.max_candidate_idx
 
     def set_weights_bit_width_index(self,
                                     index: int):
         """
         Change the "active" bitwidth index the configurable quantizer uses, so a different quantized weight
         will be used.
 
         Args:
             index: Quantization configuration candidate index to use.
 
         """
 
-        assert index < len(self.node_q_cfg), \
-            f'Quantizer has {len(self.node_q_cfg)} ' \
-            f'possible nbits. Can not set index {index}'
+        if index >= len(self.node_q_cfg):
+            Logger.critical(f'Quantizer supports only {len(self.node_q_cfg)} bit width configurations; index {index} is out of range.')# pragma: no cover
         self.active_quantization_config_index = index
 
     def __call__(self,
-                 inputs: nn.Parameter) -> torch.Tensor:
+                 inputs: tf.Tensor) -> tf.Tensor:
         """
         Method to return the quantized weight. This method is called when the framework needs to quantize a
-            float weight, and is expected to return the quantized weight. Since we already quantized the weight in
-            all possible bitwidths, we do not quantize it again, and simply return the quantized weight according
-            to the current active_quantization_config_index.
+        float weight, and is expected to return the quantized weight. Since we already quantized the weight in
+        all possible bitwidths, we do not quantize it again, and simply return the quantized weight according
+        to the current active_quantization_config_index.
 
         Args:
-            inputs: Input tensor (not used in this function since the weights are already quantized).
+            inputs: Input tensor (not relevant since the weights are already quantized).
 
         Returns:
             Quantized weight, that was quantized using number of bits that is in a
-                specific quantization configuration candidate (the candidate's index is the
-                index that is in active_quantization_config_index the quantizer holds).
+            specific quantization configuration candidate (the candidate's index is the
+            index that is in active_quantization_config_index the quantizer holds).
         """
 
         return self.quantized_weights[self.active_quantization_config_index]
+
+    def get_config(self) -> Dict[str, Any]:  # pragma: no cover
+        """
+        Returns: The ConfigurableWeightsQuantizer configuration.
+        """
+
+        return {
+            'float_weights': self.float_weights,
+            'node_q_cfg': self.node_q_cfg,
+            'active_quantization_config_index': self.active_quantization_config_index
+        }
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/pytorch_implementation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pytorch_implementation.py`

 * *Files 20% similar despite different names*

```diff
@@ -21,45 +21,45 @@
 import torch
 from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
 from torch import sigmoid, softmax, add, cat, argmax
 from torch.nn import Conv2d, ConvTranspose2d, Linear
 from torch.nn import Module, Sigmoid, Softmax
 
 import model_compression_toolkit.core.pytorch.constants as pytorch_constants
-from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.constants import HESSIAN_NUM_ITERATIONS
+from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
-from model_compression_toolkit.core.common.collectors.statistics_collector_generator import \
-    create_stats_collector_for_node
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.common.hessian import TraceHessianRequest, HessianMode, HessianInfoService
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 from model_compression_toolkit.core.common.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.common.similarity_analyzer import compute_mse, compute_kl_divergence, compute_cs
-from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework import get_pytorch_model_builder
-from model_compression_toolkit.core.pytorch.back2framework.model_gradients import \
-    pytorch_iterative_approx_jacobian_trace
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_folding import \
     pytorch_batchnorm_folding, pytorch_batchnorm_forward_folding
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_reconstruction import \
     pytorch_batchnorm_reconstruction
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.batchnorm_refusing import \
     pytorch_batchnorm_refusing
+from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.functional_batch_norm import \
+    FunctionalBatchNorm
+from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.functional_layer_norm import \
+    FunctionalLayerNorm
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.linear_collapsing import \
     pytorch_linear_collapsing
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.multi_head_attention_decomposition \
     import MultiHeadAttentionDecomposition
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.permute_call_method import \
     PermuteCallMethod
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.const_holder_conv import \
-    ConstantHolderConv
+    FunctionalConvSubstitution
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.relu_bound_to_power_of_2 import \
     ReLUBoundToPowerOfTwo
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.reshape_with_static_shapes import \
     ReshapeWithStaticShapes
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.residual_collapsing import \
     pytorch_residual_collapsing
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.scale_equalization import \
@@ -69,23 +69,32 @@
     pytorch_apply_shift_negative_correction
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.softmax_shift import \
     pytorch_softmax_shift
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.virtual_activation_weights_composition import \
     VirtualActivationWeightsComposition
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.weights_activation_split import \
     WeightsActivationSplit
+from model_compression_toolkit.core.pytorch.hessian.activation_trace_hessian_calculator_pytorch import \
+    ActivationTraceHessianCalculatorPytorch
+from model_compression_toolkit.core.pytorch.hessian.weights_trace_hessian_calculator_pytorch import \
+    WeightsTraceHessianCalculatorPytorch
 from model_compression_toolkit.core.pytorch.mixed_precision.configurable_activation_quantizer import \
     ConfigurableActivationQuantizer
 from model_compression_toolkit.core.pytorch.mixed_precision.configurable_weights_quantizer import \
     ConfigurableWeightsQuantizer
 from model_compression_toolkit.core.pytorch.pytorch_node_prior_info import create_node_prior_info
 from model_compression_toolkit.core.pytorch.reader.reader import model_reader
 from model_compression_toolkit.core.pytorch.statistics_correction.apply_second_moment_correction import \
     pytorch_apply_second_moment_correction
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy, set_model
+from model_compression_toolkit.exporter.model_wrapper.fw_agnostic.get_inferable_quantizers import \
+    get_inferable_quantizers
+from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
+    get_weights_quantizer_for_node, get_activations_quantizer_for_node
+from model_compression_toolkit.logger import Logger
 
 
 class PytorchImplementation(FrameworkImplementation):
     """
     A class with implemented methods to support optimizing Pytorch models.
     """
 
@@ -192,27 +201,14 @@
         Returns:
             Graph after SNC.
         """
         return pytorch_apply_shift_negative_correction(graph,
                                                        core_config,
                                                        fw_info)
 
-    def attach_sc_to_node(self,
-                          node: BaseNode,
-                          fw_info: FrameworkInfo) -> BaseStatsCollector:
-        """
-        Return a statistics collector that should be attached to a node's output
-        during statistics collection.
-        Args:
-            node: Node to return its collector.
-            fw_info: Information relevant to a specific framework about what is out channel axis (for statistics per-channel)
-        Returns:
-            Statistics collector for the node.
-        """
-        return create_stats_collector_for_node(node, fw_info)
 
     def get_substitutions_channel_equalization(self,
                                                quant_config: QuantizationConfig,
                                                fw_info: FrameworkInfo) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used for channel equalization.
 
@@ -234,15 +230,17 @@
 
         Returns: A list of the framework substitutions used before we collect the prior information.
 
         """
         return [ReshapeWithStaticShapes(),
                 MultiHeadAttentionDecomposition(),
                 PermuteCallMethod(),
-                ConstantHolderConv(fw_info)]
+                FunctionalConvSubstitution(fw_info),
+                FunctionalBatchNorm(),
+                FunctionalLayerNorm()]
 
     def get_substitutions_pre_statistics_collection(self,
                                                     quant_config: QuantizationConfig
                                                     ) -> List[common.BaseSubstitution]:
         """
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
@@ -280,28 +278,34 @@
 
     def get_linear_collapsing_substitution(self) -> common.BaseSubstitution:
         """
         Returns: linear collapsing substitution
         """
         return pytorch_linear_collapsing()
 
+    def get_op2d_add_const_collapsing_substitution(self) -> common.BaseSubstitution:
+        """
+        Returns: None, as Op2d add-const substitution is not supported in torch yet
+        """
+        return None
+
     def get_substitutions_post_statistics_collection(self,
                                                      quant_config: QuantizationConfig) -> List[common.BaseSubstitution]:
         """
         Return a list of the framework substitutions used after we collect statistics.
         Args:
             quant_config: QuantizationConfig to determine which substitutions to return.
         Returns:
             A list of the framework substitutions used after we collect statistics.
         """
         substitutions_list = []
         if quant_config.softmax_shift:
             substitutions_list.append(pytorch_softmax_shift())
         if quant_config.input_scaling:
-            raise Exception('Input scaling is currently not supported for Pytorch.')
+            Logger.critical('Input scaling is currently not supported for Pytorch.')
         return substitutions_list
 
     def get_substitutions_pre_build(self) -> List[common.BaseSubstitution]:
         """
         Returns: A list of the framework substitutions used before we build a quantized module.
         """
         return []
@@ -328,28 +332,31 @@
         substitutions_list = []
         if quant_config.weights_second_moment_correction:
             substitutions_list.append(pytorch_batchnorm_refusing())
         return substitutions_list
 
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
-                                  quant_config: MixedPrecisionQuantizationConfigV2,
+                                  quant_config: MixedPrecisionQuantizationConfig,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
-                                  disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
+                                  disable_activation_for_metric: bool = False,
+                                  hessian_info_service: HessianInfoService = None
+                                  ) -> SensitivityEvaluation:
         """
         Creates and returns an object which handles the computation of a sensitivity metric for a mixed-precision
         configuration (comparing to the float model).
 
         Args:
             graph: Graph to build its float and mixed-precision models.
             quant_config: QuantizationConfig of how the model should be quantized.
             representative_data_gen: Dataset to use for retrieving images for the models inputs.
             fw_info: FrameworkInfo object with information about the specific framework's model.
             disable_activation_for_metric: Whether to disable activation quantization when computing the MP metric.
+            hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
 
         Returns:
             A SensitivityEvaluation object.
         """
 
         return SensitivityEvaluation(graph=graph,
                                      quant_config=quant_config,
@@ -357,15 +364,16 @@
                                      fw_info=fw_info,
                                      fw_impl=self,
                                      set_layer_to_bitwidth=partial(set_layer_to_bitwidth,
                                                                    weights_quantizer_type=ConfigurableWeightsQuantizer,
                                                                    activation_quantizer_type=ConfigurableActivationQuantizer,
                                                                    weights_quant_layer_type=PytorchQuantizationWrapper,
                                                                    activation_quant_layer_type=PytorchActivationQuantizationHolder),
-                                     disable_activation_for_metric=disable_activation_for_metric)
+                                     disable_activation_for_metric=disable_activation_for_metric,
+                                     hessian_info_service=hessian_info_service)
 
     def get_node_prior_info(self,
                             node: BaseNode,
                             fw_info: FrameworkInfo,
                             graph: Graph) -> NodePriorInfo:
         """
         Get a NodePriorInfo object for a node that represents a Pytorch layer.
@@ -392,82 +400,51 @@
         if node.type in [Conv2d, Linear, ConvTranspose2d, Sigmoid, sigmoid, Softmax, softmax, operator.add, add, cat,
                          operator.concat]:
             return True
         return False
 
     def get_node_distance_fn(self, layer_class: type,
                              framework_attrs: Dict[str, Any],
-                             compute_distance_fn: Callable = None) -> Callable:
+                             compute_distance_fn: Callable = None,
+                             axis: int = None) -> Callable:
         """
         A mapping between layers' types and a distance function for computing the distance between
         two tensors (for loss computation purposes). Returns a specific function if node of specific types is
         given, or a default (normalized MSE) function otherwise.
 
         Args:
             layer_class: Class path of a model's layer.
             framework_attrs: Framework attributes the layer had which the graph node holds.
             compute_distance_fn: An optional distance function to use globally for all nodes.
+            axis: The axis on which the operation is preformed (if specified).
 
         Returns: A distance function between two tensors.
         """
 
         if compute_distance_fn is not None:
             return compute_distance_fn
 
-        elif layer_class in [Softmax, softmax]:
+        elif layer_class in [Softmax, softmax] and axis is not None:
             return compute_kl_divergence
         elif layer_class in [Sigmoid, sigmoid]:
             return compute_cs
         elif layer_class == Linear:
             return compute_cs
         return compute_mse
 
-    def model_grad(self,
-                   graph_float: common.Graph,
-                   model_input_tensors: Dict[BaseNode, torch.Tensor],
-                   interest_points: List[BaseNode],
-                   output_list: List[BaseNode],  # dummy - not used in pytorch
-                   all_outputs_indices: List[int],
-                   alpha: float = 0.3,
-                   n_iter: int = 50,
-                   norm_weights: bool = True) -> List[float]:
-        """
-        Calls a PyTorch specific model gradient calculation function, which computes the  jacobian-based weights of the model's
-        outputs with respect to the feature maps of the set of given interest points.
-
-        Args:
-            graph_float: Graph to build its corresponding Keras model.
-            model_input_tensors: A mapping between model input nodes to an input batch.
-            interest_points: List of nodes which we want to get their feature map as output, to calculate distance metric.
-            output_list: List of nodes that considered as model's output for the purpose of gradients computation.
-            all_outputs_indices: Indices of the model outputs and outputs replacements (if exists),
-                in a topological sorted interest points list.
-            alpha: A tuning parameter to allow calibration between the contribution of the output feature maps returned
-                weights and the other feature maps weights (since the gradient of the output layers does not provide a
-                compatible weight for the distance metric computation).
-            n_iter: The number of random iterations to calculate the approximated  jacobian-based weights for each interest point.
-            norm_weights: Whether to normalize the returned weights (to get values between 0 and 1).
-
-        Returns: A list of (possibly normalized) jacobian-based weights to be considered as the relevancy that each interest
-        point's output has on the model's output.
+    def is_output_node_compatible_for_hessian_score_computation(self,
+                                                                node: BaseNode) -> bool:
         """
+        Checks and returns whether the given node is compatible as output for Hessian-based information computation.
 
-        return pytorch_iterative_approx_jacobian_trace(graph_float, model_input_tensors, interest_points, output_list,
-                                                       all_outputs_indices, alpha, n_iter, norm_weights=norm_weights)
-
-    def is_node_compatible_for_metric_outputs(self,
-                                              node: BaseNode) -> bool:
-        """
-        Checks and returns whether the given node is compatible as output for metric computation
-        purposes and gradient-based weights calculation.
 
         Args:
             node: A BaseNode object.
 
-        Returns: Whether the node is compatible as output for metric computation or not.
+        Returns: Whether the node is compatible as output for Hessian-based information computation.
 
         """
 
         return node.layer_class not in [argmax, softmax, Softmax]
 
     def get_node_mac_operations(self,
                                 node: BaseNode,
@@ -529,8 +506,55 @@
             model: A Pytorch model to run inference for.
             inputs: Input tensors to run inference on.
 
         Returns:
             The output of the model inference on the given input.
         """
 
-        return model(*inputs)
+        return model(*inputs)
+
+    def get_trace_hessian_calculator(self,
+                                     graph: Graph,
+                                     input_images: List[Any],
+                                     trace_hessian_request: TraceHessianRequest,
+                                     num_iterations_for_approximation: int = HESSIAN_NUM_ITERATIONS):
+        """
+        Get Pytorch trace hessian approximations calculator based on the trace hessian request.
+        Args:
+            input_images: Images to use for computation.
+            graph: Float graph to compute the approximation of its different nodes.
+            trace_hessian_request: TraceHessianRequest to search for the desired calculator.
+            num_iterations_for_approximation: Number of iterations to use when approximating the Hessian trace.
+
+        Returns: TraceHessianCalculatorPytorch to use for the trace hessian approximation computation for this request.
+
+        """
+        if trace_hessian_request.mode == HessianMode.ACTIVATION:
+            return ActivationTraceHessianCalculatorPytorch(graph=graph,
+                                                           trace_hessian_request=trace_hessian_request,
+                                                           input_images=input_images,
+                                                           fw_impl=self,
+                                                           num_iterations_for_approximation=num_iterations_for_approximation)
+        elif trace_hessian_request.mode == HessianMode.WEIGHTS:
+            return WeightsTraceHessianCalculatorPytorch(graph=graph,
+                                                        trace_hessian_request=trace_hessian_request,
+                                                        input_images=input_images,
+                                                        fw_impl=self,
+                                                        num_iterations_for_approximation=num_iterations_for_approximation)
+
+    def get_inferable_quantizers(self, node: BaseNode):
+        """
+        Returns sets of Pytorch compatible weights and activation quantizers for the given node.
+
+        Args:
+           node: Node to get quantizers for.
+
+        Returns:
+            weight_quantizers: A dictionary between a weight's name to its quantizer.
+            activation_quantizers: A list of activations quantization, one for each layer output.
+
+        """
+
+        return get_inferable_quantizers(node,
+                                        get_weights_quantizer_for_node,
+                                        get_activations_quantizer_for_node,
+                                        node.get_node_weights_attributes())
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,81 +1,89 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable
-import torch
 
+
+from typing import Tuple, Callable
+
+import tensorflow as tf
+import numpy as np
+from tensorflow.python.util.object_identity import Reference as TFReference
+
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import threshold_is_power_of_two
-from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
 
 
-def get_symmetric_quantization_range_and_scale(activation_is_signed: bool,
-                                               activation_n_bits: int,
-                                               activation_threshold: float):
+def quantizer_min_max_calculator(threshold: np.ndarray,
+                                 num_bits: int,
+                                 signed: bool) -> Tuple[float, float]:
     """
-    Calculates lower and upper bounds on the quantization range, along with quantization scale,
-    for symmetric quantization (used for the symmetric and power-of-two quantizers),
-    according to whether the quantization is signed or unsigned.
+    Compute quantization range's min/max values given a threshold, number of bits,
+     and whether it's signed or not.
 
     Args:
-        activation_is_signed: Whether the quantization is signed or not.
-        activation_n_bits: Number of bits to use for quantization.
-        activation_threshold: The quantization threshold.
-
-    Returns: range lower bound, range upper bound and quantization scale.
+        threshold: Threshold for quantization range values.
+        num_bits: Number of bits to use for quantization.
+        signed: Whether the quantization range should include negative values or not.
 
+    Returns:
+        Min and max values for quantization range.
     """
-    if activation_is_signed:
-        min_value = -2 ** (activation_n_bits - 1)
-        max_value = 2 ** (activation_n_bits - 1) - 1
-        scale = activation_threshold / 2 ** (activation_n_bits - 1)
+
+    if signed:
+        delta = threshold / (2 ** (num_bits - 1))
+        min_value = -threshold
     else:
+        delta = threshold / (2 ** (num_bits))
         min_value = 0
-        max_value = (2 ** activation_n_bits) - 1
-        scale = activation_threshold / 2 ** activation_n_bits
 
-    return min_value, max_value, scale
+    max_value = threshold - delta
+    return min_value, max_value
 
 
 def power_of_two_quantization(activation_n_bits: int,
                               quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
-    build and return a fake-quantization node, quantized with a power-of-two threshold.
+    build and return a fake-quantization node with power-of-two quantization.
+
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
+
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None or activation_is_signed is None:
-        return None # pragma: no cover
+    if activation_threshold is None:
+        Logger.critical("Activation threshold must be specified.")  # pragma: no cover
+    if activation_is_signed is None:
+        Logger.critical("Parameter 'activation_is_signed' must be specified.")  # pragma: no cover
     if not threshold_is_power_of_two(activation_threshold, per_channel=False):
-        return None # pragma: no cover
+        Logger.critical("Activation threshold must be a power of two.")  # pragma: no cover
 
-    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
-                                                                             activation_n_bits,
-                                                                             activation_threshold)
+    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
+                                                        activation_n_bits,
+                                                        activation_is_signed)
 
-    return lambda x: q(x, min_value, max_value, scale)
+    return lambda x: q(x, min_value, max_value, activation_n_bits)
 
 
 def symmetric_quantization(activation_n_bits: int,
                            quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a symmetric fake-quantization node.
@@ -86,22 +94,24 @@
 
     Returns:
         A fake quantization node.
     """
     activation_threshold = quantization_params.get(THRESHOLD)
     activation_is_signed = quantization_params.get(SIGNED)
 
-    if activation_threshold is None or activation_is_signed is None:
-        return None # pragma: no cover
-
-    min_value, max_value, scale = get_symmetric_quantization_range_and_scale(activation_is_signed,
-                                                                             activation_n_bits,
-                                                                             activation_threshold)
+    if activation_threshold is None:
+        Logger.critical("Activation threshold must be specified.")  # pragma: no cover
+    if activation_is_signed is None:
+        Logger.critical("Parameter 'activation_is_signed' must be specified.")  # pragma: no cover
+
+    min_value, max_value = quantizer_min_max_calculator(activation_threshold,
+                                                        activation_n_bits,
+                                                        activation_is_signed)
 
-    return lambda x: q(x, min_value, max_value, scale)
+    return lambda x: q(x, min_value, max_value, activation_n_bits)
 
 
 def uniform_quantization(activation_n_bits: int,
                          quantization_params: dict) -> Callable:
     """
     Use a NodeQuantizationConfig to compute a quantizer min/max values, and use it to
     build and return a uniform fake-quantization node.
@@ -109,47 +119,38 @@
     Args:
         activation_n_bits: Number of bits to use for quantization.
         quantization_params: Dictionary of specific parameters for this quantization function.
 
     Returns:
         A fake quantization node.
     """
-    a, b = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
-
-    if a is None or b is None:
-        return None # pragma: no cover
-
-    # fixing quantization range to include 0
-    a = 0 if a > 0 else a
-    b = 0 if b < 0 else b
-    a, b = fix_range_to_include_zero(a, b, activation_n_bits)
+    min_value, max_value = quantization_params.get(RANGE_MIN), quantization_params.get(RANGE_MAX)
 
-    min_value = 0
-    max_value = 2 ** activation_n_bits - 1
-    scale = (b - a) / ((2 ** activation_n_bits) - 1)
-    zero_point = -round(a / scale)  # zp has to be positive, and a <=0, so we multiply by -1
+    if min_value is None:
+        Logger.critical("Minimum value must be specified.")  # pragma: no cover
+    if max_value is None:
+        Logger.critical("Maximum value must be specified.")  # pragma: no cover
 
-    return lambda x: q(x, min_value, max_value, scale, zero_point)
+    return lambda x: q(x, min_value, max_value, activation_n_bits)
 
 
-def q(x: torch.Tensor,
-      min_value: int,
-      max_value: int,
-      scale: float,
-      zero_point: int = 0) -> torch.Tensor:
+def q(x: TFReference, min_value, max_value, activation_n_bits) -> TFReference:
     """
-    Fake-quantize the input tensor x, using a pytorch fake-quantization node.
+    Fake-quantize the input tensor x, using a tensorflow fake-quantization node.
+
     Args:
-        x: input tensor to quantize.
-        min_value: lower bound of the quantized domain.
-        max_value: upper bound of the quantized domain.
-        scale: quantization scale.
-        zero_point: quantization zero_point
+        x: Input tensor to quantize.
+        min_value: quantization range lower bound.
+        max_value: quantization range upper bound.
+        activation_n_bits: Number of bits to use for quantization.
+
     Returns:
         The fake-quantized input tensor.
     """
+    if x.dtype != tf.float32:
+        x = tf.cast(x, dtype=tf.float32)  # pragma: no cover
 
-    return torch.fake_quantize_per_tensor_affine(x,
-                                                 scale=scale,
-                                                 zero_point=zero_point,
-                                                 quant_min=min_value,
-                                                 quant_max=max_value)
+    # fake_quant_with_min_max_vars expects to get x of float32
+    return tf.quantization.fake_quant_with_min_max_vars(x,
+                                                        min=min_value,
+                                                        max=max_value,
+                                                        num_bits=activation_n_bits)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/graph_builders.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/graph_builders.py`

 * *Files 19% similar despite different names*

```diff
@@ -11,49 +11,46 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import inspect
 from typing import Dict, List, Tuple, Callable
 import torch
-from torch.fx import GraphModule
+from torch.fx import GraphModule, Node
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.base_graph import OutTensor
 from model_compression_toolkit.core.common.graph.edge import Edge
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.pytorch.constants import OUTPUT, PLACEHOLDER, TENSOR_META, CALL_FUNCTION, TYPE, \
-    CALL_METHOD, BIAS, FUNCTIONAL_OP, OP_CALL_KWARGS, OP_CALL_ARGS, INPUTS_AS_LIST, GET_ATTR, CONSTANT, BUFFER
-from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, ConstantHolder, BufferHolder
+    CALL_METHOD, BIAS, FUNCTIONAL_OP, OP_CALL_KWARGS, OP_CALL_ARGS, INPUTS_AS_LIST, TENSOR_INPUT_INDICES, GET_ATTR
+from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder
 from model_compression_toolkit.logger import Logger
 
 
 def extract_holder_weights(constant_name, node_target, model, weights, to_numpy):
     """
-    Extract layer weights and named buffers for BufferHolder and ConstantHolder.
+    Extract layer weights and named buffers to a dictionary.
     Args:
-        constant_name: name to write the parameters under, CONSTANT for ConstantHolder and
-         BUFFER for BufferHolder.
+        constant_name: name to write the parameters under, should be the node name.
         node_target: relevant parameter name from Pytorch FX model.
         model: Pytorch FX model.
         weights: dictionary containing the weights of the node.
         to_numpy: Function to convert framework's tensor to a Numpy array.
 
     Returns:
         Updated weights dictionary.
     """
     named_parameters_weights = {constant_name: to_numpy(parameter) for name, parameter in
                                 model.named_parameters() if node_target == name}
     named_buffer_weights = {constant_name: to_numpy(parameter) for name, parameter in
                             model.named_buffers() if node_target == name}
     if len(named_parameters_weights) + len(named_buffer_weights) > 1:
-        raise Exception(
-            f'Constant parameter can only have one tensor. Here we have '
-            f'{len(named_parameters_weights)+ len(named_buffer_weights)}')
+        Logger.critical("A single constant parameter must correspond to exactly one tensor. Found {len(named_parameters_weights) + len(named_buffer_weights)} parameters.")
 
     weights.update(named_parameters_weights)
     weights.update(named_buffer_weights)
     return weights
 
 
 def nodes_builder(model: GraphModule,
@@ -72,14 +69,16 @@
     """
     # init function variables:
     inputs = []
     outputs = []
     nodes = []
     output_nodes = []
     fx_node_2_graph_node = {}
+    consts_dict = {}
+    used_consts = set()
 
     for node in model.graph.nodes:
         # extract node type and framework attributes
         framework_attr = dict(node.kwargs)
         node_has_activation = True
         if node.target in module_dict.keys():
             node_module = module_dict[node.target]
@@ -103,49 +102,46 @@
             continue
         elif node.op == CALL_METHOD:
             if hasattr(torch, node.target):
                 node_type = getattr(torch, node.target)
             elif hasattr(torch.Tensor, node.target):
                 node_type = getattr(torch.Tensor, node.target)
             else:
-                raise Exception(f'Call method of type \'{node.target}\' is currently not supported.')
+                Logger.critical(f"The call method '{node.target}' is not supported.")
         elif node.op == GET_ATTR:
-            if node.meta[TYPE] == torch.Tensor:
-                node_type = BufferHolder
-            else:
-                node_type = ConstantHolder
-            node_has_activation = False
             Logger.warning(
                 'Pytorch model has a parameter or constant Tensor value. This can cause unexpected behaviour when '
                 'converting the model.')
         else:
-            raise Exception(f'Unknown node type: {node.name}')
+            Logger.critical(f'Encountered an unsupported node type in node: {node.name}.')
 
         # extract layer weights and named buffers
         weights = {}
         if node.target in module_dict.keys():
             named_parameters_weights = {name: to_numpy(parameter) for name, parameter in
                                         module_dict[node.target].named_parameters()}
             named_buffer_weights = {name: to_numpy(parameter) for name, parameter in
                                     module_dict[node.target].named_buffers() if len(parameter.shape) > 0}
             weights.update(named_parameters_weights)
             weights.update(named_buffer_weights)
 
         if node.op == GET_ATTR:
-            if node_type == ConstantHolder:
-                weights = extract_holder_weights(CONSTANT, node.target, model, weights, to_numpy)
-                framework_attr.update(const_size=weights.get(CONSTANT).shape)
-            elif node_type == BufferHolder:
-                weights = extract_holder_weights(BUFFER, node.target, model, weights, to_numpy)
-                framework_attr.update(name=node.name)
+            new_const = extract_holder_weights(node, node.target, model, weights, to_numpy)
+            if list(new_const.keys())[0] in consts_dict:
+                Logger.critical('A constant weight appears to have been recorded multiple times.')
+            consts_dict.update(new_const)
+            continue
 
-        # extract input shapes
+        # extract input shapes and const weights
         input_shape = []
         if node.op != PLACEHOLDER:
-            for input_node in node.all_input_nodes:
+            for i, input_node in enumerate(node.all_input_nodes):
+                if input_node in consts_dict:
+                    used_consts.add(input_node)
+                    weights.update({i: consts_dict[input_node]})
                 tensor_meta = input_node.meta
                 if tensor_meta[TYPE] == torch.Tensor:
                     input_shape += [list(tensor_meta[TENSOR_META].shape)]
                 elif tensor_meta[TYPE] == tuple:
                     input_shape += [list(n.shape) for n in tensor_meta[TENSOR_META]]
                 elif tensor_meta[TYPE] == int:
                     input_shape += [[1]]
@@ -174,36 +170,37 @@
                 node_kwargs[k] = v
 
         # initiate graph nodes
         if node.op in [CALL_METHOD, CALL_FUNCTION]:
             graph_node_type = FunctionalNode
             inputs_as_list1 = len(node.args) > 0 and isinstance(node.args[0], (list, tuple)) and all(
                 [isinstance(n, torch.fx.node.Node) for n in node.args[0]])
-            inputs_as_list = inputs_as_list1 or \
-                             (len(node.args) > 0 and node.args[0].op == PLACEHOLDER and node.args[0].meta[TYPE] in (list, tuple))
+            inputs_as_list = inputs_as_list1 or (len(node.args) > 0 and isinstance(node.args[0], Node) and
+                                                 node.args[0].op == PLACEHOLDER and node.args[0].meta[TYPE] in (list, tuple))
+            tensor_input_index = []
+            op_call_args = list(node.args)
             if inputs_as_list:
-                num_inputs = 1
+                op_call_args.pop(0)
             else:
-                input_counter = 0
                 for in_node in node.all_input_nodes:
-                    for arg in node.args:
+                    for i, arg in enumerate(node.args):
                         if arg == in_node:
-                            input_counter += 1
-                num_inputs = max(len(node.all_input_nodes), input_counter)
-            op_call_args = list(node.args[num_inputs:])
+                            tensor_input_index.append(i)
 
             # remove torch.fx.node.Node from inputs to graph_node_type
-            for arg in op_call_args:
-                if isinstance(arg, torch.fx.node.Node):
-                    op_call_args.remove(arg)
+            op_call_args = [arg for arg in op_call_args if not isinstance(arg, Node)]
+            # convert torch.fx.immutable_collections.immutable_list to tuple
+            op_call_args = [tuple(arg) if isinstance(arg, torch.fx.immutable_collections.immutable_list) else arg
+                            for arg in op_call_args]
 
             kwargs = {FUNCTIONAL_OP: node_type,
                       OP_CALL_ARGS: op_call_args,
                       OP_CALL_KWARGS: node_kwargs,
-                      INPUTS_AS_LIST: inputs_as_list}
+                      INPUTS_AS_LIST: inputs_as_list,
+                      TENSOR_INPUT_INDICES: tensor_input_index}
         else:
             graph_node_type = BaseNode
             kwargs = {}
         graph_node = graph_node_type(name=node.name,
                                      framework_attr=framework_attr,
                                      input_shape=input_shape,
                                      output_shape=output_shape,
@@ -216,52 +213,57 @@
         if node.op == PLACEHOLDER:
             for ii in range(len(output_shape)):
                 inputs.append(graph_node)
 
         fx_node_2_graph_node[node] = graph_node
         nodes.append(graph_node)
 
+    # make sure all extracted constants were used in the graph
+    not_connected_consts = [c for c in consts_dict if c not in used_consts]
+    if not_connected_consts:
+        Logger.critical(f'Error reading graph: These constants are not connected in the graph: {not_connected_consts}.')
+
     # generate graph outputs list
     for node in output_nodes:
         outputs.append(OutTensor(fx_node_2_graph_node[node], output_nodes.index(node)))
 
     return nodes, inputs, outputs, fx_node_2_graph_node
 
 
 def edges_builder(model: GraphModule,
-                   fx_node_2_graph_node: Dict) -> List:
+                  fx_node_2_graph_node: Dict) -> List:
     """
 
     Args:
         model: Pytorch FX model.
         fx_node_2_graph_node: dictionary from fx node to graph node.
 
     Returns:
         List of graph edges
     """
-    src_index = 0 # in fx src_index is always zero because fx uses the getitem operator to fetch node outputs
+    src_index = 0  # in fx src_index is always zero because fx uses the getitem operator to fetch node outputs
     edges = []
     connectivity_dict = {}
     for node in model.graph.nodes:
         if node.op != OUTPUT:
             for input_node in node.all_input_nodes:
-
-                # n_edges_for_input_node is for the case that the input node appears more than
-                # once as the input of the node, for example add(x, x)
-                n_edges_for_input_node = sum([1 for a in node.args if input_node == a])
-                n_edges_for_input_node = max(n_edges_for_input_node, 1)
-
-                dst_index = node.all_input_nodes.index(input_node)
-                for i in range(n_edges_for_input_node):
-                    if connectivity_dict.get(input_node):
-                        connectivity_dict[input_node].append((node, dst_index))
-                    else:
-                        connectivity_dict[input_node] = [(node, dst_index)]
-                    dst_index += 1
+                if input_node in fx_node_2_graph_node:
+                    # n_edges_for_input_node is for the case that the input node appears more than
+                    # once as the input of the node, for example add(x, x)
+                    n_edges_for_input_node = sum([1 for a in node.args if input_node == a])
+                    n_edges_for_input_node = max(n_edges_for_input_node, 1)
+
+                    dst_index = node.all_input_nodes.index(input_node)
+                    for i in range(n_edges_for_input_node):
+                        if connectivity_dict.get(input_node):
+                            connectivity_dict[input_node].append((node, dst_index))
+                        else:
+                            connectivity_dict[input_node] = [(node, dst_index)]
+                        dst_index += 1
     for node in model.graph.nodes:
         out_nodes = connectivity_dict.get(node)
         if out_nodes:
             for (out_node, dst_index) in out_nodes:
                 edges.append(
                     Edge(fx_node_2_graph_node[node], fx_node_2_graph_node[out_node], src_index, dst_index))
 
-    return edges
+    return edges
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/reader/reader.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/reader.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from typing import Callable, Dict
 
 import numpy as np
 import torch
 from torch.fx import symbolic_trace
 from torch.fx.passes.shape_prop import ShapeProp
 
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.pytorch.reader.graph_builders import edges_builder, nodes_builder
 from model_compression_toolkit.core.pytorch.utils import set_model
 
 
 def generate_module_dict(model: torch.nn.Module) -> Dict:
     """
@@ -80,15 +81,20 @@
         representative_data_gen (Callable): Representative dataset used for shape inference.
         to_tensor: Function to convert a Numpy array to a framework's tensor.
 
     Returns:
         A fx.GraphModule (static model) representing the Pytorch model.
     """
     set_model(pytorch_model)
-    symbolic_traced = symbolic_trace(pytorch_model)
+
+    try:
+        symbolic_traced = symbolic_trace(pytorch_model)
+    except torch.fx.proxy.TraceError as e:
+        Logger.critical(f'Error parsing model with torch.fx\n'
+                        f'fx error: {e}')
     inputs = next(representative_data_gen())
     input_for_shape_infer = [to_tensor(i) for i in inputs]
     ShapeProp(symbolic_traced).propagate(*input_for_shape_infer)
     return symbolic_traced
 
 
 def remove_broken_nodes_from_graph(graph):
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/pytorch/utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,26 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 import numpy as np
 from typing import Union
-from model_compression_toolkit.core.pytorch.constants import CUDA, CPU
-
-
-def get_working_device() -> str:
-    """
-    Get the working device of the environment
-
-    Returns:
-        Device "cuda" if GPU is available, else "cpu"
-
-    """
-    return torch.device(CUDA if torch.cuda.is_available() else CPU)
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
+from model_compression_toolkit.logger import Logger
 
 
 def set_model(model: torch.nn.Module, train_mode: bool = False):
     """
     Set model to work in train/eval mode and GPU mode if GPU is available
 
     Args:
@@ -40,18 +30,16 @@
 
     """
     if train_mode:
         model.train()
     else:
         model.eval()
 
-    if torch.cuda.is_available():
-        model.cuda()
-    else:
-        model.cpu()
+    device = get_working_device()
+    model.to(device)
 
 
 def to_torch_tensor(tensor):
     """
     Convert a Numpy array to a Torch tensor.
     Args:
         tensor: Numpy array.
@@ -64,16 +52,18 @@
         return tensor.to(working_device)
     elif isinstance(tensor, list):
         return [to_torch_tensor(t) for t in tensor]
     elif isinstance(tensor, tuple):
         return (to_torch_tensor(t) for t in tensor)
     elif isinstance(tensor, np.ndarray):
         return torch.from_numpy(tensor.astype(np.float32)).to(working_device)
+    elif isinstance(tensor, (int, float)):
+        return torch.from_numpy(np.array(tensor).astype(np.float32)).to(working_device)
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(torch.Tensor)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to Torch.tensor: {type(tensor)}.')
 
 
 def torch_tensor_to_numpy(tensor: Union[torch.Tensor, list, tuple]) -> Union[np.ndarray, list, tuple]:
     """
     Convert a Pytorch tensor to a Numpy array.
     Args:
         tensor: Pytorch tensor.
@@ -86,8 +76,8 @@
     elif isinstance(tensor, list):
         return [torch_tensor_to_numpy(t) for t in tensor]
     elif isinstance(tensor, tuple):
         return tuple([torch_tensor_to_numpy(t) for t in tensor])
     elif isinstance(tensor, torch.Tensor):
         return tensor.cpu().detach().contiguous().numpy()
     else:
-        raise Exception(f'Conversion of type {type(tensor)} to {type(np.ndarray)} is not supported')
+        Logger.critical(f'Unsupported type for conversion to Numpy array: {type(tensor)}.')
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/core/runner.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantization_facade.py`

 * *Files 26% similar despite different names*

```diff
@@ -9,339 +9,292 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
+from typing import Callable
+from functools import partial
 
-import os
-from typing import Callable, Tuple, Any, List, Dict
-
-import numpy as np
-from tqdm import tqdm
-
-from model_compression_toolkit.core.common import FrameworkInfo
-from model_compression_toolkit.core.graph_prep_runner import graph_preparation_runner
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.common.graph.base_graph import Graph
-from model_compression_toolkit.core.common.mixed_precision.bit_width_setter import set_bit_widths
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_functions_mapping import kpi_functions_mapping
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_methods import MpKpiMetric
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_facade import search_bit_width
-from model_compression_toolkit.core.common.model_collector import ModelCollector
-from model_compression_toolkit.core.common.network_editors.edit_network import edit_network_graph
-from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common.quantization.quantization_analyzer import analyzer_graph
-from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_computation import \
-    calculate_quantization_params
-from model_compression_toolkit.core.common.statistics_correction.statistics_correction import \
-    statistics_correction_runner
-from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
+    MixedPrecisionQuantizationConfig
+from mct_quantizers import KerasActivationQuantizationHolder
+from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.visualization.final_config_visualizer import \
-    WeightsFinalBitwidthConfigVisualizer, \
-    ActivationFinalBitwidthConfigVisualizer
-from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
-
-
-def core_runner(in_model: Any,
-                representative_data_gen: Callable,
-                core_config: CoreConfig,
-                fw_info: FrameworkInfo,
-                fw_impl: FrameworkImplementation,
-                tpc: TargetPlatformCapabilities,
-                target_kpi: KPI = None,
-                tb_w: TensorboardWriter = None):
-    """
-    Quantize a trained model using post-training quantization.
-    First, the model graph is optimized using several transformations (e.g. folding BatchNormalization to preceding
-    layers).
-    Second, statistics (e.g. min/max, histogram, etc.) are collected for each layer's output
-    (and input, depends on the quantization configuration) using a given representative dataset.
-    Next, quantization parameters are calculated using the collected statistics
-    (both coefficients and activations by default).
-
-    Args:
-        in_model: Model to quantize.
-        representative_data_gen: Dataset used for calibration.
-        core_config: CoreConfig containing parameters of how the model should be quantized
-        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
-        groups of layers by how they should be quantized, etc.).
-        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
-        tpc: TargetPlatformCapabilities object that models the inference target platform and
-                                              the attached framework operator's information.
-        target_kpi: KPI to constraint the search of the mixed-precision configuration for the model.
-        tb_w: TensorboardWriter object for logging
-
-    Returns:
-        An internal graph representation of the input model.
-
-    """
-
-    graph = graph_preparation_runner(in_model,
-                                     representative_data_gen,
-                                     core_config.quantization_config,
-                                     fw_info,
-                                     fw_impl,
-                                     tpc,
-                                     tb_w,
-                                     mixed_precision_enable=core_config.mixed_precision_enable)
-
-    tg = _prepare_model_for_quantization(graph,
-                                         representative_data_gen,
-                                         core_config,
-                                         fw_info,
-                                         tb_w,
-                                         fw_impl)
-
-    ######################################
-    # Finalize bit widths
-    ######################################
-    if target_kpi is not None:
-        assert core_config.mixed_precision_enable
-        if core_config.mixed_precision_config.configuration_overwrite is None:
-
-            bit_widths_config = search_bit_width(tg,
-                                                 fw_info,
-                                                 fw_impl,
-                                                 target_kpi,
-                                                 core_config.mixed_precision_config,
-                                                 representative_data_gen)
-        else:
-            Logger.warning(
-                f'Mixed Precision has overwrite bit-width configuration{core_config.mixed_precision_config.configuration_overwrite}')
-            bit_widths_config = core_config.mixed_precision_config.configuration_overwrite
-
-    else:
-        bit_widths_config = []
-
-    tg = set_bit_widths(core_config.mixed_precision_enable,
-                        tg,
-                        bit_widths_config)
-
-    # Edit the graph again after finalizing the configurations.
-    # This is since some actions regard the final configuration and should be edited.
-    edit_network_graph(tg, fw_info, core_config.debug_config.network_editor)
-
-    _set_final_kpi(graph=tg,
-                   final_bit_widths_config=bit_widths_config,
-                   kpi_functions_dict=kpi_functions_mapping,
-                   fw_info=fw_info,
-                   fw_impl=fw_impl)
-
-    if target_kpi is not None:
-        # Retrieve lists of tuples (node, node's final weights/activation bitwidth)
-        weights_conf_nodes_bitwidth = tg.get_final_weights_config()
-        activation_conf_nodes_bitwidth = tg.get_final_activation_config()
-
-        Logger.info(
-            f'Final weights bit-width configuration: {[node_b[1] for node_b in weights_conf_nodes_bitwidth]}')
-        Logger.info(
-            f'Final activation bit-width configuration: {[node_b[1] for node_b in activation_conf_nodes_bitwidth]}')
-
-        if tb_w is not None:
-            if len(weights_conf_nodes_bitwidth) > 0:
-                visual = WeightsFinalBitwidthConfigVisualizer(weights_conf_nodes_bitwidth)
-                figure = visual.plot_config_bitwidth()
-                tb_w.add_figure(figure, f'Weights final bit-width config')
-            if len(activation_conf_nodes_bitwidth) > 0:
-                visual = ActivationFinalBitwidthConfigVisualizer(activation_conf_nodes_bitwidth)
-                figure = visual.plot_config_bitwidth()
-                tb_w.add_figure(figure, f'Activation final bit-width config')
-
-    return tg, bit_widths_config
-
-
-def _init_tensorboard_writer(fw_info: FrameworkInfo) -> TensorboardWriter:
-    """
-    Create a TensorBoardWriter object initialized with the logger dir path if it was set,
-    or None otherwise.
-
-    Args:
-        fw_info: FrameworkInfo object.
-
-    Returns:
-        A TensorBoardWriter object.
-    """
-    tb_w = None
-    if Logger.LOG_PATH is not None:
-        tb_log_dir = os.path.join(os.getcwd(), Logger.LOG_PATH, 'tensorboard_logs')
-        Logger.info(f'To use Tensorboard, please run: tensorboard --logdir {tb_log_dir}')
-        tb_w = TensorboardWriter(tb_log_dir, fw_info)
-    return tb_w
-
-
-def read_model_to_graph(in_model: Any,
-                        representative_data_gen: Callable,
-                        tpc: TargetPlatformCapabilities,
-                        fw_info: FrameworkInfo = None,
-                        fw_impl: FrameworkImplementation = None) -> Graph:
-
-    """
-    Read a model into a graph object.
-    Args:
-        in_model: Model to optimize and prepare for quantization.
-        representative_data_gen: Dataset used for calibration.
-        tpc: TargetPlatformCapabilities object that models the inference target platform and
-                      the attached framework operator's information.
-        fw_info: Information needed for quantization about the specific framework (e.g.,
-                kernel channels indices, groups of layers by how they should be quantized, etc.)
-        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
-    Returns:
-        Graph object that represents the model.
-    """
-    graph = fw_impl.model_reader(in_model,
-                                 representative_data_gen)
-    graph.set_fw_info(fw_info)
-    graph.set_tpc(tpc)
-    return graph
-
-
-def _prepare_model_for_quantization(transformed_graph: Graph,
-                                    representative_data_gen: Callable,
-                                    core_config: CoreConfig = CoreConfig(),
-                                    fw_info: FrameworkInfo = None,
-                                    tb_w: TensorboardWriter = None,
-                                    fw_impl: FrameworkImplementation = None) -> Graph:
-    """
-    Prepare a trained model for post-training quantization.
-    First, the model graph is optimized using several transformations (e.g. folding BatchNormalization to preceding layers).
-    Second, statistics (e.g. min/max, histogram, etc.) are collected for each layer's output
-    (and input, depends on the quantization configuration) using a given representative dataset.
-    Next, quantization parameters are calculated using the collected statistics.
-    Finally, more transformations (based on the statistics) are applied to increase the model's performance.
-
-    Args:
-        representative_data_gen (Callable): Dataset used for calibration.
-        core_config (CoreConfig): CoreConfig containing parameters of how the model should be quantized.
-        fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g.,
-        kernel channels indices, groups of layers by how they should be quantized, etc.)
-        tb_w (TensorboardWriter): TensorboardWriter object to use for logging events such as graphs, histograms, etc.
-        fw_impl (FrameworkImplementation): FrameworkImplementation object with a specific framework methods implementation.
-
-    Returns:
-        Graph object that represents the model, contains thresholds, and ready for quantization.
-    """
-
-    ######################################
-    # Graph analyzing (attaching statistics collectors)
-    ######################################
-    analyzer_graph(fw_impl.attach_sc_to_node,
-                   transformed_graph,
-                   fw_info,
-                   core_config.quantization_config)  # Mark points for statistics collection
-
-    if tb_w is not None:
-        tb_w.add_graph(transformed_graph, 'after_analyzer_graph')
-
-    ######################################
-    # Statistic collection
-    ######################################
-    mi = ModelCollector(transformed_graph,
-                        fw_impl,
-                        fw_info)
-
-    for _data in tqdm(representative_data_gen()):
-        mi.infer(_data)
-
-    ######################################
-    # Edit network according to user
-    # specific settings
-    ######################################
-    # Notice that not all actions affect at this stage (for example, actions that edit the final configuration as
-    # there are no final configurations at this stage of the optimization). For this reason we edit the graph
-    # again at the end of the optimization process.
-    edit_network_graph(transformed_graph, fw_info, core_config.debug_config.network_editor)
-
-    ######################################
-    # Calculate quantization params
-    ######################################
-    calculate_quantization_params(transformed_graph,
-                                  fw_info,
-                                  fw_impl=fw_impl)
-
-    if tb_w is not None:
-        tb_w.add_graph(transformed_graph, 'thresholds_selection')
-        tb_w.add_all_statistics(transformed_graph, 'thresholds_selection')
-
-    ######################################
-    # Graph substitution (post statistics collection)
-    ######################################
-    transformed_graph = substitute(transformed_graph,
-                                   fw_impl.get_substitutions_post_statistics_collection(core_config.quantization_config))
-
-    ######################################
-    # Shift Negative Activations
-    ######################################
-    if core_config.quantization_config.shift_negative_activation_correction:
-        transformed_graph = fw_impl.shift_negative_correction(transformed_graph,
-                                                              core_config,
-                                                              fw_info)
-        if tb_w is not None:
-            tb_w.add_graph(transformed_graph, 'after_shift_negative_correction')
-            tb_w.add_all_statistics(transformed_graph, 'after_shift_negative_correction')
-
-    if tb_w is not None:
-        tb_w.add_graph(transformed_graph, 'post_statistics_collection_substitutions')
-        tb_w.add_all_statistics(transformed_graph, 'post_statistics_collection_substitutions')
-
-    ######################################
-    # Statistics Correction
-    ######################################
-    tg_with_bias = statistics_correction_runner(transformed_graph, core_config, fw_info, fw_impl, tb_w)
-
-    for n in tg_with_bias.nodes:
-        assert n.final_weights_quantization_cfg is None
-
-    return tg_with_bias
-
-
-def _set_final_kpi(graph: Graph,
-                   final_bit_widths_config: List[int],
-                   kpi_functions_dict: Dict[KPITarget, Tuple[MpKpiMetric, MpKpiAggregation]],
-                   fw_info: FrameworkInfo,
-                   fw_impl: FrameworkImplementation):
-    """
-    Computing the KPIs of the model according to the final bit-width configuration,
-    and setting it (inplace) in the graph's UserInfo field.
-
-    Args:
-        graph: Graph to compute the KPI for.
-        final_bit_widths_config: The final bit-width configuration to quantize the model accordingly.
-        kpi_functions_dict: A mapping between a KPITarget and a pair of kpi method and kpi aggregation functions.
-        fw_info: A FrameworkInfo object.
-        fw_impl: FrameworkImplementation object with specific framework methods implementation.
-
-    """
-
-    final_kpis_dict = {}
-    for kpi_target, kpi_funcs in kpi_functions_dict.items():
-        kpi_method, kpi_aggr = kpi_funcs
-        if kpi_target == KPITarget.BOPS:
-            final_kpis_dict[kpi_target] = kpi_aggr(kpi_method(final_bit_widths_config, graph, fw_info, fw_impl, False), False)[0]
-        else:
-            non_conf_kpi = kpi_method([], graph, fw_info, fw_impl)
-            conf_kpi = kpi_method(final_bit_widths_config, graph, fw_info, fw_impl)
-            if len(final_bit_widths_config) > 0 and len(non_conf_kpi) > 0:
-                final_kpis_dict[kpi_target] = kpi_aggr(np.concatenate([conf_kpi, non_conf_kpi]), False)[0]
-            elif len(final_bit_widths_config) > 0 and len(non_conf_kpi) == 0:
-                final_kpis_dict[kpi_target] = kpi_aggr(conf_kpi, False)[0]
-            elif len(final_bit_widths_config) == 0 and len(non_conf_kpi) > 0:
-                # final_bit_widths_config == 0 ==> no configurable nodes,
-                # thus, KPI can be computed from non_conf_kpi alone
-                final_kpis_dict[kpi_target] = kpi_aggr(non_conf_kpi, False)[0]
-            else:
-                # No relevant nodes have been quantized with affect on the given target - since we only consider
-                # in the model's final size the quantized layers size, this means that the final size for this target
-                # is zero.
-                Logger.warning(f"No relevant quantized layers for the KPI target {kpi_target} were found, the recorded"
-                               f"final KPI for this target would be 0.")
-                final_kpis_dict[kpi_target] = 0
-
-    final_kpi = KPI()
-    final_kpi.set_kpi_by_target(final_kpis_dict)
-    graph.user_info.final_kpi = final_kpi
-    graph.user_info.mixed_precision_cfg = final_bit_widths_config
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+
+if FOUND_TF:
+    import tensorflow as tf
+    from tensorflow.keras.layers import Layer
+    from tensorflow.keras.models import Model
+
+    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
+    from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+
+    from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
+
+    from model_compression_toolkit import get_target_platform_capabilities
+
+    from model_compression_toolkit import get_target_platform_capabilities
+    from model_compression_toolkit.core import common
+    from model_compression_toolkit.core.common import BaseNode
+    from model_compression_toolkit.constants import TENSORFLOW
+    from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+    from model_compression_toolkit.qat.keras.quantizer.quantization_builder import quantization_builder, \
+    get_activation_quantizer_holder
+    from model_compression_toolkit.qat.common.qat_config import QATConfig
+
+    DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
+
+
+    def qat_wrapper(n: common.BaseNode,
+                    layer: Layer,
+                    qat_config: QATConfig):
+        """
+        A function which takes a computational graph node and a keras layer and perform the quantization wrapping
+        Args:
+            qat_config: Configuration of QAT (such as training methods for example).
+            n: A node of mct graph.
+            layer: A keras layer.
+
+        Returns: Wrapped layer
+
+        """
+        if is_qat_applicable(n, DEFAULT_KERAS_INFO):
+            # If we are here, then the node has a kernel attribute to quantize and training during QAT
+            weights_quantizers, _ = quantization_builder(n,
+                                                         qat_config,
+                                                         DEFAULT_KERAS_INFO.get_kernel_op_attributes(n.type)[0])
+            if len(weights_quantizers) > 0:
+                layer.trainable = True
+                return KerasTrainableQuantizationWrapper(layer, weights_quantizers)
+
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
+        return layer
+
+
+    def keras_quantization_aware_training_init_experimental(in_model: Model,
+                                                            representative_data_gen: Callable,
+                                                            target_resource_utilization: ResourceUtilization = None,
+                                                            core_config: CoreConfig = CoreConfig(),
+                                                            qat_config: QATConfig = QATConfig(),
+                                                            target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC):
+        """
+         Prepare a trained Keras model for quantization aware training. First the model quantization is optimized
+         with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
+         quantized using a symmetric quantization thresholds (power of two).
+         The model is first optimized using several transformations (e.g. BatchNormalization folding to
+         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+         being collected for each layer's output (and input, depends on the quantization configuration).
+         For each possible bit width (per layer) a threshold is then being calculated using the collected
+         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
+         a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
+         nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
+         quantization wrapper's weight quantizer.
+         In order to limit the maximal model's size, a target resource utilization need to be passed after weights_memory
+         is set (in bytes).
+
+         Args:
+             in_model (Model): Keras model to quantize.
+             representative_data_gen (Callable): Dataset used for initial calibration.
+             target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+             qat_config (QATConfig): QAT configuration
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
+
+         Returns:
+
+             A quantized model.
+             User information that may be needed to handle the quantized model.
+             Custom-Objects dictionary for loading the saved kers model.
+
+         Examples:
+
+             Import MCT:
+
+             >>> import model_compression_toolkit as mct
+
+             Import a Keras model:
+
+             >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
+             >>> model = MobileNetV2()
+
+             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
+             In this example a random dataset of 10 batches each containing 4 images is used.
+
+             >>> import numpy as np
+             >>> num_calibration_batches = 10
+             >>> def repr_datagen():
+             >>>     for _ in range(num_calibration_batches):
+             >>>         yield [np.random.random((4, 224, 224, 3))]
+
+             Create a MCT core config, containing the quantization configuration:
+
+             >>> config = mct.core.CoreConfig()
+
+             If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
+             The candidates bitwidth for quantization should be defined in the target platform model:
+
+             >>> config = mct.core.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfig())
+
+             For mixed-precision set a target ResourceUtilization object:
+             Create a ResourceUtilization object to limit our returned model's size. Note that this value affects only coefficients
+             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
+             while the bias will not):
+
+             >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+
+             Pass the model, the representative dataset generator, the configuration and the target Resource Utilization to get a
+             quantized model:
+
+             >>> quantized_model, quantization_info, custom_objects = mct.qat.keras_quantization_aware_training_init_experimental(model, repr_datagen, ru, core_config=config)
+
+             Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:
+
+             >>> quantized_model = tf.keras.models.load_model(model_file, custom_objects=custom_objects)
+
+             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
+
+         """
+
+        Logger.warning(f"keras_quantization_aware_training_init_experimental is experimental and is subject to future changes."
+                       f"If you encounter an issue, please open an issue in our GitHub "
+                       f"project https://github.com/sony/model_optimization")
+
+        KerasModelValidation(model=in_model,
+                             fw_info=DEFAULT_KERAS_INFO).validate()
+
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfig. Please use keras_post_training_quantization API,"
+                             "or pass a valid mixed precision configuration.")
+
+        tb_w = init_tensorboard_writer(DEFAULT_KERAS_INFO)
+
+        fw_impl = KerasImplementation()
+
+        # Ignore hessian service since is not used in QAT at the moment
+        tg, bit_widths_config, _ = core_runner(in_model=in_model,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=DEFAULT_KERAS_INFO,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
+
+        tg = ptq_runner(tg, representative_data_gen, core_config, DEFAULT_KERAS_INFO, fw_impl, tb_w)
+
+        _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
+        qat_model, user_info = KerasModelBuilder(graph=tg,
+                                                 fw_info=DEFAULT_KERAS_INFO,
+                                                 wrapper=_qat_wrapper,
+                                                 get_activation_quantizer_holder_fn=partial(get_activation_quantizer_holder,
+                                                                                            qat_config=qat_config)).build_model()
+
+        user_info.mixed_precision_cfg = bit_widths_config
+        #TODO: remove the last output after updating documentation.
+        return qat_model, user_info, {}
+
+
+    def keras_quantization_aware_training_finalize_experimental(in_model: Model) -> Model:
+        """
+         Convert a model fine-tuned by the user (Trainable quantizers) to a model with Inferable quantizers.
+
+         Args:
+             in_model (Model): Keras model to replace TrainableQuantizer with InferableQuantizer
+
+         Returns:
+             A quantized model with Inferable quantizers
+
+         Examples:
+
+             Import MCT:
+
+             >>> import model_compression_toolkit as mct
+
+             Import a Keras model:
+
+             >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
+             >>> model = MobileNetV2()
+
+             Create a random dataset generator:
+
+             >>> import numpy as np
+             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
+
+             Create a MCT core config, containing the quantization configuration:
+
+             >>> config = mct.core.CoreConfig()
+
+             If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
+             The candidates bitwidth for quantization should be defined in the target platform model:
+
+             >>> config = mct.core.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfig())
+
+             For mixed-precision set a target ResourceUtilization object:
+             Create a ResourceUtilization object to limit our returned model's size. Note that this value affects only coefficients
+             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
+             while the bias will not):
+
+             >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+
+             Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+             quantized model:
+
+             >>> quantized_model, quantization_info, custom_objects = mct.qat.keras_quantization_aware_training_init_experimental(model, repr_datagen, ru, core_config=config)
+
+             Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:
+
+             >>> quantized_model = tf.keras.models.load_model(model_file, custom_objects=custom_objects)
+             >>> quantized_model = mct.qat.keras_quantization_aware_training_finalize_experimental(quantized_model)
+
+         """
+        Logger.warning(
+            f"keras_quantization_aware_training_finalize_experimental is experimental and is subject to future changes."
+            f"If you encounter an issue, please open an issue in our GitHub "
+            f"project https://github.com/sony/model_optimization")
+
+        def _export(layer):
+            if isinstance(layer, KerasTrainableQuantizationWrapper):
+                layer = layer.convert_to_inferable_quantizers()
+            # In the KerasActivationQuantizationHolder case - converting the quantizers only
+            # is not enough. We need to create a new layer with inferable quantizers. The reason for that
+            # is that if we only convert the quantizers, the layer will have some weights (such as min, max,
+            # threshold) that do not match the configuration, thus loading such a model will fail.
+            # To overcome this, the convert_to_inferable_quantizers of KerasActivationQuantizationHolder
+            # creates a new layer from its new configuration after converting the trainable quantizer
+            # to an inferable quantizer.
+            elif isinstance(layer, KerasActivationQuantizationHolder):
+                layer = layer.convert_to_inferable_quantizers()
+            return layer
+
+        # clone each layer in the model and apply _export to layers with TrainableQuantizeWrappers
+        exported_model = tf.keras.models.clone_model(in_model, input_tensors=None, clone_function=_export)
+
+        return exported_model
+
+else:
+    # If tensorflow is not installed,
+    # we raise an exception when trying to use these functions.
+    def keras_quantization_aware_training_init_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_quantization_aware_training_init_experimental. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
+
+
+    def keras_quantization_aware_training_finalize_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_quantization_aware_training_finalize_experimental. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -8,16 +8,19 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+
 from model_compression_toolkit.constants import FOUND_TORCH, FOUND_TF
+from model_compression_toolkit.data_generation.common.data_generation_config import DataGenerationConfig
+from model_compression_toolkit.data_generation.common.enums import ImageGranularity, DataInitType, SchedulerType, BNLayerWeightingType, OutputLossType, BatchNormAlignemntLossType, ImagePipelineType, ImageNormalizationType
 
 if FOUND_TF:
     from model_compression_toolkit.data_generation.keras.keras_data_generation import (
-        tensorflow_data_generation_experimental, get_tensorflow_data_generation_config)
+        keras_data_generation_experimental, get_keras_data_generation_config)
 
 if FOUND_TORCH:
     from model_compression_toolkit.data_generation.pytorch.pytorch_data_generation import (
         pytorch_data_generation_experimental, get_pytorch_data_generation_config)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/common/pruning/mask/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/constants.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/data_generation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/data_generation.py`

 * *Files 4% similar despite different names*

```diff
@@ -60,61 +60,61 @@
     image_pipeline = (
         image_pipeline_dict.get(data_generation_config.image_pipeline_type)(
             output_image_size=output_image_size,
             extra_pixels=data_generation_config.extra_pixels))
 
     # Check if the image pipeline type is valid
     if image_pipeline is None:
-        Logger.exception(
-            f'Invalid image_pipeline_type {data_generation_config.image_pipeline_type}.'
-            f'Please choose one of {ImagePipelineType.get_values()}')
+        Logger.critical(
+            f'Invalid image_pipeline_type {data_generation_config.image_pipeline_type}. '
+            f'Please select one from {ImagePipelineType.get_values()}.')
 
     # Get the normalization values corresponding to the specified type
     normalization = image_normalization_dict.get(data_generation_config.image_normalization_type)
 
     # Check if the image normalization type is valid
     if normalization is None:
-        Logger.exception(
-            f'Invalid image_normalization_type {data_generation_config.image_normalization_type}.'
-            f'Please choose one of {ImageNormalizationType.get_values()}')
+        Logger.critical(
+            f'Invalid image_normalization_type {data_generation_config.image_normalization_type}. '
+            f'Please select one from {ImageNormalizationType.get_values()}.')
 
     # Get the layer weighting function corresponding to the specified type
     bn_layer_weighting_fn = bn_layer_weighting_function_dict.get(data_generation_config.layer_weighting_type)
 
     if bn_layer_weighting_fn is None:
-        Logger.exception(
-            f'Invalid layer_weighting_type {data_generation_config.layer_weighting_type}.'
-            f'Please choose one of {BNLayerWeightingType.get_values()}')
+        Logger.critical(
+            f'Invalid layer_weighting_type {data_generation_config.layer_weighting_type}. '
+            f'Please select one from {BNLayerWeightingType.get_values()}.')
 
     # Get the image initialization function corresponding to the specified type
     image_initialization_fn = image_initialization_function_dict.get(data_generation_config.data_init_type)
 
     # Check if the data initialization type is valid
     if image_initialization_fn is None:
-        Logger.exception(
-            f'Invalid data_init_type {data_generation_config.data_init_type}.'
-            f'Please choose one of {DataInitType.get_values()}')
+        Logger.critical(
+            f'Invalid data_init_type {data_generation_config.data_init_type}. '
+            f'Please select one from {DataInitType.get_values()}.')
 
     # Get the BatchNorm alignment loss function corresponding to the specified type
     bn_alignment_loss_fn = bn_alignment_loss_function_dict.get(data_generation_config.bn_alignment_loss_type)
 
     # Check if the BatchNorm alignment loss type is valid
     if bn_alignment_loss_fn is None:
-        Logger.exception(
-            f'Invalid bn_alignment_loss_type {data_generation_config.bn_alignment_loss_type}.'
-            f'Please choose one of {BatchNormAlignemntLossType.get_values()}')
+        Logger.critical(
+            f'Invalid bn_alignment_loss_type {data_generation_config.bn_alignment_loss_type}. '
+            f'Please select one from {BatchNormAlignemntLossType.get_values()}.')
 
     # Get the output loss function corresponding to the specified type
     output_loss_fn = output_loss_function_dict.get(data_generation_config.output_loss_type)
 
     # Check if the output loss type is valid
     if output_loss_fn is None:
-        Logger.exception(
-            f'Invalid output_loss_type {data_generation_config.output_loss_type}.'
-            f'Please choose one of {OutputLossType.get_values()}')
+        Logger.critical(
+            f'Invalid output_loss_type {data_generation_config.output_loss_type}. '
+            f'Please select one from {OutputLossType.get_values()}.')
 
     # Initialize the dataset for data generation
     init_dataset = image_initialization_fn(
         n_images=n_images,
         size=image_pipeline.get_image_input_size(),
         batch_size=data_generation_config.data_gen_batch_size)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/data_generation_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/data_generation_config.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/enums.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/enums.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,91 +25,120 @@
             List of values.
         """
         return [value.value for value in cls.__members__.values()]
 
 
 class ImageGranularity(EnumBaseClass):
     """
-    An enum for choosing the image dependence granularity when generating images.
-    0. ImageWise
-    1. BatchWise
-    2. AllImages
+    An enum for choosing the image dependence granularity when generating images:
+
+    ImageWise
+
+    BatchWise
+
+    AllImages
+
     """
 
     ImageWise = 0
     BatchWise = 1
     AllImages = 2
 
 
 class DataInitType(EnumBaseClass):
     """
-    An enum for choosing the image dependence granularity when generating images.
-    0. Gaussian
-    1. Diverse
+    An enum for choosing the image dependence granularity when generating images:
+
+    Gaussian
+
+    Diverse
+
     """
     Gaussian = 0
     Diverse = 1
 
 
 class ImagePipelineType(EnumBaseClass):
     """
-    An enum for choosing the image pipeline type for image manipulation.
-    RANDOM_CROP_FLIP: Crop and flip the images.
-    IDENTITY: Do not apply any manipulation (identity transformation).
+    An enum for choosing the image pipeline type for image manipulation:
+
+    RANDOM_CROP - Crop the images.
+
+    RANDOM_CROP_FLIP - Crop and flip the images.
+
+    IDENTITY - Do not apply any manipulation (identity transformation).
+
     """
     RANDOM_CROP = 'random_crop'
     RANDOM_CROP_FLIP = 'random_crop_flip'
     IDENTITY = 'identity'
 
 
 class ImageNormalizationType(EnumBaseClass):
     """
-    An enum for choosing the image normalization type.
-    TORCHVISION: Normalize the images using torchvision normalization.
-    KERAS_APPLICATIONS: Normalize the images using keras_applications imagenet normalization.
-    NO_NORMALIZATION: Do not apply any normalization.
+    An enum for choosing the image normalization type:
+
+    TORCHVISION - Normalize the images using torchvision normalization.
+
+    KERAS_APPLICATIONS - Normalize the images using keras_applications imagenet normalization.
+
+    NO_NORMALIZATION - Do not apply any normalization.
+
     """
     TORCHVISION = 'torchvision'
     KERAS_APPLICATIONS = 'keras_applications'
     NO_NORMALIZATION = 'no_normalization'
 
 
 class BNLayerWeightingType(EnumBaseClass):
     """
-   An enum for choosing the layer weighting type.
-   AVERAGE: Use the same weight per layer.
-   FIRST_LAYER_MULTIPLIER: Use a multiplier for the first layer, all other layers with the same weight.
-   GRAD: Use gradient-based layer weighting.
+   An enum for choosing the layer weighting type:
+
+   AVERAGE - Use the same weight per layer.
+
+   FIRST_LAYER_MULTIPLIER - Use a multiplier for the first layer, all other layers with the same weight.
+
+   GRAD - Use gradient-based layer weighting.
+
    """
     AVERAGE = 'average'
     FIRST_LAYER_MULTIPLIER = 'first_layer_multiplier'
     GRAD = 'grad'
 
 
 class BatchNormAlignemntLossType(EnumBaseClass):
     """
-    An enum for choosing the BatchNorm alignment loss type.
-    L2_SQUARE: Use L2 square loss for BatchNorm alignment.
+    An enum for choosing the BatchNorm alignment loss type:
+
+    L2_SQUARE - Use L2 square loss for BatchNorm alignment.
+
     """
     L2_SQUARE = 'l2_square'
 
 
 class OutputLossType(EnumBaseClass):
     """
-    An enum for choosing the output loss type.
-    NONE: No output loss is applied.
-    MIN_MAX_DIFF: Use min-max difference as the output loss.
-    REGULARIZED_MIN_MAX_DIFF: Use regularized min-max difference as the output loss.
+    An enum for choosing the output loss type:
+
+    NONE - No output loss is applied.
+
+    MIN_MAX_DIFF - Use min-max difference as the output loss.
+
+    REGULARIZED_MIN_MAX_DIFF - Use regularized min-max difference as the output loss.
+
     """
     NONE = 'none'
     MIN_MAX_DIFF = 'min_max_diff'
     REGULARIZED_MIN_MAX_DIFF = 'regularized_min_max_diff'
 
 
 class SchedulerType(EnumBaseClass):
     """
-    An enum for choosing the scheduler type for the optimizer.
-    REDUCE_ON_PLATEAU: Use the ReduceOnPlateau scheduler.
-    STEP: Use the Step scheduler.
+    An enum for choosing the scheduler type for the optimizer:
+
+    REDUCE_ON_PLATEAU - Use the ReduceOnPlateau scheduler.
+
+    STEP - Use the Step scheduler.
+
     """
     REDUCE_ON_PLATEAU = 'reduce_on_plateau'
     STEP = 'step'
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/image_pipeline.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/image_pipeline.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/model_info_exctractors.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/model_info_exctractors.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 
         Args:
             model (Any): The framework model.
             bn_layer_types (List): List of batch normalization layer types.
         """
         self.bn_params = self.get_bn_params(model, bn_layer_types)
         if self.get_num_bn_layers() == 0:
-            Logger.exception(
+            Logger.critical(
                 f'Data generation requires a model with at least one BatchNorm layer.')
 
     def get_bn_layer_names(self) -> List[str]:
         """
         Get the names of all batch normalization layers.
 
         Returns:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/common/optimization_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/optimization_utils.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/keras/hessian/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/constants.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/image_pipeline.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/image_pipeline.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/keras_data_generation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/keras_data_generation.py`

 * *Files 7% similar despite different names*

```diff
@@ -45,15 +45,15 @@
         KerasBatchStatsHolder
     from model_compression_toolkit.data_generation.keras.optimization_functions.output_loss_functions import \
         output_loss_function_dict
     from model_compression_toolkit.data_generation.keras.optimization_functions.scheduler_step_functions import \
         scheduler_step_function_dict
 
     # Function to create a DataGenerationConfig object with the specified configuration parameters for Tensorflow
-    def get_tensorflow_data_generation_config(
+    def get_keras_data_generation_config(
             n_iter: int = DEFAULT_N_ITER,
             optimizer: Optimizer = Adam,
             data_gen_batch_size: int = DEFAULT_DATA_GEN_BS,
             initial_lr: float = DEFAULT_KERAS_INITIAL_LR,
             output_loss_multiplier: float = DEFAULT_KERAS_OUTPUT_LOSS_MULTIPLIER,
             scheduler_type: SchedulerType = SchedulerType.REDUCE_ON_PLATEAU,
             bn_alignment_loss_type: BatchNormAlignemntLossType = BatchNormAlignemntLossType.L2_SQUARE,
@@ -111,31 +111,65 @@
             extra_pixels=extra_pixels,
             bn_layer_types=bn_layer_types,
             clip_images=clip_images,
             reflection=reflection,
             output_loss_multiplier=output_loss_multiplier)
 
 
-    def tensorflow_data_generation_experimental(
+    def keras_data_generation_experimental(
             model: tf.keras.Model,
             n_images: int,
             output_image_size: Tuple,
             data_generation_config: DataGenerationConfig) -> tf.Tensor:
         """
-        Function to perform data generation using the provided model and data generation configuration.
+        Function to perform data generation using the provided Keras model and data generation configuration.
 
         Args:
             model (Model): Keras model to generate data for.
             n_images (int): Number of images to generate.
             output_image_size (Tuple): Size of the output images.
             data_generation_config (DataGenerationConfig): Configuration for data generation.
 
         Returns:
             List[tf.Tensor]: Finalized list containing generated images.
+
+        Examples:
+
+            In this example, we'll walk through generating images using a simple Keras model and a data generation configuration. The process involves creating a model, setting up a data generation configuration, and finally generating images with specified parameters.
+
+            Start by importing the Model Compression Toolkit (MCT), TensorFlow, and some layers from `tensorflow.keras`:
+
+            >>> import model_compression_toolkit as mct
+            >>> from tensorflow.keras.models import Sequential
+            >>> from tensorflow.keras.layers import Conv2D, BatchNormalization, Flatten, Dense, Reshape
+
+            Next, define a simple Keras model:
+
+            >>> model = Sequential([Conv2D(2, 3, input_shape=(8,8,3)), BatchNormalization(), Flatten(), Dense(10)])
+
+            Configure the data generation process using `get_keras_data_generation_config`. This function allows customization of the data generation process. For simplicity, this example sets the number of iterations (`n_iter`) to 1 and the batch size (`data_gen_batch_size`) to 2.
+
+            >>> config = mct.data_generation.get_keras_data_generation_config(n_iter=1, data_gen_batch_size=2)
+
+            Finally, use the `keras_data_generation_experimental` function to generate images based on the model and data generation configuration.
+            Notice that this function is experimental and may change in future versions of MCT.
+            The `n_images` parameter specifies the number of images to generate, and `output_image_size` sets the size of the generated images.
+
+            >>> generated_images = mct.data_generation.keras_data_generation_experimental(model=model, n_images=4, output_image_size=(8, 8), data_generation_config=config)
+
+            The generated images can then be used for various purposes, such as data-free quantization.
+
+
         """
+
+        Logger.warning(f"keras_data_generation_experimental is experimental "
+                       f"and is subject to future changes."
+                       f"If you encounter an issue, please open an issue in our GitHub "
+                       f"project https://github.com/sony/model_optimization")
+
         # Get Data Generation functions and classes
         image_pipeline, normalization, bn_layer_weighting_fn, bn_alignment_loss_fn, output_loss_fn, \
             init_dataset = get_data_generation_classes(data_generation_config=data_generation_config,
                                                        output_image_size=output_image_size,
                                                        n_images=n_images,
                                                        image_pipeline_dict=image_pipeline_dict,
                                                        image_normalization_dict=image_normalization_dict,
@@ -143,26 +177,24 @@
                                                        bn_layer_weighting_function_dict,
                                                        image_initialization_function_dict=
                                                        image_initialization_function_dict,
                                                        bn_alignment_loss_function_dict=bn_alignment_loss_function_dict,
                                                        output_loss_function_dict=output_loss_function_dict)
 
         if not all(normalization[1]):
-            Logger.exception(
-                f'Invalid normalization std {normalization[1]} set to zero,'
-                f'will lead to division by zero., Please choose non-zero normalization std')
+            Logger.critical(
+                f'Invalid normalization standard deviation {normalization[1]} set to zero, which will lead to division by zero. Please select a non-zero normalization standard deviation.')
 
         # Get the scheduler functions corresponding to the specified scheduler type
         scheduler_get_fn = scheduler_step_function_dict.get(data_generation_config.scheduler_type)
 
         # Check if the scheduler type is valid
         if scheduler_get_fn is None:
-            Logger.exception(
-                f'Invalid scheduler_type {data_generation_config.scheduler_type}.'
-                f'Please choose one of {SchedulerType.get_values()}')
+            Logger.critical(
+                f'Invalid scheduler_type {data_generation_config.scheduler_type}. Please select one from {SchedulerType.get_values()}.')
 
         # Create a scheduler object with the specified number of iterations
         scheduler = scheduler_get_fn(n_iter=data_generation_config.n_iter,
                                      initial_lr=data_generation_config.initial_lr)
 
         # Set the model to eval mode
         model.trainable = False
@@ -319,15 +351,14 @@
         gradients = tape.gradient(total_loss, variables)
 
         # Return the computed gradients and individual loss components
         return gradients, total_loss, bn_loss, output_loss
 
 
 else:
-    def get_tensorflow_data_generation_config(*args, **kwargs):
-        Logger.critical('Installing tensorflow is mandatory when using get_tensorflow_data_generation_config. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def get_keras_data_generation_config(*args, **kwargs):
+        Logger.critical(
+            "Tensorflow must be installed to use get_tensorflow_data_generation_config. The 'tensorflow' package is missing.")  # pragma: no cover
 
 
-    def tensorflow_data_generation_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow is mandatory when using pytorch_data_generation_experimental. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_data_generation_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use tensorflow_data_generation_experimental. The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/model_info_exctractors.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/model_info_exctractors.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 import tensorflow as tf
 from keras.layers import Dense, Conv2D
 from tensorflow.keras.layers import BatchNormalization
 
 from model_compression_toolkit.data_generation.common.enums import ImageGranularity
 from model_compression_toolkit.data_generation.common.model_info_exctractors import OriginalBNStatsHolder, \
     ActivationExtractor
+from model_compression_toolkit.logger import Logger
 
 
 class KerasOriginalBNStatsHolder(OriginalBNStatsHolder):
     """
     Holds the original batch normalization (BN) statistics for a model.
     """
 
@@ -91,15 +92,15 @@
         self.layer_types_to_extract_inputs = tuple(layer_types_to_extract_inputs)
         self.linear_layers = linear_layers
 
         # Create a list of BatchNormalization layer names from the model.
         self.bn_layer_names = [layer.name for layer in model.layers if isinstance(layer,
                                                                                   self.layer_types_to_extract_inputs)]
         self.num_layers = len(self.bn_layer_names)
-        print(f'Number of layers = {self.num_layers}')
+        Logger.info(f'Number of layers = {self.num_layers}')
 
         # Initialize stats containers
         self.activations = {}
 
         # Initialize the last linear layer output variable as None In case the last layer is a linear layer (conv,
         # dense) last_linear_layer_output will assign with the last linear layer output value, if not the value will
         # stay None
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/common/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/batchnorm_alignment_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/batchnorm_alignment_functions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/bn_layer_weighting_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/bn_layer_weighting_functions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/image_initilization.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/image_initilization.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 import numpy as np
 import tensorflow as tf
 from tensorflow.data import Dataset
 
 from model_compression_toolkit.data_generation.common.constants import NUM_INPUT_CHANNELS
 from model_compression_toolkit.data_generation.common.enums import DataInitType
+from model_compression_toolkit.logger import Logger
 
 
 # Define a function to generate a dataset of Gaussian noise images.
 def generate_gaussian_noise_images(mean: float,
                                    std: float,
                                    num_samples: int,
                                    batch_size: int,
@@ -87,15 +88,15 @@
         size (Tuple[int, int]): Size of each image as (height, width).
         mean_factor (float): Factor to scale the mean of the Gaussian noise.
         std_factor (float): Factor to scale the standard deviation of the Gaussian noise.
 
     Returns:
         Tuple[int, Any]: A tuple containing the number of batches and a data loader iterator.
     """
-    print(f'Start generating random Gaussian data')
+    Logger.info(f'Start generating random Gaussian data')
     image_shape = size + (NUM_INPUT_CHANNELS,)
     dataset = generate_gaussian_noise_images(num_samples=n_images, image_shape=image_shape,
                                              mean=mean_factor, std=std_factor, batch_size=batch_size)
     data_loader = iter(dataset)
     return data_loader
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/output_loss_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/output_loss_functions.py`

 * *Files 3% similar despite different names*

```diff
@@ -39,18 +39,16 @@
         eps (float, optional): Small constant to prevent division by zero.
         **kwargs: Additional keyword arguments.
 
     Returns:
         tf.Tensor: The calculated loss.
     """
     if activation_extractor.last_linear_layers is None:
-        Logger.exception(
-            f'Cannot compute regularized min max output loss for the input model. The regularized min max output loss '
-            f'requires linear layer without a following BatchNormalization layer. Please choose one of '
-            f'{OutputLossType.get_values()}.')
+        Logger.critical(
+            f'Cannot compute regularized min-max output loss for the input model. This loss requires a linear layer without a subsequent BatchNormalization layer. Please select one from {OutputLossType.get_values()}.')
 
     with tape.stop_recording():
         weights_last_layer = activation_extractor.last_linear_layers.get_weights()[0]
         weights_norm = tf.norm(weights_last_layer, axis=-2)
         weights_norm = tf.squeeze(weights_norm)
         last_bn_layer = activation_extractor.get_layer_input_activation(
             activation_extractor.get_extractor_layer_names()[-1])
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_functions/scheduler_step_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/scheduler_step_functions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/keras/optimization_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_utils.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/optimization_functions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/constants.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/image_pipeline.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/image_pipeline.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/model_info_exctractors.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/model_info_exctractors.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,19 +14,20 @@
 # ==============================================================================
 from typing import Dict, Tuple, List
 import torch
 from torch import Tensor
 from torch.fx import GraphModule
 from torch.nn import Module, Conv2d, Linear
 
-from model_compression_toolkit.core.pytorch.utils import get_working_device
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
 from model_compression_toolkit.data_generation.common.model_info_exctractors import OriginalBNStatsHolder, \
     ActivationExtractor
 from model_compression_toolkit.data_generation.pytorch.constants import OUTPUT
 from model_compression_toolkit.data_generation.common.constants import IMAGE_INPUT, NUM_INPUT_CHANNELS
+from model_compression_toolkit.logger import Logger
 
 
 class PytorchOriginalBNStatsHolder(OriginalBNStatsHolder):
     """
     Holds the original batch normalization (BN) statistics for a model.
     """
     def __init__(self,
@@ -121,15 +122,15 @@
             last_layer_types_to_extract_inputs (List): Tuple or list of layer types.
         """
         self.model = model
         self.fx_model = fx_model
         self.layer_types_to_extract_inputs = tuple(layer_types_to_extract_inputs)
         self.last_layer_types_to_extract_inputs = tuple(last_layer_types_to_extract_inputs)
         self.num_layers = sum([1 if isinstance(layer, tuple(layer_types_to_extract_inputs)) else 0 for layer in model.modules()])
-        print(f'Number of layers = {self.num_layers}')
+        Logger.info(f'Number of layers = {self.num_layers}')
         self.hooks = {}  # Dictionary to store InputHook instances by layer name
         self.last_linear_layers_hooks = {}  # Dictionary to store InputHook instances by layer name
         self.hook_handles = []  # List to store hook handles
         self.last_linear_layer_weights = [] # list of the last linear layers' weights
 
         # set hooks for batch norm layers
         self._set_hooks_for_layers()
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/batchnorm_alignment_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/batchnorm_alignment_functions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/bn_layer_weighting_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/bn_layer_weighting_functions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/image_initilization.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/output_loss_functions.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 from typing import Dict, Callable
 
 import torch
 from torch import Tensor
 
-from model_compression_toolkit.core.pytorch.utils import get_working_device
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
 from model_compression_toolkit.data_generation.common.enums import OutputLossType
 from model_compression_toolkit.data_generation.pytorch.model_info_exctractors import PytorchActivationExtractor
 
 def min_max_diff(
         output_imgs: Tensor,
         activation_extractor: PytorchActivationExtractor,
         eps: float = 1e-6) -> Tensor:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/scheduler_step_functions.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/optimization_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 import torch
 from torch import Tensor
 from torch.nn import Module
 from torch.optim import Optimizer
 from torch.utils.data import DataLoader, Dataset
 from torchvision.transforms import Normalize
 
-from model_compression_toolkit.core.pytorch.utils import get_working_device
+from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
 from model_compression_toolkit.data_generation.common.enums import ImageGranularity
 from model_compression_toolkit.data_generation.common.image_pipeline import BaseImagePipeline
 from model_compression_toolkit.data_generation.common.optimization_utils import BatchStatsHolder, AllImagesStatsHolder, \
     BatchOptimizationHolder, ImagesOptimizationHandler
 from model_compression_toolkit.data_generation.common.constants import IMAGE_INPUT
 from model_compression_toolkit.data_generation.pytorch.constants import BATCH_AXIS, H_AXIS, W_AXIS
 from model_compression_toolkit.data_generation.pytorch.model_info_exctractors import ActivationExtractor
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/data_generation/pytorch/pytorch_data_generation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/pytorch_data_generation.py`

 * *Files 5% similar despite different names*

```diff
@@ -48,15 +48,15 @@
     # Importing necessary libraries
     import torch
     from torch import Tensor
     from torch.nn import Module
     from torch.optim import RAdam, Optimizer
     from torch.fx import symbolic_trace
 
-    from model_compression_toolkit.core.pytorch.utils import get_working_device
+    from model_compression_toolkit.core.pytorch.pytorch_device_config import get_working_device
 
     # Function to create a DataGenerationConfig object with the specified configuration parameters for Pytorch
     def get_pytorch_data_generation_config(
             n_iter: int = DEFAULT_N_ITER,
             optimizer: Optimizer = RAdam,
             data_gen_batch_size=DEFAULT_DATA_GEN_BS,
             initial_lr=DEFAULT_PYTORCH_INITIAL_LR,
@@ -125,28 +125,61 @@
             reflection=reflection
         )
 
 
     def pytorch_data_generation_experimental(
             model: Module,
             n_images: int,
-            output_image_size: Tuple,
+            output_image_size: int,
             data_generation_config: DataGenerationConfig) -> List[Tensor]:
         """
         Function to perform data generation using the provided model and data generation configuration.
 
         Args:
             model (Module): PyTorch model to generate data for.
             n_images (int): Number of images to generate.
-            output_image_size (Tuple): Size of the output images.
+            output_image_size (int): The hight and width size of the output images.
             data_generation_config (DataGenerationConfig): Configuration for data generation.
 
         Returns:
             List[Tensor]: Finalized list containing generated images.
+
+        Examples:
+
+            In this example, we'll walk through generating images using a simple PyTorch model and a data generation configuration. The process involves creating a model, setting up a data generation configuration, and finally generating images with specified parameters.
+
+            Start by importing the Model Compression Toolkit (MCT), PyTorch, and some modules from `torch.nn`:
+
+            >>> import model_compression_toolkit as mct
+            >>> import torch.nn as nn
+            >>> from torch.nn import Conv2d, BatchNorm2d, Flatten, Linear
+
+            Next, define a simple PyTorch model:
+
+            >>> model = nn.Sequential(nn.Conv2d(3, 2, 3), nn.BatchNorm2d(2), nn.Flatten(), nn.Linear(2*6*6, 10))
+
+            Configure the data generation process using `get_pytorch_data_generation_config`. This function allows customization of the data generation process. For simplicity, this example sets the number of iterations (`n_iter`) to 1 and the batch size (`data_gen_batch_size`) to 2.
+
+            >>> config = mct.data_generation.get_pytorch_data_generation_config(n_iter=1, data_gen_batch_size=2)
+
+            Finally, use the `pytorch_data_generation_experimental` function to generate images based on the model and data generation configuration.
+            Notice that this function is experimental and may change in future versions of MCT.
+            The `n_images` parameter specifies the number of images to generate, and `output_image_size` sets the size of the generated images.
+
+            >>> generated_images = mct.data_generation.pytorch_data_generation_experimental(model=model, n_images=4, output_image_size=8, data_generation_config=config)
+
+            The generated images can then be used for various purposes, such as data-free quantization.
+
         """
+
+        Logger.warning(f"pytorch_data_generation_experimental is experimental "
+                       f"and is subject to future changes."
+                       f"If you encounter an issue, please open an issue in our GitHub "
+                       f"project https://github.com/sony/model_optimization")
+
         # get a static graph representation of the model using torch.fx
         fx_model = symbolic_trace(model)
 		
 		# Get Data Generation functions and classes
         image_pipeline, normalization, bn_layer_weighting_fn, bn_alignment_loss_fn, output_loss_fn, \
             init_dataset = get_data_generation_classes(data_generation_config=data_generation_config,
                                                        output_image_size=output_image_size,
@@ -161,17 +194,16 @@
                                                        output_loss_function_dict=output_loss_function_dict)
 
         # Get the scheduler functions corresponding to the specified scheduler type
         scheduler_get_fn, scheduler_step_fn = scheduler_step_function_dict.get(data_generation_config.scheduler_type)
 
         # Check if the scheduler type is valid
         if scheduler_get_fn is None or scheduler_step_fn is None:
-            Logger.exception(
-                f'Invalid scheduler_type {data_generation_config.scheduler_type}. Please choose one of '
-                f'{SchedulerType.get_values()}')
+            Logger.critical(f'Invalid output_loss_type {data_generation_config.scheduler_type}. '
+                            f'Please select one from {SchedulerType.get_values()}.')
 
         # Create a scheduler object with the specified number of iterations
         scheduler = scheduler_get_fn(data_generation_config.n_iter)
 
         # Set the current model
         set_model(model)
 
@@ -181,16 +213,16 @@
             fx_model,
             data_generation_config.bn_layer_types,
             data_generation_config.last_layer_types)
 
         # Create an orig_bn_stats_holder object to hold original BatchNorm statistics
         orig_bn_stats_holder = PytorchOriginalBNStatsHolder(model, data_generation_config.bn_layer_types)
         if orig_bn_stats_holder.get_num_bn_layers() == 0:
-            Logger.exception(
-                f'Data generation requires a model with at least one Batch Norm layer.')
+            Logger.critical(
+                f'Data generation requires a model with at least one BatchNorm layer.')
 
         # Create an ImagesOptimizationHandler object for handling optimization
         all_imgs_opt_handler = PytorchImagesOptimizationHandler(model=model,
                                                                    data_gen_batch_size=data_generation_config.data_gen_batch_size,
                                                                    init_dataset=init_dataset,
                                                                    optimizer=data_generation_config.optimizer,
                                                                    image_pipeline=image_pipeline,
@@ -318,16 +350,14 @@
         finalized_imgs = all_imgs_opt_handler.get_finalized_images()
         Logger.info(f'Total time to generate {len(finalized_imgs)} images (seconds): {int(time.time() - total_time)}')
         return finalized_imgs
 else:
     # If torch is not installed,
     # we raise an exception when trying to use these functions.
     def get_pytorch_data_generation_config(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using get_pytorch_data_generation_config. '
-                        'Could not find torch package.')  # pragma: no cover
+        Logger.critical('PyTorch must be installed to use get_pytorch_data_generation_config. '
+                        "The 'torch' package is missing.")  # pragma: no cover
 
 
     def pytorch_data_generation_experimental(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_data_generation_experimental. '
-                        'Could not find the torch package.')  # pragma: no cover
+        Logger.critical("PyTorch must be installed to use 'pytorch_data_generation_experimental'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
+from model_compression_toolkit.exporter.model_exporter.fw_agonstic.quantization_format import QuantizationFormat
 from model_compression_toolkit.exporter.model_exporter.keras.export_serialization_format import \
     KerasExportSerializationFormat
 from model_compression_toolkit.exporter.model_exporter.pytorch.export_serialization_format import \
     PytorchExportSerializationFormat
 from model_compression_toolkit.exporter.model_exporter.keras.keras_export_facade import keras_export_model
 from model_compression_toolkit.exporter.model_exporter.pytorch.pytorch_export_facade import pytorch_export_model
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,18 +15,16 @@
 from typing import Callable
 
 from model_compression_toolkit.exporter.model_exporter.fw_agonstic.exporter import Exporter
 import keras
 
 import tensorflow as tf
 from packaging import version
-if version.parse(tf.__version__) >= version.parse("2.13"):
-    TMP_KERAS_EXPORT_FORMAT = ".keras"
-else:
-    TMP_KERAS_EXPORT_FORMAT = ".h5"
+
+DEFAULT_KERAS_EXPORT_EXTENTION = '.keras'
 
 
 class BaseKerasExporter(Exporter):
     """
     Base Keras exporter class.
     """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 from enum import Enum
 
 
 class KerasExportSerializationFormat(Enum):
     """
     Specify which serialization format to use for exporting a quantized Keras model.
 
-    KERAS_H5 - .keras (TF2.13 and above) or .h5 (TF2.12 and below) file format
+    KERAS - .keras file format
 
     TFLITE - .tflite file format
 
     """
 
-    KERAS_H5 = 0
+    KERAS = 0
     TFLITE = 1
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py`

 * *Files 1% similar despite different names*

```diff
@@ -42,26 +42,29 @@
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
     """
 
     def __init__(self,
                  model: keras.models.Model,
                  is_layer_exportable_fn: Callable,
-                 save_model_path: str):
+                 save_model_path: str,
+                 verbose: bool = True):
         """
 
         Args:
             model: Model to export.
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             save_model_path: Path to save the exported model.
+            verbose: Whether to log information about the export process or not.
         """
 
         super().__init__(model,
                          is_layer_exportable_fn,
                          save_model_path)
+        self._verbose = verbose
 
     def export(self) -> Dict[str, type]:
         """
         Convert an exportable (fully-quantized) Keras model to a fakely-quant model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
         """
 
@@ -92,15 +95,15 @@
 
                     # Create a list of weights for the new created layer
                     if isinstance(layer.layer, layers.DepthwiseConv2D):
                         weights_list.append(layer.get_quantized_weights()['depthwise_kernel'])
                     elif isinstance(layer.layer, (layers.Conv2D, layers.Dense, layers.Conv2DTranspose)):
                         weights_list.append(layer.get_quantized_weights()['kernel'])
                     else:
-                        Logger.error(f'KerasQuantizationWrapper should wrap only DepthwiseConv2D, Conv2D, Dense'
+                        Logger.critical(f'KerasQuantizationWrapper should wrap only DepthwiseConv2D, Conv2D, Dense'
                                      f' and Conv2DTranspose layers but wrapped layer is {layer.layer}')
 
                     if layer.layer.bias is not None:
                         weights_list.append(layer.layer.bias)
 
                     # In order to add the weights of the layer, we need to build it. To build it
                     # we need to pass its input shape. Not every layer has input_shape since some
@@ -134,15 +137,16 @@
         filtered_weights = self.get_filtered_weights()
         new_model.set_weights(filtered_weights)
         self.exported_model = new_model
 
         if self.exported_model is None:
             Logger.critical(f'Exporter can not save model as it is not exported')  # pragma: no cover
 
-        Logger.info(f'Exporting FQ Keras model to: {self.save_model_path}')
+        if self._verbose:
+            Logger.info(f'Exporting FQ Keras model to: {self.save_model_path}')
 
         keras.models.save_model(self.exported_model, self.save_model_path)
 
         return FakelyQuantKerasExporter.get_custom_objects()
 
     def transform_model_cfg(self) -> dict:
         """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,25 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import os
+from pathlib import Path
 import tempfile
 from typing import Callable
 
 import tensorflow as tf
 import keras.models
 
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.exporter.model_exporter.keras.base_keras_exporter import TMP_KERAS_EXPORT_FORMAT
 from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_keras_exporter import FakelyQuantKerasExporter
 from model_compression_toolkit.trainable_infrastructure.keras.load_model import keras_load_quantized_model
-
+from model_compression_toolkit.exporter.model_exporter.keras.base_keras_exporter import DEFAULT_KERAS_EXPORT_EXTENTION
 
 class FakelyQuantTFLiteExporter(FakelyQuantKerasExporter):
     """
     Exporter for fakely-quant TFLite models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
@@ -54,19 +54,25 @@
         """
         Convert an exportable (fully-quantized) Keras model to a fakely-quant TFLite model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
 
         """
         # Use Keras exporter to quantize model's weights before converting it to TFLite.
         # Since exporter saves the model, we use a tmp path for saving, and then we delete it.
-        _, tmp_file = tempfile.mkstemp(TMP_KERAS_EXPORT_FORMAT)
-        custom_objects = FakelyQuantKerasExporter(self.model,
-                                                  self.is_layer_exportable_fn,
-                                                  tmp_file).export()
-
-        model = keras_load_quantized_model(tmp_file)
-        os.remove(tmp_file)
+        handle, tmp_file = tempfile.mkstemp(DEFAULT_KERAS_EXPORT_EXTENTION)
+        # Close handle right away, the file is going to be reopenned by Keras exporter
+        os.close(handle)
+        try:
+            custom_objects = FakelyQuantKerasExporter(self.model,
+                                                      self.is_layer_exportable_fn,
+                                                      tmp_file,
+                                                      verbose=False).export()
+
+            model = keras_load_quantized_model(tmp_file)
+        # Ensures artifact is removed even in case of error
+        finally:
+            Path(tmp_file).unlink(missing_ok=True)
 
         self.exported_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()
         Logger.info(f'Exporting FQ tflite model to: {self.save_model_path}')
         with open(self.save_model_path, 'wb') as f:
             f.write(self.exported_model)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,13 +17,13 @@
 
 class PytorchExportSerializationFormat(Enum):
     """
     Specify which serialization format to use for exporting a quantized Pytorch model.
 
     TORCHSCRIPT - torchscript format
 
-    ONNX - onnx fromat
+    ONNX - onnx format
 
     """
 
     TORCHSCRIPT = 0
     ONNX = 1
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py`

 * *Files 5% similar despite different names*

```diff
@@ -16,54 +16,51 @@
 
 import torch.nn
 
 from mct_quantizers import PytorchActivationQuantizationHolder, PytorchQuantizationWrapper
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from model_compression_toolkit.exporter.model_exporter.pytorch.base_pytorch_exporter import BasePyTorchExporter
-from packaging import version
 from mct_quantizers import pytorch_quantizers
 
-# ONNX opset version 16 is supported from PyTorch 1.12
-if version.parse(torch.__version__) < version.parse("1.12"):
-    OPSET_VERSION = 15
-else:
-    OPSET_VERSION = 16
-
+DEFAULT_ONNX_OPSET_VERSION=15
 
 class FakelyQuantONNXPyTorchExporter(BasePyTorchExporter):
     """
     Exporter for fakely-quant PyTorch models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
     """
 
     def __init__(self,
                  model: torch.nn.Module,
                  is_layer_exportable_fn: Callable,
                  save_model_path: str,
                  repr_dataset: Callable,
-                 use_onnx_custom_quantizer_ops: bool = False):
+                 use_onnx_custom_quantizer_ops: bool = False,
+                 onnx_opset_version=DEFAULT_ONNX_OPSET_VERSION):
         """
 
         Args:
             model: Model to export.
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             save_model_path: Path to save the exported model.
             repr_dataset: Representative dataset (needed for creating torch script).
             use_onnx_custom_quantizer_ops: Whether to export quantizers custom ops in ONNX or not.
+            onnx_opset_version: ONNX opset version to use for exported ONNX model.
         """
 
         super().__init__(model,
                          is_layer_exportable_fn,
                          save_model_path,
                          repr_dataset)
 
         self._use_onnx_custom_quantizer_ops = use_onnx_custom_quantizer_ops
+        self._onnx_opset_version = onnx_opset_version
 
 
     def export(self) -> None:
         """
         Convert an exportable (fully-quantized) PyTorch model to a fakely-quant model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
 
@@ -79,22 +76,25 @@
         # wraps and quantizes the ops in place (for weights, for activation torch quantization function is
         # exported since it's used during forward.
         if self._use_onnx_custom_quantizer_ops:
             self._enable_onnx_custom_ops_export()
         else:
             self._substitute_fully_quantized_model()
 
-        Logger.info(f"Exporting PyTorch fake quant onnx model: {self.save_model_path}")
+        if self._use_onnx_custom_quantizer_ops:
+            Logger.info(f"Exporting onnx model with MCTQ quantizers: {self.save_model_path}")
+        else:
+            Logger.info(f"Exporting fake-quant onnx model: {self.save_model_path}")
 
         model_input = to_torch_tensor(next(self.repr_dataset())[0])
 
         torch.onnx.export(self.model,
                           model_input,
                           self.save_model_path,
-                          opset_version=OPSET_VERSION,
+                          opset_version=self._onnx_opset_version,
                           verbose=False,
                           input_names=['input'],
                           output_names=['output'],
                           dynamic_axes={'input': {0: 'batch_size'},
                                         'output': {0: 'batch_size'}})
 
     def _enable_onnx_custom_ops_export(self):
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,95 +11,97 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
 from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.exporter.model_exporter.fw_agonstic.quantization_format import QuantizationFormat
 from model_compression_toolkit.exporter.model_exporter.pytorch.export_serialization_format import \
     PytorchExportSerializationFormat
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+
 
 if FOUND_TORCH:
     import torch.nn
     from model_compression_toolkit.exporter.model_exporter.pytorch.fakely_quant_onnx_pytorch_exporter import \
-        FakelyQuantONNXPyTorchExporter
+    FakelyQuantONNXPyTorchExporter, DEFAULT_ONNX_OPSET_VERSION
     from model_compression_toolkit.exporter.model_exporter.pytorch.fakely_quant_torchscript_pytorch_exporter import \
         FakelyQuantTorchScriptPyTorchExporter
     from model_compression_toolkit.exporter.model_wrapper.pytorch.validate_layer import is_pytorch_layer_exportable
 
     supported_serialization_quantization_export_dict = {
         PytorchExportSerializationFormat.TORCHSCRIPT: [QuantizationFormat.FAKELY_QUANT],
-        PytorchExportSerializationFormat.ONNX: [QuantizationFormat.FAKELY_QUANT]
+        PytorchExportSerializationFormat.ONNX: [QuantizationFormat.FAKELY_QUANT, QuantizationFormat.MCTQ]
     }
 
     def pytorch_export_model(model: torch.nn.Module,
                              save_model_path: str,
                              repr_dataset: Callable,
-                             target_platform_capabilities: TargetPlatformCapabilities,
                              is_layer_exportable_fn: Callable = is_pytorch_layer_exportable,
-                             serialization_format: PytorchExportSerializationFormat =
-                             PytorchExportSerializationFormat.TORCHSCRIPT,
-                             use_onnx_custom_quantizer_ops: bool = False) -> None:
+                             serialization_format: PytorchExportSerializationFormat = PytorchExportSerializationFormat.ONNX,
+                             quantization_format : QuantizationFormat = QuantizationFormat.MCTQ,
+                             onnx_opset_version=DEFAULT_ONNX_OPSET_VERSION) -> None:
         """
         Export a PyTorch quantized model to a torchscript or onnx model.
         The model will be saved to the path in save_model_path.
         Currently, pytorch_export_model supports only QuantizationFormat.FAKELY_QUANT (where weights
         and activations are float fakely-quantized values) and PytorchExportSerializationFormat.TORCHSCRIPT
         (where the model will be saved to TorchScript model) or PytorchExportSerializationFormat.ONNX
         (where the model will be saved to ONNX model).
 
         Args:
             model: Model to export.
             save_model_path: Path to save the model.
             repr_dataset: Representative dataset for tracing the pytorch model (mandatory for exporting it).
-            target_platform_capabilities: TargetPlatformCapabilities object that describes the desired inference
-            target platform (includes quantization format).
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             serialization_format: Format to export the model according to (by default
-            PytorchExportSerializationFormat.TORCHSCRIPT).
-            use_onnx_custom_quantizer_ops: Whether to export quantizers ops in ONNX or not (affects only if serialization_format==PytorchExportSerializationFormat.ONNX). Experimental
+            PytorchExportSerializationFormat.ONNX).
+            quantization_format: Format of how quantizers are exported (fakely-quant, int8, MCTQ quantizers).
+            onnx_opset_version: ONNX opset version to use for exported ONNX model.
 
         """
 
         if serialization_format == PytorchExportSerializationFormat.TORCHSCRIPT:
-            if target_platform_capabilities.tp_model.quantization_format in \
-                    supported_serialization_quantization_export_dict[serialization_format]:
+            if quantization_format in supported_serialization_quantization_export_dict[serialization_format]:
                 exporter = FakelyQuantTorchScriptPyTorchExporter(model,
                                                                  is_layer_exportable_fn,
                                                                  save_model_path,
                                                                  repr_dataset)
             else:
                 Logger.critical(
-                    f'Unsupported quantization {target_platform_capabilities.tp_model.quantization_format} for '
+                    f'Unsupported quantization {quantization_format} for '
                     f'serialization {serialization_format} was used to export Pytorch model. Please see API for '
                     f'supported formats.')  # pragma: no cover
 
         elif serialization_format == PytorchExportSerializationFormat.ONNX:
-            if target_platform_capabilities.tp_model.quantization_format in \
-                    supported_serialization_quantization_export_dict[serialization_format]:
+            if quantization_format == QuantizationFormat.FAKELY_QUANT:
+                exporter = FakelyQuantONNXPyTorchExporter(model,
+                                                          is_layer_exportable_fn,
+                                                          save_model_path,
+                                                          repr_dataset,
+                                                          onnx_opset_version=onnx_opset_version)
+            elif quantization_format == QuantizationFormat.MCTQ:
                 exporter = FakelyQuantONNXPyTorchExporter(model,
                                                           is_layer_exportable_fn,
                                                           save_model_path,
                                                           repr_dataset,
-                                                          use_onnx_custom_quantizer_ops=use_onnx_custom_quantizer_ops)
+                                                          use_onnx_custom_quantizer_ops=True,
+                                                          onnx_opset_version=onnx_opset_version)
             else:
                 Logger.critical(
-                    f'Unsupported quantization {target_platform_capabilities.tp_model.quantization_format} for '
+                    f'Unsupported quantization {quantization_format} for '
                     f'serialization {serialization_format} was used to export Pytorch model. Please see API for '
                     f'supported formats.')  # pragma: no cover
 
         else:
             Logger.critical(
-                f'Unsupported serialization {serialization_format} was used to export Pytorch model. Please see API '
-                f'for supported formats.')  # pragma: no cover
+                f'Unsupported serialization {serialization_format} was used to export Pytorch model.'
+                f' Please see API for supported formats.')  # pragma: no cover
 
         exporter.export()
 
 else:
     def pytorch_export_model(*args, **kwargs):
-        Logger.error('Installing torch is mandatory '
-                     'when using pytorch_export_model. '
-                     'Could not find PyTorch packages.')  # pragma: no cover
+        Logger.critical("PyTorch must be installed to use 'pytorch_export_model'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py`

 * *Files 19% similar despite different names*

```diff
@@ -15,80 +15,87 @@
 
 from typing import Tuple, Callable
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.constants import FOUND_TF
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.logger import Logger
-from mct_quantizers import KerasActivationQuantizationHolder
+import model_compression_toolkit.core as C
 
 if FOUND_TF:
     import tensorflow as tf
     from tensorflow.keras.layers import Layer
     from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
-    from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizers import get_quantization_quantizers
     from mct_quantizers import KerasQuantizationWrapper
+    from mct_quantizers import KerasActivationQuantizationHolder
 
     def _get_wrapper(node: common.BaseNode,
-                     layer: Layer) -> Layer:
+                     layer: Layer,
+                     fw_impl=None) -> Layer:
         """
         A function which takes a computational graph node and a keras layer and perform the quantization wrapping
         Args:
             node: A node of mct graph.
             layer: A keras layer
 
         Returns: Wrapped layer with weights quantizers and activation quantizers
 
         """
-        weights_quantizers, _ = get_quantization_quantizers(node)
+        weights_quantizers, _ = fw_impl.get_inferable_quantizers(node)
         if len(weights_quantizers) > 0:
             return KerasQuantizationWrapper(layer,
                                             weights_quantizers)
         return layer
 
 
-    def get_activation_quantizer_holder(node: common.BaseNode) -> Callable:
+    def get_activation_quantizer_holder(node: common.BaseNode, fw_impl) -> Callable:
         """
         Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
 
         Args:
             node: Node to get ActivationQuantizationHolder to attach in its output.
 
         Returns:
             A ActivationQuantizationHolder layer for the node activation quantization.
         """
-        _, activation_quantizers = get_quantization_quantizers(node)
+        _, activation_quantizers = fw_impl.get_inferable_quantizers(node)
 
         # Holder by definition uses a single quantizer for the activation quantization
         # thus we make sure this is the only possible case (unless it's a node with no activation
         # quantization, which in this case has an empty list).
         if len(activation_quantizers) == 1:
             return KerasActivationQuantizationHolder(activation_quantizers[0])
 
-        Logger.error(
+        Logger.critical(
             f'ActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
             f'were found for node {node}')
 
-
-
     def get_exportable_keras_model(graph: Graph) -> Tuple[tf.keras.models.Model, UserInformation]:
         """
         Convert graph to an exportable Keras model (model with all quantization parameters).
         An exportable model can then be exported using model_exporter, to retrieve the
         final exported model.
 
         Args:
             graph: Graph to convert to an exportable Keras model.
 
         Returns:
             Exportable Keras model and user information.
         """
         exportable_model, user_info = KerasModelBuilder(graph=graph,
-                                                        wrapper=_get_wrapper,
-                                                        get_activation_quantizer_holder_fn=get_activation_quantizer_holder).build_model()
+                                                        wrapper=lambda n, kn:
+                                                        _get_wrapper(n, kn,
+                                                                     fw_impl=C.keras.keras_implementation.KerasImplementation()),
+                                                        get_activation_quantizer_holder_fn=lambda n:
+                                                        get_activation_quantizer_holder(n,
+                                                                                        fw_impl=C.keras.keras_implementation.KerasImplementation())).build_model()
         exportable_model.trainable = False
+
+        Logger.info("Please run your accuracy evaluation on the exported quantized model to verify it's accuracy.\n"
+                    "Checkout the FAQ and Troubleshooting pages for resolving common issues and improving the quantized model accuracy:\n"
+                    "FAQ: https://github.com/sony/model_optimization/tree/main/FAQ.md"
+                    "Quantization Troubleshooting: https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md")
         return exportable_model, user_info
 else:
     def get_exportable_keras_model(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing tensorflow is mandatory '
-                     'when using get_exportable_keras_model. '
-                     'Could not find Tensorflow package.')
+        Logger.critical("Tensorflow must be installed to use get_exportable_keras_model. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -24,64 +24,71 @@
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
 from mct_quantizers import constants as qi_keras_consts
 
 
 def get_inferable_quantizer_kwargs(node_qc: BaseNodeQuantizationConfig,
-                                   quantization_target: QuantizationTarget) -> Dict[str, Any]:
+                                   quantization_target: QuantizationTarget,
+                                   attr_name: str = None) -> Dict[str, Any]:
     """
     Get the quantization parameters for an inferable quantizer.
     Args:
         node_qc: The node quantization configuration of the node for which the quantizer is being created.
             Needs to match the specific quantization target.
         quantization_target: The target of the quantization (weights or activations).
+        attr_name: The weights attribute to get its quantizer kwargs (if target is weights quantization).
 
     Returns:
         The quantization parameters as a dictionary.
     """
 
     if quantization_target == QuantizationTarget.Weights:
         if not isinstance(node_qc, NodeWeightsQuantizationConfig):
-            Logger.error(f"Non-compatible node quantization config was given for quantization target Weights.")  # pragma: no cover
+            Logger.critical(f"Non-compatible node quantization config was given for quantization target Weights.")  # pragma: no cover
 
-        quantization_method = node_qc.weights_quantization_method
+        if attr_name is None:
+            Logger.error(f"Attribute name was not specified for retrieving weights quantizer kwargs.")
+
+        attr_node_qc = node_qc.get_attr_config(attr_name=attr_name)
+
+        quantization_method = attr_node_qc.weights_quantization_method
 
         # Return the appropriate quantization parameters based on the quantization method
         if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                    QuantizationMethod.SYMMETRIC]:
-            return {qi_keras_consts.NUM_BITS: node_qc.weights_n_bits,
-                    qi_keras_consts.THRESHOLD: list(node_qc.weights_quantization_params[THRESHOLD].flatten()),
-                    qi_keras_consts.PER_CHANNEL: node_qc.weights_per_channel_threshold,
-                    qi_keras_consts.CHANNEL_AXIS: node_qc.weights_channels_axis,
-                    qi_keras_consts.INPUT_RANK: len(node_qc.weights_quantization_params[THRESHOLD].shape)}
+            return {qi_keras_consts.NUM_BITS: attr_node_qc.weights_n_bits,
+                    qi_keras_consts.THRESHOLD: list(attr_node_qc.weights_quantization_params[THRESHOLD].flatten()),
+                    qi_keras_consts.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                    qi_keras_consts.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                    qi_keras_consts.INPUT_RANK: len(attr_node_qc.weights_quantization_params[THRESHOLD].shape)}
 
         elif quantization_method in [QuantizationMethod.UNIFORM]:
-            return {qi_keras_consts.NUM_BITS: node_qc.weights_n_bits,
-                    qi_keras_consts.PER_CHANNEL: node_qc.weights_per_channel_threshold,
-                    qi_keras_consts.MIN_RANGE: list(node_qc.weights_quantization_params[RANGE_MIN].flatten()),
-                    qi_keras_consts.MAX_RANGE: list(node_qc.weights_quantization_params[RANGE_MAX].flatten()),
-                    qi_keras_consts.CHANNEL_AXIS: node_qc.weights_channels_axis,
-                    qi_keras_consts.INPUT_RANK: len(node_qc.weights_quantization_params[RANGE_MIN].shape)}
+            return {qi_keras_consts.NUM_BITS: attr_node_qc.weights_n_bits,
+                    qi_keras_consts.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                    qi_keras_consts.MIN_RANGE: list(attr_node_qc.weights_quantization_params[RANGE_MIN].flatten()),
+                    qi_keras_consts.MAX_RANGE: list(attr_node_qc.weights_quantization_params[RANGE_MAX].flatten()),
+                    qi_keras_consts.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                    qi_keras_consts.INPUT_RANK: len(attr_node_qc.weights_quantization_params[RANGE_MIN].shape)}
 
         elif quantization_method in [QuantizationMethod.LUT_SYM_QUANTIZER, QuantizationMethod.LUT_POT_QUANTIZER]:
-            return {qi_keras_consts.NUM_BITS: node_qc.weights_n_bits,
-                    qi_keras_consts.PER_CHANNEL: node_qc.weights_per_channel_threshold,
-                    qi_keras_consts.LUT_VALUES: list(node_qc.weights_quantization_params[LUT_VALUES].flatten()),
-                    qi_keras_consts.THRESHOLD: list(node_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten()),
-                    qi_keras_consts.CHANNEL_AXIS: node_qc.weights_channels_axis,
+            return {qi_keras_consts.NUM_BITS: attr_node_qc.weights_n_bits,
+                    qi_keras_consts.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                    qi_keras_consts.LUT_VALUES: list(attr_node_qc.weights_quantization_params[LUT_VALUES].flatten()),
+                    qi_keras_consts.THRESHOLD: list(attr_node_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten()),
+                    qi_keras_consts.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
                     # TODO: how to pass multiplier nbits and eps for a specific node?
-                    qi_keras_consts.INPUT_RANK: len(node_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
+                    qi_keras_consts.INPUT_RANK: len(attr_node_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
 
         else:
             Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
 
     elif quantization_target == QuantizationTarget.Activation:
         if not isinstance(node_qc, NodeActivationQuantizationConfig):
-            Logger.error(f"Non-compatible node quantization config was given for quantization target Activation.")  # pragma: no cover
+            Logger.critical(f"Non-compatible node quantization config was given for quantization target Activation.")  # pragma: no cover
 
         quantization_method = node_qc.activation_quantization_method
 
         # Return the appropriate quantization parameters based on the quantization method
         if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                    QuantizationMethod.SYMMETRIC]:
             return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
@@ -104,32 +111,36 @@
                     }
         else:
             Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
     else:
         Logger.critical(f'{quantization_target} is not supported')  # pragma: no cover
 
 
-def get_weights_quantizer_for_node(node: BaseNode) -> BaseKerasInferableQuantizer:
+def get_weights_quantizer_for_node(node: BaseNode, attr_name: str) -> BaseKerasInferableQuantizer:
     """
-    Get weights quantizer for a node.
+    Get weights quantizer for a weights attribute of a node.
+
     Args:
         node: Node to create a weight quantizer for.
+        attr_name: Attribute name to get its quantizer.
+
     Returns:
-        Quantizer for the node's weights.
+        Quantizer for the node's weights attribute.
     """
     if node.final_weights_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final weights quantization configuration')  # pragma:
         # no cover
     node_w_qc = node.final_weights_quantization_cfg
-    weights_quantization_method = node_w_qc.weights_quantization_method
+    weights_quantization_method = node_w_qc.get_attr_config(attr_name).weights_quantization_method
 
     quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
                                                       weights_quantization_method,
                                                       BaseKerasInferableQuantizer)
-    kwargs = get_inferable_quantizer_kwargs(node_w_qc, QuantizationTarget.Weights)
+
+    kwargs = get_inferable_quantizer_kwargs(node_w_qc, QuantizationTarget.Weights, attr_name)
 
     return quantier_for_node(**kwargs)
 
 
 def get_activations_quantizer_for_node(node: BaseNode) -> BaseKerasInferableQuantizer:
     """
     Get activation quantizer for a node.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/fw_agnostic/get_inferable_quantizers.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,39 +8,42 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Dict, List, Tuple
+from typing import Dict, List, Tuple, Callable
 from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
-    get_weights_quantizer_for_node, get_activations_quantizer_for_node
 
 
-def get_quantization_quantizers(node: BaseNode) -> Tuple[Dict, List]:
+def get_inferable_quantizers(node: BaseNode,
+                             get_weights_quantizer_for_node: Callable,
+                             get_activations_quantizer_for_node: Callable,
+                             attributes_names: List[str] = []) -> Tuple[Dict, List]:
     """
     Create quantizers to wrap a layer for its corresponding node.
 
     Args:
         node: Node to create quantizers for.
+        get_weights_quantizer_for_node: A function that returns weights quantizer for the node attributes.
+        get_activations_quantizer_for_node: A function that returns activation quantizer for the node activation tensor.
+        attributes_names: A potential list of attribute names to set weights quantizers to.
 
     Returns:
         weight_quantizers: A dictionary between a weight's name to its quantizer.
         activation_quantizers: A list of activations quantization, one for each layer output.
     """
+
     weight_quantizers = {}
     activation_quantizers = []
 
-    if node.is_weights_quantization_enabled():
-        weight_attrs = DEFAULT_KERAS_INFO.get_kernel_op_attributes(node.type)
-        weight_quantizer = get_weights_quantizer_for_node(node)
-        for attr in weight_attrs:
+    for attr in attributes_names:
+        if node.is_weights_quantization_enabled(attr):
+            weight_quantizer = get_weights_quantizer_for_node(node, attr)
             weight_quantizers[attr] = weight_quantizer
 
     if node.is_activation_quantization_enabled():
         num_of_outputs = len(node.output_shape) if isinstance(node.output_shape, list) else 1
         activation_quantizers = [get_activations_quantizer_for_node(node)] * num_of_outputs
 
     return weight_quantizers, activation_quantizers
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py`

 * *Files 10% similar despite different names*

```diff
@@ -42,40 +42,39 @@
         """
         # Keras Input layers are not wrapped
         if isinstance(layer, InputLayer):
             return True
 
         valid_layer = isinstance(layer, Layer)
         if not valid_layer:
-            Logger.error(
+            Logger.critical(
                 f'Exportable layer must be a Keras layer, but layer {layer.name} is of type '
                 f'{type(layer)}') # pragma: no cover
 
         if isinstance(layer, KerasQuantizationWrapper):
             valid_weights_quantizers = isinstance(layer.weights_quantizers, dict)
             if not valid_weights_quantizers:
-                Logger.error(
+                Logger.critical(
                     f'KerasQuantizationWrapper must have a weights_quantizers but has a '
                     f'{type(layer.weights_quantizers)} object') # pragma: no cover
 
             if len(layer.weights_quantizers) == 0:
-                Logger.error(f'KerasQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers. If layer is not quantized it should be a Keras layer.')
+                Logger.critical(f'KerasQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers. If layer is not quantized it should be a Keras layer.')
 
             for _, weights_quantizer in layer.weights_quantizers.items():
                 if not isinstance(weights_quantizer, BaseInferableQuantizer):
-                    Logger.error(
+                    Logger.critical(
                         f'weights_quantizer must be a BaseInferableQuantizer object but has a '
                         f'{type(weights_quantizer)} object')  # pragma: no cover
 
         if isinstance(layer, KerasActivationQuantizationHolder):
             if not isinstance(layer.activation_holder_quantizer, BaseInferableQuantizer):
-                Logger.error(
+                Logger.critical(
                     f'activation quantizer in KerasActivationQuantizationHolder'
                     f' must be a BaseInferableQuantizer object but has a '
                     f'{type(layer.activation_holder_quantizer)} object')  # pragma: no cover
 
         return True
 else:
     def is_keras_layer_exportable(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing tensorflow is mandatory '
-                     'when using is_keras_layer_exportable. '
-                     'Could not find Tensorflow package.')
+        Logger.critical("Tensorflow must be installed to use is_keras_layer_exportable. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py`

 * *Files 17% similar despite different names*

```diff
@@ -15,71 +15,82 @@
 
 from typing import Union, Callable
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.constants import FOUND_TORCH
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import BaseNode
+import model_compression_toolkit.core as C
 
 if FOUND_TORCH:
     import torch
     from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
     from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-    from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizers import \
-        get_quantization_quantizers
 
 
     def fully_quantized_wrapper(node: common.BaseNode,
-                                module: torch.nn.Module) -> Union[torch.nn.Module,PytorchQuantizationWrapper]:
+                                module: torch.nn.Module,
+                                fw_impl) -> Union[torch.nn.Module,PytorchQuantizationWrapper]:
         """
         A function which takes a computational graph node and a pytorch module and
         perform the quantization wrapping
 
         Args:
             node: A node of mct graph.
             module: A Pytorch module
         Returns: Wrapped layer
 
         """
-        weight_quantizers, _ = get_quantization_quantizers(node)
+        weight_quantizers, _ = fw_impl.get_inferable_quantizers(node)
         if len(weight_quantizers) > 0:
             return PytorchQuantizationWrapper(module, weight_quantizers)
         return module
 
-    def get_activation_quantizer_holder(node: BaseNode) -> Callable:
+    def get_activation_quantizer_holder(node: BaseNode, fw_impl) -> Callable:
         """
         Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
         If the layer is not supposed to be wrapped with an activation quantizer - return None.
         Args:
             node: Node to attach a PytorchActivationQuantizationHolder to its output.
         Returns:
             A PytorchActivationQuantizationHolder module for the node's activation quantization.
         """
-        _, activation_quantizers = get_quantization_quantizers(node)
+        _, activation_quantizers = fw_impl.get_inferable_quantizers(node)
         # Holder by definition uses a single quantizer for the activation quantization
         # thus we make sure this is the only possible case (unless it's a node we no activation
         # quantization, which in this case has an empty list).
         if len(activation_quantizers) == 1:
             return PytorchActivationQuantizationHolder(activation_quantizers[0])
-        Logger.error(
+        Logger.critical(
             f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
             f'were found for node {node}')
 
     def get_exportable_pytorch_model(graph: Graph):
         """
         Convert graph to fully quantized PyTorch model.
 
         Args:
             graph: Graph to convert to a PyTorch model.
 
         Returns:
             Fully quantized PyTorch model.
         """
-        return PyTorchModelBuilder(graph=graph,
-                                   wrapper=fully_quantized_wrapper,
-                                   get_activation_quantizer_holder_fn=get_activation_quantizer_holder).build_model()
+        exportable_model, user_info = PyTorchModelBuilder(graph=graph,
+                                                          wrapper=lambda n, m:
+                                                          fully_quantized_wrapper(n, m,
+                                                                                  fw_impl=C.pytorch.pytorch_implementation.PytorchImplementation()),
+                                                          get_activation_quantizer_holder_fn=lambda n:
+                                                          get_activation_quantizer_holder(n,
+                                                                                          fw_impl=C.pytorch.pytorch_implementation.PytorchImplementation())).build_model()
+
+        Logger.info("Please run your accuracy evaluation on the exported quantized model to verify it's accuracy.\n"
+                    "Checkout the FAQ and Troubleshooting pages for resolving common issues and improving the quantized model accuracy:\n"
+                    "FAQ: https://github.com/sony/model_optimization/tree/main/FAQ.md"
+                    "Quantization Troubleshooting: https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md")
+
+        return exportable_model, user_info
+
 
 else:
-    def get_exportable_pytorch_model(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing torch is mandatory '
-                     'when using get_exportable_pytorch_model. '
-                     'Could not find PyTorch package.')
+    def get_exportable_pytorch_model(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'get_exportable_pytorch_model'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py`

 * *Files 12% similar despite different names*

```diff
@@ -26,53 +26,62 @@
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers import \
     constants as qi_inferable_quantizers_constants
 from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
 import numpy as np
 
 
-def get_weights_inferable_quantizer_kwargs(node_qc: NodeWeightsQuantizationConfig) -> Dict[str, Any]:
+def get_weights_inferable_quantizer_kwargs(node_qc: NodeWeightsQuantizationConfig, attr_name: str) -> Dict[str, Any]:
     """
     Get the quantization parameters for a weights inferable quantizer.
     Args:
         node_qc: The node quantization configuration of the node for which the quantizer is being created.
             Needs to match the specific quantization target.
+        attr_name: The weights attribute to get its quantizer kwargs (if target is weights quantization).
+
 
     Returns:
         The quantization parameters as a dictionary.
     """
 
     if not isinstance(node_qc, NodeWeightsQuantizationConfig):
-        Logger.error(
+        Logger.critical(
             f"Non-compatible node quantization config was given for quantization target Weights.")  # pragma: no cover
 
-    quantization_method = node_qc.weights_quantization_method
+    if attr_name is None:
+        Logger.error(f"Attribute name was not specified for retrieving weights quantizer kwargs.")
+
+    attr_node_qc = node_qc.get_attr_config(attr_name=attr_name)
+
+    quantization_method = attr_node_qc.weights_quantization_method
 
     # Return the appropriate quantization parameters based on the quantization method
     if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                QuantizationMethod.SYMMETRIC]:
-        return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.weights_n_bits,
-                qi_inferable_quantizers_constants.THRESHOLD: node_qc.weights_quantization_params[THRESHOLD].flatten().tolist(),
-                qi_inferable_quantizers_constants.PER_CHANNEL: node_qc.weights_per_channel_threshold,
-                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_qc.weights_channels_axis}
+        return {qi_inferable_quantizers_constants.NUM_BITS: attr_node_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.THRESHOLD: attr_node_qc.weights_quantization_params[THRESHOLD].flatten().tolist(),
+                qi_inferable_quantizers_constants.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                }
 
     elif quantization_method in [QuantizationMethod.UNIFORM]:
-        return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.weights_n_bits,
-                qi_inferable_quantizers_constants.PER_CHANNEL: node_qc.weights_per_channel_threshold,
-                qi_inferable_quantizers_constants.MIN_RANGE: node_qc.weights_quantization_params[RANGE_MIN].flatten().tolist(),
-                qi_inferable_quantizers_constants.MAX_RANGE: node_qc.weights_quantization_params[RANGE_MAX].flatten().tolist(),
-                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_qc.weights_channels_axis}
+        return {qi_inferable_quantizers_constants.NUM_BITS: attr_node_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.MIN_RANGE: attr_node_qc.weights_quantization_params[RANGE_MIN].flatten().tolist(),
+                qi_inferable_quantizers_constants.MAX_RANGE: attr_node_qc.weights_quantization_params[RANGE_MAX].flatten().tolist(),
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                }
 
     elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER, QuantizationMethod.LUT_SYM_QUANTIZER]:
-        return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.weights_n_bits,
-                qi_inferable_quantizers_constants.LUT_VALUES: node_qc.weights_quantization_params[LUT_VALUES].flatten().tolist(),
-                qi_inferable_quantizers_constants.THRESHOLD: node_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten().tolist(),
-                qi_inferable_quantizers_constants.PER_CHANNEL: node_qc.weights_per_channel_threshold,
-                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_qc.weights_channels_axis,
-                qi_inferable_quantizers_constants.INPUT_RANK: len(node_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
+        return {qi_inferable_quantizers_constants.NUM_BITS: attr_node_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.LUT_VALUES: attr_node_qc.weights_quantization_params[LUT_VALUES].flatten().tolist(),
+                qi_inferable_quantizers_constants.THRESHOLD: attr_node_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten().tolist(),
+                qi_inferable_quantizers_constants.PER_CHANNEL: attr_node_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: attr_node_qc.weights_channels_axis[0],  # output channel axis
+                qi_inferable_quantizers_constants.INPUT_RANK: len(attr_node_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
                 # TODO: Add LUT_VALUES_BITWIDTH & EPS to node quantization config
 
     else:
         Logger.critical(f'Not supported quantization method for weights inferable quantizers.')  # pragma: no cover
 
 
 def get_activation_inferable_quantizer_kwargs(node_qc: NodeActivationQuantizationConfig) -> Dict[str, Any]:
@@ -84,15 +93,15 @@
             Needs to match the specific quantization target.
 
     Returns:
         The quantization parameters as a dictionary.
     """
 
     if not isinstance(node_qc, NodeActivationQuantizationConfig):
-        Logger.error(
+        Logger.critical(
             f"Non-compatible node quantization config was given for quantization target Activation.")  # pragma: no cover
 
     quantization_method = node_qc.activation_quantization_method
 
     # Return the appropriate quantization parameters based on the quantization method
     if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                QuantizationMethod.SYMMETRIC]:
@@ -111,35 +120,36 @@
                 qi_inferable_quantizers_constants.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]],
                 qi_inferable_quantizers_constants.SIGNED: node_qc.activation_quantization_params.get(SIGNED)}
         # TODO: Add LUT_VALUES_BITWIDTH & EPS to node quantization config
     else:
         Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
 
 
-def get_weights_quantizer_for_node(node: BaseNode) -> BasePyTorchInferableQuantizer:
+def get_weights_quantizer_for_node(node: BaseNode, attr_name: str) -> BasePyTorchInferableQuantizer:
     """
     Get weights quantizer for a node.
 
     Args:
         node: Node to create a weight quantizer for.
+        attr_name: Attribute name to get its quantizer.
 
     Returns:
         Quantizer for the node's weights.
 
     """
     if node.final_weights_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final weights quantization configuration')  # pragma:
         # no cover
     node_w_qc = node.final_weights_quantization_cfg
-    weights_quantization_method = node_w_qc.weights_quantization_method
+    weights_quantization_method = node_w_qc.get_attr_config(attr_name).weights_quantization_method
 
     quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
                                                       weights_quantization_method,
                                                       BasePyTorchInferableQuantizer)
-    kwargs = get_weights_inferable_quantizer_kwargs(node_w_qc)
+    kwargs = get_weights_inferable_quantizer_kwargs(node_w_qc, attr_name)
 
     return quantier_for_node(**kwargs)
 
 
 def get_activations_quantizer_for_node(node: BaseNode) -> BasePyTorchInferableQuantizer:
     """
     Get activation quantizer for a node.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py`

 * *Files 12% similar despite different names*

```diff
@@ -31,40 +31,39 @@
         Args:
             layer: PyTorch module to check if considered to be valid for exporting.
 
         Returns:
             Check whether a PyTorch layer is a valid exportable layer or not.
         """
         if not isinstance(layer, nn.Module):
-            Logger.error(f'Exportable layer must be a nn.Module layer, but layer {layer.name} is of type {type(layer)}') # pragma: no cover
+            Logger.critical(f'Exportable layer must be a nn.Module layer, but layer {layer.name} is of type {type(layer)}.') # pragma: no cover
 
         if isinstance(layer, PytorchQuantizationWrapper):
             valid_weights_quantizers = isinstance(layer.weights_quantizers, dict)
             if not valid_weights_quantizers:
-                Logger.error(
+                Logger.critical(
                     f'PytorchQuantizationWrapper must have a weights_quantizers but has a '
-                    f'{type(layer.weights_quantizers)} object') # pragma: no cover
+                    f'{type(layer.weights_quantizers)} object.') # pragma: no cover
 
             if len(layer.weights_quantizers) == 0:
-                Logger.error(f'PytorchQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers.'
+                Logger.critical(f'PytorchQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers.'
                              f'If layer is not quantized it should be a Keras layer.')
 
             for _, weights_quantizer in layer.weights_quantizers.items():
                 if not isinstance(weights_quantizer, BasePyTorchInferableQuantizer):
-                    Logger.error(
+                    Logger.critical(
                         f'weights_quantizer must be a BasePyTorchInferableQuantizer object but has a '
-                        f'{type(weights_quantizer)} object')  # pragma: no cover
+                        f'{type(weights_quantizer)} object.')  # pragma: no cover
 
         elif isinstance(layer, PytorchActivationQuantizationHolder):
             if not isinstance(layer.activation_holder_quantizer, BasePyTorchInferableQuantizer):
-                Logger.error(
+                Logger.critical(
                     f'activation quantizer in PytorchActivationQuantizationHolder'
                     f' must be a BasePyTorchInferableQuantizer object but has a '
-                    f'{type(layer.activation_holder_quantizer)} object')  # pragma: no cover
+                    f'{type(layer.activation_holder_quantizer)} object.')  # pragma: no cover
 
         return True
 
 else:
     def is_pytorch_layer_exportable(*args, **kwargs):  # pragma: no cover
-        Logger.error('Installing torch is mandatory '
-                     'when using is_pytorch_layer_exportable. '
-                     'Could not find PyTorch package.')
+        Logger.critical("PyTorch must be installed to use 'is_pytorch_layer_exportable'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,12 +9,12 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType, GradientPTQConfigV2, GPTQHessianWeightsConfig
-from model_compression_toolkit.gptq.keras.quantization_facade import keras_gradient_post_training_quantization_experimental
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType, GPTQHessianScoresConfig
+from model_compression_toolkit.gptq.keras.quantization_facade import keras_gradient_post_training_quantization
 from model_compression_toolkit.gptq.keras.quantization_facade import get_keras_gptq_config
-from model_compression_toolkit.gptq.pytorch.quantization_facade import pytorch_gradient_post_training_quantization_experimental
+from model_compression_toolkit.gptq.pytorch.quantization_facade import pytorch_gradient_post_training_quantization
 from model_compression_toolkit.gptq.pytorch.quantization_facade import get_pytorch_gptq_config
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_config.py`

 * *Files 27% similar despite different names*

```diff
@@ -10,99 +10,98 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from enum import Enum
 from typing import Callable, Any, Dict
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core import common
-from model_compression_toolkit.gptq.common.gptq_constants import QUANT_PARAM_LEARNING_STR, MAX_LSB_STR, REG_DEFAULT
+from model_compression_toolkit.gptq.common.gptq_constants import REG_DEFAULT
 
 
 class RoundingType(Enum):
     """
-    An enum for choosing the GPTQ rounding methods
-    0. STRAIGHT-THROUGH ESTIMATOR
-    1. SoftQuantizer
+    An enum for choosing the GPTQ rounding methods:
+
+    STE - STRAIGHT-THROUGH ESTIMATOR
+
+    SoftQuantizer - SoftQuantizer
+
     """
     STE = 0
     SoftQuantizer = 1
 
 
-class GPTQHessianWeightsConfig:
+class GPTQHessianScoresConfig:
     """
-    Configuration to use for computing the Hessian-based weights for GPTQ loss metric.
+    Configuration to use for computing the Hessian-based scores for GPTQ loss metric.
     """
 
     def __init__(self,
                  hessians_num_samples: int = 16,
-                 norm_weights: bool = True,
+                 norm_scores: bool = True,
                  log_norm: bool = True,
-                 scale_log_norm: bool = False,
-                 hessians_n_iter: int = 50):
+                 scale_log_norm: bool = False):
 
         """
         Initialize a GPTQHessianWeightsConfig.
 
         Args:
-            hessians_num_samples (int): Number of samples to use for computing the Hessian-based weights.
-            norm_weights (bool): Whether to normalize the returned weights (to get values between 0 and 1).
-            log_norm (bool): Whether to use log normalization to the GPTQ Hessian-based weights.
-            scale_log_norm (bool): Whether to scale the final vector of the Hessian weights.
-            hessians_n_iter (int): Number of random iterations to run Hessian approximation for GPTQ weights.
+            hessians_num_samples (int): Number of samples to use for computing the Hessian-based scores.
+            norm_scores (bool): Whether to normalize the returned scores of the weighted loss function (to get values between 0 and 1).
+            log_norm (bool): Whether to use log normalization for the GPTQ Hessian-based scores.
+            scale_log_norm (bool): Whether to scale the final vector of the Hessian-based scores.
         """
 
         self.hessians_num_samples = hessians_num_samples
-        self.norm_weights = norm_weights
+        self.norm_scores = norm_scores
         self.log_norm = log_norm
         self.scale_log_norm = scale_log_norm
-        self.hessians_n_iter = hessians_n_iter
 
 
 class GradientPTQConfig:
     """
-    Configuration to use for quantization with GradientPTQ (experimental).
+    Configuration to use for quantization with GradientPTQ.
     """
-
-    def __init__(self, n_iter: int,
+    def __init__(self,
+                 n_epochs: int,
                  optimizer: Any,
                  optimizer_rest: Any = None,
                  loss: Callable = None,
                  log_function: Callable = None,
                  train_bias: bool = True,
                  rounding_type: RoundingType = RoundingType.SoftQuantizer,
                  use_hessian_based_weights: bool = True,
                  optimizer_quantization_parameter: Any = None,
                  optimizer_bias: Any = None,
                  regularization_factor: float = REG_DEFAULT,
-                 hessian_weights_config: GPTQHessianWeightsConfig = GPTQHessianWeightsConfig(),
+                 hessian_weights_config: GPTQHessianScoresConfig = GPTQHessianScoresConfig(),
                  gptq_quantizer_params_override: Dict[str, Any] = None):
         """
         Initialize a GradientPTQConfig.
 
         Args:
-            n_iter (int): Number of iterations to train.
+            n_epochs (int): Number of representative dataset epochs to train.
             optimizer (Any): Optimizer to use.
             optimizer_rest (Any): Optimizer to use for bias and quantizer parameters.
             loss (Callable): The loss to use. should accept 6 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors,
              the 3rd is a list of quantized weights, the 4th is a list of float weights, the 5th and 6th lists are the mean and std of the tensors
              accordingly. see example in multiple_tensors_mse_loss
             log_function (Callable): Function to log information about the GPTQ process.
             train_bias (bool): Whether to update the bias during the training or not.
             rounding_type (RoundingType): An enum that defines the rounding type.
             use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
             optimizer_quantization_parameter (Any): Optimizer to override the rest optimizer  for quantizer parameters.
             optimizer_bias (Any): Optimizer to override the rest optimizer for bias.
             regularization_factor (float): A floating point number that defines the regularization factor.
-            hessian_weights_config (GPTQHessianWeightsConfig): A configuration that include all necessary arguments to run a computation of Hessian weights for the GPTQ loss.
+            hessian_weights_config (GPTQHessianScoresConfig): A configuration that include all necessary arguments to run a computation of Hessian scores for the GPTQ loss.
             gptq_quantizer_params_override (dict): A dictionary of parameters to override in GPTQ quantizer instantiation. Defaults to None (no parameters).
 
         """
-        self.n_iter = n_iter
+
+        self.n_epochs = n_epochs
         self.optimizer = optimizer
         self.optimizer_rest = optimizer_rest
         self.loss = loss
         self.log_function = log_function
         self.train_bias = train_bias
 
         self.rounding_type = rounding_type
@@ -112,75 +111,7 @@
         self.regularization_factor = regularization_factor
         self.hessian_weights_config = hessian_weights_config
 
         self.gptq_quantizer_params_override = {} if gptq_quantizer_params_override is None \
             else gptq_quantizer_params_override
 
 
-class GradientPTQConfigV2(GradientPTQConfig):
-    """
-    Configuration to use for quantization with GradientPTQV2 (experimental).
-    """
-    def __init__(self, n_epochs: int,
-                 optimizer: Any,
-                 optimizer_rest: Any = None,
-                 loss: Callable = None,
-                 log_function: Callable = None,
-                 train_bias: bool = True,
-                 rounding_type: RoundingType = RoundingType.SoftQuantizer,
-                 use_hessian_based_weights: bool = True,
-                 optimizer_quantization_parameter: Any = None,
-                 optimizer_bias: Any = None,
-                 regularization_factor: float = REG_DEFAULT,
-                 hessian_weights_config: GPTQHessianWeightsConfig = GPTQHessianWeightsConfig(),
-                 gptq_quantizer_params_override: Dict[str, Any] = None):
-        """
-        Initialize a GradientPTQConfigV2.
-
-        Args:
-            n_epochs (int): Number of representative dataset epochs to train.
-            optimizer (Any): Optimizer to use.
-            optimizer_rest (Any): Optimizer to use for bias and quantizer parameters.
-            loss (Callable): The loss to use. should accept 6 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors,
-             the 3rd is a list of quantized weights, the 4th is a list of float weights, the 5th and 6th lists are the mean and std of the tensors
-             accordingly. see example in multiple_tensors_mse_loss
-            log_function (Callable): Function to log information about the GPTQ process.
-            train_bias (bool): Whether to update the bias during the training or not.
-            rounding_type (RoundingType): An enum that defines the rounding type.
-            use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
-            optimizer_quantization_parameter (Any): Optimizer to override the rest optimizer  for quantizer parameters.
-            optimizer_bias (Any): Optimizer to override the rest optimizerfor bias.
-            regularization_factor (float): A floating point number that defines the regularization factor.
-            hessian_weights_config (GPTQHessianWeightsConfig): A configuration that include all necessary arguments to run a computation of Hessian weights for the GPTQ loss.
-            gptq_quantizer_params_override (dict): A dictionary of parameters to override in GPTQ quantizer instantiation. Defaults to None (no parameters).
-
-        """
-
-        super().__init__(n_iter=None,
-                         optimizer=optimizer,
-                         optimizer_rest=optimizer_rest,
-                         loss=loss,
-                         log_function=log_function,
-                         train_bias=train_bias,
-                         rounding_type=rounding_type,
-                         use_hessian_based_weights=use_hessian_based_weights,
-                         optimizer_quantization_parameter=optimizer_quantization_parameter,
-                         optimizer_bias=optimizer_bias,
-                         regularization_factor=regularization_factor,
-                         hessian_weights_config=hessian_weights_config,
-                         gptq_quantizer_params_override=gptq_quantizer_params_override)
-        self.n_epochs = n_epochs
-
-    @classmethod
-    def from_v1(cls, n_ptq_iter: int, config_v1: GradientPTQConfig):
-        """
-        Initialize a GradientPTQConfigV2 from GradientPTQConfig instance.
-
-        Args:
-            n_ptq_iter (int): Number of PTQ calibration iters (length of representative dataset).
-            config_v1 (GradientPTQConfig): A GPTQ config to convert to V2.
-
-        """
-        n_epochs = int(round(config_v1.n_iter) / n_ptq_iter)
-        v1_params = config_v1.__dict__
-        v1_params = {k: v for k, v in v1_params.items() if k != 'n_iter'}
-        return cls(n_epochs, **v1_params)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_constants.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_framework_implementation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_framework_implementation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_graph.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/common/gptq_graph.py`

 * *Files 7% similar despite different names*

```diff
@@ -35,15 +35,17 @@
         A list of nodes std collected from BatchNorms in the graph
     """
     compare_points = []
     compare_points_mean = []
     compare_points_std = []
     compare_points_name = []
     for n in input_graph.get_topo_sorted_nodes():
-        if len(n.weights) > 0 and n.is_weights_quantization_enabled() and not n.reuse:
+        # only nodes with kernel attribute are currently trained with GPTQ and are used as compare points
+        kernel_attr = input_graph.fw_info.get_kernel_op_attributes(n.type)[0]
+        if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr) and not n.reuse:
             compare_points.append(n)
             compare_points_name.append(n.name)
             compare_points_std.append(n.prior_info.std_output)
             compare_points_mean.append(n.prior_info.mean_output)
     return compare_points, compare_points_name, compare_points_mean, compare_points_std
 
 
@@ -56,11 +58,11 @@
         fw_info: A FrameworkInfo object.
 
     Returns: The name of the kernel attribute.
 
     """
     kernel_attribute = fw_info.get_kernel_op_attributes(layer_type)
     if len(kernel_attribute) != 1:
-        Logger.error(  # pragma: no cover
-            f"In GPTQ training only the kernel weights attribute should be trained, but number of kernel "
-            f"attributes is {len(kernel_attribute)}.")
+        Logger.critical(  # pragma: no cover
+            f"In GPTQ training, only the kernel weights attribute should be trained. "
+            f"However, the number of kernel attributes is {len(kernel_attribute)}.")
     return kernel_attribute[0]
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/common/gptq_training.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/gptq_training.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,288 +8,306 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import copy
-from abc import ABC, abstractmethod
+from typing import Callable, List, Tuple, Union
+
 import numpy as np
-from typing import Callable, List, Any
+from torch.nn import Module
+from tqdm import tqdm
+import copy
+import torch
+
+from model_compression_toolkit.core.common.hessian import HessianInfoService
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
+from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
 from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.gptq.common.gptq_constants import QUANT_PARAM_LEARNING_STR
-from model_compression_toolkit.gptq.common.gptq_framework_implementation import GPTQFrameworkImplemantation
-from model_compression_toolkit.gptq.common.gptq_graph import get_compare_points
-from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
-from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.pytorch.constants import BIAS
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, set_model, torch_tensor_to_numpy
+from model_compression_toolkit.gptq.pytorch.graph_info import get_gptq_trainable_parameters, \
+    get_weights_for_loss
+from model_compression_toolkit.gptq.pytorch.quantizer.quantization_builder import quantization_builder
+from model_compression_toolkit.gptq.pytorch.quantizer.regularization_factory import get_regularization
+from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
 
 
-class GPTQTrainer(ABC):
+class PytorchGPTQTrainer(GPTQTrainer):
     """
-    Abstract GPTQ training class for fine-tuning a quantized model
+    Pytorch GPTQ training class for fine-tuning a quantized model
     """
 
     def __init__(self,
                  graph_float: Graph,
                  graph_quant: Graph,
                  gptq_config: GradientPTQConfig,
-                 fw_impl: GPTQFrameworkImplemantation,
-                 fw_info: FrameworkInfo):
+                 fw_impl: FrameworkImplementation,
+                 fw_info: FrameworkInfo,
+                 representative_data_gen: Callable,
+                 hessian_info_service: HessianInfoService = None):
         """
         Build two models from a graph: A teacher network (float model) and a student network (quantized model).
         Use the dataset generator to pass images through the teacher and student networks to get intermediate
         layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
         in the student network, to minimize it in the next similar steps.
         All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
         Args:
             graph_float: Graph to build a float networks from.
             graph_quant: Graph to build a quantized networks from.
-            gptq_config: GradientPTQConfig with parameters about the tuning process.
-            fw_impl: Framework implementation
+            gptq_config: GradientPTQConfigV2 with parameters about the tuning process.
+            fw_impl: FrameworkImplementation object with a specific framework methods implementation.
             fw_info: Framework information
+            representative_data_gen: Dataset to use for inputs of the models.
+            hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
         """
-        self.graph_float = copy.deepcopy(graph_float)
-        self.graph_quant = copy.deepcopy(graph_quant)
-        self.gptq_config = gptq_config
-        self.fw_impl = fw_impl
-        self.fw_info = fw_info
-
-        # ----------------------------------------------
-        # Build two models and create compare nodes
-        # ----------------------------------------------
-        self.compare_points, _, self.compare_points_mean, self.compare_points_std = get_compare_points(self.graph_float)
-
-        self.float_model, self.float_user_info = fw_impl.model_builder(self.graph_float,
-                                                                       mode=ModelBuilderMode.FLOAT,
-                                                                       append2output=self.compare_points,
-                                                                       fw_info=self.fw_info)
-
-        self.fxp_model, self.gptq_user_info = self.build_gptq_model()
-
-    def get_optimizer_with_param(self,
-                                 flattened_trainable_weights: List[Any],
-                                 flattened_bias_weights: List[Any],
-                                 trainable_quantization_parameters: List[Any]) -> List[Any]:
-        """
-        Create Optimizers with their trainable parameters
-        Args:
-            flattened_trainable_weights: list of trainable weights parameters (flattened)
-            flattened_bias_weights: list of trainable bias parameters (flattened)
-            trainable_quantization_parameters: list of trainable quantization parameters
-        Returns:
-            List of Optimizer objects with parameters
-        """
+        super().__init__(graph_float,
+                         graph_quant,
+                         gptq_config,
+                         fw_impl,
+                         fw_info,
+                         hessian_info_service=hessian_info_service)
+
+        self.loss_list = []
+        self.input_scale = 1
+        if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
+            Logger.critical("Input scale mismatch between float and GPTQ networks. "
+                            "Ensure both networks have matching input scales.")  # pragma: no cover
+        else:
+            self.input_scale = self.gptq_user_info.input_scale
 
-        w2train = [*flattened_trainable_weights]
+        trainable_weights, trainable_bias, trainable_threshold = get_gptq_trainable_parameters(
+            self.fxp_model,
+            add_bias=self.gptq_config.train_bias)
 
-        quant_params_learning = self.gptq_config.gptq_quantizer_params_override.get(QUANT_PARAM_LEARNING_STR, False)
+        self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
+        if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
+                self.fxp_weights_list)):
+            Logger.critical("GPTQ: Number of comparison points, layers with trainable weights, "
+                            "and float vs. quantized weights for loss calculation do not match. "
+                            "Verify consistency across these parameters for successful GPTQ training.")
 
-        optimizer_with_param = [(self.gptq_config.optimizer, w2train)]
-        if self.gptq_config.train_bias or quant_params_learning:
-            w2train_res = []
-            if self.gptq_config.train_bias:
-                if self.gptq_config.optimizer_bias is not None:
-                    optimizer_with_param.append((self.gptq_config.optimizer_bias, flattened_bias_weights))
-                else:
-                    w2train_res.extend(flattened_bias_weights)
-                    if self.gptq_config.optimizer_rest is None:
-                        Logger.error(  # pragma: no cover
-                            "To enable bias micro training an additional optimizer is required, please define the optimizer_rest")
-            if quant_params_learning:
-                if self.gptq_config.optimizer_quantization_parameter is not None:  # Ability to override optimizer
-                    optimizer_with_param.append((self.gptq_config.optimizer_quantization_parameter,
-                                                 trainable_quantization_parameters))
-                else:
-                    w2train_res.extend(trainable_quantization_parameters)
-                if self.gptq_config.optimizer_rest is None:
-                    Logger.error(  # pragma: no cover
-                        "To enable quantization parameters micro training an additional optimizer is required, please define the optimizer_rest")
-            if len(w2train_res) > 0:
-                # Either bias or quantization parameters are trainable but did not provide a specific optimizer,
-                # so we should use optimizer_rest to train them
-                if self.gptq_config.optimizer_rest is None:
-                    Logger.error(  # pragma: no cover
-                        "To enable bias or quantization parameters micro training an additional optimizer is required, please define the optimizer_rest")
-                optimizer_with_param.append((self.gptq_config.optimizer_rest, w2train_res))
+        self.optimizer_with_param = self.get_optimizer_with_param(trainable_weights,
+                                                                  trainable_bias,
+                                                                  trainable_threshold)
 
-        return optimizer_with_param
+        self.weights_for_average_loss = to_torch_tensor(self.compute_hessian_based_weights())
 
+        self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
-    def compute_hessian_based_weights(self,
-                                      representative_data_gen: Callable) -> np.ndarray:
+    def _is_gptq_weights_trainable(self,
+                                   node: BaseNode) -> bool:
         """
-        Computes the Hessian-based weights using the framework's model_grad method per batch of images.
+        A function for deciding if a layer should be fine-tuned during GPTQ.
 
         Args:
-            representative_data_gen: Dataset used for inference to compute the Hessian-based weights.
+            node (BaseNode): Node for quantization decision
 
-        Returns: A vector of weights, one for each compare point,
-        to be used for the loss metric weighted average computation when running GPTQ training.
+        Returns:
+            A boolean whether the layer is to be wrapped with a Quantization Wrapper.
         """
-        if self.gptq_config.use_hessian_based_weights:
-            images = self._generate_images_batch(representative_data_gen,
-                                                 self.gptq_config.hessian_weights_config.hessians_num_samples)
-
-            model_output_replacement = self._get_model_output_replacement()
-
-            points_apprx_jacobians_weights = []
-            for i in range(1, images.shape[0] + 1):
-                Logger.info(f"Computing Jacobian-based weights approximation for image sample {i} out of {images.shape[0]}...")
-                # Note that in GPTQ loss weights computation we assume that there aren't replacement output nodes,
-                # therefore, output_list is just the graph outputs, and we don't need the tuning factor for
-                # defining the output weights (since the output layer is not a compare point).
-                image_ip_gradients = self.fw_impl.model_grad(self.graph_float,
-                                                             {inode: self.fw_impl.to_tensor(images[i - 1:i]) for inode
-                                                              in
-                                                              self.graph_float.get_inputs()},
-                                                             self.compare_points,
-                                                             output_list=model_output_replacement,
-                                                             all_outputs_indices=[],
-                                                             alpha=0,
-                                                             norm_weights=self.gptq_config.hessian_weights_config.norm_weights,
-                                                             n_iter=self.gptq_config.hessian_weights_config.hessians_n_iter)
-                points_apprx_jacobians_weights.append(image_ip_gradients)
-            if self.gptq_config.hessian_weights_config.log_norm:
-                mean_jacobian_weights = np.mean(points_apprx_jacobians_weights, axis=0)
-                mean_jacobian_weights = np.where(mean_jacobian_weights != 0, mean_jacobian_weights,
-                                                 np.partition(mean_jacobian_weights, 1)[1])
-                log_weights = np.log10(mean_jacobian_weights)
-
-                if self.gptq_config.hessian_weights_config.scale_log_norm:
-                    return (log_weights - np.min(log_weights)) / (np.max(log_weights) - np.min(log_weights))
-
-                return log_weights - np.min(log_weights)
-            else:
-                return np.mean(points_apprx_jacobians_weights, axis=0)
-        else:
-            num_nodes = len(self.compare_points)
-            return np.asarray([1 / num_nodes for _ in range(num_nodes)])
 
-    @staticmethod
-    def _generate_images_batch(representative_data_gen: Callable, num_samples_for_loss: int) -> np.ndarray:
+        kernel_attr = self.fw_info.get_kernel_op_attributes(node.type)[0]
+        return kernel_attr is not None and node.is_weights_quantization_enabled(kernel_attr)
+
+    def gptq_wrapper(self,
+                     n: BaseNode,
+                     layer: Module) -> Union[PytorchQuantizationWrapper, Module]:
         """
-        Construct batches of image samples for inference.
+        A function which takes a computational graph node and a pytorch layer and perform the quantization wrapping.
 
         Args:
-            representative_data_gen: A callable method to retrieve images from Dataset.
-            num_samples_for_loss: Num of total images for evaluation.
+            n: A node of mct graph.
+            layer: A pytorch layer
 
-        Returns: A tensor of images batches
+        Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
         """
-        # First, select images to use for all measurements.
-        samples_count = 0  # Number of images we used so far to compute the distance matrix.
-        images = []
-        for inference_batch_input in representative_data_gen():
-            if samples_count >= num_samples_for_loss:
-                break
-            num_images = inference_batch_input[0].shape[0]
-
-            # If we sampled more images than we should,
-            # we take only a subset of these images and use only them.
-            if num_images > num_samples_for_loss - samples_count:
-                inference_batch_input = [x[:num_samples_for_loss - samples_count] for x in inference_batch_input]
-                assert num_samples_for_loss - samples_count == inference_batch_input[0].shape[0]
-                num_images = num_samples_for_loss - samples_count
 
-            images.append(inference_batch_input[0])
-            samples_count += num_images
-        else:
-            if samples_count < num_samples_for_loss:
-                Logger.warning(f'Not enough images in representative dataset to generate {num_samples_for_loss} data points, '
-                               f'only {samples_count} were generated')
+        if self._is_gptq_weights_trainable(n):
+            # If we are here, then the node has a kernel attribute to quantize and training during GPTQ
+            weights_quantizers, _ = quantization_builder(n,
+                                                         self.gptq_config,
+                                                         self.fw_info.get_kernel_op_attributes(n.type)[0])
 
-        return np.concatenate(images, axis=0)
+            if len(weights_quantizers) > 0:
+                return PytorchQuantizationWrapper(layer,
+                                                  weights_quantizers=weights_quantizers)
 
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
+        return layer
+
+    def get_activation_quantizer_holder(self, n: BaseNode) -> Callable:
+        """
+        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
+        If the layer is not supposed to be wrapped with an activation quantizer - return None.
+        Args:
+            n: Node to attach a PytorchActivationQuantizationHolder to its output.
+        Returns:
+            A PytorchActivationQuantizationHolder module for the node's activation quantization.
+        """
+        _, activation_quantizers = quantization_builder(n, self.gptq_config)
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node we no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return PytorchActivationQuantizationHolder(activation_quantizers[0])
+        Logger.critical(f"'PytorchActivationQuantizationHolder' requires exactly one quantizer, "
+                        f"but {len(activation_quantizers)} were found for node {n.name}. "
+                        f"Ensure the node is configured with a single activation quantizer.")
 
-    @abstractmethod
     def build_gptq_model(self):
         """
         Build the GPTQ model with QuantizationWrappers
         Returns:
             Quantized graph for GPTQ fine-tuning, GPTQ graph user info
         """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s GPTQ model builder method.')  # pragma: no cover
+        gptq_model, gptq_user_info = PyTorchModelBuilder(graph=self.graph_quant,
+                                                         append2output=self.compare_points,
+                                                         fw_info=self.fw_info,
+                                                         wrapper=self.gptq_wrapper,
+                                                         return_float_outputs=True,
+                                                         get_activation_quantizer_holder_fn=self.get_activation_quantizer_holder).build_model()
+
+        return gptq_model, gptq_user_info
 
-    @abstractmethod
     def train(self, representative_data_gen: Callable):
         """
-        Train the quantized model using GPTQ training process
-        Args:
-            representative_data_gen: Dataset to use for inputs of the models.
-        """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s train method.')  # pragma: no cover
+          GPTQ Training using pytorch framework
+          Args:
+              representative_data_gen: Dataset generator to get images.
+          Returns:
+              Graph after GPTQ training
+          """
+        # Set Optimizers
+        for (optimizer, params) in self.optimizer_with_param:
+            optimizer.param_groups.clear()
+            optimizer.add_param_group({'params': params})
+
+        # Set models mode
+        set_model(self.float_model, False)
+        set_model(self.fxp_model, True)
+        self._set_requires_grad()
 
-    @abstractmethod
-    def update_graph(self) -> Graph:
+        # ----------------------------------------------
+        # Training loop
+        # ----------------------------------------------
+        self.micro_training_loop(representative_data_gen, self.gptq_config.n_epochs)
+
+    def compute_gradients(self,
+                          y_float: List[torch.Tensor],
+                          input_tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, List[np.ndarray]]:
         """
-        Update a graph using GPTQ after minimizing the loss between the float model's output
-        and the quantized model's outputs.
+        Get outputs from both teacher and student networks. Compute the observed error,
+        and use it to compute the gradients and applying them to the student weights.
+        Args:
+            y_float: A list of reference tensor from the floating point network.
+            input_tensors: A list of Input tensors to pass through the networks.
         Returns:
-            Updated graph after GPTQ.
+            Loss and gradients.
         """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s update_graph method.')  # pragma: no cover
 
-    def _get_model_output_replacement(self) -> List[BaseNode]:
-        """
-        If a model's output node is not compatible for the task of gradients computation we need to find a predecessor
-        node in the model's graph representation which is compatible and use it for the gradients' computation.
-        This method searches for this predecessor node for each output of the model.
+        # Forward-pass
+        y_fxp = self.fxp_model(input_tensors)
 
-        Returns: A list of output replacement nodes.
+        # Loss
+        loss_value = self.gptq_config.loss(y_fxp,
+                                           y_float,
+                                           self.fxp_weights_list,
+                                           self.flp_weights_list,
+                                           self.compare_points_mean,
+                                           self.compare_points_std,
+                                           self.weights_for_average_loss)
 
-        """
+        reg_value = self.reg_func(self.fxp_model, self.gptq_config.regularization_factor)
 
-        replacement_outputs = []
-        for n in self.graph_float.get_outputs():
-            prev_node = n.node
-            while not self.fw_impl.is_node_compatible_for_metric_outputs(prev_node):
-                prev_node = self.graph_float.get_prev_nodes(n.node)
-                assert len(prev_node) == 1, "A none compatible output node has multiple inputs, " \
-                                            "which is incompatible for metric computation."
-                prev_node = prev_node[0]
-            replacement_outputs.append(prev_node)
-        return replacement_outputs
+        loss_value += reg_value
 
+        # Back-pass
+        loss_value.backward()
 
-def gptq_training(graph_float: Graph,
-                  graph_quant: Graph,
-                  gptq_config: GradientPTQConfig,
-                  representative_data_gen: Callable,
-                  fw_impl: GPTQFrameworkImplemantation,
-                  fw_info: FrameworkInfo) -> Graph:
-    """
-    GPTQ training process using knowledge distillation with a teacher network (float model) and a student network (quantized model).
-    Args:
-        graph_float: Graph to build a float networks from.
-        graph_quant: Graph to build a quantized networks from.
-        gptq_config: GradientPTQConfig with parameters about the tuning process.
-        representative_data_gen: Dataset to use for inputs of the models.
-        fw_impl: Framework implementation
-        fw_info: Framework information
+        # Get gradients
+        grads = []
+        for param in self.fxp_model.parameters():
+            if param.requires_grad and param.grad is not None:
+                grads.append(torch_tensor_to_numpy(param.grad))
 
-    Returns:
-        Quantized graph for export
+        return loss_value, grads
 
-    """
-    # Get GPTQ object and initialize it
-    gptq_trainer_obj = fw_impl.get_gptq_trainer_obj()
-
-    gptq_trainer = gptq_trainer_obj(graph_float,
-                                    graph_quant,
-                                    gptq_config,
-                                    fw_impl,
-                                    fw_info,
-                                    representative_data_gen)
-
-    # Training process
-    gptq_trainer.train(representative_data_gen)
+    def micro_training_loop(self,
+                            data_function: Callable,
+                            n_epochs: int):
+        """
+        This function run a micro training loop on given set of parameters.
+        Args:
+            data_function: A callable function that give a batch of samples.
+            n_epochs: Number of update iterations of representative dataset.
+        """
+        for _ in tqdm(range(n_epochs)):
+            for data in tqdm(data_function()):
+                input_data = [d * self.input_scale for d in data]
+                input_tensor = to_torch_tensor(input_data)
+                y_float = self.float_model(input_tensor)  # running float model
+                loss_value, grads = self.compute_gradients(y_float, input_tensor)
+                # Run one step of gradient descent by updating the value of the variables to minimize the loss.
+                for (optimizer, _) in self.optimizer_with_param:
+                    optimizer.step()
+                    optimizer.zero_grad()
+                if self.gptq_config.log_function is not None:
+                    self.gptq_config.log_function(loss_value.item(),
+                                                  torch_tensor_to_numpy(grads),
+                                                  torch_tensor_to_numpy(self.optimizer_with_param[0][-1]))
+                self.loss_list.append(loss_value.item())
+                Logger.debug(f'last loss value: {self.loss_list[-1]}')
 
-    # Update graph
-    graph_quant = gptq_trainer.update_graph()
+    def update_graph(self) -> Graph:
+        """
+        Update a graph using GPTQ after minimizing the loss between the float model's output
+        and the quantized model's outputs.
+        Returns:
+            Updated graph after GPTQ.
+        """
+        graph_quant = copy.copy(self.graph_quant)
 
-    return graph_quant
+        # Update graph after training
+        for name, layer in self.fxp_model.named_modules():
+            if isinstance(layer, PytorchQuantizationWrapper):
+                node = self.graph_quant.find_node_by_name(name)
+                if len(node) != 1:
+                    Logger.critical(f"Cannot update GPTQ graph: Layer with name '{name}' is missing or not unique. "
+                                    f"Ensure each layer has a unique name and exists within the graph for updates.")
+                node = node[0]
+                kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
+                                                                      fw_info=self.fw_info)
+                weights, weight_quant_config, activation_quant_config = \
+                    layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
+                for weight_attr, weight in weights.items():
+                    node.set_weights_by_keys(weight_attr, self.fw_impl.to_numpy(weight))
+                for config_attr, config_value in weight_quant_config.items():
+                    node.final_weights_quantization_cfg.set_quant_config_attr(config_attr, config_value)
+                for config_attr, config_value in activation_quant_config.items():
+                    node.final_activation_quantization_cfg.set_quant_config_attr(config_attr, config_value)
+                if self.gptq_config.train_bias and hasattr(layer.layer, BIAS):
+                    node.set_weights_by_keys(BIAS, self.fw_impl.to_numpy(getattr(layer.layer, BIAS)))
+
+        return graph_quant
+
+    def _set_requires_grad(self):
+        """
+        Set require_grad flag for trainable parameters for GPTQ training
+        """
+        # Float model: freeze all the parameters in the network
+        for param in self.float_model.parameters():
+            param.requires_grad = False
+
+        # Fxp model: unfreeze bias trainable parameters
+        for layer in self.fxp_model.modules():
+            if isinstance(layer, PytorchQuantizationWrapper):
+                if hasattr(layer.layer, BIAS):
+                    bias = getattr(layer.layer, BIAS)
+                    bias.requires_grad = self.gptq_config.train_bias
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/gptq_loss.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/gptq_loss.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/gptq_training.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/gptq_training.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 
 import tensorflow as tf
 from keras import Model
 from packaging import version
 from tensorflow.keras.layers import Layer
 from tqdm import tqdm
 
+from model_compression_toolkit.core.common.hessian import HessianInfoService
 # As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
 from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
 from model_compression_toolkit.gptq.keras.quantizer.quantization_builder import quantization_builder
 from model_compression_toolkit.logger import Logger
 from mct_quantizers import KerasActivationQuantizationHolder
@@ -32,15 +33,15 @@
     from keras.src.engine.base_layer import TensorFlowOpLayer
 else:
     from keras.engine.base_layer import TensorFlowOpLayer
 
 from model_compression_toolkit.trainable_infrastructure import KerasTrainableQuantizationWrapper
 from model_compression_toolkit.core import common
 from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.gptq.keras.graph_info import get_weights_for_loss, get_gptq_trainable_parameters
 from model_compression_toolkit.gptq.keras.quantizer.regularization_factory import get_regularization
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 import numpy as np
 import copy
@@ -51,88 +52,90 @@
     """
     Keras GPTQ training class for fine-tuning a quantized model
     """
 
     def __init__(self,
                  graph_float: Graph,
                  graph_quant: Graph,
-                 gptq_config: GradientPTQConfigV2,
+                 gptq_config: GradientPTQConfig,
                  fw_impl: FrameworkImplementation,
                  fw_info: FrameworkInfo,
-                 representative_data_gen: Callable):
+                 representative_data_gen: Callable,
+                 hessian_info_service: HessianInfoService = None):
         """
         Build two models from a graph: A teacher network (float model) and a student network (quantized model).
         Use the dataset generator to pass images through the teacher and student networks to get intermediate
         layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
         in the student network, to minimize it in the next similar steps.
         All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
         Args:
             graph_float: Graph to build a float networks from.
             graph_quant: Graph to build a quantized networks from.
             gptq_config: GradientPTQConfig with parameters about the tuning process.
             fw_impl: FrameworkImplementation object with a specific framework methods implementation.
             fw_info: Framework information.
             representative_data_gen: Dataset to use for inputs of the models.
+            hessian_info_service: HessianInfoService for fetching and computing Hessian's trace approximation.
+
         """
         super().__init__(graph_float,
                          graph_quant,
                          gptq_config,
                          fw_impl,
-                         fw_info)
+                         fw_info,
+                         hessian_info_service=hessian_info_service)
 
         self.loss_list = []
         self.input_scale = 1
 
         trainable_weights, bias_weights, trainable_threshold = get_gptq_trainable_parameters(
             self.fxp_model,
             fw_info,
             add_bias=gptq_config.train_bias)
 
         self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
 
         if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
                 self.fxp_weights_list)):
-            raise Exception(
-                "GPTQ: Mismatch between number of compare points, number of layers with trainable weights " +
-                "and number of float and quantized weights for loss")
+            Logger.critical("Mismatch in the number of comparison points, layers with trainable weights, "
+                            "and the number of float and quantized weights for loss calculation. "
+                            "Ensure all these elements align to proceed with GPTQ training.")
 
         flattened_trainable_weights = [w for layer_weights in trainable_weights for w in layer_weights]
         flattened_bias_weights = [w for layer_weights in bias_weights for w in layer_weights]
         trainable_quantization_parameters = trainable_threshold
         self.optimizer_with_param = self.get_optimizer_with_param(flattened_trainable_weights,
                                                                   flattened_bias_weights,
                                                                   trainable_quantization_parameters)
         self.has_params_to_train = np.sum(
             [len(optimizer_params_tuple[1]) for optimizer_params_tuple in self.optimizer_with_param]) > 0
 
         if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
-            Logger.error("Input scale mismatch between float and GPTQ networks")  # pragma: no cover
+            Logger.critical("Input scale mismatch detected between the float model and the GPTQ model. "
+                            "Confirm that the input scales for both models are correctly configured and aligned.")  # pragma: no cover
         else:
             self.input_scale = self.gptq_user_info.input_scale
 
-        self.weights_for_average_loss = self.compute_hessian_based_weights(representative_data_gen)
+        self.weights_for_average_loss = self.compute_hessian_based_weights()
 
         self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
     def _is_gptq_weights_trainable(self,
                                    node: common.BaseNode) -> bool:
         """
         A function for deciding if a layer should be fine-tuned during GPTQ.
 
         Args:
             node (BaseNode): Node for quantization decision
 
         Returns:
             A boolean whether the layer is to be wrapped with a QuantizeWrapper
         """
-
-        if node.is_weights_quantization_enabled() and not self.fw_info.is_kernel_op(node.type):
-            Logger.error(f"GPTQ Error: Quantizing node {node.name} of type {node.type} "
-                                f"without a kernel isn't supported")
-        return node.is_weights_quantization_enabled()
+        kernel_attr = self.fw_info.get_kernel_op_attributes(node.type)[0]
+        return kernel_attr is not None and node.is_weights_quantization_enabled(kernel_attr)
 
     def gptq_wrapper(self,
                      n: common.BaseNode,
                      layer: Layer) -> Union[KerasTrainableQuantizationWrapper, Layer]:
         """
         A function which takes a computational graph node and a keras layer and perform the quantization wrapping.
 
@@ -140,19 +143,24 @@
             n: A node of mct graph.
             layer: A keras layer
 
         Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
 
         """
         if self._is_gptq_weights_trainable(n):
+            # If we are here, then the node has a kernel attribute to quantize and training during GPTQ
             weights_quantizers, _ = quantization_builder(n,
-                                                         self.gptq_config) # TODO: split quantizers building into two functions: for weights and activations
+                                                         self.gptq_config,  # TODO: split quantizers building into two functions: for weights and activations
+                                                         self.fw_info.get_kernel_op_attributes(n.type)[0])
             if len(weights_quantizers) > 0:
                 return KerasTrainableQuantizationWrapper(layer,
-                                                   weights_quantizers=weights_quantizers)
+                                                         weights_quantizers=weights_quantizers)
+
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
         return layer
 
     def get_activation_quantizer_holder(self, n: common.BaseNode) -> Callable:
         """
         Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
         If the layer is not supposed to be wrapped with activation quantizers - return None.
 
@@ -166,17 +174,17 @@
 
         # Holder by definition uses a single quantizer for the activation quantization
         # thus we make sure this is the only possible case (unless it's a node with no activation
         # quantization, which in this case has an empty list).
         if len(activation_quantizers) == 1:
             return KerasActivationQuantizationHolder(activation_quantizers[0])
 
-        Logger.error(
-            f'KerasActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
-            f'were found for node {n}')
+        Logger.critical(f"'KerasActivationQuantizationHolder' is designed to support a single quantizer, "
+                        f"but {len(activation_quantizers)} quantizers were found for node '{n}'. "
+                        f"Ensure only one quantizer is configured for each node's activation.")
 
 
     def build_gptq_model(self) -> Tuple[Model, UserInformation]:
         """
         Build the GPTQ model with QuantizationWrappers
 
         Returns:
@@ -320,15 +328,16 @@
 
         for layer in self.fxp_model.layers:
             if isinstance(layer, KerasTrainableQuantizationWrapper):
                 node = graph.find_node_by_name(layer.layer.name)
                 if len(node) == 0 and isinstance(layer.layer, TensorFlowOpLayer):
                     node = graph.find_node_by_name('_'.join(layer.layer.name.split('_')[3:]))
                 if len(node) != 1:
-                    Logger.error(f"Can't update GPTQ graph due to missing layer named: {layer.layer.name}")
+                    Logger.critical(f"Unable to update the GPTQ graph because the layer named '{layer.layer.name}' could not be found. "
+                                    f"Verify that the layer names in the GPTQ model match those in the graph.")
                 node = node[0]
                 kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
                                                                       fw_info=self.fw_info)
                 weights, weight_quant_config, activation_quant_config = \
                     layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
                 for weight_attr, weight in weights.items():
                     node.set_weights_by_keys(weight_attr, weight.numpy())
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/graph_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/graph_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -48,15 +48,15 @@
     for layer in fxp_model.layers:
         if isinstance(layer, KerasTrainableQuantizationWrapper):
             kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
                                                                   fw_info=DEFAULT_KERAS_INFO)
 
             # collect trainable weights per quantizer
             if kernel_attribute not in layer.weights_quantizers:
-                Logger.error(f'{kernel_attribute} was not found in weight quantizers of layer {layer.layer}')
+                Logger.critical(f"'{kernel_attribute}' was not found in the weight quantizers of layer '{layer.layer}'.")
 
             quantizer_trainable_weights = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.WEIGHTS)
             quantizer_trainable_threshold = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.QPARAMS)
             trainable_weights.append(quantizer_trainable_weights)
             trainable_threshold.extend(quantizer_trainable_threshold)
 
             if add_bias:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantization_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantization_facade.py`

 * *Files 11% similar despite different names*

```diff
@@ -12,23 +12,25 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable, Tuple
 from packaging import version
 
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
+from model_compression_toolkit.gptq.common.gptq_constants import REG_DEFAULT
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
 from model_compression_toolkit.core import CoreConfig
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
+from model_compression_toolkit.core.runner import core_runner
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 
 LR_DEFAULT = 0.15
 LR_REST_DEFAULT = 1e-4
@@ -59,25 +61,27 @@
 
 
     def get_keras_gptq_config(n_epochs: int,
                               optimizer: OptimizerV2 = tf.keras.optimizers.Adam(learning_rate=LR_DEFAULT),
                               optimizer_rest: OptimizerV2 = tf.keras.optimizers.Adam(learning_rate=LR_REST_DEFAULT),
                               loss: Callable = GPTQMultipleTensorsLoss(),
                               log_function: Callable = None,
-                              use_hessian_based_weights: bool = True) -> GradientPTQConfigV2:
+                              use_hessian_based_weights: bool = True,
+                              regularization_factor: float = REG_DEFAULT) -> GradientPTQConfig:
         """
         Create a GradientPTQConfigV2 instance for Keras models.
 
         args:
             n_epochs (int): Number of epochs for running the representative dataset for fine-tuning.
             optimizer (OptimizerV2): Keras optimizer to use for fine-tuning for auxiliry variable with a default learning rate set to 0.2.
             optimizer_rest (OptimizerV2): Keras optimizer to use for fine-tuning of the bias variable.
             loss (Callable): loss to use during fine-tuning. should accept 4 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors, the 3rd is a list of quantized weights and the 4th is a list of float weights.
             log_function (Callable): Function to log information about the gptq process.
             use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
+            regularization_factor (float): A floating point number that defines the regularization factor.
 
         returns:
             a GradientPTQConfigV2 object to use when fine-tuning the quantized model using gptq.
 
         Examples:
 
             Import MCT and TensorFlow:
@@ -94,59 +98,55 @@
             >>> gptq_conf = mct.gptq.get_keras_gptq_config(n_epochs=3, optimizer=tf.keras.optimizers.Nadam())
 
             The configuration can be passed to :func:`~model_compression_toolkit.keras_post_training_quantization` in order to quantize a keras model using gptq.
 
         """
         bias_optimizer = tf.keras.optimizers.SGD(learning_rate=LR_BIAS_DEFAULT,
                                                  momentum=GPTQ_MOMENTUM)
-        return GradientPTQConfigV2(n_epochs,
-                                   optimizer,
-                                   optimizer_rest=optimizer_rest,
-                                   loss=loss,
-                                   log_function=log_function,
-                                   train_bias=True,
-                                   optimizer_bias=bias_optimizer,
-                                   use_hessian_based_weights=use_hessian_based_weights)
-
-
-    def keras_gradient_post_training_quantization_experimental(in_model: Model,
-                                                               representative_data_gen: Callable,
-                                                               gptq_config: GradientPTQConfigV2,
-                                                               gptq_representative_data_gen: Callable = None,
-                                                               target_kpi: KPI = None,
-                                                               core_config: CoreConfig = CoreConfig(),
-                                                               fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
-                                                               target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC,
-                                                               new_experimental_exporter: bool = True) -> Tuple[Model, UserInformation]:
+        return GradientPTQConfig(n_epochs,
+                                 optimizer,
+                                 optimizer_rest=optimizer_rest,
+                                 loss=loss,
+                                 log_function=log_function,
+                                 train_bias=True,
+                                 optimizer_bias=bias_optimizer,
+                                 use_hessian_based_weights=use_hessian_based_weights,
+                                 regularization_factor=regularization_factor)
+
+
+    def keras_gradient_post_training_quantization(in_model: Model, representative_data_gen: Callable,
+                                                  gptq_config: GradientPTQConfig,
+                                                  gptq_representative_data_gen: Callable = None,
+                                                  target_resource_utilization: ResourceUtilization = None,
+                                                  core_config: CoreConfig = CoreConfig(),
+                                                  target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC) -> Tuple[Model, UserInformation]:
         """
         Quantize a trained Keras model using post-training quantization. The model is quantized using a
         symmetric constraint quantization thresholds (power of two).
         The model is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
         For each possible bit width (per layer) a threshold is then being calculated using the collected
         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
         a mixed-precision configuration, and set a bit-width for each layer. The model is then quantized
         (both coefficients and activations by default).
-        In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
+        In order to limit the maximal model's size, a target resource utilization need to be passed after weights_memory
         is set (in bytes).
         Then, the quantized weights are optimized using gradient based post
         training quantization by comparing points between the float and quantized models, and minimizing the observed
         loss.
 
         Args:
             in_model (Model): Keras model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
-            gptq_config (GradientPTQConfigV2): Configuration for using gptq (e.g. optimizer).
+            gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
             gptq_representative_data_gen (Callable): Dataset used for GPTQ training. If None defaults to representative_data_gen
-            target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
+            target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-            fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
 
             A quantized model and information the user may need to handle the quantized model.
 
         Examples:
 
@@ -168,89 +168,75 @@
 
             >>> config = mct.core.CoreConfig()
 
             If mixed precision is desired, create an MCT core config with a mixed-precision configuration, to quantize a model
             with different bitwidths for different layers.
             The candidates bitwidth for quantization should be defined in the target platform model:
 
-            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfigV2(num_of_images=1))
+            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=1))
 
-            For mixed-precision set a target KPI object:
-            Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
+            For mixed-precision set a target resource utilization object:
+            Create a resource utilization object to limit our returned model's size. Note that this value affects only coefficients
             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
             while the bias will not):
 
-            >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+            >>> ru = mct.core.ResourceUtilization(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
             Create GPTQ config:
 
             >>> gptq_config = mct.gptq.get_keras_gptq_config(n_epochs=1)
 
             Pass the model with the representative dataset generator to get a quantized model:
 
-            >>> quantized_model, quantization_info = mct.gptq.keras_gradient_post_training_quantization_experimental(model, repr_datagen, gptq_config, target_kpi=kpi, core_config=config)
+            >>> quantized_model, quantization_info = mct.gptq.keras_gradient_post_training_quantization(model, repr_datagen, gptq_config, target_resource_utilization=ru, core_config=config)
 
         """
         KerasModelValidation(model=in_model,
-                             fw_info=fw_info).validate()
+                             fw_info=DEFAULT_KERAS_INFO).validate()
 
         if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
-                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config for mixed-precision is not of type 'MixedPrecisionQuantizationConfig'. "
+                                "Ensure usage of the correct API for keras_post_training_quantization "
+                                "or provide a valid mixed-precision configuration.")  # pragma: no cover
 
-            Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(fw_info)
+        tb_w = init_tensorboard_writer(DEFAULT_KERAS_INFO)
 
         fw_impl = GPTQKerasImplemantation()
 
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=fw_info,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
+        tg, bit_widths_config, hessian_info_service = core_runner(in_model=in_model,
+                                                                  representative_data_gen=representative_data_gen,
+                                                                  core_config=core_config,
+                                                                  fw_info=DEFAULT_KERAS_INFO,
+                                                                  fw_impl=fw_impl,
+                                                                  tpc=target_platform_capabilities,
+                                                                  target_resource_utilization=target_resource_utilization,
+                                                                  tb_w=tb_w)
 
         tg_gptq = gptq_runner(tg,
                               core_config,
                               gptq_config,
                               representative_data_gen,
                               gptq_representative_data_gen if gptq_representative_data_gen else representative_data_gen,
-                              fw_info,
+                              DEFAULT_KERAS_INFO,
                               fw_impl,
-                              tb_w)
+                              tb_w,
+                              hessian_info_service=hessian_info_service)
+
+        del hessian_info_service
 
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen, tb_w, tg_gptq, fw_impl, fw_info)
 
-        if new_experimental_exporter:
-            Logger.warning('Using new experimental wrapped and ready for export models. To '
-                           'disable it, please set new_experimental_exporter to False when '
-                           'calling keras_gradient_post_training_quantization_experimental. '
-                           'If you encounter an issue please file a bug.')
-
-            return get_exportable_keras_model(tg_gptq)
-
-        return export_model(tg_gptq,
-                            fw_info,
-                            fw_impl,
-                            tb_w,
-                            bit_widths_config)
+        return get_exportable_keras_model(tg_gptq)
 
 else:
     # If tensorflow is not installed,
     # we raise an exception when trying to use these functions.
     def get_keras_gptq_config(*args, **kwargs):
-        Logger.critical('Installing tensorflow is mandatory '
-                        'when using keras_post_training_quantization_mixed_precision. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        Logger.critical("Tensorflow must be installed to use get_keras_gptq_config. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
 
 
-    def keras_gradient_post_training_quantization_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow is mandatory '
-                        'when using keras_gradient_post_training_quantization_experimental. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_gradient_post_training_quantization(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_gradient_post_training_quantization. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py`

 * *Files 16% similar despite different names*

```diff
@@ -57,16 +57,16 @@
                 that changed during GPTQ retraining.
                 Keys must match NodeQuantizationConfig attributes
 
             """
             weights = {}
             for weight, quantizer_vars, quantizer in layer.get_weights_vars():
                 if not isinstance(quantizer, BaseTrainableQuantizer):
-                    Logger.error(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
-                                 f"but got {type(quantizer)} which is not callable.")
+                    Logger.critical(f"Expecting a GPTQ trainable quantizer for layer '{layer.name}', but received {type(quantizer)}. "
+                                    f"Ensure a trainable quantizer is used.") # pragma: no cover
                 weights.update({weight: quantizer(training=False, inputs=quantizer_vars)})
 
             quant_config = {WEIGHTS_QUANTIZATION_PARAMS: self.get_quant_config()}
 
             return weights, quant_config, {}
 
         def get_aux_variable(self) -> List[tf.Tensor]:
@@ -101,10 +101,9 @@
             """
             raise NotImplemented(f'{self.__class__.__name__} have to implement the '  # pragma: no cover
                                  f'quantizer\'s get_quant_config.')
 
 else:
     class BaseKerasGPTQTrainableQuantizer:  # pragma: no cover
         def __init__(self, *args, **kwargs):
-            Logger.critical('Installing tensorflow is mandatory '
-                            'when using BaseKerasGPTQTrainableQuantizer. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use BaseKerasGPTQTrainableQuantizer. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,68 +10,67 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Dict, List, Tuple
 
-from model_compression_toolkit.gptq import GradientPTQConfigV2
+from model_compression_toolkit.gptq import GradientPTQConfig
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
     get_inferable_quantizer_kwargs
-from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
 from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
     get_trainable_quantizer_weights_config
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
 
 def quantization_builder(n: common.BaseNode,
-                         gptq_config: GradientPTQConfigV2
-                         ) -> Tuple[Dict[str, BaseKerasGPTQTrainableQuantizer], List[BaseKerasInferableQuantizer]]:
+                         gptq_config: GradientPTQConfig,
+                         kernel_attr: str = None) -> Tuple[Dict[str, BaseKerasGPTQTrainableQuantizer], List[BaseKerasInferableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration and
     a global NoOpQuantizeConfig object.
 
     Args:
         n: Node to build its QuantizeConfig.
-        gptq_config (GradientPTQConfigV2): GradientPTQConfigV2 configuration.
+        gptq_config (GradientPTQConfig): GradientPTQConfig configuration.
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
 
     Returns:
         A dictionary which maps the weights kernel attribute to a quantizer for GPTQ training.
         Note that we return a dictionary although there is only a single attribute that is being mapped to a quantizer,
         to be compatible with the quantization infrastructure template.
     """
 
     weights_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
+
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during GPTQ
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
 
         quantizer_class = get_trainable_quantizer_class(quant_target=QuantizationTarget.Weights,
                                                         quantizer_id=gptq_config.rounding_type,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BaseKerasGPTQTrainableQuantizer)
-        kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=n.type,
-                                                              fw_info=DEFAULT_KERAS_INFO)
 
-        weights_quantizers.update({kernel_attribute: quantizer_class(get_trainable_quantizer_weights_config(n),
-                                                                     **gptq_config.gptq_quantizer_params_override)})
+        weights_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                                       kernel_attr),
+                                                                **gptq_config.gptq_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         if n.final_activation_quantization_cfg is None:
-            Logger.critical(f'Can not set quantizer for a node with no final activation quantization configuration')  #
-            # pragma: no cover
+            Logger.critical(f"Cannot set quantizer for a node without a final activation quantization configuration.")  # pragma: no cover
 
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
 
         quantizer_class = get_inferable_quantizer_class(quant_target=QuantizationTarget.Activation,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BaseKerasInferableQuantizer)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,16 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
-from model_compression_toolkit.gptq import RoundingType, GradientPTQConfigV2, GradientPTQConfig
-from model_compression_toolkit.gptq.keras.quantizer.soft_rounding.soft_quantizer_reg import \
+from model_compression_toolkit.gptq import RoundingType, GradientPTQConfig, GradientPTQConfig
+from model_compression_toolkit.gptq.pytorch.quantizer.soft_rounding.soft_quantizer_reg import \
     SoftQuantizerRegularization
 
 
 def get_regularization(gptq_config: GradientPTQConfig, representative_data_gen: Callable) -> Callable:
     """
     Returns a function that computes the regularization term for GPTQ training based on the given
     rounding type in the GPTQ configuration.
@@ -34,12 +34,10 @@
     """
     if gptq_config.rounding_type == RoundingType.SoftQuantizer:
         # dry run on the representative dataset to count number of batches
         num_batches = 0
         for _ in representative_data_gen():
             num_batches += 1
 
-        n_epochs = GradientPTQConfigV2.from_v1(n_ptq_iter=num_batches, config_v1=gptq_config).n_epochs if \
-            not type(gptq_config) == GradientPTQConfigV2 else gptq_config.n_epochs
-        return SoftQuantizerRegularization(total_gradient_steps=num_batches * n_epochs)
+        return SoftQuantizerRegularization(total_gradient_steps=num_batches * gptq_config.n_epochs)
     else:
         return lambda m, e_reg: 0
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/pytorch/optimization_functions/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 1% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 
 from model_compression_toolkit.gptq import RoundingType
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from mct_quantizers import QuantizationTarget
 from model_compression_toolkit.gptq.common.gptq_constants import AUXVAR, PTQ_THRESHOLD
 from model_compression_toolkit.gptq.keras.quantizer import quant_utils as qutils
 from model_compression_toolkit.constants import THRESHOLD
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
+from model_compression_toolkit.defaultdict import DefaultDict
 from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig
 from mct_quantizers import mark_quantizer
 from model_compression_toolkit.trainable_infrastructure.common.quant_utils import \
     get_threshold_reshape_shape
 from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
@@ -73,15 +73,15 @@
 class STEWeightGPTQQuantizer(BaseKerasGPTQTrainableQuantizer):
     """
     Trainable symmetric quantizer to quantize a layer weights.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
-                 max_lsbs_change_map: dict = DefaultDict({}, lambda: 1)):
+                 max_lsbs_change_map: dict = DefaultDict(default_value=1)):
         """
         Initialize a STEWeightGPTQQuantizer object with parameters to use for the quantization.
 
         Args:
             quantization_config: Trainable weights quantizer config.
             max_lsbs_change_map: a mapping between number of bits to max lsb change.
         """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/gptq_loss.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/gptq_loss.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/gptq_training.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantization_facade.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,287 +8,241 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable, List, Tuple, Union
-
-import numpy as np
-from torch.nn import Module
-from tqdm import tqdm
 import copy
-import torch
+from typing import Callable
+from functools import partial
+
+from model_compression_toolkit.constants import FOUND_TORCH, PYTORCH
+
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
 from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
-from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
-from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.pytorch.constants import BIAS
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, set_model, torch_tensor_to_numpy
-from model_compression_toolkit.gptq.pytorch.graph_info import get_gptq_trainable_parameters, \
-    get_weights_for_loss
-from model_compression_toolkit.gptq.pytorch.quantizer.quantization_builder import quantization_builder
-from model_compression_toolkit.gptq.pytorch.quantizer.regularization_factory import get_regularization
-from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
-
-
-class PytorchGPTQTrainer(GPTQTrainer):
-    """
-    Pytorch GPTQ training class for fine-tuning a quantized model
-    """
-
-    def __init__(self,
-                 graph_float: Graph,
-                 graph_quant: Graph,
-                 gptq_config: GradientPTQConfigV2,
-                 fw_impl: FrameworkImplementation,
-                 fw_info: FrameworkInfo,
-                 representative_data_gen: Callable):
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
+    MixedPrecisionQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
+    TargetPlatformCapabilities
+from model_compression_toolkit.core.runner import core_runner
+from model_compression_toolkit.ptq.runner import ptq_runner
+
+if FOUND_TORCH:
+    import torch.nn as nn
+    from torch.nn import Module
+    from mct_quantizers import PytorchActivationQuantizationHolder
+    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
+    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
+    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+    from mct_quantizers import PytorchQuantizationWrapper
+    from model_compression_toolkit import get_target_platform_capabilities
+    from model_compression_toolkit.qat.common.qat_config import QATConfig
+    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import get_activation_quantizer_holder
+    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import quantization_builder
+
+    DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
+
+
+    def qat_wrapper(n: common.BaseNode,
+                    module: nn.Module,
+                    qat_config: QATConfig):
         """
-        Build two models from a graph: A teacher network (float model) and a student network (quantized model).
-        Use the dataset generator to pass images through the teacher and student networks to get intermediate
-        layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
-        in the student network, to minimize it in the next similar steps.
-        All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
+        A function which takes a computational graph node and a pytorch module and perform the quantization wrapping
         Args:
-            graph_float: Graph to build a float networks from.
-            graph_quant: Graph to build a quantized networks from.
-            gptq_config: GradientPTQConfigV2 with parameters about the tuning process.
-            fw_impl: FrameworkImplementation object with a specific framework methods implementation.
-            fw_info: Framework information
-            representative_data_gen: Dataset to use for inputs of the models.
+            n: A node of mct graph.
+            module: A Pytorch module
+            qat_config (QATConfig): QAT configuration
+        Returns: Wrapped layer
+
         """
-        super().__init__(graph_float, graph_quant, gptq_config, fw_impl, fw_info)
-        self.loss_list = []
-        self.input_scale = 1
-        if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
-            Logger.error("Input scale mismatch between float and GPTQ networks")  # pragma: no cover
-        else:
-            self.input_scale = self.gptq_user_info.input_scale
-
-        trainable_weights, trainable_bias, trainable_threshold = get_gptq_trainable_parameters(
-            self.fxp_model,
-            add_bias=self.gptq_config.train_bias)
-
-        self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
-        if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
-                self.fxp_weights_list)):
-            Logger.error(
-                "GPTQ: Mismatch between number of compare points, number of layers with trainable weights " +
-                "and number of float and quantized weights for loss")
-
-        self.optimizer_with_param = self.get_optimizer_with_param(trainable_weights,
-                                                                  trainable_bias,
-                                                                  trainable_threshold)
+        if is_qat_applicable(n, DEFAULT_PYTORCH_INFO):
+            # If we are here, then the node has a kernel attribute to quantize and training during QAT
+            weights_quantizers, _ = quantization_builder(n, qat_config,
+                                                         DEFAULT_PYTORCH_INFO.get_kernel_op_attributes(n.type)[0])
+            if len(weights_quantizers) > 0:
+                return PytorchQuantizationWrapper(module, weights_quantizers)
 
-        self.weights_for_average_loss = to_torch_tensor(self.compute_hessian_based_weights(representative_data_gen))
+        # TODO: need to check if in this case, if there are other weights attributes that are not trainable but are
+        #  quantized, do we need to wrap them as well?
+        return module
 
-        self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
-    def _is_gptq_weights_trainable(self,
-                                   node: BaseNode) -> bool:
-        """
-        A function for deciding if a layer should be fine-tuned during GPTQ.
-        Args:
-            node (BaseNode): Node for quantization decision
-        Returns:
-            A boolean whether the layer is to be wrapped with a Quantization Wrapper.
+    def pytorch_quantization_aware_training_init_experimental(in_model: Module,
+                                                              representative_data_gen: Callable,
+                                                              target_resource_utilization: ResourceUtilization = None,
+                                                              core_config: CoreConfig = CoreConfig(),
+                                                              qat_config: QATConfig = QATConfig(),
+                                                              target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
         """
+         Prepare a trained Pytorch model for quantization aware training. First the model quantization is optimized
+         with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
+         quantized using a symmetric quantization thresholds (power of two).
+         The model is first optimized using several transformations (e.g. BatchNormalization folding to
+         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
+         being collected for each layer's output (and input, depends on the quantization configuration).
+         For each possible bit width (per layer) a threshold is then being calculated using the collected
+         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
+         a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
+         nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
+         quantization wrapper's weight quantizer.
+         In order to limit the maximal model's size, a target resource utilization need to be passed after weights_memory
+         is set (in bytes).
 
-        if node.is_weights_quantization_enabled() and not self.fw_info.is_kernel_op(node.type):
-            Logger.error(f"GPTQ Error: Quantizing node {node.name} of type {node.type} "
-                         f"without a kernel isn't supported.")
-        return node.is_weights_quantization_enabled()
-
-    def gptq_wrapper(self,
-                     n: BaseNode,
-                     layer: Module) -> Union[PytorchQuantizationWrapper, Module]:
-        """
-        A function which takes a computational graph node and a pytorch layer and perform the quantization wrapping.
+         Args:
+             in_model (Model): Pytorch model to quantize.
+             representative_data_gen (Callable): Dataset used for initial calibration.
+             target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
+             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
+             qat_config (QATConfig): QAT configuration
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Pytorch model according to.
 
-        Args:
-            n: A node of mct graph.
-            layer: A pytorch layer
+         Returns:
 
-        Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
-        """
+             A quantized model.
+             User information that may be needed to handle the quantized model.
 
-        if self._is_gptq_weights_trainable(n):
-            weights_quantizers, activation_quantizers = quantization_builder(n, self.gptq_config)
-            return PytorchQuantizationWrapper(layer,
-                                              weights_quantizers=weights_quantizers)
-        else:
-            return layer
+         Examples:
 
-    def get_activation_quantizer_holder(self, n: BaseNode) -> Callable:
-        """
-        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
-        If the layer is not supposed to be wrapped with an activation quantizer - return None.
-        Args:
-            n: Node to attach a PytorchActivationQuantizationHolder to its output.
-        Returns:
-            A PytorchActivationQuantizationHolder module for the node's activation quantization.
-        """
-        _, activation_quantizers = quantization_builder(n, self.gptq_config)
-        # Holder by definition uses a single quantizer for the activation quantization
-        # thus we make sure this is the only possible case (unless it's a node we no activation
-        # quantization, which in this case has an empty list).
-        if len(activation_quantizers) == 1:
-            return PytorchActivationQuantizationHolder(activation_quantizers[0])
-        Logger.error(
-            f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
-            f'were found for node {n}')
+             Import MCT:
 
-    def build_gptq_model(self):
-        """
-        Build the GPTQ model with QuantizationWrappers
-        Returns:
-            Quantized graph for GPTQ fine-tuning, GPTQ graph user info
-        """
-        gptq_model, gptq_user_info = PyTorchModelBuilder(graph=self.graph_quant,
-                                                         append2output=self.compare_points,
-                                                         fw_info=self.fw_info,
-                                                         wrapper=self.gptq_wrapper,
-                                                         return_float_outputs=True,
-                                                         get_activation_quantizer_holder_fn=self.get_activation_quantizer_holder).build_model()
+             >>> import model_compression_toolkit as mct
 
-        return gptq_model, gptq_user_info
+             Import a Pytorch model:
 
-    def train(self, representative_data_gen: Callable):
-        """
-          GPTQ Training using pytorch framework
-          Args:
-              representative_data_gen: Dataset generator to get images.
-          Returns:
-              Graph after GPTQ training
-          """
-        # Set Optimizers
-        for (optimizer, params) in self.optimizer_with_param:
-            optimizer.param_groups.clear()
-            optimizer.add_param_group({'params': params})
-
-        # Set models mode
-        set_model(self.float_model, False)
-        set_model(self.fxp_model, True)
-        self._set_requires_grad()
-
-        # ----------------------------------------------
-        # Training loop
-        # ----------------------------------------------
-        self.micro_training_loop(representative_data_gen, self.gptq_config.n_epochs)
-
-    def compute_gradients(self,
-                          y_float: List[torch.Tensor],
-                          input_tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, List[np.ndarray]]:
-        """
-        Get outputs from both teacher and student networks. Compute the observed error,
-        and use it to compute the gradients and applying them to the student weights.
-        Args:
-            y_float: A list of reference tensor from the floating point network.
-            input_tensors: A list of Input tensors to pass through the networks.
-        Returns:
-            Loss and gradients.
-        """
+             >>> from torchvision.models import mobilenet_v2
+             >>> model = mobilenet_v2(pretrained=True)
 
-        # Forward-pass
-        y_fxp = self.fxp_model(input_tensors)
+            Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
+            In this example a random dataset of 10 batches each containing 4 images is used.
 
-        # Loss
-        loss_value = self.gptq_config.loss(y_fxp,
-                                           y_float,
-                                           self.fxp_weights_list,
-                                           self.flp_weights_list,
-                                           self.compare_points_mean,
-                                           self.compare_points_std,
-                                           self.weights_for_average_loss)
-
-        reg_value = self.reg_func(self.fxp_model, self.gptq_config.regularization_factor)
-
-        loss_value += reg_value
-
-        # Back-pass
-        loss_value.backward()
-
-        # Get gradients
-        grads = []
-        for param in self.fxp_model.parameters():
-            if param.requires_grad and param.grad is not None:
-                grads.append(torch_tensor_to_numpy(param.grad))
-
-        return loss_value, grads
-
-    def micro_training_loop(self,
-                            data_function: Callable,
-                            n_epochs: int):
-        """
-        This function run a micro training loop on given set of parameters.
-        Args:
-            data_function: A callable function that give a batch of samples.
-            n_epochs: Number of update iterations of representative dataset.
-        """
-        for _ in tqdm(range(n_epochs)):
-            for data in tqdm(data_function()):
-                input_data = [d * self.input_scale for d in data]
-                input_tensor = to_torch_tensor(input_data)
-                y_float = self.float_model(input_tensor)  # running float model
-                loss_value, grads = self.compute_gradients(y_float, input_tensor)
-                # Run one step of gradient descent by updating the value of the variables to minimize the loss.
-                for (optimizer, _) in self.optimizer_with_param:
-                    optimizer.step()
-                    optimizer.zero_grad()
-                if self.gptq_config.log_function is not None:
-                    self.gptq_config.log_function(loss_value.item(),
-                                                  torch_tensor_to_numpy(grads),
-                                                  torch_tensor_to_numpy(self.optimizer_with_param[0][-1]))
-                self.loss_list.append(loss_value.item())
-                Logger.debug(f'last loss value: {self.loss_list[-1]}')
+            >>> import numpy as np
+            >>> num_calibration_batches = 10
+            >>> def repr_datagen():
+            >>>     for _ in range(num_calibration_batches):
+            >>>         yield [np.random.random((4, 3, 224, 224))]
 
-    def update_graph(self) -> Graph:
-        """
-        Update a graph using GPTQ after minimizing the loss between the float model's output
-        and the quantized model's outputs.
-        Returns:
-            Updated graph after GPTQ.
-        """
-        graph_quant = copy.copy(self.graph_quant)
+             Create a MCT core config, containing the quantization configuration:
 
-        # Update graph after training
-        for name, layer in self.fxp_model.named_modules():
-            if isinstance(layer, PytorchQuantizationWrapper):
-                node = self.graph_quant.find_node_by_name(name)
-                if len(node) != 1:
-                    Logger.error(f"Can't update GPTQ graph due to missing layer named: {name}")
-                node = node[0]
-                kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
-                                                                      fw_info=self.fw_info)
-                weights, weight_quant_config, activation_quant_config = \
-                    layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
-                for weight_attr, weight in weights.items():
-                    node.set_weights_by_keys(weight_attr, self.fw_impl.to_numpy(weight))
-                for config_attr, config_value in weight_quant_config.items():
-                    node.final_weights_quantization_cfg.set_quant_config_attr(config_attr, config_value)
-                for config_attr, config_value in activation_quant_config.items():
-                    node.final_activation_quantization_cfg.set_quant_config_attr(config_attr, config_value)
-                if self.gptq_config.train_bias and hasattr(layer.layer, BIAS):
-                    node.set_weights_by_keys(BIAS, self.fw_impl.to_numpy(getattr(layer.layer, BIAS)))
+             >>> config = mct.core.CoreConfig()
 
-        return graph_quant
+             Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+             quantized model. Now the model contains quantizer wrappers for fine tunning the weights:
 
-    def _set_requires_grad(self):
-        """
-        Set require_grad flag for trainable parameters for GPTQ training
+             >>> quantized_model, quantization_info = mct.qat.pytorch_quantization_aware_training_init_experimental(model, repr_datagen, core_config=config)
+
+             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
+
+         """
+        Logger.warning(
+            f"pytorch_quantization_aware_training_init_experimental is experimental and is subject to future changes."
+            f"If you encounter an issue, please open an issue in our GitHub "
+            f"project https://github.com/sony/model_optimization")
+
+        if core_config.mixed_precision_enable:
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfig. Please use pytorch_post_training_quantization API,"
+                             "or pass a valid mixed precision configuration.")
+
+        tb_w = init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
+        fw_impl = PytorchImplementation()
+
+        # Ignore trace hessian service as we do not use it here
+        tg, bit_widths_config, _ = core_runner(in_model=in_model,
+                                               representative_data_gen=representative_data_gen,
+                                               core_config=core_config,
+                                               fw_info=DEFAULT_PYTORCH_INFO,
+                                               fw_impl=fw_impl,
+                                               tpc=target_platform_capabilities,
+                                               target_resource_utilization=target_resource_utilization,
+                                               tb_w=tb_w)
+
+        tg = ptq_runner(tg, representative_data_gen, core_config, DEFAULT_PYTORCH_INFO, fw_impl, tb_w)
+
+        _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
+
+        qat_model, user_info = PyTorchModelBuilder(graph=tg,
+                                                   fw_info=DEFAULT_PYTORCH_INFO,
+                                                   wrapper=_qat_wrapper,
+                                                   get_activation_quantizer_holder_fn=partial(
+                                                       get_activation_quantizer_holder,
+                                                       qat_config=qat_config)).build_model()
+
+        user_info.mixed_precision_cfg = bit_widths_config
+
+        # Remove fw_info from graph to enable saving the pytorch model (fw_info can not be pickled)
+        delattr(qat_model.graph, 'fw_info')
+
+        return qat_model, user_info
+
+
+    def pytorch_quantization_aware_training_finalize_experimental(in_model: Module):
         """
-        # Float model: freeze all the parameters in the network
-        for param in self.float_model.parameters():
-            param.requires_grad = False
-
-        # Fxp model: unfreeze bias trainable parameters
-        for layer in self.fxp_model.modules():
-            if isinstance(layer, PytorchQuantizationWrapper):
-                if hasattr(layer.layer, BIAS):
-                    bias = getattr(layer.layer, BIAS)
-                    bias.requires_grad = self.gptq_config.train_bias
+         Convert a model fine-tuned by the user to a network with QuantizeWrappers containing
+         InferableQuantizers, that quantizes both the layers weights and outputs
+
+         Args:
+             in_model (Model): Pytorch model to remove QuantizeWrappers.
+
+         Returns:
+             A quantized model with QuantizeWrappers and InferableQuantizers.
+
+         Examples:
+
+             Import MCT:
+
+             >>> import model_compression_toolkit as mct
+
+             Import a Pytorch model:
+
+             >>> from torchvision.models import mobilenet_v2
+             >>> model = mobilenet_v2(pretrained=True)
+
+             Create a random dataset generator:
+
+             >>> import numpy as np
+             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
+
+             Create a MCT core config, containing the quantization configuration:
+
+             >>> config = mct.core.CoreConfig()
+
+             Pass the model, the representative dataset generator, the configuration and the target resource utilization to get a
+             quantized model:
+
+             >>> quantized_model, quantization_info = mct.qat.pytorch_quantization_aware_training_init_experimental(model, repr_datagen, core_config=config)
+
+             Use the quantized model for fine-tuning. Finally, remove the quantizer wrappers and keep a quantize model ready for inference.
+
+             >>> quantized_model = mct.qat.pytorch_quantization_aware_training_finalize_experimental(quantized_model)
+
+         """
+        Logger.warning(
+            f"pytorch_quantization_aware_training_finalize_experimental is experimental and is subject to future changes."
+            f"If you encounter an issue, please open an issue in our GitHub "
+            f"project https://github.com/sony/model_optimization")
+
+        for _, layer in in_model.named_children():
+            if isinstance(layer, (PytorchQuantizationWrapper, PytorchActivationQuantizationHolder)):
+                layer.convert_to_inferable_quantizers()
+
+        return in_model
+
+
+else:
+    # If torch is not installed,
+    # we raise an exception when trying to use these functions.
+    def pytorch_quantization_aware_training_init_experimental(*args, **kwargs):
+        Logger.critical('PyTorch must be installed to use pytorch_quantization_aware_training_init_experimental. '
+                        "The 'torch' package is missing.")  # pragma: no cover
+
+
+    def pytorch_quantization_aware_training_finalize_experimental(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_quantization_aware_training_finalize_experimental'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/graph_info.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/graph_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     for layer in fxp_model.modules():
         if isinstance(layer, PytorchQuantizationWrapper):
             kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
                                                                   fw_info=DEFAULT_PYTORCH_INFO)
 
             # collect trainable weights per quantizer
             if kernel_attribute not in layer.weights_quantizers:
-                Logger.error(f'{kernel_attribute} was not found in weight quantizers of layer {layer.layer}')
+                Logger.critical(f"'{kernel_attribute}' was not found in the weight quantizers of layer '{layer.layer}'.")
             quantizer_trainable_weights = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.WEIGHTS)
             quantizer_trainable_threshold = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.QPARAMS)
             trainable_aux_weights.extend(quantizer_trainable_weights)
             trainable_threshold.extend(quantizer_trainable_threshold)
 
             if add_bias and hasattr(layer.layer, BIAS):
                 bias = getattr(layer.layer, BIAS)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantization_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantization_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,27 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 from model_compression_toolkit.core import common
 from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import init_tensorboard_writer
+from model_compression_toolkit.gptq.common.gptq_constants import REG_DEFAULT
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.constants import PYTORCH
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.runner import core_runner
 from model_compression_toolkit.gptq.keras.quantization_facade import GPTQ_MOMENTUM
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
+    MixedPrecisionQuantizationConfig
 
 LR_DEFAULT = 1e-4
 LR_REST_DEFAULT = 1e-4
 LR_BIAS_DEFAULT = 1e-4
 LR_QUANTIZATION_PARAM_DEFAULT = 1e-4
 
 if FOUND_TORCH:
@@ -40,35 +42,34 @@
     from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.gptq.pytorch.gptq_loss import multiple_tensors_mse_loss
     from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
     import torch
     from torch.nn import Module
     from torch.optim import Adam, Optimizer
     from model_compression_toolkit import get_target_platform_capabilities
-
-
     DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
-
     def get_pytorch_gptq_config(n_epochs: int,
                                 optimizer: Optimizer = Adam([torch.Tensor([])], lr=LR_DEFAULT),
                                 optimizer_rest: Optimizer = Adam([torch.Tensor([])], lr=LR_REST_DEFAULT),
                                 loss: Callable = multiple_tensors_mse_loss,
                                 log_function: Callable = None,
-                                use_hessian_based_weights: bool = True) -> GradientPTQConfigV2:
+                                use_hessian_based_weights: bool = True,
+                                regularization_factor: float = REG_DEFAULT) -> GradientPTQConfig:
         """
         Create a GradientPTQConfigV2 instance for Pytorch models.
 
         args:
             n_epochs (int): Number of epochs for running the representative dataset for fine-tuning.
             optimizer (Optimizer): Pytorch optimizer to use for fine-tuning for auxiliry variable.
             optimizer_rest (Optimizer): Pytorch optimizer to use for fine-tuning of the bias variable.
             loss (Callable): loss to use during fine-tuning. should accept 4 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors, the 3rd is a list of quantized weights and the 4th is a list of float weights.
             log_function (Callable): Function to log information about the gptq process.
             use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
+            regularization_factor (float): A floating point number that defines the regularization factor.
 
         returns:
             a GradientPTQConfigV2 object to use when fine-tuning the quantized model using gptq.
 
         Examples:
 
             Import MCT and Create a GradientPTQConfigV2 to run for 5 epochs:
@@ -81,26 +82,27 @@
             >>> import torch
             >>> gptq_conf = mct.gptq.get_pytorch_gptq_config(n_epochs=3, optimizer=torch.optim.Adam([torch.Tensor(1)]))
 
             The configuration can be passed to :func:`~model_compression_toolkit.pytorch_post_training_quantization` in order to quantize a pytorch model using gptq.
 
         """
         bias_optimizer = torch.optim.SGD([torch.Tensor([])], lr=LR_BIAS_DEFAULT, momentum=GPTQ_MOMENTUM)
-        return GradientPTQConfigV2(n_epochs, optimizer, optimizer_rest=optimizer_rest, loss=loss,
-                                   log_function=log_function, train_bias=True, optimizer_bias=bias_optimizer, use_hessian_based_weights=use_hessian_based_weights)
+        return GradientPTQConfig(n_epochs, optimizer, optimizer_rest=optimizer_rest, loss=loss,
+                                 log_function=log_function, train_bias=True, optimizer_bias=bias_optimizer,
+                                 use_hessian_based_weights=use_hessian_based_weights,
+                                 regularization_factor=regularization_factor)
 
 
-    def pytorch_gradient_post_training_quantization_experimental(model: Module,
-                                                                 representative_data_gen: Callable,
-                                                                 target_kpi: KPI = None,
-                                                                 core_config: CoreConfig = CoreConfig(),
-                                                                 gptq_config: GradientPTQConfigV2 = None,
-                                                                 gptq_representative_data_gen: Callable = None,
-                                                                 target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC,
-                                                                 new_experimental_exporter: bool = True):
+    def pytorch_gradient_post_training_quantization(model: Module,
+                                                    representative_data_gen: Callable,
+                                                    target_resource_utilization: ResourceUtilization = None,
+                                                    core_config: CoreConfig = CoreConfig(),
+                                                    gptq_config: GradientPTQConfig = None,
+                                                    gptq_representative_data_gen: Callable = None,
+                                                    target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
         """
         Quantize a trained Pytorch module using post-training quantization.
         By default, the module is quantized using a symmetric constraint quantization thresholds
         (power of two) as defined in the default TargetPlatformCapabilities.
         The module is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
@@ -112,26 +114,29 @@
         Then, the quantized weights are optimized using gradient based post
         training quantization by comparing points between the float and quantized models, and minimizing the observed
         loss.
 
         Args:
             model (Module): Pytorch model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
-            target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
+            target_resource_utilization (ResourceUtilization): ResourceUtilization object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-            gptq_config (GradientPTQConfigV2): Configuration for using gptq (e.g. optimizer).
+            gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
             gptq_representative_data_gen (Callable): Dataset used for GPTQ training. If None defaults to representative_data_gen
             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
-            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
             A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
+            Import Model Compression Toolkit:
+
+            >>> import model_compression_toolkit as mct
+
             Import a Pytorch module:
 
             >>> from torchvision import models
             >>> module = models.mobilenet_v2()
 
             Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
             In this example a random dataset of 10 batches each containing 4 images is used.
@@ -144,76 +149,63 @@
 
             Create MCT core configurations with number of calibration iterations set to 1:
 
             >>> config = mct.core.CoreConfig()
 
             Pass the module, the representative dataset generator and the configuration (optional) to get a quantized module
 
-            >>> quantized_module, quantization_info = mct.gptq.pytorch_gradient_post_training_quantization_experimental(module, repr_datagen, core_config=config, gptq_config=gptq_conf)
+            >>> quantized_module, quantization_info = mct.gptq.pytorch_gradient_post_training_quantization(module, repr_datagen, core_config=config, gptq_config=gptq_conf)
 
         """
 
         if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
-                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
+            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfig):
+                Logger.critical("Given quantization config for mixed-precision is not of type 'MixedPrecisionQuantizationConfig'. "
+                                "Ensure usage of the correct API for 'keras_post_training_quantization' "
+                                "or provide a valid mixed-precision configuration.")  # pragma: no cover
 
-            Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
+        tb_w = init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
 
         fw_impl = GPTQPytorchImplemantation()
 
         # ---------------------- #
         # Core Runner
         # ---------------------- #
-        graph, bit_widths_config = core_runner(in_model=model,
-                                               representative_data_gen=representative_data_gen,
-                                               core_config=core_config,
-                                               fw_info=DEFAULT_PYTORCH_INFO,
-                                               fw_impl=fw_impl,
-                                               tpc=target_platform_capabilities,
-                                               target_kpi=target_kpi,
-                                               tb_w=tb_w)
+        graph, bit_widths_config, hessian_info_service = core_runner(in_model=model,
+                                                                     representative_data_gen=representative_data_gen,
+                                                                     core_config=core_config,
+                                                                     fw_info=DEFAULT_PYTORCH_INFO,
+                                                                     fw_impl=fw_impl,
+                                                                     tpc=target_platform_capabilities,
+                                                                     target_resource_utilization=target_resource_utilization,
+                                                                     tb_w=tb_w)
 
         # ---------------------- #
         # GPTQ Runner
         # ---------------------- #
-        graph_gptq = gptq_runner(graph, core_config, gptq_config,
+        graph_gptq = gptq_runner(graph,
+                                 core_config,
+                                 gptq_config,
                                  representative_data_gen,
                                  gptq_representative_data_gen if gptq_representative_data_gen else representative_data_gen,
-                                 DEFAULT_PYTORCH_INFO, fw_impl, tb_w)
+                                 DEFAULT_PYTORCH_INFO,
+                                 fw_impl,
+                                 tb_w,
+                                 hessian_info_service=hessian_info_service)
+
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen, tb_w, graph_gptq, fw_impl, DEFAULT_PYTORCH_INFO)
 
-        # ---------------------- #
-        # Export
-        # ---------------------- #
-        if new_experimental_exporter:
-            Logger.warning('Using new experimental wrapped and ready for export models. To '
-                           'disable it, please set new_experimental_exporter to False when '
-                           'calling pytorch_gradient_post_training_quantization_experimental. '
-                           'If you encounter an issue please file a bug.')
-
-            return get_exportable_pytorch_model(graph_gptq)
-
-        return export_model(graph_gptq,
-                            DEFAULT_PYTORCH_INFO,
-                            fw_impl,
-                            tb_w,
-                            bit_widths_config)
+        return get_exportable_pytorch_model(graph_gptq)
+
 
 else:
     # If torch is not installed,
     # we raise an exception when trying to use these functions.
     def get_pytorch_gptq_config(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_gradient_post_training_quantization_experimental. '
-                        'Could not find torch package.')  # pragma: no cover
+        Logger.critical("PyTorch must be installed to use 'get_pytorch_gptq_config'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
 
 
-    def pytorch_gradient_post_training_quantization_experimental(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_gradient_post_training_quantization_experimental. '
-                        'Could not find the torch package.')  # pragma: no cover
+    def pytorch_gradient_post_training_quantization(*args, **kwargs):
+        Logger.critical("PyTorch must be installed to use 'pytorch_gradient_post_training_quantization'. "
+                        "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -59,15 +59,15 @@
                 that changed during GPTQ retraining.
                 Keys must match NodeQuantizationConfig attributes
 
             """
             weights = {}
             for weight, quantizer_vars, quantizer in layer.get_weights_vars():
                 if not isinstance(quantizer, BaseTrainableQuantizer):
-                    Logger.error(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
+                    Logger.critical(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
                                  f"but got {type(quantizer)} which is not callable.")
                 weights.update({weight: quantizer(training=False, inputs=quantizer_vars)})
 
             quant_config = {WEIGHTS_QUANTIZATION_PARAMS: self.get_quant_config()}
 
             return weights, quant_config, {}
 
@@ -83,10 +83,9 @@
             """
             raise NotImplemented(f'{self.__class__.__name__} have to implement the '  # pragma: no cover
                                  f'quantizer\'s get_quant_config.')
 
 else:
     class BasePytorchGPTQTrainableQuantizer:  # pragma: no cover
         def __init__(self, *args, **kwargs):
-            Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchGPTQTrainableQuantizer. '
-                            'Could not find torch package.')  # pragma: no cover
+            Logger.critical("PyTorch must be installed to use 'BasePytorchGPTQTrainableQuantizer'. "
+                            "The 'torch' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,17 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Dict, Tuple
 
-from model_compression_toolkit.gptq import GradientPTQConfigV2
+from model_compression_toolkit.gptq import GradientPTQConfig
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.pytorch.constants import KERNEL
 from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
     get_activation_inferable_quantizer_kwargs
 from model_compression_toolkit.gptq.pytorch.quantizer.base_pytorch_gptq_quantizer import \
     BasePytorchGPTQTrainableQuantizer
 from mct_quantizers import QuantizationTarget
 from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
 from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
@@ -30,45 +29,47 @@
     get_trainable_quantizer_weights_config
 from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
 
 def quantization_builder(n: common.BaseNode,
-                         gptq_config: GradientPTQConfigV2,
-                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
-                                    List[BasePyTorchInferableQuantizer]]:
+                         gptq_config: GradientPTQConfig,
+                         kernel_attr: str = None
+                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer], List[BasePyTorchInferableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration and
     a global NoOpQuantizeConfig object.
 
     Args:
         n: Node to build its QuantizeConfig.
-        gptq_config (GradientPTQConfigV2): GradientPTQConfigV2 configuration.
+        gptq_config (GradientPTQConfig): GradientPTQConfig configuration.
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
 
     Returns:
         A dictionary which maps the weights kernel attribute to a quantizer for GPTQ training.
         Note that we return a dictionary although there is only a single attribute that is being mapped to a quantizer,
         to be compatible with the quantization infrastructure template.
     """
 
     weights_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during GPTQ
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
         quantizer_class = get_trainable_quantizer_class(quant_target=QuantizationTarget.Weights,
                                                         quantizer_id=gptq_config.rounding_type,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BasePytorchGPTQTrainableQuantizer)
-        weights_quantizers.update({KERNEL: quantizer_class(get_trainable_quantizer_weights_config(n),
-                                                           **gptq_config.gptq_quantizer_params_override)})
+        weights_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                                       kernel_attr),
+                                                                **gptq_config.gptq_quantizer_params_override)})
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         if n.final_activation_quantization_cfg is None:
-            Logger.critical(f'Can not set quantizer for a node with no final activation quantization configuration')  #
-            # pragma: no cover
+            Logger.critical(f"Cannot set quantizer for a node without a final activation quantization configuration.")  # pragma: no cover
 
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
 
         quantizer_class = get_inferable_quantizer_class(quant_target=QuantizationTarget.Activation,
                                                         quant_method=quant_method,
                                                         quantizer_base_class=BasePyTorchInferableQuantizer)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py`

 * *Files 17% similar despite different names*

```diff
@@ -10,16 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
-from model_compression_toolkit.gptq import RoundingType, GradientPTQConfigV2, GradientPTQConfig
-from model_compression_toolkit.gptq.pytorch.quantizer.soft_rounding.soft_quantizer_reg import \
+from model_compression_toolkit.gptq import RoundingType, GradientPTQConfig, GradientPTQConfig
+from model_compression_toolkit.gptq.keras.quantizer.soft_rounding.soft_quantizer_reg import \
     SoftQuantizerRegularization
 
 
 def get_regularization(gptq_config: GradientPTQConfig, representative_data_gen: Callable) -> Callable:
     """
     Returns a function that computes the regularization term for GPTQ training based on the given
     rounding type in the GPTQ configuration.
@@ -34,12 +34,10 @@
     """
     if gptq_config.rounding_type == RoundingType.SoftQuantizer:
         # dry run on the representative dataset to count number of batches
         num_batches = 0
         for _ in representative_data_gen():
             num_batches += 1
 
-        n_epochs = GradientPTQConfigV2.from_v1(n_ptq_iter=num_batches, config_v1=gptq_config).n_epochs if \
-            not type(gptq_config) == GradientPTQConfigV2 else gptq_config.n_epochs
-        return SoftQuantizerRegularization(total_gradient_steps=num_batches * n_epochs)
+        return SoftQuantizerRegularization(total_gradient_steps=num_batches * gptq_config.n_epochs)
     else:
         return lambda m, e_reg: 0
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/hessian/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 import torch.nn as nn
 from typing import Dict
 import numpy as np
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
+from model_compression_toolkit.defaultdict import DefaultDict
 
 from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from mct_quantizers import QuantizationTarget, PytorchQuantizationWrapper
 from model_compression_toolkit.gptq.common.gptq_config import RoundingType
 from model_compression_toolkit.gptq.pytorch.quantizer.base_pytorch_gptq_quantizer import \
     BasePytorchGPTQTrainableQuantizer
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy
@@ -80,15 +80,15 @@
 class STEWeightGPTQQuantizer(BasePytorchGPTQTrainableQuantizer):
     """
     Trainable symmetric quantizer to quantize a layer weights.
     """
 
     def __init__(self,
                  quantization_config: TrainableQuantizerWeightsConfig,
-                 max_lsbs_change_map: dict = DefaultDict({}, lambda: 1)):
+                 max_lsbs_change_map: dict = DefaultDict(default_value=1)):
         """
         Construct a Pytorch model that utilize a fake weight quantizer of STE (Straight Through Estimator) for symmetric quantizer.
 
         Args:
             quantization_config: Trainable weights quantizer config.
         """
         super().__init__(quantization_config)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/gptq/runner.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/runner.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,89 +13,92 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
 from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common.hessian import HessianInfoService
 from model_compression_toolkit.core.common.statistics_correction.statistics_correction import \
     apply_statistics_correction
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.gptq.common.gptq_training import gptq_training
 
 from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
 from model_compression_toolkit.core.common.statistics_correction.apply_bias_correction_to_graph import \
     apply_bias_correction_to_graph
 from model_compression_toolkit.logger import Logger
 
 
-def _apply_gptq(gptq_config: GradientPTQConfigV2,
+def _apply_gptq(gptq_config: GradientPTQConfig,
                 representative_data_gen: Callable,
                 tb_w: TensorboardWriter,
                 tg: Graph,
                 tg_bias: Graph,
                 fw_info: FrameworkInfo,
-                fw_impl: FrameworkImplementation) -> Graph:
+                fw_impl: FrameworkImplementation,
+                hessian_info_service: HessianInfoService = None) -> Graph:
     """
     Apply GPTQ to improve accuracy of quantized model.
     Build two models from a graph: A teacher network (float model) and a student network (quantized model).
     and use the dataset generator to pass images through the teacher and student networks to get intermediate
     layers outputs and maximize their similarity.
 
     Args:
         gptq_config: Configuration for using GPTQ (e.g. optimizer).
         representative_data_gen: Dataset used for calibration.
         tb_w: TensorBoardWriter object to log events.
         tg: Float Reference Graph.
         tg_bias: Graph of quantized model.
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
         fw_impl: Framework implementation per framework
+        hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
     Returns:
 
     """
     if gptq_config is not None and gptq_config.n_epochs > 0:
-        Logger.info("Using experimental Gradient Based PTQ: If you encounter an issue "
-                           "please file a bug. To disable it, do not pass a gptq configuration.")
-
         tg_bias = gptq_training(tg,
                                 tg_bias,
                                 gptq_config,
                                 representative_data_gen,
                                 fw_impl,
-                                fw_info)
+                                fw_info,
+                                hessian_info_service=hessian_info_service)
 
         if tb_w is not None:
             tb_w.add_graph(tg_bias, 'after_gptq')
     return tg_bias
 
 
 def gptq_runner(tg: Graph,
                 core_config: CoreConfig,
-                gptq_config: GradientPTQConfigV2,
+                gptq_config: GradientPTQConfig,
                 representative_data_gen: Callable,
                 gptq_representative_data_gen: Callable,
                 fw_info: FrameworkInfo,
                 fw_impl: FrameworkImplementation,
-                tb_w: TensorboardWriter) -> Graph:
+                tb_w: TensorboardWriter,
+                hessian_info_service: HessianInfoService = None) -> Graph:
     """
     Quantize a graph that has final weights candidates quantization configurations.
     Before we quantize the graph weights, we apply GPTQ to get an improved graph.
 
     Args:
         tg: Graph to apply GPTQ and to quantize.
         core_config: CoreConfig containing parameters of how the model should be quantized.
         gptq_config: GradientPTQConfig with parameters about the tuning process.
         representative_data_gen: Dataset used for calibration.
         gptq_representative_data_gen: Dataset used for GPTQ training
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.)
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
         tb_w: A TensorBoardWriter object initialized with the logger dir path if it was set, or None otherwise.
+        hessian_info_service: HessianInfoService to fetch approximations of the hessian traces for the float model.
 
     Returns:
         A graph after model weights GPTQ fine-tuning.
 
     """
 
     #############################################
@@ -110,10 +113,11 @@
     #############################################
     tg_gptq = _apply_gptq(gptq_config,
                           gptq_representative_data_gen,
                           tb_w,
                           tg,
                           tg_bias,
                           fw_info,
-                          fw_impl)
+                          fw_impl,
+                          hessian_info_service=hessian_info_service)
 
     return tg_gptq
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/legacy/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/data_generation/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/logger.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/logger.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 
 
 import logging
 import os
 from datetime import datetime
 from pathlib import Path
 
-LOGGER_NAME = 'Constrained Model Optimization'
+LOGGER_NAME = 'Model Compression Toolkit'
 
 
 class Logger:
     # Logger has levels of verbosity.
     log_level_translate = {
         'debug': logging.DEBUG,
         'info': logging.INFO,
@@ -113,25 +113,14 @@
             msg: Message to log.
 
         """
         Logger.get_logger().critical(msg)
         raise Exception(msg)
 
     @staticmethod
-    def exception(msg: str):
-        """
-        Log a message at 'exception' severity and raise an exception.
-        Args:
-            msg: Message to log.
-
-        """
-        Logger.get_logger().exception(msg)
-        raise Exception(msg)
-
-    @staticmethod
     def debug(msg: str):
         """
         Log a message at 'debug' severity.
 
         Args:
             msg: Message to log.
 
@@ -168,15 +157,14 @@
         Log a message at 'error' severity and raise an exception.
 
         Args:
             msg: Message to log.
 
         """
         Logger.get_logger().error(msg)
-        raise Exception(msg)
 
 
 def set_log_folder(folder: str, level: int = logging.INFO):
     """
     Set a directory path for saving a log file.
 
     Args:
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,9 +9,9 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.ptq.pytorch.quantization_facade import pytorch_post_training_quantization_experimental
-from model_compression_toolkit.ptq.keras.quantization_facade import keras_post_training_quantization_experimental
+from model_compression_toolkit.ptq.pytorch.quantization_facade import pytorch_post_training_quantization
+from model_compression_toolkit.ptq.keras.quantization_facade import keras_post_training_quantization
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/keras/quantization_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/pruning/keras/pruning_facade.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,176 +1,152 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Callable
+from typing import Callable, Tuple
 
-from model_compression_toolkit.core import CoreConfig
-from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-from model_compression_toolkit.logger import Logger
+from model_compression_toolkit import get_target_platform_capabilities
 from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization
+from model_compression_toolkit.core.common.pruning.pruner import Pruner
+from model_compression_toolkit.core.common.pruning.pruning_config import PruningConfig
+from model_compression_toolkit.core.common.pruning.pruning_info import PruningInfo
+from model_compression_toolkit.core.common.quantization.set_node_quantization_config import set_quantization_configuration_to_graph
+from model_compression_toolkit.core.graph_prep_runner import read_model_to_graph
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.core.exporter import export_model
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
-from model_compression_toolkit.ptq.runner import ptq_runner
+from model_compression_toolkit.core.common.quantization.quantization_config import DEFAULTCONFIG
+from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
 
 if FOUND_TF:
+    from model_compression_toolkit.core.keras.back2framework.float_model_builder import FloatKerasModelBuilder
+    from model_compression_toolkit.core.keras.pruning.pruning_keras_implementation import PruningKerasImplementation
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
-    from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
     from tensorflow.keras.models import Model
-    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
 
-    from model_compression_toolkit import get_target_platform_capabilities
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
-
-    def keras_post_training_quantization_experimental(in_model: Model,
-                                                      representative_data_gen: Callable,
-                                                      target_kpi: KPI = None,
-                                                      core_config: CoreConfig = CoreConfig(),
-                                                      target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC,
-                                                      new_experimental_exporter: bool = True):
+    def keras_pruning_experimental(model: Model,
+                                   target_resource_utilization: ResourceUtilization,
+                                   representative_data_gen: Callable,
+                                   pruning_config: PruningConfig = PruningConfig(),
+                                   target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC) -> Tuple[Model, PruningInfo]:
         """
-         Quantize a trained Keras model using post-training quantization. The model is quantized using a
-         symmetric constraint quantization thresholds (power of two).
-         The model is first optimized using several transformations (e.g. BatchNormalization folding to
-         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
-         being collected for each layer's output (and input, depends on the quantization configuration).
-         For each possible bit width (per layer) a threshold is then being calculated using the collected
-         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
-         a mixed-precision configuration, and set a bit-width for each layer. The model is then quantized
-         (both coefficients and activations by default).
-         In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
-         is set (in bytes).
-
-         Args:
-             in_model (Model): Keras model to quantize.
-             representative_data_gen (Callable): Dataset used for calibration.
-             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
-             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-             new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
+        Perform structured pruning on a Keras model to meet a specified target resource utilization.
+        This function prunes the provided model according to the target resource utilization by grouping and pruning
+        channels based on each layer's SIMD configuration in the Target Platform Capabilities (TPC).
+        By default, the importance of each channel group is determined using the Label-Free Hessian
+        (LFH) method, assessing each channel's sensitivity to the Hessian of the loss function.
+        This pruning strategy considers groups of channels together for a more hardware-friendly
+        architecture. The process involves analyzing the model with a representative dataset to
+        identify groups of channels that can be removed with minimal impact on performance.
+
+        Notice that the pruned model must be retrained to recover the compressed model's performance.
+
+        Args:
+            model (Model): The original Keras model to be pruned.
+            target_resource_utilization (ResourceUtilization): The target Key Performance Indicators to be achieved through pruning.
+            representative_data_gen (Callable): A function to generate representative data for pruning analysis.
+            pruning_config (PruningConfig): Configuration settings for the pruning process. Defaults to standard config.
+            target_platform_capabilities (TargetPlatformCapabilities): Platform-specific constraints and capabilities. Defaults to DEFAULT_KERAS_TPC.
 
-         Returns:
+        Returns:
+            Tuple[Model, PruningInfo]: A tuple containing the pruned Keras model and associated pruning information.
 
-             A quantized model and information the user may need to handle the quantized model.
+        Note:
+            The pruned model should be fine-tuned or retrained to recover or improve its performance post-pruning.
 
-         Examples:
+        Examples:
 
             Import MCT:
 
             >>> import model_compression_toolkit as mct
 
             Import a Keras model:
 
-            >>> from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2
-            >>> model = MobileNetV2()
+            >>> from tensorflow.keras.applications.resnet50 import ResNet50
+            >>> model = ResNet50()
 
-            Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
-            In this example a random dataset of 10 batches each containing 4 images is used.
+            Create a random dataset generator:
 
             >>> import numpy as np
-            >>> num_calibration_batches = 10
-            >>> def repr_datagen():
-            >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 224, 224, 3))]
-
-            Create a MCT core config, containing the quantization configuration:
-
-            >>> config = mct.core.CoreConfig()
-
-            If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
-            The candidates bitwidth for quantization should be defined in the target platform model.
-            In this example we use 1 image to search mixed-precision configuration:
-
-            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfigV2(num_of_images=1))
-
-            For mixed-precision set a target KPI object:
-            Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
-            that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
-            while the bias will not):
-
-            >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
-
-            Pass the model, the representative dataset generator, the configuration and the target KPI to get a
-            quantized model:
+            >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
 
-            >>> quantized_model, quantization_info = mct.ptq.keras_post_training_quantization_experimental(model, repr_datagen, kpi, core_config=config)
+            Define a target resource utilization for pruning.
+            Here, we aim to reduce the memory footprint of weights by 50%, assuming the model weights
+            are represented in float32 data type (thus, each parameter is represented using 4 bytes):
 
-            For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
+            >>> dense_nparams = sum([l.count_params() for l in model.layers])
+            >>> target_resource_utilization = mct.core.ResourceUtilization(weights_memory=dense_nparams * 4 * 0.5)
 
-         """
+            Optionally, define a pruning configuration. num_score_approximations can be passed
+            to configure the number of importance scores that will be calculated for each channel.
+            A higher value for this parameter yields more precise score approximations but also
+            extends the duration of the pruning process:
 
-        fw_info = DEFAULT_KERAS_INFO
+            >>> pruning_config = mct.pruning.PruningConfig(num_score_approximations=1)
 
-        KerasModelValidation(model=in_model,
-                             fw_info=fw_info).validate()
+            Perform pruning:
 
-        if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
-                                    "API, or pass a valid mixed precision configuration.")  # pragma: no cover
+            >>> pruned_model, pruning_info = mct.pruning.keras_pruning_experimental(model=model, target_resource_utilization=target_resource_utilization, representative_data_gen=repr_datagen, pruning_config=pruning_config)
 
-            Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(fw_info)
-
-        fw_impl = KerasImplementation()
-
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=fw_info,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
-
-        tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
-
-        if core_config.debug_config.analyze_similarity:
-            analyzer_model_quantization(representative_data_gen,
-                                        tb_w, tg,
-                                        fw_impl,
-                                        fw_info)
-
-        if new_experimental_exporter:
-            Logger.warning('Using new experimental wrapped and ready for export models. To '
-                           'disable it, please set new_experimental_exporter to False when '
-                           'calling keras_post_training_quantization_experimental. '
-                           'If you encounter an issue please file a bug.')
-
-            return get_exportable_keras_model(tg)
-
-        return export_model(tg,
-                            fw_info,
-                            fw_impl,
-                            tb_w,
-                            bit_widths_config)
+        """
 
+        Logger.warning(f"keras_pruning_experimental is experimental and is subject to future changes."
+                       f"If you encounter an issue, please open an issue in our GitHub "
+                       f"project https://github.com/sony/model_optimization")
+
+        # Instantiate the Keras framework implementation.
+        fw_impl = PruningKerasImplementation()
+
+        # Convert the original Keras model to an internal graph representation.
+        float_graph = read_model_to_graph(model,
+                                          representative_data_gen,
+                                          target_platform_capabilities,
+                                          DEFAULT_KERAS_INFO,
+                                          fw_impl)
+
+        # Apply quantization configuration to the graph. This step is necessary even when not quantizing,
+        # as it prepares the graph for the pruning process.
+        float_graph_with_compression_config = set_quantization_configuration_to_graph(float_graph,
+                                                                                      quant_config=DEFAULTCONFIG,
+                                                                                      mixed_precision_enable=False)
+
+        # Create a Pruner object with the graph and configuration.
+        pruner = Pruner(float_graph_with_compression_config,
+                        DEFAULT_KERAS_INFO,
+                        fw_impl,
+                        target_resource_utilization,
+                        representative_data_gen,
+                        pruning_config,
+                        target_platform_capabilities)
+
+        # Apply the pruning process.
+        pruned_graph = pruner.prune_graph()
+
+        # Retrieve pruning information which includes the pruning masks and scores.
+        pruning_info = pruner.get_pruning_info()
+
+        # Rebuild the pruned graph back into a trainable Keras model.
+        pruned_model, _ = FloatKerasModelBuilder(graph=pruned_graph).build_model()
+        pruned_model.trainable = True
 
+        # Return the pruned model along with its pruning information.
+        return pruned_model, pruning_info
 
 else:
     # If tensorflow is not installed,
     # we raise an exception when trying to use these functions.
-    def keras_post_training_quantization_experimental(*args, **kwargs):
-        Logger.critical('Installing tensorflow is mandatory '
-                        'when using keras_post_training_quantization_experimental. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+    def keras_pruning_experimental(*args, **kwargs):
+        Logger.critical("Tensorflow must be installed to use keras_pruning_experimental. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/ptq/runner.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/ptq/runner.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,9 +10,9 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from model_compression_toolkit.qat.common.qat_config import QATConfig, TrainingMethod
 
-from model_compression_toolkit.qat.keras.quantization_facade import keras_quantization_aware_training_init, keras_quantization_aware_training_finalize
-from model_compression_toolkit.qat.pytorch.quantization_facade import pytorch_quantization_aware_training_init, pytorch_quantization_aware_training_finalize
+from model_compression_toolkit.qat.keras.quantization_facade import keras_quantization_aware_training_init_experimental, keras_quantization_aware_training_finalize_experimental
+from model_compression_toolkit.qat.pytorch.quantization_facade import pytorch_quantization_aware_training_init_experimental, pytorch_quantization_aware_training_finalize_experimental
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/common/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/common/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/common/qat_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/common/qat_config.py`

 * *Files 9% similar despite different names*

```diff
@@ -20,38 +20,43 @@
 from model_compression_toolkit.logger import Logger
 
 
 def is_qat_applicable(node: common.BaseNode,
                       fw_info: FrameworkInfo) -> bool:
     """
     A function for deciding if a layer should be fine-tuned during QAT
+
     Args:
         node (BaseNode): Node for quantization decision
         fw_info (FrameworkInfo): Pytorch quantization information
 
     Returns:
         A boolean whether the layer is to be wrapped with a QuantizeWrapper
     """
 
-    if node.is_weights_quantization_enabled() and not fw_info.is_kernel_op(node.type):
-        Logger.error("QAT Error: Quantizing a node without a kernel isn't supported")
-    return node.is_weights_quantization_enabled() or node.is_activation_quantization_enabled()
+    kernel_attr = fw_info.get_kernel_op_attributes(node.type)[0]
+    return (kernel_attr is not None and node.is_weights_quantization_enabled(kernel_attr)) \
+            or node.is_activation_quantization_enabled()
+
 
 
 class TrainingMethod(Enum):
     """
     An enum for selecting a QAT training method
 
     STE - Standard straight-through estimator. Includes PowerOfTwo, symmetric & uniform quantizers
 
     DQA -  DNN Quantization with Attention. Includes a smooth quantization introduces by DQA method
 
+    LSQ - Learned Step size Quantization. Includes PowerOfTwo, symmetric & uniform quantizers: https://arxiv.org/pdf/1902.08153.pdf
+
     """
     STE = "STE",
-    DQA = "DQA"
+    DQA = "DQA",
+    LSQ = "LSQ"
 
 
 class QATConfig:
     """
     QAT configuration class.
     """
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,7 +11,9 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import model_compression_toolkit.qat.keras.quantizer.ste_rounding.symmetric_ste
 import model_compression_toolkit.qat.keras.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.qat.keras.quantizer.lsq.symmetric_lsq
+import model_compression_toolkit.qat.keras.quantizer.lsq.uniform_lsq
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py`

 * *Files 6% similar despite different names*

```diff
@@ -40,10 +40,9 @@
 
 else:
     class BaseKerasQATTrainableQuantizer(BaseKerasTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
 
             super().__init__(quantization_config)
-            Logger.critical('Installing tensorflow is mandatory '
-                            'when using BaseKerasQATTrainableQuantizer. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use BaseKerasQATTrainableQuantizer. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/quant_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/quant_utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,14 +13,31 @@
 # limitations under the License.
 # ==============================================================================
 
 import tensorflow as tf
 from typing import Tuple
 
 
+def ste_round(x: tf.Tensor) -> tf.Tensor:
+    """
+    Return the rounded values of a tensor.
+    """
+    error = tf.stop_gradient(tf.math.round(x) - x)
+    return error + x
+
+
+def grad_scale(x: tf.Tensor, scale=1.0) -> tf.Tensor:
+    """
+    Return x in forward and x*scale in backward (for scaling the gradients).
+    """
+    x_scaled = scale * x
+    error = tf.stop_gradient(x - x_scaled)
+    return error + x_scaled
+
+
 def adjust_range_to_include_zero(range_min: tf.Tensor,
                                  range_max: tf.Tensor,
                                  n_bits: int) -> Tuple[tf.Tensor, tf.Tensor]:
     """
     Adjusting the quantization range to include representation of 0.0 in the quantization grid.
     For per_channel quantization range_min\range_max should be tensors in the specific shape that allows
     quantization along the channel_axis.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,97 +8,96 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Tuple, Dict, List, Callable
+from typing import List, Dict, Tuple, Callable
+
+from mct_quantizers import PytorchActivationQuantizationHolder, QuantizationTarget
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.qat.common.qat_config import QATConfig
-from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
-from mct_quantizers import QuantizationTarget, KerasActivationQuantizationHolder
+from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
-    get_trainable_quantizer_weights_config, get_trainable_quantizer_activation_config, \
-    get_trainable_quantizer_quantization_candidates
+    get_trainable_quantizer_quantization_candidates, get_trainable_quantizer_weights_config, \
+    get_trainable_quantizer_activation_config
+from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
-
 def get_activation_quantizer_holder(n: common.BaseNode,
                                     qat_config: QATConfig) -> Callable:
     """
-    Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
+    Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
     If the layer is not supposed to be wrapped with activation quantizers - return None.
 
     Args:
-        n: Node to get KerasActivationQuantizationHolder to attach in its output.
-        qat_config: Configuration of QAT (such as training methods for example).
+        n: Node for which to retrieve anActivationQuantizationHolder to attach to its output.
+        qat_config: QAT configuration (for example, training methods).
 
     Returns:
-        A KerasActivationQuantizationHolder layer for the node activation quantization.
+        A ActivationQuantizationHolder layer for the node's activation quantization.
     """
     _, activation_quantizers = quantization_builder(n,
-                                                    qat_config,
-                                                    DEFAULT_KERAS_INFO)
+                                                    qat_config)
 
     # Holder by definition uses a single quantizer for the activation quantization
     # thus we make sure this is the only possible case (unless it's a node with no activation
     # quantization, which in this case has an empty list).
     if len(activation_quantizers) == 1:
-        return KerasActivationQuantizationHolder(activation_quantizers[0])
-    Logger.error(f'KerasActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers were found for node {n}')
+        return PytorchActivationQuantizationHolder(activation_quantizers[0])
+    Logger.critical(f'ActivationQuantizationHolder supports only a single quantizer, but ({len(activation_quantizers)}) quantizers were found for node {n}.')
 
 
 def quantization_builder(n: common.BaseNode,
                          qat_config: QATConfig,
-                         fw_info: FrameworkInfo,
-                         ) -> Tuple[Dict[str, BaseKerasQATTrainableQuantizer], List[BaseKerasQATTrainableQuantizer]]:
+                         kernel_attr: str = None,
+                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
+                                    List[BasePytorchQATTrainableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration.
 
     Args:
         n: Node to build its QuantizeConfig.
         qat_config (QATConfig): QAT configuration
-        fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
 
     Returns:
         weights_quantizers: A dictionary between a weight's name to its quantizer.
-        activation_quantizers: A list of activations quantization, one for each layer output.
+        activation_quantizers: A list of activations quantization, one for each layer output.).
     """
+
     if len(n.candidates_quantization_cfg) > 1:
-        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n)
+        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n, kernel_attr)
     else:
         wq_cand, aq_cand = None, None
 
     weight_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
-
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during QAT
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Weights,
                                                         qat_config.weight_training_method,
                                                         quant_method,
-                                                        BaseKerasQATTrainableQuantizer)
-        attributes = fw_info.get_kernel_op_attributes(n.type)
-        for attr in attributes:
-            weight_quantizers.update({attr: quantizer_class(get_trainable_quantizer_weights_config(n, wq_cand),
-                                                            **qat_config.weight_quantizer_params_override)})
+                                                        BasePytorchQATTrainableQuantizer)
+
+        weight_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                               attr_name=kernel_attr,
+                                                                                               weights_quantization_candidates=wq_cand),
+                                                        **qat_config.weight_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
-        # single output -> normalize to list of output_shapes
-        output_shapes = n.output_shape if isinstance(n.output_shape[0], (list, tuple)) else [n.output_shape]
-
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Activation,
                                                         qat_config.activation_training_method,
                                                         quant_method,
-                                                        BaseKerasQATTrainableQuantizer)
+                                                        BasePytorchQATTrainableQuantizer)
 
         activation_quantizers = [quantizer_class(get_trainable_quantizer_activation_config(n, aq_cand),
-                                                 **qat_config.activation_quantizer_params_override)] * len(output_shapes)
+                                                 **qat_config.activation_quantizer_params_override)]
 
     return weight_quantizers, activation_quantizers
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantization_facade.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/runner.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,233 +8,206 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import copy
-from typing import Callable
-from functools import partial
 
-from model_compression_toolkit.constants import FOUND_TORCH, PYTORCH
 
-from model_compression_toolkit.core import CoreConfig
-from model_compression_toolkit.core import common
-from model_compression_toolkit.logger import Logger
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
-    TargetPlatformCapabilities
-from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
-from model_compression_toolkit.ptq.runner import ptq_runner
-
-if FOUND_TORCH:
-    import torch.nn as nn
-    from torch.nn import Module
-    from mct_quantizers import PytorchActivationQuantizationHolder
-    from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
-    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
-    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-    from mct_quantizers import PytorchQuantizationWrapper
-    from model_compression_toolkit import get_target_platform_capabilities
-    from model_compression_toolkit.qat.common.qat_config import QATConfig
-    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import get_activation_quantizer_holder
-    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import quantization_builder
-
-    DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
-
-
-    def qat_wrapper(n: common.BaseNode,
-                    module: nn.Module,
-                    qat_config: QATConfig):
-        """
-        A function which takes a computational graph node and a pytorch module and perform the quantization wrapping
-        Args:
-            n: A node of mct graph.
-            module: A Pytorch module
-            qat_config (QATConfig): QAT configuration
-        Returns: Wrapped layer
-
-        """
-        if is_qat_applicable(n, DEFAULT_PYTORCH_INFO):
-            weights_quantizers, _ = quantization_builder(n, qat_config, DEFAULT_PYTORCH_INFO)
-            if len(weights_quantizers) > 0:
-                return PytorchQuantizationWrapper(module, weights_quantizers)
-        return module
-
-
-    def pytorch_quantization_aware_training_init(in_model: Module,
-                                                 representative_data_gen: Callable,
-                                                 target_kpi: KPI = None,
-                                                 core_config: CoreConfig = CoreConfig(),
-                                                 qat_config: QATConfig = QATConfig(),
-                                                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
-                                                 target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC):
-        """
-         Prepare a trained Pytorch model for quantization aware training. First the model quantization is optimized
-         with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
-         quantized using a symmetric quantization thresholds (power of two).
-         The model is first optimized using several transformations (e.g. BatchNormalization folding to
-         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
-         being collected for each layer's output (and input, depends on the quantization configuration).
-         For each possible bit width (per layer) a threshold is then being calculated using the collected
-         statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
-         a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
-         nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
-         quantization wrapper's weight quantizer.
-         In order to limit the maximal model's size, a target KPI need to be passed after weights_memory
-         is set (in bytes).
-
-         Args:
-             in_model (Model): Pytorch model to quantize.
-             representative_data_gen (Callable): Dataset used for initial calibration.
-             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
-             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
-             qat_config (QATConfig): QAT configuration
-             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).  `Default Pytorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Pytorch model according to.
-
-         Returns:
-
-             A quantized model.
-             User information that may be needed to handle the quantized model.
-
-         Examples:
-
-             Import MCT:
-
-             >>> import model_compression_toolkit as mct
-
-             Import a Pytorch model:
-
-             >>> from torchvision.models import mobilenet_v2
-             >>> model = mobilenet_v2(pretrained=True)
-
-            Create a random dataset generator, for required number of calibration iterations (num_calibration_batches):
-            In this example a random dataset of 10 batches each containing 4 images is used.
-
-            >>> import numpy as np
-            >>> num_calibration_batches = 10
-            >>> def repr_datagen():
-            >>>     for _ in range(num_calibration_batches):
-            >>>         yield [np.random.random((4, 3, 224, 224))]
-
-             Create a MCT core config, containing the quantization configuration:
-
-             >>> config = mct.core.CoreConfig()
-
-             Pass the model, the representative dataset generator, the configuration and the target KPI to get a
-             quantized model. Now the model contains quantizer wrappers for fine tunning the weights:
-
-             >>> quantized_model, quantization_info = pytorch_quantization_aware_training_init(model, repr_datagen, core_config=config)
-
-             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
-
-         """
-
-        if core_config.mixed_precision_enable:
-            if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                Logger.error("Given quantization config to mixed-precision facade is not of type "
-                             "MixedPrecisionQuantizationConfigV2. Please use pytorch_post_training_quantization API,"
-                             "or pass a valid mixed precision configuration.")
-
-            Logger.info("Using experimental mixed-precision quantization. "
-                        "If you encounter an issue please file a bug.")
-
-        tb_w = _init_tensorboard_writer(fw_info)
-
-        fw_impl = PytorchImplementation()
-
-        tg, bit_widths_config = core_runner(in_model=in_model,
-                                            representative_data_gen=representative_data_gen,
-                                            core_config=core_config,
-                                            fw_info=DEFAULT_PYTORCH_INFO,
-                                            fw_impl=fw_impl,
-                                            tpc=target_platform_capabilities,
-                                            target_kpi=target_kpi,
-                                            tb_w=tb_w)
+from typing import Callable, Tuple, Any, List, Dict
 
-        tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
+import numpy as np
 
-        _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
-
-        qat_model, user_info = PyTorchModelBuilder(graph=tg,
-                                                   fw_info=fw_info,
-                                                   wrapper=_qat_wrapper,
-                                                   get_activation_quantizer_holder_fn=partial(
-                                                       get_activation_quantizer_holder,
-                                                       qat_config=qat_config)).build_model()
-
-        user_info.mixed_precision_cfg = bit_widths_config
-
-        # Remove fw_info from graph to enable saving the pytorch model (fw_info can not be pickled)
-        delattr(qat_model.graph, 'fw_info')
-
-        return qat_model, user_info
-
-
-    def pytorch_quantization_aware_training_finalize(in_model: Module):
-        """
-         Convert a model fine-tuned by the user to a network with QuantizeWrappers containing
-         InferableQuantizers, that quantizes both the layers weights and outputs
-
-         Args:
-             in_model (Model): Pytorch model to remove QuantizeWrappers.
-
-         Returns:
-             A quantized model with QuantizeWrappers and InferableQuantizers.
-
-         Examples:
-
-             Import MCT:
-
-             >>> import model_compression_toolkit as mct
-
-             Import a Pytorch model:
-
-             >>> from torchvision.models import mobilenet_v2
-             >>> model = mobilenet_v2(pretrained=True)
-
-             Create a random dataset generator:
-
-             >>> import numpy as np
-             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
-
-             Create a MCT core config, containing the quantization configuration:
-
-             >>> config = mct.core.CoreConfig()
-
-             Pass the model, the representative dataset generator, the configuration and the target KPI to get a
-             quantized model:
-
-             >>> quantized_model, quantization_info = pytorch_quantization_aware_training_init(model, repr_datagen, core_config=config)
-
-             Use the quantized model for fine-tuning. Finally, remove the quantizer wrappers and keep a quantize model ready for inference.
-
-             >>> quantized_model = mct.pytorch_quantization_aware_training_finalize(quantized_model)
-
-         """
-        for _, layer in in_model.named_children():
-            if isinstance(layer, (PytorchQuantizationWrapper, PytorchActivationQuantizationHolder)):
-                layer.convert_to_inferable_quantizers()
-
-        return in_model
-
-
-else:
-    # If torch is not installed,
-    # we raise an exception when trying to use these functions.
-    def pytorch_quantization_aware_training_init(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_quantization_aware_training_init. '
-                        'Could not find the torch package.')  # pragma: no cover
-
-
-    def pytorch_quantization_aware_training_finalize(*args, **kwargs):
-        Logger.critical('Installing Pytorch is mandatory '
-                        'when using pytorch_quantization_aware_training_finalize. '
-                        'Could not find the torch package.')  # pragma: no cover
+from model_compression_toolkit.core.common import FrameworkInfo
+from model_compression_toolkit.core.common.hessian.hessian_info_service import HessianInfoService
+from model_compression_toolkit.core.graph_prep_runner import graph_preparation_runner
+from model_compression_toolkit.core.quantization_prep_runner import quantization_preparation_runner
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.core.common.graph.base_graph import Graph
+from model_compression_toolkit.core.common.mixed_precision.bit_width_setter import set_bit_widths
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.resource_utilization import ResourceUtilization, RUTarget
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_aggregation_methods import MpRuAggregation
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_functions_mapping import ru_functions_mapping
+from model_compression_toolkit.core.common.mixed_precision.resource_utilization_tools.ru_methods import MpRuMetric
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_facade import search_bit_width
+from model_compression_toolkit.core.common.network_editors.edit_network import edit_network_graph
+from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.core.common.visualization.final_config_visualizer import \
+    WeightsFinalBitwidthConfigVisualizer, \
+    ActivationFinalBitwidthConfigVisualizer
+from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter, \
+    finalize_bitwidth_in_tb
+
+
+def core_runner(in_model: Any,
+                representative_data_gen: Callable,
+                core_config: CoreConfig,
+                fw_info: FrameworkInfo,
+                fw_impl: FrameworkImplementation,
+                tpc: TargetPlatformCapabilities,
+                target_resource_utilization: ResourceUtilization = None,
+                tb_w: TensorboardWriter = None):
+    """
+    Quantize a trained model using post-training quantization.
+    First, the model graph is optimized using several transformations (e.g. folding BatchNormalization to preceding
+    layers).
+    Second, statistics (e.g. min/max, histogram, etc.) are collected for each layer's output
+    (and input, depends on the quantization configuration) using a given representative dataset.
+    Next, quantization parameters are calculated using the collected statistics
+    (both coefficients and activations by default).
+
+    Args:
+        in_model: Model to quantize.
+        representative_data_gen: Dataset used for calibration.
+        core_config: CoreConfig containing parameters of how the model should be quantized
+        fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices,
+        groups of layers by how they should be quantized, etc.).
+        fw_impl: FrameworkImplementation object with a specific framework methods implementation.
+        tpc: TargetPlatformCapabilities object that models the inference target platform and
+                                              the attached framework operator's information.
+        target_resource_utilization: ResourceUtilization to constraint the search of the mixed-precision configuration for the model.
+        tb_w: TensorboardWriter object for logging
+
+    Returns:
+        An internal graph representation of the input model.
+
+    """
+
+    # Warn is representative dataset has batch-size == 1
+    batch_data = iter(representative_data_gen()).__next__()
+    if isinstance(batch_data, list):
+        batch_data = batch_data[0]
+    if batch_data.shape[0] == 1:
+        Logger.warning('representative_data_gen generates a batch size of 1 which can be slow for optimization:'
+                       ' consider increasing the batch size')
+
+    # Checking whether to run mixed precision quantization
+    if target_resource_utilization is not None:
+        if core_config.mixed_precision_config is None:
+            Logger.critical("Provided an initialized target_resource_utilization, that means that mixed precision quantization is "
+                            "enabled, but the provided MixedPrecisionQuantizationConfig is None.")
+        core_config.mixed_precision_config.set_mixed_precision_enable()
+
+    graph = graph_preparation_runner(in_model,
+                                     representative_data_gen,
+                                     core_config.quantization_config,
+                                     fw_info,
+                                     fw_impl,
+                                     tpc,
+                                     tb_w,
+                                     mixed_precision_enable=core_config.mixed_precision_enable)
+
+    hessian_info_service = HessianInfoService(graph=graph,
+                                              representative_dataset=representative_data_gen,
+                                              fw_impl=fw_impl)
+
+    tg = quantization_preparation_runner(graph=graph,
+                                         representative_data_gen=representative_data_gen,
+                                         core_config=core_config,
+                                         fw_info=fw_info,
+                                         fw_impl=fw_impl,
+                                         tb_w=tb_w)
+
+    ######################################
+    # Finalize bit widths
+    ######################################
+    if core_config.mixed_precision_enable:
+        if core_config.mixed_precision_config.configuration_overwrite is None:
+
+            bit_widths_config = search_bit_width(tg,
+                                                 fw_info,
+                                                 fw_impl,
+                                                 target_resource_utilization,
+                                                 core_config.mixed_precision_config,
+                                                 representative_data_gen,
+                                                 hessian_info_service=hessian_info_service)
+        else:
+            Logger.warning(
+                f'Mixed Precision has overwrite bit-width configuration{core_config.mixed_precision_config.configuration_overwrite}')
+            bit_widths_config = core_config.mixed_precision_config.configuration_overwrite
+
+    else:
+        bit_widths_config = []
+
+    tg = set_bit_widths(core_config.mixed_precision_enable,
+                        tg,
+                        bit_widths_config)
+
+    # Edit the graph again after finalizing the configurations.
+    # This is since some actions regard the final configuration and should be edited.
+    edit_network_graph(tg, fw_info, core_config.debug_config.network_editor)
+
+    _set_final_resource_utilization(graph=tg,
+                                    final_bit_widths_config=bit_widths_config,
+                                    ru_functions_dict=ru_functions_mapping,
+                                    fw_info=fw_info,
+                                    fw_impl=fw_impl)
+
+    if core_config.mixed_precision_enable:
+        # Retrieve lists of tuples (node, node's final weights/activation bitwidth)
+        weights_conf_nodes_bitwidth = tg.get_final_weights_config(fw_info)
+        activation_conf_nodes_bitwidth = tg.get_final_activation_config()
+
+        if len(weights_conf_nodes_bitwidth) > 0:
+            Logger.info(
+                f'Final weights bit-width configuration: {[node_b[1] for node_b in weights_conf_nodes_bitwidth]}')
+
+        if len(activation_conf_nodes_bitwidth) > 0:
+            Logger.info(
+                f'Final activation bit-width configuration: {[node_b[1] for node_b in activation_conf_nodes_bitwidth]}')
+
+        if tb_w is not None:
+            finalize_bitwidth_in_tb(tb_w, weights_conf_nodes_bitwidth, activation_conf_nodes_bitwidth)
+
+    return tg, bit_widths_config, hessian_info_service
+
+
+def _set_final_resource_utilization(graph: Graph,
+                                    final_bit_widths_config: List[int],
+                                    ru_functions_dict: Dict[RUTarget, Tuple[MpRuMetric, MpRuAggregation]],
+                                    fw_info: FrameworkInfo,
+                                    fw_impl: FrameworkImplementation):
+    """
+    Computing the resource utilization of the model according to the final bit-width configuration,
+    and setting it (inplace) in the graph's UserInfo field.
+
+    Args:
+        graph: Graph to compute the resource utilization for.
+        final_bit_widths_config: The final bit-width configuration to quantize the model accordingly.
+        ru_functions_dict: A mapping between a RUTarget and a pair of resource utilization method and resource utilization aggregation functions.
+        fw_info: A FrameworkInfo object.
+        fw_impl: FrameworkImplementation object with specific framework methods implementation.
+
+    """
+
+    final_ru_dict = {}
+    for ru_target, ru_funcs in ru_functions_dict.items():
+        ru_method, ru_aggr = ru_funcs
+        if ru_target == RUTarget.BOPS:
+            final_ru_dict[ru_target] = \
+            ru_aggr(ru_method(final_bit_widths_config, graph, fw_info, fw_impl, False), False)[0]
+        else:
+            non_conf_ru = ru_method([], graph, fw_info, fw_impl)
+            conf_ru = ru_method(final_bit_widths_config, graph, fw_info, fw_impl)
+            if len(final_bit_widths_config) > 0 and len(non_conf_ru) > 0:
+                final_ru_dict[ru_target] = ru_aggr(np.concatenate([conf_ru, non_conf_ru]), False)[0]
+            elif len(final_bit_widths_config) > 0 and len(non_conf_ru) == 0:
+                final_ru_dict[ru_target] = ru_aggr(conf_ru, False)[0]
+            elif len(final_bit_widths_config) == 0 and len(non_conf_ru) > 0:
+                # final_bit_widths_config == 0 ==> no configurable nodes,
+                # thus, ru can be computed from non_conf_ru alone
+                final_ru_dict[ru_target] = ru_aggr(non_conf_ru, False)[0]
+            else:
+                # No relevant nodes have been quantized with affect on the given target - since we only consider
+                # in the model's final size the quantized layers size, this means that the final size for this target
+                # is zero.
+                Logger.warning(f"No relevant quantized layers for the ru target {ru_target} were found, the recorded"
+                               f"final ru for this target would be 0.")
+                final_ru_dict[ru_target] = 0
+
+    final_ru = ResourceUtilization()
+    final_ru.set_resource_utilization_by_target(final_ru_dict)
+    graph.user_info.final_resource_utilization = final_ru
+    graph.user_info.mixed_precision_cfg = final_bit_widths_config
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,8 +10,10 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.symmetric_ste
-import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.qat.pytorch.quantizer.lsq.symmetric_lsq
+import model_compression_toolkit.qat.pytorch.quantizer.lsq.uniform_lsq
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py`

 * *Files 19% similar despite different names*

```diff
@@ -40,10 +40,9 @@
             super().__init__(quantization_config)
 
 else:
     class BasePytorchQATTrainableQuantizer(BasePytorchTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             super().__init__(quantization_config)
-            Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchQATTrainableQuantizer. '
-                            'Could not find torch package.')  # pragma: no cover
+            Logger.critical("Installation of PyTorch is required to use BasePytorchQATTrainableQuantizer. "
+                            "The 'torch' package was not found.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,94 +8,99 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import List, Dict, Tuple, Callable
-
-from mct_quantizers import PytorchActivationQuantizationHolder, QuantizationTarget
+from typing import Tuple, Dict, List, Callable
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.qat.common.qat_config import QATConfig
-from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.qat.common.qat_config import QATConfig
+from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
+from mct_quantizers import QuantizationTarget, KerasActivationQuantizationHolder
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
-    get_trainable_quantizer_quantization_candidates, get_trainable_quantizer_weights_config, \
-    get_trainable_quantizer_activation_config
-from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
+    get_trainable_quantizer_weights_config, get_trainable_quantizer_activation_config, \
+    get_trainable_quantizer_quantization_candidates
 from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
     get_trainable_quantizer_class
 
+
 def get_activation_quantizer_holder(n: common.BaseNode,
                                     qat_config: QATConfig) -> Callable:
     """
-    Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
+    Retrieve a KerasActivationQuantizationHolder layer to use for activation quantization for a node.
     If the layer is not supposed to be wrapped with activation quantizers - return None.
 
     Args:
-        n: Node for which to retrieve anActivationQuantizationHolder to attach to its output.
-        qat_config: QAT configuration (for example, training methods).
+        n: Node to get KerasActivationQuantizationHolder to attach in its output.
+        qat_config: Configuration of QAT (such as training methods for example).
 
     Returns:
-        A ActivationQuantizationHolder layer for the node's activation quantization.
+        A KerasActivationQuantizationHolder layer for the node activation quantization.
     """
     _, activation_quantizers = quantization_builder(n,
-                                                    qat_config,
-                                                    DEFAULT_PYTORCH_INFO)
+                                                    qat_config)
 
     # Holder by definition uses a single quantizer for the activation quantization
     # thus we make sure this is the only possible case (unless it's a node with no activation
     # quantization, which in this case has an empty list).
     if len(activation_quantizers) == 1:
-        return PytorchActivationQuantizationHolder(activation_quantizers[0])
-    Logger.error(f'ActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers were found for node {n}')
+        return KerasActivationQuantizationHolder(activation_quantizers[0])
+    Logger.critical(f'KerasActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers were found for node {n}.')
 
 
 def quantization_builder(n: common.BaseNode,
                          qat_config: QATConfig,
-                         fw_info: FrameworkInfo,
-                         ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
-                                    List[BasePytorchQATTrainableQuantizer]]:
+                         kernel_attr: str = None,
+                         ) -> Tuple[Dict[str, BaseKerasQATTrainableQuantizer], List[BaseKerasQATTrainableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration.
 
     Args:
         n: Node to build its QuantizeConfig.
         qat_config (QATConfig): QAT configuration
-        fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
+        kernel_attr: A potential kernel attribute name to build its trainable quantizer.
+
 
     Returns:
         weights_quantizers: A dictionary between a weight's name to its quantizer.
-        activation_quantizers: A list of activations quantization, one for each layer output.).
+        activation_quantizers: A list of activations quantization, one for each layer output.
     """
     if len(n.candidates_quantization_cfg) > 1:
-        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n)
+        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n, kernel_attr)
     else:
         wq_cand, aq_cand = None, None
 
     weight_quantizers = {}
-    if n.is_weights_quantization_enabled():
-        quant_method = n.final_weights_quantization_cfg.weights_quantization_method
+    if kernel_attr is not None and n.is_weights_quantization_enabled(kernel_attr):
+        # Only nodes with kernel attribute are trainable during QAT
+        quant_method = n.final_weights_quantization_cfg.get_attr_config(kernel_attr).weights_quantization_method
+
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Weights,
                                                         qat_config.weight_training_method,
                                                         quant_method,
-                                                        BasePytorchQATTrainableQuantizer)
-        attributes = fw_info.get_kernel_op_attributes(n.type)
-        for attr in attributes:
-            weight_quantizers.update({attr: quantizer_class(get_trainable_quantizer_weights_config(n, wq_cand),
-                                                           **qat_config.weight_quantizer_params_override)})
+                                                        BaseKerasQATTrainableQuantizer)
+
+        weight_quantizers.update({kernel_attr: quantizer_class(get_trainable_quantizer_weights_config(n,
+                                                                                                      attr_name=kernel_attr,
+                                                                                                      weights_quantization_candidates=wq_cand),
+                                                        **qat_config.weight_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
+        # single output -> normalize to list of output_shapes
+        output_shapes = n.output_shape if isinstance(n.output_shape[0], (list, tuple)) else [n.output_shape]
+
         quantizer_class = get_trainable_quantizer_class(QuantizationTarget.Activation,
                                                         qat_config.activation_training_method,
                                                         quant_method,
-                                                        BasePytorchQATTrainableQuantizer)
+                                                        BaseKerasQATTrainableQuantizer)
 
         activation_quantizers = [quantizer_class(get_trainable_quantizer_activation_config(n, aq_cand),
-                                                 **qat_config.activation_quantizer_params_override)]
+                                                 **qat_config.activation_quantizer_params_override)] * len(output_shapes)
 
     return weight_quantizers, activation_quantizers
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -36,17 +36,30 @@
         max_val: maximum value for clipping
     Returns:
         clipped variable
     """
     return (torch.clip(x, min=min_val, max=max_val) - x).detach() + x
 
 
-def fix_range_to_include_zero(range_min: torch.Tensor,
-                              range_max: torch.Tensor,
-                              n_bits: int) -> Tuple[torch.Tensor, torch.Tensor]:
+def grad_scale(x: torch.Tensor, scale=1.0) -> torch.Tensor:
+    """
+    Gradient scale
+    Args:
+        x: input variable
+        scale: scale factor
+    Returns:
+        x in forward and x*scale in backward (for scaling the gradients).
+    """
+    x_scaled = x * scale
+    return (x - x_scaled).detach() + x_scaled
+
+
+def adjust_range_to_include_zero(range_min: torch.Tensor,
+                                 range_max: torch.Tensor,
+                                 n_bits: int) -> Tuple[torch.Tensor, torch.Tensor]:
     """
     Adjusting the quantization range to include representation of 0.0 in the quantization grid.
     If quantization per-channel, then range_min and range_max should be tensors in the specific shape that allows
     quantization along the channel_axis.
     Args:
         range_min: min bound of the quantization range (before adjustment).
         range_max: max bound of the quantization range (before adjustment).
@@ -116,15 +129,15 @@
         range_min: minimum bound of the range for quantization (or array of min values per channel).
         range_max: maximum bound of the range for quantization (or array of max values per channel).
         n_bits: Number of bits to quantize the tensor.
     Returns:
         Quantized data.
     """
     # adjusts the quantization range so the quantization grid includes zero.
-    a, b = fix_range_to_include_zero(range_min, range_max, n_bits)
+    a, b = adjust_range_to_include_zero(range_min, range_max, n_bits)
 
     # Compute the step size of quantized values.
     delta_tensor = (b - a) / (2 ** n_bits - 1)
 
     # Apply rounding
     input_tensor_int = ste_round((tensor_data - a) / delta_tensor)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/constants.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,19 +9,12 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-# TP Model constants
-OPS_SET_LIST = 'ops_set_list'
-
-# Version
-LATEST = 'latest'
-
-
-# Supported TP models names:
-DEFAULT_TP_MODEL = 'default'
-IMX500_TP_MODEL = 'imx500'
-TFLITE_TP_MODEL = 'tflite'
-QNNPACK_TP_MODEL = 'qnnpack'
+# Quantizers constants (for GPTQ, QAT, etc.)
+FQ_MIN = "min"
+FQ_MAX = "max"
+THRESHOLD_TENSOR = "ptq_threshold_tensor"
+WEIGHTS_QUANTIZATION_PARAMS = 'weights_quantization_params'
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/immutable.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/immutable.py`

 * *Files 18% similar despite different names*

```diff
@@ -10,14 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Any
 
+from model_compression_toolkit.logger import Logger
+
 
 class ImmutableClass(object):
     """
     Class to make inherits classes immutable.
     """
     _initialized = False
 
@@ -32,22 +34,22 @@
 
         Args:
             *args: Arguments to set to attribute.
             **kwargs: Keyword-arguments to set to attribute.
 
         """
         if self._initialized:
-            raise Exception('Immutable class. Can\'t edit attributes')
+            Logger.critical("Immutable class. Can't edit attributes.")
         else:
             object.__setattr__(self,
                                *args,
                                **kwargs)
 
     def initialized_done(self):
         """
 
         Method to use when object should be immutable.
 
         """
         if self._initialized:
-            raise Exception('reinitialized')  # Can not get finalized again.
+            Logger.critical('Object reinitialization error: object cannot be finalized again.')  # Can not get finalized again.
         self._initialized = True  # Finalize object.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,20 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from model_compression_toolkit.target_platform_capabilities.target_platform.fusing import Fusing
-from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
-    TargetPlatformCapabilities, OperationsSetToLayers, Smaller, SmallerEq, NotEq, Eq, GreaterEq, Greater, LayerFilterParams, OperationsToLayers, get_current_tpc
-
-from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model import \
-    get_default_quantization_config_options, TargetPlatformModel
-
-from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, \
-    QuantizationConfigOptions
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import AttributeFilter
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities, OperationsSetToLayers, Smaller, SmallerEq, NotEq, Eq, GreaterEq, Greater, LayerFilterParams, OperationsToLayers, get_current_tpc
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model import get_default_quantization_config_options, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, QuantizationConfigOptions, AttributeQuantizationConfig
 from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorsSet, OperatorSetConcat
 
 from mct_quantizers import QuantizationMethod
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,15 +36,15 @@
     def get(self):
         """
 
         Returns: The current TargetPlatformModel that is being defined.
 
         """
         if self.tp_model is None:
-            Logger.error('Target platform model is not initialized.')  # pragma: no cover
+            Logger.critical('Target platform model is not initialized.')  # pragma: no cover
         return self.tp_model
 
     def reset(self):
         """
 
         Reset the current TargetPlatformModel so a new TargetPlatformModel can be wrapped and
         used as the current TargetPlatformModel object.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py`

 * *Files 26% similar despite different names*

```diff
@@ -10,41 +10,76 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
-from typing import Any
+from typing import Any, List, Union
 
-from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat, \
+    OperatorsSet
 from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model_component import TargetPlatformModelComponent
 
 
 class Fusing(TargetPlatformModelComponent):
-
-    def __init__(self, operator_groups_list, name=None):
+    """
+     Fusing defines a list of operators that should be combined and treated as a single operator,
+     hence no quantization is applied between them.
+    """
+
+    def __init__(self,
+                 operator_groups_list: List[Union[OperatorsSet, OperatorSetConcat]],
+                 name: str = None):
+        """
+        Args:
+            operator_groups_list (List[Union[OperatorsSet, OperatorSetConcat]]): A list of operator groups, each being either an OperatorSetConcat or an OperatorsSet.
+            name (str): The name for the Fusing instance. If not provided, it's generated from the operator groups' names.
+        """
         assert isinstance(operator_groups_list,
                           list), f'List of operator groups should be of type list but is {type(operator_groups_list)}'
         assert len(operator_groups_list) >= 2, f'Fusing can not be created for a single operators group'
+
+        # Generate a name from the operator groups if no name is provided
         if name is None:
             name = '_'.join([x.name for x in operator_groups_list])
+
         super().__init__(name)
         self.operator_groups_list = operator_groups_list
 
-    def contains(self, other: Any):
+    def contains(self, other: Any) -> bool:
+        """
+        Determines if the current Fusing instance contains another Fusing instance.
+
+        Args:
+            other: The other Fusing instance to check against.
+
+        Returns:
+            A boolean indicating whether the other instance is contained within this one.
+        """
         if not isinstance(other, Fusing):
             return False
+
+        # Check for containment by comparing operator groups
         for i in range(len(self.operator_groups_list) - len(other.operator_groups_list) + 1):
             for j in range(len(other.operator_groups_list)):
-                if self.operator_groups_list[i + j] != other.operator_groups_list[j] and not (isinstance(self.operator_groups_list[i + j], OperatorSetConcat) and (other.operator_groups_list[j] in self.operator_groups_list[i + j].op_set_list)):
+                if self.operator_groups_list[i + j] != other.operator_groups_list[j] and not (
+                        isinstance(self.operator_groups_list[i + j], OperatorSetConcat) and (
+                        other.operator_groups_list[j] in self.operator_groups_list[i + j].op_set_list)):
                     break
             else:
+                # If all checks pass, the other Fusing instance is contained
                 return True
+        # Other Fusing instance is not contained
         return False
 
-
     def get_info(self):
+        """
+        Retrieves information about the Fusing instance, including its name and the sequence of operator groups.
+
+        Returns:
+            A dictionary with the Fusing instance's name as the key and the sequence of operator groups as the value,
+            or just the sequence of operator groups if no name is set.
+        """
         if self.name is not None:
             return {self.name: ' -> '.join([x.name for x in self.operator_groups_list])}
-        return ' -> '.join([x.name for x in self.operator_groups_list])
-
+        return ' -> '.join([x.name for x in self.operator_groups_list])
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/reader/node_holders.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,31 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from enum import Enum
 
 
-class QuantizationFormat(Enum):
-    FAKELY_QUANT = 0
-    INT8 = 1
+import torch
+
+from model_compression_toolkit.core.pytorch.constants import PLACEHOLDER
+
+
+class DummyPlaceHolder(torch.nn.Module):
+    """
+    Class for PlaceHolder operator since a Pytorch model doesn't have one but FX does.
+    """
+
+    def __name__(self):
+        return PLACEHOLDER
+
+    def forward(self, x):
+        return x
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py`

 * *Files 19% similar despite different names*

```diff
@@ -72,14 +72,15 @@
         self.name = name
         self.operator_set = []
         assert isinstance(default_qco, QuantizationConfigOptions)
         assert len(default_qco.quantization_config_list) == 1, \
             f'Default QuantizationConfigOptions must contain only one option'
         self.default_qco = default_qco
         self.fusing_patterns = []
+        self.is_simd_padding = False
 
     def get_config_options_by_operators_set(self,
                                             operators_set_name: str) -> QuantizationConfigOptions:
         """
         Get the QuantizationConfigOptions of a OperatorsSet by the OperatorsSet name.
         If the name is not in the model, the default QuantizationConfigOptions is returned.
 
@@ -151,15 +152,15 @@
 
         """
         if isinstance(tp_model_component, Fusing):
             self.fusing_patterns.append(tp_model_component)
         elif isinstance(tp_model_component, OperatorsSetBase):
             self.operator_set.append(tp_model_component)
         else:
-            raise Exception(f'Trying to append an unfamiliar TargetPlatformModelComponent of type: {type(tp_model_component)}')
+            Logger.critical(f'Attempted to append an unrecognized TargetPlatformModelComponent of type: {type(tp_model_component)}.')
 
     def __enter__(self):
         """
         Start defining the TargetPlatformModel using 'with'.
 
         Returns: Initialized TargetPlatformModel object.
 
@@ -187,15 +188,15 @@
         Assert model is valid.
         Model is invalid if, for example, it contains multiple operator sets with the same name,
         as their names should be unique.
 
         """
         opsets_names = [op.name for op in self.operator_set]
         if (len(set(opsets_names)) != len(opsets_names)):
-            Logger.error(f'OperatorsSet must have unique names')
+            Logger.critical(f'Operator Sets must have unique names.')
 
     def get_default_config(self) -> OpQuantizationConfig:
         """
 
         Returns:
 
         """
@@ -220,15 +221,19 @@
         """
 
         Display the TargetPlatformModel.
 
         """
         pprint.pprint(self.get_info(), sort_dicts=False)
 
-    def set_quantization_format(self,
-                                quantization_format: Any):
+    def set_simd_padding(self,
+                         is_simd_padding: bool):
         """
-        Set quantization format.
+        Set flag is_simd_padding to indicate whether this TP model defines
+        that padding due to SIMD constrains occurs.
+
         Args:
-            quantization_format: A quantization format (fake-quant, int8 etc.) from enum QuantizationFormat.
+            is_simd_padding: Whether this TP model defines that padding due to SIMD constrains occurs.
+
         """
-        self.quantization_format = quantization_format
+        self.is_simd_padding = is_simd_padding
+
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py`

 * *Files 2% similar despite different names*

```diff
@@ -83,29 +83,29 @@
             other: Filter to add to self with logic OR.
 
         Returns:
             OrAttributeFilter that filters with OR between the current AttributeFilter and the passed AttributeFilter.
         """
 
         if not isinstance(other, AttributeFilter):
-            Logger.error("Not an attribute filter. Can not run an OR operation.")  # pragma: no cover
+            Logger.critical("Not an attribute filter. Cannot perform an 'OR' operation.")  # pragma: no cover
         return OrAttributeFilter(self, other)
 
     def __and__(self, other: Any):
         """
         Create a filter that combines multiple AttributeFilters with a logic AND between them.
 
         Args:
             other: Filter to add to self with logic AND.
 
         Returns:
             AndAttributeFilter that filters with AND between the current AttributeFilter and the passed AttributeFilter.
         """
         if not isinstance(other, AttributeFilter):
-            Logger.error("Not an attribute filter. Can not run an AND operation.")  # pragma: no cover
+            Logger.critical("Not an attribute filter. Can not perform an 'AND' operation.")  # pragma: no cover
         return AndAttributeFilter(self, other)
 
     def match(self,
               layer_config: Dict[str, Any]) -> bool:
         """
         Check whether the passed configuration matches the filter.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from model_compression_toolkit.logger import Logger
 
 
 def get_current_tpc():
     """
 
     Returns: The current TargetPlatformCapabilities that is being used and accessed.
 
@@ -34,15 +35,15 @@
     def get(self):
         """
 
         Returns: The current TargetPlatformCapabilities that is being defined.
 
         """
         if self.tpc is None:
-            raise Exception('TargetPlatformCapabilities is not initialized.')
+            Logger.critical("'TargetPlatformCapabilities' (TPC) instance is not initialized.")
         return self.tpc
 
     def reset(self):
         """
 
         Reset the current TargetPlatformCapabilities so a new TargetPlatformCapabilities can be wrapped and
         used as the current TargetPlatformCapabilities object.
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,38 +9,42 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import List, Any
+from typing import List, Any, Dict
 
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.current_tpc import  _current_tpc
 from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.target_platform_capabilities_component import TargetPlatformCapabilitiesComponent
 from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat, \
     OperatorsSetBase
-
+from model_compression_toolkit import DefaultDict
 
 
 class OperationsSetToLayers(TargetPlatformCapabilitiesComponent):
     """
     Associate an OperatorsSet to a list of framework's layers.
     """
     def __init__(self,
                  op_set_name: str,
-                 layers: List[Any]):
+                 layers: List[Any],
+                 attr_mapping: Dict[str, DefaultDict] = None):
         """
 
         Args:
             op_set_name (str): Name of OperatorsSet to associate with layers.
             layers (List[Any]): List of layers/FilterLayerParams to associate with OperatorsSet.
+            attr_mapping (Dict[str, DefaultDict]): A mapping between a general attribute name to a DefaultDict that maps a layer type to the layer's framework name of this attribute.
+
         """
         self.layers = layers
+        self.attr_mapping = attr_mapping
         super(OperationsSetToLayers, self).__init__(name=op_set_name)
         _current_tpc.get().remove_opset_from_not_used_list(op_set_name)
 
     def __repr__(self) -> str:
         """
 
         Returns: String to represent the mapping from an OperatorsSet's label to the list of layers.
@@ -87,14 +91,26 @@
             layers = []
             for o in op.op_set_list:
                 layers.extend(self.get_layers_by_op(o))
             return layers
         Logger.warning(f'{op.name} is not in model.')
         return []
 
+    def get_layers(self) -> Any:
+        """
+        Get list of layers of all OperatorsSet objects.
+
+        Returns:
+            List of Layers that are associated with the passed OperatorsSet object.
+        """
+        layers = []
+        for o in self.op_sets_to_layers:
+            layers.extend(o.layers)
+        return layers
+
     def __add__(self,
                 op_set_to_layers: OperationsSetToLayers):
         """
         Add a OperationsSetToLayers to self's OperationsSetToLayers existing OperationsSetToLayers objects.
         Args:
             op_set_to_layers: OperationsSetToLayers to add.
 
@@ -127,11 +143,11 @@
             assert not (ops2layers.name in existing_opset_names), f'OperationsSetToLayers names should be unique, but {ops2layers.name} appears to violate it.'
             existing_opset_names.append(ops2layers.name)
 
             # Assert that a layer does not appear in more than a single OperatorsSet in the TargetPlatformModel.
             for layer in ops2layers.layers:
                 qco_by_opset_name = _current_tpc.get().tp_model.get_config_options_by_operators_set(ops2layers.name)
                 if layer in existing_layers:
-                    Logger.error(f'Found layer {layer.__name__} in more than one '
+                    Logger.critical(f'Found layer {layer.__name__} in more than one '
                                  f'OperatorsSet')  # pragma: no cover
                 else:
                     existing_layers.update({layer: qco_by_opset_name})
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py`

 * *Files 14% similar despite different names*

```diff
@@ -71,14 +71,23 @@
         """
         opset = self.tp_model.get_opset_by_name(opset_name)
         if opset is None:
             Logger.warning(f'{opset_name} was not found in TargetPlatformCapabilities.')
             return None
         return self.get_layers_by_opset(opset)
 
+    def get_layers(self) -> List[Any]:
+        """
+        Get a list of layers of all OperatorsSet objects.
+
+        Returns:
+            List of layers/LayerFilterParams in the TPC.
+        """
+        return self.op_sets_to_layers.get_layers()
+
     def get_layers_by_opset(self, op: OperatorsSetBase) -> List[Any]:
         """
         Get a list of layers that are attached to an OperatorsSet by the OperatorsSet object.
 
         Args:
             op: OperatorsSet object to get its layers.
 
@@ -126,16 +135,15 @@
         Args:
             tpc_component: Component to append to TargetPlatformCapabilities.
 
         """
         if isinstance(tpc_component, OperationsSetToLayers):
             self.op_sets_to_layers += tpc_component
         else:
-            Logger.error(f'Trying to append an unfamiliar TargetPlatformCapabilitiesComponent of type: '
-                         f'{type(tpc_component)}')  # pragma: no cover
+            Logger.critical(f"Attempt to append an unrecognized 'TargetPlatformCapabilitiesComponent' of type: '{type(tpc_component)}'. Ensure the component is compatible.")  # pragma: no cover
 
     def __enter__(self):
         """
         Init a TargetPlatformCapabilities object.
         """
         _current_tpc.set(self)
         return self
@@ -175,14 +183,24 @@
         layer2qco = {}
         filterlayer2qco = {}
         for op2layers in self.op_sets_to_layers.op_sets_to_layers:
             for l in op2layers.layers:
                 qco = self.tp_model.get_config_options_by_operators_set(op2layers.name)
                 if qco is None:
                     qco = self.tp_model.default_qco
+
+                # here, we need to take care of mapping a general attribute name into a framework and
+                # layer type specific attribute name.
+                # attr_mapping is a mapping between an attribute generic name to a dictionary that maps each
+                # layer type to its framework-specific attribute name.
+                # in the loop below, v is the inner dictionary.
+                layer_attrs_mapping = None if op2layers.attr_mapping is None else \
+                    {k: v.get(l) for k, v in op2layers.attr_mapping.items()}
+                qco = qco.clone_and_map_weights_attr_keys(layer_attrs_mapping)
+
                 if isinstance(l, LayerFilterParams):
                     filterlayer2qco.update({l: qco})
                 else:
                     layer2qco.update({l: qco})
         return layer2qco, filterlayer2qco
 
     def remove_fusing_names_from_not_used_list(self):
@@ -210,7 +228,16 @@
         """
 
         Log warnings regards unused opsets.
 
         """
         for op in self.__tp_model_opsets_not_used:
             Logger.warning(f'{op} is defined in TargetPlatformModel, but is not used in TargetPlatformCapabilities.')
+
+    @property
+    def is_simd_padding(self) -> bool:
+        """
+
+        Returns: Check if the TP model defines that padding due to SIMD constrains occurs.
+
+        """
+        return self.tp_model.is_simd_padding
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,56 +32,90 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='imx500_tp_model')
+                             name='qnnpack_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
-    # Create a quantization config.
-    # A quantization configuration defines how an operator
-    # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
-        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
         weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
-        activation_n_bits=8,
         weights_n_bits=8,
-        weights_per_channel_threshold=True,
+        weights_per_channel_threshold=False,
         enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # Create a quantization config. A quantization configuration defines how an operator
+    # should be quantized on the modeled hardware.
+    # For qnnpack backend, Pytorch uses a QConfig with torch.per_tensor_affine for
+    # activations quantization and a torch.per_tensor_symmetric quantization scheme
+    # for weights quantization (https://pytorch.org/docs/stable/quantization.html#natively-supported-backends):
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=32)
+
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        activation_quantization_method=tp.QuantizationMethod.UNIFORM,
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_n_bits=8,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=None
+    )
 
-    # To quantize a model using mixed-precision, create
-    # a list with more than one OpQuantizationConfig.
-    # In this example, we quantize some operations' weights
-    # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
-    # to quantize the operations' activations using LUT.
-    four_bits = eight_bits.clone_and_edit(weights_n_bits=4)
-    two_bits = eight_bits.clone_and_edit(weights_n_bits=2)
-    mixed_precision_cfg_list = [eight_bits, four_bits, two_bits]
+    mixed_precision_cfg_list = []  # No mixed precision
 
-    return eight_bits, mixed_precision_cfg_list
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -104,59 +140,27 @@
 
     # Create a TargetPlatformModel and set its default quantization config.
     # This default configuration will be used for all operations
     # unless specified otherwise (see OperatorsSet, for example):
     generated_tpc = tp.TargetPlatformModel(default_configuration_options, name=name)
 
     # To start defining the model's components (such as operator sets, and fusing patterns),
-    # use 'with' the TargetPlatformModel instance, and create them as below:
+    # use 'with' the target platform model instance, and create them as below:
     with generated_tpc:
-        # Create an OperatorsSet to represent a set of operations.
-        # Each OperatorsSet has a unique label.
-        # If a quantization configuration options is passed, these options will
-        # be used for operations that will be attached to this set's label.
-        # Otherwise, it will be a configure-less set (used in fusing):
-
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
-
-        # May suit for operations like: Dropout, Reshape, etc.
-        tp.OperatorsSet("NoQuantization",
-                        tp.get_default_quantization_config_options().clone_and_edit(
-                            enable_weights_quantization=False,
-                            enable_activation_quantization=False))
-
-        # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
-        mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
-                                                                             base_config=base_config)
-
-        # Define operator sets that use mixed_precision_configuration_options:
-        conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
-        fc = tp.OperatorsSet("FullyConnected", mixed_precision_configuration_options)
-
-        # Define operations sets without quantization configuration
-        # options (useful for creating fusing patterns, for example):
-        any_relu = tp.OperatorsSet("AnyReLU")
-        add = tp.OperatorsSet("Add")
-        sub = tp.OperatorsSet("Sub")
-        mul = tp.OperatorsSet("Mul")
-        div = tp.OperatorsSet("Div")
-        prelu = tp.OperatorsSet("PReLU")
-        swish = tp.OperatorsSet("Swish")
-        sigmoid = tp.OperatorsSet("Sigmoid")
-        tanh = tp.OperatorsSet("Tanh")
-
-        # Combine multiple operators into a single operator to avoid quantization between
-        # them. To do this we define fusing patterns using the OperatorsSets that were created.
-        # To group multiple sets with regard to fusing, an OperatorSetConcat can be created
-        activations_after_conv_to_fuse = tp.OperatorSetConcat(any_relu, swish, prelu, sigmoid, tanh)
-        activations_after_fc_to_fuse = tp.OperatorSetConcat(any_relu, swish, sigmoid)
-        any_binary = tp.OperatorSetConcat(add, sub, mul, div)
+        # Combine operations/modules into a single module.
+        # Pytorch supports the next fusing patterns:
+        # [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]
+        # Source: # https://pytorch.org/docs/stable/quantization.html#model-preparation-for-quantization-eager-mode
+        conv = tp.OperatorsSet("Conv")
+        batchnorm = tp.OperatorsSet("BatchNorm")
+        relu = tp.OperatorsSet("Relu")
+        linear = tp.OperatorsSet("Linear")
 
         # ------------------- #
         # Fusions
         # ------------------- #
-        tp.Fusing([conv, activations_after_conv_to_fuse])
-        tp.Fusing([fc, activations_after_fc_to_fuse])
-        tp.Fusing([any_binary, any_relu])
+        tp.Fusing([conv, batchnorm, relu])
+        tp.Fusing([conv, batchnorm])
+        tp.Fusing([conv, relu])
+        tp.Fusing([linear, relu])
 
     return generated_tpc
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py`

 * *Files 25% similar despite different names*

```diff
@@ -11,27 +11,34 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
 
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.constants import FOUND_SONY_CUSTOM_LAYERS
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, \
+    KERAS_DEPTHWISE_KERNEL, BIAS
+
+if FOUND_SONY_CUSTOM_LAYERS:
+    from sony_custom_layers.keras.object_detection.ssd_post_process import SSDPostProcess
 
 if version.parse(tf.__version__) >= version.parse("2.13"):
     from keras.src.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
@@ -50,44 +57,63 @@
         tp_model: TargetPlatformModel object.
 
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     keras_tpc = tp.TargetPlatformCapabilities(tp_model, name=name, version=TPC_VERSION)
 
+    no_quant_list = [Reshape,
+                     tf.reshape,
+                     Permute,
+                     tf.transpose,
+                     Flatten,
+                     Cropping2D,
+                     ZeroPadding2D,
+                     Dropout,
+                     MaxPooling2D,
+                     tf.split,
+                     tf.quantization.fake_quant_with_min_max_vars,
+                     tf.math.argmax,
+                     tf.shape,
+                     tf.math.equal,
+                     tf.gather,
+                     tf.cast,
+                     tf.unstack,
+                     tf.compat.v1.gather,
+                     tf.nn.top_k,
+                     tf.__operators__.getitem,
+                     tf.image.combined_non_max_suppression,
+                     tf.compat.v1.shape]
+
+    if FOUND_SONY_CUSTOM_LAYERS:
+        no_quant_list.append(SSDPostProcess)
+
     with keras_tpc:
-        tp.OperationsSetToLayers("NoQuantization", [Reshape,
-                                                    tf.reshape,
-                                                    Permute,
-                                                    tf.transpose,
-                                                    Flatten,
-                                                    Cropping2D,
-                                                    ZeroPadding2D,
-                                                    Dropout,
-                                                    MaxPooling2D,
-                                                    tf.split,
-                                                    tf.quantization.fake_quant_with_min_max_vars,
-                                                    tf.math.argmax,
-                                                    tf.shape,
-                                                    tf.math.equal,
-                                                    tf.gather,
-                                                    tf.cast,
-                                                    tf.unstack,
-                                                    tf.compat.v1.gather,
-                                                    tf.nn.top_k,
-                                                    tf.__operators__.getitem,
-                                                    tf.compat.v1.shape])
-
-        tp.OperationsSetToLayers("Conv", [Conv2D,
-                                          DepthwiseConv2D,
-                                          Conv2DTranspose,
-                                          tf.nn.conv2d,
-                                          tf.nn.depthwise_conv2d,
-                                          tf.nn.conv2d_transpose])
-        tp.OperationsSetToLayers("FullyConnected", [Dense])
+        tp.OperationsSetToLayers("NoQuantization", no_quant_list)
+
+        tp.OperationsSetToLayers("Conv",
+                                 [Conv2D,
+                                  DepthwiseConv2D,
+                                  Conv2DTranspose,
+                                  tf.nn.conv2d,
+                                  tf.nn.depthwise_conv2d,
+                                  tf.nn.conv2d_transpose],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict({
+                                         DepthwiseConv2D: KERAS_DEPTHWISE_KERNEL,
+                                         tf.nn.depthwise_conv2d: KERAS_DEPTHWISE_KERNEL}, default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
+        tp.OperationsSetToLayers("FullyConnected", [Dense],
+                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})
         tp.OperationsSetToLayers("AnyReLU", [tf.nn.relu,
                                              tf.nn.relu6,
                                              tf.nn.leaky_relu,
                                              ReLU,
                                              LeakyReLU,
                                              tp.LayerFilterParams(Activation, activation="relu"),
                                              tp.LayerFilterParams(Activation, activation="leaky_relu")])
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py`

 * *Files 24% similar despite different names*

```diff
@@ -13,34 +13,38 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, transpose, permute
+    gather, equal, transpose, permute, argmax, squeeze
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, PYTORCH_KERNEL, BIAS_ATTR, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_pot.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_pot import (
+    __version__ as TPC_VERSION)
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    imx500_tpc_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='imx500_tpc_pytorch_tpc', tp_model=imx500_tpc_tp_model)
+    imx500_pot_tpc_tp_model = get_tp_model()
+    return generate_pytorch_tpc(name='imx500_pot_tpc_pytorch_tpc', tp_model=imx500_pot_tpc_tp_model)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
@@ -48,14 +52,22 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("NoQuantization", [Dropout,
                                                     Flatten,
                                                     dropout,
                                                     flatten,
                                                     split,
                                                     operator.getitem,
@@ -64,19 +76,23 @@
                                                     BatchNorm2d,
                                                     chunk,
                                                     unbind,
                                                     torch.Tensor.size,
                                                     permute,
                                                     transpose,
                                                     equal,
+                                                    argmax,
                                                     gather,
-                                                    topk])
+                                                    topk,
+                                                    squeeze])
 
-        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
+        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+        tp.OperationsSetToLayers("FullyConnected", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
         tp.OperationsSetToLayers("AnyReLU", [torch.relu,
                                              ReLU,
                                              ReLU6,
                                              LeakyReLU,
                                              relu,
                                              relu6,
                                              leaky_relu,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py`

 * *Files 20% similar despite different names*

```diff
@@ -11,18 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, WEIGHTS_N_BITS
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
+
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,58 +33,96 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='imx500_lut_tp_model')
+                             name='imx500_pot_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
-    default configuration for mixed-precision quantization with non-uniform quantizer for 2 and 4 bit candidates.
+    default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
+        enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
     # Create a quantization config.
     # A quantization configuration defines how an operator
     # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
         activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
         activation_n_bits=8,
-        weights_n_bits=8,
-        weights_per_channel_threshold=True,
-        enable_weights_quantization=True,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
+
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=32)
 
     # To quantize a model using mixed-precision, create
     # a list with more than one OpQuantizationConfig.
     # In this example, we quantize some operations' weights
     # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
     # to quantize the operations' activations using LUT.
-    four_bits_lut = eight_bits.clone_and_edit(weights_n_bits=4,
-                                              weights_quantization_method=tp.QuantizationMethod.LUT_SYM_QUANTIZER)
-    two_bits_lut = eight_bits.clone_and_edit(weights_n_bits=2,
-                                             weights_quantization_method=tp.QuantizationMethod.LUT_SYM_QUANTIZER)
-    mixed_precision_cfg_list = [eight_bits, four_bits_lut, two_bits_lut]
+    four_bits = linear_eight_bits.clone_and_edit(attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 4}},
+                                                 simd_size=linear_eight_bits.simd_size * 2)
+    two_bits = linear_eight_bits.clone_and_edit(attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 2}},
+                                                simd_size=linear_eight_bits.simd_size * 4)
+    mixed_precision_cfg_list = [linear_eight_bits, four_bits, two_bits]
 
-    return eight_bits, mixed_precision_cfg_list
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -114,22 +155,19 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
-
         # May suit for operations like: Dropout, Reshape, etc.
+        default_qco = tp.get_default_quantization_config_options()
         tp.OperatorsSet("NoQuantization",
-                        tp.get_default_quantization_config_options().clone_and_edit(
-                            enable_weights_quantization=False,
-                            enable_activation_quantization=False))
+                        default_qco.clone_and_edit(enable_activation_quantization=False)
+                        .clone_and_edit_weight_attribute(enable_weights_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
         mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
                                                                              base_config=base_config)
 
         # Define operator sets that use mixed_precision_configuration_options:
         conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_keras.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py`

 * *Files 25% similar despite different names*

```diff
@@ -11,26 +11,34 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
 
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.constants import FOUND_SONY_CUSTOM_LAYERS
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_DEPTHWISE_KERNEL, \
+    KERAS_KERNEL, BIAS_ATTR, BIAS
+
+if FOUND_SONY_CUSTOM_LAYERS:
+    from sony_custom_layers.keras.object_detection.ssd_post_process import SSDPostProcess
+
 if version.parse(tf.__version__) >= version.parse("2.13"):
     from keras.src.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
@@ -49,44 +57,62 @@
         tp_model: TargetPlatformModel object.
 
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     keras_tpc = tp.TargetPlatformCapabilities(tp_model, name=name, version=TPC_VERSION)
 
+    no_quant_list = [Reshape,
+                     tf.reshape,
+                     Permute,
+                     tf.transpose,
+                     Flatten,
+                     Cropping2D,
+                     ZeroPadding2D,
+                     Dropout,
+                     MaxPooling2D,
+                     tf.split,
+                     tf.quantization.fake_quant_with_min_max_vars,
+                     tf.math.argmax,
+                     tf.shape,
+                     tf.math.equal,
+                     tf.gather,
+                     tf.cast,
+                     tf.unstack,
+                     tf.compat.v1.gather,
+                     tf.nn.top_k,
+                     tf.__operators__.getitem,
+                     tf.image.combined_non_max_suppression,
+                     tf.compat.v1.shape]
+
+    if FOUND_SONY_CUSTOM_LAYERS:
+        no_quant_list.append(SSDPostProcess)
+
     with keras_tpc:
-        tp.OperationsSetToLayers("NoQuantization", [Reshape,
-                                                    tf.reshape,
-                                                    Permute,
-                                                    tf.transpose,
-                                                    Flatten,
-                                                    Cropping2D,
-                                                    ZeroPadding2D,
-                                                    Dropout,
-                                                    MaxPooling2D,
-                                                    tf.split,
-                                                    tf.quantization.fake_quant_with_min_max_vars,
-                                                    tf.math.argmax,
-                                                    tf.shape,
-                                                    tf.math.equal,
-                                                    tf.gather,
-                                                    tf.cast,
-                                                    tf.unstack,
-                                                    tf.compat.v1.gather,
-                                                    tf.nn.top_k,
-                                                    tf.__operators__.getitem,
-                                                    tf.compat.v1.shape])
-
-        tp.OperationsSetToLayers("Conv", [Conv2D,
-                                          DepthwiseConv2D,
-                                          Conv2DTranspose,
-                                          tf.nn.conv2d,
-                                          tf.nn.depthwise_conv2d,
-                                          tf.nn.conv2d_transpose])
-        tp.OperationsSetToLayers("FullyConnected", [Dense])
+        tp.OperationsSetToLayers("NoQuantization", no_quant_list)
+        tp.OperationsSetToLayers("Conv",
+                                 [Conv2D,
+                                  DepthwiseConv2D,
+                                  Conv2DTranspose,
+                                  tf.nn.conv2d,
+                                  tf.nn.depthwise_conv2d,
+                                  tf.nn.conv2d_transpose],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict({
+                                         DepthwiseConv2D: KERAS_DEPTHWISE_KERNEL,
+                                         tf.nn.depthwise_conv2d: KERAS_DEPTHWISE_KERNEL}, default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
+        tp.OperationsSetToLayers("FullyConnected", [Dense],
+                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})
         tp.OperationsSetToLayers("AnyReLU", [tf.nn.relu,
                                              tf.nn.relu6,
                                              tf.nn.leaky_relu,
                                              ReLU,
                                              LeakyReLU,
                                              tp.LayerFilterParams(Activation, activation="relu"),
                                              tp.LayerFilterParams(Activation, activation="leaky_relu")])
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py`

 * *Files 21% similar despite different names*

```diff
@@ -13,23 +13,26 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, transpose, permute
+    gather, equal, transpose, permute, argmax, squeeze
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, PYTORCH_KERNEL, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
@@ -48,14 +51,22 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("NoQuantization", [Dropout,
                                                     Flatten,
                                                     dropout,
                                                     flatten,
                                                     split,
                                                     operator.getitem,
@@ -64,19 +75,23 @@
                                                     BatchNorm2d,
                                                     chunk,
                                                     unbind,
                                                     torch.Tensor.size,
                                                     permute,
                                                     transpose,
                                                     equal,
+                                                    argmax,
                                                     gather,
-                                                    topk])
+                                                    topk,
+                                                    squeeze])
 
-        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
+        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+        tp.OperationsSetToLayers("FullyConnected", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
         tp.OperationsSetToLayers("AnyReLU", [torch.relu,
                                              ReLU,
                                              ReLU6,
                                              LeakyReLU,
                                              relu,
                                              relu6,
                                              leaky_relu,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tp_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import BIAS_ATTR, KERNEL_ATTR
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    QuantizationMethod, AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,56 +32,88 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='imx500_pot_tp_model')
+                             name='tflite_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
     default configuration for mixed-precision quantization.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
+        enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
     # Create a quantization config.
     # A quantization configuration defines how an operator
     # should be quantized on the modeled hardware:
-    eight_bits = tp.OpQuantizationConfig(
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
         activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
-        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
         activation_n_bits=8,
-        weights_n_bits=8,
-        weights_per_channel_threshold=True,
-        enable_weights_quantization=True,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None)
+        simd_size=32)
 
-    # To quantize a model using mixed-precision, create
-    # a list with more than one OpQuantizationConfig.
-    # In this example, we quantize some operations' weights
-    # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
-    # to quantize the operations' activations using LUT.
-    four_bits = eight_bits.clone_and_edit(weights_n_bits=4)
-    two_bits = eight_bits.clone_and_edit(weights_n_bits=2)
-    mixed_precision_cfg_list = [eight_bits, four_bits, two_bits]
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        activation_quantization_method=QuantizationMethod.UNIFORM,
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=None
+    )
 
-    return eight_bits, mixed_precision_cfg_list
+    mixed_precision_cfg_list = []  # No mixed precision
+
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -106,57 +140,59 @@
     # This default configuration will be used for all operations
     # unless specified otherwise (see OperatorsSet, for example):
     generated_tpc = tp.TargetPlatformModel(default_configuration_options, name=name)
 
     # To start defining the model's components (such as operator sets, and fusing patterns),
     # use 'with' the TargetPlatformModel instance, and create them as below:
     with generated_tpc:
-        # Create an OperatorsSet to represent a set of operations.
-        # Each OperatorsSet has a unique label.
-        # If a quantization configuration options is passed, these options will
-        # be used for operations that will be attached to this set's label.
-        # Otherwise, it will be a configure-less set (used in fusing):
+        # In TFLite, the quantized operator specifications constraint operators quantization
+        # differently. For more details:
+        # https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications
+        tp.OperatorsSet("NoQuantization",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            quantization_preserving=True))
 
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+        fc_qco = tp.get_default_quantization_config_options()
+        fc = tp.OperatorsSet("FullyConnected",
+                             fc_qco.clone_and_edit_weight_attribute(weights_per_channel_threshold=False))
 
-        # May suit for operations like: Dropout, Reshape, etc.
-        tp.OperatorsSet("NoQuantization",
+        tp.OperatorsSet("L2Normalization",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=0, fixed_scale=1 / 128))
+        tp.OperatorsSet("LogSoftmax",
                         tp.get_default_quantization_config_options().clone_and_edit(
-                            enable_weights_quantization=False,
-                            enable_activation_quantization=False))
+                            fixed_zero_point=127, fixed_scale=16 / 256))
+        tp.OperatorsSet("Tanh",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=0, fixed_scale=1 / 128))
+        tp.OperatorsSet("Softmax",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=-128, fixed_scale=1 / 256))
+        tp.OperatorsSet("Logistic",
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=-128, fixed_scale=1 / 256))
 
-        # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
-        mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
-                                                                             base_config=base_config)
-
-        # Define operator sets that use mixed_precision_configuration_options:
-        conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
-        fc = tp.OperatorsSet("FullyConnected", mixed_precision_configuration_options)
-
-        # Define operations sets without quantization configuration
-        # options (useful for creating fusing patterns, for example):
-        any_relu = tp.OperatorsSet("AnyReLU")
-        add = tp.OperatorsSet("Add")
-        sub = tp.OperatorsSet("Sub")
-        mul = tp.OperatorsSet("Mul")
-        div = tp.OperatorsSet("Div")
-        prelu = tp.OperatorsSet("PReLU")
-        swish = tp.OperatorsSet("Swish")
-        sigmoid = tp.OperatorsSet("Sigmoid")
-        tanh = tp.OperatorsSet("Tanh")
-
-        # Combine multiple operators into a single operator to avoid quantization between
-        # them. To do this we define fusing patterns using the OperatorsSets that were created.
-        # To group multiple sets with regard to fusing, an OperatorSetConcat can be created
-        activations_after_conv_to_fuse = tp.OperatorSetConcat(any_relu, swish, prelu, sigmoid, tanh)
-        activations_after_fc_to_fuse = tp.OperatorSetConcat(any_relu, swish, sigmoid)
-        any_binary = tp.OperatorSetConcat(add, sub, mul, div)
+        conv2d = tp.OperatorsSet("Conv2d")
+        kernel = tp.OperatorSetConcat(conv2d, fc)
+
+        relu = tp.OperatorsSet("Relu")
+        elu = tp.OperatorsSet("Elu")
+        activations_to_fuse = tp.OperatorSetConcat(relu, elu)
 
+        batch_norm = tp.OperatorsSet("BatchNorm")
+        bias_add = tp.OperatorsSet("BiasAdd")
+        add = tp.OperatorsSet("Add")
+        squeeze = tp.OperatorsSet("Squeeze",
+                                  qc_options=tp.get_default_quantization_config_options().clone_and_edit(
+                                      quantization_preserving=True))
         # ------------------- #
         # Fusions
         # ------------------- #
-        tp.Fusing([conv, activations_after_conv_to_fuse])
-        tp.Fusing([fc, activations_after_fc_to_fuse])
-        tp.Fusing([any_binary, any_relu])
+        # Source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper
+        tp.Fusing([kernel, bias_add])
+        tp.Fusing([kernel, bias_add, activations_to_fuse])
+        tp.Fusing([conv2d, batch_norm, activations_to_fuse])
+        tp.Fusing([conv2d, squeeze, activations_to_fuse])
+        tp.Fusing([batch_norm, activations_to_fuse])
+        tp.Fusing([batch_norm, add, activations_to_fuse])
 
     return generated_tpc
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_pot/tpc_pytorch.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tpc_pytorch.py`

 * *Files 17% similar despite different names*

```diff
@@ -13,35 +13,37 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, transpose, permute
+    gather, equal, transpose, permute, argmax, squeeze
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_pot.tp_model import get_tp_model
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, PYTORCH_KERNEL, BIAS_ATTR, \
+    BIAS
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_pot import (
-    __version__ as TPC_VERSION)
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1_lut import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    imx500_pot_tpc_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='imx500_pot_tpc_pytorch_tpc', tp_model=imx500_pot_tpc_tp_model)
+    imx500_tpc_tp_model = get_tp_model()
+    return generate_pytorch_tpc(name='imx500_tpc_pytorch_tpc', tp_model=imx500_tpc_tp_model)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
@@ -49,14 +51,22 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("NoQuantization", [Dropout,
                                                     Flatten,
                                                     dropout,
                                                     flatten,
                                                     split,
                                                     operator.getitem,
@@ -65,19 +75,23 @@
                                                     BatchNorm2d,
                                                     chunk,
                                                     unbind,
                                                     torch.Tensor.size,
                                                     permute,
                                                     transpose,
                                                     equal,
+                                                    argmax,
                                                     gather,
-                                                    topk])
+                                                    topk,
+                                                    squeeze])
 
-        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d])
-        tp.OperationsSetToLayers("FullyConnected", [Linear])
+        tp.OperationsSetToLayers("Conv", [Conv2d, ConvTranspose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
+        tp.OperationsSetToLayers("FullyConnected", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
         tp.OperationsSetToLayers("AnyReLU", [torch.relu,
                                              ReLU,
                                              ReLU6,
                                              LeakyReLU,
                                              relu,
                                              relu6,
                                              leaky_relu,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1_lut/tp_model.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR, WEIGHTS_N_BITS, \
+    WEIGHTS_QUANTIZATION_METHOD
 from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
     TargetPlatformModel
-from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
-    QuantizationFormat
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    AttributeQuantizationConfig
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -30,52 +33,100 @@
     NOTE: in order to generate a target platform model with different configurations but with the same Operators Sets
     (for tests, experiments, etc.), use this method implementation as a test-case, i.e., override the
     'get_op_quantization_configs' method and use its output to call 'generate_tp_model' with your configurations.
 
     Returns: A TargetPlatformModel object.
 
     """
-    base_config, mixed_precision_cfg_list = get_op_quantization_configs()
-    return generate_tp_model(default_config=base_config,
+    base_config, mixed_precision_cfg_list, default_config = get_op_quantization_configs()
+    return generate_tp_model(default_config=default_config,
                              base_config=base_config,
                              mixed_precision_cfg_list=mixed_precision_cfg_list,
-                             name='qnnpack_tp_model')
+                             name='imx500_lut_tp_model')
 
 
-def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig]]:
+def get_op_quantization_configs() -> Tuple[OpQuantizationConfig, List[OpQuantizationConfig], OpQuantizationConfig]:
     """
     Creates a default configuration object for 8-bit quantization, to be used to set a default TargetPlatformModel.
     In addition, creates a default configuration objects list (with 8, 4 and 2 bit quantization) to be used as
-    default configuration for mixed-precision quantization.
+    default configuration for mixed-precision quantization with non-uniform quantizer for 2 and 4 bit candidates.
 
     Returns: An OpQuantizationConfig config object and a list of OpQuantizationConfig objects.
 
     """
-    # Create a quantization config. A quantization configuration defines how an operator
-    # should be quantized on the modeled hardware.
-    # For qnnpack backend, Pytorch uses a QConfig with torch.per_tensor_affine for
-    # activations quantization and a torch.per_tensor_symmetric quantization scheme
-    # for weights quantization (https://pytorch.org/docs/stable/quantization.html#natively-supported-backends):
-    eight_bits = tp.OpQuantizationConfig(
-        activation_quantization_method=tp.QuantizationMethod.UNIFORM,
+
+    # We define a default quantization config for all non-specified weights attributes.
+    default_weight_attr_config = AttributeQuantizationConfig(
         weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
-        activation_n_bits=8,
         weights_n_bits=8,
         weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the kernel (for layers where there is a kernel attribute).
+    kernel_base_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,
+        weights_n_bits=8,
+        weights_per_channel_threshold=True,
         enable_weights_quantization=True,
+        lut_values_bitwidth=None)
+
+    # We define a quantization config to quantize the bias (for layers where there is a bias attribute).
+    bias_config = AttributeQuantizationConfig(
+        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        weights_n_bits=FLOAT_BITWIDTH,
+        weights_per_channel_threshold=False,
+        enable_weights_quantization=False,
+        lut_values_bitwidth=None)
+
+    # Create a quantization config.
+    # A quantization configuration defines how an operator
+    # should be quantized on the modeled hardware:
+
+    # We define a default config for operation without kernel attribute.
+    # This is the default config that should be used for non-linear operations.
+    eight_bits_default = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={},
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        activation_n_bits=8,
+        enable_activation_quantization=True,
+        quantization_preserving=False,
+        fixed_scale=None,
+        fixed_zero_point=None,
+        simd_size=32)
+
+    # We define an 8-bit config for linear operations quantization, that include a kernel and bias attributes.
+    linear_eight_bits = tp.OpQuantizationConfig(
+        default_weight_attr_config=default_weight_attr_config,
+        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config, BIAS_ATTR: bias_config},
+        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,
+        activation_n_bits=8,
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
-        weights_multiplier_nbits=None
-    )
+        simd_size=32)
 
-    mixed_precision_cfg_list = [] # No mixed precision
+    # To quantize a model using mixed-precision, create
+    # a list with more than one OpQuantizationConfig.
+    # In this example, we quantize some operations' weights
+    # using 2, 4 or 8 bits, and when using 2 or 4 bits, it's possible
+    # to quantize the operations' activations using LUT.
+    four_bits_lut = linear_eight_bits.clone_and_edit(
+        attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 4,
+                                    WEIGHTS_QUANTIZATION_METHOD: tp.QuantizationMethod.LUT_SYM_QUANTIZER}},
+        simd_size=linear_eight_bits.simd_size * 2)
+    two_bits_lut = linear_eight_bits.clone_and_edit(
+        attr_to_edit={KERNEL_ATTR: {WEIGHTS_N_BITS: 2,
+                                    WEIGHTS_QUANTIZATION_METHOD: tp.QuantizationMethod.LUT_SYM_QUANTIZER}},
+        simd_size=linear_eight_bits.simd_size * 4)
+    mixed_precision_cfg_list = [linear_eight_bits, four_bits_lut, two_bits_lut]
 
-    return eight_bits, mixed_precision_cfg_list
+    return linear_eight_bits, mixed_precision_cfg_list, eight_bits_default
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
                       name: str) -> TargetPlatformModel:
     """
@@ -100,30 +151,56 @@
 
     # Create a TargetPlatformModel and set its default quantization config.
     # This default configuration will be used for all operations
     # unless specified otherwise (see OperatorsSet, for example):
     generated_tpc = tp.TargetPlatformModel(default_configuration_options, name=name)
 
     # To start defining the model's components (such as operator sets, and fusing patterns),
-    # use 'with' the target platform model instance, and create them as below:
+    # use 'with' the TargetPlatformModel instance, and create them as below:
     with generated_tpc:
-        # Combine operations/modules into a single module.
-        # Pytorch supports the next fusing patterns:
-        # [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]
-        # Source: # https://pytorch.org/docs/stable/quantization.html#model-preparation-for-quantization-eager-mode
-        conv = tp.OperatorsSet("Conv")
-        batchnorm = tp.OperatorsSet("BatchNorm")
-        relu = tp.OperatorsSet("Relu")
-        linear = tp.OperatorsSet("Linear")
+        # Create an OperatorsSet to represent a set of operations.
+        # Each OperatorsSet has a unique label.
+        # If a quantization configuration options is passed, these options will
+        # be used for operations that will be attached to this set's label.
+        # Otherwise, it will be a configure-less set (used in fusing):
+
+        # May suit for operations like: Dropout, Reshape, etc.
+        default_qco = tp.get_default_quantization_config_options()
+        tp.OperatorsSet("NoQuantization",
+                        default_qco.clone_and_edit(enable_activation_quantization=False)
+                        .clone_and_edit_weight_attribute(enable_weights_quantization=False))
+
+        # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
+        mixed_precision_configuration_options = tp.QuantizationConfigOptions(mixed_precision_cfg_list,
+                                                                             base_config=base_config)
+
+        # Define operator sets that use mixed_precision_configuration_options:
+        conv = tp.OperatorsSet("Conv", mixed_precision_configuration_options)
+        fc = tp.OperatorsSet("FullyConnected", mixed_precision_configuration_options)
+
+        # Define operations sets without quantization configuration
+        # options (useful for creating fusing patterns, for example):
+        any_relu = tp.OperatorsSet("AnyReLU")
+        add = tp.OperatorsSet("Add")
+        sub = tp.OperatorsSet("Sub")
+        mul = tp.OperatorsSet("Mul")
+        div = tp.OperatorsSet("Div")
+        prelu = tp.OperatorsSet("PReLU")
+        swish = tp.OperatorsSet("Swish")
+        sigmoid = tp.OperatorsSet("Sigmoid")
+        tanh = tp.OperatorsSet("Tanh")
+
+        # Combine multiple operators into a single operator to avoid quantization between
+        # them. To do this we define fusing patterns using the OperatorsSets that were created.
+        # To group multiple sets with regard to fusing, an OperatorSetConcat can be created
+        activations_after_conv_to_fuse = tp.OperatorSetConcat(any_relu, swish, prelu, sigmoid, tanh)
+        activations_after_fc_to_fuse = tp.OperatorSetConcat(any_relu, swish, sigmoid)
+        any_binary = tp.OperatorSetConcat(add, sub, mul, div)
 
         # ------------------- #
         # Fusions
         # ------------------- #
-        tp.Fusing([conv, batchnorm, relu])
-        tp.Fusing([conv, batchnorm])
-        tp.Fusing([conv, relu])
-        tp.Fusing([linear, relu])
-
-        # Set quantization format to fakely quant
-        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+        tp.Fusing([conv, activations_after_conv_to_fuse])
+        tp.Fusing([fc, activations_after_fc_to_fuse])
+        tp.Fusing([any_binary, any_relu])
 
     return generated_tpc
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py`

 * *Files 25% similar despite different names*

```diff
@@ -11,14 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 
 from packaging import version
+
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, \
+    KERAS_DEPTHWISE_KERNEL, BIAS
 from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
 
 if version.parse(tf.__version__) >= version.parse("2.13"):
     from keras.src.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, BatchNormalization, ReLU, Activation
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, BatchNormalization, ReLU, Activation
 
@@ -49,22 +53,35 @@
     """
 
     keras_tpc = tp.TargetPlatformCapabilities(tp_model,
                                               name=name,
                                               version=TPC_VERSION)
 
     with keras_tpc:
-        tp.OperationsSetToLayers("Conv", [Conv2D,
-                                          DepthwiseConv2D,
-                                          Conv2DTranspose,
-                                          tf.nn.conv2d,
-                                          tf.nn.depthwise_conv2d,
-                                          tf.nn.conv2d_transpose])
-
-        tp.OperationsSetToLayers("Linear", [Dense])
+        tp.OperationsSetToLayers("Conv",
+                                 [Conv2D,
+                                  DepthwiseConv2D,
+                                  Conv2DTranspose,
+                                  tf.nn.conv2d,
+                                  tf.nn.depthwise_conv2d,
+                                  tf.nn.conv2d_transpose],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict({
+                                         DepthwiseConv2D: KERAS_DEPTHWISE_KERNEL,
+                                         tf.nn.depthwise_conv2d: KERAS_DEPTHWISE_KERNEL}, default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
+
+        tp.OperationsSetToLayers("Linear", [Dense],
+                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})
 
         tp.OperationsSetToLayers("BatchNorm", [BatchNormalization,
                                                tf.nn.batch_normalization])
 
         tp.OperationsSetToLayers("Relu", [tf.nn.relu,
                                           tf.nn.relu6,
                                           tp.LayerFilterParams(ReLU, negative_slope=0.0),
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py`

 * *Files 26% similar despite different names*

```diff
@@ -12,14 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d, Hardtanh, ReLU, ReLU6
 from torch.nn.functional import relu, relu6, hardtanh
 
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, PYTORCH_KERNEL, BIAS_ATTR, \
+    BIAS
 from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
 from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
@@ -41,21 +44,31 @@
     Returns: a TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
 
     pytorch_tpc = tp.TargetPlatformCapabilities(tp_model,
                                                 name=name,
                                                 version=TPC_VERSION)
 
+    # we provide attributes mapping that maps each layer type in the operations set
+    # that has weights attributes with provided quantization config (in the tp model) to
+    # its framework-specific attribute name.
+    # note that a DefaultDict should be provided if not all the layer types in the
+    # operation set are provided separately in the mapping.
+    pytorch_linear_attr_mapping = {KERNEL_ATTR: DefaultDict(default_value=PYTORCH_KERNEL),
+                                   BIAS_ATTR: DefaultDict(default_value=BIAS)}
+
     with pytorch_tpc:
         tp.OperationsSetToLayers("Conv", [Conv2d,
                                           torch.nn.functional.conv2d,
                                           ConvTranspose2d,
-                                          torch.nn.functional.conv_transpose2d])
+                                          torch.nn.functional.conv_transpose2d],
+                                 attr_mapping=pytorch_linear_attr_mapping)
 
-        tp.OperationsSetToLayers("Linear", [Linear])
+        tp.OperationsSetToLayers("Linear", [Linear],
+                                 attr_mapping=pytorch_linear_attr_mapping)
 
         tp.OperationsSetToLayers("BatchNorm", [BatchNorm2d])
 
         tp.OperationsSetToLayers("Relu", [torch.relu,
                                           ReLU,
                                           ReLU6,
                                           relu,
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,14 +11,17 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
 
+from model_compression_toolkit.defaultdict import DefaultDict
+from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, BIAS
+
 if version.parse(tf.__version__) >= version.parse("2.13"):
     from keras.src.layers import Conv2D, Dense, Reshape, ZeroPadding2D, AveragePooling2D, Activation, DepthwiseConv2D, \
         MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
 else:
     from keras.layers import Conv2D, Dense, Reshape, ZeroPadding2D, AveragePooling2D, Activation, DepthwiseConv2D, \
         MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
 
@@ -81,15 +84,23 @@
                                                     Maximum,
                                                     tf.minimum,
                                                     Minimum,
                                                     tf.pad,
                                                     tf.slice,
                                                     SlicingOpLambda])
 
-        tp.OperationsSetToLayers("FullyConnected", [Dense])
+        tp.OperationsSetToLayers("FullyConnected", [Dense],
+                                 # we provide attributes mapping that maps each layer type in the operations set
+                                 # that has weights attributes with provided quantization config (in the tp model) to
+                                 # its framework-specific attribute name.
+                                 # note that a DefaultDict should be provided if not all the layer types in the
+                                 # operation set are provided separately in the mapping.
+                                 attr_mapping={
+                                     KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),
+                                     BIAS_ATTR: DefaultDict(default_value=BIAS)})
         tp.OperationsSetToLayers("L2Normalization", [tf.math.l2_normalize])
         tp.OperationsSetToLayers("LogSoftmax", [tf.nn.log_softmax])
         tp.OperationsSetToLayers("Tanh", [tf.nn.tanh, tp.LayerFilterParams(Activation, activation="tanh")])
         tp.OperationsSetToLayers("Softmax", [tf.nn.softmax,
                                              Softmax,
                                              tp.LayerFilterParams(Activation, activation="softmax")])
         tp.OperationsSetToLayers("Logistic", [tf.sigmoid, tp.LayerFilterParams(Activation, activation="sigmoid")])
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py`

 * *Files 12% similar despite different names*

```diff
@@ -51,42 +51,42 @@
             quantization_config: quantizer config class contains all the information about the quantizer configuration.
         """
 
         # verify the quantizer class that inherits this class only has a config argument and key-word arguments
         for i, (k, v) in enumerate(self.get_sig().parameters.items()):
             if i == 0:
                 if v.annotation not in [TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]:
-                    Logger.error(f"First parameter must be either TrainableQuantizerWeightsConfig or TrainableQuantizerActivationConfig")  # pragma: no cover
+                    Logger.critical(f"The first parameter must be either 'TrainableQuantizerWeightsConfig' or 'TrainableQuantizerActivationConfig'.")  # pragma: no cover
             elif v.default is v.empty:
-                Logger.error(f"Parameter {k} doesn't have a default value")  # pragma: no cover
+                Logger.critical(f"Parameter '{k}' lacks a default value.")  # pragma: no cover
 
         super(BaseTrainableQuantizer, self).__init__()
         self.quantization_config = quantization_config
 
         # Inherited class should be decorated with @mark_quantizer decorator, and define the following static properties
         static_quantization_method = getattr(self, QUANTIZATION_METHOD, None)
         static_quantization_target = getattr(self, QUANTIZATION_TARGET, None)
 
         if static_quantization_method is None or static_quantization_target is None:
-            Logger.error("A quantizer class that inherit from BaseTrainableQuantizer is not defined appropriately."
-                         "Either it misses the @mark_quantizer decorator or the decorator is not used correctly.")
+            Logger.critical("Quantizer class inheriting from 'BaseTrainableQuantizer' is improperly defined. "
+                            "Ensure it includes the '@mark_quantizer' decorator and is correctly applied.")
 
         if static_quantization_target == QuantizationTarget.Weights:
             self.validate_weights()
             if self.quantization_config.weights_quantization_method not in static_quantization_method:
-                Logger.error(
-                    f'Quantization method mismatch expected: {static_quantization_method} and got  {self.quantization_config.weights_quantization_method}')
+                Logger.critical(
+                    f"Quantization method mismatch. Expected methods: {static_quantization_method}, received: {self.quantization_config.weights_quantization_method}.")
         elif static_quantization_target == QuantizationTarget.Activation:
             self.validate_activation()
             if self.quantization_config.activation_quantization_method not in static_quantization_method:
-                Logger.error(
-                    f'Quantization method mismatch expected: {static_quantization_method} and got  {self.quantization_config.activation_quantization_method}')
+                Logger.critical(
+                    f"Quantization method mismatch. Expected methods: {static_quantization_method}, received: {self.quantization_config.activation_quantization_method}.")
         else:
-            Logger.error(
-                f'Unknown Quantization Part:{static_quantization_target}')  # pragma: no cover
+            Logger.critical(
+                f"Unrecognized 'QuantizationTarget': {static_quantization_target}.")  # pragma: no cover
 
         self.quantizer_parameters = {}
 
     @classmethod
     def get_sig(cls):
         return signature(cls)
 
@@ -141,23 +141,23 @@
     def validate_weights(self) -> None:
         """
         This function validates the quantization config compared with its parameters.
 
 
         """
         if self.activation_quantization() or not self.weights_quantization():
-            Logger.error(f'Expect weight quantization got activation')
+            Logger.critical(f'Expected weight quantization configuration; received activation quantization instead.')
 
     def validate_activation(self) -> None:
         """
         This function validates the quantization config compared with its parameters.
 
         """
         if not self.activation_quantization() or self.weights_quantization():
-            Logger.error(f'Expect activation quantization got weight')
+            Logger.critical(f'Expected activation quantization configuration; received weight quantization instead.')
 
     def convert2inferable(self) -> BaseInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseInferableQuantizer object.
@@ -179,15 +179,15 @@
 
         Returns:
             trainable variable
         """
         if name in self.quantizer_parameters:
             return self.quantizer_parameters[name][VAR]
         else:
-            Logger.error(f'Variable {name} is not exist in quantizers parameters!') # pragma: no cover
+            Logger.critical(f"Variable '{name}' does not exist in quantizer parameters.") # pragma: no cover
 
 
     @abstractmethod
     def get_trainable_variables(self, group: VariableGroup) -> List[Any]:
         """
         Get trainable parameters with specific group from quantizer
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/constants.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/core/pytorch/pruning/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,14 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-# Quantizers constants (for GPTQ, QAT, etc.)
-FQ_MIN = "min"
-FQ_MAX = "max"
-THRESHOLD_TENSOR = "ptq_threshold_tensor"
-WEIGHTS_QUANTIZATION_PARAMS = 'weights_quantization_params'
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,37 +17,42 @@
 from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig, TrainableQuantizerCandidateConfig
 
 
 def get_trainable_quantizer_weights_config(
         n: BaseNode,
+        attr_name: str,
         weights_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None
 ) -> TrainableQuantizerWeightsConfig:
     """
     Returns the relevant configuration for weights trainable quantizer
 
     Args:
         n: BaseNode - the node to build a trainable quantizer from.
+        attr_name: Attribute name to get its weights quantizer configuration.
         weights_quantization_candidates: A list of weights quantizer config candidates.
 
     Returns:
          TrainableQuantizerWeightsConfig: an object that contains the quantizer configuration
     """
     if n.final_weights_quantization_cfg is None:
-        Logger.error(f'Node must have final_weights_quantization_cfg in order to build quantizer configuration')  # pragma: no cover
-
-    final_cfg = n.final_weights_quantization_cfg
-    return TrainableQuantizerWeightsConfig(final_cfg.weights_quantization_method,
-                                           final_cfg.weights_n_bits,
-                                           final_cfg.weights_quantization_params,
-                                           final_cfg.enable_weights_quantization,
-                                           final_cfg.weights_channels_axis,
-                                           final_cfg.weights_per_channel_threshold,
-                                           final_cfg.min_threshold,
+        Logger.critical(
+            "The node requires a 'final_weights_quantization_cfg' configuration to build a "
+            "quantizer. Please ensure this configuration is set for the node.")# pragma: no cover
+
+    final_node_cfg = n.final_weights_quantization_cfg
+    final_attr_cfg = final_node_cfg.get_attr_config(attr_name)
+    return TrainableQuantizerWeightsConfig(final_attr_cfg.weights_quantization_method,
+                                           final_attr_cfg.weights_n_bits,
+                                           final_attr_cfg.weights_quantization_params,
+                                           final_attr_cfg.enable_weights_quantization,
+                                           final_attr_cfg.weights_channels_axis[0],  # Output channel axis
+                                           final_attr_cfg.weights_per_channel_threshold,
+                                           final_node_cfg.min_threshold,
                                            weights_quantization_candidates)
 
 
 def get_trainable_quantizer_activation_config(
         n: BaseNode,
         activation_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None
 ) -> TrainableQuantizerActivationConfig:
@@ -58,60 +63,68 @@
         n: BaseNode - the node to build a trainable quantizer from.
         activation_quantization_candidates: A list of activation quantizer candidates config.
 
     Returns:
          TrainableQuantizerActivationConfig - an object that contains the quantizer configuration
     """
     if n.final_activation_quantization_cfg is None:
-        Logger.error(f'Node must have final_activation_quantization_cfg in order to build quantizer configuration')  # pragma: no cover
+        Logger.critical(
+            "The node requires a 'final_activation_quantization_cfg' configuration to build a "
+            "quantizer. Please ensure this configuration is set for the node.")# pragma: no cover
 
     final_cfg = n.final_activation_quantization_cfg
     return TrainableQuantizerActivationConfig(final_cfg.activation_quantization_method,
                                               final_cfg.activation_n_bits,
                                               final_cfg.activation_quantization_params,
                                               final_cfg.enable_activation_quantization,
                                               final_cfg.min_threshold,
                                               activation_quantization_candidates)
 
 
-def get_trainable_quantizer_quantization_candidates(n: BaseNode):
+def get_trainable_quantizer_quantization_candidates(n: BaseNode, attr: str = None):
     """
     Returns quantization configuration candidates for activation and weights trainable quantizer.
     Checks that the candidates are compatible with trainable quantizer
 
     Args:
         n: BaseNode - the node to build a trainable quantizer from
+        attr: Weights attribute to get its quantization configuration candidates and trainable quantizer.
 
     Returns:
          weights_quantization_candidates - A list of configuration candidates for weights
          activation_quantization_candidates - A list of configuration candidates for activation
     """
-    # all candidates must have the same weights quantization method
-    weights_quantization_methods = set([cfg.weights_quantization_cfg.weights_quantization_method for cfg in n.candidates_quantization_cfg])
-    if len(weights_quantization_methods) > 1:
-        Logger.error(f'Unsupported candidates_quantization_cfg with different weights quantization methods: {weights_quantization_methods}') # pragma: no cover
+
+    if attr is not None:
+        # all candidates must have the same weights quantization method
+        weights_quantization_methods = set([cfg.weights_quantization_cfg.get_attr_config(attr).weights_quantization_method
+             for cfg in n.candidates_quantization_cfg])
+        if len(weights_quantization_methods) > 1:
+            Logger.critical(f"Invalid 'candidates_quantization_cfg': Inconsistent weights "
+                            f"quantization methods detected: {weights_quantization_methods}. "
+                            f"Trainable quantizer requires all candidates to have the same weights "
+                            f"quantization method.")  # pragma: no cover
 
     # all candidates must have the same activation quantization method
-    activation_quantization_methods = set([cfg.activation_quantization_cfg.activation_quantization_method for cfg in n.candidates_quantization_cfg])
+    activation_quantization_methods = set([cfg.activation_quantization_cfg.activation_quantization_method
+                                           for cfg in n.candidates_quantization_cfg])
     if len(activation_quantization_methods) > 1:
-        Logger.error(f'Unsupported candidates_quantization_cfg with different activation quantization methods: {activation_quantization_methods}') # pragma: no cover
+        Logger.critical(f"Invalid 'candidates_quantization_cfg': Inconsistent activation quantization "
+                        f"methods detected: {activation_quantization_methods}. "
+                        f"Trainable quantizer requires all candidates to have the same activation quantization method.")# pragma: no cover
 
     # get unique lists of candidates
-    unique_weights_candidates = n.get_unique_weights_candidates()
+    unique_weights_candidates = n.get_unique_weights_candidates(attr)
     unique_activation_candidates = n.get_unique_activation_candidates()
 
-    # verify all the combinations of weights_n_bits and activation_n_bits are allowed
-    if len(n.candidates_quantization_cfg) != len(unique_weights_candidates) * len(unique_activation_candidates):
-        Logger.error(f'Unsupported candidates_quantization_cfg for a trainable quantizer,'
-                            f'it must contain all the combinations of (weights_n_bits X activations_n_bits)') # pragma: no cover
-
     # generate list of weights quantizer candidates
     weights_cfg_candidates = [TrainableQuantizerCandidateConfig(
-        cfg.weights_quantization_cfg.weights_n_bits,
-        cfg.weights_quantization_cfg.weights_quantization_params) for cfg in unique_weights_candidates]
+        cfg.weights_quantization_cfg.get_attr_config(attr).weights_n_bits,
+        cfg.weights_quantization_cfg.get_attr_config(attr).weights_quantization_params)
+        for cfg in unique_weights_candidates]
 
     # generate list of activation quantizer candidates
     activation_cfg_candidates = [TrainableQuantizerCandidateConfig(
         cfg.activation_quantization_cfg.activation_n_bits,
         cfg.activation_quantization_cfg.activation_quantization_params) for cfg in unique_activation_candidates]
 
     return weights_cfg_candidates, activation_cfg_candidates
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py`

 * *Files 5% similar despite different names*

```diff
@@ -40,23 +40,23 @@
         quantizer_base_class: A type of quantizer that the requested quantizer should inherit from.
 
     Returns: A class of a quantizer that inherits from BaseKerasQATTrainableQuantizer.
 
     """
     qat_quantizer_classes = get_all_subclasses(quantizer_base_class)
     if len(qat_quantizer_classes) == 0:
-        Logger.error(f"No quantizers were found that inherit from {quantizer_base_class}.")  # pragma: no cover
+        Logger.critical(f"No quantizer classes inherited from {quantizer_base_class} were detected.")  # pragma: no cover
 
     filtered_quantizers = list(filter(lambda q_class: getattr(q_class, QUANTIZATION_TARGET, None) is not None and
                                                       getattr(q_class, QUANTIZATION_TARGET) == quant_target and
                                                       getattr(q_class, QUANTIZATION_METHOD, None) is not None and
                                                        quant_method in getattr(q_class, QUANTIZATION_METHOD, []) and
                                                       getattr(q_class, QUANTIZER_ID, None) == quantizer_id,
                                       qat_quantizer_classes))
 
     if len(filtered_quantizers) != 1:
-        Logger.error(f"Found {len(filtered_quantizers)} quantizer for target {quant_target.value} "  # pragma: no cover
-                     f"that matches the requested quantization method {quant_method.name} and "
-                     f"quantizer type {quantizer_id.value} but there should be exactly one."
-                     f"The possible quantizers that were found are {filtered_quantizers}.")
+        Logger.critical(f"Found {len(filtered_quantizers)} quantizers for target {quant_target.value}, "
+                        f"matching the requested quantization method {quant_method.name} and "
+                        f"quantizer type {quantizer_id.value}, but exactly one is required. "
+                        f"Identified quantizers: {filtered_quantizers}.")
 
     return filtered_quantizers[0]
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/qat/pytorch/quantizer/lsq/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py`

 * *Files 3% similar despite different names*

```diff
@@ -82,10 +82,9 @@
 
 else:
     class BaseKerasTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
 
             super().__init__(quantization_config)
-            Logger.critical('Installing tensorflow is mandatory '
-                            'when using BaseKerasQuantizer. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use BaseKerasTrainableQuantizer. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/load_model.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/load_model.py`

 * *Files 10% similar despite different names*

```diff
@@ -44,16 +44,16 @@
 
         qi_trainable_custom_objects = {subclass.__name__: subclass for subclass in
                                        get_all_subclasses(BaseKerasTrainableQuantizer)}
         qi_trainable_custom_objects.update({
             KerasTrainableQuantizationWrapper.__name__: KerasTrainableQuantizationWrapper})
         all_trainable_names = list(qi_trainable_custom_objects.keys())
         if len(set(all_trainable_names)) < len(all_trainable_names):
-            Logger.error(f"Found multiple quantizers with the same name that inherit from BaseKerasTrainableQuantizer"
-                         f"while trying to load a model.")
+            Logger.critical("Found multiple quantizers with identical names inheriting from "
+                            "'BaseKerasTrainableQuantizer' while trying to load a model.")
 
         qi_custom_objects = {**qi_trainable_custom_objects}
 
         if custom_objects is not None:
             qi_custom_objects.update(custom_objects)
         return mct_quantizers.keras_load_quantized_model(filepath,
                                                          custom_objects=qi_custom_objects, compile=compile,
@@ -68,10 +68,9 @@
             custom_objects: Additional custom objects
             compile: Boolean, whether to compile the model after loading.
             options: Optional `tf.saved_model.LoadOptions` object that specifies options for loading from SavedModel.
 
         Returns: A keras Model
 
         """
-        Logger.critical('Installing tensorflow is mandatory '
-                        'when using keras_load_quantized_model. '
-                        'Could not find Tensorflow package.')  # pragma: no cover
+        Logger.critical("Tensorflow must be installed to use keras_load_quantized_model. "
+                        "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/quantize_wrapper.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/quantize_wrapper.py`

 * *Files 2% similar despite different names*

```diff
@@ -87,15 +87,15 @@
             weight_keys = [_weight_name(w.name) for w in inferable_quantizers_wrapper.layer.weights]
             layer_weights_list = [None] * len(weight_keys)
             for w in self.weights:
                 if _weight_name(w.name) in weight_keys:
                     layer_weights_list[weight_keys.index(_weight_name(w.name))] = w
             # Verify all the weights in the list are ready. The "set_weights" method expects all the layer's weights
             if not all(w is not None for w in layer_weights_list):
-                Logger.error(f'Not all weights are set for layer {self.layer.name}')
+                Logger.critical(f"Not all weights are set for layer '{self.layer.name}'")
             assert all(w is not None for w in layer_weights_list)
             inferable_quantizers_wrapper.set_weights(layer_weights_list)
 
             # The wrapper inference is using the weights of the quantizers, so it expects to create them by running _set_weights_vars
             inferable_quantizers_wrapper._set_weights_vars(False)
             inferable_quantizers_wrapper.trainable = False
             return inferable_quantizers_wrapper
@@ -106,10 +106,9 @@
             """
             Keras Quantization Wrapper takes a keras layer and quantizers and infer a quantized layer.
 
             Args:
                 layer: A keras layer.
                 weights_quantizers: A dictionary between a weight's name to its quantizer.
             """
-            Logger.critical('Installing tensorflow is mandatory '
-                            'when using KerasTrainableQuantizationWrapper. '
-                            'Could not find Tensorflow package.')  # pragma: no cover
+            Logger.critical("Tensorflow must be installed to use KerasTrainableQuantizationWrapper. "
+                            "The 'tensorflow' package is missing.")  # pragma: no cover
```

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/common/__init__.py`

 * *Files identical despite different names*

### Comparing `mct-nightly-1.9.0.20230927.post404/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py` & `mct-nightly-2.0.0.20240402.404/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -56,10 +56,10 @@
             return quantizer_trainable
 
 else:
     class BasePytorchTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             super().__init__(quantization_config)
-            Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchTrainableQuantizer. '
-                            'Could not find torch package.')  # pragma: no cover
+            Logger.critical("PyTorch must be installed to use 'BasePytorchTrainableQuantizer'. "
+                            "The 'torch' package is missing.")  # pragma: no cover
+
```

### Comparing `mct-nightly-1.9.0.20230927.post404/setup.py` & `mct-nightly-2.0.0.20240402.404/setup.py`

 * *Files identical despite different names*

