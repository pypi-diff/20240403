# Comparing `tmp/trove-21.0.0.0rc2.tar.gz` & `tmp/trove-8.0.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "trove-21.0.0.0rc2.tar", last modified: Fri Mar 29 17:21:13 2024, max compression
+gzip compressed data, was "dist/trove-8.0.1.tar", last modified: Fri May 10 16:32:41 2019, max compression
```

## Comparing `trove-21.0.0.0rc2.tar` & `trove-8.0.1.tar`

### file list

```diff
@@ -1,1412 +1,1636 @@
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      584 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/.coveragerc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       72 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/.stestr.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14508 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/AUTHORS
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10834 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/CONTRIBUTING.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)   126493 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/ChangeLog
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      162 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/HACKING.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/LICENSE
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3008 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/PKG-INFO
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1725 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/README.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/api-ref/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.782867 trove-21.0.0.0rc2/api-ref/source/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      394 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/api-versions.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2583 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/backup-strategy.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4833 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/backups.inc
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     6674 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/conf.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3317 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/configurations.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3134 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/databases.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8609 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/datastore-versions.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      992 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/datastores.inc
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      441 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7577 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/instance-actions.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5563 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/instance-logs.inc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    12451 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/instances.inc
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)    22094 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/parameters.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1987 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/quotas.inc
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/api-ref/source/samples/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      175 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      551 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-create-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      586 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-get-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1366 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      131 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-strategy-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      229 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-strategy-create-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      267 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/backup-strategy-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      262 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-group-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      519 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-group-create-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      138 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-group-list-instances-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       97 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-group-patch-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-group-put-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      519 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-group-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      482 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/config-groups-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      272 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/databases-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      187 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/databases-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3873 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1132 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      258 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      671 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      981 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-mgmt-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       99 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-mgmt-patch-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      439 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-mgmt-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      190 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-parameter-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      296 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-parameter-create-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1060 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-parameter-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      187 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-parameter-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      190 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-parameter-update-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      239 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-parameter-update-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      594 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/datastore-version-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       35 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-action-eject-replica-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       40 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-action-promote-replica-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       26 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-action-reset-status-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       51 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-action-resize-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       78 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-action-resize-volume-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       22 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-action-restart-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      763 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-backup-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1647 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-configuration-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      702 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1436 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-create-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2753 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-list-detail-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1000 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       44 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-disable-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      346 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-disable-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       44 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-discard-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      290 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-discard-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       43 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-enable-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      290 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-enable-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      726 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       44 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-publish-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      346 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-publish-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       26 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-show-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      347 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-log-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       56 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-action-migrate-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       20 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-action-reboot-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       79 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-action-rebuild-instance-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       31 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-action-reset-task-status-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       18 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-action-stop-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3856 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3743 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-mgmt-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       55 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-patch-detach-replica-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       62 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-patch-update-name-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       84 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-patch-upgrade-datastore-version-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       92 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-put-attach-config-group-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1337 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      134 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/instance-update-access-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1370 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/limit-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      422 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/quota-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       72 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/quota-update.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       28 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-check-root-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      597 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-create-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      107 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-enable-root-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       74 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-grant-databases-access-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      593 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-list-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      119 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-put-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       56 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-root-enable-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      168 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-show-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      178 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/user-show-root-history-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      156 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/users-put-request.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      320 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/samples/versions-response.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5465 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/api-ref/source/users.inc
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/backup/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1242 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/Dockerfile
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/backup/drivers/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8903 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4344 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/innobackupex.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3668 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/mariabackup.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5586 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/mysql_base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11199 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/postgres.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5163 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/drivers/xtrabackup.py
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     3928 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/install.sh
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5860 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/main.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      337 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/requirements.txt
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/backup/storage/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/storage/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1554 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/storage/base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11093 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/storage/swift.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/backup/tests/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/backup/tests/unittests/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/tests/unittests/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/backup/tests/unittests/drivers/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/tests/unittests/drivers/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4303 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/tests/unittests/drivers/test_innobackupex.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4032 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/tests/unittests/drivers/test_mariadb.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11844 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/tests/unittests/drivers/test_postgres.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6963 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/tests/unittests/drivers/test_xtrabackup.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/backup/utils/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1790 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/utils/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2432 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/backup/utils/postgresql.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      926 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/bindep.txt
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/contrib/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1240 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/contrib/trove-guestagent
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1043 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/contrib/trove-network-driver
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/devstack/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1090 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/README.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/devstack/files/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1568 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/files/apache-trove-api.template
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/devstack/files/debs/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       26 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/files/debs/trove
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/devstack/files/rpms/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       26 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/files/rpms/trove
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/devstack/files/rpms-suse/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       26 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/files/rpms-suse/trove
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    29123 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/plugin.sh
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5285 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/devstack/settings
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/doc/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      615 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/requirements.txt
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.790867 trove-21.0.0.0rc2/doc/source/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.794867 trove-21.0.0.0rc2/doc/source/admin/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    12625 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/building_guest_images.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1871 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/database_management.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5280 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/datastore.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      266 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1874 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/network_isolation.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    18431 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/run_trove_in_production.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    47193 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/secure_oslo_messaging.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5904 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/troubleshooting.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13127 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/admin/upgrade.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.794867 trove-21.0.0.0rc2/doc/source/cli/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/cli/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10075 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/cli/trove-manage.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1942 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/cli/trove-status.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9509 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/conf.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.794867 trove-21.0.0.0rc2/doc/source/contributor/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3481 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/contributor/contributing.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5102 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/contributor/design.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4164 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/contributor/functional_test.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      294 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/contributor/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4453 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/contributor/release-notes.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3743 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/contributor/testing.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1414 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/index.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.794867 trove-21.0.0.0rc2/doc/source/install/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1495 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/apache-mod-wsgi.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3014 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/common_configure.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5783 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/common_prerequisites.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      738 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/dashboard.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3201 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/get_started.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      440 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4729 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/install-devstack.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7040 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/install-manual.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      995 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/install-redhat.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1226 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/install-suse.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1237 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/install-ubuntu.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      227 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/next-steps.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1105 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/install/verify.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.794867 trove-21.0.0.0rc2/doc/source/reference/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      740 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/conf-file.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      314 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/conf.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      193 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      144 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/notifier.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      609 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/policy-file.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      417 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/policy.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      117 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/reference/trove_api_extensions.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/doc/source/user/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    27667 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/backup-db.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8938 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/create-db.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      587 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/index.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1705 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/instance-status.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4492 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/manage-db-and-users.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11671 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/manage-db-config.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7629 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/set-up-clustering.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7088 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/set-up-replication.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3332 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/upgrade-cluster-datastore.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5254 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/doc/source/user/upgrade-datastore.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/etc/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/etc/apache2/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1236 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/apache2/trove
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/etc/tests/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1204 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/tests/core.test.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3197 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/tests/localhost.test.conf
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/etc/trove/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      646 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/trove/README-policy.generated.md
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1505 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/trove/api-paste.ini
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1338 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/trove/api-paste.ini.test
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      668 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/trove/trove-logging-guestagent.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5252 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/etc/trove/trove.conf.test
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1372 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/generate_examples.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/integration/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6854 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/integration/scripts/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/integration/scripts/conf/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/cassandra.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/couchbase.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      278 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/couchdb.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/db2.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/mariadb.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/mongodb.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/mysql.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/percona.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      286 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/postgresql.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      282 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/pxc.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      278 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/redis.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2686 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/test_begin.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       23 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/test_end.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      286 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/conf/vertica.conf
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     2657 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/create_vm
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/integration/scripts/files/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1579 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/15-trove-dep
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1272 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/20-guest-systemd
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1296 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/62-ssh-key
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.798867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      946 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/15-trove-dep
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      280 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/20-etc
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      369 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/21-use-fedora-certificates
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      449 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/50-user
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1153 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/62-ssh-key
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      140 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/05-ipforwarding
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      510 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/62-trove-guest-sudoers
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      165 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/90-yum-update
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mariadb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      118 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mariadb/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mariadb/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      181 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mariadb/install.d/10-mariadb
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mariadb/pre-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      186 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mariadb/pre-install.d/10-percona-copr
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       39 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mongodb/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mongodb/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      709 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mongodb/install.d/10-mongodb
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      197 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mongodb/install.d/25-trove-mongo-dep
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      116 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      432 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/install.d/10-mysql
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/install.d/40-xtrabackup
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/post-install.d/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      118 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mysql/post-install.d/30-register-mysql-service
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-percona/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-percona/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      484 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-percona/install.d/05-percona-server
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      707 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-percona/install.d/10-mysql
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-postgresql/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-postgresql/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     2255 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-postgresql/install.d/10-postgresql
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       45 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-redis/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-redis/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      160 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-redis/install.d/10-redis
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-cassandra/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-cassandra/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1479 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-cassandra/install.d/10-cassandra
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchbase/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchbase/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      342 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchbase/install.d/10-couchbase
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.766867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchdb/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchdb/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      583 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchdb/install.d/10-couchdb
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1877 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/extra-data.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1164 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/extra-data.d/20-copy-db2-pkgs
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1886 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/install.d/10-db2
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       40 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-mongodb/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-mongodb/pre-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      389 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-mongodb/pre-install.d/10-mongodb-apt-key
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      779 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/install.d/30-mysql
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/pre-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      758 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/pre-install.d/10-percona-apt-key
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      199 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/pre-install.d/20-apparmor-mysql-local
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-pxc/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-pxc/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      408 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-pxc/install.d/30-mysql
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-pxc/pre-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      758 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-pxc/pre-install.d/10-percona-apt-key
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      199 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-pxc/pre-install.d/20-apparmor-mysql-local
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       45 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-redis/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-redis/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     3359 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-redis/install.d/30-redis
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      197 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-redis/install.d/80-fix-in-guest-agent-env
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       85 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/README.md
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/extra-data.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      459 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/extra-data.d/93-copy-vertica-deb
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     1716 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/install.d/97-vertica
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-cassandra/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       17 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-cassandra/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-couchbase/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       17 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-couchbase/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-couchdb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       15 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-couchdb/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.802867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       15 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      599 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/10-mongodb-thp
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      110 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/20-mongodb
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      198 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/25-trove-mongo-dep
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      492 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/30-mongodb-conf
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      637 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/35-check-numa
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      307 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/40-check-numa-systemd
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      663 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/41-mongod-systemd
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      560 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/42-mongos-systemd
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-percona/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       15 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-percona/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-percona/post-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      179 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-percona/post-install.d/10-fix-mycnf
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-pxc/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       11 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-pxc/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-pxc/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      294 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-pxc/install.d/31-fix-my-cnf
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       13 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      582 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/install.d/31-fix-init-file
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/elements/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      133 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/README.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       69 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     2994 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/31-guest-agent-install
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      329 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/docker-hostnic-dev.service
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      274 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/docker-hostnic.service
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      138 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/docker-hostnic.socket
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1423 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent-dev.service
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      382 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2024 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.init
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      142 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.logrotate
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      390 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.service
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      470 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-log-collection.service
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      196 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-log-collection.timer
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      614 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/package-installs.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      654 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/pkg-map
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/post-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      309 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/post-install.d/31-enable-guest-agent-systemd
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      194 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/post-install.d/99-clean-apt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      248 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/source-repository-guest-agent
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       64 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/svc-map
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/root-passwd/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      202 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/root-passwd/README.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/root-passwd/post-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      487 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/root-passwd/post-install.d/99-setup
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-docker/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       12 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-docker/element-deps
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-docker/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      540 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-docker/install.d/21-docker
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/extra-data.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      490 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/extra-data.d/11-ssh-key-dev
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      544 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/install.d/11-user
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      723 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/install.d/12-ssh-key-dev
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/post-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      101 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/post-install.d/11-ipforwarding
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/post-install.d/12-ntp
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      510 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/post-install.d/13-trove-guest-sudoers
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/pre-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      229 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-guest/pre-install.d/11-baseline-tools
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-mysql/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-mysql/pre-install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      686 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-mysql/pre-install.d/10-percona-apt-key
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-xenial-guest/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-xenial-guest/install.d/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      300 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/elements/ubuntu-xenial-guest/install.d/22-decrease-networking-timeout
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.806867 trove-21.0.0.0rc2/integration/scripts/files/keys/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      395 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/keys/authorized_keys
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1678 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/keys/id_rsa
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      395 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/keys/id_rsa.pub
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.810867 trove-21.0.0.0rc2/integration/scripts/files/requirements/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1016 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/requirements/fedora-requirements.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      852 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/requirements/ubuntu-requirements.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1787 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/trove-guest.systemd.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1852 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/files/trove-guest.upstart.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11298 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/functions
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3719 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/functions_qemu
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       18 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/image-projects-list
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2359 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/localrc.rc
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      147 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/projects-list
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      268 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/reviews.rc
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)    53187 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/trovestack
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4377 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/scripts/trovestack.rc
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/integration/tests/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.810867 trove-21.0.0.0rc2/integration/tests/integration/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1620 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/core.test.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8744 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/int_tests.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2008 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/localhost.test.conf
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.810867 trove-21.0.0.0rc2/integration/tests/integration/tests/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      937 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    16247 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/colorizer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2412 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/initialize.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.810867 trove-21.0.0.0rc2/integration/tests/integration/tests/util/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/util/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2179 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/util/report.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3762 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/util/rpc.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9561 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/integration/tests/integration/tests/util/services.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.810867 trove-21.0.0.0rc2/playbooks/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.810867 trove-21.0.0.0rc2/playbooks/image-build/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2705 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/playbooks/image-build/docker-registry.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      257 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/playbooks/image-build/post.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      638 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/playbooks/image-build/run.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       62 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/playbooks/trove-devstack-base.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      153 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/pylintrc
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/releasenotes/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.822867 trove-21.0.0.0rc2/releasenotes/notes/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/.placeholder
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       72 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-cassandra-log-retrieval-a295f3d0d4c56804.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      164 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-cinder-az-option-d4ff1968e6064ff2.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       35 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-cors-support-fe3ecbecb68f7efd.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-designate-v2-dns-driver-8d1be56ab2c71b83.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      126 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-icmp-flag-58937cce344e77d9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      102 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-instance-detailed-list-e712dccf6c9091c0.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      160 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-max-prep-stmts-ac1056e127de7609.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      335 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-network-isolation-support-640f7105eb90651a.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-new-relic-license-driver-0f314edabb7561c4.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      392 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/add-support-create-instance-with-rootdisk-cinder.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       87 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/alter-user-portable-021f4b792e2c129b.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      527 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/associate-volume-type-datastore-97defb9279b61c1f.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      607 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/avoid-diverged-slave-when-migrating-mariadb-master-37e2429a1ea75913.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      106 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cassandra-backup-and-restore-00de234de67ea5ee.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      182 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cassandra-configuration-groups-e6bcf4014a79f14f.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3818 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cassandra-user-functions-041abfa4f4baa591.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       87 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cluster-configuration-groups-37f7de9e5a343165.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      366 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cluster-notifications-fd205f5f0148b052.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       83 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cluster-volume-type-901329a3b3667cb4.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      149 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cluster_list_show_all_ips-3547635440.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       57 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/cluster_restart-bb5abb7372131ee0.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       73 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/couchdb-backup-restore-0cc3324c3088f947.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       82 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/couchdb-user-db-functions-fa41ac47fce095cb.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       58 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/datastore-manager-refactor-5aeac4e6bfa6e07b.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       79 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/db2-backup-restore-96ab214cddd15181.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       88 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/db2-configuration-groups-ca2164be741d35f9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      634 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/db2-online-backup-restore-3783afe752562e70.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/dbaas-ceilometer-notifications-5a623d0d6520be72.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/deprecate-default_neutron_networks-84cd00224d6b7bc1.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      391 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/deprecate-long-query-time-b85af24772e2e7cb.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      112 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/disply_module_bools_properly-571cca9a87f28339.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/drop-py-2-7-010fe6df0c10352d.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       54 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/drop-python-26-support-39dff0c5636edc74.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      129 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/drop-python-3-6-and-3-7-51489f1a80c2e5e5.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      461 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-apply-configuration-on-prepare-4cff827b7f3c4d33.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      284 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-bad-swift-endpoint-in-guestlog-05f7483509dacbbf.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      117 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-cluster-creation-error-c1fd7674e71dd6a2.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      111 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-cluster-show-346798b3e3.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      407 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-cluster-type-error-71cd846897dfd32e.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      235 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-clustering-database-status-check-61fa0f49dd786c72.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-deprecated-SafeConfigParse-ca3fd3e9f52a8cc8.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      114 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-docker-start-failed-160e79b6e5494edd.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      206 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-galera_common-cluster-shrink-e2c80913423772dd.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      135 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-gtid-parsing-9f60ad6e9e8f173f.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      151 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-guest-agent-config-missing.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      199 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-illegal-value-be1acadc8c54c224.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-ipv6-route-exists-b5f07569270b6066.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      142 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-missing-request-id-in-log.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      146 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-module-apply-after-remove-97c84c30fb320a46.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      174 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-mongo-cluster-grow-8fa4788af0ce5309.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-mysql-replication-bf2b131994a5a772.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      126 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-mysql-replication-ca0928069c0bfab8.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       49 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-postgres-pg-rewind-6eef0afb568439ce.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      101 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-postgresql-database-create-failed-abd4f99cc7dde44c.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      198 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-postgresql-socket-path-2028103b91543e4c.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      284 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-postgress-create-database-75fbe03e3b4e296d.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      321 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-prepare-mariadb-after-restore-472112bbcdcecdb9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      144 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-redis-configuration-f0543ede84f8aac3.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      193 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-server-restart-bug-9746b2c2ed00a45a.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      233 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-swift-connect-over-tls-c4e62213a8d38fe2.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       86 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix-trove-events-8ce54233504065cf.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      328 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix_mod_inst_cmd-3a46c7233e3.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix_module_apply-042fc6e61f721540.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       81 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix_module_driver_logging-666601f411db784a.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      264 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix_mysql_missing_configdir-fa9e2e647dd46846.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      155 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix_mysql_permission_problem-2698e6a4dcc6e444.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      108 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fix_notification_err_msgs-e52771108633c9cf.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      293 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/fixes-mariadb-config-groups-b5fa4f44a8ed7b85.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       59 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/flavor-list-disk-6213c3760e374441.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       66 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/flavor-list-ephemeral-edf2dc35d5c247b3.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       60 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/flavor-list-vcpu-817b0f5715820377.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      249 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/force_delete-c2b06dbead554726.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      123 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/grow-cluster-nic-az-0e0fe4083666c300.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      200 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/guest-call-timeout-2781a57ca8feb89a.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      162 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/healthcheck-middleware-670a667bfb245123.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      162 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/implement-cassandra-clustering-9f7bc3ae6817c19e.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      140 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/implement-cassandra-root-b0870d23dbf1a848.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      160 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/implement-mariadb-clustering-088ac2f6012689fb.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      148 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/implement-redis-root-347b5ee0107debb5.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      121 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/improve-mysql-user-list-pagination-71457d934500f817.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      238 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/incremental_backup-1910ded0fc3474a3.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      123 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/instance-show-comp-vol-id-964db9f52a5ac9c1.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      177 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/instance-upgrade-7d464f85e025d729.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      238 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/locality-support-for-clusters-78bb74145d867df2.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      223 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/locality-support-for-replication-01d9b05d27b92d82.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       86 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/make-password-length-datastore-specific-7cdb1bfeab6e6227.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      339 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mariadb-gtid-replication-1ea972bcfe909773.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       64 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mask-configuration-passwords-317ff6d2415b2ca1.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      403 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/module-management-66d3979cc45ed440.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      292 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/module-ordering-92b6445a8ac3a3bf.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      146 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/module-support-for-clusters-87b41dd7648275bf.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      183 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/module_reapply-342c0965a4318d4e.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      252 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/module_reapply_update_values-1fb88dc58701368d.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      324 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mongo-cluster-create-use-extended-perperties-ced87fde31c6c110.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      168 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mongo-cluster-grow-use-az-and-nic-values-207b041113e7b4fb.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      156 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mountpoint-detection-096734f0097eb75a.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      123 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/multi-region-cd8da560bfe00de5.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      196 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mysql-config-preserve-types-77b970162bf6df08.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      131 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mysql-root-fix-35079552e25170ca.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      208 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mysql-user-list-pagination-9496c401c180f605.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       65 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/mysql8-6a81a8498ee2c229.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      316 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/percona-2.3-support-2eab8f12167e44bc.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      136 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/persist-error-message-fb69ddf885bcde84.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      125 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/pgsql-incremental-backup-acb4421f7de3ac09.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      137 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/pgsql-streaming-replication-f4df7e4047988b21.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      160 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/post-upgrade-fixes-828811607826d433.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       89 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/postgres-user-list-race-46624dc9e4420e02.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      152 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/postgresql-use-proper-guestagent-models-7ba601c7b4c001d6.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       67 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/pxc-cluster-root-enable-30c366e3b5bcda51.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      614 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/pxc-grow-shrink-0b1ee689cbc77743.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      138 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/quota-management-3792cbc25ebe16bb.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       96 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/redis-upgrade-63769ddb1b546cb9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       68 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/remove-bionic-support-85f506117e566813.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      165 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/remove-global-default_password_length-a30b75abb0c5091f.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      179 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/remove-idle-timeout-e4a5db0d5ee524d7.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       79 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/remove-override-templates-85429da7f66e006a.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      226 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/remove-support-of-use-nova-server-volume-2a334f57d8213810.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      169 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/return-http-204-for-disable-root-api-a818fc41fd6e75eb.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      105 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/reuse-cassandra-connections-092cf2a762a2e796.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      235 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/secure-mongodb-instances-1e6d7df3febab8f4.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      179 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/separate-backup-docker-image-884165.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      379 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/slo-backups-3c35135316f837e1.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      295 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/support-backup-strategy.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      226 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/support-new-mariadb-versions-in-backup-docker-image-7e3106446dcb4dd0.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      823 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/support-nova-keypair-a2cdb2da5c1511e9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      345 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/support-online-resize.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      327 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/support-subnet-and-ip-address.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      196 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/train-01-backup-filtering-90ff6deac7b411e9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      517 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/train-02-management-security-group.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      287 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/train-03-public-trove-instance-8ec456bed46411e9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      144 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/train-04-public-trove-images-127300c0df6c11e9.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      462 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/trove-status-upgrade-check-framework-b9d3d3e2463ec26d.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      119 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/update-myisam-recover-opt-232b9d680bc362bf.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      234 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/use-oslo-policy-bbd1b911e6487c36.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/use-osprofiler-options-58263c311617b127.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      278 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-add-ip-addresses-for-instance.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      195 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-add-service-status-updated.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      329 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-admin-clients-a14514a835ae11ea.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      306 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-database-instance-healthy.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      103 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-delete-datastoredad784e2345711ea.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      363 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-fix-delete-datastore-version.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      567 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-service-credential-config.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      144 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/ussuri-support-xfs-disk-format.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       86 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/vertica-configuration-groups-710c892c1e3d6a90.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      208 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/vertica-grow-shrink-cluster-e32d48f5b2e1bfab.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      450 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/vertica-load-via-curl-call-4d47c4e0b1b53471.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      348 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-check-subnet-router-association.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      763 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-database-containerization.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      129 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-expired-database-status.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       79 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-list-project-backups.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      460 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-rebuild-instance.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      172 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-resize-vollume-for-replication.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      133 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-show-replicas-for-instance-list.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       94 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-show-update-instance-access.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      194 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/victoria-trove-manager-remove-config-params.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      304 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-add-ram-quota-d8e64d0385b1429f.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      240 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-datastore-version-image-tags.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1008 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-deprecate-json-formatted-policy-file-21c88ff2ad490a2e.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      167 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-docker-registry.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      201 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-fix-deleting-volume.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      133 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-fix-race-condition-create-delete.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      134 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-mysql-8.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      386 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-operating-status.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      208 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/wallaby-restore-backup.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      233 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-add-iptables-persistent-package.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       79 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-allow-project-show-resource-quota.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      602 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-container-bridge-network.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       98 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-fix-backup-custom-image-registry.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      269 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-fix-postgresql-wal-archive-size.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      102 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-fix-resize-instance.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       68 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/notes/xena-show-network.yaml
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/releasenotes/source/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      130 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/2023.1.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      130 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/2023.2.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/releasenotes/source/_static/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/_static/.placeholder
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/releasenotes/source/_templates/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/_templates/.placeholder
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9088 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/conf.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      262 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/index.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/releasenotes/source/locale/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/releasenotes/source/locale/en_GB/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/releasenotes/source/locale/en_GB/LC_MESSAGES/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    73414 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      154 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/mitaka.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      154 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/newton.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      152 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/ocata.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/pike.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      147 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/queens.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      145 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/rocky.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      145 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/stein.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      126 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/train.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      112 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/unreleased.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      130 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/ussuri.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      144 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/victoria.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      140 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/wallaby.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      128 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/xena.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      128 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/yoga.rst
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      118 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/source/zed.rst
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/releasenotes/templates/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      253 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/templates/feature.yml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      241 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/releasenotes/templates/fix.yml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2013 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/requirements.txt
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.770867 trove-21.0.0.0rc2/roles/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/roles/trove-devstack/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      135 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/roles/trove-devstack/README
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/roles/trove-devstack/defaults/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      151 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/roles/trove-devstack/defaults/main.yml
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/roles/trove-devstack/tasks/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      379 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/roles/trove-devstack/tasks/main.yml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8298 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/run_tests.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2292 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/setup.cfg
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1030 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/setup.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1000 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/test-requirements.txt
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/tools/
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)     4522 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/install_venv.py
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      379 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/start-fake-mode.sh
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      325 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/stop-fake-mode.sh
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      351 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/trove-config-generator.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       71 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/trove-policy-generator.conf
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5256 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/trove-pylint.README
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    47451 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/trove-pylint.config
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)    11317 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/trove-pylint.py
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)      478 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tools/with_venv.sh
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3711 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/tox.ini
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/trove/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       30 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/README
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.830868 trove-21.0.0.0rc2/trove/backup/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/backup/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    18389 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/backup/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7391 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/backup/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      949 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/backup/state.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2803 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/backup/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.830868 trove-21.0.0.0rc2/trove/cluster/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cluster/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    28368 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cluster/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10837 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cluster/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2676 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cluster/tasks.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5058 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cluster/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.830868 trove-21.0.0.0rc2/trove/cmd/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1415 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1414 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1397 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/app_wsgi.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1957 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/common.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1666 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/conductor.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2238 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/fakemode.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4559 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/guest.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17350 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/manage.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10615 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/network_driver.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2732 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/status.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1641 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/cmd/taskmanager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14785 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    29116 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/apischema.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3508 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/auth.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3247 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/base_exception.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    27795 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/base_wsgi.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2116 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/cache.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    84532 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/cfg.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8795 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/clients.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5893 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/clients_admin.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2516 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/configurations.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      924 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/constants.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2923 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/context.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3175 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/crypto_utils.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/cassandra/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/cassandra/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1377 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/cassandra/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/couchdb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/couchdb/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1033 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/couchdb/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14376 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/mongodb/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5206 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/mongodb/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/mysql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/mysql/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    18592 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/mysql/data.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5503 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/mysql/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/postgresql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/postgresql/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1979 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/postgresql/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/db/redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/redis/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      901 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/db/redis/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5533 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/debug_utils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    19929 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/exception.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    20737 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/extensions.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1389 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/glance.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1157 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/i18n.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13582 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/limits.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1677 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/local.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4182 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9824 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/neutron.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    23527 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/notification.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5347 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/pagination.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4945 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/pastedeploy.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.842868 trove-21.0.0.0rc2/trove/common/policies/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1706 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3111 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/backups.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2343 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3131 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/clusters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2350 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/configuration_parameters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2954 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/configurations.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2243 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/databases.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3307 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/datastores.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1242 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/flavors.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7734 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/instances.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1009 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/limits.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2745 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/modules.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2783 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/root.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1689 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/user_access.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2918 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policies/users.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2898 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/policy.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1756 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/profile.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/rpc/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2026 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/conductor_guest_serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2810 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/conductor_host_serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1990 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/secure_serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2633 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3234 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      856 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/rpc/version.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11854 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/schemata.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1187 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/serializable_notification.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3618 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/server_group.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1392 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/base.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8598 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4708 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/guestagent.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15451 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/taskmanager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8696 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3375 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/guestagent.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14529 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/taskmanager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    30219 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5193 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/guestagent.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17512 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/taskmanager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8088 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3074 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/guestagent.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6184 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/taskmanager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9232 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3459 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/guestagent.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9443 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/taskmanager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1619 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/cluster/strategy.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1997 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/strategies/strategy.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17841 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/stream_codecs.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1414 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/swift.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4776 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/template.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2613 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/timeutils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2004 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/trove_remote.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13744 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/utils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1361 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/views.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    23625 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/wsgi.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2896 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/common/xmlutils.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/conductor/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/conductor/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4700 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/conductor/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6379 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/conductor/manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1595 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/conductor/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/configuration/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/configuration/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14757 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/configuration/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    18439 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/configuration/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4692 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/configuration/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/datastore/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/datastore/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    29844 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/datastore/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5379 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/datastore/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4672 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/datastore/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.846868 trove-21.0.0.0rc2/trove/db/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2904 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5232 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.850868 trove-21.0.0.0rc2/trove/db/sqlalchemy/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4314 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3833 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/mappers.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.850868 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      107 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/README
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      769 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/manage.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1020 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/migrate.cfg
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2663 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/schema.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1586 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/001_base_schema.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1213 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/002_service_images.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1469 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/003_service_statuses.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1291 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/004_root_enabled.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1295 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/005_heartbeat.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1161 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/006_dns_records.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1206 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/007_add_volume_flavor.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1107 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/008_add_instance_fields.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1148 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/009_add_deleted_flag_to_instances.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1564 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/010_add_usage.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2624 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/011_quota.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1974 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/012_backup.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3230 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/013_add_security_group_artifacts.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1452 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/014_update_instance_flavor_id.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1110 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/015_add_service_type.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2279 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/016_add_datastore_type.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1764 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/017_update_datastores.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      939 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/018_datastore_versions_fix.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4443 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/019_datastore_fix.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2259 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/020_configurations.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1304 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/021_conductor_last_seen.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1036 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/022_add_backup_parent_id.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1372 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/023_add_instance_indexes.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1435 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/024_add_backup_indexes.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1221 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/025_add_service_statuses_indexes.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1674 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/026_datastore_versions_unique_fix.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1972 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/027_add_datastore_capabilities.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2165 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/028_recreate_agent_heartbeat.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1219 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/029_add_backup_datastore.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1131 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/030_add_master_slave.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1116 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/031_add_timestamps_to_configurations.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2396 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/032_clusters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2064 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/033_datastore_parameters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      954 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/034_change_task_description.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      949 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/035_flavor_id_int_to_string.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2078 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/036_add_datastore_version_metadata.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3193 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/037_modules.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1879 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/038_instance_faults.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1270 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/039_region.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1942 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/040_module_priority.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1028 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/041_instance_keys.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1407 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/042_add_cluster_configuration_id.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1905 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/043_instance_ds_version_nullable.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2962 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/044_remove_datastore_configuration_parameters_deleted.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1739 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/045_add_backup_strategy.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1032 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/046_add_access_to_instance.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1101 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/047_image_tag_in_datastore_version.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2646 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/048_add_version_to_datastore_version.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3925 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/migration.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3620 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/session.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2942 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/db/sqlalchemy/utils.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/dns/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/dns/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/dns/designate/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/dns/designate/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5194 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/dns/designate/driver.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3776 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/dns/driver.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2714 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/dns/manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2443 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/dns/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5023 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/common/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11599 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/common/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1343 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/common/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/mgmt/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1779 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3358 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2025 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/mgmt/configuration/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/configuration/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5698 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/configuration/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1774 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/configuration/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/mgmt/datastores/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/datastores/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8548 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/datastores/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2003 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/datastores/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.854868 trove-21.0.0.0rc2/trove/extensions/mgmt/instances/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/instances/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10839 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/instances/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9497 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/instances/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6128 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/instances/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/mgmt/quota/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/quota/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3211 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/quota/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1306 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/quota/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/mgmt/upgrade/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/upgrade/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1654 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/upgrade/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1682 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mgmt/upgrade/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mongodb/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1833 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mongodb/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/mysql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mysql/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3255 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mysql/common.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10257 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mysql/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17334 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mysql/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1900 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/mysql/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/pxc/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/pxc/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1108 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/pxc/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/redis/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1075 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/redis/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7562 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/redis/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1069 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/redis/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/routes/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/routes/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3077 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/routes/mgmt.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2833 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/routes/mysql.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/security_group/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/security_group/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6236 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/security_group/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/extensions/vertica/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/vertica/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1531 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/extensions/vertica/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/flavor/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/flavor/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2471 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/flavor/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1966 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/flavor/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2138 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/flavor/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/guestagent/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    28344 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/api.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.858868 trove-21.0.0.0rc2/trove/guestagent/common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    22850 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/common/configuration.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6116 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/common/guestagent_utils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    32115 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/common/operating_system.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11683 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/common/sql_query.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/datastore/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    39470 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/manager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/datastore/mariadb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mariadb/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1400 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mariadb/manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3498 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mariadb/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1834 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql/manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3802 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql_common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql_common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15334 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql_common/manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    37297 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/mysql_common/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11006 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5062 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/query.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    29919 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    22028 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/datastore/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2981 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/dbaas.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15367 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/guest_log.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3048 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/module/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3642 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/driver_manager.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/module/drivers/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/drivers/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7840 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/drivers/module_driver.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3337 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/drivers/new_relic_license_driver.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1956 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/drivers/ping_driver.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9066 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/module/module_manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    16357 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/pkg.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1196 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/strategies/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2029 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2613 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2523 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/mariadb_gtid.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6503 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/mysql_base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2680 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/mysql_gtid.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9531 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/strategies/replication/postgresql.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/guestagent/utils/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/utils/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9149 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/utils/docker.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2708 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/utils/mysql.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14838 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/guestagent/volume.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/hacking/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/hacking/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3039 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/hacking/checks.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.862868 trove-21.0.0.0rc2/trove/instance/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/instance/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    86655 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/instance/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    32100 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/instance/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4099 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/instance/service_status.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5586 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/instance/tasks.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9075 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/instance/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/limits/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/limits/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1459 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/limits/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1904 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/limits/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/module/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/module/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    19145 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/module/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9877 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/module/service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4210 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/module/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/network/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/network/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1201 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/network/base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2217 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/network/neutron.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2114 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/network/nova.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/quota/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/quota/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/quota/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13471 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/quota/quota.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/rpc.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/taskmanager/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/taskmanager/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11813 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/taskmanager/api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    26117 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/taskmanager/manager.py
--rwxrwxr-x   0 zuul      (1000) zuul      (1000)    96052 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/taskmanager/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      769 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/taskmanager/service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.774867 trove-21.0.0.0rc2/trove/templates/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/cassandra/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    39019 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/cassandra/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14798 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/cassandra/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/couchbase/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/couchbase/config.template
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/couchdb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/couchdb/config.template
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/db2/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/db2/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14005 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/db2/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/mariadb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      590 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mariadb/cluster.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1539 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mariadb/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      117 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mariadb/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      100 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mariadb/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6850 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mariadb/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/mongodb/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       79 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mongodb/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9070 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mongodb/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/mysql/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/mysql/5.5/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      122 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/5.5/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       53 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/5.5/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1618 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/config.template
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/mysql/mysql-test/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       11 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/mysql-test/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      289 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      143 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13482 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/mysql/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/percona/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.866868 trove-21.0.0.0rc2/trove/templates/percona/5.5/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      122 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/percona/5.5/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       53 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/percona/5.5/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1627 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/percona/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      289 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/percona/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      175 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/percona/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6184 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/percona/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/templates/postgresql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    27431 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/postgresql/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       19 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/postgresql/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       19 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/postgresql/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    19874 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/postgresql/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/templates/pxc/
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/templates/pxc/5.5/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      122 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/5.5/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       53 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/5.5/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      537 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/cluster.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1627 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      267 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      175 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6185 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/pxc/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/templates/redis/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    44950 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/redis/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       20 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/redis/replica.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       51 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/redis/replica_source.config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7813 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/redis/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/templates/vertica/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/vertica/config.template
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    57617 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/templates/vertica/validation-rules.json
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/tests/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1554 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.870868 trove-21.0.0.0rc2/trove/tests/api/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    19473 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/backups.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    38151 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/configurations.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7596 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/databases.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7964 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/datastores.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    44181 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/instances.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    22591 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/instances_actions.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3896 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/instances_delete.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17278 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/instances_resize.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5837 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/limits.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.874868 trove-21.0.0.0rc2/trove/tests/api/mgmt/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/mgmt/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8042 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/mgmt/configurations.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6964 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/mgmt/datastore_versions.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7786 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/mgmt/instances_actions.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6989 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/mgmt/quotas.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15859 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/replication.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7371 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/root.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    21186 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/user_access.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    18969 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/users.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3219 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/api/versions.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7130 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/config.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.874868 trove-21.0.0.0rc2/trove/tests/db/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/db/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7056 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/db/migrations.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.874868 trove-21.0.0.0rc2/trove/tests/fakes/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      752 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      856 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/common.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      760 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/conf.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3285 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/dns.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13791 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/guestagent.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2656 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/keystone.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      799 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/limits.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2047 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/neutron.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    26135 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/nova.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    21500 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/swift.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2103 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/fakes/taskmanager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11289 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/int_tests.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2587 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/root_logger.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.874868 trove-21.0.0.0rc2/trove/tests/scenario/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.874868 trove-21.0.0.0rc2/trove/tests/scenario/groups/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7278 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15172 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/backup_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    19130 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/cluster_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11443 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/configuration_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6787 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/database_actions_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    12669 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/guest_log_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4291 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_actions_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4734 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_create_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2025 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_delete_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3659 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_error_create_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2231 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_force_delete_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4191 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_upgrade_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    30330 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/module_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13198 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/replication_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9616 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/root_actions_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      841 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/test_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9664 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/groups/user_actions_group.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.878868 trove-21.0.0.0rc2/trove/tests/scenario/helpers/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6201 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/cassandra_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4110 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/couchbase_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4182 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/couchdb_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1656 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/db2_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      862 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/mariadb_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1634 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/mongodb_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2286 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/mysql_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      862 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/percona_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2600 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/postgresql_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      854 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/pxc_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9461 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/redis_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6230 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/sql_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    20083 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/test_helper.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2125 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/helpers/vertica_helper.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.878868 trove-21.0.0.0rc2/trove/tests/scenario/runners/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      166 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    20285 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/backup_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    35025 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/cluster_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    25421 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/configuration_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9536 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/database_actions_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    35667 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/guest_log_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4878 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_actions_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15678 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_create_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2068 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_delete_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5037 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_error_create_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2316 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_force_delete_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2781 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_upgrade_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    70128 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/module_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4506 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/negative_cluster_actions_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    20328 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/replication_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10215 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/root_actions_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    43746 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/test_runners.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    22684 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/scenario/runners/user_actions_runners.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.878868 trove-21.0.0.0rc2/trove/tests/unittests/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.878868 trove-21.0.0.0rc2/trove/tests/unittests/api/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/api/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.878868 trove-21.0.0.0rc2/trove/tests/unittests/api/common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/api/common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4121 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/api/common/test_extensions.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    28626 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/api/common/test_limits.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9754 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/api/test_versions.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.878868 trove-21.0.0.0rc2/trove/tests/unittests/backup/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/backup/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6504 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/backup/test_backup_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    26455 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/backup/test_backup_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3161 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/backup/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/cluster/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4312 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cassandra_cluster.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9899 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17494 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1917 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11269 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_pxc_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    12750 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_redis_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11316 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_vertica_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7763 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_views.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17525 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_galera_cluster.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9660 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    24851 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_mongodb_cluster.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11870 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_redis_cluster.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13283 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_vertica_cluster.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/cmd/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cmd/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2012 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/cmd/test_status.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1250 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_auth.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4414 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_conductor_serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3600 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_context.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4293 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_crypto_utils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13633 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_dbmodels.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1046 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_exception.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15466 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_notification.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5093 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_pagination.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2314 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_policy.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2470 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_secure_serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4765 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_serializer.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4824 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_server_group.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1462 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_stream_codecs.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4601 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_template.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3811 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_timeutils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7897 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_utils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2323 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/common/test_wsgi.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/conductor/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/conductor/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2596 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/conductor/test_conf.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     8279 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/conductor/test_methods.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/configuration/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/configuration/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9868 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/configuration/test_configuration_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4659 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/configuration/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/datastore/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/datastore/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3622 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/datastore/base.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1766 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_capability.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3461 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_datastore.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9168 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_datastore_version_metadata.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2437 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_datastore_versions.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/db/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/db/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5339 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/db/test_migration_utils.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/domain-name-service/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/domain-name-service/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5221 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/domain-name-service/test_designate_driver.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/extensions/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/extensions/common/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/common/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    16964 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/common/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.882868 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/datastores/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/datastores/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    14711 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/datastores/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/instances/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/instances/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    20685 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/instances/test_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3279 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/instances/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/quota/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/quota/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2749 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/quota/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/flavor/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/flavor/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3188 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/flavor/test_flavor_views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/__init__.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/postgres/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/postgres/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4241 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/postgres/test_manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2221 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/postgres/test_service.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3855 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/datastore/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/utils/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/utils/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6759 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/guestagent/utils/test_docker.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/hacking/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/hacking/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4896 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/hacking/test_check.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/instance/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/instance/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17389 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    16479 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6134 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_status.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6911 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_views.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10314 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/instance/test_service.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/module/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/module/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3212 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/module/test_module_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6406 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/module/test_module_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3297 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/module/test_module_views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/mysql/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/mysql/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6883 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/mysql/test_common.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    17015 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/mysql/test_user_controller.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/quota/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/quota/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    27939 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/quota/test_quota.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.886868 trove-21.0.0.0rc2/trove/tests/unittests/router/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/router/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1555 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/router/test_router.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6011 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_api.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    29330 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_clusters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13758 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_galera_clusters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    15170 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_manager.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    53397 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    11944 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_vertica_clusters.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4275 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/trove_testtools.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/trove/tests/unittests/upgrade/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/upgrade/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     4994 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/upgrade/test_controller.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3542 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/upgrade/test_models.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/trove/tests/unittests/util/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/util/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6618 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/util/matchers.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1294 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/util/util.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/trove/tests/unittests/volume_type/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/volume_type/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2006 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/volume_type/test_volume_type.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2304 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/unittests/volume_type/test_volume_type_views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/trove/tests/util/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10540 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     7183 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/check.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2482 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/client.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    10991 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/event_simulator.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     6170 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/mysql.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3173 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/server_connection.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2980 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/usage.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     5088 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/users.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2011 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/tests/util/utils.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      704 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/version.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3164 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/versions.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/trove/volume_type/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/volume_type/__init__.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     2448 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/volume_type/models.py
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1458 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/trove/volume_type/views.py
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.826867 trove-21.0.0.0rc2/trove.egg-info/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     3008 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/PKG-INFO
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    55692 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/SOURCES.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        1 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/dependency_links.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1459 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/entry_points.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        1 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/not-zip-safe
--rw-rw-r--   0 zuul      (1000) zuul      (1000)       47 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/pbr.json
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1178 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/requires.txt
--rw-rw-r--   0 zuul      (1000) zuul      (1000)        6 2024-03-29 17:21:13.000000 trove-21.0.0.0rc2/trove.egg-info/top_level.txt
-drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2024-03-29 17:21:13.890868 trove-21.0.0.0rc2/zuul.d/
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     9544 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/zuul.d/deprecated_jobs.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)    13654 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/zuul.d/jobs.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)      404 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/zuul.d/nodesets.yaml
--rw-rw-r--   0 zuul      (1000) zuul      (1000)     1680 2024-03-29 17:20:46.000000 trove-21.0.0.0rc2/zuul.d/projects.yaml
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/apidocs/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/apidocs/src/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/apidocs/src/samples/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/apidocs/src/samples/db-get-default-instance-configuration-response-json.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       17 2019-05-10 16:30:46.000000 trove-8.0.1/babel.cfg
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       72 2019-05-10 16:30:46.000000 trove-8.0.1/.stestr.conf
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove.egg-info/
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)     1161 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/requires.txt
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)        1 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/not-zip-safe
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)     1326 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/entry_points.txt
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)     1932 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/PKG-INFO
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)    67715 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/SOURCES.txt
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)        6 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/top_level.txt
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)       47 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/pbr.json
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)        1 2019-05-10 16:32:40.000000 trove-8.0.1/trove.egg-info/dependency_links.txt
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/devstack/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    25267 2019-05-10 16:30:49.000000 trove-8.0.1/devstack/plugin.sh
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/devstack/files/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/devstack/files/debs/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       26 2019-05-10 16:30:46.000000 trove-8.0.1/devstack/files/debs/trove
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/devstack/files/rpms/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       27 2019-05-10 16:30:46.000000 trove-8.0.1/devstack/files/rpms/trove
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/devstack/files/rpms-suse/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       26 2019-05-10 16:30:46.000000 trove-8.0.1/devstack/files/rpms-suse/trove
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1547 2019-05-10 16:30:46.000000 trove-8.0.1/devstack/files/apache-trove-api.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3491 2019-05-10 16:30:49.000000 trove-8.0.1/devstack/settings
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1112 2019-05-10 16:30:49.000000 trove-8.0.1/devstack/README.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/api-ref/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/api-ref/source/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      834 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/api-versions.inc
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     8129 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/conf.py
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      345 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/index.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/api-ref/source/samples/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      249 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-root-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-delete-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       91 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-change-user-attributes-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      558 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-create-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-enable-root-user-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      243 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-users-pagination-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      227 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-get-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      276 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-version-by-id-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      250 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameters-without-datastore-version-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      292 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-databases-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      196 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-list-accounts-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-databases-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       81 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-detach-replica-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      376 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-storage-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      167 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-update-config-group-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-list-accounts-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      465 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-host-detail-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1640 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-cfg-defaults-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      191 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-incremental-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      100 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-user-access-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      106 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-revoke-user-access-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      197 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-list-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      198 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-create-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-check-root-user-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      295 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-revoke-user-access-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1366 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backups-by-instance-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      262 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-instance-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      192 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instances-index-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-users-pagination-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      243 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-get-default-instance-configuration-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-config-group-details-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      229 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-attach-to-instance-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      114 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-instance-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       49 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-flavor-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1288 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instances-index-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      243 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-update-parameters-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      266 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-databases-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      240 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-databases-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8454 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-parameters-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-instance-details-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-host-detail-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      304 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameter-for-datastore-version-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-cfg-groups-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      203 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-version-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameters-for-datastore-version-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1634 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-get-default-instance-configuration-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      175 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-databases-pagination-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-create-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      246 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-instance-diagnostics-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-show-parameter-details-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      190 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-flavors-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       97 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-patch-config-group-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-versions-list-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      234 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-check-root-user-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameters-without-datastore-version-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      134 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-user-access-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-restart-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      248 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-config-group-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-list-accounts-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      239 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-versions-list-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-attach-config-group-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      101 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-patch-config-group-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2137 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-instance-index-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      204 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-host-detail-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       98 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-edit-parameters-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-delete-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-delete-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      101 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-update-config-group-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      213 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-config-group-details-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      158 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-faults-instanceFault.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backups-by-instance-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-flavors-by-id-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      604 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/samples/db-list-datastore-versions.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      463 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-list-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1366 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-list-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2684 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-instance-details-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-change-user-attributes-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backups-by-instance-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-flavor-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      452 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-flavors-by-id-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      193 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-instance-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      247 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-users-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      243 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-detach-replica-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-get-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      286 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-version-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      287 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameters-for-datastore-version-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-disable-root-user-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      260 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-incremental-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2548 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instances-index-pagination-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1549 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameters-without-datastore-version-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-update-parameters-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      196 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-list-hosts-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-version-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      242 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-reboot-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       40 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-promote-replica-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       21 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-reboot-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-volume-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      229 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-detach-from-instance-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      232 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-instance-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-parameters-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      191 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-show-parameter-details.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6794 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-flavors-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      229 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-status-detail-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      665 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-instance-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instances-index-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      195 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-storage-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      170 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameter-without-datastore-version-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       28 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-check-root-user-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-versions-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      198 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-versions-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-config-group-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-volume-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      116 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-instance-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      219 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-datastore-versions-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      110 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-incremental-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      110 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      209 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-cfg-groups-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      106 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-grant-user-access-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-cfg-defaults-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1131 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-by-id-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1207 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-restore-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-flavors-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastores-list-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-instance-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      193 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-restore-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-config-group-instances-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      716 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-users-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      192 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-flavors-by-id-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-users-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       35 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-eject-replica-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-edit-parameters-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      426 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-cfg-groups-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      343 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-versions-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      236 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-users-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      608 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-versions-list-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-get-default-instance-configuration-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-change-users-password-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-detach-config-group-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-version-by-id-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       78 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-volume-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      551 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      193 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-list-hosts-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      272 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-user-dbs-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      247 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-promote-replica-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      230 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-delete-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      215 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-cfg-defaults-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-datastore-versions-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      232 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-restore-delete-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1268 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastores-list-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      244 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-list-instances-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-attach-to-instance-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-list-hosts-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-users-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      586 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-get-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      188 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-config-group-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8454 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-parameters-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      151 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-root-details-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      214 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-update-config-group-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-instance-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       23 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-detach-config-group-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      194 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-instance-diagnostics-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      240 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-restore-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       76 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-enable-root-user-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      234 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-details-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-list-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      399 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-create-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      193 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastores-list-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      558 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-details-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-databases-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameter-for-datastore-version-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      877 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-users-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      241 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-detach-config-group-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-by-id-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      236 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-edit-parameters-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      234 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-update-parameters-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-root-details-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      260 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-change-users-password-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-list-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      235 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-enable-root-user-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       56 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-detach-from-instance-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      100 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-user-dbs-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-instance-diagnostics-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      148 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-list-instances-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      241 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-attach-config-group-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      211 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-instance-index-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-user-access-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      223 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-config-group-instances-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      170 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameter-for-datastore-version-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      138 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-config-group-instances-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-account-details-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-details-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      239 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-databases-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      208 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-change-users-password-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      239 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-root-details-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-grant-user-access-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      103 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-list-instances-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      261 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-parameters-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-resize-flavor-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      285 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-databases-pagination-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      252 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-databases-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      230 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-by-id-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      191 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      535 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-datastore-version-by-id-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      200 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instances-index-pagination-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      267 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameter-without-datastore-version-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-status-detail-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1549 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameters-for-datastore-version-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-status-detail-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      697 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-users-pagination-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1210 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-instance-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      210 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-config-group-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      247 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-eject-replica-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-reboot-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      247 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-databases-pagination-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      216 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-patch-config-group-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       92 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-attach-config-group-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      136 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-account-details-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-storage-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-instance-index-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-detach-from-instance-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      107 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-faults-itemNotFound.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-databases-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      190 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-list-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instances-index-pagination-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      201 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-account-details-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      217 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-user-dbs-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      243 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-change-user-attributes-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/samples/db-disable-root-user-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      149 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-faults-badRequest.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      102 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-detach-replica-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       22 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-restart-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      277 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-show-parameter-details-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      385 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-create-config-group-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      235 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-list-users-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      467 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-config-group-details-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      234 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-mgmt-get-instance-details-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-parameter-without-datastore-version-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      104 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-restore-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-restore-delete-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-delete-users-response-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      237 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-instance-restart-request-json-http.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       92 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-configuration-attach-to-instance-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      134 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-grant-user-access-request.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      609 2019-05-10 16:30:46.000000 trove-8.0.1/api-ref/source/samples/db-backup-create-incremental-response.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2267 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/datastores.inc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4100 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/configurations.inc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4638 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/database-instance-actions.inc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7027 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/database-instances.inc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1805 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/databases.inc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9303 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/user-management.inc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1488 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/flavors.inc
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     5184 2019-05-10 16:30:49.000000 trove-8.0.1/api-ref/source/parameters.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      255 2019-05-10 16:30:49.000000 trove-8.0.1/HACKING.rst
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)    94379 2019-05-10 16:32:40.000000 trove-8.0.1/ChangeLog
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8959 2019-05-10 16:30:49.000000 trove-8.0.1/run_tests.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      584 2019-05-10 16:30:46.000000 trove-8.0.1/.coveragerc
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1372 2019-05-10 16:30:46.000000 trove-8.0.1/generate_examples.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      387 2019-05-10 16:30:49.000000 trove-8.0.1/.zuul.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      167 2019-05-10 16:30:49.000000 trove-8.0.1/.testr.conf
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)     1932 2019-05-10 16:32:41.000000 trove-8.0.1/PKG-INFO
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1048 2019-05-10 16:30:49.000000 trove-8.0.1/test-requirements.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      133 2019-05-10 16:30:49.000000 trove-8.0.1/blacklist-py3.txt
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       41 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/image-projects-list
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      235 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/conf.json.example
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      848 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4507 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/trovestack.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      268 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/reviews.rc
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/conf/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      278 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/redis.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      286 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/vertica.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      278 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/conf/couchdb.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/mysql.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/percona.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/db2.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       23 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/conf/test_end.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/pxc.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      286 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/postgresql.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/mongodb.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3600 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/test_begin.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/cassandra.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/couchbase.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      282 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/conf/mariadb.conf
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1598 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/trove-guest.upstart.conf
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mongodb/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mongodb/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      197 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mongodb/install.d/25-trove-mongo-dep
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      696 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mongodb/install.d/10-mongodb
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       39 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mongodb/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-percona/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-percona/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      484 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-percona/install.d/05-percona-server
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      707 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-percona/install.d/10-mysql
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      973 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/15-trove-dep
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      490 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/50-user
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      163 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/98-ssh
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      232 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/05-base-apps
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1084 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/62-ssh-key
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      169 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/install.d/99-clean-apt
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/post-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      510 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/post-install.d/62-trove-guest-sudoers
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      101 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/post-install.d/05-ipforwarding
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      195 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/post-install.d/10-ntp
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      170 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/post-install.d/90-apt-get-update
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1636 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/extra-data.d/15-trove-dep
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1296 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/extra-data.d/62-ssh-key
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      160 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/pre-install.d/04-baseline-tools
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-percona/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       15 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-percona/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-couchbase/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-couchbase/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      342 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-couchbase/install.d/10-couchbase
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/apt-conf-dir/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/apt-conf-dir/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      646 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/apt-conf-dir/extra-data.d/99-use-host-apt-confd
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      456 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/apt-conf-dir/README.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-couchbase/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       17 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-couchbase/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-redis/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-redis/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      582 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-redis/install.d/31-fix-init-file
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       13 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-redis/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-postgresql/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-postgresql/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     2782 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/fedora-postgresql/install.d/10-postgresql
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-couchdb/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-couchdb/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      583 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-couchdb/install.d/10-couchdb
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      432 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/install.d/10-mysql
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      167 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/install.d/40-xtrabackup
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/post-install.d/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      118 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/post-install.d/30-register-mysql-service
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      116 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mysql/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-cassandra/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-cassandra/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      903 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-cassandra/install.d/10-cassandra
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mysql/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mysql/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      712 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mysql/install.d/30-mysql
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       13 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mysql/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-vertica/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       15 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-vertica/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      280 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/20-etc
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      946 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/15-trove-dep
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      449 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/50-user
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1153 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/62-ssh-key
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/post-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      510 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/post-install.d/62-trove-guest-sudoers
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      165 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/post-install.d/90-yum-update
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      140 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/post-install.d/05-ipforwarding
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1636 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/extra-data.d/15-trove-dep
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1272 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/extra-data.d/20-guest-systemd
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1296 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-guest/extra-data.d/62-ssh-key
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1065 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/10-mongodb-thp
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      134 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/20-mongodb
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1154 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/41-mongod-init
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      198 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/25-trove-mongo-dep
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      635 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/42-mongos-init
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      524 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/install.d/30-mongodb-conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       40 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      423 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mongodb/pre-install.d/10-mongodb-apt-key
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-redis/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-redis/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     3346 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-redis/install.d/30-redis
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       45 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-redis/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mongodb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       15 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mongodb/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-redis/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-redis/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      160 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-redis/install.d/10-redis
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       45 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-redis/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-cassandra/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       17 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-cassandra/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mariadb/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mariadb/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1353 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mariadb/install.d/30-mariadb
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       15 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mariadb/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-pxc/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-pxc/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      408 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-pxc/install.d/30-mysql
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-pxc/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1372 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-pxc/pre-install.d/10-percona-apt-key
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      199 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-pxc/pre-install.d/20-apparmor-mysql-local
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1703 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/install.d/97-vertica
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      459 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/extra-data.d/93-copy-vertica-deb
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       85 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-db2/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       11 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-db2/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mariadb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      118 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mariadb/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mariadb/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1316 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mariadb/pre-install.d/10-percona-apt-key
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      199 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mariadb/pre-install.d/20-apparmor-mysql-local
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mariadb/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mariadb/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1087 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mariadb/install.d/30-mariadb
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       15 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-mariadb/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-pxc/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-pxc/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      294 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-pxc/install.d/31-fix-my-cnf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       11 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-pxc/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mariadb/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mariadb/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      181 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mariadb/install.d/10-mariadb
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      118 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mariadb/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mariadb/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      186 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/fedora-mariadb/pre-install.d/10-percona-copr
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      779 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/install.d/30-mysql
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1372 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/pre-install.d/10-percona-apt-key
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      199 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/pre-install.d/20-apparmor-mysql-local
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      278 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/install.d/20-etc
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1273 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/extra-data.d/20-guest-systemd
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       13 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     2532 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-guest/pre-install.d/01-trim-pkgs
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-postgresql/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-postgresql/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     2563 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-postgresql/install.d/30-postgresql
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-postgresql/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      347 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-postgresql/pre-install.d/10-postgresql-repo
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-pxc/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       11 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-pxc/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mysql/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mysql/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      787 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mysql/install.d/30-mysql
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       13 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-mysql/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      224 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/install.d/20-etc
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1269 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/extra-data.d/20-guest-upstart
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       13 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     3409 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-guest/pre-install.d/01-trim-pkgs
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-postgresql/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-postgresql/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      172 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-postgresql/install.d/31-fix-init-script
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       18 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-postgresql/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1798 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/install.d/10-db2
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/extra-data.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1164 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/extra-data.d/20-copy-db2-pkgs
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1848 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-couchdb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       15 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-couchdb/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-redis/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       13 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-redis/element-deps
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mysql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      116 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mysql/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mysql/pre-install.d/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1325 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mysql/pre-install.d/10-percona-apt-key
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      199 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-mysql/pre-install.d/20-apparmor-mysql-local
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-postgresql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       18 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/elements/ubuntu-trusty-postgresql/element-deps
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1558 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/trove-guest.systemd.conf
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/keys/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      395 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/keys/authorized_keys
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1678 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/keys/id_rsa
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      395 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/files/keys/id_rsa.pub
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/files/requirements/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      856 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/requirements/ubuntu-requirements.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1009 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/files/requirements/fedora-requirements.txt
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/scripts/local.conf.d/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1476 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/sample.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       74 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/ceilometer_cinder.conf.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       46 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/use_uuid_token.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       69 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/ceilometer_services.conf.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       66 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/ceilometer_nova.conf.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      378 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/using_vagrant.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      578 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/trove_services.conf.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       69 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/local.conf.d/use_kvm.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      147 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/projects-list
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     2654 2019-05-10 16:30:46.000000 trove-8.0.1/integration/scripts/create_vm
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2402 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/localrc.rc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11291 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/functions
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)    59804 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/trovestack
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6179 2019-05-10 16:30:49.000000 trove-8.0.1/integration/scripts/functions_qemu
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6250 2019-05-10 16:30:49.000000 trove-8.0.1/integration/README.md
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1209 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/run_local.sh
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9009 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/int_tests.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2008 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/localhost.test.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      510 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tox.ini
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1706 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/core.test.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      929 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/setup.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/tests/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      937 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/tests/util/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/util/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2179 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/util/report.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3765 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/util/rpc.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9567 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/util/services.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    16258 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/colorizer.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/tests/volumes/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      934 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/volumes/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20997 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/volumes/driver.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       19 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/README
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6300 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/initialize.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/tests/api/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/api/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3102 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/api/instances_states.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1729 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/api/instances_quotas.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1106 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/api/delete_all.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7733 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/api/instances_pagination.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/tests/smoke/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/smoke/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3110 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/smoke/instance.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/integration/tests/dns/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/dns/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3242 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/dns/dns.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4092 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/dns/conversion.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6684 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/integration/tests/dns/check_domain.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3898 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/integration/tests/dns/concurrency.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/examples/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      326 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/local.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      309 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/README
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      335 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/tox.ini
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/integration/tests/examples/examples/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      396 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/examples/local.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    38708 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/examples/examples/example_generation.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9360 2019-05-10 16:30:49.000000 trove-8.0.1/integration/tests/examples/examples/client.py
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)       71 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/gendoc.sh
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      932 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/setup.py
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)       76 2019-05-10 16:30:46.000000 trove-8.0.1/integration/tests/examples/example_gen.sh
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3014 2019-05-10 16:30:49.000000 trove-8.0.1/tox.ini
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/votes/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      364 2019-05-10 16:30:49.000000 trove-8.0.1/doc/votes/channel_logging
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/source/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9905 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/conf.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1685 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/index.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/source/install/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15609 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/install/manual_install.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10137 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/install/conf.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      494 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/index.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3905 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/install/common_configure.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      647 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/install.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1162 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/install-obs.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      932 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/install-rdo.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      227 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/next-steps.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2366 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/get_started.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5783 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/common_prerequisites.txt
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      760 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/install/dashboard.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3830 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/install/verify.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1397 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/install/apache-mod-wsgi.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1870 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/install/install-ubuntu.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/source/reference/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      117 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/reference/trove_api_extensions.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      131 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/reference/index.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      144 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/reference/notifier.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/source/admin/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      207 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/admin/index.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    27581 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/admin/building_guest_images.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    16881 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/admin/basics.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2368 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/admin/guest_cloud_init.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    47634 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/admin/secure_oslo_messaging.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    24230 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/admin/database_module_usage.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/source/contributor/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      280 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/contributor/index.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5138 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/contributor/design.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    19547 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/contributor/how_to_create_a_trove_instance.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5402 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/contributor/testing.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/doc/source/cli/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      127 2019-05-10 16:30:49.000000 trove-8.0.1/doc/source/cli/index.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9446 2019-05-10 16:30:46.000000 trove-8.0.1/doc/source/cli/trove-manage.rst
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/etc/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/etc/apache2/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1231 2019-05-10 16:30:46.000000 trove-8.0.1/etc/apache2/trove
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/etc/tests/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3135 2019-05-10 16:30:46.000000 trove-8.0.1/etc/tests/localhost.test.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1267 2019-05-10 16:30:46.000000 trove-8.0.1/etc/tests/core.test.conf
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/etc/trove/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4111 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/policy.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5690 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/trove.conf.test
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/etc/trove/cloudinit/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      150 2019-05-10 16:30:46.000000 trove-8.0.1/etc/trove/cloudinit/README
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9963 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/trove.conf.sample
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1319 2019-05-10 16:30:46.000000 trove-8.0.1/etc/trove/api-paste.ini
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6045 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/trove-guestagent.conf.sample
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1889 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/trove-conductor.conf.sample
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1326 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/api-paste.ini.test
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8737 2019-05-10 16:30:49.000000 trove-8.0.1/etc/trove/trove-taskmanager.conf.sample
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/etc/trove/conf.d/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      236 2019-05-10 16:30:46.000000 trove-8.0.1/etc/trove/conf.d/README
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       57 2019-05-10 16:30:46.000000 trove-8.0.1/etc/trove/conf.d/guest_info.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      426 2019-05-10 16:30:46.000000 trove-8.0.1/etc/trove/trove-workbook.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      668 2019-05-10 16:30:46.000000 trove-8.0.1/etc/trove/trove-logging-guestagent.conf
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10143 2019-05-10 16:30:46.000000 trove-8.0.1/LICENSE
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2478 2019-05-10 16:32:41.000000 trove-8.0.1/setup.cfg
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/contrib/
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1239 2019-05-10 16:30:46.000000 trove-8.0.1/contrib/trove-guestagent
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      153 2019-05-10 16:30:46.000000 trove-8.0.1/pylintrc
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1030 2019-05-10 16:30:46.000000 trove-8.0.1/setup.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/releasenotes/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/releasenotes/notes/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      156 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mountpoint-detection-096734f0097eb75a.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       73 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/couchdb-backup-restore-0cc3324c3088f947.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      135 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-gtid-parsing-9f60ad6e9e8f173f.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      123 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/multi-region-cd8da560bfe00de5.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      196 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mysql-config-preserve-types-77b970162bf6df08.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      339 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mariadb-gtid-replication-1ea972bcfe909773.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      126 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/add-icmp-flag-58937cce344e77d9.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      160 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/add-max-prep-stmts-ac1056e127de7609.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      406 2019-05-10 16:30:49.000000 trove-8.0.1/releasenotes/notes/fix-cluster-type-error-71cd846897dfd32e.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       67 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/pxc-cluster-root-enable-30c366e3b5bcda51.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      126 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-mysql-replication-ca0928069c0bfab8.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      200 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/guest-call-timeout-2781a57ca8feb89a.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      284 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-bad-swift-endpoint-in-guestlog-05f7483509dacbbf.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      252 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/module_reapply_update_values-1fb88dc58701368d.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      112 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/disply_module_bools_properly-571cca9a87f28339.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      138 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/quota-management-3792cbc25ebe16bb.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3818 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cassandra-user-functions-041abfa4f4baa591.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      119 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/update-myisam-recover-opt-232b9d680bc362bf.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       83 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cluster-volume-type-901329a3b3667cb4.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      152 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/postgresql-use-proper-guestagent-models-7ba601c7b4c001d6.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      238 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/locality-support-for-clusters-78bb74145d867df2.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      403 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/module-management-66d3979cc45ed440.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      121 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/improve-mysql-user-list-pagination-71457d934500f817.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       87 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/alter-user-portable-021f4b792e2c129b.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      527 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/associate-volume-type-datastore-97defb9279b61c1f.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      125 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/pgsql-incremental-backup-acb4421f7de3ac09.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       86 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/make-password-length-datastore-specific-7cdb1bfeab6e6227.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      143 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/use-osprofiler-options-58263c311617b127.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      450 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/vertica-load-via-curl-call-4d47c4e0b1b53471.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       79 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/db2-backup-restore-96ab214cddd15181.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       58 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/datastore-manager-refactor-5aeac4e6bfa6e07b.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      168 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mongo-cluster-grow-use-az-and-nic-values-207b041113e7b4fb.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      634 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/db2-online-backup-restore-3783afe752562e70.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      136 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/persist-error-message-fb69ddf885bcde84.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      195 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix_module_apply-042fc6e61f721540.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      195 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-deprecated-SafeConfigParse-ca3fd3e9f52a8cc8.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      146 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-module-apply-after-remove-97c84c30fb320a46.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      167 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/add-new-relic-license-driver-0f314edabb7561c4.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      316 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/percona-2.3-support-2eab8f12167e44bc.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       59 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/flavor-list-disk-6213c3760e374441.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       60 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/flavor-list-vcpu-817b0f5715820377.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      208 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mysql-user-list-pagination-9496c401c180f605.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      234 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/use-oslo-policy-bbd1b911e6487c36.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       82 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/couchdb-user-db-functions-fa41ac47fce095cb.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      105 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/reuse-cassandra-connections-092cf2a762a2e796.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       49 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-postgres-pg-rewind-6eef0afb568439ce.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      160 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/implement-mariadb-clustering-088ac2f6012689fb.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      379 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/slo-backups-3c35135316f837e1.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       87 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cluster-configuration-groups-37f7de9e5a343165.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      208 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/vertica-grow-shrink-cluster-e32d48f5b2e1bfab.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       72 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/add-cassandra-log-retrieval-a295f3d0d4c56804.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      149 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cluster_list_show_all_ips-3547635440.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/.placeholder
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       89 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/postgres-user-list-race-46624dc9e4420e02.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      167 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/dbaas-ceilometer-notifications-5a623d0d6520be72.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      146 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/module-support-for-clusters-87b41dd7648275bf.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      293 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fixes-mariadb-config-groups-b5fa4f44a8ed7b85.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      160 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/post-upgrade-fixes-828811607826d433.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      177 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/instance-upgrade-7d464f85e025d729.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      249 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/force_delete-c2b06dbead554726.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       57 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cluster_restart-bb5abb7372131ee0.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      183 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/module_reapply-342c0965a4318d4e.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      131 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mysql-root-fix-35079552e25170ca.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      391 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/deprecate-long-query-time-b85af24772e2e7cb.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      123 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/grow-cluster-nic-az-0e0fe4083666c300.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      174 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-mongo-cluster-grow-8fa4788af0ce5309.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      328 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix_mod_inst_cmd-3a46c7233e3.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       86 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-trove-events-8ce54233504065cf.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       54 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/drop-python-26-support-39dff0c5636edc74.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       79 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/remove-override-templates-85429da7f66e006a.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       66 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/flavor-list-ephemeral-edf2dc35d5c247b3.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       86 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/vertica-configuration-groups-710c892c1e3d6a90.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      614 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/pxc-grow-shrink-0b1ee689cbc77743.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      182 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cassandra-configuration-groups-e6bcf4014a79f14f.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      238 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/incremental_backup-1910ded0fc3474a3.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      195 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-mysql-replication-bf2b131994a5a772.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      223 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/locality-support-for-replication-01d9b05d27b92d82.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      123 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/instance-show-comp-vol-id-964db9f52a5ac9c1.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      106 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/cassandra-backup-and-restore-00de234de67ea5ee.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      162 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/implement-cassandra-clustering-9f7bc3ae6817c19e.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      461 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-apply-configuration-on-prepare-4cff827b7f3c4d33.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      108 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix_notification_err_msgs-e52771108633c9cf.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      137 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/pgsql-streaming-replication-f4df7e4047988b21.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      111 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-cluster-show-346798b3e3.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      235 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/secure-mongodb-instances-1e6d7df3febab8f4.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       88 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/db2-configuration-groups-ca2164be741d35f9.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      144 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix-redis-configuration-f0543ede84f8aac3.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      292 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/module-ordering-92b6445a8ac3a3bf.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      140 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/implement-cassandra-root-b0870d23dbf1a848.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       35 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/add-cors-support-fe3ecbecb68f7efd.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       81 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/fix_module_driver_logging-666601f411db784a.yaml
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       64 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/notes/mask-configuration-passwords-317ff6d2415b2ca1.yaml
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/releasenotes/source/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/releasenotes/source/_static/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/source/_static/.placeholder
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/releasenotes/source/_templates/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/source/_templates/.placeholder
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9338 2019-05-10 16:30:49.000000 trove-8.0.1/releasenotes/source/conf.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      141 2019-05-10 16:30:49.000000 trove-8.0.1/releasenotes/source/index.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      152 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/source/ocata.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      112 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/source/unreleased.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      154 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/source/newton.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      154 2019-05-10 16:30:46.000000 trove-8.0.1/releasenotes/source/mitaka.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10673 2019-05-10 16:30:49.000000 trove-8.0.1/CONTRIBUTING.rst
+-rw-rw-r--   0 zuul      (1000) zuul      (1000)    10875 2019-05-10 16:32:40.000000 trove-8.0.1/AUTHORS
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/tools/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    46326 2019-05-10 16:30:49.000000 trove-8.0.1/tools/trove-pylint.config
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      325 2019-05-10 16:30:46.000000 trove-8.0.1/tools/stop-fake-mode.sh
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)    11358 2019-05-10 16:30:49.000000 trove-8.0.1/tools/trove-pylint.py
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     1990 2019-05-10 16:30:46.000000 trove-8.0.1/tools/test-setup.sh
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      379 2019-05-10 16:30:46.000000 trove-8.0.1/tools/start-fake-mode.sh
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     4700 2019-05-10 16:30:49.000000 trove-8.0.1/tools/install_venv.py
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)      478 2019-05-10 16:30:46.000000 trove-8.0.1/tools/with_venv.sh
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5321 2019-05-10 16:30:49.000000 trove-8.0.1/tools/trove-pylint.README
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)     4771 2019-05-10 16:30:46.000000 trove-8.0.1/run_tests.sh
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      908 2019-05-10 16:30:49.000000 trove-8.0.1/README.rst
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1868 2019-05-10 16:30:49.000000 trove-8.0.1/requirements.txt
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/hacking/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/hacking/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3955 2019-05-10 16:30:49.000000 trove-8.0.1/trove/hacking/translation_checks.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/backup/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/backup/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1861 2019-05-10 16:30:46.000000 trove-8.0.1/trove/backup/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3970 2019-05-10 16:30:49.000000 trove-8.0.1/trove/backup/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13916 2019-05-10 16:30:49.000000 trove-8.0.1/trove/backup/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      913 2019-05-10 16:30:46.000000 trove-8.0.1/trove/backup/state.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/cmd/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2083 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cmd/common.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1043 2019-05-10 16:30:46.000000 trove-8.0.1/trove/cmd/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1724 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cmd/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1395 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cmd/app.wsgi
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2717 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cmd/guest.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13067 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cmd/manage.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1601 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cmd/conductor.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1414 2019-05-10 16:30:46.000000 trove-8.0.1/trove/cmd/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2238 2019-05-10 16:30:46.000000 trove-8.0.1/trove/cmd/fakemode.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/trove/locale/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/trove/locale/fr/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/locale/fr/LC_MESSAGES/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1417 2019-05-10 16:30:49.000000 trove-8.0.1/trove/locale/fr/LC_MESSAGES/trove-log-error.po
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1413 2019-05-10 16:30:49.000000 trove-8.0.1/trove/locale/fr/LC_MESSAGES/trove-log-warning.po
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4475 2019-05-10 16:30:49.000000 trove-8.0.1/trove/locale/fr/LC_MESSAGES/trove-log-info.po
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/limits/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/limits/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1896 2019-05-10 16:30:46.000000 trove-8.0.1/trove/limits/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1459 2019-05-10 16:30:46.000000 trove-8.0.1/trove/limits/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/network/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/network/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6288 2019-05-10 16:30:49.000000 trove-8.0.1/trove/network/neutron.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1681 2019-05-10 16:30:46.000000 trove-8.0.1/trove/network/base.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3169 2019-05-10 16:30:49.000000 trove-8.0.1/trove/network/nova.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/configuration/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/configuration/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4578 2019-05-10 16:30:46.000000 trove-8.0.1/trove/configuration/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    18362 2019-05-10 16:30:49.000000 trove-8.0.1/trove/configuration/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15636 2019-05-10 16:30:49.000000 trove-8.0.1/trove/configuration/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/module/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/module/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4210 2019-05-10 16:30:46.000000 trove-8.0.1/trove/module/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9895 2019-05-10 16:30:49.000000 trove-8.0.1/trove/module/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    18762 2019-05-10 16:30:49.000000 trove-8.0.1/trove/module/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/account/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/account/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1117 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/account/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1947 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/account/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2001 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/account/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mongodb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mongodb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1815 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mongodb/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/security_group/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/security_group/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3788 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/security_group/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6230 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/security_group/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9612 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/security_group/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/pxc/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/pxc/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1108 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/pxc/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/postgresql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/postgresql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1248 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/postgresql/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/vertica/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/vertica/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1531 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/vertica/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/host/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/host/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1423 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/host/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1844 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/host/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/host/instance/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/host/instance/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2315 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/host/instance/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3861 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/host/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/volume/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/volume/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1439 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/volume/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1461 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/volume/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1698 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/volume/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/clusters/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/clusters/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2025 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/clusters/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3373 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/clusters/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1779 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/clusters/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/configuration/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/configuration/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1868 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/configuration/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5771 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/configuration/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/instances/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/instances/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6029 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/instances/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8991 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/instances/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10854 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/instances/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/upgrade/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/upgrade/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1719 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/upgrade/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1653 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/upgrade/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/datastores/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/datastores/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1800 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/datastores/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6447 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/datastores/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mgmt/quota/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/quota/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1306 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mgmt/quota/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2758 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mgmt/quota/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/cassandra/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/cassandra/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1171 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/cassandra/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/routes/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/routes/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1325 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/routes/account.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2022 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/routes/security_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3974 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/routes/mgmt.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2833 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/routes/mysql.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/mysql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3265 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mysql/common.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mysql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1900 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mysql/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17780 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/mysql/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10256 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/mysql/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/extensions/common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1343 2019-05-10 16:30:46.000000 trove-8.0.1/trove/extensions/common/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11495 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/common/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4409 2019-05-10 16:30:49.000000 trove-8.0.1/trove/extensions/common/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/volume_type/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/volume_type/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1458 2019-05-10 16:30:46.000000 trove-8.0.1/trove/volume_type/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1437 2019-05-10 16:30:46.000000 trove-8.0.1/trove/volume_type/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2432 2019-05-10 16:30:46.000000 trove-8.0.1/trove/volume_type/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/instance/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/instance/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7675 2019-05-10 16:30:49.000000 trove-8.0.1/trove/instance/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    25914 2019-05-10 16:30:49.000000 trove-8.0.1/trove/instance/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    72594 2019-05-10 16:30:49.000000 trove-8.0.1/trove/instance/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5160 2019-05-10 16:30:49.000000 trove-8.0.1/trove/instance/tasks.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      704 2019-05-10 16:30:46.000000 trove-8.0.1/trove/version.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       30 2019-05-10 16:30:46.000000 trove-8.0.1/trove/README
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/flavor/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/flavor/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2138 2019-05-10 16:30:46.000000 trove-8.0.1/trove/flavor/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1992 2019-05-10 16:30:46.000000 trove-8.0.1/trove/flavor/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2475 2019-05-10 16:30:46.000000 trove-8.0.1/trove/flavor/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/conductor/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/conductor/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1595 2019-05-10 16:30:46.000000 trove-8.0.1/trove/conductor/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5906 2019-05-10 16:30:49.000000 trove-8.0.1/trove/conductor/manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4736 2019-05-10 16:30:49.000000 trove-8.0.1/trove/conductor/api.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/quota/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/quota/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13397 2019-05-10 16:30:49.000000 trove-8.0.1/trove/quota/quota.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3072 2019-05-10 16:30:49.000000 trove-8.0.1/trove/quota/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/db/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3592 2019-05-10 16:30:49.000000 trove-8.0.1/trove/db/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4909 2019-05-10 16:30:49.000000 trove-8.0.1/trove/db/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/db/sqlalchemy/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2942 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3925 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migration.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1036 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/022_add_backup_parent_id.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1291 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/004_root_enabled.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1110 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/015_add_service_type.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2279 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/016_add_datastore_type.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1221 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/025_add_service_statuses_indexes.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1161 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/006_dns_records.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1219 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/029_add_backup_datastore.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1295 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/005_heartbeat.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3193 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/037_modules.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1131 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/030_add_master_slave.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2259 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/020_configurations.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1116 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/031_add_timestamps_to_configurations.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      939 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/018_datastore_versions_fix.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1435 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/024_add_backup_indexes.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      949 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/035_flavor_id_int_to_string.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      954 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/034_change_task_description.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4349 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/019_datastore_fix.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2624 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/011_quota.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1148 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/009_add_deleted_flag_to_instances.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1028 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/041_instance_keys.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1974 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/012_backup.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1879 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/038_instance_faults.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1407 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/042_add_cluster_configuration_id.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1372 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/023_add_instance_indexes.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1107 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/008_add_instance_fields.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1764 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/017_update_datastores.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1564 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/010_add_usage.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2078 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/036_add_datastore_version_metadata.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1213 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/002_service_images.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1244 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/039_region.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1452 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/014_update_instance_flavor_id.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3230 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/013_add_security_group_artifacts.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1908 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/040_module_priority.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1674 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/026_datastore_versions_unique_fix.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1972 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/027_add_datastore_capabilities.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2396 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/032_clusters.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1469 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/003_service_statuses.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1304 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/021_conductor_last_seen.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1206 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/007_add_volume_flavor.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2165 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/028_recreate_agent_heartbeat.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2064 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/033_datastore_parameters.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1586 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/001_base_schema.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      769 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/manage.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      107 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/README
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1020 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/migrate.cfg
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2664 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/migrate_repo/schema.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4719 2019-05-10 16:30:49.000000 trove-8.0.1/trove/db/sqlalchemy/session.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3743 2019-05-10 16:30:46.000000 trove-8.0.1/trove/db/sqlalchemy/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3722 2019-05-10 16:30:49.000000 trove-8.0.1/trove/db/sqlalchemy/mappers.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3096 2019-05-10 16:30:49.000000 trove-8.0.1/trove/versions.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/datastore/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/datastore/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4402 2019-05-10 16:30:46.000000 trove-8.0.1/trove/datastore/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4815 2019-05-10 16:30:46.000000 trove-8.0.1/trove/datastore/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    30715 2019-05-10 16:30:49.000000 trove-8.0.1/trove/datastore/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3852 2019-05-10 16:30:49.000000 trove-8.0.1/trove/rpc.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/taskmanager/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/taskmanager/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      769 2019-05-10 16:30:46.000000 trove-8.0.1/trove/taskmanager/service.py
+-rwxrwxrwx   0 zuul      (1000) zuul      (1000)    89426 2019-05-10 16:30:49.000000 trove-8.0.1/trove/taskmanager/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    22596 2019-05-10 16:30:49.000000 trove-8.0.1/trove/taskmanager/manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11150 2019-05-10 16:30:46.000000 trove-8.0.1/trove/taskmanager/api.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:40.000000 trove-8.0.1/trove/templates/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/mongodb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       79 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mongodb/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9070 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mongodb/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/percona/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1627 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/percona/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6184 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/percona/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/percona/5.5/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      122 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/percona/5.5/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       53 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/percona/5.5/replica_source.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      289 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/percona/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      175 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/percona/replica_source.config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/couchdb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/couchdb/config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/pxc/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1627 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      537 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/cluster.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6185 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/pxc/5.5/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      122 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/5.5/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       53 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/5.5/replica_source.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      267 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      175 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/pxc/replica_source.config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/postgresql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    22849 2019-05-10 16:30:49.000000 trove-8.0.1/trove/templates/postgresql/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    23467 2019-05-10 16:30:49.000000 trove-8.0.1/trove/templates/postgresql/validation-rules.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       19 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/postgresql/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       19 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/postgresql/replica_source.config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/mariadb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1606 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mariadb/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      590 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mariadb/cluster.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6525 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mariadb/validation-rules.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      123 2019-05-10 16:30:49.000000 trove-8.0.1/trove/templates/mariadb/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       55 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mariadb/replica_source.config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/vertica/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/vertica/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    57617 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/vertica/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/cassandra/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    39020 2019-05-10 16:30:49.000000 trove-8.0.1/trove/templates/cassandra/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14798 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/cassandra/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/mysql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1651 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mysql/config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/mysql/mysql-test/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       11 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mysql/mysql-test/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6525 2019-05-10 16:30:49.000000 trove-8.0.1/trove/templates/mysql/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/mysql/5.5/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      122 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mysql/5.5/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       53 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mysql/5.5/replica_source.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      289 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mysql/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      143 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/mysql/replica_source.config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/redis/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    44950 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/redis/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7813 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/redis/validation-rules.json
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       20 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/redis/replica.config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)       51 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/redis/replica_source.config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/couchbase/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/couchbase/config.template
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/templates/db2/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/db2/config.template
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14005 2019-05-10 16:30:46.000000 trove-8.0.1/trove/templates/db2/validation-rules.json
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    16276 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/pkg.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/backup/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1740 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/backup/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7810 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/backup/backupagent.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10406 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/volume.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1196 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/module/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/module/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9078 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/module/module_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3660 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/module/driver_manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/module/drivers/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/module/drivers/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7907 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/module/drivers/module_driver.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3393 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/module/drivers/new_relic_license_driver.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1997 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/module/drivers/ping_driver.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3118 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2956 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/dbaas.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15157 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mongodb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mongodb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1819 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mongodb/system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    34543 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mongodb/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10644 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mongodb/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/percona/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/percona/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2878 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/percona/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1375 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/percona/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchdb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchdb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3386 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchdb/system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    23984 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchdb/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6414 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchdb/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/pxc/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/pxc/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2011 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/pxc/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1102 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/pxc/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    38751 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13117 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5468 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/pgsql_query.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mariadb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mariadb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3907 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mariadb/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1119 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/mariadb/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/vertica/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/vertica/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5231 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/vertica/system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    28068 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/vertica/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6080 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/vertica/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/cassandra/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/cassandra/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    51598 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/cassandra/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11692 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/cassandra/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/redis/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/redis/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1212 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/redis/system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20043 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/redis/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10696 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/redis/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchbase/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchbase/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2432 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchbase/system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10458 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchbase/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4550 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/couchbase/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/experimental/db2/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/experimental/db2/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3881 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/db2/system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    26708 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/db2/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5890 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/experimental/db2/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/mysql_common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/mysql_common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    44308 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/mysql_common/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17840 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/mysql_common/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/galera_common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/galera_common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3604 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/galera_common/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3255 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/galera_common/manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    38836 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/technical-preview/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/technical-preview/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/datastore/mysql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/mysql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3685 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/datastore/mysql/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1366 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/datastore/mysql/manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    25707 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15540 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/guest_log.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/backup/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      953 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/backup/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4520 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/backup/mysql_impl.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4085 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/mongo_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1031 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/mariadb_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6453 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/db2_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1353 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/couchdb_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4735 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/cassandra_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10049 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/postgresql_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1407 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/redis_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4111 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/backup/experimental/couchbase_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4345 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/backup/base.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/restore/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      900 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/restore/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12746 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/restore/mysql_impl.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1813 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/mongo_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1032 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/mariadb_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3271 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/db2_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1556 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/couchdb_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2561 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/cassandra_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7737 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/postgresql_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3273 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/redis_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9593 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/restore/experimental/couchbase_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3509 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/restore/base.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/replication/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2033 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/replication/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/strategies/replication/experimental/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/replication/experimental/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3521 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/replication/experimental/redis_sync.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12559 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/replication/experimental/postgresql_impl.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2505 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/replication/experimental/mariadb_gtid.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6022 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/replication/mysql_base.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3179 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/replication/mysql_binlog.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3343 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/strategies/replication/mysql_gtid.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2625 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/strategies/replication/base.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/guestagent/common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11664 2019-05-10 16:30:46.000000 trove-8.0.1/trove/guestagent/common/sql_query.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    22061 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/common/configuration.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    29545 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/common/operating_system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4429 2019-05-10 16:30:49.000000 trove-8.0.1/trove/guestagent/common/guestagent_utils.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/cluster/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/cluster/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5081 2019-05-10 16:30:46.000000 trove-8.0.1/trove/cluster/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10287 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cluster/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    26659 2019-05-10 16:30:49.000000 trove-8.0.1/trove/cluster/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2676 2019-05-10 16:30:46.000000 trove-8.0.1/trove/cluster/tasks.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/dns/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/dns/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2443 2019-05-10 16:30:46.000000 trove-8.0.1/trove/dns/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2714 2019-05-10 16:30:46.000000 trove-8.0.1/trove/dns/manager.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/dns/designate/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/dns/designate/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6714 2019-05-10 16:30:49.000000 trove-8.0.1/trove/dns/designate/driver.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3776 2019-05-10 16:30:46.000000 trove-8.0.1/trove/dns/driver.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      396 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/util/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10994 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2550 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/server_connection.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2980 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/util/usage.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2047 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5303 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/users.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6187 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/mysql.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7180 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/util/check.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3744 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/client.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10997 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/util/event_simulator.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1406 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/config.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/services/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/services/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/services/database/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/services/database/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/services/database/json/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/services/database/json/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1364 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/services/database/json/versions_client.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1362 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/services/database/json/flavors_client.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1163 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/services/database/json/limits_client.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1583 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/plugin.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/tests/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/tests/api/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/flavors/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/flavors/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1357 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/flavors/test_flavors_negative.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3744 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/flavors/test_flavors.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/limits/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/limits/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2029 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/limits/test_limits.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/versions/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/versions/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1659 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/versions/test_versions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2868 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/tempest/tests/api/database/base.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/scenario/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/scenario/helpers/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      862 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/percona_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1656 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/db2_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4110 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/couchbase_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6201 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/cassandra_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      862 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/mariadb_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2125 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/vertica_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20083 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/test_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4182 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/couchdb_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2278 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/helpers/mysql_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7903 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/helpers/redis_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1634 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/mongodb_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2600 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/postgresql_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6121 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/helpers/sql_helper.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      854 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/helpers/pxc_helper.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/scenario/groups/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      861 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/test_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1587 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/negative_cluster_actions_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2380 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/instance_force_delete_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7220 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3882 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/instance_error_create_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11824 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/configuration_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3820 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/instance_upgrade_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    30330 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/module_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6985 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/database_actions_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    19130 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/cluster_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13850 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/replication_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    16292 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/backup_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3121 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/instance_delete_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9710 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/groups/root_actions_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12757 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/guest_log_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9823 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/user_actions_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4874 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/instance_actions_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5085 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/groups/instance_create_group.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/scenario/runners/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      166 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    34388 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/guest_log_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    25334 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/runners/configuration_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4773 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/instance_error_create_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20478 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/backup_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1874 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/instance_upgrade_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2068 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/instance_delete_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9536 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/database_actions_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    68739 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/runners/module_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    41223 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/runners/test_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20356 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/runners/replication_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4875 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/instance_actions_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9706 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/runners/root_actions_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    22694 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/user_actions_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    35073 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/cluster_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4499 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/scenario/runners/negative_cluster_actions_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15674 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/instance_create_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2316 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/scenario/runners/instance_force_delete_runners.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14361 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/int_tests.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7128 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/config.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/fakes/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      856 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/common.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      752 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2656 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/keystone.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2103 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      760 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/conf.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    21656 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/fakes/swift.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3338 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/dns.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13793 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/fakes/guestagent.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    28631 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/fakes/nova.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      799 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/fakes/limits.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/examples/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    45156 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/examples/snippets.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/examples/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11824 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/examples/client.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/db/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/db/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7056 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/db/migrations.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2554 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/root_logger.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/secgroups/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/secgroups/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6773 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/secgroups/test_security_group.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/hacking/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/hacking/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3845 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/hacking/test_translation_checks.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/backup/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/backup/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4103 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/backup/test_backup_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    24493 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/backup/test_backup_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    21238 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/backup/test_backupagent.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14682 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/backup/test_storage.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/util/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/util/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6618 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/util/matchers.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1007 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/util/util.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/network/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/network/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6065 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/network/test_neutron_driver.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/configuration/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/configuration/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9859 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/configuration/test_configuration_controller.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/module/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/module/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6397 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/module/test_module_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3212 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/module/test_module_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3288 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/module/test_module_views.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/domain-name-service/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/domain-name-service/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9821 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/domain-name-service/test_designate_driver.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/volume_type/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/volume_type/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2290 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/volume_type/test_volume_type_views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1990 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/volume_type/test_volume_type.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/upgrade/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/upgrade/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3532 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/upgrade/test_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4985 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/upgrade/test_controller.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/instance/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/instance/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5961 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/instance/test_instance_views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14426 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/instance/test_instance_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5664 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/instance/test_instance_status.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17820 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/instance/test_instance_models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/flavor/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/flavor/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2632 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/flavor/test_flavor_views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6549 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/trove_testtools.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/conductor/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/conductor/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2561 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/conductor/test_conf.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8262 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/conductor/test_methods.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/quota/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/quota/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    26171 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/quota/test_quota.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/db/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/db/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5311 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/db/test_migration_utils.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/datastore/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/datastore/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10108 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/datastore/test_datastore_version_metadata.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2019 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/datastore/test_capability.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2422 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/datastore/test_datastore_versions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3799 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/datastore/base.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3450 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/datastore/test_datastore.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/mgmt/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/mgmt/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20515 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/mgmt/test_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6987 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/mgmt/test_datastore_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7175 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/mgmt/test_datastores.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3720 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/mgmt/test_clusters.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/taskmanager/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/taskmanager/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    50712 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/taskmanager/test_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5961 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/taskmanager/test_api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14004 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/taskmanager/test_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12515 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/taskmanager/test_galera_clusters.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10713 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/taskmanager/test_vertica_clusters.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    22534 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/taskmanager/test_clusters.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/api/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/api/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8368 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/api/test_versions.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/api/common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3904 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/api/common/test_extensions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/api/common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    28349 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/api/common/test_limits.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/router/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/router/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1553 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/router/test_router.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/guestagent/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13989 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_couchdb_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5126 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_vertica_api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7707 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_guestagent_utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14685 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_redis_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2780 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    18558 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    52423 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_backups.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      932 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_datastore_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    23264 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5876 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_galera_cluster_api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20222 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_configuration.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    44768 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_operating_system.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12400 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_db2_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    21209 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_pkg.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6377 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_couchbase_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5182 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_galera_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2752 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_mariadb_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10090 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_volume.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8892 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_agent_heartbeats_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14152 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_query.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)   171804 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_dbaas.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17768 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_vertica_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11105 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_mongodb_cluster_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    32291 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_mysql_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    37103 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_cassandra_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15121 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_mongodb_manager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1127 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/guestagent/test_service.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/mysql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/mysql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17015 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/mysql/test_user_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6883 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/mysql/test_common.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/cluster/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15788 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/cluster/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10728 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_mongodb_cluster.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11256 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster_vertica_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11842 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/cluster/test_redis_cluster.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8158 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17039 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_galera_cluster.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1908 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster_models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4008 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cassandra_cluster.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9883 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7482 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster_views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12702 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster_redis_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11209 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_cluster_pxc_controller.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13273 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/cluster/test_vertica_cluster.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/unittests/common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4555 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_template.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4751 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    16356 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_notification.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5084 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_pagination.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1046 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_exception.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13619 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_dbmodels.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4400 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_conductor_serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4815 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_server_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6666 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2250 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_policy.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13265 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_common_extensions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3811 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_timeutils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1485 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_stream_codecs.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1584 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_wsgi.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    26381 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_remote.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4134 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_context.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2470 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/unittests/common/test_secure_serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4395 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/unittests/common/test_crypto_utils.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/api/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    68458 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/instances.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    21716 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/user_access.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14321 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/replication.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8496 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/databases.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5045 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/root_on_create.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1963 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/header.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    19348 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/users.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7830 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/root.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    28578 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/instances_actions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4772 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/instances_mysql_down.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6401 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/instances_delete.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    19164 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/backups.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3296 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/versions.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/tests/api/mgmt/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11442 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/instances.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5013 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/storage.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6805 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/mgmt/datastore_versions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12202 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/mgmt/malformed_json.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8797 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/mgmt/hosts.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8112 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/accounts.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7801 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/mgmt/instances_actions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3393 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/admin_required.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8091 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/configurations.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6989 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/mgmt/quotas.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    40656 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/configurations.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10548 2019-05-10 16:30:49.000000 trove-8.0.1/trove/tests/api/instances_resize.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10898 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/flavors.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7709 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/datastores.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5514 2019-05-10 16:30:46.000000 trove-8.0.1/trove/tests/api/limits.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4945 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/pastedeploy.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    23105 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/apischema.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1827 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/profile.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1357 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/views.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    18477 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/exception.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4776 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/template.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    12210 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2424 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/crypto_utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    23604 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/wsgi.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1891 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/trove_remote.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2613 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/timeutils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1187 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/serializable_notification.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/rpc/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/rpc/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3269 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/rpc/service.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      856 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/rpc/version.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1990 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/rpc/secure_serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2810 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/rpc/conductor_host_serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2633 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/rpc/serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2026 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/rpc/conductor_guest_serializer.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3283 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/base_exception.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4071 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/single_tenant_remote.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1677 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/local.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4178 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3820 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/instance.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2896 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/xmlutils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    23489 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/notification.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5357 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/pagination.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/db/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/db/mongodb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/mongodb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5205 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/db/mongodb/models.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14423 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/db/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/db/couchdb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/couchdb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1033 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/couchdb/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/db/postgresql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/postgresql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1996 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/postgresql/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/db/cassandra/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/cassandra/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1376 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/db/cassandra/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/db/mysql/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/db/mysql/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    10180 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/db/mysql/data.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5494 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/db/mysql/models.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/schemas/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3585 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/schemas/atom-link.rng
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/schemas/v1.1/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      979 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/schemas/v1.1/limits.rng
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    15522 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/schemas/atom.rng
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     6641 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/remote.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1460 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/i18n.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    86697 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/cfg.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14537 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1940 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/glance_remote.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    20760 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/extensions.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3517 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/server_group.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5577 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/debug_utils.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9227 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/policy.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3390 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/auth.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2516 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/configurations.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/storage/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)      958 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/storage/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/storage/experimental/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/storage/experimental/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    11301 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/storage/swift.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1530 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/storage/base.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     2017 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/strategy.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/__init__.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    17372 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    28389 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5285 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/guestagent.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14665 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8793 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3390 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/guestagent.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     9270 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8964 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3474 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/guestagent.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    14714 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     8339 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     4758 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/guestagent.py
+drwxrwxr-x   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:32:41.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/redis/
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)        0 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/redis/__init__.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     5863 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/redis/taskmanager.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     7706 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/redis/api.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3104 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/strategies/cluster/experimental/redis/guestagent.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1392 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/base.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     1619 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/strategies/cluster/strategy.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    27795 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/base_wsgi.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)     3494 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/context.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    13572 2019-05-10 16:30:46.000000 trove-8.0.1/trove/common/limits.py
+-rw-rw-rw-   0 zuul      (1000) zuul      (1000)    16965 2019-05-10 16:30:49.000000 trove-8.0.1/trove/common/stream_codecs.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive
+POSIX tar archive (GNU)
```

### Comparing `trove-21.0.0.0rc2/.coveragerc` & `trove-8.0.1/.coveragerc`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # .coveragerc to control coverage.py
 [run]
 branch = True
 
 source=trove
-omit=trove/tests/*
+omit=*trove/tests*
 
 [report]
 # Regexes for lines to exclude from consideration
 exclude_lines =
     # Have to re-enable the standard pragma
     pragma: no cover
```

### Comparing `trove-21.0.0.0rc2/AUTHORS` & `trove-8.0.1/AUTHORS`

 * *Files 17% similar despite different names*

```diff
@@ -1,130 +1,102 @@
 Aaron Crickenberger <aaron.crickenberger@hp.com>
 Abitha Palaniappan <abitha.palaniappan@hp.com>
 Adam Gandelman <adamg@ubuntu.com>
 Alex Tomic <atomic@tesora.com>
-Alexander Epaneshnikov <aepaneshnikov@sardinasystems.com>
 Alexander Ignatov <aignatov@mirantis.com>
 Ali Adil <aadil@tesora.com>
 Amrith Kumar <amrith.kumar@gmail.com>
 Amrith Kumar <amrith@amrith.org>
 Amrith Kumar <amrith@tesora.com>
 Andreas Jaeger <aj@suse.com>
 Andreas Jaeger <aj@suse.de>
-Andrew Bogott <abogott@wikimedia.org>
 Andrew Bramley <abramley@Bram-Mac.local>
 Andrey Shestakov <ashestakov@mirantis.com>
-Andy Botting <andy@andybotting.com>
 Anh Tran <anhtt@vn.fujitsu.com>
 Anna Philips <ms.annaphilips@gmail.com>
 Anna Shen <ruiyuan.shen@hp.com>
 Ashleigh Farnham <ashleighfarnham@gmail.com>
 Attila Fazekas <afazekas@redhat.com>
 Auston McReynolds <amcreynolds@ebaysf.com>
-Bartosz Zurkowski <b.zurkowski@samsung.com>
 Bertrand Lallau <bertrand.lallau@gmail.com>
 Bertrand Lallau <bertrand.lallau@thalesgroup.com>
 Bin Zhou <zhou.bin9@zte.com.cn>
-Bo Tran <botv@vccloud.vn>
-Bo Tran <ministry.96.nd@gmail.com>
-Bo Tran <yeuconay.caconaynua.96.nd@gmail.com>
-Bo Tran Van <ministry.96.nd@gmail.com>
 Bo Wang <bo.wang@easystack.cn>
 Boden R <bodenvmw@gmail.com>
 Brandon Irizarry <iz.brandon@yahoo.com>
 Brian Hunter <bhunter@tesora.com>
 Cao Xuan Hoang <hoangcx@vn.fujitsu.com>
-Carl Perry <caperry@edolnx.net>
-Chandan Kumar <chkumar@redhat.com>
 Chang Bo Guo <guochbo@cn.ibm.com>
 ChangBo Guo(gcb) <eric.guo@easystack.cn>
 Chaozhe.Chen <chaozhe.chen@easystack.cn>
 Christian Berendt <berendt@b1-systems.de>
 Clark Boylan <clark.boylan@gmail.com>
 Colleen Murphy <colleen.murphy@suse.com>
 Conrad Weidenkeller <conrad@weidenkeller.com>
 Corey Bryant <corey.bryant@canonical.com>
 Craig <cp16net@gmail.com>
 Craig Vyvial <cp16net@gmail.com>
 Cyril Roelandt <cyril.roelandt@enovance.com>
 DJ Johnstone <dj.johnstone@rackspace.com>
-Dai Dang Van <daidv@vn.fujitsu.com>
 Dan Nguyen <dan.nguyens.mail@gmail.com>
-Daniel Krysiak <d.krysiak@samsung.com>
 Daniel Mellado <dmellado@redhat.com>
 Daniel Salinas <imsplitbit@gmail.com>
-Dao Cong Tien <tiendc@vn.fujitsu.com>
-Dariusz Krol <d.krol@samsung.com>
 David Fecker <david.fecker@rackspace.com>
 David Rabel <rabel@b1-systems.de>
 David Sariel <dsariel@redhat.com>
 Debasish Chowdhury <debasish.chowdhury@globallogic.com>
 DeepaJon <deepak.kumar@nectechnologies.in>
 Denis M <dmakogon@mirantis.com>
 Denis Makogon <dmakogon@mirantis.com>
 Denys Makogon <Denys@gigaspaces.com>
 Dina Belova <dbelova@mirantis.com>
 Dirk Mueller <dirk@dmllr.de>
-Dmitriy Rabotyagov <dmitriy.rabotyagov@citynetwork.eu>
 Dmitriy Ukhlov <dukhlov@mirantis.com>
 Dong Ma <winterma.dong@gmail.com>
 Doug Hellmann <doug@doughellmann.com>
 Doug Shelley <doug@parelastic.com>
 Dror Kagan <dror.kagan@hp.com>
 Duk Loi <duk@tesora.com>
-Dylan McCulloch <dylan.mcculloch@gmail.com>
 Ed Cranford <ed.cranford@rackspace.com>
 Edmond Kotowski <ekotowski@gmail.com>
 Edu Alcaniz <ealcaniz@redhat.com>
+Elod Illes <elod.illes@ericsson.com>
 Emilien Macchi <emilien@redhat.com>
 Erik Olof Gunnar Andersson <eandersson@blizzard.com>
 Erik Redding <erik.redding@rackspace.com>
-Fan Zhang <zh.f@outlook.com>
 Felipe Reyes <freyes@suse.com>
 Felipe Reyes <freyes@tty.cl>
 Flavio Percoco <flaper87@gmail.com>
-Gabriel Adrian Samfira <gsamfira@cloudbasesolutions.com>
-Gaetan Trellu <gaetan.trellu@incloudus.com>
 Gauvain Pocentek <gauvain@pocentek.net>
-Gaëtan Trellu <gaetan.trellu@incloudus.com>
 George Peristerakis <george.peristerakis@enovance.com>
-Ghanshyam Mann <gmann@ghanshyammann.com>
 Graham Hayes <graham.hayes@hpe.com>
 Greg Hill <greg.hill@rackspace.com>
 Greg Lucas <glucas@tesora.com>
 Greg Retkowski <greg@rage.net>
 Gábor Antal <antal@inf.u-szeged.hu>
-Ha Minh Cong <conghm1@viettel.com.vn>
 Ha Van Tu <tuhv@vn.fujitsu.com>
 Haomai Wang <haomai@unitedstack.com>
 He Yongli <yongli.he@intel.com>
-Hervé Beraud <hberaud@redhat.com>
-Hirotaka Wakabayashi <hiwkby@yahoo.com>
 Huan Zhang <zhanghuan@chinac.com>
 Ian Wienand <iwienand@redhat.com>
 Iccha Sethi <iccha.sethi@rackspace.com>
 Ihar Hrachyshka <ihrachys@redhat.com>
 Illia Khudoshyn <ikhudoshyn@mirantis.com>
 Ilya Sviridov <isviridov@mirantis.com>
 Ionuț Arțăriși <iartarisi@suse.cz>
 Ionuț Arțăriși <ionut@artarisi.eu>
 Ishita Mandhan <imandha@us.ibm.com>
 Iswarya_Vakati <v.iswarya@nectechnologies.in>
-Ivan Kolodyazhny <e0ne@e0ne.info>
 J Daniel Ritchie <dan.ritchie@hp.com>
-Jacek Kaniuk <j.kaniuk@samsung.com>
 James E. Blair <jeblair@hp.com>
 James E. Blair <jeblair@openstack.org>
-James E. Blair <jeblair@redhat.com>
 James Page <james.page@ubuntu.com>
 Jamie Lennox <jamielennox@redhat.com>
 Jared Rohe <jared.rohe@hp.com>
-Javier Castillo Alcíbar <javier.castillo.alcibar@gmail.com>
 Jeremy Stanley <fungi@yuggoth.org>
-Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
 Jian Xu <jianxuz@ebaysf.com>
 Joe Cruz <jcruz7@gmail.com>
 Joe Cruz <joe.cruz@RACKSPACE.COM>
 Joe Cruz <joe.cruz@rackspace.com>
 Joe Gordon <joe.gordon0@gmail.com>
 Josh Dorothy <josh.dorothy@hp.com>
 Joshua Harlow <harlowja@yahoo-inc.com>
@@ -132,58 +104,44 @@
 Julien Danjou <julien@danjou.info>
 Julien Vey <julien.vey@numergy.com>
 KATO Tomoyuki <kato.tomoyuki@jp.fujitsu.com>
 KIYOHIRO ADACHI <adachi@mxs.nes.nec.co.jp>
 Kaleb Pomeroy <kaleb.pomeroy@rackspace.com>
 Kamil Rykowski <kamil.rykowski@intel.com>
 Kapil Saxena <batcheet@gmail.com>
-Kasper Hasior <k.hasior@samsung.com>
-Katarina Strenkova <kstrenko@redhat.com>
 Kenneth Wilke <kenneth.wilke@rackspace.com>
 Kevin Conway <kevinjacobconway@gmail.com>
 Khyati Sheth <khysheth@ebaysf.com>
 Kim Jensen <kimberly.jensen@twcable.com>
-Krzysztof Opasiak <k.opasiak@samsung.com>
-Lance Bragstad <lbragstad@gmail.com>
 Laurel Michaels <lmichaels@parelastic.com>
 Li Ma <skywalker.nick@gmail.com>
-Lingxian Kong <anlin.kong@gmail.com>
 LiuNanke <nanke.liu@easystack.cn>
 LiuYang <yippeetry@gmail.com>
 Luigi Toscano <ltoscano@redhat.com>
-Luke Browning <lukebrowning@us.ibm.com>
 Luong Anh Tuan <tuanla@vn.fujitsu.com>
-Manoj Kumar <kumarmn@us.ibm.com>
-Marcin Piwowarczyk <m.piwowarczy@samsung.com>
 Mariam John <mariamj@us.ibm.com>
 Mark Biciunas <mbiciunas@tesora.com>
-Mark Kirkwood <mark.kirkwood@catalyst.net.nz>
 Mark McLoughlin <markmc@redhat.com>
 Martin Kletzander <mkletzan@redhat.com>
 Masaki Matsushita <glass.saga@gmail.com>
 Mat Lowery <mlowery@ebaysf.com>
 Matt Fischer <matt@mattfischer.com>
 Matt Riedemann <mriedem.os@gmail.com>
 Matt Riedemann <mriedem@us.ibm.com>
 Matt Van Dijk <mvandijk@tesora.com>
 Matthew Treinish <mtreinish@kortar.org>
 Mayuri Ganguly <mayuri.ganguly@hp.com>
 Michael Basnight <mbasnight@gmail.com>
 Michael Krotscheck <krotscheck@gmail.com>
 Michael Still <mikal@stillhq.com>
 Michael Yu <michayu@ebaysf.com>
-Minmin Ren <renmm6@chinaunicom.cn>
 Monty Taylor <mordred@inaugust.com>
 Morgan Jones <morgan@parelastic.com>
 Morgan Jones <morgan@tesora.com>
-Nguyen Hai Truong <truongnh@vn.fujitsu.com>
 Nguyen Hung Phuong <phuongnh@vn.fujitsu.com>
-Nguyen Thanh Cong <ntcong1705@gmail.com>
-Nguyen Van Trung <trungnv@vn.fujitsu.com>
-Nguyen Van Trung <trungnvfet@outlook.com>
 Nikhil <SlickNik@gmail.com>
 Nikhil Manchanda <SlickNik@gmail.com>
 Nilakhya Chatterjee <email: nilakhya.chatterjee@globallogic.com>
 Nirav Shah <nshah@tesora.com>
 Nirmal Ranganathan <rnirmal@gmail.com>
 OTSUKA, Yuanying <yuanying@fraction.jp>
 OctopusZhang <zhang.yufei@99cloud.net>
@@ -194,193 +152,138 @@
 Paul Lodronio <paul.lodronio@rackspace.com>
 Paul Marshall <paul.marshall@colorado.edu>
 Paul Marshall <paul.marshall@rackspace.com>
 Peter MacKinnon <pmackinn@redhat.com>
 Peter Stachowski <peter@tesora.com>
 Petr Malik <pmalik@tesora.com>
 Petra Sargent <petra_sargent@yahoo.com>
-Phung HoangLong <longph2710@gmail.com>
 Pierre Blanc <pierre.blanc@ormuco.com>
 Pierre RAMBAUD <pierre.rambaud@numergy.com>
 Pradeep Chandani <pradeep.chandani@globallogic.com>
-Przemysław Godek <p.godek@partner.samsung.com>
-Qian Min Chen <chen.qiaomin@99cloud.net>
 Ramashri Umale <rumale@ebaysf.com>
-Riccardo Pittau <rpittau@blizzard.com>
 Riddhi Shah <ridhi.j.shah@gmail.com>
 Riley Bastrop <rileynortonbastrop@gmail.com>
 Robert Myers <robert.myers@rackspace.com>
-Rocky <shi.yan@unimelb.edu.au>
 Ronald Bradford <ronald.bradford@gmail.com>
 Russell Bryant <rbryant@redhat.com>
 Sam Morrison <sorrison@gmail.com>
 Samuel Matzek <smatzek@us.ibm.com>
 Satoshi Mitani <samitani@yahoo-corp.jp>
 Saurabh Surana <saurabh.surana@hp.com>
 Sean McCully <sean_mccully@yahoo.com>
-Sean McGinnis <sean.mcginnis@gmail.com>
 Sebastien Badia <sebastien.badia@enovance.com>
 Sergey Gotliv <sgotliv@redhat.com>
 Sergey Lukjanov <slukjanov@mirantis.com>
 Sergey Vilgelm <sergey@vilgelm.info>
 Shaik Apsar <sa709c@intl.att.com>
 Shuichiro MAKIGAKI <shuichiro.makigaki@gmail.com>
 Shuquan Huang <huang.shuquan@99cloud.net>
 Simon Chang <schang@tesora.com>
 Sonali Goyal <sonaligoyal654321@gmail.com>
 Sreedhar Chidambaram <sreedhara.chidambaram@blackboard.com>
-Stephen Ma <sma@suse.com>
 Steve Leon <kokhang@gmail.com>
 Steve Leon <steve.leon@hp.com>
 Sudarshan Acharya <sudarshan.acharya@rackspace.com>
 Sushil Kumar <skm.net@gmail.com>
 Sushil Kumar <sushil.kumar2@globallogic.com>
 Sushil Kumar <sushil.kumar3@hp.com>
 SushilKM <skm.net@gmail.com>
 Swapnil Kulkarni (coolsvap) <me@coolsvap.net>
 Swapnil Kulkarni <me@coolsvap.net>
-Takashi Kajinami <kajinamit@oss.nttdata.com>
-Takashi Kajinami <tkajinam@redhat.com>
 Takashi NATSUME <natsume.takashi@lab.ntt.co.jp>
-Takashi Natsume <takanattie@gmail.com>
 Takeaki Matsumoto <takeaki.matsumoto@ntt.com>
 Tanis De Luna <tanis.deluna@rackspace.com>
 Telles Nobrega <tenobreg@redhat.com>
 Theron Voran <theron.voran@rackspace.com>
 Thierry Carrez <thierry@openstack.org>
 Thomas Bechtold <tbechtold@suse.com>
-Thomas Goirand <zigo@debian.org>
 Tim Simpson <tim.simpson@rackspace.com>
 Timothy He <the@ebaysf.com>
-Tobias Urdin <tobias.urdin@binero.com>
-Tobias Urdin <tobias.urdin@binero.se>
-Tomasz Nowak <tnowak@mirantis.com>
 Tony Xu <hhktony@gmail.com>
-Tovin Seven <vinhnt@vn.fujitsu.com>
 Trevor McCasland <TM2086@att.com>
 Trevor McCasland <Trevor McCasland>
 Tristan Cacqueray <tristan.cacqueray@enovance.com>
-Valencia Serrao <vserrao@us.ibm.com>
 Victor Stinner <vstinner@redhat.com>
 Victoria Martinez de la Cruz <victoria@redhat.com>
 Victoria Martinez de la Cruz <victoria@vmartinezdelacruz.com>
 Vincent Untz <vuntz@suse.com>
 Vipul Sabhaya <vipuls@gmail.com>
 Viswa Vutharkar <vpvutharkar@ebaysf.com>
-Vu Cong Tuan <tuanvc@vn.fujitsu.com>
 XiaBing Yao <yao.xiabing@99cloud.net>
-XiaojueGuan <guanalbertjone@gmail.com>
-Yang Youseok <ileixe@gmail.com>
 YuYang <yuyangwang1985@gmail.com>
-Zhangfei Gao <zhangfei.gao@linaro.org>
-Zhao Chao <zhaochao1984@gmail.com>
 Zhenguo Niu <Niu.ZGlinux@gmail.com>
 Zhenguo Niu <niuzhenguo@huawei.com>
 Zhi Yan Liu <zhiyanl@cn.ibm.com>
 ZhiQiang Fan <aji.zqfan@gmail.com>
-ZhongShengping <chdzsp@163.com>
 Zhongyue Luo <zhongyue.nah@intel.com>
 abhishekkekane <abhishek.kekane@nttdata.com>
 alex <wujian3659@163.com>
 amcrn <amcreynolds@ebaysf.com>
 baz <baz@ubuntu.(none)>
 bhagyashris <bhagyashri.shewale@nttdata.com>
 boden <boden@linux.vnet.ibm.com>
-brandonzhao <zhaolong@inspur.com>
-caoyuan <cao.yuan@99cloud.net>
 caoyue <yue.cao@easystack.cn>
 chenaidong1 <chen.aidong@zte.com.cn>
 chengyang <374519141@qq.com>
-chengyang <chengyang@cloudin.cn>
 chenshujuan <chenshujuan@chinac.com>
-chenxiangui <chenxiangui@inspur.com>
-chenxing <chason.chan@foxmail.com>
-chimeng <chisimon98k@gmail.com>
 dagnello <davide_agnello@hotmail.com>
 dangming <dangming@unitedstack.com>
 daniel-a-nguyen <dan.nguyens.mail@gmail.com>
 daniel-a-nguyen <daniel.a.nguyen@hp.com>
 debasish <debasish.chowdhury@globallogic.com>
-deepakmourya <deepak.mourya@nectechnologies.in>
 dineshbhor <dinesh.bhor@nttdata.com>
 ekotowski <ekotowski@gmail.com>
-ericxiett <eric_xiett@163.com>
 ewilson-tesora <ewilson@tesora.com>
 gecong1973 <ge.cong@zte.com.cn>
 geng chc <578043796@qq.com>
 gengchc2 <geng.changcai2@zte.com.cn>
 ghanshyam <ghanshyam.mann@nectechnologies.in>
-ghanshyam <ghanshyammann@gmail.com>
-ghanshyam <gmann@ghanshyammann.com>
 guang-yee <guang.yee@hp.com>
 hardy.jung <hardy.jung@kakaocorp.com>
 himani <himani.relan@nectechnologies.in>
 jcannava <jason@cannavale.com>
-jeremy.zhang <zhangjun_inspur@163.com>
 jiansong <jian.song@easystack.cn>
 justin-hopper <justin.hopper@hp.com>
-lijunjie <lijunjie@cloudin.cn>
-likui <likui@yovole.com>
 liuqing <970717493@qq.com>
 liuqing <jing.liuqing@99cloud.net>
-ljhuang <huang.liujie@99cloud.net>
 lvdongbing <dongbing.lv@kylin-cloud.com>
 mariam john <mariamj@us.ibm.com>
 mariamj <mariamj@us.ibm.com>
 mariamj@us.ibm.com <mariamj@us.ibm.com>
 mbasnight <mbasnight@gmail.com>
-melissaml <ma.lei@99cloud.net>
 ming dang <743759846@qq.com>
 nbziouech <nourhene.bziouech@supcom.tn>
-npraveen35 <npraveen35@gmail.com>
 pangliye <pangliye@inspur.com>
 pawnesh.kumar <pawnesh.kumar@nectechnologies.in>
 pradeep <pradeep.chandani@globallogic.com>
-rajat29 <rajat.sharma@nectechnologies.in>
 rameshsahu <ramesh.sahu@gmail.com>
 rico.lin <rico.lin.guanyu@gmail.com>
 ricolin <rico.lin@easystack.cn>
-ricolin <ricolin@ricolky.com>
 ridhi.j.shah@gmail.com <ridhi.j.shah@gmail.com>
 ruiyuan-shen <ruiyuan.shen@hp.com>
 rumale <rumale@ebay.com>
 rumale <rumale@ebaysf.com>
 rvemula <rvemula@ebaysf.com>
 sandrely26 <sandrely2611@gmail.com>
 saradpatel <sarad.patel@hp.com>
 sarvesh-ranjan <saranjan@cisco.com>
 shalini khandelwal <shalini.khandelwal@globallogic.com>
-shangxiaobj <shangxiaobj@inspur.com>
 sharika <sharika.pongubala@hp.com>
 shashank-gl <shashank.gupta1@globallogic.com>
 shayne-burgess <shayne.burgess@hp.com>
 stewie925 <st6218@att.com>
 svenkataramanaia <svenkataramanaia@ebay.com>
 svenkataramanaia <svenkataramanaia@ebaysf.com>
 tanlin <lin.tan@intel.com>
-taoguo <guotao.bj@inspur.com>
 tbbrave <detong.chang@foxmail.com>
 tianqing <jazeltq@163.com>
 ting.wang <ting.wang@easystack.cn>
 venkatamahesh <Venkata.Kotha@infinite.com>
 venkatamahesh <venkatamaheshkotha@gmail.com>
-wangjun <wangjun@yovole.com>
 wangjunqing <junqing.wang@8lab.cn>
-wangqi <wang.qi@99cloud.net>
 wangyao <wangyao@cmss.chinamobile.com>
-wangzihao <wangzihao@yovole.com>
-weiyj <weiyuanjun@inspur.com>
-whoami-rajat <rajatdhasmana@gmail.com>
-wu.chunyang <wchy1001@gmail.com>
-wu.chunyang <wuchunyang@yovole.com>
-wuchunyang <wuchunyang@yovole.com>
 xhzhf <guoyongxhzhf@163.com>
 yangyapeng <yang.yapeng@99cloud.net>
-yangyong <yangyonglc@inspur.com>
-yatin <ykarel@redhat.com>
 yfzhao <zhaoyunfeng@fiberhome.com>
 yuhui_inspur <yuhui@inspur.com>
-zhangboye <zhangboye@inspur.com>
-zhangdebo <zhangdebo@inspur.com>
 zhanggang <zhanggang@cmss.chinamobile.com>
 zhangyanxian <zhang.yanxian@zte.com.cn>
-zhufl <zhu.fanglei@zte.com.cn>
```

### Comparing `trove-21.0.0.0rc2/CONTRIBUTING.rst` & `trove-8.0.1/CONTRIBUTING.rst`

 * *Files 4% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 ============
 Contributing
 ============
 
 Our community welcomes all people interested in open source cloud
 computing, and encourages you to join the `OpenStack Foundation
-<https://www.openstack.org/join>`_.
+<http://www.openstack.org/join>`_.
 
 If you would like to contribute to the development of OpenStack,
 you must follow the steps documented at:
 
-   https://docs.openstack.org/infra/manual/developers.html#development-workflow
+   http://docs.openstack.org/infra/manual/developers.html#development-workflow
 
 Once those steps have been completed, changes to OpenStack
 should be submitted for review via the Gerrit tool, following
 the workflow documented at:
 
-   https://docs.openstack.org/infra/manual/developers.html#development-workflow
+   http://docs.openstack.org/infra/manual/developers.html#development-workflow
 
 (Pull requests submitted through GitHub will be ignored.)
 
-Bugs should be filed on Storyboard now, not GitHub:
+Bugs should be filed on Launchpad, not GitHub:
 
-   https://storyboard.openstack.org/#!/project/openstack/trove
+   https://bugs.launchpad.net/trove
 
 We welcome all types of contributions, from blueprint designs to
 documentation to testing to deployment scripts. The best way to get
 involved with the community is to talk with others online or at a
 meetup and offer contributions through our processes, the `OpenStack
-wiki <https://wiki.openstack.org>`_, blogs, or on IRC at
-``#openstack-trove`` on OFTC.
+wiki <http://wiki.openstack.org>`_, blogs, or on IRC at
+``#openstack-trove`` on ``irc.freenode.net``.
 
 
 House Rules
 ===========
 
 Code Reviews
 ------------
@@ -81,19 +81,19 @@
    - The code style guidelines accepted by the project are part of
      tox.ini, a violation of some other hacking rule(s), or pep8 is
      not a reason to -1 a change.
 
 Other references:
 
    - https://wiki.openstack.org/wiki/CodeReviewGuidelines
-   - https://docs.openstack.org/infra/manual/developers.html
+   - http://docs.openstack.org/infra/manual/developers.html
    - https://wiki.openstack.org/wiki/ReviewChecklist
    - https://wiki.openstack.org/wiki/GitCommitMessages
-   - https://docs.openstack.org/hacking/latest/
-   - https://review.opendev.org/#/c/116176/
+   - http://docs.openstack.org/developer/hacking/
+   - https://review.openstack.org/#/c/116176/
    - trove-pylint readme file in tools/trove-pylint.README
 
 Code Review Priority
 --------------------
 
 At the design summit in Barcelona (October 2016) we discussed code
 review priority. We have a significant number of priorities for what
@@ -167,37 +167,37 @@
 
 3. In general, changes will be proposed for abandonment if the change
    being proposed has either been addressed in some other patch set,
    or if the patch is not being actively maintained by the author and
    there is no available volunteer who will step up to take over the
    patch set.
 
-Storyboard Bugs
----------------
+Launchpad Bugs
+--------------
 
-Bugs should be filed on Storyboard at:
+Bugs should be filed on Launchpad at:
 
-    https://storyboard.openstack.org/#!/project/openstack/trove
+    https://bugs.launchpad.net/trove
 
-All changes that address a Storyboard bug should include the bug in the
-Commit Message using the Story and Task.
+All changes that address a Launchpad bug should include the bug in the
+Commit Message using the Closes-Bug, Related-Bug, or Partial-Bug keyword.
 
-It is not required that a Storyboard bug be filed for every change.
+It is not required that a Launchpad bug be filed for every change.
 
 Release Notes
 -------------
 
 All user visible changes should include a release note. Trove uses
 reno to generate release notes and therefore only those release notes
 that are submitted as part of a change will be included in the release
 notes. The failure to add a release note for a user visible change
 should be identified in review, and corrected.
 
-If a Storyboard bug is being fixed, the release note should list the
-story and task number.
+If a Launchpad bug is being fixed, the release note should list the
+bug number.
 
 For help using reno, the release notes tool, see:
 
     https://wiki.openstack.org/wiki/Trove/create-release-notes-with-reno
 
 Trove Documentation
 ===================
@@ -254,15 +254,16 @@
 
 Before committing code to Gerrit for review, please at least do the
 following on your development system and ensure that they pass.
 
 .. code-block:: bash
 
     $ tox -e pep8
-    $ tox -e py39
+    $ tox -e py27
+    $ tox -e py34
     $ tox -e pylint
 
 If you are unable to get these to pass locally, it is a waste of the
 CI resources to push up a change for review.
 
 
 Testing
@@ -289,19 +290,19 @@
 
 To run all tests and PEP8, run tox, like so:
 
 .. code-block:: bash
 
     $ tox
 
-To run just the tests for Python 3.9, run:
+To run just the tests for Python 2.7, run:
 
 .. code-block:: bash
 
-    $ tox -epy39
+    $ tox -epy27
 
 To run just PEP8, run:
 
 .. code-block:: bash
 
     $ tox -epep8
 
@@ -318,14 +319,7 @@
 .. code-block:: bash
 
     $ python -m testtools.run trove.tests.unittests.python.module.path
 
 Note that some unit tests can use an existing database. The script
 ``tools/test-setup.sh`` sets up the database for CI jobs and can be
 used for local setup.
-
-Is there something missing?
----------------------------
-
-Do not hesitate to chat and clear your doubts about Trove in
-IRC: #openstack-trove on OFTC.
-
```

### Comparing `trove-21.0.0.0rc2/ChangeLog` & `trove-8.0.1/ChangeLog`

 * *Files 13% similar despite different names*

```diff
@@ -1,871 +1,45 @@
 CHANGES
 =======
 
-21.0.0.0rc2
------------
-
-* db: add commit action after querying
-* Retire TripleO: remove the TripleO jobs/ref
-* Update TOX\_CONSTRAINTS\_FILE for stable/2024.1
-* Update .gitreview for stable/2024.1
-
-21.0.0.0rc1
------------
-
-* db: Remove use of autocommit
-* reno: Update master for unmaintained/xena
-* reno: Update master for unmaintained/wallaby
-* reno: Update master for unmaintained/victoria
-* Fix missing oslo.db options
-* Enable healthcheck middleware
-* Imported Translations from Zanata
-* Set swift\_api\_insecure to false in trove-guest
-* reno: Update master for unmaintained/yoga
-* Remove global default\_password\_length
-* Change postgresql socket path to a persistent path
-* Fixes the DIB script
-* Clean up requirements for backup script
-* bandit: Install baseline extra
-* The backup\_aes\_cbc\_key option should be secret
-* Updates guest-agent image issues
-* [CI] fix coverage name in git ignore
-* Change mysql socket path to a persistent path
-* fix: prepare mariadb after restore
-* Update python classifier in setup.cfg
-* Fix potential network conflict
-* Disables shell=True in backup and restore
-* [CI] setting backup&replication voting to false
-* [CI] Allow to perform insecure SSL requests to swift
-* Cinder Support For Boot Volume
-* [CI] Fix Ci failure
-* Revert "Fix docker daemon failed to set IPv6 gateway"
-* Don't configure ip route for slaac ipv6 subnet
-* Fix guest agent failed to use swiftclient over TLS
-* Fix docker daemon failed to set IPv6 gateway
-* Remove unnecessary ceilometer service overrides
-* [CI]Add postgresql jobs in zuul
-* [CI] Add mariadb10.4 jobs in zuul
-* [CI] Improve CI tests for mysql
-* Add region name to trove-guestagent
-* follow up: 3bbeeb87e074016db45e6df397b0b379210a5549
-* [CI]: test mysql 5.7 version instead of 5.7.29
-* Fixes the way to check db instance status
-* [CI] fix openstack-tox-py311 job
-* Separate backup docker image for each database version
-* Allows users to specify a MariaDB version in Dockerfile
-* Imported Translations from Zanata
-* Update master for stable/2023.2
-
-20.0.0
-------
-
-* Fix bindep.txt for python 3.11 job(Debian Bookworm)
-* Fix Zuul warnings with regexps
-* [CI]: Add support for Ubuntu Jammy
-* Fix typo in release note
-* Fix error create backup in PostgreSQL
-* Fix typo
-* Fix: failed to create the replicas
-* [CI]: fix trove api started failed
-* Imported Translations from Zanata
-* Adds postgres guestagent test driver
-* Add network isolation for trove
-* Use py3 as the default runtime for tox
-* Fix postgresql database creation failures from prepare func
-* Replace	deprecated terms
-* [CI] optimize the image build script
-* Imported Translations from Zanata
-* Remove the idle\_timeout option
-* Remove logics for old Python versions
-* [CI] fix devstack install failed
-* Update backup requirements
-* [CI]: fix the tox cover zuul jobs
-* Prevent docker from manipulating iptables
-* Build backup images during the devstack installation
-* Fixes cluster creation error
-* mysql: explicitly use utf8mb3
-* [CI]: Don't run periodic jobs on stable branchs
-* Imported Translations from Zanata
-* Add python 3.10 to setup.cfg metadata
-* Enable flake8 E129 rule
-* Fix "create database" failed for postgress
-* CI: fix trove tempest postgresql job
-* Don't ignore E125 check
-* synchronize guest logs to controller node
-* Revert "Add debug log for CI tests"
-* Add debug log for CI tests
-* Don't ignore H306 pep8 check
-* Imported Translations from Zanata
-* Remove tempest ipv6 job to experimental pipeline
-* Fix inspect.getargspec() deprecation warning
-* Test 5.7.29 for mysql datastore
-* Add missing releasenotes
-* Using 5.7.29 tag for mysql image
-* Ignore more files for zuul jobs
-* Do not configure removed [DEFAULT] rpc\_backend
-* Imported Translations from Zanata
-* Imported Translations from Zanata
-* Imported Translations from Zanata
-* Use new get\_rpc\_client API from oslo.messaging
-* Update master for stable/2023.1
-
-19.0.0
-------
-
-* Fix tox cover check
-* Remove unmaintained zuul checks
-* Using local registry in devstack
-* container\_registry\_password should be secret
-* Updates the backup document
-* Fixes permission problem when restoring backup
-* Creates the mysqld extra configuration dir
-* Fixing tests with tox 4.2.6
-* Fix the missing log request-id
-* Follow up: CI: test build docker image
-* Imported Translations from Zanata
-* CI: test build docker image
-* Adds database\_service\_uid to the document
-* Fix deprecation warnings
-* Imported Translations from Zanata
-* add openstack-python3-zed-jobs-arm64 job
-* Change StrictRedis usage to Redis
-* Switch to 2023.1 Python3 unit tests and generic template name
-* [CI] Move queue setting to project level
-* Update master for stable/zed
-
-18.0.0
-------
-
-* Fix illegal shell characters
-* Fix compatibility with oslo.db 12.1.0
-* Rename api\_wsgi.py to app\_wsgi.py
-* Rename app.wsgi to app\_wsgi.py
-* Adapt bindep ubuntu-jammy
-* Fix the missing of guest-agent.conf in guest vm
-* Imported Translations from Zanata
-* Drop bionic support
-* Increase guest image size to 5G
-* Set os release to focal in devstack
-* Fix docker start failed in guest-agent
-* Imported Translations from Zanata
-* Changed py39 from py27 in tox
-* Add a check before executing ip replace
-* Update python testing as per zed cycle testing runtime
-* Fix error when list database instances
-* Ignore vscode files in git
-* Add release note(aka. reno) guide
-* Imported Translations from Zanata
-* Imported Translations from Zanata
-* Adds a configuration to use a local docker registry
-* Uses ML2/OVN as network backend driver
-* Uses userdata instead of personality files
-* Adds new configurations for injected file owner
-* Add python3.9 support
-* Uses glance to get image\_id when creating a cluster
-* Don't check task\_status when creating a cluster
-* Add ubuntu focal support for trove guest image
-* Add image build test on Centos8 stream
-* WIP: add guest image build check
-* Validates "network\_id" param creating a cluster
-* Fixes the wrong instruction
-* Adds docker daemon config to injected files
-* Removes the deprecated argument tenant from TroveContext
-* Stop using deprecated functions in std Python lib
-* Removes the deprecated argument tenant from TroveContext
-* Defines Q\_AGENT openvswitch explicitly
-* Fix import from collections
-* Add Python3 zed unit tests
-* Update master for stable/yoga
-
-17.0.0.0rc1
------------
-
-* Use os-cloud trove creds
-* Adapt to file injection deprecation in nova
-* Improve the detect backup was created when parse log
-* [Doc] Describe the usage of root\_on\_create option
-* Fix trove guest agent logrotate
-* Add Python3 yoga unit tests
-* Update master for stable/xena
-* Make more transparence with variable name from replica snapshot
-
-16.0.0
-------
-
-* Allow regular user to get quotas
-* Doc: Upgrade trove guest agent
-* Run reset master when setting up mysql replicas
-* Replace deprecated import of ABCs from collections
-* Show user network ID for getting instance
-* Use Block Storage API v3 instead of API v2
-* Add periodic task to remove postgres archived wal files
-* Fix getting config option value for database
-* Allow admin user to get instances attached with any configuration
-* Add doc for running unit tests
-* Fix rebuild instances in replication cluster
-* Show all the instances in descending order of creation time
-* Keep user defined configuration after resizing instance
-* Release note for adding iptables-persistent package
-* Add package iptables-persistent inside guest instance
-* Use bridge network for db container
-* Fix backup using customized container image registry
-* Add float types to load\_items to support configuration parameters of float type
-* Support customized database container images in DevStack
-* Enable tls-proxy support in test jobs
-* Fix sqlalchemy engine listener
-* Changed minversion in tox to 3.18.0
-* Wait for volume status before resize\_fs
-* Don't create nova instances with fqdn as their name
-* Doc: move from freenode to OFTC
-* Fix CI for trove Xena
-* instance model: fix extraction of ip addresses from instance record
-* setup.cfg: Replace dashes with underscores
-* Fix periodic public image job
-* setup.cfg: add oslo.policy.enforcer
-* Dropping lower constraints testing
-* Fix typo in zuul config
-* Fix check\_subnet\_router filters
-* Add taskmanager\_manager option to common
-* [doc] Add configuration reference
-* Add CI job trove-tempest-postgres
-* Improve guest agent when start
-* [Doc] Add another method to check guest agent log
-* Fix test\_create\_inactive\_datastore\_by\_admin
-* Imported Translations from Zanata
-* Add Python3 xena unit tests
-* Update master for stable/wallaby
-
-15.0.0.0rc1
------------
-
-* doc: Restore backup from other regions
-* Support to restore backup from remote location
-* Change the default agent\_heartbeat\_expiry to 90s
-* Remove the incorrect log for creating replication cluster
-* Do not override RESTART\_REQUIRED service status
-* Doc: custom container image registry
-* Support custom container registry for database images
-* Stop using pip-and-virtualenv diskimage element
-* update ci job description
-* add python37 setup.cfg
-* remove unicode from code
-* [goal] Deprecate the JSON formatted policy file
-* remove py37
-* update cliff to 3.5.0 support python3.8
-* Disable some tempest tests that always failed in CI
-* Support instance operating\_status
-* Do not rely on description for checking port type
-* Disable openstack-lower-constraints-jobs
-* Update doc8 version
-* Add Neutron extension check before calling some APIs
-* use HTTPStatus instead of http.client
-* Add support for python 3.8
-* Fixup zuul nodetype for nested
-* Use nested virt node for trove functional test
-* Fix the guest service name in troubleshooting guide
-* Remove the experimental claim of dev\_mode false
-* Improve trove guest image build script
-* Support --image-tags for trove-manager datastore\_version\_update command
-* Remove use of removed tail\_log command
-* [docs]Fix wrong links in README
-* [Doc] Support datastore version number
-* Support datastore version number for configuration
-* Get slave\_pos to choose latest replica
-* Use current slave\_pos of slave to continue replicate
-* [API Doc] Support datastore version number
-* convert to type str to compare
-* Support datastore version number for creating configuration
-* Support datastore version number for creating instance
-* Update datastore version name
-* No validate when perform eject replica source
-* Add 'version' to datastore version
-* Fix the race condition for creating and deleting instance
-* Do not check server status when waiting for instance removed
-* Support ram quota
-* Fix deleting volume for instance
-* Pass availability zone through to volume creation
-* Fix upgrading instance datastore version
-* Fix edit instance action return code
-* Fix the functional test related to flavor check
-* Make guest agent api changes backwards compatible
-* Revert "Remove flavor API"
-* Use datastore manager as opposed to name to restrict actions
-* Add doc for running functional test
-* Update TOX\_CONSTRAINTS\_FILE
-* Fix restore encrypted backup
-* Return instances by created order
-* Release note for mysql 8 support
-* Support mysql 8.0
-* Fix getting user port for instance
-* Update quota API description
-* Support to get instances of a specified project
-* Remove the unused coding style modules
-* [TrivialFix] Fix '--replica-of' argument format in docs
-* Remove six usage and  basestring check
-* [Doc] datastore guide
-* Fix error handling on instance create
-* fix typo in docs
-* Add resource quota description for production guide
-* Create floating IP in the user's project
-* Fix the image CI job
-* Remove six.moves
-* bump py37 to py38 in tox.ini
-* Support database version image tags for creating instance
-* Image tags support in datastore version
-* Set default-authentication-plugin default for mysql
-* Warning user running command trove-manage db\_recreate
-* Update OS Ubuntu and fix create configuration
-* abbreviation error correction
-* Remove mysql query cache settings
-* Add user/project/trove-id metadata to trove server instances
-* Only confirm resize when nova instance status is VERIFY\_RESIZE
-* Handle case where no networks are selected at create
-* [Postgresql] Adding config parameters
-* Remove six.moves.urllib
-* Add Python3 wallaby unit tests
-* Update master for stable/victoria
-
-14.0.0.0rc1
------------
-
-* Set limit to -1 when getting Nova instances
-* Set default limit for novaclient
-* Install pip3 for image build
-* [Doc] More description for nics for creating instance
-* [Doc] Improve building Trove guest image instruction
-* [Backup] Ignore 404 error when deleting swift objects
-* Change access column type for instance
-* Add project\_id to backup response
-* Support getting backups of a specific project
-* [Doc] Refine the root enable API description
-* Only enable user and database API for MySQL
-* Fix some API calls for PostgreSQL
-* [Postgresql] Create replica
-* [API doc] Add replica\_of param for getting instance
-* Postgresql: Backup and restore
-* Use sudo when deleting image cache dir
-* Remove six.PY3
-* Use pip3 instead of pip in trovestack
-* Improve docker image for database backup/restore
-* Remove the definition of trove-grenade
-* [API doc] Add more description for deleting backups
-* PostgreSQL support
-* [API doc] Improve description of swift\_container for creating backups
-* Support to check if subnet is associated with router
-* Trove upgrade guide
-* Fix access field for instances created before upgrade
-* Allow to specify root password when building guest image
-* Fix functional test for instance access operation
-* Support to update instance access
-* Show instance access information
-* Wait for instance after rebuild
-* Add instruction for upgrading trove guest agent
-* Support to rebuild instance by admin
-* Resize replicas (volume) together with primary
-* Using same config with primary for replicas
-* Fix oslo-config-generator command example in doc
-* Support online volume resize
-* Monkey patch original current\_thread \_active
-* Fix the slow start of mariadb container
-* Support backup strategy API
-* Correct some information in docs
-* Doc: Fix configuration API
-* Update location of file validation-rules.json
-* Support image type for guest image building
-* Use unittest.mock instead of mock
-* Release note for adding replicas in list instance response
-* Show replicas info for listing instances
-* Support subnet and IP for instance creation
-* Switch to newer openstackdocstheme and reno versions
-* Set status to ERROR if heartbeat expires
-* Stop to use the \_\_future\_\_ module
-* Support to remove datastore config parameters
-* Support to generate config sample file
-* Doc: update trove design
-* Add doc for how to run functional test
-* Fix job publish-trove-guest-image-ubuntu-bionic-dev
-* Trove doc and CI
-* Remove translation sections from setup.cfg
-* Fix hacking min version to 3.0.1
-* Update docs upgrade datastore
-* Datastore containerization
-* Cap jsonschema 3.2.0 as the minimal version
-* Fix CI job
-* Add command example for preparing datastore
-* Add py38 package metadata
-* Add troubleshooting guide
-* Remove flavor API
-* Remove volume-types API
-* Use unittest.mock instead of third party mock
-* Support ip address type for instances
-* Add Python3 victoria unit tests
-* Update master for stable/ussuri
-
-13.0.0
-------
-
-* Added checks for deleting datastore version
-* Change @property usage to function
-* Update hacking for Python3
-* Add innodb configuration parameters log\_file\_size and flush\_method
-* Devstack should install the trove-dashboad module by default
-* Release note for XFS disk format support
-* Support XFS disk format
-* Fixes "a2ensite" command arg and adds mod\_wsgi package installation
-* Fix devstack installation guide
-* Improve the doc
-* Fixes the following syntax error of etc/apache2/trove apache conf
-* Delete error volumes when deleting instance
-* Support to test non-dev guest image in CI
-* Add python-troveclient in requirements.txt
-* Small cleanups
-* [Community goal] Add contributor and PTL guide
-* Cleanup docs building
-* Improve the function tests
-* Add 'Quota Management' in production guide
-* Config admin clients as default
-* Add running trove in production guide
-* Fix missing parameter in log message
-* Check network conflict
-* Delete datastore
-* Fix duplicated words issume like "object of of the instance"
-* Fix unmount path for instance upgrade
-* Fix an invalid assertIsNotNone statement
-* Improve API doc
-* Fix trovestack tox job
-* Show service\_status\_updated in instance API
-* Support PostgreSQL 12
-* Fix the log related tests
-* About Trove datastore image
-* Remove some unrelated code
-* Support HEALTHY status for db instance
-* Add quotas resource operation in API doc
-* Fix delete instance
-* Add trove-tempest CI job
-* Fix Trove periodic CI jobs
-* Fix some issues with replicate with mysql
-* Support incremental backup for MariaDB
-* Move the iptable change from trovestack to devstack
-* Mark CI job trove-scenario-mariadb-single voting
-* [ussuri][goal] Drop python 2.7 support and testing
-* Support pip3 and run on guest-agent service for redis
-* Support pip3-virtualenv during image creation
-* Fix mariadb CI - trove-scenario-mariadb-single
-* Add CI job to build Ubuntu Xenial based Trove guest image for dev
-* Build reusable Trove guest image for dev
-* Rename devstack variable TROVE\_DISABLE\_IMAGE\_SETUP
-* Release note for service\_credentials config
-* Improve image building doc
-* Support to specify branch when building the image
-* Use dedicated service credential config
-* Remove all the resources when the Nova VM creation failed
-* Use correct Swift credential for instance backup
-* Remove the annoying debug logs
-* Add branch param for image building
-* Doc: Add public trove guest images info
-* Update master for stable/train
-* Add release note for public images
-* Fix Trove periodic job
-* Fix an error when generate root password during DB initialization
-
-12.0.0.0rc1
------------
+8.0.1
+-----
 
-* Change imag build job to the periodic pipeline
-* Some clean up
-* Add image build experimental CI job
-* Fix Trove CI failure
-* Fix issue with pip2 command and update pip3 for mongodb
-* [doc] Create instance in user guide
-* Release note for public instance
-* API doc: 'access' support for creating trove instance
-* Support to create public trove instance
-* Improve guest image creation in devstack
-* Add periodic logs during database mechanism
-* Fix MariaDB image build
-* fix bug report url
-* Modify CreateInstanceTest setUp to initially be OK
-* Improve image build
-* Support management security group
-* Minor change to image build guide
-* Support keypair in devstack
-* Mark the instance ERROR when Nova VM creation fails
-* Support python3 in guest agent
-* Make volume type optional
-* Release note for backup filtering
-* Filtering description for backup API
-* Improve devmode=flase when building the image
-* Fix all\_projects filtering for backups
-* Fix backup tests
-* Support backup filtering
-* Fix python3 failure inside guest when doing restore
-* [train][goal] Run 'trove-tempest-ipv6-only' job
-* Support new mysql8.0 utf8mb4 character and collation sets
-* Fix 31-fix-init-script for Postgresql
-* Refactor variables in plugin.sh to simplify setting a datastore
-* Add a designate V2 API dns driver
-* Remove invalid assert state
-* Update api-ref location
-* Add Python 3 Train unit tests
-* Fix Trove CI jobs
-* Re-define the 'nics' parameter for creating database
-* Support renamed postgresql log functions
-* Use newer style mysql syntax for users/passwords
-* Fix incorrect use of raise in template test
-* Ignore new 'sys' mysql database by default
-* Update the outdated content
-* trovestack guide
-* Enable service tenant deployment model by default in DevStack
-* Remove flavor operations from API doc
-* Fix the structure in releasenotes folder
-* Remove the trove-tox-apiexamples CI job
-* docs: fix build failure on html\_last\_updated\_fmt
-* Add releasenotes for Redis upgrade
-* Add Redis datastore upgrade
-* Fix tox debug mode
-* Extend cluster events
-* Update Python 3 test runtimes for Train
-* Fix syntax error
-* Instead of deprecated keystone cli in docs
-* Fix error URL
-* Update min tox version to 2.0
-* Dropping the py35 testing
-* Add Cassandra datastore upgrade
-* Skip image building in DevStack for functional tests
-* Changing file owner when upgrading mariadb
-* Move to opendev
-* Use opendev.org instead of git.openstack.org
-* OpenDev Migration Patch
-* Fix cloudinit mariadb scenario test error
-* Pass kwargs to exception to get better format of error message
-* Nova keypair support
-* Improve trove guest agent image building
-* Add error handling when Swift is not installed
-* Disable devstack image building for trove-scenario-mariadb-single CI job
-* Fix mariadb status after upgrade
-* Remove SecurityGroup API extension
-* Add new Galera Cluster bootstraping method
-* Migrate legacy jobs to Ubuntu Bionic
-* Fix tests for Ubuntu Bionic migration of CI jobs
-* Fix poll\_until exception type
-* Fix redis expected parameter type
-* User guide update to use Openstack client syntax
-* add python 3.7 unit test job
-* Deprecate the config option default\_neutron\_networks
-* Fix the way to get localhost IP in devstack
 * Replace openstack.org git:// URLs with https://
-* Update master for stable/stein
-* Share networks created by Trove Devstack plugin
-* Fix Mariadb replication config
-* Add documentation for managing databases and users
-* Additional logs for Mariadb restore strategy
-
-11.0.0
-------
-
-* Fix modules import order
-* Add log retrieval to Redis
-* Skip IP addresses from management networks
-* Fix SUBNETPOOL\_V4\_ID check in devstack
-* Fix key generation for devstack
-* Fix the misspelling of "directory"
-* Add upgrade status check - instances with assigned tasks
-* Do not use self in classmethod
-* update spelling error
-* Fix the misspelling of "configuration"
-* Use existing CNF\_INCLUDE\_DIR to create mysql-flavor directory
-* Add image setup to trove devstack plugin
-* Execute functional test jobs running under python3
-* Use Ubuntu Xenial distro for integration tests
-* Change openstack-dev to openstack-discuss
-* Set Tempest's service\_availability setting for Trove
-* Add missing ws separator between words
-* [fix\_typos] fix wrongly spell word "configration "
-* Update http to https
-* Fix the wrong url
-* Extend contribution doc by IRC contact details
-* Add python 3.6 unit test job
-* Fix incorrect test\_group in zull job definition
-* Fix home direcroty path in trovestack script
-* Fix the conflict of urlparse between python2 and python3
-* Add trove-status upgrade check command framework
-* Fix build ubuntu-geust issue on arm64
-* Increment versioning with pbr instruction
-* Be compilance with latest oslo.messaging
-* Add detailed list for instances
-* Fix the mysql start up issue after restore
-* Fix cover job
-* Add blueprints and bugs link in documents
-* endpoint\_type option not used with single tenant
-* Cleanup zuul.yaml
-* add python 3.6 unit test job
-* switch documentation job to new PTI
-* make tox -e pylint only run pylint
-* fix tox python3 overrides
-* update pylint to 1.9.2
-* Use latests version of python-troveclient in tests
 * import zuul job settings from project-config
-* Replace 14.04 trusty with 16.04 xenial
-* Enable mutable config in trove
-* Remove unused imports from the integration tests
-* Use print function rather than statement in tests
-* Update reno for stable/rocky
-* Reject zero volume size in API
-* Add extended properties support for mongo cluster
-* Remove nova conf for guestagent
-
-10.0.0
-------
-
-* Fix replication failure when Swift isn't available
-
-10.0.0.0b3
-----------
-
-* Sync the data fields of DB\* class and table fields
-* Migrate to Zuul v3 native job definitions
-* More reliable gpg keys importing in DIB elements
-* Format service apache2 reload section for Debian/Ubuntu
-* [doc] Use openstack client command to replace others
-* Raise timeout for instance resizing checking job
-* Fix invalid escape sequence warnings
-* Add release note link in README
-* py3.x: Fix usage of gettext.install
-* Remove pycrypto from requirements
-* change pylint wrapper to let messages be prefixes
-* Update Trove's README
-* Add volume\_type to apischema
-* Add a hook for restore process to check if successful
-* Switch to cryptography from pycrypto
-* Update character set and coallaction for mysql5.7
-* Trivial: Update pypi url to new url
-* Cleanup testrepository and os-testr requirements
-* Run unittests under the low-constraints job
-* Fix dict iteration in PropertiesCodec
-
-10.0.0.0b1
-----------
-
-* Fix lower-constraints and uncap eventlet
-* Update auth\_uri option to www\_authenticate\_uri
-* Add check\_process() for mysqldump
-* fix a typo
-* Updated from global requirements
-* fix a typo in documentation
-* add lower-constraints job
-* Mox removal for instances\_resize API tests
-* Revert "Fix false-negative failure report for mysqldump backup"
-* Updated from global requirements
-* Fix os.path.join() for unittests under python 3.6
-* Updated from global requirements
-* Mox removal for MgmtInstance actions API tests
-* Updated from global requirements
-* Avoid diverged slave when migrating MariaDB master
-* Skip root state inherting scenario tests for Redis
-* Fix client recreation in Redis root-disable test
-* Register all replicas in replication scenario test
-* Fix create mongodb cluster error in multi-network env
-* Fix annotation info error in guestagent-api
-* Fix false-negative failure report for mysqldump backup
-* Use neutronclient for floatingip operations
-* Return 204 instead of 200 for root-disable API
-* Fix guestagent.test\_operating\_system for Python3
-* Remove entry of policy.json from setup.cfg
-* Use RootHistory to check if root is ever enabled
-* [api-ref] Add sections for backups
-* Fix client ping in redis scenario tests
-* Fix PostgreSQL non-dynamic configration tests
-* Fix incorrect usage of assertTrue
-* drop extra word to fix typo
-* Only launch in-tree zuul jobs when necessary
-* Remove install-guide tox env
-* Generate policy sample file automatically
-* [api-ref]: update instance creating parameters
-* Remove support of creating volume from Nova
-* Remove security.authorization option from mongos
-* Update reno for stable/queens
-* Add bandit-baseline check job
-* report\_root should always use context.user
-
-9.0.0
------
-
-* Remove unused optparse code in trove.db
-* Allow host URL for versions to be configurable
-* Zuul: Remove project name
-* Unable to grow/shrink Vertica 9.x cluster
-* Accept the IBM DB2 license during the DIB process
-* Remove hardcoded version in DB2 install path
-* Improve Vertica 9.x support
-* Update the validation template for postgresql
-* Unable to perform backup on DB2 instance
-* Fix gate error
-* [api-ref] Add sections for instance logs
-* Add innodb rules for mysql validation template
-* Revert Cassandra version to 2 on ubuntu element
-* Fix Cassandra element
-* Change file permissions on element script
-* [api-ref] Update style and instances api
-
-9.0.0.0b3
----------
-
-* Use neutronclient to get networks
-* [api-ref] Add fault names for the error response codes
-* Remove log translations
-* Updated from global requirements
-* Missing element-deps files for xenial
-* Remove checkpoint\_segments validation rules
-* Adding missing dependencay
-* Remove use of unsupported TEMPEST\_SERVICES variable
-* Fix api exception with unicode tenant name
-* Fix a error exception code
-* Add missing permission on 10-fix-mycnf for Percona
-* Unable to build cassandra images
-* Upgrade Postgresql support to v9.6
-* Use DocumentedRuleDefault instead of RuleDefault
-* Guest agent won't start on Xenial Percona 5.7
-* Updated from global requirements
-* Remove the heat related documents
-* Fix systemd service mongodb on xenial element
-* Updated from global requirements
-* Always kill all child processes when backup runner exits
-* Import experimental Zuul jobs
-* Initialize BadRequest exception with correct message
-* Add validate\_instances\_network for cluster create
-* Add support for MySQL 5.7 on Ubuntu Xenial
-* Move legacy trove zuul jobs to trove project
-* Replace outdated image info in guest\_cloud\_init doc
-* Fix integration cgit url in image building doc
-* Add functionality to define different Message and Notification destination
-* Remove bundled intree trove tempest plugin
-* Updated from global requirements
-* Fix wrong error message for secgroup\_rule method
-* TrivialFix: remove redundant import alias
-* Remove the deprecated Nova-network
-* Fix Increase guest agent cmd process timeout
-* Implementation of root-enable, root-disable in redis
-* Remove policy.json file
-* Replace assertRaisesRegexp with assertRaisesRegex
-* Add validation for galera\_common grow
-* Use keystone session for single tenant remote clients
-* Set right status when grow/shrink failed
-* Fix status message inside validate\_can\_perform\_action()
-* Improve .gitignore file in the project
-* Add doc8 to pep8 check for trove project
-* Updated from global requirements
-* Apply pep8 check to app.wsgi
-* Add overrides related unittests for redis manager
-* Fix create redis instance with new requirepass
-* Unuse an undefined local variable 'name'
-* cluster-create support volume\_type
-* fix typos in cluster/test\_models.py
-* Add Database service user guide
-* Fix trove-guestagent startup for redis mangled config cmd
-* Don't refresh trove code in guestagent once installed
-* Add #!/bin/bash to /etc/rc.local
-* Improve code to reduce traverse times
-* Fix mongodb database create
-* Fix typo in trovestack cleanup
-* Remove setting of version/release from releasenotes
-* Utilize Ubuntu's hardware enablement stack for trovestack
-* Increase guest agent cmd process timeout
-* Allow tunable for guest agent process timeout
-* Updated from global requirements
-* Initialize RedisAdmin with correct config command
-* Fix nova proxy admin login
-* Allow the user to disable tmpfs when building guest images
-* Updated from global requirements
-* Enable other Ubuntu architectures to utilize diskimage-builder
-* Add volume size verify for replica based on master
-* Fix variable user's definition in unittest code
-* Let cluster action\_\*\*\* load the right schema
-* For Python 3 Common patterns-six string
-* Update DIB doc
-* Fix qemu image compatibility mode
-* Stop polling if nova instances goto error
-* Lazy load all configuration options
-* Add timestamp to cluster instance name
-* Support -1 as unlimited quota restraint in Trove
-* Configure guestagent on Ubuntu guest images to use CA certificates
-* Missing import of 'assert\_equal' in tests/util/\_\_init\_\_.py
-* MongoDB create raise index out of range error
-* Allow py27 test selection
-* Fix mysql instance create failed when enable skip-name-resolve
-* Fix duplicate instancetask code
-* Support insecure SSL when talking to services
-* Avoid load deleted instances that belong to a cluster
-* Update URLs in documents according to document migration
+* Add .stestr.conf to fix tox-py27 stable job
+* Failed to build mongo image
 * Open the volume\_support of redis
-* Change RPC dispatcher access\_policy to DefaultRPCAccessPolicy
-* Force delete any instance or cluster
-* Fix some typos in trove/instance/models.py
-* Fix indent in docs
-* Do not configure kvm virt\_type in devstack
-* Fix requirepass problem with redis
+* Fix mysql instance create failed when enable skip-name-resolve
 * Remove Mitaka reference in install/dashboard.rst
-* Optimize import inside trove.common.remote.neutron\_client
-* Fix python2/unicode/string issue in mongodb/cluster
-* Enhance test case fail to build  message
 * Enable longer Keystone token life
-* Remove tempest from the test requirements
-* Updated from global requirements
-* When creating a replica do not allow to create users or databases in the same call
-* Add test for flavor
-* Open test\_create\_too\_many\_instances
-* Avoid deleting parent backup failed caused by 404
-
-9.0.0.0b1
----------
-
-* Enable integration tests
-* TrivialFix: Redundant alias in import statement
-* Fix integration test exception handling
-* Imported Translations from Zanata
-* Move Pylint ignore
 * Fix gate issues
-* Replace deprecated alias 'os' with 'os\_primary'
-* Adding mongodb support to xenial
-* Add default configuration files to data\_files
-* Fix inaccurate message while creating replica
-* [Trivialfix]Fix typos in trove
-* Fix to use "." to source script files
-* Remove the use of deprecated attributes in novaclient calls
-* Comment out the option oslo\_messaging\_rabbit.rabbit\_password
-* Remove unneeded msgfmt test
-* Update ubuntu cassandra to supported repo and version
-* Update reno for stable/pike
-* Remove inexistent option in install guide
 
 8.0.0
 -----
 
 * Fix AttributeError in api example snippets tests
-* Remove exists\_notification\_ticks from sample conf
 
 8.0.0.0b3
 ---------
 
 * move from oslosphinx to openstackdocstheme
 * import content from cli-reference guide in openstack-manuals
 * import admin-guide content from openstack-manuals
 * rearrange existing docs to fit the new standard layout
 * Updated from global requirements
 * Fix a exception error
 * Remoe obsolete apidocs
 * When running in gate environment, don't start services
-* Wrong load removed node of galera cluster
 * TrivialFix: Update api-ref link
 * Redis 'repl-backlog-size' conf parameter using wrong MIN value
 * Updated from global requirements
 * Handle isotime deprecation in oslo\_utils.timeutils
 * Updated from global requirements
 * Updated from global requirements
-* Use get\_rpc\_transport instead of get\_transport
 
 8.0.0.0b2
 ---------
 
 * Handle log message interpolation by the logger part 10
 * Log the right attached configuration id
 * Improve list-of-ports validation
@@ -887,16 +61,14 @@
 * Add Couchbase helper client methods
 * Add port 16379 to conf.sample
 * handle impending oslo.messaging deprecation
 * enable trove-api behind mod-wsgi
 * Fixing PROXY\_AUTH\_URL not being populated properly
 * Updated from global requirements
 * fix-gate: change trove auth URL's to reflect new URL settings
-* Remove the check about related\_to
-* Call wrong father class's method with super()
 * Fix Cassandra cluster restart
 * Update SUSE distro information in install guide
 * fix the gate: heat-cfntools was yanked out from under us
 
 8.0.0.0b1
 ---------
```

### Comparing `trove-21.0.0.0rc2/LICENSE` & `trove-8.0.1/LICENSE`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/PKG-INFO` & `trove-8.0.1/trove.egg-info/PKG-INFO`

 * *Files 17% similar despite different names*

```diff
@@ -1,79 +1,56 @@
-Metadata-Version: 1.2
+Metadata-Version: 1.1
 Name: trove
-Version: 21.0.0.0rc2
+Version: 8.0.1
 Summary: OpenStack DBaaS
-Home-page: https://docs.openstack.org/trove/latest/
+Home-page: https://wiki.openstack.org/wiki/Trove
 Author: OpenStack
-Author-email: openstack-discuss@lists.openstack.org
+Author-email: openstack-dev@lists.openstack.org
 License: UNKNOWN
 Description: =====
         Trove
         =====
         
-        .. image:: https://governance.openstack.org/tc/badges/trove.svg
-            :target: https://governance.openstack.org/tc/reference/tags/index.html
+        .. image:: http://governance.openstack.org/badges/trove.svg
+            :target: http://governance.openstack.org/reference/tags/index.html
         
         Trove is Database as a Service for OpenStack.
         
         Getting Started
         ---------------
         
         If you'd like to run from the master branch, you can clone the git repo:
         
-            git clone https://opendev.org/openstack/trove
+            git clone https://github.com/openstack/trove
         
-        For information on how to contribute to trove, please see
-        CONTRIBUTING.rst_ and HACKING.rst_
         
-        .. _CONTRIBUTING.rst: https://opendev.org/openstack/trove/src/branch/master/CONTRIBUTING.rst
-        .. _HACKING.rst: https://opendev.org/openstack/trove/src/branch/master/HACKING.rst
+        * Wiki: https://wiki.openstack.org/wiki/Trove
+        * Developer Docs: http://docs.openstack.org/developer/trove
         
-        * `Wiki <https://wiki.openstack.org/wiki/Trove>`_
-        * `Developer Docs <https://docs.openstack.org/trove/latest/>`_
-        
-        You can raise bugs here:
-        `Bug Tracker <https://storyboard.openstack.org/#!/project/openstack/trove>`_
-        
-        The plan for trove can be found at
-        `Trove Specs <https://specs.openstack.org/openstack/trove-specs/>`_
-        
-        Release notes for the project can be found at:
-          https://docs.openstack.org/releasenotes/trove
+        You can raise bugs here: https://bugs.launchpad.net/trove
         
         Python client
         -------------
-        Python-troveclient_ is a client for Trove.
-        
-        .. _Python-troveclient: https://opendev.org/openstack/python-troveclient
-        
-        Dashboard plugin
-        ----------------
-        Trove-dashboard_ is OpenStack dashbaord plugin for Trove.
-        
-        .. _Trove-dashboard: https://opendev.org/openstack/trove-dashboard
+        https://git.openstack.org/cgit/openstack/python-troveclient
         
         References
         ----------
         
-        * `Installation docs`_
-        * `Manual installation docs`_
-        * `Build guest image`_
-        
-        .. _Installation docs: https://docs.openstack.org/trove/latest/install/
-        .. _Manual installation docs: https://docs.openstack.org/trove/latest/install/install-manual.html
-        .. _Build guest image: https://docs.openstack.org/trove/latest/admin/building_guest_images.html
+        * Installation docs:
+          http://docs.openstack.org/developer/trove/dev/install.html
+        * Manual installation docs:
+          http://docs.openstack.org/developer/trove/dev/manual_install.html
+        * Build guest image:
+          http://docs.openstack.org/developer/trove/dev/building_guest_images.html
         
         
 Platform: UNKNOWN
 Classifier: Environment :: OpenStack
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: System Administrators
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 2
+Classifier: Programming Language :: Python :: 2.7
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Requires-Python: >= 3.8
+Classifier: Programming Language :: Python :: 3.5
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/conf.py` & `trove-8.0.1/api-ref/source/conf.py`

 * *Files 17% similar despite different names*

```diff
@@ -21,29 +21,29 @@
 # Note that not all possible configuration values are present in this
 # autogenerated file.
 #
 # All configuration values have a default; values that are commented out
 # serve to show the default.
 
 import os
+import subprocess
 import sys
+import warnings
+
+import openstackdocstheme
 
 extensions = [
     'os_api_ref',
-    'openstackdocstheme'
 ]
 
 html_theme = 'openstackdocs'
+html_theme_path = [openstackdocstheme.get_html_theme_path()]
 html_theme_options = {
     "sidebar_mode": "toc",
 }
-openstackdocs_repo_name = 'openstack/trove'
-openstack_auto_name = False
-openstackdocs_bug_project = 'trove'
-openstackdocs_bug_tag = ''
 
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
 # documentation root, use os.path.abspath to make it absolute, like shown here.
 sys.path.insert(0, os.path.abspath('../../'))
 sys.path.insert(0, os.path.abspath('../'))
 sys.path.insert(0, os.path.abspath('./'))
@@ -60,16 +60,34 @@
 #
 # source_encoding = 'utf-8'
 
 # The master toctree document.
 master_doc = 'index'
 
 # General information about the project.
-project = 'Database API Reference'
-copyright = '2010-present, OpenStack Foundation'
+project = u'Database API Reference'
+copyright = u'2010-present, OpenStack Foundation'
+
+# The version info for the project you're documenting, acts as replacement for
+# |version| and |release|, also used in various other places throughout the
+# built documents.
+#
+from trove.version import version_info
+# The full version, including alpha/beta/rc tags.
+release = version_info.release_string()
+# The short X.Y version.
+version = version_info.version_string()
+
+# Config logABug feature
+giturl = u'http://git.openstack.org/cgit/openstack/trove/tree/api-ref/source'
+# source tree
+# html_context allows us to pass arbitrary values into the html template
+html_context = {"bug_tag": "api-ref",
+                "giturl": giturl,
+                "bug_project": "trove"}
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 #
 # language = None
 
 # There are two options for replacing |today|: either, you set today to some
@@ -90,20 +108,29 @@
 add_module_names = False
 
 # If true, sectionauthor and moduleauthor directives will be shown in the
 # output. They are ignored by default.
 show_authors = False
 
 # The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'native'
+pygments_style = 'sphinx'
+
+# Config logABug feature
+# source tree
+giturl = (
+    u'http://git.openstack.org/cgit/openstack/trove/tree/api-ref/source')
+# html_context allows us to pass arbitrary values into the html template
+html_context = {'bug_tag': 'api-ref',
+                'giturl': giturl,
+                'bug_project': 'trove'}
 
 # -- Options for man page output ----------------------------------------------
 
 # Grouping the document tree for man pages.
-# List of tuples 'sourcefile', 'target', 'title', 'Authors name', 'manual'
+# List of tuples 'sourcefile', 'target', u'title', u'Authors name', 'manual'
 
 
 # -- Options for HTML output --------------------------------------------------
 
 # The theme to use for HTML and HTML Help pages.  Major themes that come with
 # Sphinx are currently 'default' and 'sphinxdoc'.
 # html_theme_path = ["."]
@@ -134,14 +161,26 @@
 # html_favicon = None
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
 # html_static_path = ['_static']
 
+# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
+# using the given strftime format.
+# html_last_updated_fmt = '%b %d, %Y'
+git_cmd = ["git", "log", "--pretty=format:'%ad, commit %h'", "--date=local",
+           "-n1"]
+try:
+    html_last_updated_fmt = subprocess.Popen(
+        git_cmd, stdout=subprocess.PIPE).communicate()[0]
+except Exception:
+    warnings.warn('Cannot get last updated time from git repository. '
+                  'Not setting "html_last_updated_fmt".')
+
 # If true, SmartyPants will be used to convert quotes and dashes to
 # typographically correct entities.
 # html_use_smartypants = True
 
 # Custom sidebar templates, maps document names to template names.
 # html_sidebars = {}
 
@@ -181,16 +220,16 @@
 # The font size ('10pt', '11pt' or '12pt').
 # latex_font_size = '10pt'
 
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title, author, documentclass
 # [howto/manual]).
 latex_documents = [
-    ('index', 'Trove.tex', 'OpenStack Database API Documentation',
-     'OpenStack Foundation', 'manual'),
+    ('index', 'Trove.tex', u'OpenStack Database API Documentation',
+     u'OpenStack Foundation', 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
 # the title page.
 # latex_logo = None
 
 # For "manual" documents, if this is true, then toplevel headings are parts,
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/configurations.inc` & `trove-8.0.1/api-ref/source/configurations.inc`

 * *Files 26% similar despite different names*

```diff
@@ -1,206 +1,313 @@
 .. -*- rst -*-
 
-====================
-Configuration groups
-====================
+=====================================
+Configuration groups (configurations)
+=====================================
 
+Creates and lists all configuration groups.
 
 
+Create configuration group
+==========================
 
-List configuration groups
-~~~~~~~~~~~~~~~~~~~~~~~~~
+.. rest_method::  POST /v1.0/{accountId}/configurations
 
-.. rest_method::  GET /v1.0/{project_id}/configurations
+Creates a configuration group.
 
-Lists all configuration groups.
 
 Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
 
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
+   - datastore: datastore
+   - values: values
+   - name: name
+   - accountId: accountId
+
+Request Example
+---------------
+
+.. literalinclude:: samples/db-create-config-group-request.json
+   :language: javascript
+
+
+
 
 
 Response Example
 ----------------
 
-.. literalinclude:: samples/config-groups-list-response.json
+.. literalinclude:: samples/db-create-config-group-response.json
    :language: javascript
 
 
 
 
-Create configuration group
-~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/configurations
 
-Creates a configuration group.
+
+
+
+
+
+
+
+
+List configuration groups
+=========================
+
+.. rest_method::  GET /v1.0/{accountId}/configurations
+
+Lists all configuration groups.
+
+The list includes the associated data store and data store version
+for each configuration group.
+
 
 Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
+
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - datastore: datastore
-   - name: name
-   - values: values
-
+   - accountId: accountId
 
-Request Example
----------------
 
-.. literalinclude:: samples/config-group-create-request.json
-   :language: javascript
 
 
 Response Example
 ----------------
 
-.. literalinclude:: samples/config-group-create-response.json
+.. literalinclude:: samples/db-list-cfg-groups-response.json
    :language: javascript
 
 
 
 
-Show configuration group details
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  GET /v1.0/{project_id}/configurations/{configId}
 
-Lists details about a configuration group, including its values.
+
+
+
+
+
+
+
+
+List configuration group instances
+==================================
+
+.. rest_method::  GET /v1.0/{accountId}/configurations/{configId}/instances
+
+Lists the instances associated with the specified configuration group.
+
 
 Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
 
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
    - configId: configId
+   - accountId: accountId
+
+
 
 
 Response Example
 ----------------
 
-.. literalinclude:: samples/config-group-show-response.json
+.. literalinclude:: samples/db-config-group-instances-response.json
    :language: javascript
 
 
 
 
-List instances applied the configuration group
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  GET /v1.0/{project_id}/configurations/{configId}/instances
 
-Lists the instances associated with the specified configuration group.
+
+
+
+
+
+
+
+
+Delete configuration group
+==========================
+
+.. rest_method::  DELETE /v1.0/{accountId}/configurations/{configId}
+
+Deletes a configuration group.
+
+Error response codes:202,413,405,404,403,401,400,422,503,500,501,
+
+
+Request
+-------
+
+.. rest_parameters:: parameters.yaml
+
+   - configId: configId
+   - accountId: accountId
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+Patch configuration group
+=========================
+
+.. rest_method::  PATCH /v1.0/{accountId}/configurations/{configId}
+
+Sets new values for a configuration group.
+
 
 Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
 
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
+   - values: values
    - configId: configId
+   - accountId: accountId
+
+Request Example
+---------------
+
+.. literalinclude:: samples/db-patch-config-group-request.json
+   :language: javascript
+
+
+
 
 
 Response Example
 ----------------
 
-.. literalinclude:: samples/config-group-list-instances-response.json
+.. literalinclude:: samples/db-patch-config-group-response-json-http.txt
    :language: javascript
 
 
 
 
-Patch configuration group
-~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  PATCH /v1.0/{project_id}/configurations/{configId}
 
-Sets new values for a configuration group.
+
+
+
+
+
+
+
+
+Show configuration group details
+================================
+
+.. rest_method::  GET /v1.0/{accountId}/configurations/{configId}
+
+Lists details about a configuration group, including its values.
+
 
 Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
 
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
    - configId: configId
-   - values: values
+   - accountId: accountId
 
 
-Request Example
----------------
 
-.. literalinclude:: samples/config-group-patch-request.json
+
+Response Example
+----------------
+
+.. literalinclude:: samples/db-config-group-details-response.json
    :language: javascript
 
 
 
 
+
+
+
+
+
+
+
+
+
+
 Update configuration group
-~~~~~~~~~~~~~~~~~~~~~~~~~~
+==========================
 
-.. rest_method::  PUT /v1.0/{project_id}/configurations/{configId}
+.. rest_method::  PUT /v1.0/{accountId}/configurations/{configId}
 
-Sets new values for a configuration group. Also lets you change the name and
-description of the configuration group.
+Sets new values for a configuration group. Also lets you change the name and description of the configuration group.
 
-Normal response codes: 202
+Error response codes:202,413,405,404,403,401,400,422,503,500,501,
 
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - configId: configId
    - values: values
    - description: description
    - name: name
-
+   - configId: configId
+   - accountId: accountId
 
 Request Example
 ---------------
 
-.. literalinclude:: samples/config-group-put-request.json
+.. literalinclude:: samples/db-update-config-group-request.json
    :language: javascript
 
 
 
 
-Delete configuration group
-~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  DELETE /v1.0/{project_id}/configurations/{configId}
 
-Deletes a configuration group.
 
-Normal response codes: 202
 
 
-Request
--------
 
-.. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - configId: configId
+
+
+
+
+
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/instance-actions.inc` & `trove-8.0.1/api-ref/source/database-instances.inc`

 * *Files 26% similar despite different names*

```diff
@@ -1,344 +1,426 @@
 .. -*- rst -*-
 
-================
-Instance actions
-================
+==============================
+Database instances (instances)
+==============================
 
+Creates, lists, shows details for, attaches a configuration group
+to, detaches a configuration group from, deletes, lists
+configuration defaults, creates root, and determines whether root
+is enabled for instances.
 
 
+Delete database instance
+========================
 
-Restart instance
-~~~~~~~~~~~~~~~~
+.. rest_method::  DELETE /v1.0/{accountId}/instances/{instanceId}
 
-.. rest_method::  POST /v1.0/{project_id}/instances/{instanceId}/action
+Deletes a database instance, including any associated data.
 
-Restarts the database service for an instance.
+This operation does not delete any read slaves.
 
-The restart operation restarts only the database instance. Restarting
-the database erases any dynamic configuration settings that you make
-in the database instance.
+You cannot complete this operation when the instance state is
+either ``REBUILDING`` or ``BUILDING``.
 
-The database service is unavailable until the instance restart finishes.
+Error response codes:202,413,405,404,403,401,400,422,503,500,501,
 
-Normal response codes: 202
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
    - instanceId: instanceId
+   - accountId: accountId
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-action-restart-request.json
-   :language: javascript
 
 
 
 
-Resize instance flavor
-~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/instances/{instanceId}/action
 
-Resizes the (Nova)flavor for an instance.
 
-If you provide a valid ``flavorRef``, this operation changes the
-memory size of the instance, and restarts the database.
 
-Normal response codes: 202
 
-Request
--------
 
-.. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - instanceId: instanceId
-   - flavorRef: flavorRef
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-action-resize-request.json
-   :language: javascript
 
+Show database instance details
+==============================
 
+.. rest_method::  GET /v1.0/{accountId}/instances/{instanceId}
 
+Shows database instance details.
 
-Resize instance volume
-~~~~~~~~~~~~~~~~~~~~~~
+Lists the status and details of the database instance.
 
-.. rest_method::  POST /v1.0/{project_id}/instances/{instanceId}/action
+Lists the volume size in gigabytes (GB) and the approximate GB
+used.
 
-Resizes the volume that is attached to an instance.
+After instance creation, the ``used`` value is greater than 0, which
+is expected as databases may create some basic (non empty) files to
+represent an empty schema. The response does not include the ``used``
+attribute when the instance status is ``BUILD``, ``REBOOT``,
+``RESIZE``, or ``ERROR``.
 
-You can use this operation to increase but not decrease the volume
-size. A valid volume size is an integer value in gigabytes (GB).
+The list operations return a DNS-resolvable host name for the
+database instance rather than an IP address. Because the host name
+always resolves to the correct IP address for the database
+instance, you do not need to maintain the mapping. Although the IP
+address might change when you resize, migrate, or perform other
+operations, the host name always resolves to the correct database
+instance.
 
-You cannot increase the volume to a size that is larger than the
-API volume size limit.
 
-For replication cluster, resizing volume of the primary will also resize
-replicas automatically.
+Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
 
-Normal response codes: 202
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
    - instanceId: instanceId
-   - volume: volume
+   - accountId: accountId
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-action-resize-volume-request.json
+
+
+Response Example
+----------------
+
+.. literalinclude:: samples/db-instance-status-detail-response.json
    :language: javascript
 
 
 
 
-Promote instance to replica master
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/instances/{instanceId}/action
 
-Promotes a replica.
 
-If you have set up replication, and the master instance is still reachable, you
-can use this operation to promote a replica to be the new master instance.
 
-This can be useful if you want to make a configuration change or maintenance to
-the master instance. If you made the change on the master instance directly,
-you would need to take the master instance down for the duration of the
-operation. Instead, you can create a replica, make the configuration change on
-the replica, and then promote the replica to become the new master instance.
 
-Once this command is executed, the status of all the instances will be set to
-``PROMOTE`` and Trove will work its magic until all of them to come back to
-``HEALTHY``.
 
-The ``instanceId`` is the instance ID of the replica you want to promote.
 
-Normal response codes: 202
+
+
+
+Attach configuration group
+==========================
+
+.. rest_method::  PUT /v1.0/{accountId}/instances/{instanceId}
+
+Attaches a configuration group to an instance.
+
+Error response codes:202,413,415,405,404,403,401,400,422,503,500,501,
+
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
+   - configuration: configuration
    - instanceId: instanceId
+   - accountId: accountId
 
 Request Example
 ---------------
 
-.. literalinclude:: samples/instance-action-promote-replica-request.json
+.. literalinclude:: samples/db-attach-config-group-request.json
    :language: javascript
 
 
 
 
-Eject the master instance
-~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/instances/{instanceId}/action
 
-Remove the master instance in a replication set.
 
-This should be only done in a failed master scenario. This command ejects the
-current master and then forces a re-election for the new master. The new master
-is effectively the one with the most current replica of the old master.
 
-Once this command is executed, the status of all the instances will be set to
-``EJECT`` and Trove will work its magic until all of them to come back to
-``HEALTHY``.
 
-The ``instanceId`` is the ID of the current unavailable master instance.
 
-Normal response codes: 202
 
-Request
--------
 
-.. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - instanceId: instanceId
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-action-eject-replica-request.json
-   :language: javascript
 
 
 
+Detach configuration group
+==========================
 
-Reset instance status
-~~~~~~~~~~~~~~~~~~~~~
+.. rest_method::  PUT /v1.0/{accountId}/instances/{instanceId}
 
-.. rest_method::  POST /v1.0/{project_id}/instances/{instanceId}/action
+Detaches a configuration group from an instance.
 
-Set instance service status to ``ERROR`` and clear the current task status.
-Mark any running backup operations as ``FAILED``.
+When you pass in only an instance ID and omit the configuration ID,
+this operation detaches any configuration group that was attached
+to the instance.
+
+Error response codes:202,413,415,405,404,403,401,400,422,503,500,501,
 
-Normal response codes: 202
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
+   - configuration: configuration
    - instanceId: instanceId
+   - accountId: accountId
 
 Request Example
 ---------------
 
-.. literalinclude:: samples/instance-action-reset-status-request.json
+.. literalinclude:: samples/db-detach-config-group-request.json
    :language: javascript
 
 
 
 
-Stop database service
-~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/mgmt/instances/{instanceId}/action
 
-Admin only API. Stop database service inside an instance.
 
-Normal response codes: 202
+
+
+
+
+
+
+
+
+
+
+
+Detach replica
+==============
+
+.. rest_method::  PATCH /v1.0/{accountId}/instances/{instanceId}
+
+Detaches a replica from its replication source.
+
+If you created an instance that is a replica of a source instance,
+you can detach the replica from the source. This can be useful if
+the source becomes unavailable. In this case, you can detach the
+replica from the source, making the replica a standalone database
+instance. You can then take the new standalone instance and create
+a new replica of that instance.
+
+Error response codes:202,413,415,405,404,403,401,400,422,503,500,501,
+
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
+   - replica_of: replica_of
+   - slave_of: slave_of
    - instanceId: instanceId
+   - accountId: accountId
 
 Request Example
 ---------------
 
-.. literalinclude:: samples/instance-mgmt-action-stop-request.json
+.. literalinclude:: samples/db-detach-replica-request.json
    :language: javascript
 
 
 
 
-Reboot instance
-~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/mgmt/instances/{instanceId}/action
 
-Admin only API. Reboot the database instance, database service will be stopped
-before rebooting.
 
-Normal response codes: 202
+
+
+
+
+
+
+
+
+
+
+
+Create database instance
+========================
+
+.. rest_method::  POST /v1.0/{accountId}/instances
+
+Creates a database instance.
+
+Asynchronously provisions a database instance. You must specify a
+flavor and a volume size. The service provisions the instance with
+a volume of the requested size, which serves as storage for the
+database instance.
+
+ **Notes**
+
+- You can create only one database instance per POST request.
+
+- You can create a database instance with one or more databases. You
+  associate users with each database.
+
+- The port on which the database instance is listening is database
+  specific.
+
+
+Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
+
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - instanceId: instanceId
+   - users: users
+   - password: password
+   - datastore_version: datastore_version
+   - name: name
+   - flavorRef: flavorRef
+   - characterSet: characterSet
+   - replica_count: replica_count
+   - instance: instance
+   - collate: collate
+   - databases: databases
+   - datastore: datastore
+   - configuration: configuration
+   - type: type
+   - replica_of: replica_of
+   - size: size
+   - accountId: accountId
 
 Request Example
 ---------------
 
-.. literalinclude:: samples/instance-mgmt-action-reboot-request.json
+.. literalinclude:: samples/db-create-instance-request.json
+   :language: javascript
+
+
+
+Response Parameters
+-------------------
+
+.. rest_parameters:: parameters.yaml
+
+   - updated: updated
+   - name: name
+   - created: created
+   - characterSet: characterSet
+   - instance: instance
+   - collate: collate
+   - databases: databases
+   - flavor: flavor
+   - users: users
+
+
+
+Response Example
+----------------
+
+.. literalinclude:: samples/db-create-instance-response.json
    :language: javascript
 
 
 
 
-Cold Migrate instance
-~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/mgmt/instances/{instanceId}/action
 
-Admin only API. Migrate(resize) the database instance, database service will be
-stopped before migrating.
 
-Normal response codes: 202
+
+
+
+
+
+
+
+List database instances
+=======================
+
+.. rest_method::  GET /v1.0/{accountId}/instances
+
+Lists information, including status, for all database instances.
+
+Lists status and information for all database instances.
+
+
+Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
+
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - instanceId: instanceId
+   - accountId: accountId
+
+
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-mgmt-action-migrate-request.json
+Response Example
+----------------
+
+.. literalinclude:: samples/db-instances-index-response.json
    :language: javascript
 
 
 
 
-Reset instance task status
-~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/mgmt/instances/{instanceId}/action
 
-Admin only API. Reset task status of an instance, mark any running backup
-operations as ``FAILED``.
 
-Normal response codes: 202
+
+
+
+
+
+
+
+List configuration defaults
+===========================
+
+.. rest_method::  GET /v1.0/{accountId}/instances/{instanceId}/configuration
+
+Lists the configuration defaults for an instance.
+
+
+Normal response codes: 200
+Error response codes:413,405,404,403,401,400,422,503,500,501,
+
 
 Request
 -------
 
 .. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
    - instanceId: instanceId
+   - accountId: accountId
+
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-mgmt-action-reset-task-status-request.json
+
+Response Example
+----------------
+
+.. literalinclude:: samples/db-list-cfg-defaults-response.json
    :language: javascript
 
 
-Rebuild instance
-~~~~~~~~~~~~~~~~
 
-.. rest_method::  POST /v1.0/{project_id}/mgmt/instances/{instanceId}/action
 
-Admin only API. Rebuild the Nova server's operating system for the database
-instance. The rebuild operation is mainly for Trove upgrade, especially when
-the interface between Trove controller and guest agent changes. After Trove
-controller is upgraded, the cloud administrator needs to send rebuild request
-with the new guest image ID. Communication with the end users is needed as the
-database service goes offline during the process. User's data in the database
-is not affected.
 
-Normal response codes: 202
 
-Request
--------
 
-.. rest_parameters:: parameters.yaml
 
-   - project_id: project_id
-   - instanceId: instanceId
 
-Request Example
----------------
 
-.. literalinclude:: samples/instance-mgmt-action-rebuild-instance-request.json
-   :language: javascript
+
+
+
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/backup-create-response.json` & `trove-8.0.1/api-ref/source/samples/db-backup-create-response.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/backup-get-response.json` & `trove-8.0.1/api-ref/source/samples/db-backup-get-response.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/backup-list-response.json` & `trove-8.0.1/api-ref/source/samples/db-backups-by-instance-response.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/config-group-show-response.json` & `trove-8.0.1/api-ref/source/samples/db-configuration-create-response.json`

 * *Files 23% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7556818181818181%*

 * *Differences: {"'configuration'": "{'datastore_version_id': 'b00000b0-00b0-0b00-00b0-000b000000bb', "*

 * *                    "'datastore_version_name': '5.5', 'updated': '2014-10-30T12:30:00', 'values': "*

 * *                    "{'connect_timeout': 120, 'collation_server': 'latin1_swedish_ci'}, 'name': "*

 * *                    "'example-configuration-name', 'created': '2014-10-30T12:30:00', "*

 * *                    "'instance_count': 0, 'id': '43a6ea86-e959-4735-9e46-a6a5d4a2d80f', "*

 * *                    "'description': 'example desc […]*

```diff
@@ -1,17 +1,17 @@
 {
     "configuration": {
-        "created": "2015-11-20T20:51:24",
+        "created": "2014-10-30T12:30:00",
         "datastore_name": "mysql",
-        "datastore_version_id": "b9f97132-467b-4f8e-b12d-947cfc223ac3",
-        "datastore_version_name": "mysql-5.7",
-        "datastore_version_number": "5.7.29",
-        "description": null,
-        "id": "1c8a4fdd-690c-4e6e-b2e1-148b8d738770",
-        "instance_count": 1,
-        "name": "group1",
-        "updated": "2015-11-22T19:07:20",
+        "datastore_version_id": "b00000b0-00b0-0b00-00b0-000b000000bb",
+        "datastore_version_name": "5.5",
+        "description": "example description",
+        "id": "43a6ea86-e959-4735-9e46-a6a5d4a2d80f",
+        "instance_count": 0,
+        "name": "example-configuration-name",
+        "updated": "2014-10-30T12:30:00",
         "values": {
-            "connect_timeout": 17
+            "collation_server": "latin1_swedish_ci",
+            "connect_timeout": 120
         }
     }
 }
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/instance-configuration-list-response.json` & `trove-8.0.1/api-ref/source/samples/db-get-default-instance-configuration-response.json`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9583333333333334%*

 * *Differences: {"'instance'": "{'configuration': {'innodb_buffer_pool_size': '150M', 'key_buffer_size': '50M', "*

 * *               "'max_allowed_packet': '1024K', 'max_connections': 100, 'max_heap_table_size': "*

 * *               "'16M', 'max_user_connections': 100, 'open_files_limit': 512, 'query_cache_size': "*

 * *               "'8M', 'server_id': 271898715, 'table_definition_cache': 256, 'table_open_cache': "*

 * *               "256, 'thread_cache_size': 4, 'tmp_table_size': '16M'}}"}*

```diff
@@ -1,45 +1,45 @@
 {
     "instance": {
         "configuration": {
             "basedir": "/usr",
             "connect_timeout": 15,
             "datadir": "/var/lib/mysql/data",
             "default_storage_engine": "innodb",
-            "innodb_buffer_pool_size": "2401M",
+            "innodb_buffer_pool_size": "150M",
             "innodb_data_file_path": "ibdata1:10M:autoextend",
             "innodb_file_per_table": 1,
             "innodb_log_buffer_size": "25M",
             "innodb_log_file_size": "50M",
             "innodb_log_files_in_group": 2,
             "join_buffer_size": "1M",
-            "key_buffer_size": "800M",
+            "key_buffer_size": "50M",
             "local-infile": 0,
-            "max_allowed_packet": "16392K",
-            "max_connections": 1600,
-            "max_heap_table_size": "256M",
-            "max_user_connections": 1600,
+            "max_allowed_packet": "1024K",
+            "max_connections": 100,
+            "max_heap_table_size": "16M",
+            "max_user_connections": 100,
             "myisam-recover-options": "BACKUP,FORCE",
-            "open_files_limit": 8196,
+            "open_files_limit": 512,
             "performance_schema": "ON",
             "pid-file": "/var/run/mysqld/mysqld.pid",
             "port": 3306,
             "query_cache_limit": "1M",
-            "query_cache_size": "128M",
+            "query_cache_size": "8M",
             "query_cache_type": 1,
             "read_buffer_size": "512K",
             "read_rnd_buffer_size": "512K",
-            "server_id": 1468542390,
+            "server_id": 271898715,
             "skip-external-locking": 1,
             "socket": "/var/run/mysqld/mysqld.sock",
             "sort_buffer_size": "1M",
-            "table_definition_cache": 4098,
-            "table_open_cache": 4098,
-            "thread_cache_size": 64,
+            "table_definition_cache": 256,
+            "table_open_cache": 256,
+            "thread_cache_size": 4,
             "thread_stack": "192K",
-            "tmp_table_size": "256M",
+            "tmp_table_size": "16M",
             "tmpdir": "/var/tmp",
             "user": "mysql",
             "wait_timeout": 120
         }
     }
 }
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/instance-create-request.json` & `trove-8.0.1/api-ref/source/samples/db-create-instance-request.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7760416666666666%*

 * *Differences: {"'instance'": "{'name': 'json_rack_instance', 'databases': {0: {'character_set': 'utf8', "*

 * *               "'collate': 'utf8_general_ci'}}, delete: ['nics', 'datastore', 'access']}"}*

```diff
@@ -1,36 +1,21 @@
 {
     "instance": {
-        "access": {
-            "allowed_cidrs": [
-                "202.78.240.0/24"
-            ],
-            "is_public": true
-        },
         "databases": [
             {
-                "character_set": "utf8mb3",
-                "collate": "utf8mb3_general_ci",
+                "character_set": "utf8",
+                "collate": "utf8_general_ci",
                 "name": "sampledb"
             },
             {
                 "name": "nextround"
             }
         ],
-        "datastore": {
-            "type": "mysql",
-            "version": "5.7"
-        },
         "flavorRef": 1,
-        "name": "test",
-        "nics": [
-            {
-                "net-id": "a5330d7d-0e8c-48b4-9f6c-0f2c4ab1b854"
-            }
-        ],
+        "name": "json_rack_instance",
         "users": [
             {
                 "databases": [
                     {
                         "name": "sampledb"
                     }
                 ],
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/instance-list-response.json` & `trove-8.0.1/api-ref/source/samples/db-instances-index-response.json`

 * *Files 22% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7784090909090908%*

 * *Differences: {"'instances'": "{0: {'datastore': {'version': '5.5'}, 'flavor': {'id': '1', 'links': "*

 * *                "[OrderedDict([('href', 'https://troveapi.org/v1.0/1234/flavors/1'), ('rel', "*

 * *                "'self')]), OrderedDict([('href', 'https://troveapi.org/flavors/1'), ('rel', "*

 * *                "'bookmark')])]}, 'id': '44b277eb-39be-4921-be31-3d61b43651d7', 'links': {0: "*

 * *                "{'href': "*

 * *                "'https://troveapi.org/v1.0/1234/instances/44b277eb-39be-4921-be31-3d61b43651d7'}, "*

 * *           […]*

```diff
@@ -1,35 +1,41 @@
 {
     "instances": [
         {
             "datastore": {
                 "type": "mysql",
-                "version": "5.7"
+                "version": "5.5"
             },
             "flavor": {
-                "id": "6"
+                "id": "1",
+                "links": [
+                    {
+                        "href": "https://troveapi.org/v1.0/1234/flavors/1",
+                        "rel": "self"
+                    },
+                    {
+                        "href": "https://troveapi.org/flavors/1",
+                        "rel": "bookmark"
+                    }
+                ]
             },
-            "id": "7de1bed8-6983-4d46-9a52-0abfbb0d27a2",
-            "ip": [
-                "10.1.0.62",
-                "172.24.5.114"
-            ],
+            "hostname": "e09ad9a3f73309469cf1f43d11e79549caf9acf2.troveexampledb.com",
+            "id": "44b277eb-39be-4921-be31-3d61b43651d7",
             "links": [
                 {
-                    "href": "https://127.0.0.1:8779/v1.0/9f8dd5eacb074c9f87d2d822c9092aa5/instances/7de1bed8-6983-4d46-9a52-0abfbb0d27a2",
+                    "href": "https://troveapi.org/v1.0/1234/instances/44b277eb-39be-4921-be31-3d61b43651d7",
                     "rel": "self"
                 },
                 {
-                    "href": "https://127.0.0.1:8779/instances/7de1bed8-6983-4d46-9a52-0abfbb0d27a2",
+                    "href": "https://troveapi.org/instances/44b277eb-39be-4921-be31-3d61b43651d7",
                     "rel": "bookmark"
                 }
             ],
-            "name": "test",
-            "operating_status": "HEALTHY",
+            "name": "json_rack_instance",
             "region": "RegionOne",
             "status": "ACTIVE",
             "volume": {
-                "size": 1
+                "size": 2
             }
         }
     ]
 }
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/user-create-request.json` & `trove-8.0.1/api-ref/source/samples/db-list-users-pagination-response.json`

 * *Files 24% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.3333333333333333%*

 * *Differences: {"'links'": "[OrderedDict([('href', "*

 * *            "'https://troveapi.org/v1.0/1234/instances/44b277eb-39be-4921-be31-3d61b43651d7/users?limit=2&marker=dbuser2%2540%2525'), "*

 * *            "('rel', 'next')])]",*

 * * "'users'": "{0: {'host': '%', delete: ['password']}, 1: {'host': '%', delete: ['password']}, "*

 * *            'delete: [2]}'}*

```diff
@@ -1,29 +1,31 @@
 {
+    "links": [
+        {
+            "href": "https://troveapi.org/v1.0/1234/instances/44b277eb-39be-4921-be31-3d61b43651d7/users?limit=2&marker=dbuser2%2540%2525",
+            "rel": "next"
+        }
+    ],
     "users": [
         {
             "databases": [
                 {
                     "name": "databaseA"
                 }
             ],
-            "name": "dbuser1",
-            "password": "password"
+            "host": "%",
+            "name": "dbuser1"
         },
         {
             "databases": [
                 {
                     "name": "databaseB"
                 },
                 {
                     "name": "databaseC"
                 }
             ],
-            "name": "dbuser2",
-            "password": "password"
-        },
-        {
-            "name": "dbuser3",
-            "password": "password"
+            "host": "%",
+            "name": "dbuser2"
         }
     ]
 }
```

### Comparing `trove-21.0.0.0rc2/api-ref/source/samples/user-list-response.json` & `trove-8.0.1/api-ref/source/samples/db-list-users-response.json`

 * *Files 27% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8541666666666667%*

 * *Differences: {"'users'": "{2: {'databases': [OrderedDict([('name', 'databaseD')])]}, insert: [(3, "*

 * *            "OrderedDict([('databases', [OrderedDict([('name', 'sampledb')])]), ('host', '%'), "*

 * *            "('name', 'demouser')]))]}"}*

```diff
@@ -18,13 +18,26 @@
                     "name": "databaseC"
                 }
             ],
             "host": "%",
             "name": "dbuser2"
         },
         {
-            "databases": [],
+            "databases": [
+                {
+                    "name": "databaseD"
+                }
+            ],
             "host": "%",
             "name": "dbuser3"
+        },
+        {
+            "databases": [
+                {
+                    "name": "sampledb"
+                }
+            ],
+            "host": "%",
+            "name": "demouser"
         }
     ]
 }
```

### Comparing `trove-21.0.0.0rc2/backup/drivers/mariabackup.py` & `trove-8.0.1/trove/guestagent/strategies/restore/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,104 +1,106 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-import re
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
 
-from oslo_concurrency import processutils
-from oslo_config import cfg
+from eventlet.green import subprocess
 from oslo_log import log as logging
 
-from backup.drivers import mysql_base
+from trove.common import cfg
+from trove.common.strategies.strategy import Strategy
+from trove.common import utils
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+CHUNK_SIZE = CONF.backup_chunk_size
+BACKUP_USE_GZIP = CONF.backup_use_gzip_compression
+BACKUP_USE_OPENSSL = CONF.backup_use_openssl_encryption
+BACKUP_DECRYPT_KEY = CONF.backup_aes_cbc_key
+
+
+class RestoreError(Exception):
+    """Error running the Backup Command."""
+
+
+class RestoreRunner(Strategy):
+    """Base class for Restore Strategy implementations."""
+    """Restore a database from a previous backup."""
+
+    __strategy_type__ = 'restore_runner'
+    __strategy_ns__ = 'trove.guestagent.strategies.restore'
+
+    # The actual system calls to run the restore and prepare
+    restore_cmd = None
+
+    # The backup format type
+    restore_type = None
+
+    # Decryption Parameters
+    is_zipped = BACKUP_USE_GZIP
+    is_encrypted = BACKUP_USE_OPENSSL
+    decrypt_key = BACKUP_DECRYPT_KEY
+
+    def __init__(self, storage, **kwargs):
+        self.storage = storage
+        self.location = kwargs.pop('location')
+        self.checksum = kwargs.pop('checksum')
+        self.restore_location = kwargs.get('restore_location')
+        self.restore_cmd = (self.decrypt_cmd +
+                            self.unzip_cmd +
+                            (self.base_restore_cmd % kwargs))
+        super(RestoreRunner, self).__init__()
+
+    def pre_restore(self):
+        """Hook that is called before the restore command."""
+        pass
 
+    def post_restore(self):
+        """Hook that is called after the restore command."""
+        pass
 
-class MariaBackup(mysql_base.MySQLBaseRunner):
-    """Implementation of Backup and Restore using mariabackup."""
-    restore_cmd = ('mbstream -x -C %(restore_location)s')
-    prepare_cmd = 'mariabackup --prepare --target--dir=%(restore_location)s'
-
-    def __init__(self, *args, **kwargs):
-        super(MariaBackup, self).__init__(*args, **kwargs)
-        self.backup_log = '/tmp/mariabackup.log'
-        self._gzip = True
-
-    @property
-    def cmd(self):
-        cmd = ('mariabackup --backup --stream=xbstream ' +
-               self.user_and_pass)
-        return cmd
-
-    def check_restore_process(self):
-        LOG.info('Checking return code of mbstream restore process.')
-        return_code = self.process.returncode
-        if return_code != 0:
-            LOG.error('mbstream exited with %s', return_code)
-            return False
-
-        return True
+    def restore(self):
+        self.pre_restore()
+        content_length = self._run_restore()
+        self.post_restore()
+        return content_length
+
+    def _run_restore(self):
+        return self._unpack(self.location, self.checksum, self.restore_cmd)
+
+    def _unpack(self, location, checksum, command):
+        stream = self.storage.load(location, checksum)
+        process = subprocess.Popen(command, shell=True,
+                                   stdin=subprocess.PIPE,
+                                   stderr=subprocess.PIPE)
+        content_length = 0
+        for chunk in stream:
+            process.stdin.write(chunk)
+            content_length += len(chunk)
+        process.stdin.close()
+        utils.raise_if_process_errored(process, RestoreError)
+        LOG.debug("Restored %s bytes from stream.", content_length)
 
-    def post_restore(self):
-        """Prepare after data restore."""
-        LOG.info("Running prepare command: %s.", self.prepare_command)
-        stdout, stderr = processutils.execute(*self.prepare_command.split())
-        LOG.info("The command: %s : stdout: %s, stderr: %s",
-                 self.prepare_command, stdout, stderr)
-        LOG.info("Checking prepare log")
-        if not stderr:
-            msg = "Empty prepare log file"
-            raise Exception(msg)
-        last_line = stderr.splitlines()[-1].strip()
-        if not re.search('completed OK!', last_line):
-            msg = "Prepare did not complete successfully"
-            raise Exception(msg)
-
-
-class MariaBackupIncremental(MariaBackup):
-    """Incremental backup and restore using mariabackup."""
-    incremental_prep = ('mariabackup --prepare '
-                        '--target-dir=%(restore_location)s '
-                        '%(incremental_args)s')
-
-    def __init__(self, *args, **kwargs):
-        if not kwargs.get('lsn'):
-            raise AttributeError('lsn attribute missing')
-        self.parent_location = kwargs.pop('parent_location', '')
-        self.parent_checksum = kwargs.pop('parent_checksum', '')
+        return content_length
 
-        super(MariaBackupIncremental, self).__init__(*args, **kwargs)
+    @property
+    def decrypt_cmd(self):
+        if self.is_encrypted:
+            return ('openssl enc -d -aes-256-cbc -salt -pass pass:%s | '
+                    % self.decrypt_key)
+        else:
+            return ''
 
     @property
-    def cmd(self):
-        cmd = (
-            'mariabackup --backup --stream=xbstream'
-            ' --incremental-lsn=%(lsn)s ' +
-            self.user_and_pass
-        )
-        LOG.info('cmd:{}'.format(cmd))
-        return cmd
-
-    def get_metadata(self):
-        meta = super(MariaBackupIncremental, self).get_metadata()
-
-        meta.update({
-            'parent_location': self.parent_location,
-            'parent_checksum': self.parent_checksum,
-        })
-        return meta
-
-    def run_restore(self):
-        """Run incremental restore."""
-        LOG.info('Running incremental restore')
-        self.incremental_restore(self.location, self.checksum)
-        return self.restore_content_length
+    def unzip_cmd(self):
+        return 'gzip -d -c | ' if self.is_zipped else ''
```

### Comparing `trove-21.0.0.0rc2/backup/drivers/postgres.py` & `trove-8.0.1/trove/guestagent/strategies/restore/mysql_impl.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,293 +1,309 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+
+import glob
 import os
 import re
+import tempfile
 
 from oslo_log import log as logging
+import pexpect
 
-from backup.drivers import base
-from backup.utils import poll_until
-from backup.utils import postgresql as psql_util
+from trove.common import cfg
+from trove.common import exception
+from trove.common.i18n import _
+from trove.common import utils
+from trove.guestagent.common import operating_system
+from trove.guestagent.common.operating_system import FileMode
+import trove.guestagent.datastore.mysql.service as dbaas
+from trove.guestagent.strategies.restore import base
 
 LOG = logging.getLogger(__name__)
 
 
-class PgBasebackup(base.BaseRunner):
-    _is_read_only = None
-
-    def __init__(self, *args, **kwargs):
-        self.backup_log = '/tmp/pgbackup.log'
-        self._gzip = False
-        if not kwargs.get('wal_archive_dir'):
-            raise AttributeError('wal_archive_dir attribute missing')
-        self.wal_archive_dir = kwargs.pop('wal_archive_dir')
-        self.datadir = kwargs.pop(
-            'db_datadir', '/var/lib/postgresql/data/pgdata')
-
-        self.label = None
-        self.stop_segment = None
-        self.start_segment = None
-        self.start_wal_file = None
-        self.stop_wal_file = None
-        self.checkpoint_location = None
-        self.metadata = {}
-
-        super(PgBasebackup, self).__init__(*args, **kwargs)
-
-        self.restore_command = (f"{self.decrypt_cmd}tar xzf - -C "
-                                f"{self.datadir}")
-
-    @property
-    def cmd(self):
-        cmd = (f"pg_basebackup -U postgres -Ft -z --wal-method=fetch "
-               f"--label={self.filename} --pgdata=-")
-        return cmd + self.encrypt_cmd
-
-    @property
-    def manifest(self):
-        """Target file name."""
-        return "%s.tar.gz%s" % (self.filename, self.encrypt_manifest)
-
-    @property
-    def is_read_only(self):
-        """Checks if PostgreSQL is in read-only mode.
-
-        Returns:
-            True if PostgreSQL is in read-only mode, False otherwise.
-        """
-        if self._is_read_only is None:
-            with psql_util.PostgresConnection('postgres') as conn:
-                self._is_read_only = conn.query(
-                    "SELECT pg_is_in_recovery();")[0][0]
-
-        return self._is_read_only
-
-    def get_wal_files(self, backup_pos=0):
-        """Return the WAL files since the provided last backup.
-
-        pg_archivebackup depends on alphanumeric sorting to decide wal order,
-        so we'll do so too:
-        https://github.com/postgres/postgres/blob/REL9_4_STABLE/contrib
-           /pg_archivecleanup/pg_archivecleanup.c#L122
-        """
-        backup_file = self.get_backup_file(backup_pos=backup_pos)
-        last_wal = backup_file.split('.')[0]
-        wal_re = re.compile("^[0-9A-F]{24}$")
-        wal_files = [wal_file for wal_file in os.listdir(self.wal_archive_dir)
-                     if wal_re.search(wal_file) and wal_file >= last_wal]
-        return wal_files
-
-    def get_backup_file(self, backup_pos=0, regex=None):
-        """Look for the most recent .backup file that basebackup creates
-
-        :return: a string like 000000010000000000000006.00000168.backup
-        """
-        regex = regex or r"[0-9A-F]{24}\..*\.backup"
-        backup_re = re.compile(regex)
-        wal_files = [wal_file for wal_file in os.listdir(self.wal_archive_dir)
-                     if backup_re.search(wal_file)]
-        wal_files = sorted(wal_files, reverse=True)
-        if not wal_files:
-            return None
-        return wal_files[backup_pos]
-
-    def get_backup_metadata(self, metadata_file):
-        """Parse the contents of the .backup file"""
-        metadata = {}
-
-        start_re = re.compile(r"START WAL LOCATION: (.*) \(file (.*)\)")
-        stop_re = re.compile(r"STOP WAL LOCATION: (.*) \(file (.*)\)")
-        checkpt_re = re.compile("CHECKPOINT LOCATION: (.*)")
-        label_re = re.compile("LABEL: (.*)")
-
-        with open(metadata_file, 'r') as file:
-            metadata_contents = file.read()
-
-        match = start_re.search(metadata_contents)
-        if match:
-            self.start_segment = match.group(1)
-            metadata['start-segment'] = self.start_segment
-            self.start_wal_file = match.group(2)
-            metadata['start-wal-file'] = self.start_wal_file
-
-        match = stop_re.search(metadata_contents)
-        if match:
-            self.stop_segment = match.group(1)
-            metadata['stop-segment'] = self.stop_segment
-            self.stop_wal_file = match.group(2)
-            metadata['stop-wal-file'] = self.stop_wal_file
-
-        match = checkpt_re.search(metadata_contents)
-        if match:
-            self.checkpoint_location = match.group(1)
-            metadata['checkpoint-location'] = self.checkpoint_location
-
-        match = label_re.search(metadata_contents)
-        if match:
-            self.label = match.group(1)
-            metadata['label'] = self.label
-
-        return metadata
-
-    def get_metadata(self):
-        """Get metadata.
-
-        pg_basebackup may complete, and we arrive here before the
-        history file is written to the wal archive. So we need to
-        handle two possibilities:
-        - this is the first backup, and no history file exists yet
-        - this isn't the first backup, and so the history file we retrieve
-        isn't the one we just ran!
-         """
-        def _metadata_found():
-            backup_file = self.get_backup_file()
-            LOG.info("backup_file: %s" % [backup_file])
-            LOG.info("Doing backup Postgres in read-only mode: %s" %
-                     self.is_read_only)
-            if not backup_file and self.is_read_only:
-                return True
-
-            if not backup_file:
-                return False
-
-            self.metadata = self.get_backup_metadata(
-                os.path.join(self.wal_archive_dir, backup_file))
-            LOG.info("Metadata for backup: %s.", self.metadata)
-            return self.metadata['label'] == self.filename
+class MySQLRestoreMixin(object):
+    """Common utils for restoring MySQL databases."""
+    RESET_ROOT_RETRY_TIMEOUT = 100
+    RESET_ROOT_SLEEP_INTERVAL = 10
+
+    RESET_ROOT_MYSQL_COMMANDS = ("SET PASSWORD FOR "
+                                 "'root'@'localhost'=PASSWORD('');")
+    # This is a suffix MySQL appends to the file name given in
+    # the '--log-error' startup parameter.
+    _ERROR_LOG_SUFFIX = '.err'
+    _ERROR_MESSAGE_PATTERN = re.compile("ERROR")
 
+    def mysql_is_running(self):
         try:
-            LOG.debug("Polling for backup metadata... ")
-            poll_until(_metadata_found, sleep_time=5, time_out=60)
-        except Exception as e:
-            raise RuntimeError(f"Failed to get backup metadata for backup "
-                               f"{self.filename}: {str(e)}")
-
-        return self.metadata
-
-    def check_process(self):
-        # If any of the below variables were not set by either metadata()
-        # or direct retrieval from the pgsql backup commands, then something
-        # has gone wrong
-        if self.is_read_only:
+            utils.execute_with_timeout("/usr/bin/mysqladmin", "ping")
+            LOG.debug("MySQL is up and running.")
             return True
-
-        if not self.start_segment or not self.start_wal_file:
-            LOG.error("Unable to determine starting WAL file/segment")
-            return False
-        if not self.stop_segment or not self.stop_wal_file:
-            LOG.error("Unable to determine ending WAL file/segment")
-            return False
-        if not self.label:
-            LOG.error("No backup label found")
+        except exception.ProcessExecutionError:
+            LOG.debug("MySQL is not running.")
             return False
-        return True
 
-    def check_restore_process(self):
-        LOG.info('Checking return code of postgres restore process.')
-        return_code = self.process.returncode
-        if return_code != 0:
-            LOG.error('postgres process exited with %s', return_code)
+    def mysql_is_not_running(self):
+        try:
+            utils.execute_with_timeout("/usr/bin/pgrep", "mysqld")
+            LOG.info(_("MySQL is still running."))
             return False
-        return True
-
+        except exception.ProcessExecutionError:
+            LOG.debug("MySQL is not running.")
+            return True
 
-class PgBasebackupIncremental(PgBasebackup):
-    """Incremental backup/restore for PostgreSQL.
+    def poll_until_then_raise(self, event, exc):
+        try:
+            utils.poll_until(event,
+                             sleep_time=self.RESET_ROOT_SLEEP_INTERVAL,
+                             time_out=self.RESET_ROOT_RETRY_TIMEOUT)
+        except exception.PollTimeOut:
+            raise exc
+
+    def _start_mysqld_safe_with_init_file(self, init_file, err_log_file):
+        child = pexpect.spawn(
+            "sudo mysqld_safe --init-file=%s --log-error=%s" %
+            (init_file.name, err_log_file.name))
+        try:
+            index = child.expect(['Starting mysqld daemon'])
+            if index == 0:
+                LOG.info(_("Starting MySQL"))
+        except pexpect.TIMEOUT:
+            LOG.exception(_("Got a timeout launching mysqld_safe"))
+        finally:
+            # There is a race condition here where we kill mysqld before
+            # the init file been executed. We need to ensure mysqld is up.
+            #
+            # mysqld_safe will start even if init-file statement(s) fail.
+            # We therefore also check for errors in the log file.
+            self.poll_until_then_raise(
+                self.mysql_is_running,
+                base.RestoreError("Reset root password failed:"
+                                  " mysqld did not start!"))
+            first_err_message = self._find_first_error_message(err_log_file)
+            if first_err_message:
+                raise base.RestoreError("Reset root password failed: %s"
+                                        % first_err_message)
+
+            LOG.info(_("Root password reset successfully."))
+            LOG.debug("Cleaning up the temp mysqld process.")
+            utils.execute_with_timeout("mysqladmin", "-uroot",
+                                       "--protocol=tcp", "shutdown")
+            LOG.debug("Polling for shutdown to complete.")
+            try:
+                utils.poll_until(self.mysql_is_not_running,
+                                 sleep_time=self.RESET_ROOT_SLEEP_INTERVAL,
+                                 time_out=self.RESET_ROOT_RETRY_TIMEOUT)
+                LOG.debug("Database successfully shutdown")
+            except exception.PollTimeOut:
+                LOG.debug("Timeout shutting down database "
+                          "- performing killall on mysqld_safe.")
+                utils.execute_with_timeout("killall", "mysqld_safe",
+                                           root_helper="sudo",
+                                           run_as_root=True)
+                self.poll_until_then_raise(
+                    self.mysql_is_not_running,
+                    base.RestoreError("Reset root password failed: "
+                                      "mysqld did not stop!"))
+
+    def reset_root_password(self):
+        """Reset the password of the localhost root account used by Trove
+        for initial datastore configuration.
+        """
 
-    To restore an incremental backup from a previous backup, in PostgreSQL,
-    is effectively to replay the WAL entries to a designated point in time.
-    All that is required is the most recent base backup, and all WAL files
-    """
+        with tempfile.NamedTemporaryFile(mode='w') as init_file:
+            operating_system.write_file(init_file.name,
+                                        self.RESET_ROOT_MYSQL_COMMANDS)
+            operating_system.chmod(init_file.name, FileMode.ADD_READ_ALL,
+                                   as_root=True)
+            # Do not attempt to delete the file as the 'trove' user.
+            # The process writing into it may have assumed its ownership.
+            # Only owners can delete temporary
+            # files (restricted deletion).
+            err_log_file = tempfile.NamedTemporaryFile(
+                suffix=self._ERROR_LOG_SUFFIX,
+                delete=False)
+            try:
+                self._start_mysqld_safe_with_init_file(init_file, err_log_file)
+            finally:
+                err_log_file.close()
+                operating_system.remove(
+                    err_log_file.name, force=True, as_root=True)
+
+    def _find_first_error_message(self, fp):
+        if self._is_non_zero_file(fp):
+            return self._find_first_pattern_match(
+                fp, self._ERROR_MESSAGE_PATTERN)
+        return None
+
+    def _is_non_zero_file(self, fp):
+        file_path = fp.name
+        return os.path.isfile(file_path) and (os.path.getsize(file_path) > 0)
+
+    def _find_first_pattern_match(self, fp, pattern):
+        for line in fp:
+            if pattern.match(line):
+                return line
+        return None
+
+
+class MySQLDump(base.RestoreRunner, MySQLRestoreMixin):
+    """Implementation of Restore Strategy for MySQLDump."""
+    __strategy_name__ = 'mysqldump'
+    base_restore_cmd = 'sudo mysql'
+
+
+class InnoBackupEx(base.RestoreRunner, MySQLRestoreMixin):
+    """Implementation of Restore Strategy for InnoBackupEx."""
+    __strategy_name__ = 'innobackupex'
+    base_restore_cmd = 'sudo xbstream -x -C %(restore_location)s'
+    base_prepare_cmd = ('sudo innobackupex'
+                        ' --defaults-file=%(restore_location)s/backup-my.cnf'
+                        ' --ibbackup=xtrabackup'
+                        ' --apply-log'
+                        ' %(restore_location)s'
+                        ' 2>/tmp/innoprepare.log')
 
     def __init__(self, *args, **kwargs):
-        self.parent_location = kwargs.pop('parent_location', '')
-        self.parent_checksum = kwargs.pop('parent_checksum', '')
-        self.parent_stop_wal = kwargs.pop('stop_wal_file', '')
-
-        super(PgBasebackupIncremental, self).__init__(*args, **kwargs)
-
-        self.incr_restore_cmd = f'tar -xzf - -C {self.wal_archive_dir}'
-
-    def pre_backup(self):
-        # Check if the parent stop wal file still exists. It may be removed
-        # by trove-guestagent.
-        parent_wal_name = self.get_backup_file(
-            backup_pos=0, regex=fr'{self.parent_stop_wal}\..+\.backup')
-        if not parent_wal_name:
-            raise Exception("Cannot find parent backup WAL file.")
-
-        with psql_util.PostgresConnection('postgres') as conn:
-            self.start_segment = conn.query(
-                f"SELECT pg_start_backup('{self.filename}', false, false)"
-            )[0][0]
-            self.start_wal_file = conn.query(
-                f"SELECT pg_walfile_name('{self.start_segment}')")[0][0]
-            self.stop_segment = conn.query(
-                "SELECT * FROM pg_stop_backup(false, true)")[0][0]
-
-        # We have to hack this because self.command is
-        # initialized in the base class before we get here, which is
-        # when we will know exactly what WAL files we want to archive
-        self.command = self._cmd()
-
-    def _cmd(self):
-        wal_file_list = self.get_wal_files(backup_pos=1)
-        cmd = (f'tar -czf - -C {self.wal_archive_dir} '
-               f'{" ".join(wal_file_list)}')
-        return cmd + self.encrypt_cmd
-
-    def get_metadata(self):
-        _meta = super(PgBasebackupIncremental, self).get_metadata()
-        _meta.update({
-            'parent_location': self.parent_location,
-            'parent_checksum': self.parent_checksum,
-        })
-        return _meta
-
-    def incremental_restore_cmd(self, incr=False):
-        cmd = self.restore_command
-        if incr:
-            cmd = self.incr_restore_cmd
-        return self.decrypt_cmd + cmd
+        self._app = None
+        super(InnoBackupEx, self).__init__(*args, **kwargs)
+        self.prepare_cmd = self.base_prepare_cmd % kwargs
+        self.prep_retcode = None
 
-    def incremental_restore(self, location, checksum):
-        """Perform incremental restore.
+    @property
+    def app(self):
+        if self._app is None:
+            self._app = self._build_app()
+        return self._app
+
+    def _build_app(self):
+        return dbaas.MySqlApp(dbaas.MySqlAppStatus.get())
+
+    def pre_restore(self):
+        self.app.stop_db()
+        LOG.info(_("Cleaning out restore location: %s."),
+                 self.restore_location)
+        operating_system.chmod(self.restore_location, FileMode.SET_FULL,
+                               as_root=True)
+        utils.clean_out(self.restore_location)
+
+    def _run_prepare(self):
+        LOG.debug("Running innobackupex prepare: %s.", self.prepare_cmd)
+        self.prep_retcode = utils.execute(self.prepare_cmd, shell=True)
+        LOG.info(_("Innobackupex prepare finished successfully."))
+
+    def post_restore(self):
+        self._run_prepare()
+        operating_system.chown(self.restore_location, 'mysql', None,
+                               force=True, as_root=True)
+        self._delete_old_binlogs()
+        self.reset_root_password()
+        self.app.start_mysql()
+
+    def _delete_old_binlogs(self):
+        files = glob.glob(os.path.join(self.restore_location, "ib_logfile*"))
+        for f in files:
+            os.unlink(f)
+
+
+class InnoBackupExIncremental(InnoBackupEx):
+    __strategy_name__ = 'innobackupexincremental'
+    incremental_prep = ('sudo innobackupex'
+                        ' --defaults-file=%(restore_location)s/backup-my.cnf'
+                        ' --ibbackup=xtrabackup'
+                        ' --apply-log'
+                        ' --redo-only'
+                        ' %(restore_location)s'
+                        ' %(incremental_args)s'
+                        ' 2>/tmp/innoprepare.log')
+
+    def __init__(self, *args, **kwargs):
+        super(InnoBackupExIncremental, self).__init__(*args, **kwargs)
+        self.restore_location = kwargs.get('restore_location')
+        self.content_length = 0
+
+    def _incremental_restore_cmd(self, incremental_dir):
+        """Return a command for a restore with a incremental location."""
+        args = {'restore_location': incremental_dir}
+        return (self.decrypt_cmd +
+                self.unzip_cmd +
+                (self.base_restore_cmd % args))
+
+    def _incremental_prepare_cmd(self, incremental_dir):
+        if incremental_dir is not None:
+            incremental_arg = '--incremental-dir=%s' % incremental_dir
+        else:
+            incremental_arg = ''
 
-        For the child backups, restore the wal files to wal archive dir.
-        For the base backup, restore to datadir.
+        args = {
+            'restore_location': self.restore_location,
+            'incremental_args': incremental_arg,
+        }
+
+        return self.incremental_prep % args
+
+    def _incremental_prepare(self, incremental_dir):
+        prepare_cmd = self._incremental_prepare_cmd(incremental_dir)
+        LOG.debug("Running innobackupex prepare: %s.", prepare_cmd)
+        utils.execute(prepare_cmd, shell=True)
+        LOG.info(_("Innobackupex prepare finished successfully."))
+
+    def _incremental_restore(self, location, checksum):
+        """Recursively apply backups from all parents.
+
+        If we are the parent then we restore to the restore_location and
+        we apply the logs to the restore_location only.
+
+        Otherwise if we are an incremental we restore to a subfolder to
+        prevent stomping on the full restore data. Then we run apply log
+        with the '--incremental-dir' flag
         """
         metadata = self.storage.load_metadata(location, checksum)
+        incremental_dir = None
         if 'parent_location' in metadata:
-            LOG.info("Restoring parent: %(parent_location)s, "
-                     "checksum: %(parent_checksum)s.", metadata)
-
+            LOG.info(_("Restoring parent: %(parent_location)s"
+                       " checksum: %(parent_checksum)s."), metadata)
             parent_location = metadata['parent_location']
             parent_checksum = metadata['parent_checksum']
-
             # Restore parents recursively so backup are applied sequentially
-            self.incremental_restore(parent_location, parent_checksum)
-
-            command = self.incremental_restore_cmd(incr=True)
+            self._incremental_restore(parent_location, parent_checksum)
+            # for *this* backup set the incremental_dir
+            # just use the checksum for the incremental path as it is
+            # sufficiently unique /var/lib/mysql/<checksum>
+            incremental_dir = os.path.join(
+                cfg.get_configuration_property('mount_point'), checksum)
+            operating_system.create_directory(incremental_dir, as_root=True)
+            command = self._incremental_restore_cmd(incremental_dir)
         else:
-            # For the parent base backup, revert to the default restore cmd
-            LOG.info("Restoring back to full backup.")
-            command = self.incremental_restore_cmd(incr=False)
-
-        self.restore_content_length += self.unpack(location, checksum, command)
-
-    def run_restore(self):
-        """Run incremental restore."""
-        LOG.debug('Running incremental restore')
-        self.incremental_restore(self.location, self.checksum)
-        return self.restore_content_length
+            # The parent (full backup) use the same command from InnobackupEx
+            # super class and do not set an incremental_dir.
+            command = self.restore_cmd
+
+        self.content_length += self._unpack(location, checksum, command)
+        self._incremental_prepare(incremental_dir)
+
+        # Delete unpacked incremental backup metadata
+        if incremental_dir:
+            operating_system.remove(incremental_dir, force=True, as_root=True)
+
+    def _run_restore(self):
+        """Run incremental restore.
+
+        First grab all parents and prepare them with '--redo-only'. After
+        all backups are restored the super class InnoBackupEx post_restore
+        method is called to do the final prepare with '--apply-log'
+        """
+        self._incremental_restore(self.location, self.checksum)
+        return self.content_length
```

### Comparing `trove-21.0.0.0rc2/backup/storage/base.py` & `trove-8.0.1/trove/tests/tempest/services/database/json/versions_client.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,48 +1,37 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2014 OpenStack Foundation
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-
-import abc
-
-
-class Storage(object):
-    """Base class for Storage driver implementation."""
-
-    @abc.abstractmethod
-    def save(self, stream, metadata=None, **kwargs):
-        """Persist information from the stream.
-
-        Should return the new backup checkshum and location.
-        """
-
-    @abc.abstractmethod
-    def load(self, location, backup_checksum, **kwargs):
-        """Load a stream from the data location.
-
-        Should return an object that provides "read" method.
-        """
-
-    def load_metadata(self, parent_location, parent_checksum):
-        """Load metadata for a parent backup.
-
-        It's up to the storage driver to decide how to implement this function.
-        """
-        return {}
-
-    def is_incremental_backup(self, location):
-        """Check if the location is an incremental backup."""
-        return False
-
-    @abc.abstractmethod
-    def get_backup_lsn(self, location):
-        """Get the backup LSN."""
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+from oslo_serialization import jsonutils as json
+from six.moves.urllib import parse as urllib
+from tempest.lib.common import rest_client
+
+
+class DatabaseVersionsClient(rest_client.RestClient):
+
+    def __init__(self, auth_provider, service, region, **kwargs):
+        super(DatabaseVersionsClient, self).__init__(
+            auth_provider, service, region, **kwargs)
+        self.skip_path()
+
+    def list_db_versions(self, params=None):
+        """List all versions."""
+        url = ''
+        if params:
+            url += '?%s' % urllib.urlencode(params)
+
+        resp, body = self.get(url)
+        self.expected_success(200, resp.status)
+        body = json.loads(body)
+        return rest_client.ResponseBody(resp, body)
```

### Comparing `trove-21.0.0.0rc2/backup/storage/swift.py` & `trove-8.0.1/trove/common/strategies/storage/swift.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,77 +1,57 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
 
 import hashlib
 import json
 
-from keystoneauth1.identity import v3
-from keystoneauth1 import session
-from oslo_config import cfg
 from oslo_log import log as logging
-import swiftclient
+import six
 
-from backup.storage import base
+from trove.common import cfg
+from trove.common.i18n import _
+from trove.common.remote import create_swift_client
+from trove.common.strategies.storage import base
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
+CHUNK_SIZE = CONF.backup_chunk_size
+MAX_FILE_SIZE = CONF.backup_segment_max_size
+BACKUP_CONTAINER = CONF.backup_swift_container
+
+
+class DownloadError(Exception):
+    """Error running the Swift Download Command."""
 
-def _get_user_keystone_session(auth_url, token, tenant_id):
-    auth = v3.Token(
-        auth_url=auth_url, token=token,
-        project_domain_name="Default",
-        project_id=tenant_id
-    )
-    return session.Session(auth=auth, verify=False)
-
-
-def _get_service_client(auth_url, token, tenant_id, region_name=None):
-    sess = _get_user_keystone_session(auth_url, token, tenant_id)
-    os_options = None
-    if region_name:
-        os_options = {
-            'region_name': region_name
-        }
-    return swiftclient.Connection(session=sess,
-                                  os_options=os_options,
-                                  insecure=True)
-
-
-def _set_attr(original):
-    """Return a swift friendly header key."""
-    key = original.replace('_', '-')
-    return 'X-Object-Meta-%s' % key
-
-
-def _get_attr(original):
-    """Get a friendly name from an object header key."""
-    key = original.replace('-', '_')
-    key = key.replace('x_object_meta_', '')
-    return key
+
+class SwiftDownloadIntegrityError(Exception):
+    """Integrity error while running the Swift Download Command."""
 
 
 class StreamReader(object):
     """Wrap the stream from the backup process and chunk it into segements."""
 
-    def __init__(self, stream, container, filename, max_file_size):
+    def __init__(self, stream, filename, max_file_size=MAX_FILE_SIZE):
         self.stream = stream
-        self.container = container
         self.filename = filename
+        self.container = BACKUP_CONTAINER
         self.max_file_size = max_file_size
         self.segment_length = 0
         self.process = None
         self.file_number = 0
         self.end_of_file = False
         self.end_of_segment = False
         self.segment_checksum = hashlib.md5()
@@ -89,15 +69,15 @@
     def first_segment(self):
         return '%s_%08d' % (self.base_filename, 0)
 
     @property
     def segment_path(self):
         return '%s/%s' % (self.container, self.segment)
 
-    def read(self, chunk_size=2 ** 16):
+    def read(self, chunk_size=CHUNK_SIZE):
         if self.end_of_segment:
             self.segment_length = 0
             self.segment_checksum = hashlib.md5()
             self.end_of_segment = False
 
         # Upload to a new file if we are starting or too large
         if self.segment_length > (self.max_file_size - chunk_size):
@@ -112,195 +92,207 @@
 
         self.segment_checksum.update(chunk)
         self.segment_length += len(chunk)
         return chunk
 
 
 class SwiftStorage(base.Storage):
-    def __init__(self):
-        self.client = _get_service_client(CONF.os_auth_url, CONF.os_token,
-                                          CONF.os_tenant_id,
-                                          region_name=CONF.os_region_name)
-
-    def save(self, stream, metadata=None, container='database_backups'):
-        """Persist data from the stream to swift.
-
-        * Read data from stream, upload to swift
-        * Update the new object metadata, stream provides method to get
-          metadata.
+    """Implementation of Storage Strategy for Swift."""
+    __strategy_name__ = 'swift'
 
-        :returns the new object checkshum and swift full URL.
+    def __init__(self, *args, **kwargs):
+        super(SwiftStorage, self).__init__(*args, **kwargs)
+        self.connection = create_swift_client(self.context)
+
+    def save(self, filename, stream, metadata=None):
+        """Persist information from the stream to swift.
+
+        The file is saved to the location <BACKUP_CONTAINER>/<filename>.
+        It will be a Swift Static Large Object (SLO).
+        The filename is defined on the backup runner manifest property
+        which is typically in the format '<backup_id>.<ext>.gz'
         """
-        filename = stream.manifest
-        LOG.info('Saving %(filename)s to %(container)s in swift.',
-                 {'filename': filename, 'container': container})
+
+        LOG.info(_('Saving %(filename)s to %(container)s in swift.'),
+                 {'filename': filename, 'container': BACKUP_CONTAINER})
 
         # Create the container if it doesn't already exist
-        LOG.debug('Ensuring container %s', container)
-        self.client.put_container(container)
+        LOG.debug('Creating container %s.', BACKUP_CONTAINER)
+        self.connection.put_container(BACKUP_CONTAINER)
 
         # Swift Checksum is the checksum of the concatenated segment checksums
         swift_checksum = hashlib.md5()
+
         # Wrap the output of the backup process to segment it for swift
-        stream_reader = StreamReader(stream, container, filename,
-                                     2 * (1024 ** 3))
+        stream_reader = StreamReader(stream, filename, MAX_FILE_SIZE)
+        LOG.debug('Using segment size %s', stream_reader.max_file_size)
 
-        url = self.client.url
+        url = self.connection.url
         # Full location where the backup manifest is stored
-        location = "%s/%s/%s" % (url, container, filename)
-        LOG.info('Uploading to %s', location)
+        location = "%s/%s/%s" % (url, BACKUP_CONTAINER, filename)
 
         # Information about each segment upload job
         segment_results = []
 
         # Read from the stream and write to the container in swift
         while not stream_reader.end_of_file:
-            LOG.debug('Uploading segment %s.', stream_reader.segment)
+            LOG.debug('Saving segment %s.', stream_reader.segment)
             path = stream_reader.segment_path
-            etag = self.client.put_object(container,
-                                          stream_reader.segment,
-                                          stream_reader)
+            etag = self.connection.put_object(BACKUP_CONTAINER,
+                                              stream_reader.segment,
+                                              stream_reader)
 
             segment_checksum = stream_reader.segment_checksum.hexdigest()
 
             # Check each segment MD5 hash against swift etag
+            # Raise an error and mark backup as failed
             if etag != segment_checksum:
-                msg = ('Failed to upload data segment to swift. ETAG: %(tag)s '
-                       'Segment MD5: %(checksum)s.' %
-                       {'tag': etag, 'checksum': segment_checksum})
-                raise Exception(msg)
+                LOG.error(_("Error saving data segment to swift. "
+                          "ETAG: %(tag)s Segment MD5: %(checksum)s."),
+                          {'tag': etag, 'checksum': segment_checksum})
+                return False, "Error saving data to Swift!", None, location
 
             segment_results.append({
                 'path': path,
                 'etag': etag,
                 'size_bytes': stream_reader.segment_length
             })
 
-            swift_checksum.update(segment_checksum.encode())
+            if six.PY3:
+                swift_checksum.update(segment_checksum.encode())
+            else:
+                swift_checksum.update(segment_checksum)
 
         # All segments uploaded.
         num_segments = len(segment_results)
         LOG.debug('File uploaded in %s segments.', num_segments)
 
         # An SLO will be generated if the backup was more than one segment in
         # length.
         large_object = num_segments > 1
 
         # Meta data is stored as headers
         if metadata is None:
             metadata = {}
-        metadata.update(stream.get_metadata())
+        metadata.update(stream.metadata())
         headers = {}
         for key, value in metadata.items():
-            headers[_set_attr(key)] = value
+            headers[self._set_attr(key)] = value
 
-        LOG.info('Metadata headers: %s', headers)
+        LOG.debug('Metadata headers: %s', str(headers))
         if large_object:
+            LOG.info(_('Creating the manifest file.'))
             manifest_data = json.dumps(segment_results)
-            LOG.info('Creating the SLO manifest file, manifest content: %s',
-                     manifest_data)
+            LOG.debug('Manifest contents: %s', manifest_data)
             # The etag returned from the manifest PUT is the checksum of the
             # manifest object (which is empty); this is not the checksum we
             # want.
-            self.client.put_object(container,
-                                   filename,
-                                   manifest_data,
-                                   query_string='multipart-manifest=put')
+            self.connection.put_object(BACKUP_CONTAINER,
+                                       filename,
+                                       manifest_data,
+                                       query_string='multipart-manifest=put')
 
             # Validation checksum is the Swift Checksum
             final_swift_checksum = swift_checksum.hexdigest()
         else:
-            LOG.info('Moving segment %(segment)s to %(filename)s.',
+            LOG.info(_('Backup fits in a single segment. Moving segment '
+                       '%(segment)s to %(filename)s.'),
                      {'segment': stream_reader.first_segment,
                       'filename': filename})
             segment_result = segment_results[0]
             # Just rename it via a special put copy.
             headers['X-Copy-From'] = segment_result['path']
-            self.client.put_object(container,
-                                   filename, '',
-                                   headers=headers)
-
+            self.connection.put_object(BACKUP_CONTAINER,
+                                       filename, '',
+                                       headers=headers)
             # Delete the old segment file that was copied
-            LOG.info('Deleting the old segment file %s.',
-                     stream_reader.first_segment)
-            try:
-                self.client.delete_object(container,
+            LOG.debug('Deleting the old segment file %s.',
+                      stream_reader.first_segment)
+            self.connection.delete_object(BACKUP_CONTAINER,
                                           stream_reader.first_segment)
-            except swiftclient.exceptions.ClientException as e:
-                if e.http_status != 404:
-                    raise
-
             final_swift_checksum = segment_result['etag']
 
         # Validate the object by comparing checksums
-        resp = self.client.head_object(container, filename)
+        # Get the checksum according to Swift
+        resp = self.connection.head_object(BACKUP_CONTAINER, filename)
         # swift returns etag in double quotes
         # e.g. '"dc3b0827f276d8d78312992cc60c2c3f"'
         etag = resp['etag'].strip('"')
 
         # Raise an error and mark backup as failed
         if etag != final_swift_checksum:
-            msg = ('Failed to upload data to swift. Manifest ETAG: %(tag)s '
-                   'Swift MD5: %(checksum)s' %
-                   {'tag': etag, 'checksum': final_swift_checksum})
-            raise Exception(msg)
+            LOG.error(
+                _("Error saving data to swift. Manifest "
+                  "ETAG: %(tag)s Swift MD5: %(checksum)s"),
+                {'tag': etag, 'checksum': final_swift_checksum})
+            return False, "Error saving data to Swift!", None, location
 
-        return (final_swift_checksum, location)
+        return (True, "Successfully saved data to Swift!",
+                final_swift_checksum, location)
 
     def _explodeLocation(self, location):
         storage_url = "/".join(location.split('/')[:-2])
         container = location.split('/')[-2]
         filename = location.split('/')[-1]
         return storage_url, container, filename
 
     def _verify_checksum(self, etag, checksum):
         etag_checksum = etag.strip('"')
         if etag_checksum != checksum:
-            msg = ('Checksum validation failure, actual: %s, expected: %s' %
-                   (etag_checksum, checksum))
-            raise Exception(msg)
+            msg = (_("Original checksum: %(original)s does not match"
+                     " the current checksum: %(current)s") %
+                   {'original': etag_checksum, 'current': checksum})
+            LOG.error(msg)
+            raise SwiftDownloadIntegrityError(msg)
+        return True
 
     def load(self, location, backup_checksum):
-        """Get object from the location."""
+        """Restore a backup from the input stream to the restore_location."""
         storage_url, container, filename = self._explodeLocation(location)
 
-        headers, contents = self.client.get_object(container, filename,
-                                                   resp_chunk_size=2 ** 16)
+        headers, info = self.connection.get_object(container, filename,
+                                                   resp_chunk_size=CHUNK_SIZE)
 
-        if backup_checksum:
+        if CONF.verify_swift_checksum_on_restore:
             self._verify_checksum(headers.get('etag', ''), backup_checksum)
 
-        return contents
+        return info
+
+    def _get_attr(self, original):
+        """Get a friendly name from an object header key."""
+        key = original.replace('-', '_')
+        key = key.replace('x_object_meta_', '')
+        return key
+
+    def _set_attr(self, original):
+        """Return a swift friendly header key."""
+        key = original.replace('_', '-')
+        return 'X-Object-Meta-%s' % key
 
-    def load_metadata(self, parent_location, parent_checksum):
+    def load_metadata(self, location, backup_checksum):
         """Load metadata from swift."""
-        if not parent_location:
-            return {}
 
-        _, container, filename = self._explodeLocation(parent_location)
-        headers = self.client.head_object(container, filename)
+        storage_url, container, filename = self._explodeLocation(location)
+
+        headers = self.connection.head_object(container, filename)
 
-        if parent_checksum:
-            self._verify_checksum(headers.get('etag', ''), parent_checksum)
+        if CONF.verify_swift_checksum_on_restore:
+            self._verify_checksum(headers.get('etag', ''), backup_checksum)
 
         _meta = {}
         for key, value in headers.items():
             if key.startswith('x-object-meta'):
-                _meta[_get_attr(key)] = value
+                _meta[self._get_attr(key)] = value
 
         return _meta
 
-    def is_incremental_backup(self, location):
-        """Check if the location is an incremental backup."""
-        _, container, filename = self._explodeLocation(location)
-        headers = self.client.head_object(container, filename)
-
-        if 'x-object-meta-parent-location' in headers:
-            return True
-
-        return False
-
-    def get_backup_lsn(self, location):
-        """Get the backup LSN if exists."""
-        _, container, filename = self._explodeLocation(location)
-        headers = self.client.head_object(container, filename)
-        return headers.get('x-object-meta-lsn')
+    def save_metadata(self, location, metadata={}):
+        """Save metadata to a swift object."""
+
+        storage_url, container, filename = self._explodeLocation(location)
+
+        headers = {}
+        for key, value in metadata.items():
+            headers[self._set_attr(key)] = value
+
+        LOG.info(_("Writing metadata: %s"), str(headers))
+        self.connection.post_object(container, filename, headers=headers)
```

### Comparing `trove-21.0.0.0rc2/contrib/trove-guestagent` & `trove-8.0.1/contrib/trove-guestagent`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/python3
+#!/usr/bin/python
 
 # Copyright 2011 OpenStack Foundation
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
```

### Comparing `trove-21.0.0.0rc2/contrib/trove-network-driver` & `trove-8.0.1/trove/taskmanager/service.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,29 +1,22 @@
-#!/usr/bin/python3
-
+# Copyright 2011 OpenStack Foundation
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-import os
-import sys
 
-possible_topdir = os.path.normpath(os.path.join(os.path.abspath(sys.argv[0]),
-                                                os.pardir,
-                                                os.pardir))
-if os.path.exists(os.path.join(possible_topdir, 'trove', '__init__.py')):
-    sys.path.insert(0, possible_topdir)
+class TaskService(object):
+    """Task Manager interface."""
 
-from trove.cmd.network_driver import main
 
-if __name__ == "__main__":
-    sys.exit(main())
+def app_factory(global_conf, **local_conf):
+    return TaskService()
```

### Comparing `trove-21.0.0.0rc2/devstack/README.rst` & `trove-8.0.1/devstack/README.rst`

 * *Files 26% similar despite different names*

```diff
@@ -10,37 +10,36 @@
     control how the client gets installed, set the TROVECLIENT_REPO,
     TROVECLIENT_DIR and TROVECLIENT_BRANCH environment variables appropriately.
 
 
 Download DevStack
 =================
 
-.. code-block:: bash
+.. sourcecode:: bash
 
     export DEVSTACK_DIR=~/devstack
-    git clone https://opendev.org/openstack/devstack.git $DEVSTACK_DIR
+    git clone https://git.openstack.org/openstack-dev/devstack.git $DEVSTACK_DIR
 
 Enable the Trove plugin
 =======================
 
-Enable the plugin by adding the following section to
-``$DEVSTACK_DIR/local.conf``
+Enable the plugin by adding the following section to ``$DEVSTACK_DIR/local.conf``
 
-.. code-block:: bash
+.. sourcecode:: bash
 
      [[local|localrc]]
-     enable_plugin trove https://opendev.org/openstack/trove
+     enable_plugin trove https://git.openstack.org/openstack/trove
 
 Optionally, a git refspec (branch or tag or commit) may be provided as follows:
 
-.. code-block:: bash
+.. sourcecode:: bash
 
      [[local|localrc]]
-     enable_plugin trove https://opendev.org/openstack/trove <refspec>
+     enable_plugin trove https://git.openstack.org/openstack/trove <refspec>
 
 Run the DevStack utility
 ========================
 
-.. code-block:: bash
+.. sourcecode:: bash
 
      cd $DEVSTACK_DIR
      ./stack.sh
```

### Comparing `trove-21.0.0.0rc2/devstack/files/apache-trove-api.template` & `trove-8.0.1/devstack/files/apache-trove-api.template`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 # Watcher API through mod_wsgi.  This version assumes you are
 # running devstack to configure the software.
 
 Listen %TROVE_SERVICE_PORT%
 
 <VirtualHost *:%TROVE_SERVICE_PORT%>
     WSGIDaemonProcess trove-api user=%USER% processes=%APIWORKERS% threads=1 display-name=%{GROUP}
-    WSGIScriptAlias / %TROVE_WSGI_DIR%/app_wsgi.py
+    WSGIScriptAlias / %TROVE_WSGI_DIR%/app.wsgi
     WSGIApplicationGroup %{GLOBAL}
     WSGIProcessGroup trove-api
     WSGIPassAuthorization On
 
     ErrorLogFormat "%M"
     ErrorLog /var/log/%APACHE_NAME%/trove-api.log
     CustomLog /var/log/%APACHE_NAME%/trove-api-access.log combined
@@ -39,9 +39,7 @@
         </IfVersion>
         <IfVersion < 2.4>
             Order allow,deny
             Allow from all
         </IfVersion>
     </Directory>
 </VirtualHost>
-
-%WSGIPYTHONHOME%
```

### Comparing `trove-21.0.0.0rc2/devstack/plugin.sh` & `trove-8.0.1/devstack/plugin.sh`

 * *Files 25% similar despite different names*

```diff
@@ -47,61 +47,52 @@
 
 # Tenant               User       Roles
 # ------------------------------------------------------------------
 # service              trove     admin        # if enabled
 
 function create_trove_accounts {
     if [[ "$ENABLED_SERVICES" =~ "trove" ]]; then
-        create_service_user "trove" "admin"
 
-        # Add trove user to the clouds.yaml
-        CLOUDS_YAML=${CLOUDS_YAML:-/etc/openstack/clouds.yaml}
-        $PYTHON $TOP_DIR/tools/update_clouds_yaml.py \
-            --file $CLOUDS_YAML \
-            --os-cloud trove \
-            --os-region-name $REGION_NAME \
-            $CA_CERT_ARG \
-            --os-auth-url $KEYSTONE_SERVICE_URI \
-            --os-username trove \
-            --os-password $SERVICE_PASSWORD \
-            --os-project-name $SERVICE_PROJECT_NAME
+        create_service_user "trove"
 
         local trove_service=$(get_or_create_service "trove" \
             "database" "Trove Service")
         get_or_create_endpoint $trove_service \
             "$REGION_NAME" \
             "http://$SERVICE_HOST:8779/v1.0/\$(tenant_id)s" \
             "http://$SERVICE_HOST:8779/v1.0/\$(tenant_id)s" \
             "http://$SERVICE_HOST:8779/v1.0/\$(tenant_id)s"
     fi
 }
 
-# Removes all the WSGI related files and restart apache.
-function cleanup_trove_apache_wsgi {
+# _cleanup_trove_apache_wsgi - Removes all the WSGI related files and
+# restart apache.
+function _cleanup_trove_apache_wsgi {
     sudo rm -rf $TROVE_WSGI_DIR
-    sudo rm -f $(apache_site_config_for trove-api)
+    sudo rm -f ${apache_site_config_for trove-api}
     restart_apache_server
 }
 
 # stack.sh entry points
 # ---------------------
 
 # cleanup_trove() - Remove residual data files, anything left over from previous
 # runs that a clean run would need to clean up
 function cleanup_trove {
     # Clean up dirs
+    rm -fr $TROVE_AUTH_CACHE_DIR/*
     rm -fr $TROVE_CONF_DIR/*
 
     if is_service_enabled horizon; then
         cleanup_trove_dashboard
     fi
 
     if [[ "${TROVE_USE_MOD_WSGI}" == "TRUE" ]]; then
         echo "Cleaning up Trove's WSGI setup"
-        cleanup_trove_apache_wsgi
+        _cleanup_trove_apache_wsgi
     fi
 }
 
 
 # cleanup_trove_dashboard() - Remove Trove dashboard files from Horizon
 function cleanup_trove_dashboard {
     rm -f $HORIZON_DIR/openstack_dashboard/local/enabled/_17*database*.py
@@ -160,174 +151,175 @@
         fi
     fi
 
     virt_type=$(iniget $NOVA_CONF libvirt virt_type)
     echo "configure_nova_kvm: using virt_type: ${virt_type} for cpu: ${cpu}."
 }
 
-# Setup WSGI config files for Trove and enable the site
-function config_trove_apache_wsgi {
+# _config_trove_apache_wsgi() - Setup WSGI config files for Trove and
+# enable the site
+function _config_trove_apache_wsgi {
     local trove_apache_conf
 
     sudo mkdir -p ${TROVE_WSGI_DIR}
-    sudo cp $TROVE_DIR/trove/cmd/app_wsgi.py $TROVE_WSGI_DIR/app_wsgi.py
+    sudo cp $TROVE_DIR/trove/cmd/app.wsgi $TROVE_WSGI_DIR/app.wsgi
     trove_apache_conf=$(apache_site_config_for trove-api)
     sudo cp $TROVE_DEVSTACK_FILES/apache-trove-api.template ${trove_apache_conf}
-    local wsgi_venv_config=""
-    if [[ "$GLOBAL_VENV" == "True" ]] ; then
-        wsgi_venv_config="WSGIPythonHome $DEVSTACK_VENV"
-    fi
     sudo sed -e "
         s|%TROVE_SERVICE_PORT%|${TROVE_SERVICE_PORT}|g;
         s|%TROVE_WSGI_DIR%|${TROVE_WSGI_DIR}|g;
         s|%USER%|${STACK_USER}|g;
         s|%APACHE_NAME%|${APACHE_NAME}|g;
         s|%APIWORKERS%|${API_WORKERS}|g;
-        s|%WSGIPYTHONHOME%|${wsgi_venv_config}|g;
     " -i ${trove_apache_conf}
     enable_apache_site trove-api
+    tail_log trove-access /var/log/${APACHE_NAME}/trove-api-access.log
+    tail_log trove-api /var/log/${APACHE_NAME}/trove-api.log
 }
 
 # configure_trove() - Set config files, create data dirs, etc
 function configure_trove {
     setup_develop $TROVE_DIR
 
-    # Temporarily disable re-configuring nova_kvm until
-    # more nodes in the pool can support it without crashing.
-    # configure_nova_kvm
+    configure_nova_kvm
     configure_keystone_token_life
 
     # Create the trove conf dir and cache dirs if they don't exist
-    sudo install -d -o $STACK_USER ${TROVE_CONF_DIR}
+    sudo install -d -o $STACK_USER ${TROVE_CONF_DIR} ${TROVE_AUTH_CACHE_DIR}
+
     # Copy api-paste file over to the trove conf dir
     cp $TROVE_LOCAL_API_PASTE_INI $TROVE_API_PASTE_INI
+
+    # Copy the default policy file over to the trove conf dir
+    cp $TROVE_LOCAL_POLICY_JSON $TROVE_POLICY_JSON
+
+    # (Re)create trove conf files
+    rm -f $TROVE_CONF
+    rm -f $TROVE_TASKMANAGER_CONF
+    rm -f $TROVE_CONDUCTOR_CONF
+
+    TROVE_AUTH_ENDPOINT=$KEYSTONE_AUTH_URI/v$IDENTITY_API_VERSION
+
+    # (Re)create trove api conf file if needed
+    if is_service_enabled tr-api; then
+        # Set common configuration values (but only if they're defined)
+        iniset_conditional $TROVE_CONF DEFAULT max_accepted_volume_size $TROVE_MAX_ACCEPTED_VOLUME_SIZE
+        iniset_conditional $TROVE_CONF DEFAULT max_instances_per_tenant $TROVE_MAX_INSTANCES_PER_TENANT
+        iniset_conditional $TROVE_CONF DEFAULT max_volumes_per_tenant $TROVE_MAX_VOLUMES_PER_TENANT
+
+        iniset $TROVE_CONF DEFAULT rpc_backend "rabbit"
+        iniset $TROVE_CONF DEFAULT control_exchange trove
+        iniset $TROVE_CONF oslo_messaging_rabbit rabbit_hosts $RABBIT_HOST
+        iniset $TROVE_CONF oslo_messaging_rabbit rabbit_password $RABBIT_PASSWORD
+        iniset $TROVE_CONF oslo_messaging_rabbit rabbit_userid $RABBIT_USERID
+
+
+        iniset $TROVE_CONF database connection `database_connection_url trove`
+        iniset $TROVE_CONF DEFAULT default_datastore $TROVE_DATASTORE_TYPE
+        setup_trove_logging $TROVE_CONF
+        iniset $TROVE_CONF DEFAULT trove_api_workers "$API_WORKERS"
+
+        configure_auth_token_middleware $TROVE_CONF trove $TROVE_AUTH_CACHE_DIR
+        iniset $TROVE_CONF DEFAULT trove_auth_url $TROVE_AUTH_ENDPOINT
+    fi
+
     # configure apache related files
     if [[ "${TROVE_USE_MOD_WSGI}" == "TRUE" ]]; then
         echo "Configuring Trove to use mod-wsgi and Apache"
-        config_trove_apache_wsgi
+        _config_trove_apache_wsgi
     fi
-    # (Re)create trove conf files
-    rm -f $TROVE_CONF $TROVE_GUESTAGENT_CONF
 
-    TROVE_AUTH_ENDPOINT=$KEYSTONE_AUTH_URI/v$IDENTITY_API_VERSION
+    # (Re)create trove taskmanager conf file if needed
+    if is_service_enabled tr-tmgr; then
+        # Use these values only if they're set
+        iniset_conditional $TROVE_TASKMANAGER_CONF DEFAULT agent_call_low_timeout $TROVE_AGENT_CALL_LOW_TIMEOUT
+        iniset_conditional $TROVE_TASKMANAGER_CONF DEFAULT agent_call_high_timeout $TROVE_AGENT_CALL_HIGH_TIMEOUT
+        iniset_conditional $TROVE_TASKMANAGER_CONF DEFAULT resize_time_out $TROVE_RESIZE_TIME_OUT
+        iniset_conditional $TROVE_TASKMANAGER_CONF DEFAULT usage_timeout $TROVE_USAGE_TIMEOUT
+        iniset_conditional $TROVE_TASKMANAGER_CONF DEFAULT state_change_wait_time $TROVE_STATE_CHANGE_WAIT_TIME
+
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT rpc_backend "rabbit"
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT control_exchange trove
+        iniset $TROVE_TASKMANAGER_CONF oslo_messaging_rabbit rabbit_hosts $RABBIT_HOST
+        iniset $TROVE_TASKMANAGER_CONF oslo_messaging_rabbit rabbit_password $RABBIT_PASSWORD
+        iniset $TROVE_TASKMANAGER_CONF oslo_messaging_rabbit rabbit_userid $RABBIT_USERID
+
+        iniset $TROVE_TASKMANAGER_CONF database connection `database_connection_url trove`
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT taskmanager_manager trove.taskmanager.manager.Manager
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT nova_proxy_admin_user radmin
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT nova_proxy_admin_tenant_name trove
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT nova_proxy_admin_pass $RADMIN_USER_PASS
+        iniset $TROVE_TASKMANAGER_CONF DEFAULT trove_auth_url $TROVE_AUTH_ENDPOINT
+
+        iniset $TROVE_TASKMANAGER_CONF cassandra tcp_ports 22,7000,7001,7199,9042,9160
+        iniset $TROVE_TASKMANAGER_CONF couchbase tcp_ports 22,8091,8092,4369,11209-11211,21100-21199
+        iniset $TROVE_TASKMANAGER_CONF couchdb tcp_ports 22,5984
+        iniset $TROVE_TASKMANAGER_CONF db2 tcp_ports 22,50000
+        iniset $TROVE_TASKMANAGER_CONF mariadb tcp_ports 22,3306,4444,4567,4568
+        iniset $TROVE_TASKMANAGER_CONF mongodb tcp_ports 22,2500,27017,27019
+        iniset $TROVE_TASKMANAGER_CONF mysql tcp_ports 22,3306
+        iniset $TROVE_TASKMANAGER_CONF percona tcp_ports 22,3306
+        iniset $TROVE_TASKMANAGER_CONF postgresql tcp_ports 22,5432
+        iniset $TROVE_TASKMANAGER_CONF pxc tcp_ports 22,3306,4444,4567,4568
+        iniset $TROVE_TASKMANAGER_CONF redis tcp_ports 22,6379,16379
+        iniset $TROVE_TASKMANAGER_CONF vertica tcp_ports 22,5433,5434,5444,5450,4803
+
+        setup_trove_logging $TROVE_TASKMANAGER_CONF
+    fi
+
+    # (Re)create trove conductor conf file if needed
+    if is_service_enabled tr-cond; then
+        iniset $TROVE_CONDUCTOR_CONF DEFAULT rpc_backend "rabbit"
+        iniset $TROVE_CONDUCTOR_CONF oslo_messaging_rabbit rabbit_hosts $RABBIT_HOST
+        iniset $TROVE_CONDUCTOR_CONF oslo_messaging_rabbit rabbit_password $RABBIT_PASSWORD
+        iniset $TROVE_CONDUCTOR_CONF oslo_messaging_rabbit rabbit_userid $RABBIT_USERID
+
+        iniset $TROVE_CONDUCTOR_CONF database connection `database_connection_url trove`
+        iniset $TROVE_CONDUCTOR_CONF DEFAULT nova_proxy_admin_user radmin
+        iniset $TROVE_CONDUCTOR_CONF DEFAULT nova_proxy_admin_tenant_name trove
+        iniset $TROVE_CONDUCTOR_CONF DEFAULT nova_proxy_admin_pass $RADMIN_USER_PASS
+        iniset $TROVE_CONDUCTOR_CONF DEFAULT trove_auth_url $TROVE_AUTH_ENDPOINT
+        iniset $TROVE_CONDUCTOR_CONF DEFAULT control_exchange trove
 
-    ################################################################ trove conf
-    setup_trove_logging $TROVE_CONF
-    iniset_conditional $TROVE_CONF DEFAULT max_accepted_volume_size $TROVE_MAX_ACCEPTED_VOLUME_SIZE
-    iniset_conditional $TROVE_CONF DEFAULT max_instances_per_tenant $TROVE_MAX_INSTANCES_PER_TENANT
-    iniset_conditional $TROVE_CONF DEFAULT max_volumes_per_tenant $TROVE_MAX_VOLUMES_PER_TENANT
-    iniset_conditional $TROVE_CONF DEFAULT agent_call_low_timeout $TROVE_AGENT_CALL_LOW_TIMEOUT
-    iniset_conditional $TROVE_CONF DEFAULT agent_call_high_timeout $TROVE_AGENT_CALL_HIGH_TIMEOUT
-    iniset_conditional $TROVE_CONF DEFAULT resize_time_out $TROVE_RESIZE_TIME_OUT
-    iniset_conditional $TROVE_CONF DEFAULT usage_timeout $TROVE_USAGE_TIMEOUT
-    iniset_conditional $TROVE_CONF DEFAULT state_change_wait_time $TROVE_STATE_CHANGE_WAIT_TIME
-    iniset_conditional $TROVE_CONF DEFAULT reboot_time_out 300
-    iniset $TROVE_CONF DEFAULT controller_address ${SERVICE_HOST}
-
-    configure_keystone_authtoken_middleware $TROVE_CONF trove
-    iniset $TROVE_CONF service_credentials username trove
-    iniset $TROVE_CONF service_credentials user_domain_name Default
-    iniset $TROVE_CONF service_credentials project_domain_name Default
-    iniset $TROVE_CONF service_credentials password $SERVICE_PASSWORD
-    iniset $TROVE_CONF service_credentials project_name $SERVICE_PROJECT_NAME
-    iniset $TROVE_CONF service_credentials region_name $REGION_NAME
-    iniset $TROVE_CONF service_credentials auth_url $TROVE_AUTH_ENDPOINT
-
-    iniset $TROVE_CONF database connection `database_connection_url trove`
-
-    iniset $TROVE_CONF DEFAULT control_exchange trove
-    iniset $TROVE_CONF DEFAULT transport_url rabbit://$RABBIT_USERID:$RABBIT_PASSWORD@$RABBIT_HOST:5672/
-    iniset $TROVE_CONF DEFAULT trove_api_workers "$API_WORKERS"
-    iniset $TROVE_CONF DEFAULT taskmanager_manager trove.taskmanager.manager.Manager
-    iniset $TROVE_CONF DEFAULT default_datastore $TROVE_DATASTORE_TYPE
-
-    iniset $TROVE_CONF cache enabled true
-    iniset $TROVE_CONF cache backend dogpile.cache.memory
-
-    iniset $TROVE_CONF cassandra tcp_ports 7000,7001,7199,9042,9160
-    iniset $TROVE_CONF couchbase tcp_ports 8091,8092,4369,11209-11211,21100-21199
-    iniset $TROVE_CONF couchdb tcp_ports 5984
-    iniset $TROVE_CONF db2 tcp_ports 50000
-    iniset $TROVE_CONF mariadb tcp_ports 3306,4444,4567,4568
-    iniset $TROVE_CONF mongodb tcp_ports 2500,27017,27019
-    iniset $TROVE_CONF mysql tcp_ports 3306
-    iniset $TROVE_CONF percona tcp_ports 3306
-    iniset $TROVE_CONF postgresql tcp_ports 5432
-    iniset $TROVE_CONF pxc tcp_ports 3306,4444,4567,4568
-    iniset $TROVE_CONF redis tcp_ports 6379,16379
-    iniset $TROVE_CONF vertica tcp_ports 5433,5434,5444,5450,4803
-
-    ################################################################ trove guest agent conf
-    setup_trove_logging $TROVE_GUESTAGENT_CONF
+        setup_trove_logging $TROVE_CONDUCTOR_CONF
+    fi
 
+    # Use these values only if they're set
     iniset_conditional $TROVE_GUESTAGENT_CONF DEFAULT state_change_wait_time $TROVE_STATE_CHANGE_WAIT_TIME
-    iniset_conditional $TROVE_GUESTAGENT_CONF DEFAULT command_process_timeout $TROVE_COMMAND_PROCESS_TIMEOUT
-    iniset $TROVE_GUESTAGENT_CONF DEFAULT transport_url rabbit://$RABBIT_USERID:$RABBIT_PASSWORD@$TROVE_HOST_GATEWAY:5672/
+
+    # Set up Guest Agent conf
+    iniset $TROVE_GUESTAGENT_CONF DEFAULT rpc_backend "rabbit"
+    iniset $TROVE_GUESTAGENT_CONF oslo_messaging_rabbit rabbit_password $RABBIT_PASSWORD
+    iniset $TROVE_GUESTAGENT_CONF oslo_messaging_rabbit rabbit_userid $RABBIT_USERID
+    iniset $TROVE_GUESTAGENT_CONF oslo_messaging_rabbit rabbit_hosts $TROVE_HOST_GATEWAY
+
+    iniset $TROVE_GUESTAGENT_CONF DEFAULT nova_proxy_admin_user radmin
+    iniset $TROVE_GUESTAGENT_CONF DEFAULT nova_proxy_admin_tenant_name trove
+    iniset $TROVE_GUESTAGENT_CONF DEFAULT nova_proxy_admin_pass $RADMIN_USER_PASS
+    iniset $TROVE_GUESTAGENT_CONF DEFAULT trove_auth_url $TROVE_AUTH_ENDPOINT
     iniset $TROVE_GUESTAGENT_CONF DEFAULT control_exchange trove
     iniset $TROVE_GUESTAGENT_CONF DEFAULT ignore_users os_admin
     iniset $TROVE_GUESTAGENT_CONF DEFAULT log_dir /var/log/trove/
     iniset $TROVE_GUESTAGENT_CONF DEFAULT log_file trove-guestagent.log
-    iniset $TROVE_GUESTAGENT_CONF DEFAULT swift_api_insecure false
 
-    iniset $TROVE_GUESTAGENT_CONF service_credentials username trove
-    iniset $TROVE_GUESTAGENT_CONF service_credentials user_domain_name Default
-    iniset $TROVE_GUESTAGENT_CONF service_credentials project_domain_name Default
-    iniset $TROVE_GUESTAGENT_CONF service_credentials password $SERVICE_PASSWORD
-    iniset $TROVE_GUESTAGENT_CONF service_credentials project_name $SERVICE_PROJECT_NAME
-    iniset $TROVE_GUESTAGENT_CONF service_credentials region_name $REGION_NAME
-    iniset $TROVE_GUESTAGENT_CONF service_credentials auth_url $TROVE_AUTH_ENDPOINT
-
-    iniset $TROVE_GUESTAGENT_CONF mysql docker_image ${TROVE_DATABASE_IMAGE_MYSQL}
-    iniset $TROVE_GUESTAGENT_CONF mysql backup_docker_image ${TROVE_DATABASE_BACKUP_IMAGE_MYSQL}
-    iniset $TROVE_GUESTAGENT_CONF mariadb docker_image ${TROVE_DATABASE_IMAGE_MARIADB}
-    iniset $TROVE_GUESTAGENT_CONF mariadb backup_docker_image ${TROVE_DATABASE_BACKUP_IMAGE_MARIADB}
-    iniset $TROVE_GUESTAGENT_CONF postgresql docker_image ${TROVE_DATABASE_IMAGE_POSTGRES}
-    iniset $TROVE_GUESTAGENT_CONF postgresql backup_docker_image ${TROVE_DATABASE_BACKUP_IMAGE_POSTGRES}
-
-    # 1. To avoid 'Connection timed out' error of sudo command inside the guest agent
-    # 2. Config the controller IP address used by guest-agent to download Trove code during initialization (only valid for dev_mode=true).
-    common_cloudinit=/etc/trove/cloudinit/common.cloudinit
-    sudo mkdir -p $(dirname ${common_cloudinit})
-    sudo touch ${common_cloudinit}
-    sudo tee ${common_cloudinit} >/dev/null <<EOF
-#cloud-config
-manage_etc_hosts: "localhost"
-write_files:
-  - path: /etc/trove/controller.conf
-    content: |
-      CONTROLLER=${SERVICE_HOST}
-EOF
-
-    # NOTE(lxkong): Remove this when we support common cloud-init file for all datastores.
-    for datastore in "mysql" "mariadb" "postgresql"
-    do
-        sudo cp ${common_cloudinit} /etc/trove/cloudinit/${datastore}.cloudinit
-    done
+    setup_trove_logging $TROVE_GUESTAGENT_CONF
 }
 
 # install_trove() - Collect source and prepare
 function install_trove {
-    install_package jq
-
-    echo "Changing stack user sudoers"
-    echo "stack ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/60_stack_sh_allow_all
-
     setup_develop $TROVE_DIR
 
     if [[ "${TROVE_USE_MOD_WSGI}" == "TRUE" ]]; then
         echo "Installing apache wsgi"
         install_apache_wsgi
     fi
 
     if is_service_enabled horizon; then
         install_trove_dashboard
     fi
-
-    # Fix iptables rules that prevent amqp connections from the devstack box to the guests
-    sudo iptables -D openstack-INPUT -j REJECT --reject-with icmp-host-prohibited || true
 }
 
 # install_trove_dashboard() - Collect source and prepare
 function install_trove_dashboard {
     git_clone $TROVE_DASHBOARD_REPO $TROVE_DASHBOARD_DIR $TROVE_DASHBOARD_BRANCH
     setup_develop $TROVE_DASHBOARD_DIR
     cp $TROVE_DASHBOARD_DIR/trove_dashboard/enabled/_17*database*.py $HORIZON_DIR/openstack_dashboard/local/enabled
@@ -337,374 +329,277 @@
 function install_python_troveclient {
     if use_library_from_git "python-troveclient"; then
         git_clone $TROVE_CLIENT_REPO $TROVE_CLIENT_DIR $TROVE_CLIENT_BRANCH
         setup_develop $TROVE_CLIENT_DIR
     fi
 }
 
-function init_trove_db {
+# init_trove() - Initializes Trove Database as a Service
+function init_trove {
     # (Re)Create trove db
     recreate_database trove
 
     # Initialize the trove database
     $TROVE_MANAGE db_sync
-}
 
-function create_mgmt_subnet_v4 {
+    # Add an admin user to the 'tempest' alt_demo tenant.
+    # This is needed to test the guest_log functionality.
+    # The first part mimics the tempest setup, so make sure we have that.
+    ALT_USERNAME=${ALT_USERNAME:-alt_demo}
+    ALT_TENANT_NAME=${ALT_TENANT_NAME:-alt_demo}
+    ALT_TENANT_ID=$(get_or_create_project ${ALT_TENANT_NAME} default)
+    get_or_create_user ${ALT_USERNAME} "$ADMIN_PASSWORD" "default" "alt_demo@example.com"
+    get_or_add_user_project_role Member ${ALT_USERNAME} ${ALT_TENANT_NAME}
+
+    # The second part adds an admin user to the tenant.
+    ADMIN_ALT_USERNAME=${ADMIN_ALT_USERNAME:-admin_${ALT_USERNAME}}
+    get_or_create_user ${ADMIN_ALT_USERNAME} "$ADMIN_PASSWORD" "default" "admin_alt_demo@example.com"
+    get_or_add_user_project_role admin ${ADMIN_ALT_USERNAME} ${ALT_TENANT_NAME}
+    # Now add these credentials to the clouds.yaml file
+    ADMIN_ALT_DEMO_CLOUD=devstack-alt-admin
+    CLOUDS_YAML=${CLOUDS_YAML:-/etc/openstack/clouds.yaml}
+    $TOP_DIR/tools/update_clouds_yaml.py \
+        --file ${CLOUDS_YAML} \
+        --os-cloud ${ADMIN_ALT_DEMO_CLOUD} \
+        --os-region-name ${REGION_NAME} \
+        --os-identity-api-version 3 \
+        ${CA_CERT_ARG} \
+        --os-auth-url ${KEYSTONE_AUTH_URI} \
+        --os-username ${ADMIN_ALT_USERNAME} \
+        --os-password ${ADMIN_PASSWORD} \
+        --os-project-name ${ALT_TENANT_NAME}
+
+    # If no guest image is specified, skip remaining setup
+    [ -z "$TROVE_GUEST_IMAGE_URL" ] && return 0
+
+    # Find the glance id for the trove guest image
+    # The image is uploaded by stack.sh -- see $IMAGE_URLS handling
+    GUEST_IMAGE_NAME=$(basename "$TROVE_GUEST_IMAGE_URL")
+    GUEST_IMAGE_NAME=${GUEST_IMAGE_NAME%.*}
+
+    TOKEN=$(openstack token issue -c id -f value)
+    TROVE_GUEST_IMAGE_ID=$(openstack --os-token $TOKEN --os-url $GLANCE_SERVICE_PROTOCOL://$GLANCE_HOSTPORT image list | grep "${GUEST_IMAGE_NAME}" | get_field 1)
+    if [ -z "$TROVE_GUEST_IMAGE_ID" ]; then
+        # If no glance id is found, skip remaining setup
+        echo "Datastore ${TROVE_DATASTORE_TYPE} will not be created: guest image ${GUEST_IMAGE_NAME} not found."
+        return 1
+    fi
+
+    # Now that we have the guest image id, initialize appropriate datastores / datastore versions
+    $TROVE_MANAGE datastore_update "$TROVE_DATASTORE_TYPE" ""
+    $TROVE_MANAGE datastore_version_update "$TROVE_DATASTORE_TYPE" "$TROVE_DATASTORE_VERSION" "$TROVE_DATASTORE_TYPE" \
+        "$TROVE_GUEST_IMAGE_ID" "$TROVE_DATASTORE_PACKAGE" 1
+    $TROVE_MANAGE datastore_version_update "$TROVE_DATASTORE_TYPE" "inactive_version" "inactive_manager" "$TROVE_GUEST_IMAGE_ID" "" 0
+    $TROVE_MANAGE datastore_update "$TROVE_DATASTORE_TYPE" "$TROVE_DATASTORE_VERSION"
+    $TROVE_MANAGE datastore_update "Inactive_Datastore" ""
+
+    # Some datastores provide validation rules.
+    # if one is provided, configure it.
+    if [ -f "${TROVE_DIR}/trove/templates/${TROVE_DATASTORE_TYPE}"/validation-rules.json ]; then
+        echo "Configuring validation rules for ${TROVE_DATASTORE_TYPE}"
+        $TROVE_MANAGE db_load_datastore_config_parameters \
+            "$TROVE_DATASTORE_TYPE" "$TROVE_DATASTORE_VERSION" \
+            "${TROVE_DIR}/trove/templates/${TROVE_DATASTORE_TYPE}"/validation-rules.json
+    fi
+}
+
+# Create private IPv4 subnet
+# Note: This was taken from devstack:lib/neutron_plugins/services/l3 and will need to be maintained
+function _create_private_subnet_v4 {
     local project_id=$1
     local net_id=$2
-    local name=$3
-    local ip_range=$4
-    local gateway=$5
+    local name=${3:-$PRIVATE_SUBNET_NAME}
+    local os_cloud=${4:-devstack-admin}
 
-    subnet_id=$(openstack subnet create --project ${project_id} --ip-version 4 --subnet-range ${ip_range} --gateway ${gateway} --dns-nameserver 8.8.8.8 --network ${net_id} $name -c id -f value)
-    die_if_not_set $LINENO subnet_id "Failed to create private IPv4 subnet for network: ${net_id}, project: ${project_id}"
+    local subnet_params="--project $project_id "
+    subnet_params+="--ip-version 4 "
+    if [[ -n "$NETWORK_GATEWAY" ]]; then
+        subnet_params+="--gateway $NETWORK_GATEWAY "
+    fi
+    if [ -n $SUBNETPOOL_V4_ID ]; then
+        subnet_params+="--subnet-pool $SUBNETPOOL_V4_ID "
+    else
+        subnet_params+="--subnet-range $FIXED_RANGE "
+    fi
+    subnet_params+="--network $net_id $name"
+    local subnet_id
+    subnet_id=$(openstack --os-cloud $os_cloud --os-region "$REGION_NAME" subnet create $subnet_params | grep ' id ' | get_field 2)
+    die_if_not_set $LINENO subnet_id "Failure creating private IPv4 subnet for $project_id"
     echo $subnet_id
 }
 
 # Create private IPv6 subnet
-# Note: Trove is not fully tested in IPv6.
-function create_subnet_v6 {
+# Note: This was taken from devstack:lib/neutron_plugins/services/l3 and will need to be maintained
+function _create_private_subnet_v6 {
     local project_id=$1
     local net_id=$2
-    local name=$3
-    local subnet_params="--ip-version 6 "
+    local name=${3:-$IPV6_PRIVATE_SUBNET_NAME}
+    local os_cloud=${4:-devstack-admin}
 
     die_if_not_set $LINENO IPV6_RA_MODE "IPV6 RA Mode not set"
     die_if_not_set $LINENO IPV6_ADDRESS_MODE "IPV6 Address Mode not set"
     local ipv6_modes="--ipv6-ra-mode $IPV6_RA_MODE --ipv6-address-mode $IPV6_ADDRESS_MODE"
-
+    local subnet_params="--project $project_id "
+    subnet_params+="--ip-version 6 "
     if [[ -n "$IPV6_PRIVATE_NETWORK_GATEWAY" ]]; then
         subnet_params+="--gateway $IPV6_PRIVATE_NETWORK_GATEWAY "
     fi
-    if [[ -n $SUBNETPOOL_V6_ID ]]; then
+    if [ -n $SUBNETPOOL_V6_ID ]; then
         subnet_params+="--subnet-pool $SUBNETPOOL_V6_ID "
     else
         subnet_params+="--subnet-range $FIXED_RANGE_V6 $ipv6_modes} "
     fi
     subnet_params+="--network $net_id $name "
-
-    ipv6_subnet_id=$(openstack --project ${project_id} subnet create $subnet_params | grep ' id ' | get_field 2)
-    die_if_not_set $LINENO ipv6_subnet_id "Failed to create private IPv6 subnet for network: ${net_id}, project: ${project_id}"
+    local ipv6_subnet_id
+    ipv6_subnet_id=$(openstack --os-cloud $os_cloud --os-region "$REGION_NAME" subnet create $subnet_params | grep ' id ' | get_field 2)
+    die_if_not_set $LINENO ipv6_subnet_id "Failure creating private IPv6 subnet for $project_id"
     echo $ipv6_subnet_id
 }
 
-function setup_mgmt_network() {
-    local PROJECT_ID=$1
-    local NET_NAME=$2
-    local SUBNET_NAME=$3
-    local SUBNET_RANGE=$4
-    local SUBNET_GATEWAY=$5
-    local SHARED=$6
-
-    local share_flag=""
-    if [[ "${SHARED}" == "TRUE" ]]; then
-        share_flag="--share"
-    fi
-
-    network_id=$(openstack network create --project ${PROJECT_ID} ${share_flag} $NET_NAME -c id -f value)
-    die_if_not_set $LINENO network_id "Failed to create network: $NET_NAME, project: ${PROJECT_ID}"
+# Set up a network on the alt_demo tenant.  Requires ROUTER_ID, REGION_NAME and IP_VERSION to be set
+function set_up_network() {
+    local CLOUD_USER=$1
+    local PROJECT_ID=$2
+    local NET_NAME=$3
+    local SUBNET_NAME=$4
+    local IPV6_SUBNET_NAME=$5
 
+    NEW_NET_ID=$(openstack --os-cloud ${CLOUD_USER} --os-region "$REGION_NAME" network create --project ${PROJECT_ID} "$NET_NAME" | grep ' id ' | get_field 2)
     if [[ "$IP_VERSION" =~ 4.* ]]; then
-        net_subnet_id=$(create_mgmt_subnet_v4 ${PROJECT_ID} ${network_id} ${SUBNET_NAME} ${SUBNET_RANGE} ${SUBNET_GATEWAY})
-        if [[ ${SUBNET_GATEWAY} != "none" ]]; then
-            openstack router add subnet ${ROUTER_ID} ${net_subnet_id}
-        fi
-    fi
-
-    # Trove doesn't support IPv6 for now.
-#    if [[ "$IP_VERSION" =~ .*6 ]]; then
-#        NEW_IPV6_SUBNET_ID=$(create_subnet_v6 ${PROJECT_ID} ${network_id} ${IPV6_SUBNET_NAME})
-#        openstack router add subnet $ROUTER_ID $NEW_IPV6_SUBNET_ID
-#    fi
-}
-
-# start_trove() - Start running processes, including screen
-function start_trove {
-    if [[ ${TROVE_USE_MOD_WSGI}" == TRUE" ]]; then
-        echo "Restarting Apache server ..."
-        enable_apache_site trove-api
-        restart_apache_server
-    else
-        run_process tr-api "$TROVE_BIN_DIR/trove-api --config-file=$TROVE_CONF"
+        NEW_SUBNET_ID=$(_create_private_subnet_v4 ${PROJECT_ID} ${NEW_NET_ID} ${SUBNET_NAME} ${CLOUD_USER})
+        openstack --os-cloud ${CLOUD_USER} --os-region "$REGION_NAME" router add subnet $ROUTER_ID $NEW_SUBNET_ID
     fi
-    run_process tr-tmgr "$TROVE_BIN_DIR/trove-taskmanager --config-file=$TROVE_CONF"
-    run_process tr-cond "$TROVE_BIN_DIR/trove-conductor --config-file=$TROVE_CONF"
-}
-
-# stop_trove() - Stop running processes
-function stop_trove {
-    # Kill the trove screen windows
-    local serv
-    if [[ ${TROVE_USE_MOD_WSGI} == "TRUE" ]]; then
-        echo "Disabling Trove API in Apache"
-        disable_apache_site trove-api
-    else
-        stop_process tr-api
+    if [[ "$IP_VERSION" =~ .*6 ]]; then
+        NEW_IPV6_SUBNET_ID=$(_create_private_subnet_v6 ${PROJECT_ID} ${NEW_NET_ID} ${IPV6_SUBNET_NAME} ${CLOUD_USER})
+        openstack --os-cloud ${CLOUD_USER} --os-region "$REGION_NAME" router add subnet $ROUTER_ID $NEW_IPV6_SUBNET_ID
     fi
-    for serv in tr-tmgr tr-cond; do
-        stop_process $serv
-    done
-}
 
-# configure_tempest_for_trove() - Set Trove related setting on Tempest
-# NOTE (gmann): Configure all the Tempest setting for Trove service in
-# this function.
-function configure_tempest_for_trove {
-    if is_service_enabled tempest; then
-        iniset $TEMPEST_CONFIG service_available trove True
-    fi
+    echo $NEW_NET_ID
 }
 
-# Use trovestack to create guest image and register the image in the datastore.
-function create_guest_image {
-    TROVE_ENABLE_IMAGE_BUILD=`echo ${TROVE_ENABLE_IMAGE_BUILD,,}`
-    if [[ ${TROVE_ENABLE_IMAGE_BUILD} == "false" ]]; then
-        echo "Skip creating guest image."
-        return 0
-    fi
-
-    image_name="trove-guest-${TROVE_IMAGE_OS}-${TROVE_IMAGE_OS_RELEASE}"
-    mkdir -p $HOME/images
-    image_file=$HOME/images/${image_name}.qcow2
-
-    if [[ -n ${TROVE_NON_DEV_IMAGE_URL} ]]; then
-        echo "Downloading guest image from ${TROVE_NON_DEV_IMAGE_URL}"
-        curl -sSL ${TROVE_NON_DEV_IMAGE_URL} -o ${image_file}
-    else
-        echo "Starting to create guest image"
-        export SYNC_LOG_TO_CONTROLLER=${SYNC_LOG_TO_CONTROLLER:-"False"}
-        $DEST/trove/integration/scripts/trovestack \
-          build-image \
-          ${TROVE_IMAGE_OS} \
-          ${TROVE_IMAGE_OS_RELEASE} \
-          true \
-          ${TROVE_IMAGE_OS} \
-          ${image_file}
-    fi
-
-    if [[ ! -f ${image_file} ]]; then
-        echo "Image file was not found at ${image_file}"
-        exit 1
-    fi
-
-    echo "Add the image to glance"
-    glance_image_id=$(openstack --os-cloud trove \
-      image create ${image_name} \
-      --disk-format qcow2 --container-format bare \
-      --tag trove \
-      --property hw_rng_model='virtio' \
-      --file ${image_file} \
-      --debug \
-      -c id -f value)
-     echo "Glance image ${glance_image_id} uploaded"
-
-    echo "Register the image in datastore"
-    $TROVE_MANAGE datastore_update $TROVE_DATASTORE_TYPE ""
-    $TROVE_MANAGE datastore_version_update $TROVE_DATASTORE_TYPE $TROVE_DATASTORE_VERSION $TROVE_DATASTORE_TYPE "" "" 1 --image-tags trove
-    $TROVE_MANAGE datastore_update $TROVE_DATASTORE_TYPE $TROVE_DATASTORE_VERSION
-
-    echo "Add parameter validation rules if available"
-    if [[ -f $DEST/trove/trove/templates/$TROVE_DATASTORE_TYPE/validation-rules.json ]]; then
-        $TROVE_MANAGE db_load_datastore_config_parameters "$TROVE_DATASTORE_TYPE" "$TROVE_DATASTORE_VERSION" \
-            $DEST/trove/trove/templates/$TROVE_DATASTORE_TYPE/validation-rules.json
-    fi
-    # NOTE(wuchunyang): Create log directory so that guest agent can rsync logs to this directory
-    if [[ ${SYNC_LOG_TO_CONTROLLER} == "True" ]]; then
-    test -e /var/log/guest-agent-logs || sudo mkdir -p /var/log/guest-agent-logs/ && sudo chmod 777 /var/log/guest-agent-logs
-    fi
-}
-
-function create_registry_container {
-    # install docker on the host.
-    local ret='0'
-    which docker >/dev/null 2>&1 || { local ret='1'; }
-    if [[ "$ret" -ne 0 ]]; then
-        echo "Installing docker on the host"
-        $DEST/trove/integration/scripts/trovestack install-docker
-    fi
-    # running a docker registry container
-    echo "Running a docker registry container..."
-    container=$(sudo docker ps -a --format "{{.Names}}" --filter name=registry)
-    if [ -z $container ]; then
-        sudo docker run -d --net=host -e REGISTRY_HTTP_ADDR=0.0.0.0:4000 --restart=always -v /opt/trove_registry/:/var/lib/registry --name registry registry:2
-        for img in {"mysql:5.7","mysql:8.0","mariadb:10.4","postgres:12"};
-        do
-        sudo docker pull ${img} && sudo docker tag ${img} 127.0.0.1:4000/trove-datastores/${img} && sudo docker push 127.0.0.1:4000/trove-datastores/${img}
-        done
-        pushd $DEST/trove/backup
-        # build backup images
-        sudo docker build --network host -t 127.0.0.1:4000/trove-datastores/db-backup-mysql:5.7 --build-arg DATASTORE=mysql --build-arg DATASTORE_VERSION=5.7 .
-        sudo docker build --network host -t 127.0.0.1:4000/trove-datastores/db-backup-mysql:8.0 --build-arg DATASTORE=mysql --build-arg DATASTORE_VERSION=8.0 .
-        sudo docker build --network host -t 127.0.0.1:4000/trove-datastores/db-backup-mariadb:10.4 --build-arg DATASTORE=mariadb --build-arg DATASTORE_VERSION=10.4 .
-        sudo docker build --network host -t 127.0.0.1:4000/trove-datastores/db-backup-postgresql:12 --build-arg DATASTORE=postgresql --build-arg DATASTORE_VERSION=12 .
-        popd
-        # push backup images
-        for backupimg in {"db-backup-mysql:5.7","db-backup-mysql:8.0","db-backup-mariadb:10.4","db-backup-postgresql:12"};
-        do
-        sudo docker push 127.0.0.1:4000/trove-datastores/${backupimg}
-        done
-        # clean up backup images.
-        sudo docker image prune -a -f
-    fi
-    iniset $TROVE_CONF DEFAULT docker_insecure_registries "$TROVE_HOST_GATEWAY:4000"
-}
+# finalize_trove_network() - do the last thing(s) before starting Trove
+function finalize_trove_network {
 
-# Set up Trove management network and make configuration change.
-function config_trove_network {
     echo "Finalizing Neutron networking for Trove"
     echo "Dumping current network parameters:"
     echo "  SERVICE_HOST: $SERVICE_HOST"
     echo "  BRIDGE_IP: $BRIDGE_IP"
     echo "  PUBLIC_NETWORK_GATEWAY: $PUBLIC_NETWORK_GATEWAY"
     echo "  NETWORK_GATEWAY: $NETWORK_GATEWAY"
     echo "  IPV4_ADDRS_SAFE_TO_USE: $IPV4_ADDRS_SAFE_TO_USE"
     echo "  IPV6_ADDRS_SAFE_TO_USE: $IPV6_ADDRS_SAFE_TO_USE"
     echo "  FIXED_RANGE: $FIXED_RANGE"
     echo "  FLOATING_RANGE: $FLOATING_RANGE"
     echo "  SUBNETPOOL_PREFIX_V4: $SUBNETPOOL_PREFIX_V4"
     echo "  SUBNETPOOL_SIZE_V4: $SUBNETPOOL_SIZE_V4"
     echo "  SUBNETPOOL_V4_ID: $SUBNETPOOL_V4_ID"
     echo "  ROUTER_GW_IP: $ROUTER_GW_IP"
-    echo "  TROVE_MGMT_SUBNET_RANGE: ${TROVE_MGMT_SUBNET_RANGE}"
-    echo "  TROVE_MGMT_GATEWAY: ${TROVE_MGMT_GATEWAY}"
 
-    # Save xtrace setting
-    local orig_xtrace
-    orig_xtrace=$(set +o | grep xtrace)
-    set -x
-
-    echo "Creating Trove management network/subnet for Trove service project."
-    trove_service_project_id=$(openstack project show $SERVICE_PROJECT_NAME -c id -f value)
-    setup_mgmt_network ${trove_service_project_id} ${TROVE_MGMT_NETWORK_NAME} ${TROVE_MGMT_SUBNET_NAME} ${TROVE_MGMT_SUBNET_RANGE} ${TROVE_MGMT_GATEWAY}
-    mgmt_net_id=$(openstack network show ${TROVE_MGMT_NETWORK_NAME} -c id -f value)
-    echo "Created Trove management network ${TROVE_MGMT_NETWORK_NAME}(${mgmt_net_id})"
-
-    # Share the private network to other projects for testing purpose. We make
-    # the private network accessible to control plane below so that we could
-    # reach the private network for integration tests without floating ips
-    # associated, no matter which user the tests are using.
-    shared=$(openstack network show ${PRIVATE_NETWORK_NAME} -c shared -f value)
-    if [[ "$shared" == "False" ]]; then
-        openstack network set ${PRIVATE_NETWORK_NAME} --share
-    fi
-    if [[ -n ${ROUTER_GW_IP} && -n ${IPV4_ADDRS_SAFE_TO_USE} ]]; then
-        sudo ip route replace ${IPV4_ADDRS_SAFE_TO_USE} via $ROUTER_GW_IP
-    fi
-
-    # Make sure we can reach the management port of the service VM, this
-    # configuration is only for testing purpose. In production, it's
-    # recommended to config the router in the cloud infrastructure for the
-    # communication between Trove control plane and service VMs.
-    INTERFACE=trove-mgmt
-    MGMT_PORT_ID=$(openstack port create --project ${trove_service_project_id} --security-group ${TROVE_MGMT_SECURITY_GROUP} --device-owner trove --network ${TROVE_MGMT_NETWORK_NAME} --host=$(hostname) -c id -f value ${INTERFACE}-port)
-    MGMT_PORT_MAC=$(openstack port show -c mac_address -f value $MGMT_PORT_ID)
-    MGMT_PORT_IP=$(openstack port show -f value -c fixed_ips $MGMT_PORT_ID)
-    MGMT_PORT_IP=${MGMT_PORT_IP//u\'/\'}
-    MGMT_PORT_IP=$(echo ${MGMT_PORT_IP//\'/\"} | jq -r '.[0].ip_address')
-    sudo ovs-vsctl -- --may-exist add-port ${OVS_BRIDGE:-br-int} $INTERFACE -- set Interface $INTERFACE type=internal -- set Interface $INTERFACE external-ids:iface-status=active -- set Interface $INTERFACE external-ids:attached-mac=$MGMT_PORT_MAC -- set Interface $INTERFACE external-ids:iface-id=$MGMT_PORT_ID -- set Interface $INTERFACE external-ids:skip_cleanup=true
-    sudo ip link set dev $INTERFACE address $MGMT_PORT_MAC
-    mask=$(echo ${TROVE_MGMT_SUBNET_RANGE} | awk -F'/' '{print $2}')
-    sudo ip addr add ${MGMT_PORT_IP}/${mask} dev $INTERFACE
-    sudo ip link set $INTERFACE up
+    # Create the net/subnet for the alt_demo tenant so the int-tests have a proper network
+    echo "Creating network/subnets for ${ALT_TENANT_NAME} project"
+    ALT_PRIVATE_NETWORK_NAME=${TROVE_PRIVATE_NETWORK_NAME}
+    ALT_PRIVATE_SUBNET_NAME=${TROVE_PRIVATE_SUBNET_NAME}
+    ALT_PRIVATE_IPV6_SUBNET_NAME=ipv6-${ALT_PRIVATE_SUBNET_NAME}
+    ALT_NET_ID=$(set_up_network $ADMIN_ALT_DEMO_CLOUD $ALT_TENANT_ID $ALT_PRIVATE_NETWORK_NAME $ALT_PRIVATE_SUBNET_NAME $ALT_PRIVATE_IPV6_SUBNET_NAME)
+    echo "Created network ${ALT_PRIVATE_NETWORK_NAME} (${ALT_NET_ID})"
+
+    # Set up a management network to test that functionality
+    ALT_MGMT_NETWORK_NAME=trove-mgmt
+    ALT_MGMT_SUBNET_NAME=${ALT_MGMT_NETWORK_NAME}-subnet
+    ALT_MGMT_IPV6_SUBNET_NAME=ipv6-${ALT_MGMT_SUBNET_NAME}
+    ALT_MGMT_ID=$(set_up_network $ADMIN_ALT_DEMO_CLOUD $ALT_TENANT_ID $ALT_MGMT_NETWORK_NAME $ALT_MGMT_SUBNET_NAME $ALT_MGMT_IPV6_SUBNET_NAME)
+    echo "Created network ${ALT_MGMT_NETWORK_NAME} (${ALT_MGMT_ID})"
+
+    # Make sure we can reach the VMs
+    local replace_range=${SUBNETPOOL_PREFIX_V4}
+    if [[ -z "${SUBNETPOOL_V4_ID}" ]]; then
+        replace_range=${FIXED_RANGE}
+    fi
+    sudo ip route replace $replace_range via $ROUTER_GW_IP
 
     echo "Neutron network list:"
-    openstack network list
-    echo "Neutron subnet list:"
-    openstack subnet list
-    echo "Neutron router:"
-    openstack router show ${ROUTER_ID} -f yaml
-    echo "ip route:"
-    sudo ip route
+    openstack --os-cloud devstack-admin --os-region "$REGION_NAME" network list
 
     # Now make sure the conf settings are right
+    iniset $TROVE_CONF DEFAULT network_label_regex "${ALT_PRIVATE_NETWORK_NAME}"
     iniset $TROVE_CONF DEFAULT ip_regex ""
     iniset $TROVE_CONF DEFAULT black_list_regex ""
-    iniset $TROVE_CONF DEFAULT management_networks ${mgmt_net_id}
+    # Don't use a default network for now, until the neutron issues are figured out
+    #iniset $TROVE_CONF DEFAULT default_neutron_networks "${ALT_MGMT_ID}"
+    iniset $TROVE_CONF DEFAULT default_neutron_networks ""
     iniset $TROVE_CONF DEFAULT network_driver trove.network.neutron.NeutronDriver
 
-    # Restore xtrace setting
-    $orig_xtrace
+    iniset $TROVE_TASKMANAGER_CONF DEFAULT network_label_regex "${ALT_PRIVATE_NETWORK_NAME}"
+    iniset $TROVE_TASKMANAGER_CONF DEFAULT ip_regex ""
+    iniset $TROVE_TASKMANAGER_CONF DEFAULT black_list_regex ""
+    # Don't use a default network for now, until the neutron issues are figured out
+    #iniset $TROVE_TASKMANAGER_CONF DEFAULT default_neutron_networks "${ALT_MGMT_ID}"
+    iniset $TROVE_CONF DEFAULT default_neutron_networks ""
+    iniset $TROVE_TASKMANAGER_CONF DEFAULT network_driver trove.network.neutron.NeutronDriver
 }
 
-function config_nova_keypair {
-    export SSH_DIR=${SSH_DIR:-"$HOME/.ssh"}
-
-    if [[ ! -f ${SSH_DIR}/id_rsa.pub ]]; then
-        mkdir -p ${SSH_DIR}
-        /usr/bin/ssh-keygen -f ${SSH_DIR}/id_rsa -q -N ""
-        # This is to allow guest agent ssh into the controller in dev mode.
-        cat ${SSH_DIR}/id_rsa.pub >> ${SSH_DIR}/authorized_keys
+# start_trove() - Start running processes, including screen
+function start_trove {
+    if [[ ${TROVE_USE_MOD_WSGI}" == TRUE" ]]; then
+        echo "Restarting Apache server ..."
+        enable_apache_site trove-api
+        restart_apache_server
     else
-        # This is to allow guest agent ssh into the controller in dev mode.
-        cat ${SSH_DIR}/id_rsa.pub >> ${SSH_DIR}/authorized_keys
-        sort ${SSH_DIR}/authorized_keys | uniq > ${SSH_DIR}/authorized_keys.uniq
-        mv ${SSH_DIR}/authorized_keys.uniq ${SSH_DIR}/authorized_keys
-        chmod 600 ${SSH_DIR}/authorized_keys
+        run_process tr-api "$TROVE_BIN_DIR/trove-api --config-file=$TROVE_CONF --debug"
     fi
-
-    echo "Creating Trove management keypair ${TROVE_MGMT_KEYPAIR_NAME}"
-    openstack --os-cloud trove keypair create --public-key ${SSH_DIR}/id_rsa.pub ${TROVE_MGMT_KEYPAIR_NAME}
-
-    iniset $TROVE_CONF DEFAULT nova_keypair ${TROVE_MGMT_KEYPAIR_NAME}
-}
-
-function config_cinder_volume_type {
-    volume_type=$(openstack --os-cloud trove volume type list -c Name -f value | awk 'NR==1 {print}')
-
-    iniset $TROVE_CONF DEFAULT cinder_volume_type ${volume_type}
+    run_process tr-tmgr "$TROVE_BIN_DIR/trove-taskmanager --config-file=$TROVE_TASKMANAGER_CONF --debug"
+    run_process tr-cond "$TROVE_BIN_DIR/trove-conductor --config-file=$TROVE_CONDUCTOR_CONF --debug"
 }
 
-function config_mgmt_security_group {
-    local sgid
-
-    echo "Creating Trove management security group."
-    sgid=$(openstack --os-cloud trove security group create ${TROVE_MGMT_SECURITY_GROUP} -f value -c id)
-
-    # Allow ICMP
-    openstack --os-cloud trove security group rule create --proto icmp $sgid
-    # Allow SSH
-    openstack --os-cloud trove security group rule create --protocol tcp --dst-port 22 $sgid
-
-    iniset $TROVE_CONF DEFAULT management_security_groups $sgid
+# stop_trove() - Stop running processes
+function stop_trove {
+    # Kill the trove screen windows
+    local serv
+    if [[ ${TROVE_USE_MOD_WSGI} == "TRUE" ]]; then
+        echo "Disabling Trove API in Apache"
+        disable_apache_site trove-api
+    else
+        stop_process tr-api
+    fi
+    for serv in tr-tmgr tr-cond; do
+        stop_process $serv
+    done
 }
 
 # Dispatcher for trove plugin
 if is_service_enabled trove; then
     if [[ "$1" == "stack" && "$2" == "install" ]]; then
         echo_summary "Installing Trove"
         install_trove
         install_python_troveclient
     elif [[ "$1" == "stack" && "$2" == "post-config" ]]; then
+        echo_summary "Configuring Trove"
+        configure_trove
+
         if is_service_enabled key; then
             create_trove_accounts
         fi
 
-        echo_summary "Configuring Trove"
-        configure_trove
     elif [[ "$1" == "stack" && "$2" == "extra" ]]; then
-        init_trove_db
-        config_nova_keypair
-        config_cinder_volume_type
-        config_mgmt_security_group
-        config_trove_network
-        create_guest_image
-        if [ "$TROVE_ENABLE_LOCAL_REGISTRY" == "True" ] ; then
-            create_registry_container
+        # Initialize trove
+        init_trove
+
+        # finish the last step in trove network configuration
+        echo_summary "Finalizing Trove Network Configuration"
+
+        if is_service_enabled neutron; then
+            echo "finalize_trove_network: Neutron is enabled."
+            finalize_trove_network
+        else
+            echo "finalize_trove_network: Neutron is not enabled. Nothing to do."
         fi
 
+        # Start the trove API and trove taskmgr components
         echo_summary "Starting Trove"
         start_trove
-
-        # Guarantee the file permission in the trove code repo in order to
-        # download trove code from trove-guestagent.
-        sudo chown -R $STACK_USER:$STACK_USER "$DEST/trove"
-    elif [[ "$1" == "stack" && "$2" == "test-config" ]]; then
-        echo_summary "Configuring Tempest for Trove"
-        configure_tempest_for_trove
     fi
 
     if [[ "$1" == "unstack" ]]; then
         stop_trove
         cleanup_trove
     fi
 fi
```

### Comparing `trove-21.0.0.0rc2/doc/source/admin/secure_oslo_messaging.rst` & `trove-8.0.1/doc/source/admin/secure_oslo_messaging.rst`

 * *Files 2% similar despite different names*

```diff
@@ -225,15 +225,17 @@
     [...]
 
 We can now relaunch the control plane software but before we do that,
 we inspect the configuration parameters and disable secure RPC
 messaging by adding this line into the configuration files::
 
     amrith@amrith-work:/etc/trove$ grep enable_secure_rpc_messaging *.conf
+    trove-conductor.conf:enable_secure_rpc_messaging = False
     trove.conf:enable_secure_rpc_messaging = False
+    trove-taskmanager.conf:enable_secure_rpc_messaging = False
 
 The first thing we observe is that heartbeat messages from the
 existing instance are still properly handled by the conductor and the
 instance remains active::
 
     2017-01-09 13:26:57.742 DEBUG oslo_messaging._drivers.amqpdriver [-] received message with unique_id: eafe22c08bae485e9346ce0fbdaa4d6c from (pid=96551) __call__ /usr/local/lib/python2.7/dist-packages/oslo_messaging/_drivers/amqpdriver.py:196
     2017-01-09 13:26:57.744 DEBUG trove.conductor.manager [-] Instance ID: bb0c9213-31f8-4427-8898-c644254b3642, Payload: {u'service_status': u'running'} from (pid=96551) heartbeat /opt/stack/trove/trove/conductor/manager.py:88
@@ -307,15 +309,17 @@
     guest_id=514ef051-0bf7-48a5-adcf-071d4a6625fb
     datastore_manager=mysql
     tenant_id=56cca8484d3e48869126ada4f355c284
 
 We can now shutdown the control plane again and enable the secure RPC
 capability. Observe that we've just commented out the lines (below)::
 
+    trove-conductor.conf:# enable_secure_rpc_messaging = False
     trove.conf:# enable_secure_rpc_messaging = False
+    trove-taskmanager.conf:# enable_secure_rpc_messaging = False
 
 And create another database instance::
 
     amrith@amrith-work:/etc/trove$ trove create m20 25 --size 3 --nic net-id=4bab02e7-87bb-4cc0-8c07-2f282c777c85
     +-------------------+--------------------------------------+
     | Property          | Value                                |
     +-------------------+--------------------------------------+
@@ -654,8 +658,8 @@
 .. note::
 
    This field is only returned if the user is an 'admin'. Non admin
    users do not see the field.
 
 ::
 
-   RESP BODY: {"instance": {"status": "ACTIVE", "updated": "2017-01-09T18:29:00", "name": "m10", "links": [{"href": "https://192.168.126.130:8779/v1.0/56cca8484d3e48869126ada4f355c284/instances/514ef051-0bf7-48a5-adcf-071d4a6625fb", "rel": "self"}, {"href": "https://192.168.126.130:8779/instances/514ef051-0bf7-48a5-adcf-071d4a6625fb", "rel": "bookmark"}], "created": "2017-01-09T18:28:56", "region": "RegionOne", "server_id": "2452263e-3d33-48ec-8f24-2851fe74db28", "id": "514ef051-0bf7-48a5-adcf-071d4a6625fb", "volume": {"used": 0.11, "size": 3}, "volume_id": "cee2e17b-80fa-48e5-a488-da8b7809373a", "flavor": {"id": "25"}, "datastore": {"version": "5.6", "type": "mysql"}, "encrypted_rpc_messaging": false}}
+   RESP BODY: {"instance": {"status": "ACTIVE", "updated": "2017-01-09T18:29:00", "name": "m10", "links": [{"href": "https://192.168.126.130:8779/v1.0/56cca8484d3e48869126ada4f355c284/instances/514ef051-0bf7-48a5-adcf-071d4a6625fb", "rel": "self"}, {"href": "https://192.168.126.130:8779/instances/514ef051-0bf7-48a5-adcf-071d4a6625fb", "rel": "bookmark"}], "created": "2017-01-09T18:28:56", "region": "RegionOne", "server_id": "2452263e-3d33-48ec-8f24-2851fe74db28", "id": "514ef051-0bf7-48a5-adcf-071d4a6625fb", "volume": {"used": 0.11, "size": 3}, "volume_id": "cee2e17b-80fa-48e5-a488-da8b7809373a", "flavor": {"id": "25", "links": [{"href": "https://192.168.126.130:8779/v1.0/56cca8484d3e48869126ada4f355c284/flavors/25", "rel": "self"}, {"href": "https://192.168.126.130:8779/flavors/25", "rel": "bookmark"}]}, "datastore": {"version": "5.6", "type": "mysql"}, "encrypted_rpc_messaging": false}}
```

### Comparing `trove-21.0.0.0rc2/doc/source/cli/trove-manage.rst` & `trove-8.0.1/doc/source/cli/trove-manage.rst`

 * *Files 4% similar despite different names*

```diff
@@ -197,15 +197,14 @@
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. code-block:: console
 
    usage: trove-manage datastore_version_update [-h]
                                                 datastore version_name manager
                                                 image_id packages active
-                                                --image-tags <image_tags>
 
 Add or update a datastore version. If the datastore version already exists,
 all values except the datastore name and version will be updated.
 
 **positional arguments:**
 
 ``datastore``
@@ -229,21 +228,14 @@
   Accepted values are ``0`` and ``1``.
 
 **optional arguments:**
 
 ``-h, --help``
   show this help message and exit
 
-``--image-tags``
-  List of image tags separated by comma. If the image ID is not provided
-  explicitly, the image can be retrieved by the image tags. Multiple image tags
-  are separated by comma, e.g. trove,mysql. Using image tags is more flexible
-  than ID especially when new guest image is uploaded to Glance, Trove can pick
-  up the latest image automatically for creating instances.
-
 trove-manage db_downgrade
 ~~~~~~~~~~~~~~~~~~~~~~~~~
 
 .. code-block:: console
 
    usage: trove-manage db_downgrade [-h] [--repo_path REPO_PATH] version
 
@@ -290,18 +282,14 @@
 **optional arguments:**
 
 ``-h, --help``
   show this help message and exit
 
 trove-manage db_recreate
 ~~~~~~~~~~~~~~~~~~~~~~~~
-.. caution::
-
-   Running this command will drop the database and recreate it. It means all
-   data in the database will be lost. Becareful when running this command.
 
 .. code-block:: console
 
    usage: trove-manage db_recreate [-h] [--repo_path REPO_PATH]
 
 Drop the database and recreate it.
```

### Comparing `trove-21.0.0.0rc2/doc/source/conf.py` & `trove-8.0.1/doc/source/conf.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,56 +10,51 @@
 
 # If your documentation needs a minimal Sphinx version, state it here.
 #needs_sphinx = '1.0'
 
 # Add any Sphinx extension module names here, as strings. They can be
 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
 extensions = ['sphinx.ext.todo',
-    'sphinx.ext.autodoc',
-    'sphinx.ext.coverage',
-    'sphinx.ext.viewcode',
-    'openstackdocstheme',
-    'stevedore.sphinxext',
-    'oslo_config.sphinxconfiggen',
-    'oslo_config.sphinxext',
-    'oslo_policy.sphinxpolicygen',
-    'oslo_policy.sphinxext',
-]
+              'sphinx.ext.viewcode',
+              'openstackdocstheme',
+              'stevedore.sphinxext']
 
 # openstackdocstheme options
-openstackdocs_repo_name = 'openstack/trove'
-openstackdocs_bug_project = 'trove'
-openstack_auto_name = False
-openstackdocs_bug_tag = ''
+repository_name = 'openstack/trove'
+bug_project = 'trove'
+bug_tag = ''
+html_last_updated_fmt = '%Y-%m-%d %H:%M'
 html_theme = 'openstackdocs'
 
-# sphinxcontrib.apidoc options
-config_generator_config_file = (
-    '../../tools/trove-config-generator.conf')
-sample_config_basename = '_static/trove'
-
-policy_generator_config_file = (
-    '../../tools/trove-policy-generator.conf')
-sample_policy_basename = '_static/trove'
-
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ['_templates']
 
 # The suffix of source filenames.
 source_suffix = '.rst'
 
 # The encoding of source files.
 #source_encoding = 'utf-8-sig'
 
 # The master toctree document.
 master_doc = 'index'
 
 # General information about the project.
-project = 'Trove'
-copyright = '2013, OpenStack Foundation'
+project = u'Trove'
+copyright = u'2013, OpenStack Foundation'
+
+# The version info for the project you're documenting, acts as replacement for
+# |version| and |release|, also used in various other places throughout the
+# built documents.
+#
+# The short X.Y version.
+from trove.version import version_info as trove_version
+# The full version, including alpha/beta/rc tags.
+release = trove_version.version_string_with_vcs()
+# The short X.Y version.
+version = trove_version.canonical_version_string()
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 #language = None
 
 # There are two options for replacing |today|: either, you set today to some
 # non-false value, then it is used:
@@ -82,15 +77,15 @@
 add_module_names = True
 
 # If true, sectionauthor and moduleauthor directives will be shown in the
 # output. They are ignored by default.
 #show_authors = False
 
 # The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'native'
+pygments_style = 'sphinx'
 
 # A list of ignored prefixes for module index sorting.
 modindex_common_prefix = ['trove.']
 
 # If true, keep warnings as "system message" paragraphs in the built documents.
 #keep_warnings = False
 
@@ -127,14 +122,18 @@
 #html_favicon = None
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
 # html_static_path = ['_static']
 
+# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
+# using the given strftime format.
+#html_last_updated_fmt = '%b %d, %Y'
+
 # If true, SmartyPants will be used to convert quotes and dashes to
 # typographically correct entities.
 #html_use_smartypants = True
 
 # Custom sidebar templates, maps document names to template names.
 #html_sidebars = {}
 
@@ -170,22 +169,33 @@
 
 # Output file base name for HTML help builder.
 htmlhelp_basename = '%sdoc' % project
 
 
 # -- Options for LaTeX output -------------------------------------------------
 
+latex_elements = {
+    # The paper size ('letterpaper' or 'a4paper').
+    #'papersize': 'letterpaper',
+
+    # The font size ('10pt', '11pt' or '12pt').
+    #'pointsize': '10pt',
+
+    # Additional stuff for the LaTeX preamble.
+    #'preamble': '',
+}
+
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title, author, documentclass [howto/manual])
 latex_documents = [
     (
         'index',
         '%s.tex' % project,
-        '%s Documentation' % project,
-        'OpenStack Foundation',
+        u'%s Documentation' % project,
+        u'OpenStack Foundation',
         'manual'
     ),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
 # the title page.
 #latex_logo = None
@@ -211,16 +221,16 @@
 
 # One entry per manual page. List of tuples
 # (source start file, name, description, authors, manual section).
 # man_pages = [
 #     (
 #         'index',
 #         '%s' % project,
-#         '%s Documentation' % project,
-#         'OpenStack Foundation',
+#         u'%s Documentation' % project,
+#         u'OpenStack Foundation',
 #         1
 #     ),
 # ]
 
 # If true, show URL addresses after external links.
 #man_show_urls = False
 
@@ -230,16 +240,16 @@
 # Grouping the document tree into Texinfo files. List of tuples
 # (source start file, target name, title, author,
 #  dir menu entry, description, category)
 texinfo_documents = [
     (
         'index',
         '%s' % project,
-        '%s Documentation' % project,
-        'OpenStack Foundation',
+        u'%s Documentation' % project,
+        u'OpenStack Foundation',
         '%s' % project,
         'Database as a service.',
         'Miscellaneous'
         'manual'
     ),
 ]
 
@@ -255,18 +265,18 @@
 # If true, do not generate a @detailmenu in the "Top" node's menu.
 #texinfo_no_detailmenu = False
 
 
 # -- Options for Epub output --------------------------------------------------
 
 # Bibliographic Dublin Core info.
-epub_title = '%s' % project
-epub_author = 'OpenStack Foundation'
-epub_publisher = 'OpenStack Foundation'
-epub_copyright = '2013, OpenStack Foundation'
+epub_title = u'%s' % project
+epub_author = u'OpenStack Foundation'
+epub_publisher = u'OpenStack Foundation'
+epub_copyright = u'2013, OpenStack Foundation'
 
 # The language of the text. It defaults to the language option
 # or en if the language is not set.
 #epub_language = ''
 
 # The scheme of the identifier. Typical schemes are ISBN or URL.
 #epub_scheme = ''
```

### Comparing `trove-21.0.0.0rc2/doc/source/contributor/design.rst` & `trove-8.0.1/doc/source/contributor/design.rst`

 * *Files 16% similar despite different names*

```diff
@@ -3,36 +3,36 @@
 ============
 Trove Design
 ============
 
 High Level description
 ======================
 
-Trove is designed to support multi-tenant database within a Nova
+Trove is designed to support a single-tenant database within a Nova
 instance. There will be no restrictions on how Nova is configured,
 since Trove interacts with other OpenStack components purely through
 the API.
 
 
 Trove-api
 =========
 
-The trove-api service provides a RESTful API that supports JSON to
-provision and manage Trove instances.
+The trove-api service provides a RESTful API that supports JSON and
+XML to provision and manage Trove instances.
 
 * A REST-ful component
-* Entry point - trove/cmd/api.py
+* Entry point - trove/bin/trove-api
 * Uses a WSGI launcher configured by etc/trove/api-paste.ini
 * Defines the pipeline of filters; authtoken, ratelimit, etc.
 * Defines the app_factory for the troveapp as
   trove.common.api:app_factory
 * The API class (a wsgi Router) wires the REST paths to the
   appropriate Controllers
 * Implementation of the Controllers are under the relevant module
-  (versions/instance/backup/configuration), in the service.py module
+  (versions/instance/flavor/limits), in the service.py module
 * Controllers usually redirect implementation to a class in the
   models.py module
 * At this point, an api module of another component (TaskManager,
   GuestAgent, etc.) is used to send the request onwards through
   RabbitMQ
 
 
@@ -40,17 +40,17 @@
 =================
 
 The trove-taskmanager service does the heavy lifting as far as
 provisioning instances, managing the lifecycle of instances, and
 performing operations on the Database instance.
 
 * A service that listens on a RabbitMQ topic
-* Entry point - trove/cmd/taskmanager.py
+* Entry point - trove/bin/trove-taskmanager
 * Runs as a RpcService configured by
-  etc/trove/trove.conf which defines
+  etc/trove/trove-taskmanager.conf.sample which defines
   trove.taskmanager.manager.Manager as the manager - basically this is
   the entry point for requests arriving through the queue
 * As described above, requests for this component are pushed to MQ
   from another component using the TaskManager's api module using
   _cast() or _call() (sync/a-sync) and putting the method's name as a
   parameter
 * In module oslo.messaging, oslo_messaging/rpc/dispatcher.py
@@ -70,30 +70,29 @@
 itself. The Guest Agent listens for RPC messages through the message
 bus and performs the requested operation.
 
 * Similar to TaskManager in the sense of running as a service that
   listens on a RabbitMQ topic
 * GuestAgent runs on every DB instance, and a dedicated MQ topic is
   used (identified as the instance's id)
-* Entry point - trove/cmd/guest.py
+* Entry point - trove/bin/trove-guestagent
 * Runs as a RpcService configured by
-  /etc/trove/conf.d/trove-guestagent.conf which defines
+  etc/trove/trove-guestagent.conf.sample which defines
   trove.guestagent.datastore.manager.Manager as the manager - basically
   this is the entry point for requests arriving through the queue
 * As described above, requests for this component are pushed to MQ
   from another component using the GuestAgent's api module using
   _cast() or _call() (sync/a-sync) and putting the method's name as a
   parameter
 * In module oslo.messaging, oslo_messaging/rpc/dispatcher.py
   - RpcDispatcher.dispatch()invokes the proper method in the Manager
   by some equivalent to reflection
 * The Manager then redirect the handling to an object (usually) from
   the dbaas.py module.
-* The database service is running as a docker container inside the Nova
-  VM.
+* Actual handling is usually done in the dbaas.py module
 
 
 Trove-conductor
 ===============
 
 Conductor is a service that runs on the host, responsible for receiving
 messages from guest instances to update information on the host.
@@ -104,24 +103,25 @@
 
 * Similar to guest-agent in that it is a service that listens to a
   RabbitMQ topic. The difference is conductor lives on the host, not
   the guest.
 * Guest agents communicate to conductor by putting messages on the
   topic defined in cfg as conductor_queue. By default this is
   "trove-conductor".
-* Entry point - trove/cmd/conductor.py
+* Entry point - trove/bin/trove-conductor
 * Runs as RpcService configured by
-  etc/trove/trove.conf which defines
+  etc/trove/trove-conductor.conf.sample which defines
   trove.conductor.manager.Manager as the manager. This is the entry
   point for requests arriving on the queue.
 * As guestagent above, requests are pushed to MQ from another component
   using _cast() (synchronous), generally of the form
   {"method": "<method_name>", "args": {<arguments>}}
 * Actual database update work is done by trove/conductor/manager.py
 * The "heartbeat" method updates the status of an instance. This is
   used to report that instance has changed from NEW to BUILDING to
   ACTIVE and so on.
 * The "update_backup" method changes the details of a backup, including
   its current status, size of the backup, type, and checksum.
 
 
+
 .. Trove - Database as a Service: https://wiki.openstack.org/wiki/Trove
```

### Comparing `trove-21.0.0.0rc2/doc/source/index.rst` & `trove-8.0.1/doc/source/index.rst`

 * *Files 27% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-=================================
-Welcome to Trove's documentation!
-=================================
+===========================================
+Welcome to Trove's developer documentation!
+===========================================
 
 Trove is Database as a Service for OpenStack. It's designed to run
 entirely on OpenStack, with the goal of allowing users to quickly and
 easily utilize the features of a relational database without the
 burden of handling complex administrative tasks. Cloud users and
 database administrators can provision and manage multiple database
 instances as needed.
@@ -17,33 +17,43 @@
 For an in-depth look at the project's design and structure, see the
 :doc:`contributor/design` page.
 
 .. toctree::
    :maxdepth: 2
 
    install/index
-   user/index
    admin/index
    cli/index
    contributor/index
    reference/index
 
 
 * Source Code Repositories
 
   - `Trove`_
   - `Trove Client`_
 
-* `Trove API Documentation`_ on docs.openstack.org
-* `Trove storyboard`_ on storyboard.openstack.org
+* `Trove Wiki`_ on OpenStack
+* `Trove API Documentation`_ on developer.openstack.org
+
+
+Guest Images
+============
+
+In order to use Trove, you need to have Guest Images for each
+datastore and version. These images are loaded into Glance and
+registered with Trove.
+
+For those wishing to develop guest images, please refer to the
+:ref:`build_guest_images` page.
 
 
 Search Trove Documentation
 ==========================
 
 * :ref:`search`
 
 
-.. _Trove: https://opendev.org/openstack/trove
-.. _Trove Client: https://opendev.org/openstack/python-troveclient
-.. _Trove API Documentation: https://docs.openstack.org/api-ref/database/
-.. _Trove storyboard: https://storyboard.openstack.org/#!/project/openstack/trove
+.. _Trove Wiki: https://wiki.openstack.org/wiki/Trove
+.. _Trove: https://git.openstack.org/cgit/openstack/trove
+.. _Trove Client: https://git.openstack.org/cgit/openstack/python-troveclient
+.. _Trove API Documentation: http://developer.openstack.org/api-ref/database/
```

### Comparing `trove-21.0.0.0rc2/doc/source/install/apache-mod-wsgi.rst` & `trove-8.0.1/doc/source/install/apache-mod-wsgi.rst`

 * *Files 10% similar despite different names*

```diff
@@ -6,47 +6,44 @@
 
 
 Installing API behind mod_wsgi
 ==============================
 
 #. Install the Apache Service::
 
-    RHEL7/CentOS7:
-      sudo yum install httpd mod_wsgi
+    Fedora 21/RHEL7/CentOS7:
+      sudo yum install httpd
 
-    RHEL8/CentOS8:
-      sudo dnf install httpd python3-mod_wsgi
-
-    Fedora:
-      sudo dnf install httpd mod_wsgi
+    Fedora 22 (or higher):
+      sudo dnf install httpd
 
     Debian/Ubuntu:
-      sudo apt-get install apache2 libapache2-mod-wsgi-py3
+      apt-get install apache2
 
 #. Copy ``etc/apache2/trove`` under the apache sites::
 
-    Fedora/RHEL/CentOS:
+    Fedora/RHEL7/CentOS7:
       sudo cp etc/apache2/trove /etc/httpd/conf.d/trove-api.conf
 
     Debian/Ubuntu:
       sudo cp etc/apache2/trove /etc/apache2/sites-available/trove-api.conf
 
 #. Edit ``<apache-configuration-dir>/trove-api.conf`` according to installation
    and environment.
 
    * Modify the ``WSGIDaemonProcess`` directive to set the ``user`` and
      ``group`` values to appropriate user on your server.
    * Modify the ``WSGIScriptAlias`` directive to point to the
-     trove/api/app_wsgi.py script.
+     trove/api/app.wsgi script.
    * Modify the ``Directory`` directive to set the path to the Trove API
      code.
    * Modify the ``ErrorLog and CustomLog`` to redirect the logs to the right
      directory.
 
 #. Enable the apache trove site and reload::
 
     Fedora/RHEL7/CentOS7:
       sudo systemctl reload httpd
 
-    Debian/Ubuntu:
-      sudo a2ensite trove-api
-      sudo service apache2 reload
+   Debian/Ubuntu:
+    sudo a2ensite trove
+    sudo service apache2 reload
```

### Comparing `trove-21.0.0.0rc2/doc/source/install/common_prerequisites.txt` & `trove-8.0.1/doc/source/install/common_prerequisites.txt`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/doc/source/install/dashboard.rst` & `trove-8.0.1/doc/source/install/dashboard.rst`

 * *Files 21% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 
 .. _dashboard:
 
 Install and configure the Trove dashboard
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-* Installation of the Trove dashboard for Horizon is straightforward.
-  It is best to install it via pip.
+  * Installation of the Trove dashboard for Horizon is straightforward.
+    It is best to install it via pip.
 
-  .. code-block:: console
+   .. code-block:: console
 
-     # pip install trove-dashboard
+      # pip install trove-dashboard
 
-* The command above will install the latest version which is
-  appropriate if you are running the latest Trove. If you are
-  running an earlier version of Trove you may need to specify
-  a compatible version of trove-dashboard.
+  * The command above will install the latest version which is
+    appropriate if you are running the latest Trove. If you are
+    running an earlier version of Trove you may need to specify
+    a compatible version of trove-dashboard.
 
-* After pip installs it, locate the trove-dashboard directory and
-  copy the contents of the ``enabled/`` directory to your horizon
-  ``openstack_dashboard/local/enabled/`` directory.
+  * After pip installs it, locate the trove-dashboard directory and
+    copy the contents of the ``enabled/`` directory to your horizon
+    ``openstack_dashboard/local/enabled/`` directory.
 
-* Reload Apache to pick up the changes to Horizon.
+  * Reload Apache to pick up the changes to Horizon.
```

### Comparing `trove-21.0.0.0rc2/doc/source/install/install-redhat.rst` & `trove-8.0.1/doc/source/install/install-rdo.rst`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,12 @@
 .. _install-rdo:
 
 Install and configure for Red Hat Enterprise Linux and CentOS
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. warning::
-
-    This guide is not tested since stable/train.
 
 This section describes how to install and configure the Database service
 for Red Hat Enterprise Linux 7 and CentOS 7.
 
 .. include:: common_prerequisites.txt
 
 Install and configure components
```

### Comparing `trove-21.0.0.0rc2/doc/source/install/install-suse.rst` & `trove-8.0.1/doc/source/install/install-obs.rst`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,13 @@
 .. _install-obs:
 
 
 Install and configure for openSUSE and SUSE Linux Enterprise
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-.. warning::
-
-    This guide is not tested since stable/train.
-
 This section describes how to install and configure the Database service
 for openSUSE Leap 42.2 and SUSE Linux Enterprise Server 12 SP2.
 
 .. include:: common_prerequisites.txt
 
 Install and configure components
 --------------------------------
```

### Comparing `trove-21.0.0.0rc2/doc/source/install/install-ubuntu.rst` & `trove-8.0.1/doc/source/install/install-ubuntu.rst`

 * *Files 24% similar despite different names*

```diff
@@ -1,47 +1,70 @@
 .. _install-ubuntu:
 
 Install and configure for Ubuntu
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 This section describes how to install and configure the Database
-service for Ubuntu 18.04 (LTS).
+service for Ubuntu 14.04 (LTS).
 
 .. include:: common_prerequisites.txt
 
 
 Install and configure components
 --------------------------------
 
 #. Install the packages:
 
-   .. warning::
-
-       Please be aware that the trove debian packages for Ubuntu are not
-       usually up to date, especially when there are bugfixes for stable
-       branch, the debian packages are not guaranteed to contain those changes.
-       The recommended way to install OpenStack services is either using docker
-       image with source code or installing source code inside a Python
-       virutual environment.
-
-   The commands to install Trove compoments:
-
    .. code-block:: console
 
       # apt-get update
-      # apt-get install python-trove trove-common trove-api trove-taskmanager trove-conductor
-      # pip3 install python-troveclient
+
+      # apt-get install python-trove python-troveclient \
+        python-glanceclient trove-common trove-api trove-taskmanager \
+        trove-conductor
 
 
 .. include:: common_configure.txt
 
 
 Finalize installation
 ---------------------
 
-1. Restart the Database services:
+1. Due to a bug in the Ubuntu packages, edit the service definition files
+   to use the correct configuration settings.
+
+   To do this, navigate to ``/etc/init`` and edit the following files
+   as described below:
+
+   ``trove-taskmanager.conf``
+
+   ``trove-conductor.conf``
+
+   (Note that, although they have the same names, these files are
+   in a different location and have different content than the similarly
+   named files you edited earlier in this procedure.)
+
+   In each file, find this line:
+
+   .. code-block:: ini
+
+      exec start-stop-daemon --start --chdir /var/lib/trove \
+         --chuid trove:trove --make-pidfile \
+         --pidfile /var/run/trove/trove-conductor.pid \
+         --exec /usr/bin/trove-conductor -- \
+         --config-file=/etc/trove/trove.conf ${DAEMON_ARGS}
+
+   Note that ``--config-file`` incorrectly points to ``trove.conf``.
+
+   In ``trove-taskmanager.conf``, edit ``config-file`` to point to
+   ``/etc/trove/trove-taskmanager.conf``.
+
+   In ``trove-conductor.conf``, edit ``config-file`` to point to
+   ``/etc/trove/trove-conductor.conf``.
+
+2. Restart the Database services:
 
    .. code-block:: console
 
       # service trove-api restart
       # service trove-taskmanager restart
       # service trove-conductor restart
```

### Comparing `trove-21.0.0.0rc2/etc/apache2/trove` & `trove-8.0.1/etc/apache2/trove`

 * *Files 2% similar despite different names*

```diff
@@ -9,21 +9,21 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 # License for the specific language governing permissions and limitations
 # under the License.
 
 # This is an example Apache2 configuration file for using Trove API
-# through mod_wsgi
+through mod_wsgi
 
 Listen 8779
 
 <VirtualHost *:8779>
     WSGIDaemonProcess trove-api user=stack group=stack processes=2 threads=2 display-name=%{GROUP}
-    WSGIScriptAlias / /opt/stack/trove/trove/cmd/app_wsgi.py
+    WSGIScriptAlias / /opt/stack/trove/trove/cmd/app.wsgi
     WSGIProcessGroup trove-api
 
     ErrorLog /var/log/httpd/trove_error.log
     LogLevel info
     CustomLog /var/log/httpd/trove_access.log combined
 
     <Directory /opt/stack/trove/trove/cmd>
```

### Comparing `trove-21.0.0.0rc2/etc/tests/core.test.conf` & `trove-8.0.1/etc/tests/core.test.conf`

 * *Files 2% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9375%*

 * *Differences: {"'keystone_use_combined'": 'True', "'start_services'": 'False'}*

```diff
@@ -4,21 +4,23 @@
     "glance_image": "fakey_fakerson.tar.gz",
     "glance_images_directory": "/glance_images",
     "glance_reg_conf": "/vagrant/conf/glance-reg.conf",
     "instance_bigger_flavor_name": "m1.rd-smaller",
     "instance_flavor_name": "m1.tiny",
     "keystone_code_root": "/opt/stack/keystone",
     "keystone_conf": "/etc/keystone/keystone.conf",
+    "keystone_use_combined": true,
     "management_api_disabled": true,
     "nova_code_root": "/opt/stack/nova",
     "nova_conf": "/home/vagrant/nova.conf",
     "openvz_disabled": false,
     "report_directory": "rdli-test-report",
     "root_removed_from_instance_api": true,
     "root_timestamp_disabled": false,
+    "start_services": false,
     "test_mgmt": false,
     "trove_api_updated": "2012-08-01T00:00:00Z",
     "trove_can_have_volume": true,
     "trove_code_root": "/opt/stack/trove",
     "trove_conf": "/tmp/trove.conf",
     "trove_main_instance_has_volume": true,
     "trove_max_accepted_volume_size": 25,
```

### Comparing `trove-21.0.0.0rc2/etc/tests/localhost.test.conf` & `trove-8.0.1/etc/tests/localhost.test.conf`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9473684210526315%*

 * *Differences: {'delete': "['shared_network']"}*

```diff
@@ -83,15 +83,14 @@
     ],
     "include-files": [
         "core.test.conf"
     ],
     "nova_auth_url": "http://localhost:8779/v1.0/auth",
     "nova_client": null,
     "sentinel": null,
-    "shared_network": "b19b5da0-d2f6-11e9-9382-00224d6b7bc1",
     "trove_api_updated": "2012-08-01T00:00:00Z",
     "trove_auth_url": "http://localhost:8779/v1.0/auth",
     "trove_client_insecure": false,
     "trove_dns_checker": "trove.tests.fakes.dns.FakeDnsChecker",
     "trove_dns_support": true,
     "trove_ip_support": false,
     "trove_version": "v1.0",
```

### Comparing `trove-21.0.0.0rc2/etc/trove/api-paste.ini` & `trove-8.0.1/etc/trove/api-paste.ini`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 [composite:trove]
 use = call:trove.common.wsgi:versioned_urlmap
 /: versions
-/healthcheck: healthcheck
 /v1.0: troveapi
 
 [app:versions]
 paste.app_factory = trove.versions:app_factory
 
 [pipeline:troveapi]
 pipeline = cors http_proxy_to_wsgi faultwrapper osprofiler authtoken authorization contextwrapper ratelimit extensions troveapp
@@ -39,14 +38,9 @@
 [app:troveapp]
 paste.app_factory = trove.common.api:app_factory
 
 #Add this filter to log request and response for debugging
 [filter:debug]
 paste.filter_factory = trove.common.wsgi:Debug
 
-[app:healthcheck]
-paste.app_factory = oslo_middleware:Healthcheck.app_factory
-backends = disable_by_file
-disable_by_file_path = /etc/trove/healthcheck_disable
-
 [filter:http_proxy_to_wsgi]
 use = egg:oslo.middleware#http_proxy_to_wsgi
```

### Comparing `trove-21.0.0.0rc2/etc/trove/api-paste.ini.test` & `trove-8.0.1/etc/trove/api-paste.ini.test`

 * *Files 8% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 paste.filter_factory = trove.tests.fakes.keystone:filter_factory
 service_protocol = http
 service_host = 127.0.0.1
 service_port = 5000
 auth_host = 127.0.0.1
 auth_port = 35357
 auth_protocol = http
-www_authenticate_uri = http://127.0.0.1/identity/
+auth_uri = http://127.0.0.1/identity/
 signing_dir = /tmp/keystone-signing-trove
 
 [filter:authorization]
 paste.filter_factory = trove.common.auth:AuthorizationMiddleware.factory
 
 [filter:contextwrapper]
 paste.filter_factory = trove.common.wsgi:ContextMiddleware.factory
```

### Comparing `trove-21.0.0.0rc2/etc/trove/trove-logging-guestagent.conf` & `trove-8.0.1/etc/trove/trove-logging-guestagent.conf`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/etc/trove/trove.conf.test` & `trove-8.0.1/etc/trove/trove.conf.test`

 * *Files 10% similar despite different names*

```diff
@@ -1,36 +1,40 @@
 [DEFAULT]
 
 # Fake out the remote implementations
 remote_nova_client = trove.tests.fakes.nova.fake_create_nova_client
 remote_guest_client = trove.tests.fakes.guestagent.fake_create_guest_client
 remote_swift_client = trove.tests.fakes.swift.fake_create_swift_client
 remote_cinder_client = trove.tests.fakes.nova.fake_create_cinder_client
-remote_neutron_client = trove.tests.fakes.neutron.fake_create_neutron_client
 
 # Fake out the RPC implementation
-transport_url = 'fake:/'
+rpc_backend = fake
 
 # Skip running periodic tasks
 report_interval = 0
 
 # Fake out DNS.
 trove_dns_support = True
 dns_driver = trove.tests.fakes.dns.FakeDnsDriver
 dns_instance_entry_factory = trove.tests.fakes.dns.FakeDnsInstanceEntryFactory
 
+
 # This will remove some of the verbose logging when trying to diagnose tox issues
 default_log_levels=routes.middleware=ERROR,trove.common.auth=WARN
 
 log_file = trovetest.log
+
 use_stderr = False
+
+# Show debugging output in logs (sets DEBUG log level output)
 debug = True
 
 # Address to bind the API server
 bind_host = 0.0.0.0
+
 # Port the bind the API server to
 bind_port = 8779
 
 # Number of workers for the API service. The default will
 # be the number of CPUs available. (integer value)
 #trove_api_workers=None
 
@@ -41,19 +45,21 @@
 # These options are for an admin user in your keystone config.
 # It proxy's the token received from the user to send to nova via this admin users creds,
 # basically acting like the client via that proxy token.
 nova_proxy_admin_user = admin
 nova_proxy_admin_pass = 3de4922d8b6ac5a1aad9
 nova_proxy_admin_tenant_id =
 trove_auth_url = http://0.0.0.0/identity/v2.0
+
 os_region_name = RegionOne
 nova_compute_service_type = compute
 nova_service_name = Compute Service
 
 # Config option for showing the IP address that nova doles out
+network_label_regex = ^private$
 ip_regex = ^(15.|123.)
 black_list_regex = ^(10.0.0.)
 
 # Config options for enabling volume service
 trove_volume_support = True
 nova_volume_service_type = volume
 nova_volume_service_name = Volume Service
@@ -82,28 +88,44 @@
 
 # Guest related conf
 agent_heartbeat_time = 10
 agent_call_low_timeout = 5
 agent_call_high_timeout = 150
 
 server_delete_time_out=10
+use_nova_server_volume = False
 dns_time_out = 120
 resize_time_out = 120
 revert_time_out = 120
 
 # usage notifications
 notification_driver = trove.tests.util.usage
 notification_service_id = mysql:123,percona:abc
 
+# ============ notifer queue kombu connection options ========================
+
+notifier_queue_hostname = localhost
+notifier_queue_userid = guest
+notifier_queue_password = guest
+notifier_queue_ssl = False
+notifier_queue_port = 5672
+notifier_queue_virtual_host = /
+notifier_queue_transport = memory
+
 control_exchange = trove
 
 paste_config_file=api-paste.ini.test
 
+[oslo_messaging_rabbit]
+# AMQP Connection info
+# Deprecated group/name - [DEFAULT]/rabbit_password
+rabbit_password=f7999d1955c5014aa32c
+
+
 [mysql]
-root_on_create = False
 volume_support = True
 device_path = /dev/vdb
 
 [redis]
 # redis uses local storage
 volume_support = False
 # default device_path = None
@@ -131,15 +153,15 @@
 # Period in seconds after which SQLAlchemy should reestablish its connection
 # to the database.
 #
 # MySQL uses a default `wait_timeout` of 8 hours, after which it will drop
 # idle connections. This can result in 'MySQL Gone Away' exceptions. If you
 # notice this, you can lower this value to ensure that SQLAlchemy reconnects
 # before MySQL can drop the connection.
-connection_recycle_time = 3600
+idle_timeout = 3600
 
 [composite:trove]
 use = call:trove.common.wsgi:versioned_urlmap
 /: versions
 /v1.0: troveapi
 
 [app:versions]
@@ -156,15 +178,15 @@
 paste.filter_factory = trove.tests.fakes.keystone:filter_factory
 service_protocol = http
 service_host = 127.0.0.1
 service_port = 5000
 auth_host = 127.0.0.1
 auth_port = 35357
 auth_protocol = http
-www_authenticate_uri = http://127.0.0.1/identity/
+auth_uri = http://127.0.0.1/identity/
 
 [filter:authorization]
 paste.filter_factory = trove.common.auth:AuthorizationMiddleware.factory
 
 [filter:contextwrapper]
 paste.filter_factory = trove.common.wsgi:ContextMiddleware.factory
```

### Comparing `trove-21.0.0.0rc2/generate_examples.py` & `trove-8.0.1/generate_examples.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/README.md` & `trove-8.0.1/integration/README.md`

 * *Files 17% similar despite different names*

```diff
@@ -1,201 +1,191 @@
-# Trove integration script - trovestack
+## Integration dev scripts, tests and docs for Trove.
 
-## Steps to setup environment
+***
 
-Install a fresh Ubuntu 22.04 (jammy) image. We suggest creating a development virtual machine using the image.
+### Steps to setup this environment:
 
-1. Login to the machine as root
-1. Make sure we have git installed
+Install a fresh Ubuntu 14.04 (Trusty Tahr) image ( _We suggest creating a development virtual machine using the image_ )
+
+#### Login to the machine as root
+
+#### Make sure we have git installed:
 
-    ```
     # apt-get update
     # apt-get install git-core -y
-    ```
 
-1. Add a user named ubuntu if you do not already have one:
+#### Add a user named ubuntu if you do not already have one:
 
-    ```
     # adduser ubuntu
     # visudo
-    ```
 
-    Add this line to the file below the root user
+  add this line to the file below the root user
 
-        ubuntu  ALL=(ALL:ALL) ALL
+    ubuntu  ALL=(ALL:ALL) ALL
 
-    Or use this if you dont want to type your password to sudo a command:
+    **OR use this if you dont want to type your password to sudo a command**
 
-        ubuntu  ALL=(ALL) NOPASSWD: ALL
+    ubuntu  ALL=(ALL) NOPASSWD: ALL
 
-    if /dev/pts/0 does not have read/write for your user
+  if /dev/pts/0 does not have read/write for your user
 
-        # chmod 666 /dev/pts/0
+    # chmod 666 /dev/pts/0
 
-    > Note that this number can change and if you can not connect to the screen session then the /dev/pts/# needs modding like above.
+  *Note that this number can change and if you can not connect to the screen session then the /dev/pts/# needs modding like above.*
 
-1. Login with ubuntu and download the Trove code.
+#### Login with ubuntu:
 
-    ```shell
     # su ubuntu
     $ mkdir -p /opt/stack
     $ cd /opt/stack
-    ```
 
-    > Note that it is important that you clone the repository
-    here. This is a change from the earlier trove-integration where
-    you could clone trove-integration anywhere you wanted (like HOME)
-    and trove would get cloned for you in the right place. Since
-    trovestack is now in the trove repository, if you wish to test
-    changes that you have made to trove, it is advisable for you to
-    have your trove repository in /opt/stack to avoid another trove
-    repository being cloned for you.
+    *Note that it is important that you clone the repository
+     here. This is a change from the earlier trove-integration where
+     you could clone trove-integration anywhere you wanted (like HOME)
+     and trove would get cloned for you in the right place. Since
+     trovestack is now in the trove repository, if you wish to test
+     changes that you have made to trove, it is advisable for you to
+     have your trove repository in /opt/stack to avoid another trove
+     repository being cloned for you.
+
+#### Clone this repo:
 
-1. Clone this repo and go into the scripts directory
-    ```
     $ git clone https://github.com/openstack/trove.git
-    $ cd trove/integration/scripts/
-    ```
 
-## Running trovestack
+#### Go into the scripts directory:
 
-Run this to get the command list with a short description of each
+    $ cd trove/integration/scripts/
+
+#### Running trovestack:
+*Run this to get the command list with a short description of each*
 
     $ ./trovestack
 
-### Install Trove
-*This brings up trove services and initializes the trove database.*
+#### Install all the dependencies and then install trove via trovestack.
+*This brings up trove (rd-api rd-tmgr) and initializes the trove database.*
 
     $ ./trovestack install
 
-### Connecting to the screen session
+***
+
+#### Connecting to the screen session
 
     $ screen -x stack
 
-If that command fails with the error
+*If that command fails with the error*
 
     Cannot open your terminal '/dev/pts/1'
 
-If that command fails with the error chmod the corresponding /dev/pts/#
+*If that command fails with the error chmod the corresponding /dev/pts/#*
 
     $ chmod 660 /dev/pts/1
 
-### Navigate the log screens
+#### Navigate the log screens
 To produce the list of screens that you can scroll through and select
 
     ctrl+a then "
 
-An example of screen list:
-```
+Num Name
+
 ..... (full list ommitted)
+
 20 c-vol
 21 h-eng
 22 h-api
 23 h-api-cfn
 24 h-api-cw
 25 tr-api
 26 tr-tmgr
 27 tr-cond
-```
 
 Alternatively, to go directly to a specific screen window
 
     ctrl+a then '
 
 then enter a number (like 25) or name (like tr-api)
 
-### Detach from the screen session
+#### Detach from the screen session
 Allows the services to continue running in the background
 
     ctrl+a then d
 
-### Kick start the build/test-init/build-image commands
+***
+
+#### Kick start the build/test-init/build-image commands
 *Add mysql as a parameter to set build and add the mysql guest image. This will also populate /etc/trove/test.conf with appropriate values for running the integration tests.*
 
     $ ./trovestack kick-start mysql
 
-### Initialize the test configuration and set up test users (overwrites /etc/trove/test.conf)
-
-    $ ./trovestack test-init
-
-### Build guest agent image
-The trove guest agent image could be created using `trovestack` script 
-according to the following command:
-
-```shell
-PATH_DEVSTACK_OUTPUT=/opt/stack \
-  ./trovestack build-image \
-  ${datastore_type} \
-  ${guest_os} \
-  ${guest_os_release} \
-  ${dev_mode}
-```
-
-- If the script is running as a part of DevStack, the viriable 
-  `PATH_DEVSTACK_OUTPUT` is set automatically.
-- if `dev_mode=false`, the trove code for guest agent is injected into the 
-  image at the building time.
-- If `dev_mode=true`, no Trove code is injected into the guest image. The guest
-  agent will download Trove code during the service initialization.
-
-For example, build a Mysql image for Ubuntu jammy operating system:
-
-```shell
-$ ./trovestack build-image mysql ubuntu jammy false
-```
-
-### Running Integration Tests
-Check the values in /etc/trove/test.conf in case it has been re-initialized prior to running the tests. For example, from the previous mysql steps:
-
-    "dbaas_datastore": "%datastore_type%",
-    "dbaas_datastore_version": "%datastore_version%",
-
-should be:
-
-    "dbaas_datastore": "mysql",
-    "dbaas_datastore_version": "5.5",
-
-Once Trove is running on DevStack, you can run the integration tests locally.
+*Optional commands if you did not run kick-start*
 
-    $./trovestack int-tests
+#### Initialize the test configuration and set up test users (overwrites /etc/trove/test.conf)
 
-This will runs all of the blackbox tests by default. Use the `--group` option to run a different group:
+    $ ./trovestack test-init
 
-    $./trovestack int-tests --group=simple_blackbox
+#### Build the image and add it to glance
 
-You can also specify the `TESTS_USE_INSTANCE_ID` environment variable to have the integration tests use an existing instance for the tests rather than creating a new one.
+    $ ./trovestack build-image mysql
 
-    $./TESTS_DO_NOT_DELETE_INSTANCE=True TESTS_USE_INSTANCE_ID=INSTANCE_UUID ./trovestack int-tests --group=simple_blackbox
+***
 
-## Reset your environment
+### Reset your environment
 
-### Stop all the services running in the screens and refresh the environment
+#### Stop all the services running in the screens and refresh the environment:
 
     $ killall -9 screen
     $ screen -wipe
     $ RECLONE=yes ./trovestack install
     $ ./trovestack kick-start mysql
 
  or
 
     $ RECLONE=yes ./trovestack install
     $ ./trovestack test-init
     $ ./trovestack build-image mysql
 
-## Recover after reboot
+***
+
+### Recover after reboot
 If the VM was restarted, then the process for bringing up Openstack and Trove is quite simple
 
     $./trovestack start-deps
     $./trovestack start
 
 Use screen to ensure all modules have started without error
 
     $screen -r stack
 
-## VMware Fusion 5 speed improvement
+***
+
+### Running Integration Tests
+Check the values in /etc/trove/test.conf in case it has been re-initialized prior to running the tests. For example, from the previous mysql steps:
+
+    "dbaas_datastore": "%datastore_type%",
+    "dbaas_datastore_version": "%datastore_version%",
+
+should be:
+
+    "dbaas_datastore": "mysql",
+    "dbaas_datastore_version": "5.5",
+
+Once Trove is running on DevStack, you can use the dev scripts to run the integration tests locally.
+
+    $./trovestack int-tests
+
+This will runs all of the blackbox tests by default. Use the --group option to run a different group:
+
+    $./trovestack int-tests --group=simple_blackbox
+
+You can also specify the TESTS_USE_INSTANCE_ID environment variable to have the integration tests use an existing instance for the tests rather than creating a new one.
+
+    $./TESTS_DO_NOT_DELETE_INSTANCE=True TESTS_USE_INSTANCE_ID=INSTANCE_UUID ./trovestack int-tests --group=simple_blackbox
+
+***
+
+### VMware Fusion 5 speed improvement
 Running Ubuntu with KVM or Qemu can be extremely slow without certain optimizations. The following are some VMware settings that can improve performance and may also apply to other virtualization platforms.
 
 1. Shutdown the Ubuntu VM.
 
 2. Go to VM Settings -> Processors & Memory -> Advanced Options.
    Check the "Enable hypervisor applications in this virtual machine"
 
@@ -209,15 +199,15 @@
 6. To verify that KVM is setup properly after the devstack installation you can run these commands.
 ```
 ubuntu@ubuntu:~$ kvm-ok
 INFO: /dev/kvm exists
 KVM acceleration can be used
 ```
 
-## VMware Workstation performance improvements
+### VMware Workstation performance improvements
 
 In recent versions of VMWare, you can get much better performance if you enable the right virtualization options. For example, in VMWare Workstation (found in version 10.0.2), click on VM->Settings->Processor.
 
 You should see a box of "Virtualization Engine" options that can be changed only when the VM is shutdown.
 
 Make sure you check "Virtualize Intel VT-x/EPT or AMD-V/RVI" and "Virtualize CPU performance counters". Set the preferred mode to "Automatic".
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/create_vm` & `trove-8.0.1/integration/scripts/create_vm`

 * *Files 1% similar despite different names*

```diff
@@ -75,11 +75,10 @@
                                % (key, vm_path, local_path))
             file.write("""
 
   end
 end
             """)
 
-
-if __name__ == "__main__":
+if __name__=="__main__":
     conf = Config.load("conf.json")
     conf.write_vagrant_file()
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/15-trove-dep` & `trove-8.0.1/integration/scripts/files/elements/fedora-guest/extra-data.d/15-trove-dep`

 * *Files 7% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 
 # CONTEXT: HOST prior to IMAGE BUILD as SCRIPT USER
 # PURPOSE: Setup the requirements file for use by 15-reddwarf-dep
 
 source $_LIB/die
 
 BRANCH_OVERRIDE=${BRANCH_OVERRIDE:-default}
+ADD_BRANCH=$(basename ${BRANCH_OVERRIDE})
 REQUIREMENTS_FILE=${TROVESTACK_SCRIPTS}/files/requirements/fedora-requirements.txt
 
 [ -n "$TMP_HOOKS_PATH" ] || die "Temp hook path not set"
 [ -e ${REQUIREMENTS_FILE} ] || die "Requirements not found"
 [ -n "$HOST_USERNAME" ] || die "HOST_USERNAME not set"
 
 sudo -Hiu ${HOST_USERNAME} dd if=${REQUIREMENTS_FILE} of=${TMP_HOOKS_PATH}/requirements.txt
@@ -27,20 +28,21 @@
 if [ -f "${DEST}/requirements/${UC_FILE}" ]; then
     echo "Found ${DEST}/requirements/${UC_FILE}, using that"
     sudo -Hiu ${HOST_USERNAME} dd if="${DEST}/requirements/${UC_FILE}" \
         of="${TMP_HOOKS_PATH}/${UC_FILE}"
 else
     UC_DIR=$(pwd)
     UC_BRANCH=${BRANCH_OVERRIDE}
-    if [[ "${UC_BRANCH}" == "default" ]]; then
+    if [ "${ADD_BRANCH}" == "default" ]; then
         UC_BRANCH=master
     fi
 
     set +e
-    curl -o "${UC_DIR}/${UC_FILE}" "https://opendev.org/openstack/requirements/raw/branch/${UC_BRANCH}/${UC_FILE}"
+    curl -o "${UC_DIR}/${UC_FILE}" \
+        https://git.openstack.org/cgit/openstack/requirements/plain/${UC_FILE}?h=${UC_BRANCH}
     set -e
 
     if [ -f "${UC_DIR}/${UC_FILE}" ]; then
         sudo -Hiu ${HOST_USERNAME} dd if="${UC_DIR}/${UC_FILE}" of=${TMP_HOOKS_PATH}/${UC_FILE}
         rm -f "${UC_DIR}/${UC_FILE}"
     fi
 fi
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/20-guest-systemd` & `trove-8.0.1/integration/scripts/files/elements/fedora-guest/extra-data.d/20-guest-systemd`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/62-ssh-key` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-guest/extra-data.d/62-ssh-key`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/15-trove-dep` & `trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/15-trove-dep`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-guest/install.d/62-ssh-key` & `trove-8.0.1/integration/scripts/files/elements/fedora-guest/install.d/62-ssh-key`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-mongodb/install.d/10-mongodb` & `trove-8.0.1/integration/scripts/files/elements/fedora-mongodb/install.d/10-mongodb`

 * *Files 3% similar despite different names*

```diff
@@ -3,16 +3,14 @@
 # CONTEXT: GUEST during CONSTRUCTION as ROOT
 # PURPOSE: Install controller base required packages
 
 set -e
 set -o xtrace
 
 cat > "/etc/rc.local" << _EOF_
-#!/bin/bash
-
 # Make sure to disable Linux kernel feature transparent huge pages,
 # it will affect greatly both memory usage and latency in a negative way.
 # See: http://docs.mongodb.org/manual/tutorial/transparent-huge-pages/
 if test -f /sys/kernel/mm/redhat_transparent_hugepage/defrag; then
   echo never > /sys/kernel/mm/redhat_transparent_hugepage/defrag
 fi
 if test -f /sys/kernel/mm/redhat_transparent_hugepage/enabled; then
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/fedora-percona/install.d/10-mysql` & `trove-8.0.1/integration/scripts/files/elements/fedora-percona/install.d/10-mysql`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-cassandra/install.d/10-cassandra` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-cassandra/install.d/10-cassandra`

 * *Files 25% similar despite different names*

```diff
@@ -1,37 +1,25 @@
 #!/bin/bash
 
 set -ex
 set -o xtrace
 
-# Set CASSANDRA_JAVA to override which release of Java to use, defaults to 8
-if [ -z "$CASSANDRA_JAVA" ]; then CASSANDRA_JAVA=8; fi
-
-# Set CASSANDRA_RELEASE to override which casandra release to use, defaults to 311x
-#  For current releases supported, see http://cassandra.apache.org/download/ or
-#  http://dl.bintray.com/apache/cassandra/dists/
-if [ -z "$CASSANDRA_RELEASE" ]; then CASSANDRA_RELEASE=21x; fi
-
 export DEBIAN_FRONTEND=noninteractive
 apt-get --allow-unauthenticated install -qy curl
-echo "deb http://www.apache.org/dist/cassandra/debian ${CASSANDRA_RELEASE} main" >> /etc/apt/sources.list.d/cassandra.sources.list
-curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -
-apt-get -y update
-apt-get --allow-unauthenticated install -qy openjdk-${CASSANDRA_JAVA}-jdk expect python-dev
-apt-get --allow-unauthenticated install -qy libxml2-dev ntp mc libev4 libev-dev
+echo "deb http://debian.datastax.com/community stable main" >> /etc/apt/sources.list.d/cassandra.sources.list
+curl -L http://debian.datastax.com/debian/repo_key | apt-key add -
+apt-get update
+apt-get --allow-unauthenticated install -qy openjdk-7-jdk expect python-dev
+apt-get --allow-unauthenticated install -qy libxml2-dev ntp mc
 apt-get --allow-unauthenticated install -qy libxslt1-dev python-pexpect
 apt-get --allow-unauthenticated install -qy python-migrate build-essential
-apt-get --allow-unauthenticated install -qy python-setuptools python-pip python-snappy
 
-apt-get --allow-unauthenticated install -qy cassandra cassandra-tools
-
-pip2 install Cython
+apt-get --allow-unauthenticated install dsc21=2.1.* cassandra=2.1.* -qy
 
 # The Python Driver 2.0 for Apache Cassandra.
 pip2 install cassandra-driver
-
 # Sorted sets support for the Python driver.
 pip2 install blist
 
 service cassandra stop
 rm -rf /var/lib/cassandra/data/system/*
 service cassandra start
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-couchdb/install.d/10-couchdb` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-couchdb/install.d/10-couchdb`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/README.md` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,30 @@
- Creates an image for DB2 Express-C v11.1
+ Creates an image for DB2 Express-C v10.5
 
  The packages for DB2 Express-C can be downloaded from:
- https://www.ibm.com/developerworks/downloads/im/db2express/
- and click on the link "Download" button and then click on
- "DB2 Express-C for Linux 64-bit".
+ http://www-01.ibm.com/software/data/db2/express-c/download.html
+ and click on the link "DB2 Express-C for Linux 64-bit".
  New users can either get an IBM ID or click on the "Proceed without an
  IBM ID". User is provided with a registration form which needs to be
  completed in order to proceed further to download the DB2 Express-C
- packages. After accepting the license agreement, user can download
+ packages. After accepting the license agreement, user can download the
  the DB2 Express-C package (.tar.gz file).
 
  There are 2 options for making the DB2 Express-C package accessible to
  the Trove disk-image building process:
   - place the package in a private repository and set the environment
     variable DATASTORE_PKG_LOCATION with the url to this private
     repository.
-    e.g. export DATASTORE_PKG_LOCATION="http://www.foo.com/db2/v11.1_linuxx64_expc.tar.gz"
+    e.g. export DATASTORE_PKG_LOCATION="http://www.foo.com/db2/v10.5_linuxx64_expc.tar.gz"
 
   - download the package and place it in any directory on the local
     filesystem that the trovestack script can access. Set the
     environment variable DATASTORE_PKG_LOCATION with the full path to
     the downloaded package.
-    e.g. export DATASTORE_PKG_LOCATION="/home/stack/db2/v11.1_linuxx64_expc.tar.gz"
+    e.g. export DATASTORE_PKG_LOCATION="/home/stack/db2/v10.5_linuxx64_expc.tar.gz"
 
  The environment variables used are as follows:
 
  DATASTORE_PKG_LOCATION - is the place where user stores the DB2
              Express-C package after registration. This can either be a
              url to a private repository or the full path to the
              downloaded package on a local filesystem.
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/extra-data.d/20-copy-db2-pkgs` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/extra-data.d/20-copy-db2-pkgs`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-db2/install.d/10-db2` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-db2/install.d/10-db2`

 * *Files 13% similar despite different names*

```diff
@@ -8,41 +8,38 @@
 
 # DB2_PKG_LOCATION points to the directory where the DB2 packages
 # are located to install.
 DB2_PKG_LOCATION="/db2"
 mkdir ${DB2_PKG_LOCATION}
 cd ${DB2_PKG_LOCATION}
 
-# DB2 install location
-DB2_INSTALL_LOCATION="/opt/ibm/db2/current"
-
 # DB2 install requires the hostname to be resolved correctly
 host_name=`hostname`
 echo "127.0.0.1 ${host_name}" >> /etc/hosts
 
 tar -xvzf /tmp/in_target.d/db2.tar.gz
 
 # installing dependencies
 apt-get --allow-unauthenticated install libaio1
 apt-get --allow-unauthenticated install libstdc++6
 
-# start the installation process. Accepts the default installation directory '/opt/ibm/db2/current'
-${DB2_PKG_LOCATION}/expc/db2_install -b ${DB2_INSTALL_LOCATION} -f sysreq -l ${DB2_PKG_LOCATION}/db2_install.log -y
+# start the installation process. Accepts the default installation directory '/opt/ibm/db2/V10.5'
+${DB2_PKG_LOCATION}/expc/db2_install -b /opt/ibm/db2/V10.5 -f sysreq -l ${DB2_PKG_LOCATION}/db2_install.log
 
 # create the DB2 users.
 # DB2 instance owner - db2inst1
 # DB2 fence user - db2fenc1
 # DB2 admin user - db2das1
 useradd -m db2inst1
 useradd -m db2fenc1
 useradd -m db2das1
 
 # Create the DB2 server instance
-${DB2_INSTALL_LOCATION}/instance/db2icrt -a server -u db2fenc1 db2inst1
-${DB2_INSTALL_LOCATION}/cfg/db2ln
+/opt/ibm/db2/V10.5/instance/db2icrt -a server -u db2fenc1 db2inst1
+/opt/ibm/db2/V10.5/cfg/db2ln
 
 # Configure DB2 server instance to communicate via TCP/IP on a particulat port.
 echo 'db2c_db2inst1 50000/tcp # DB2 connection service port' >> /etc/services
 
 # Configure DB2 to use the TCP/IP settings defined above.
 su - db2inst1 -c "db2 update database manager configuration using svcename db2c_db2inst1"
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-percona/install.d/30-mysql` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-percona/install.d/30-mysql`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-redis/install.d/30-redis` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-redis/install.d/30-redis`

 * *Files 1% similar despite different names*

```diff
@@ -17,16 +17,14 @@
 # 'somaxconn' and 'tcp_max_syn_backlog' in order to get the desired effect.
 net.ipv4.tcp_max_syn_backlog=1024
 net.core.somaxconn=1024
 
 _EOF_
 
 cat > "/etc/rc.local" << _EOF_
-#!/bin/bash
-
 # Make sure to disable Linux kernel feature transparent huge pages,
 # it will affect greatly both memory usage and latency in a negative way.
 if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
   echo never > /sys/kernel/mm/transparent_hugepage/defrag
 fi
 if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
   echo never > /sys/kernel/mm/transparent_hugepage/enabled
@@ -109,8 +107,8 @@
 # This may be required for high-concurrency environments. Redis itself cannot
 # alter its limits as it is not being run as root.
 ULIMIT=65536
 
 _EOF_
 
 # Install Python driver for Redis ('redis-py').
-pip3 install redis
+pip2 install redis
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-vertica/install.d/97-vertica` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-vertica/install.d/97-vertica`

 * *Files 2% similar despite different names*

```diff
@@ -37,16 +37,14 @@
 
 # Compile the SDK examples - the supplied UDFs can then be loaded
 cd /opt/vertica/sdk/examples
 TMPDIR=/tmp JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 make
 cd
 
 cat > "/etc/rc.local" << _EOF_
-#!/bin/bash
-
 # Vertica requires THP to be turned off
 if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
   echo never > /sys/kernel/mm/transparent_hugepage/defrag
 fi
 if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
   echo never > /sys/kernel/mm/transparent_hugepage/enabled
 fi
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/install.d/31-fix-init-file` & `trove-8.0.1/integration/scripts/files/elements/ubuntu-xenial-redis/install.d/31-fix-init-file`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent-dev.service` & `trove-8.0.1/integration/scripts/files/trove-guest.systemd.conf`

 * *Files 17% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 [Unit]
-Description=OpenStack Trove Guest Agent Service for Development
-After=syslog.target network.target
-
-[Install]
-WantedBy=multi-user.target
+Description=Trove Guest
+After=syslog.target
+After=network.target
 
 [Service]
 Type=simple
 User=GUEST_USERNAME
 Group=GUEST_USERNAME
 
-# This script is only for testing purpose for dev_mode=true, the controller
-# IP address should be defined in /etc/trove/controller.conf, e.g.
-# CONTROLLER=192.168.32.151
-EnvironmentFile=/etc/trove/controller.conf
+ExecStartPre=/bin/bash -c "sudo mkdir -p GUEST_LOGDIR ; sudo chown GUEST_USERNAME:root GUEST_LOGDIR"
 
 # If ~/trove-installed does not exist, copy the trove source from
 # the user's development environment, then touch the sentinel file
-ExecStartPre=/bin/bash -c "test -e /home/GUEST_USERNAME/trove-installed || sudo rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/GUEST_USERNAME/.ssh/id_rsa' -az --exclude='.*' HOST_SCP_USERNAME@$CONTROLLER:PATH_TROVE/ /home/GUEST_USERNAME/trove && touch /home/GUEST_USERNAME/trove-installed"
+ExecStartPre=/bin/bash -c "test -d /home/GUEST_USERNAME/trove-installed || sudo -u GUEST_USERNAME rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no' -avz --exclude='.*' HOST_SCP_USERNAME@CONTROLLER_IP:PATH_TROVE/ /home/GUEST_USERNAME/trove && touch /home/GUEST_USERNAME/trove-installed"
 
-ExecStartPre=/bin/bash -c "sudo chown -R GUEST_USERNAME:root /etc/trove /var/log/trove/ /home/GUEST_USERNAME/trove"
+# If /etc/trove does not exist, create it and then copy the trove-guestagent.conf
+# from /etc/trove on the user's development environment,
+ExecStartPre=/bin/bash -c "test -d /etc/trove/conf.d || sudo mkdir -p /etc/trove/conf.d && sudo -u GUEST_USERNAME rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no' -avz --exclude='.*' HOST_SCP_USERNAME@CONTROLLER_IP:/etc/trove/trove-guestagent.conf ~GUEST_USERNAME/ && sudo mv ~GUEST_USERNAME/trove-guestagent.conf /etc/trove/conf.d/trove-guestagent.conf"
 
-# Take care of the changes in requirements.txt
-ExecStartPre=/bin/bash -c "sudo /opt/guest-agent-venv/bin/pip install -r /home/GUEST_USERNAME/trove/requirements.txt -c /opt/upper-constraints.txt"
+ExecStartPre=/bin/bash -c "sudo chown -R GUEST_USERNAME:root /etc/trove"
 
-# Start guest-agent.service in virtual environment
-ExecStart=/bin/bash -c "/opt/guest-agent-venv/bin/python /home/GUEST_USERNAME/trove/contrib/trove-guestagent --config-dir=/etc/trove/conf.d"
+ExecStart=/home/GUEST_USERNAME/trove/contrib/trove-guestagent --config-dir=/etc/trove/conf.d
 
+# Give a reasonable amount of time for the server to start up/shut down
 TimeoutSec=300
-Restart=on-failure
+
+# PgSql doesn't play nice with PrivateTmp
+PrivateTmp=false
+
+[Install]
+WantedBy=multi-user.target
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/keys/id_rsa` & `trove-8.0.1/integration/scripts/files/keys/id_rsa`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/requirements/fedora-requirements.txt` & `trove-8.0.1/integration/scripts/files/requirements/ubuntu-requirements.txt`

 * *Files 16% similar despite different names*

```diff
@@ -1,31 +1,30 @@
 # The order of packages is significant, because pip processes them in the order
 # of appearance. Changing the order has an impact on the overall integration
 # process, which may cause wedges in the gate later.
 unittest2
 testtools
 extras
-python-novaclient>=2.22.0
+python-novaclient>=2.18.0
 python-swiftclient>=2.2.0
 python-cinderclient>=1.1.0
 python-keystoneclient>=2.0.0,!=2.1.0  # Apache-2.0
 kombu>=2.5.0
-babel>=1.3
-python-heatclient>=0.3.0
+six>=1.7.0
+babel
+python-heatclient>=0.2.9
 passlib
-jinja2>=2.6
+jinja2
 PyMySQL>=0.6.2  # MIT License
-python-neutronclient>=2.3.11,<3
+python-neutronclient>=2.3.6,<3
 netifaces>=0.10.4
-oslo.config>=1.9.3  # Apache-2.0
-oslo.messaging>=1.8.0  # Apache-2.0
-oslo.i18n>=1.5.0  # Apache-2.0
-oslo.serialization>=1.4.0               # Apache-2.0
+oslo.config>=1.4.0 # Apache-2.0
+oslo.messaging>=1.4.0,!=1.5.0
+oslo.i18n>=1.0.0
+oslo.serialization>=1.0.0
 oslo.service>=0.1.0 # Apache-2.0
-oslo.utils>=1.4.0                       # Apache-2.0
-oslo.log>=1.8.0  # Apache-2.0
+oslo.utils>=1.1.0
 osprofiler>=0.3.0
-oslo.concurrency>=1.8.0         # Apache-2.0
-pexpect>=3.1,!=3.3
+oslo.concurrency>=0.3.0
 enum34;python_version=='2.7' or python_version=='2.6' or python_version=='3.3'  # BSD
-cryptography>=2.1.4  # BSD/Apache-2.0
+pycrypto>=2.6  # Public Domain
 xmltodict>=0.10.1  # MIT
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/requirements/ubuntu-requirements.txt` & `trove-8.0.1/integration/scripts/files/requirements/fedora-requirements.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 # The order of packages is significant, because pip processes them in the order
 # of appearance. Changing the order has an impact on the overall integration
 # process, which may cause wedges in the gate later.
 unittest2
 testtools
 extras
-python-novaclient>=2.18.0
+python-novaclient>=2.22.0
 python-swiftclient>=2.2.0
 python-cinderclient>=1.1.0
 python-keystoneclient>=2.0.0,!=2.1.0  # Apache-2.0
 kombu>=2.5.0
-babel
-python-heatclient>=0.2.9
+babel>=1.3
+python-heatclient>=0.3.0
 passlib
-jinja2
+jinja2>=2.6
 PyMySQL>=0.6.2  # MIT License
-python-neutronclient>=2.3.6,<3
+python-neutronclient>=2.3.11,<3
 netifaces>=0.10.4
-oslo.config>=1.4.0 # Apache-2.0
-oslo.messaging>=1.4.0,!=1.5.0
-oslo.i18n>=1.0.0
-oslo.serialization>=1.0.0
+oslo.config>=1.9.3  # Apache-2.0
+oslo.messaging>=1.8.0  # Apache-2.0
+oslo.i18n>=1.5.0  # Apache-2.0
+oslo.serialization>=1.4.0               # Apache-2.0
 oslo.service>=0.1.0 # Apache-2.0
-oslo.utils>=1.1.0
+oslo.utils>=1.4.0                       # Apache-2.0
+oslo.log>=1.8.0  # Apache-2.0
 osprofiler>=0.3.0
-oslo.concurrency>=0.3.0
+oslo.concurrency>=1.8.0         # Apache-2.0
+pexpect>=3.1,!=3.3
 enum34;python_version=='2.7' or python_version=='2.6' or python_version=='3.3'  # BSD
-cryptography>=2.1.4  # BSD/Apache-2.0
+pycrypto>=2.6  # Public Domain
 xmltodict>=0.10.1  # MIT
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/files/trove-guest.systemd.conf` & `trove-8.0.1/integration/scripts/files/trove-guest.upstart.conf`

 * *Files 26% similar despite different names*

```diff
@@ -1,37 +1,40 @@
-[Unit]
-Description=Trove Guest
-After=syslog.target network.target
+description "Trove Guest"
+author "Auto-Gen"
 
-[Install]
-WantedBy=multi-user.target
+start on (filesystem and net-device-up IFACE!=lo)
+stop on runlevel [016]
+chdir /var/run
+pre-start script
+    mkdir -p /var/run/trove
+    chown GUEST_USERNAME:root /var/run/trove/
+
+    mkdir -p /var/lock/trove
+    chown GUEST_USERNAME:root /var/lock/trove/
+
+    mkdir -p GUEST_LOGDIR
+    chown GUEST_USERNAME:root GUEST_LOGDIR
+
+    # Copy the trove source from the user's development environment
+    if [ ! -d /home/GUEST_USERNAME/trove ]; then
+        sudo -u GUEST_USERNAME rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no' -avz --exclude='.*' HOST_SCP_USERNAME@CONTROLLER_IP:PATH_TROVE/ /home/GUEST_USERNAME/trove
+    fi
+
+    # Ensure conf dir exists and is readable
+    mkdir -p /etc/trove/conf.d
+    chmod -R +r /etc/trove
+end script
+
+script
+    # For backwards compatibility until https://review.openstack.org/#/c/100381 merges
+    TROVE_CONFIG="--config-dir=/etc/trove/conf.d"
+    if [ ! -f /etc/trove/conf.d/guest_info ] && [ ! -f /etc/trove/conf.d/trove-guestagent.conf ]; then
+
+        chmod +r /etc/guest_info
+        sudo -u GUEST_USERNAME rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no' -avz --exclude='.*' HOST_SCP_USERNAME@CONTROLLER_IP:/etc/trove/trove-guestagent.conf ~GUEST_USERNAME/
+        mv ~GUEST_USERNAME/trove-guestagent.conf /etc/trove/trove-guestagent.conf
+        TROVE_CONFIG="--config-file=/etc/guest_info --config-file=/etc/trove/trove-guestagent.conf"
 
-[Service]
-Type=simple
-User=GUEST_USERNAME
-Group=GUEST_USERNAME
+    fi
 
-# This script is only for testing purpose for dev_mode=true, the controller
-# IP address should be defined in /etc/trove/controller.conf, e.g.
-# CONTROLLER=192.168.32.151
-EnvironmentFile=/etc/trove/controller.conf
-
-ExecStartPre=/bin/bash -c "sudo mkdir -p GUEST_LOGDIR"
-
-# If ~/trove-installed does not exist, copy the trove source from
-# the user's development environment, then touch the sentinel file
-ExecStartPre=/bin/bash -c "test -e /home/GUEST_USERNAME/trove-installed || sudo rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/GUEST_USERNAME/.ssh/id_rsa' -avz --exclude='.*' HOST_SCP_USERNAME@$CONTROLLER:PATH_TROVE/ /home/GUEST_USERNAME/trove && touch /home/GUEST_USERNAME/trove-installed"
-
-# If /etc/trove does not exist, create it and then copy the trove-guestagent.conf
-# from /etc/trove on the user's development environment,
-ExecStartPre=/bin/bash -c "test -d /etc/trove/conf.d || sudo mkdir -p /etc/trove/conf.d && sudo rsync -e 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i /home/GUEST_USERNAME/.ssh/id_rsa' -avz --exclude='.*' HOST_SCP_USERNAME@$CONTROLLER:/etc/trove/trove-guestagent.conf ~GUEST_USERNAME/ && sudo mv ~GUEST_USERNAME/trove-guestagent.conf /etc/trove/conf.d/trove-guestagent.conf"
-
-ExecStartPre=/bin/bash -c "sudo chown -R GUEST_USERNAME:root /etc/trove /home/GUEST_USERNAME/trove GUEST_LOGDIR"
-
-# Start trove-guest.service
-ExecStart=/bin/bash -c "/home/GUEST_USERNAME/trove/contrib/trove-guestagent --config-dir=/etc/trove/conf.d"
-
-TimeoutSec=300
-Restart=on-failure
-
-# PostgreSQL doesn't play nice with PrivateTmp
-PrivateTmp=false
+    exec su -c "/home/GUEST_USERNAME/trove/contrib/trove-guestagent $TROVE_CONFIG" GUEST_USERNAME
+end script
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/functions` & `trove-8.0.1/integration/scripts/functions`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,8 @@
 #!/bin/bash
-
-set -x
-
 # This file format was stolen from devstack <3
 
 # This method was stolen from devstack
 # git clone only if directory doesn't exist already.  Since ``DEST`` might not
 # be owned by the installation user, we create the directory and change the
 # ownership to the proper user.
 # Set global RECLONE=yes to simulate a clone when dest-dir exists
@@ -233,31 +230,30 @@
 # (Fedora, RHEL, CentOS, etc).
 # is_fedora
 function is_fedora {
     if [[ -z "$os_VENDOR" ]]; then
         GetOSVersion
     fi
 
-    [ "$os_VENDOR" = "Fedora" ] || [ "$os_VENDOR" = "Red Hat" ]\
-    || [ "$os_VENDOR" = "CentOS" ] || [ "$os_VENDOR" = "CentOSStream" ]
+    [ "$os_VENDOR" = "Fedora" ] || [ "$os_VENDOR" = "Red Hat" ] || [ "$os_VENDOR" = "CentOS" ]
 }
 
 
 # Determine if current distribution is a SUSE-based distribution
 # (openSUSE, SLE).
 # is_suse
 function is_suse {
     if [[ -z "$os_VENDOR" ]]; then
         GetOSVersion
     fi
 
     [ "$os_VENDOR" = "openSUSE" ] || [ "$os_VENDOR" = "SUSE LINUX" ]
 }
 
-# Get the path to the directory where python executables are installed.
+# Get the path to the direcotry where python executables are installed.
 # get_python_exec_prefix
 function get_python_exec_prefix() {
     if is_fedora || is_suse; then
         echo "/usr/bin"
     else
         echo "/usr/local/bin"
     fi
@@ -301,15 +297,15 @@
     local PROJECT=$1
     local REPO_DIR=$2
     local REPO_BRANCH=$3
     local REPO_BRANCH_VAR_NAME=$4
 
     if [ -n "$REPO_BRANCH" ]; then
         pushd "$REPO_DIR"
-        if [ $PROJECT == "diskimage-builder" ]; then
+        if [ $PROJECT == "diskimage-builder" ] || [ $PROJECT == "tripleo-image-elements" ]; then
             REPO_BRANCH=master
         fi
         CURRENT_BRANCH=$(git branch | grep "\*" | awk '{print $2}')
         GIT_STATUS=$(git checkout "$REPO_BRANCH" &> /dev/null || echo "failed")
         if [[ "$GIT_STATUS" = "failed" ]]; then
             exclaim "${COLOR_RED}Could not switch to branch/commit '$REPO_BRANCH' in $PROJECT, exiting${COLOR_NONE}"
             echo "Please set '$REPO_BRANCH_VAR_NAME' to a valid branch/commit and try again."
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/localrc.rc` & `trove-8.0.1/integration/scripts/localrc.rc`

 * *Files 7% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 RABBIT_PASSWORD=$RABBIT_PASSWORD
 SERVICE_TOKEN=$SERVICE_TOKEN
 ADMIN_PASSWORD=$ADMIN_PASSWORD
 SERVICE_PASSWORD=$SERVICE_PASSWORD
 
 IP_VERSION=4
 TROVE_LOGDIR=$TROVE_LOGDIR
+TROVE_AUTH_CACHE_DIR=$TROVE_AUTH_CACHE_DIR
 
 # Enable the Trove plugin for devstack
 enable_plugin trove $TROVE_REPO $TROVE_BRANCH
 
 # Enable Trove, Swift, and Heat
 ENABLED_SERVICES+=,trove,tr-api,tr-tmgr,tr-cond
 ENABLED_SERVICES+=,s-proxy,s-object,s-container,s-account
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/trovestack` & `trove-8.0.1/integration/scripts/trovestack`

 * *Files 7% similar despite different names*

```diff
@@ -5,16 +5,14 @@
 #                                                                             #
 # This script provides all the functionality to run all the steps from        #
 # setting up the environment, resetting the nova database to running the      #
 # test.                                                                       #
 #                                                                             #
 ###############################################################################
 
-set -x
-
 SCRIPT_DIRNAME=$(dirname "$0")
 PATH_TROVE=${PATH_TROVE:=$(readlink -f "${SCRIPT_DIRNAME}"/../..)}
 TROVESTACK_SCRIPTS=${TROVESTACK_SCRIPTS:=$(readlink -f "${SCRIPT_DIRNAME}")}
 TROVESTACK_TESTS=$TROVESTACK_SCRIPTS/../tests/
 DEFAULT_LOCAL_CONF=local.conf.rc
 DEFAULT_LOCALRC=localrc.rc
 LOCAL_CONF=local.conf
@@ -77,14 +75,16 @@
 [[ -f $PATH_DEVSTACK_SRC/functions-common ]] && source $PATH_DEVSTACK_SRC/functions-common
 [[ -f $PATH_DEVSTACK_SRC/functions ]] && source $PATH_DEVSTACK_SRC/functions
 [[ -f $PATH_DEVSTACK_SRC/lib/apache ]] && source $PATH_DEVSTACK_SRC/lib/apache
 
 # Set up variables for the CONF files - this has to happen after loading trovestack.rc, since
 # TROVE_CONF_DIR is defined there - these will be used by devstack too
 export TROVE_CONF=$TROVE_CONF_DIR/trove.conf
+export TROVE_TASKMANAGER_CONF=$TROVE_CONF_DIR/trove-taskmanager.conf
+export TROVE_CONDUCTOR_CONF=$TROVE_CONF_DIR/trove-conductor.conf
 export TROVE_GUESTAGENT_CONF=$TROVE_CONF_DIR/trove-guestagent.conf
 export TROVE_API_PASTE_INI=$TROVE_CONF_DIR/api-paste.ini
 export TEST_CONF=$TROVE_CONF_DIR/test.conf
 
 # Public facing bits
 SERVICE_PROTOCOL=${SERVICE_PROTOCOL:-http}
 NETWORK_INTERFACE=${NETWORK_INTERFACE:-eth0}
@@ -93,18 +93,23 @@
 BRIDGE_IP=${BRIDGE_IP:-172.24.4.1}
 KEYSTONE_AUTH_HOST=${KEYSTONE_AUTH_HOST:-$SERVICE_HOST}
 KEYSTONE_AUTH_PROTOCOL=${KEYSTONE_AUTH_PROTOCOL:-$SERVICE_PROTOCOL}
 KEYSTONE_AUTH_PORT=${KEYSTONE_AUTH_PORT:-35357}
 GLANCE_HOSTPORT=${GLANCE_HOSTPORT:-$SERVICE_HOST:9292}
 GLANCE_SERVICE_PROTOCOL=${GLANCE_SERVICE_PROTOCOL:-http}
 
+# The following depends on whether neutron is used or nova-network
+# neutron uses a bridge, nova-network does not
+[[ $ENABLE_NEUTRON = true ]] && CONTROLLER_IP=$BRIDGE_IP || CONTROLLER_IP=$NETWORK_GATEWAY
+
 # PATH_TROVE more than likely has file separators, which sed does not like
 # This will escape them
 ESCAPED_PATH_TROVE=$(echo $PATH_TROVE | sed 's/\//\\\//g')
 ESCAPED_TROVESTACK_SCRIPTS=$(echo $TROVESTACK_SCRIPTS | sed 's/\//\\\//g')
+TROVE_AUTH_CACHE_DIR=${TROVE_AUTH_CACHE_DIR:-/var/cache/trove}
 TROVE_LOGDIR=${TROVE_LOGDIR:-$DEST/logs}
 TROVE_DEVSTACK_SETTINGS="$DEST/trove/devstack/settings"
 TROVE_DEVSTACK_PLUGIN="$DEST/trove/devstack/plugin.sh"
 # DATASTORE_PKG_LOCATION defines the location from where the datastore packages
 # can be accessed by the DIB elements. This is applicable only for datastores
 # that do not have a public repository from where their packages can be accessed.
 # This can either be a url to a private repository or a location on the local
@@ -122,17 +127,16 @@
 if is_fedora; then
     PKG_INSTALL_OPTS=""
     PKG_MGR=dnf
     PKG_GET_ARGS="-y"
 else
     PKG_INSTALL_OPTS="DEBIAN_FRONTEND=noninteractive"
     PKG_MGR=apt-get
-    PKG_GET_ARGS="-y --allow-unauthenticated --force-yes -qq"
+    PKG_GET_ARGS="-y --allow-unauthenticated --force-yes"
 fi
-
 PKG_INSTALL_ARG="install"
 PKG_UPDATE_ARG="update"
 
 ###############################################################################
 # Utility functions
 ###############################################################################
 
@@ -187,14 +191,16 @@
                 cat ${filename}
             else
                 exclaim "File '${filename}' not found"
             fi
         done
         exclaim "Dumping pip modules:"
         pip freeze | sort
+        exclaim "Dumping domain list:"
+        openstack --os-cloud=devstack-admin domain list
         exclaim "Dumping configuration completed"
         set -e
     fi
 }
 
 # Add a flavor and a corresponding flavor.resize
 # (flavor.resize adds 16 to the memory and one more vcpu)
@@ -258,14 +264,15 @@
     pkg_install python-pip
     if is_fedora; then
       pkg_install git gettext
     else
       #pkg_install git-core kvm-ipxe gettext
       pkg_install git-core gettext
     fi
+    sudo -H $HTTP_PROXY pip install --upgrade pip dib-utils
 }
 
 function install_devstack_code() {
     exclaim "Installing devstack..."
     # Installs devstack (if needed).
     if [ ! -d $PATH_DEVSTACK_SRC ]; then
         echo "DevStack not in a shared folder, cloning from git."
@@ -317,15 +324,15 @@
     GIT_NAME=$1
     PATH_ARG=$2
     REVIEW_ARG=$3
     for review in `echo $REVIEW_ARG| tr ":" "\n"`
     do
         # This should be the ref spec for what we pull
         pushd $PATH_ARG
-        git_timed pull https://review.opendev.org/p/openstack/$GIT_NAME refs/changes/$review
+        git_timed pull https://review.openstack.org/p/openstack/$GIT_NAME refs/changes/$review
         popd
     done
 }
 
 function fixup_broken_devstack() {
     # Nothing to do here, devstack is working
     :
@@ -510,31 +517,106 @@
         else
             field="\$$(($1 + 1))"
         fi
         echo "$data" | awk -F'[ \t]*\\|[ \t]*' "{print $field}"
     done
 }
 
+function get_glance_id () {
+    echo `$@ | grep ' id ' | get_field 2`
+}
+
 function set_bin_path() {
     if is_fedora; then
        sed -i "s|%bin_path%|/usr/bin|g" $TEST_CONF
     else
        sed -i "s|%bin_path%|/usr/local/bin|g" $TEST_CONF
     fi
 }
 
+function set_mysql_pkg() {
+    if is_fedora; then
+       MYSQL_PKG="mysql-community-server"
+    else
+       MYSQL_PKG="mysql-server-5.6"
+    fi
+}
+
+
 function cmd_set_datastore() {
-    rd_manage datastore_update "$datastore" ""
-    # Use image tags for datastore version.
-    rd_manage datastore_version_update "${DATASTORE_TYPE}" "${DATASTORE_VERSION}" "${DATASTORE_TYPE}" "" "" 1 --image-tags trove
-    rd_manage datastore_update "${DATASTORE_TYPE}" "${DATASTORE_VERSION}"
+    local IMAGEID=$1
+    local DATASTORE_TYPE=$2
+    local RESTART_TROVE=${3:-$(get_bool RESTART_TROVE "true")}
+
+    # rd_manage datastore_update <datastore_name> <default_version>
+    rd_manage datastore_update "$DATASTORE_TYPE" ""
+    PACKAGES=${PACKAGES:-""}
+    if [ "$DATASTORE_TYPE" == "mysql" ]; then
+        set_mysql_pkg
+        PACKAGES=${PACKAGES:-$MYSQL_PKG}
+        VERSION="5.6"
+    elif [ "$DATASTORE_TYPE" == "percona" ]; then
+        PACKAGES=${PACKAGES:-"percona-server-server-5.6"}
+        VERSION="5.6"
+    elif [ "$DATASTORE_TYPE" == "pxc" ]; then
+        PACKAGES=${PACKAGES:-"percona-xtradb-cluster-server-5.6"}
+        VERSION="5.6"
+    elif [ "$DATASTORE_TYPE" == "mariadb" ]; then
+        PACKAGES=${PACKAGES:-"mariadb-server"}
+        VERSION="10.1"
+    elif [ "$DATASTORE_TYPE" == "mongodb" ]; then
+        PACKAGES=${PACKAGES:-"mongodb-org"}
+        VERSION="3.2"
+    elif [ "$DATASTORE_TYPE" == "redis" ]; then
+        PACKAGES=${PACKAGES:-""}
+        VERSION="3.2.6"
+    elif [ "$DATASTORE_TYPE" == "cassandra" ]; then
+        PACKAGES=${PACKAGES:-"cassandra"}
+        VERSION="2.1.0"
+    elif [ "$DATASTORE_TYPE" == "couchbase" ]; then
+        PACKAGES=${PACKAGES:-"couchbase-server"}
+        VERSION="2.2.0"
+    elif [ "$DATASTORE_TYPE" == "postgresql" ]; then
+        PACKAGES=${PACKAGES:-"postgresql-9.4"}
+        VERSION="9.4"
+    elif [ "$DATASTORE_TYPE" == "couchdb" ]; then
+        PACKAGES=${PACKAGES:-"couchdb"}
+        VERSION="1.6.1"
+    elif [ "$DATASTORE_TYPE" == "vertica" ]; then
+        PACKAGES=${PACKAGES:-"vertica"}
+        VERSION="7.1"
+    elif [ "$DATASTORE_TYPE" == "db2" ]; then
+        PACKAGES=${PACKAGES:-""}
+        VERSION="10.5"
+    else
+        echo "Unrecognized datastore type. ($DATASTORE_TYPE)"
+        exit 1
+    fi
+
+    sed -i "s/%datastore_type%/$DATASTORE_TYPE/g" $TEST_CONF
+    sed -i "s/%datastore_version%/$VERSION/g" $TEST_CONF
+
+    #rd_manage datastore_version_update <datastore_name> <version_name> <datastore_manager> <image_id> <packages> <active>
+    rd_manage datastore_version_update "$DATASTORE_TYPE" "$VERSION" "$DATASTORE_TYPE" $IMAGEID "$PACKAGES" 1
+    rd_manage datastore_version_update "$DATASTORE_TYPE" "inactive_version" "manager1" $IMAGEID "" 0
+    rd_manage datastore_update "$DATASTORE_TYPE" "$VERSION"
+    rd_manage datastore_update Test_Datastore_1 ""
 
-    if [[ -f "$PATH_TROVE"/trove/templates/${DATASTORE_TYPE}/validation-rules.json ]]; then
+    if [ -f "$PATH_TROVE"/trove/templates/$DATASTORE_TYPE/validation-rules.json ]; then
         # add the configuration parameters to the database for the kick-start datastore
-        rd_manage db_load_datastore_config_parameters "${DATASTORE_TYPE}" "${DATASTORE_VERSION}" "$PATH_TROVE"/trove/templates/${DATASTORE_TYPE}/validation-rules.json
+        rd_manage db_load_datastore_config_parameters "$DATASTORE_TYPE" "$VERSION" "$PATH_TROVE"/trove/templates/$DATASTORE_TYPE/validation-rules.json
+    fi
+
+    if [[ "${RESTART_TROVE}" == true ]]; then
+        cmd_stop
+    fi
+    iniset $TROVE_CONF DEFAULT default_datastore "$DATASTORE_TYPE"
+    sleep 1.5
+    if [[ "${RESTART_TROVE}" == true ]]; then
+        cmd_start
     fi
 }
 
 ###############################################################################
 # Run Unit Tests
 ###############################################################################
 
@@ -581,237 +663,215 @@
     popd
 }
 
 function install_test_packages() {
     DATASTORE_TYPE=$1
 
     sudo -H $HTTP_PROXY pip install openstack.nose_plugin proboscis pexpect
-    if [[ "$DATASTORE_TYPE" = "couchbase" ]]; then
-        if [[ "$DISTRO" == "ubuntu" ]]; then
+    if [ "$DATASTORE_TYPE" = "couchbase" ]; then
+        if [ "$DISTRO" == "ubuntu" ]; then
 	        # Install Couchbase SDK for scenario tests.
 	        sudo -H $HTTP_PROXY curl http://packages.couchbase.com/ubuntu/couchbase.key | sudo apt-key add -
 	        echo "deb http://packages.couchbase.com/ubuntu trusty trusty/main" | sudo tee /etc/apt/sources.list.d/couchbase-csdk.list
 	        sudo -H $HTTP_PROXY apt-get update
 	        sudo -H $HTTP_PROXY apt-get --allow-unauthenticated -y install libcouchbase-dev
 	        sudo -H $HTTP_PROXY pip install --upgrade couchbase
         fi
     fi
 }
 
 function mod_confs() {
-    local DATASTORE_TYPE=$1
-    local DATASTORE_VERSION=$2
+    DATASTORE_TYPE=$1
     exclaim "Running mod_confs ..."
 
     sudo install -b --mode 0664 $TROVESTACK_SCRIPTS/conf/test_begin.conf $TEST_CONF
     # cmd_dsvm_gate_tests will set this to be $HOME/report
     TROVE_REPORT_DIR=${TROVE_REPORT_DIR:=$TROVESTACK_SCRIPTS/../report/}
 
     EXTRA_CONF=$TROVESTACK_SCRIPTS/conf/test.extra.conf
-    if [[ -e $EXTRA_CONF ]]; then
+    if [ -e $EXTRA_CONF ]; then
         cat $EXTRA_CONF >> $TEST_CONF
     fi
     # Append datastore specific configuration file
     DATASTORE_CONF=$TROVESTACK_SCRIPTS/conf/$DATASTORE_TYPE.conf
-    if [[ ! -f $DATASTORE_CONF ]]; then
+    if [ ! -f $DATASTORE_CONF ]; then
         exclaim "Datastore configuration file ${DATASTORE_CONF} not found"
         exit 1
     fi
     cat $DATASTORE_CONF | sudo tee -a $TEST_CONF > /dev/null
     cat $TROVESTACK_SCRIPTS/conf/test_end.conf | sudo tee -a $TEST_CONF > /dev/null
 
+    #When running in the gate, don't start services
+    if [ "${DEVSTACK_GATE_TROVE}" == "1" ]; then
+        sed -i "s,%startservices%,false,g" ${TEST_CONF}
+    else
+        sed -i "s,%startservices%,true,g" ${TEST_CONF}
+    fi
     #Add the paths to the test conf
     sed -i "s,%report_directory%,$TROVE_REPORT_DIR,g" $TEST_CONF
+    sed -i "s,%keystone_path%,$PATH_KEYSTONE,g" $TEST_CONF
+    sed -i "s,%nova_path%,$PATH_NOVA,g" $TEST_CONF
+    sed -i "s,%glance_path%,$PATH_GLANCE,g" $TEST_CONF
+    sed -i "s,%trove_path%,$PATH_TROVE,g" $TEST_CONF
     sed -i "s,%service_host%,$SERVICE_HOST,g" $TEST_CONF
+    sed -i "s,%swifth_path%,$PATH_SWIFT,g" $TEST_CONF
 
     # Add the region name into test.conf
     sed -i "s/%region_name%/${REGION_NAME}/g" $TEST_CONF
 
     # Add the tenant id's into test.conf
-    sed -i "s/%service_tenant_id%/$(get_attribute_id project service 1)/g" $TEST_CONF
+    sed -i "s/%admin_tenant_id%/$(get_attribute_id project admin 1)/g" $TEST_CONF
     sed -i "s/%alt_demo_tenant_id%/$(get_attribute_id project alt_demo 1)/g" $TEST_CONF
     sed -i "s/%demo_tenant_id%/$(get_attribute_id project demo 1)/g" $TEST_CONF
     sed -i "s/%admin_password%/$ADMIN_PASSWORD/g" $TEST_CONF
-    sed -i "s/%service_password%/$SERVICE_PASSWORD/g" $TEST_CONF
 
     # Enable neutron tests if needed
     sed -i "s/%neutron_enabled%/$ENABLE_NEUTRON/g" $TEST_CONF
 
-    # Enable backup related tests if Swift is enabled
-    sed -i "s/%swift_enabled%/$ENABLE_SWIFT/g" $TEST_CONF
-
-    # If neutron is enabled, the devstack plugin has already set up the shared
-    # private network for testing.
+    # If neutron is enabled, the devstack plugin will have created an alt_demo
+    # network - write this info to the confs so that the integration tests can
+    # use it.
     if [[ $ENABLE_NEUTRON = true ]]; then
         TROVE_NET_ID=$(openstack $CLOUD_ADMIN_ARG network list | grep " $TROVE_PRIVATE_NETWORK_NAME " | awk '{print $2}')
         TROVE_SUBNET_ID=$(openstack $CLOUD_ADMIN_ARG subnet list | grep " $TROVE_PRIVATE_SUBNET_NAME " | awk '{print $2}')
         echo "Using network ${TROVE_PRIVATE_NETWORK_NAME} (${TROVE_NET_ID}): ${TROVE_PRIVATE_SUBNET_NAME} (${TROVE_SUBNET_ID})"
         sed -i "s,%shared_network%,$TROVE_NET_ID,g" $TEST_CONF
         sed -i "s,%shared_network_subnet%,$TROVE_SUBNET_ID,g" $TEST_CONF
     else
         # do not leave invalid keys in the configuration when using Nova for networking
         sed -i "/%shared_network%/d" $TEST_CONF
         sed -i "/%shared_network_subnet%/d" $TEST_CONF
     fi
 
-    if [[ "$DATASTORE_TYPE" = "vertica" ]]; then
+    if [ "$DATASTORE_TYPE" = "vertica" ]; then
         # Vertica needs more time than mysql for its boot/start/stop operations.
         setup_cluster_configs cluster_member_count 3
-    elif [[ "$DATASTORE_TYPE" = "pxc" ]]; then
+    elif [ "$DATASTORE_TYPE" = "pxc" ]; then
         setup_cluster_configs min_cluster_member_count 2
-    elif [[ "$DATASTORE_TYPE" = "cassandra" ]]; then
+    elif [ "$DATASTORE_TYPE" = "cassandra" ]; then
         setup_cluster_configs cluster_member_count 2
-    elif [[ "$DATASTORE_TYPE" = "mongodb" ]]; then
+    elif [ "$DATASTORE_TYPE" = "mongodb" ]; then
         setup_cluster_configs cluster_member_count 2
         # Decrease the number of required config servers per cluster to save resources.
         iniset $TROVE_CONF $DATASTORE_TYPE num_config_servers_per_cluster 1
     fi
 
-    sed -i "s/%datastore_type%/$DATASTORE_TYPE/g" $TEST_CONF
-    sed -i "s/%datastore_version%/${DATASTORE_VERSION}/g" $TEST_CONF
-
     set_bin_path
+
 }
 
 function setup_cluster_configs() {
     # Setting cluster_member_count to 2 to decrease cluster spawn time.
     iniset $TROVE_CONF $DATASTORE_TYPE $1 $2
 }
 
 # Add useful flavors for testing (with corresponding *.resize flavors)
 function add_test_flavors() {
     # name id ram root_vol vcpu
     # the ram and vcpu for name.resize are automatically calculated
     # eph and non-eph flavors are created for each entry
-    add_flavor 'tiny' 10 768 4 1
+    add_flavor 'tiny' 10 512 3 1
 
-    add_flavor 'small' 15 1024 6 1
-    add_flavor 'small' 16 1024 7 1
-    add_flavor 'small' 17 1024 8 1
-
-    add_flavor 'medium' 20 1536 7 1
-    add_flavor 'medium' 21 1536 8 1
-
-    add_flavor 'large' 25 2048 8 1
-    add_flavor 'large' 26 2048 13 1
-    add_flavor 'large' 27 2048 18 1
+    add_flavor 'small' 15 768 3 1
+    add_flavor 'small' 16 768 4 1
+    add_flavor 'small' 17 768 5 1
+
+    add_flavor 'medium' 20 1024 4 1
+    add_flavor 'medium' 21 1024 5 1
+
+    add_flavor 'large' 25 2048 5 1
+    add_flavor 'large' 26 2048 10 1
+    add_flavor 'large' 27 2048 15 1
 
     # This will allow Nova to create an instance, but not enough disk to boot the image
-    add_flavor 'fault_1' 30 1536 1 1 'skip_resize'
+    add_flavor 'fault_1' 30 512 1 1 'skip_resize'
     # This should be enough memory to cause Nova to fail entirely due to too much allocation
-    add_flavor 'fault_2' 31 131072 7 1 'skip_resize'
+    add_flavor 'fault_2' 31 131072 5 1 'skip_resize'
 }
 
 function cmd_test_init() {
     local DATASTORE_TYPE=$1
-    local DATASTORE_VERSION=$2
 
-    if [[ -z "${DATASTORE_TYPE}" ]]; then
+    if [ -z "${DATASTORE_TYPE}" ]; then
         exclaim "${COLOR_RED}Datastore argument was not specified.${COLOR_NONE}"
         exit 1
     fi
 
     exclaim 'Initializing Configuration for Running Tests...'
 
     exclaim "Installing python test packages."
     install_test_packages "${DATASTORE_TYPE}"
 
     exclaim "Modifying test.conf and guest.conf with appropriate values."
-    mod_confs "${DATASTORE_TYPE}" "${DATASTORE_VERSION}"
+    mod_confs "${DATASTORE_TYPE}"
 
     exclaim "Creating Test Flavors."
     add_test_flavors
 
-    exclaim "Re-installing python-troveclient from git"
-    pip3 uninstall -y python-troveclient
-    pip3 install -U git+https://opendev.org/openstack/python-troveclient@master#egg=python-troveclient
-}
-
-function cmd_install_docker() {
-    exclaim "install and configure docker: $@"
-    # It seems that rocky8 or newer use podman to emulate docker cli.
-    # the daemon.json file may make no sense here for rocky, but it may be useful for centos distro.
-    sudo mkdir /etc/docker
-    sudo tee /etc/docker/daemon.json >/dev/null <<EOF
-{
-    "bridge": "none",
-    "ip-forward": false,
-    "iptables": false
-}
-EOF
-    sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS update
-    if is_fedora; then
-      sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS install docker
-    else
-      sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS install docker.io
+    if [[ -n $KEY_DIR ]]; then
+        exclaim "Installing the SSH key from $KEY_DIR to the test environment."
+        mkdir -m 700 -p $USERHOME/.ssh
+        install -b --mode 0400 $KEY_DIR/id_rsa $USERHOME/.ssh
+        cat $KEY_DIR/authorized_keys >> $USERHOME/.ssh/authorized_keys
+        chmod 600 $USERHOME/.ssh/authorized_keys
     fi
 }
 
-# Build trove guest image
 function cmd_build_image() {
-    exclaim "Params for cmd_build_image function: $@"
+    local IMAGE_DATASTORE_TYPE=${1:-'mysql'}
+    local ESCAPED_PATH_TROVE=${2:-'\/opt\/stack\/trove'}
+    local HOST_SCP_USERNAME=${3:-'ubuntu'}
+    local GUEST_USERNAME=${4:-'ubuntu'}
 
-    local image_guest_os=${1:-'ubuntu'}
-    local image_guest_release=${2:-'jammy'}
-    local dev_mode=${3:-'true'}
-    local guest_username=${4:-'ubuntu'}
-    local output=$5
-
-    if [[ -z "$output" ]]; then
-        image_name="trove-guest-${image_guest_os}-${image_guest_release}"
-        if [[ ${dev_mode} == "true" ]]; then
-            image_name="${image_name}-dev"
-        fi
-        image_folder=$HOME/images
-        output="${image_folder}/${image_name}.qcow2"
-    fi
+    exclaim "Ensuring we have all packages needed to build image."
+    sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS update
+    sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS install qemu
+    sudo -H $HTTP_PROXY pip install --upgrade pip dib-utils
+    pkg_install python-yaml
 
-    # Always rebuild the image.
-    sudo rm -rf ${output}
-    sudo mkdir -p $(dirname ${output}); sudo chmod 777 -R $(dirname ${output})
+    install_devstack_code
 
-    echo "Ensuring we have all packages needed to build image."
-    sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS update
-    if is_fedora; then
-      sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS install epel-release
-      sudo $PKG_MGR config-manager --set-enabled epel
-      sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS install qemu-img git kpartx debootstrap squashfs-tools python3-pip python3-setuptools zstd
-    else
-      sudo $HTTP_PROXY $PKG_MGR $PKG_GET_ARGS install qemu-utils git kpartx debootstrap squashfs-tools python3-pip python3-setuptools
-    fi
-    sudo -H $HTTP_PROXY pip3 install diskimage-builder
+    cmd_clone_projects do_not_force_update $TROVESTACK_SCRIPTS/image-projects-list
 
-    build_guest_image ${image_guest_os} ${image_guest_release} ${dev_mode} ${guest_username} ${output}
+    exclaim "Use tripleo-diskimagebuilder to actually build the Trove Guest Agent Image."
+    build_guest_image $IMAGE_DATASTORE_TYPE
 }
 
-# Build guest image and upload to Glance, register the datastore and configuration parameters.
 function cmd_build_and_upload_image() {
-    local guest_os=${1:-"ubuntu"}
-    local guest_release=${2:-"jammy"}
-    local dev_mode=${3:-"true"}
-    local guest_username=${4:-"ubuntu"}
-    local output_dir=${5:-"$HOME/images"}
-
-    name=trove-guest-${guest_os}-${guest_release}
-    glance_imageid=$(openstack ${CLOUD_ADMIN_ARG} image list \
-      --tag trove --sort created_at:desc \
-      -f value -c ID | awk 'NR==1 {print}')
-    if [[ -z ${glance_imageid} ]]; then
-        mkdir -p ${output_dir}
-        output=${output_dir}/${name}.qcow2
-        cmd_build_image ${guest_os} ${guest_release} ${dev_mode} ${guest_username} ${output}
+    local DATASTORE_TYPE=$1
+    local RESTART_TROVE=${2:-$(get_bool RESTART_TROVE "true")}
 
-        glance_imageid=$(openstack ${CLOUD_ADMIN_ARG} image create ${name} --public --disk-format qcow2 --container-format bare --file ${output} --property hw_rng_model='virtio' --tag trove -c id -f value)
-        [[ -z "$glance_imageid" ]] && echo "Glance upload failed!" && exit 1
+    if [ -z "${DATASTORE_TYPE}" ]; then
+        exclaim "${COLOR_RED}Datastore argument was not specified.${COLOR_NONE}"
+        exit 1
     fi
 
-    exclaim "Using Glance image ID: $glance_imageid"
+    local IMAGE_URL=""
+    # Use /tmp as file_cache
+    FILES=/tmp
+    if [[ -n $IMAGE_DOWNLOAD_URL ]]; then
+        exclaim "Downloading and using cached image"
+        IMAGE_URL=$IMAGE_DOWNLOAD_URL
+    else
+        exclaim "Trying to build image"
+        build_guest_image "${DATASTORE_TYPE}"
+        QCOW_IMAGE=`find $VM_PATH -name '*.qcow2'`
+        IMAGE_URL="file://$QCOW_IMAGE"
+    fi
+
+    GLANCE_IMAGEIDS=$(openstack $CLOUD_ADMIN_ARG image list | grep $(basename $IMAGE_URL .qcow2) | get_field 1)
+    if [[ -n $GLANCE_IMAGEIDS ]]; then
+        openstack $CLOUD_ADMIN_ARG image delete $GLANCE_IMAGEIDS
+    fi
+    GLANCE_IMAGEID=`get_glance_id upload_image $IMAGE_URL`
+    [[ -z "$GLANCE_IMAGEID" ]] && echo "Glance upload failed!" && exit 1
+    echo "IMAGE ID: $GLANCE_IMAGEID"
 
     exclaim "Updating Datastores"
-    cmd_set_datastore
+    cmd_set_datastore "${GLANCE_IMAGEID}" "${DATASTORE_TYPE}" "${RESTART_TROVE}"
 }
 
 
 function cmd_initialize() {
     exclaim '(Re)Initializing Trove...'
     pushd $PATH_DEVSTACK_SRC
     ./unstack.sh
@@ -851,15 +911,15 @@
     $TROVE_BIN_DIR/trove-manage --config-file=$CONF_FILE db_sync
     sqlite3 trove_test.sqlite \
         "INSERT INTO datastores VALUES ('a00000a0-00a0-0a00-00a0-000a000000aa', \
             'mysql', 'b00000b0-00b0-0b00-00b0-000b000000bb'); \
          INSERT INTO datastores values ('e00000e0-00e0-0e00-00e0-000e000000ee', \
             'Test_Datastore_1', ''); \
          INSERT INTO datastore_versions VALUES ('b00000b0-00b0-0b00-00b0-000b000000bb', \
-            'a00000a0-00a0-0a00-00a0-000a000000aa', $MYSQL_VER, \
+            'a00000a0-00a0-0a00-00a0-000a000000aa', '5.6', \
             'c00000c0-00c0-0c00-00c0-000c000000cc', $MYSQL_PKG, 1, 'mysql'); \
          INSERT INTO datastore_versions VALUES ('d00000d0-00d0-0d00-00d0-000d000000dd', \
             'a00000a0-00a0-0a00-00a0-000a000000aa', 'inactive_version', \
             '', '', 0, 'manager1'); \
         INSERT INTO datastore_configuration_parameters VALUES \
             ('00000000-0000-0000-0000-000000000001', \
             'key_buffer_size', 'b00000b0-00b0-0b00-00b0-000b000000bb', \
@@ -960,45 +1020,83 @@
 
 ###############################################################################
 # Run Integration Tests
 ###############################################################################
 
 function cmd_int_tests() {
     exclaim "Running Trove Integration Tests..."
-    if [[ ! $USAGE_ENDPOINT ]]; then
+    if [ ! $USAGE_ENDPOINT ]; then
         export USAGE_ENDPOINT=trove.tests.util.usage.FakeVerifier
     fi
     cd $TROVESTACK_SCRIPTS
-    if [[ $# -lt 1 ]]; then
-        args="--group=mysql"
+    if [ $# -lt 1 ]; then
+        args="--group=blackbox"
     else
         args="$@"
     fi
 
-    # Referenced in test script
-    export TROVE_TEST_SSH_USER=${TROVE_TEST_SSH_USER:-"ubuntu"}
-    export TROVE_TEST_SSH_KEY_FILE=${TROVE_TEST_SSH_KEY_FILE:-"$HOME/.ssh/id_rsa"}
-
     dump_env
     # -- verbose makes it prettier.
     # -- logging-clear-handlers keeps the novaclient and other things from
     #    spewing logs to stdout.
     args="$INT_TEST_OPTIONS -B $TROVESTACK_TESTS/integration/int_tests.py --verbose --logging-clear-handlers $args"
     echo "Running: python $args"
     python $args
 }
 
+function cmd_int_tests_simple() {
+    exclaim "Running Trove Simple Integration Tests..."
+    cd $TROVESTACK_SCRIPTS
+    if [ $# -lt 1 ]; then
+        args="--group=simple_blackbox"
+    else
+        args="$@"
+    fi
+
+    # -- verbose makes it prettier.
+    # -- logging-clear-handlers keeps the novaclient and other things from
+    #    spewing logs to stdout.
+    args="$INT_TEST_OPTIONS -B $TROVESTACK_TESTS/integration/int_tests.py --verbose --logging-clear-handlers $args"
+    echo "python $args"
+    python $args
+}
+
 function cmd_int_tests_white_box() {
     export PYTHONPATH=$PYTHONPATH:$PATH_TROVE
     export PYTHONPATH=$PYTHONPATH:$PATH_NOVA
     cmd_int_tests --test-config white_box=True \
         --config-file=$TROVE_CONF \
         --nova-flags=/etc/nova/nova.conf $@
 }
 
+function cmd_example_tests() {
+    set +e
+    cmd_stop
+    set -e
+    cmd_start_fake
+    sleep 3
+    echo "
+{
+    \"directory\": \"$TROVESTACK_TESTS/../apidocs/src/resources/samples/\",
+    \"auth_url\":\"http://$KEYSTONE_AUTH_HOST/identity/v2.0/tokens\",
+    \"api_url\":\"http://$SERVICE_HOST:8779\",
+    \"replace_host\":\"https://ord.databases.api.rackspacecloud.com\",
+    \"replace_dns_hostname\": \"e09ad9a3f73309469cf1f43d11e79549caf9acf2.rackspaceclouddb.com\",
+    \"username\":\"examples\",
+    \"password\":\"examples\",
+    \"tenant\":\"trove\"
+}" > /tmp/example-tests.conf
+    python $TROVESTACK_TESTS/examples/examples/example_generation.py /tmp/example-tests.conf
+    pushd $TROVESTACK_TESTS/../apidocs
+    mvn clean
+    mvn generate-sources
+    popd
+    cmd_stop
+}
+
 
 ###############################################################################
 # Misc. tools
 ###############################################################################
 
 function mysql_nova() {
     echo mysql nova --execute "$@"
@@ -1051,14 +1149,15 @@
     exclaim "Running CI suite..."
     set +e
     cmd_stop_deps
     cmd_stop
     set -e
     cmd_install
     cmd_test_init "${DATASTORE_TYPE}"
+    # The arg will be the image type
     cmd_build_and_upload_image "${DATASTORE_TYPE}" "${RESTART_TROVE}"
 
     # Test in fake mode.
     exclaim "Testing in fake mode."
     cmd_start_fake
     FAKE_MODE=True cmd_int_tests
     cmd_stop
@@ -1170,57 +1269,92 @@
     exec_cmd_on_output "openstack $cloud_arg security group list" "openstack $cloud_arg security group delete" 0 "default"
     # delete server groups
     exec_cmd_on_output "openstack $cloud_arg server group list" "openstack $cloud_arg server group delete"
 }
 
 function cmd_kick_start() {
     local DATASTORE_TYPE=$1
-    local DATASTORE_VERSION=$2
+    local RESTART_TROVE=${2:-$(get_bool RESTART_TROVE "true")}
 
-    if [[ -z "${DATASTORE_TYPE}" ]]; then
+    if [ -z "${DATASTORE_TYPE}" ]; then
         exclaim "${COLOR_RED}Datastore argument was not specified.${COLOR_NONE}"
         exit 1
     fi
 
-    exclaim "Running kick-start for $DATASTORE_TYPE"
-    cmd_test_init "${DATASTORE_TYPE}" "${DATASTORE_VERSION}"
-
+    exclaim "Running kick-start for $DATASTORE_TYPE (restart trove: $RESTART_TROVE)"
     dump_env
+    cmd_test_init "${DATASTORE_TYPE}"
+    cmd_build_and_upload_image "${DATASTORE_TYPE}" "${RESTART_TROVE}"
 }
 
-# Start functional test. The guest image should be created and registered in
-# appropriate datastore before the test, the configuration parameters should
-# also be loaded as well. DevStack has done all of that.
-function cmd_gate_tests() {
+function cmd_dsvm_gate_tests() {
+    ACTUAL_HOSTNAME=$(hostname -I | sed 's/[0-9a-z][0-9a-z]*:.*:[0-9a-z][0-9a-z]*//g' | sed 's/[0-9]*\.[0-9]*\.[0-9]*\.1\b//g' | sed 's/ /\n/g' | sed '/^$/d' | sort -bu | head -1)
+
     local DATASTORE_TYPE=${1:-'mysql'}
     local TEST_GROUP=${2:-${DATASTORE_TYPE}}
-    local DATASTORE_VERSION=${3:-'5.7.29'}
-    local HOST_SCP_USERNAME=${4:-$(whoami)}
-    local GUEST_USERNAME=${5:-'ubuntu'}
-
-    export DATASTORE_TYPE=${DATASTORE_TYPE}
-    export DATASTORE_VERSION=${DATASTORE_VERSION}
-
-    exclaim "Running cmd_gate_tests ..."
-
-    export REPORT_DIRECTORY=${REPORT_DIRECTORY:=$HOME/gate-tests-report/}
-    export TROVE_REPORT_DIR=$HOME/gate-tests-report/
-    export TROVESTACK_DUMP_ENV=true
-    export SSH_DIR=${SSH_DIR:-"$HOME/.ssh"}
-    # The user is used to connect with the db instance during testing.
-    export TROVE_TEST_SSH_USER=${TROVE_TEST_SSH_USER:-"ubuntu"}
-    # This var is used to ssh into the db instance during testing.
-    export TROVE_TEST_SSH_KEY_FILE=${SSH_DIR}/id_rsa
-
+    local HOST_SCP_USERNAME=${3:-$USER}
+    local GUEST_USERNAME=${4:-'ubuntu'}
+    local CONTROLLER_IP=${5:-$ACTUAL_HOSTNAME}
+    local ESCAPED_PATH_TROVE=${6:-'\/opt\/stack\/new\/trove'}
+
+    exclaim "Running cmd_dsvm_gate_tests ..."
+
+    # Sometimes in the gate the ACTUAL_HOSTNAME is blank - this code attempts to debug it
+    if [[ -z "${CONTROLLER_IP// }" ]]; then
+        echo "*** CONTROLLER_IP is blank, trying to determine actual hostname"
+        local hostname_part=$(hostname -I)
+        echo "Hostname pass 1: '$hostname_part'"
+        hostname_part=$(echo $hostname_part | sed 's/[0-9a-z][0-9a-z]*:.*:[0-9a-z][0-9a-z]*//g')
+        echo "Hostname pass 2: '$hostname_part'"
+        hostname_part_no_ip6=$hostname_part
+        hostname_part=$(echo $hostname_part | sed 's/[0-9]*\.[0-9]*\.[0-9]*\.1\b//g')
+        echo "Hostname pass 3: '$hostname_part'"
+        if [[ -z "${hostname_part// }" ]]; then
+            # This seems to occur when the actual hostname ends with '.1'
+            # If this happens, take the first one that doesn't start with '192' or '172'
+            hostname_part=$(echo $hostname_part_no_ip6 | sed 's/1[79]2\.[0-9]*\.[0-9]*\.1\b//g')
+            echo "Hostname pass 3a: '$hostname_part'"
+        fi
+        hostname_part=$(echo $hostname_part | sed 's/ /\n/g')
+        echo "Hostname pass 4: '$hostname_part'"
+        hostname_part=$(echo $hostname_part | sed '/^$/d')
+        echo "Hostname pass 5: '$hostname_part'"
+        hostname_part=$(echo $hostname_part | sort -bu)
+        echo "Hostname pass 6: '$hostname_part'"
+        hostname_part=$(echo $hostname_part | head -1)
+        echo "Hostname pass 7: '$hostname_part'"
+        CONTROLLER_IP=$hostname_part
+        echo "*** CONTROLLER_IP was blank (CONTROLLER_IP now set to '$CONTROLLER_IP')"
+    fi
+
+    export REPORT_DIRECTORY=${REPORT_DIRECTORY:=$HOME/dsvm-report/}
+    export TROVE_REPORT_DIR=$HOME/dsvm-report/
+    TROVESTACK_DUMP_ENV=true
+
+    # Devstack vm-gate runs as a non-ubuntu user, but needs to connect to the guest image as ubuntu
+    echo "User=ubuntu" >> /home/$USER/.ssh/config
+
+    # Fix iptables rules that prevent amqp connections from the devstack box to the guests
+    sudo iptables -D openstack-INPUT -j REJECT --reject-with icmp-host-prohibited || true
+
+    sudo chown -R $(whoami) /etc/trove
+    iniset $TROVE_GUESTAGENT_CONF DEFAULT rabbit_host $CONTROLLER_IP
+    iniset $TROVE_GUESTAGENT_CONF oslo_messaging_rabbit rabbit_hosts $CONTROLLER_IP
     cd $TROVESTACK_SCRIPTS
+    sudo -H $HTTP_PROXY pip install --upgrade pip dib-utils
 
-    # Build and upload guest image, register datastore version.
-    cmd_build_and_upload_image
+    local RESTART_TROVE=false
+    cmd_kick_start "${DATASTORE_TYPE}" "${RESTART_TROVE}"
 
-    cmd_kick_start "${DATASTORE_TYPE}" "${DATASTORE_VERSION}"
+    # Update the local swift endpoint in the catalog to use the CONTROLLER_IP instead of 127.0.0.1
+    SWIFT_ENDPOINTS=$(openstack $CLOUD_ADMIN_ARG endpoint list --service swift -c ID -f value)
+    openstack $CLOUD_ADMIN_ARG endpoint create swift public 'http://'$CONTROLLER_IP':8080/v1/AUTH_$(tenant_id)s' --region RegionOne
+    openstack $CLOUD_ADMIN_ARG endpoint create swift internal 'http://'$CONTROLLER_IP':8080/v1/AUTH_$(tenant_id)s' --region RegionOne
+    openstack $CLOUD_ADMIN_ARG endpoint create swift admin 'http://'$CONTROLLER_IP':8080' --region RegionOne
+    echo $SWIFT_ENDPOINTS | xargs -n 1 openstack $CLOUD_ADMIN_ARG endpoint delete
 
     cmd_int_tests --group=$TEST_GROUP
 }
 
 function cmd_reset_task() {
     mysql_trove "UPDATE instances SET task_id=1 WHERE id='$1'"
 }
@@ -1298,15 +1432,14 @@
           install         - Install all the required dependencies and bring up tr-api and tr-tmgr
                           - devstack config can be altered by using a USER_LOCAL_CONF file
                             which will be copied into devstack/local.conf on each 'install' run
                             (defaults to \$HOME/$USER_LOCAL_CONF_NAME)
                           - Set DEVSTACK_BRANCH to switch the branch/commit of devstack
                             (i.e. 'stable/kilo' or '7ef2462')
           test-init       - Configure the test configuration files and add keystone test users
-          install-docker  - Install docker and configure docker to not manipulate iptables.
           build-image     - Builds the vm image for the trove guest
           initialize      - Reinitialize the trove database, users, services, and test config
 
         --helper for environment--
           kick-start      - kick start the setup of trove.
                             (trovestack test-init/build-image in one step)
                           - Set REBUILD_IMAGE=True to force rebuild (won't use cached image)
@@ -1326,16 +1459,15 @@
                             See trove/tests/int_tests.py for list of registered groups.
                             Examples:
                               Run original MySQL tests:        ./trovestack int-tests
                               Run all MySQL scenario tests:    ./trovestack int-tests --group=mysql-supported
                               Run single Redis scenario tests: ./trovestack int-tests --group=redis-supported-single
                               Run specific functional tests:   ./trovestack int-tests --group=module-create --group=configuration-create
           simple-tests    - Runs the simple integration tests (requires all daemons).
-          dsvm-gate-tests - Configures and runs the int-tests in a devstack vm-gate environment(legacy Zuul v2 jobs only).
-          gate-tests      - Configures and runs the int-tests in a devstack vm-gate environment.
+          dsvm-gate-tests - Configures and runs the int-tests in a devstack vm-gate environment.
 
         --tools--
           debug           - Debug this script (shows all commands).
           wipe-logs       - Resets all log files.
           rd-sql          - Opens the Trove MySQL database.
           vagrant-ssh     - Runs a command from the host on the server.
           clear           - Destroy instances and rabbit queues.
@@ -1353,22 +1485,49 @@
 function run_command() {
     # Print the available commands
     if [ $# -lt 1 ]; then
         print_usage
     fi
 
     case "$1" in
+        "install" ) cmd_install;;
+        "test-init" ) shift; cmd_test_init $@;;
         "build-image" ) shift; cmd_build_image $@;;
-        "upload-image" ) shift; cmd_build_and_upload_image $@;;
+        "initialize" ) cmd_initialize;;
+        "unit-tests" ) cmd_unit_tests;;
+        "start-deps" ) cmd_start_deps;;
+        "stop-deps" ) cmd_stop_deps;;
+        "start" ) cmd_start;;
         "int-tests" ) shift; cmd_int_tests $@;;
-        "install-docker" ) shift; cmd_install_docker $@;;
+        "int-tests-wb" ) shift; cmd_int_tests_white_box $@;;
+        "simple-tests") shift; cmd_int_tests_simple $@;;
+        "stop" ) cmd_stop;;
+        "restart" ) cmd_stop; cmd_start;;
+        "wipe-logs" ) cmd_wipe_logs;;
+        "rd-sql" ) shift; cmd_rd_sql $@;;
+        "fake-sql" ) shift; cmd_fake_sql $@;;
+        "run-ci" ) shift; cmd_run_ci $@;;
+        "vagrant-ssh" ) shift; cmd_vagrant_ssh $@;;
         "debug" ) shift; echo "Enabling debugging."; \
                   set -o xtrace; TROVESTACK_DUMP_ENV=true; run_command $@;;
-        "gate-tests" ) shift; cmd_gate_tests $@;;
+        "clear" ) shift; cmd_clear $@;;
+        "clean" ) shift; cmd_clean $@;;
+        "run" ) shift; cmd_run $@;;
+        "kick-start" ) shift; cmd_kick_start $@;;
+        "dsvm-gate-tests" ) shift; cmd_dsvm_gate_tests $@;;
+        "run-fake" ) shift; cmd_run_fake $@;;
+        "start-fake" ) shift; cmd_start_fake $@;;
+        "update-projects" ) cmd_clone_projects force_update \
+                            $TROVESTACK_SCRIPTS/projects-list \
+                            $TROVESTACK_SCRIPTS/image-projects-list;;
+        "reset-task" ) shift; cmd_reset_task $@;;
         "wipe-queues" ) shift; cmd_wipe_queues $@;;
+        "example-tests" ) shift; cmd_example_tests $@;;
+        "repl" ) shift; cmd_repl $@;;
+        "help" ) print_usage;;
         * )
             echo "'$1' not a valid command"
             exit 1
     esac
 }
 
 run_command $@
```

### Comparing `trove-21.0.0.0rc2/integration/scripts/trovestack.rc` & `trove-8.0.1/integration/scripts/trovestack.rc`

 * *Files 5% similar despite different names*

```diff
@@ -9,17 +9,14 @@
 # Set up the region name
 # Try REGION_NAME then OS_REGION_NAME then RegionOne (the devstack default)
 REGION_NAME=${REGION_NAME:-${OS_REGION_NAME:-RegionOne}}
 
 # Enable Neutron
 ENABLE_NEUTRON=$(get_bool ENABLE_NEUTRON true)
 
-# Enable Swift
-ENABLE_SWIFT=$(get_bool ENABLE_SWIFT true)
-
 # Enable osprofiler - note: Enables Ceilometer as well
 ENABLE_PROFILER=$(get_bool ENABLE_PROFILER false)
 PROFILER_TRACE_SQL=$(get_bool PROFILER_TRACE_SQL false)
 [ -z $PROFILER_HMAC_KEYS ] && PROFILER_HMAC_KEYS=SECRET_KEY
 
 # Enable ceilometer
 ENABLE_CEILOMETER=$(get_bool ENABLE_CEILOMETER $ENABLE_PROFILER)
@@ -46,33 +43,35 @@
 # PATH_TROVE is set at the top of trovestack
 PATH_PYTHON_NOVACLIENT=$DEST/python-novaclient
 PATH_KEYSTONECLIENT=$DEST/python-keystoneclient
 PATH_OPENSTACKCLIENT=$DEST/python-openstackclient
 PATH_PYTHON_SWIFTCLIENT=$DEST/python-swiftclient
 PATH_PYTHON_TROVECLIENT=$DEST/python-troveclient
 PATH_TROVE_DASHBOARD=$DEST/trove-dashboard
+PATH_DISKIMAGEBUILDER_ELEMENTS=$(python -c 'import os, diskimage_builder, pkg_resources;print(os.path.abspath(pkg_resources.resource_filename(diskimage_builder.__name__, "elements")))')
+PATH_TRIPLEO_ELEMENTS=$DEST/tripleo-image-elements
 
 # Save the state of TROVE_BRANCH first, since it's used in trovestack
 TROVE_BRANCH_ORIG=${TROVE_BRANCH}
 # Devstack and OpenStack git repo source paths, etc.
-GIT_BASE=${GIT_BASE:-https://opendev.org}
+GIT_BASE=${GIT_BASE:-https://git.openstack.org}
 GIT_OPENSTACK=${GIT_OPENSTACK:-${GIT_BASE}/openstack}
 DEVSTACK_REPO=${DEVSTACK_REPO:-${GIT_BASE}/openstack-dev/devstack.git}
 TROVE_REPO=${TROVE_REPO:-${GIT_OPENSTACK}/trove.git}
 TROVE_DIR=${TROVE_DIR:-${PATH_TROVE}}
 TROVE_BRANCH=${TROVE_BRANCH:-master}
 TROVE_CLIENT_REPO=${TROVE_CLIENT_REPO:-${TROVECLIENT_REPO:-${GIT_OPENSTACK}/python-troveclient.git}}
 TROVE_CLIENT_DIR=${TROVE_CLIENT_DIR:-${TROVECLIENT_DIR:-${PATH_PYTHON_TROVECLIENT}}}
 TROVE_CLIENT_BRANCH=${TROVE_CLIENT_BRANCH:-${TROVECLIENT_BRANCH:-master}}
 TROVE_DASHBOARD_REPO=${TROVE_DASHBOARD_REPO:-${TROVEDASHBOARD_REPO:-${GIT_OPENSTACK}/trove-dashboard.git}}
 TROVE_DASHBOARD_DIR=${TROVE_DASHBOARD_DIR:-${TROVEDASHBOARD_DIR:-${PATH_TROVE_DASHBOARD}}}
 TROVE_DASHBOARD_BRANCH=${TROVE_DASHBOARD_BRANCH:-${TROVEDASHBOARD_BRANCH:-master}}
 # Trove specific networking options
-TROVE_PRIVATE_NETWORK_NAME=private
-TROVE_PRIVATE_SUBNET_NAME=private-subnet
+TROVE_PRIVATE_NETWORK_NAME=alt-private
+TROVE_PRIVATE_SUBNET_NAME=alt-private-subnet
 
 # Destination for working data
 DATA_DIR=${DEST}/data
 # Destination for status files
 SERVICE_DIR=${DEST}/status
 
 # Cinder Volume Group Name
@@ -81,30 +80,28 @@
 VOLUME_BACKING_FILE_SIZE=${VOLUME_BACKING_FILE_SIZE:-51200M}
 
 # Passwords used by devstack.
 MYSQL_PASSWORD=e1a2c042c828d3566d0a
 RABBIT_PASSWORD=f7999d1955c5014aa32c
 SERVICE_TOKEN=be19c524ddc92109a224
 ADMIN_PASSWORD=${ADMIN_PASSWORD:-${OS_PASSWORD:-3de4922d8b6ac5a1aad9}}
-SERVICE_PASSWORD=${SERVICE_PASSWORD:-"secretservice"}
+SERVICE_PASSWORD=7de4162d826bc5a11ad9
 
 # Swift hash used by devstack.
 SWIFT_HASH=12go358snjw24501
+
 # Swift Disk Image
 SWIFT_DATA_DIR=${DATA_DIR}/swift
 SWIFT_DISK_IMAGE=${SWIFT_DATA_DIR}/drives/images/swift.img
 
 # The following values can be used to tweak how devstack sets
 # up Trove. If not explicitly set, the defaults in the code are used.
 # To make changes without modifying the repo, add these variables
 # to options.rc or ~/trovestack.options.rc
 #export TROVE_MAX_ACCEPTED_VOLUME_SIZE=10
 #export TROVE_MAX_INSTANCES_PER_TENANT=10
 #export TROVE_MAX_VOLUMES_PER_TENANT=40
 #export TROVE_AGENT_CALL_LOW_TIMEOUT=15
 #export TROVE_AGENT_CALL_HIGH_TIMEOUT=300
-#export TROVE_RESIZE_TIME_OUT=3600
+#export TROVE_RESIZE_TIME_OUT=900
 #export TROVE_USAGE_TIMEOUT=1500
 #export TROVE_STATE_CHANGE_WAIT_TIME=180
-
-# Image
-MYSQL_IMAGE_ID=${MYSQL_IMAGE_ID:-""}
```

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/core.test.conf` & `trove-8.0.1/integration/tests/integration/core.test.conf`

 * *Files 9% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9285714285714286%*

 * *Differences: {"'keystone_use_combined'": 'True', "'start_services'": 'False', "'use_nova_volume'": 'False'}*

```diff
@@ -9,33 +9,36 @@
     "glance_images_directory": "/glance_images",
     "glance_reg_conf": "/vagrant/conf/glance-reg.conf",
     "instance_bigger_flavor_name": "m1.rd-smaller",
     "instance_flavor_name": "m1.rd-tiny",
     "instances_page_size": 20,
     "keystone_code_root": "/opt/stack/keystone",
     "keystone_conf": "/etc/keystone/keystone.conf",
+    "keystone_use_combined": true,
     "management_api_disabled": true,
     "nova_code_root": "/opt/stack/nova",
     "nova_conf": "/home/vagrant/nova.conf",
     "openvz_disabled": false,
     "rabbit_runs_locally": false,
     "report_directory": "rdli-test-report",
     "root_removed_from_instance_api": true,
     "root_timestamp_disabled": false,
     "sentinel": null,
+    "start_services": false,
     "test_mgmt": false,
     "trove_api_updated": "2012-08-01T00:00:00Z",
     "trove_can_have_volume": true,
     "trove_code_root": "/opt/stack/trove",
     "trove_conf": "/tmp/trove.conf",
     "trove_main_instance_has_volume": true,
     "trove_max_accepted_volume_size": 1000,
     "trove_max_instances_per_user": 55,
     "trove_max_volumes_per_user": 100,
     "trove_must_have_volume": false,
     "trove_version": "v1.0",
     "use_local_ovz": false,
+    "use_nova_volume": false,
     "use_reaper": false,
     "use_venv": false,
     "users_page_size": 20,
     "white_box": false
 }
```

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/int_tests.py` & `trove-8.0.1/integration/tests/integration/int_tests.py`

 * *Files 13% similar despite different names*

```diff
@@ -34,17 +34,21 @@
 saved and sys.exit is neutralized so that unittest.TestProgram will not exit
 and instead sys.stdout and stderr are restored so that interactive mode can
 be used.
 
 """
 
 
+from __future__ import absolute_import
 import atexit
 import gettext
+import logging
 import os
+import time
+import unittest
 import sys
 import proboscis
 
 from nose import config
 from nose import core
 
 from tests.colorizer import NovaTestRunner
@@ -65,15 +69,15 @@
 
     """
     path = os.path.join(os.path.abspath(sys.argv[0]), os.pardir, os.pardir)
     possible_topdir = os.path.normpath(path)
     if os.path.exists(os.path.join(possible_topdir, 'nova', '__init__.py')):
         sys.path.insert(0, possible_topdir)
 
-    gettext.install('nova')
+    gettext.install('nova', unicode=1)
 
 
 MAIN_RUNNER = None
 
 
 def initialize_rdl_config(config_file):
     from trove.common import cfg
@@ -103,41 +107,53 @@
         for c in service.cmd:
             sys.stderr.write(c + " ")
         sys.stderr.write("...\n\r")
         service.stop()
 
 
 def import_tests():
+
+    # TODO(tim.simpson): Import these again once white box test functionality
+    #                    is restored.
+    # from tests.dns import check_domain
+    # from tests.dns import concurrency
+    # from tests.dns import conversion
+
     # The DNS stuff is problematic. Not loading the other tests allow us to
     # run its functional tests only.
     ADD_DOMAINS = os.environ.get("ADD_DOMAINS", "False") == 'True'
     if not ADD_DOMAINS:
-        # F401 unused imports needed for tox tests
-        from trove.tests.api import backups  # noqa
-        from trove.tests.api import configurations  # noqa
-        from trove.tests.api import databases  # noqa
-        from trove.tests.api import datastores  # noqa
-        from trove.tests.api import instances as rd_instances  # noqa
-        from trove.tests.api import instances_actions as acts  # noqa
-        from trove.tests.api import instances_delete  # noqa
-        from trove.tests.api import instances_resize  # noqa
-        from trove.tests.api import limits  # noqa
-        from trove.tests.api.mgmt import datastore_versions # noqa
-        from trove.tests.api.mgmt import instances_actions as mgmt_acts  # noqa
-        from trove.tests.api import replication  # noqa
-        from trove.tests.api import root  # noqa
-        from trove.tests.api import user_access  # noqa
-        from trove.tests.api import users  # noqa
-        from trove.tests.api import versions  # noqa
-        from trove.tests.db import migrations  # noqa
+        from tests.api import delete_all
+        from tests.api import instances_pagination
+        from tests.api import instances_quotas
+        from tests.api import instances_states
+        from tests.dns import dns
+        from tests import initialize
+        from tests.smoke import instance
+        from tests.volumes import driver
 
         # Groups that exist as core int-tests are registered from the
         # trove.tests.int_tests module
         from trove.tests import int_tests
 
+        # Groups defined in trove/integration, or any other externally
+        # defined groups can be registered here
+        heavy_black_box_groups = [
+            "dbaas.api.instances.pagination",
+            "dbaas.api.instances.delete",
+            "dbaas.api.instances.status",
+            "dbaas.api.instances.down",
+            "dbaas.api.mgmt.hosts.update",
+            "fake.dbaas.api.mgmt.instances",
+            "fake.dbaas.api.mgmt.accounts.broken",
+            "fake.dbaas.api.mgmt.allaccounts"
+        ]
+        proboscis.register(groups=["heavy_blackbox"],
+                           depends_on_groups=heavy_black_box_groups)
+
 
 def run_main(test_importer):
 
     add_support_for_localization()
 
     # Strip non-nose arguments out before passing this to nosetests
 
@@ -237,9 +253,10 @@
         sys.exit = lambda x: None
 
     proboscis.TestProgram(argv=nose_args, groups=groups, config=c,
                           testRunner=MAIN_RUNNER).run_and_exit()
     sys.stdout = sys.__stdout__
     sys.stderr = sys.__stderr__
 
+
 if __name__ == "__main__":
     run_main(import_tests)
```

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/localhost.test.conf` & `trove-8.0.1/integration/tests/integration/localhost.test.conf`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/tests/__init__.py` & `trove-8.0.1/integration/tests/integration/tests/__init__.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/tests/colorizer.py` & `trove-8.0.1/integration/tests/integration/tests/colorizer.py`

 * *Files 0% similar despite different names*

```diff
@@ -58,15 +58,15 @@
 import heapq
 import logging
 import os
 import unittest
 import sys
 import time
 
-gettext.install('nova')
+gettext.install('nova', unicode=1)
 
 from nose import config
 from nose import core
 from nose import result
 from proboscis import case
 from proboscis import SkipTest
```

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/tests/util/report.py` & `trove-8.0.1/integration/tests/integration/tests/util/report.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/tests/util/rpc.py` & `trove-8.0.1/integration/tests/integration/tests/util/rpc.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,15 +50,15 @@
             any unescaped characters.
 
             """
             proc = start_proc(["/usr/bin/sudo", "rabbitmqctl", "list_queues"],
                               shell=False)
             for line in iter(proc.stdout.readline, ""):
                 print("LIST QUEUES:" + line)
-                m = re.search(r"%s\s+([0-9]+)" % queue_name, line)
+                m = re.search("""%s\s+([0-9]+)""" % queue_name, line)
                 if m:
                     return int(m.group(1))
             return None
 
         @property
         def is_alive(self):
             """Calls list_queues, should fail."""
```

### Comparing `trove-21.0.0.0rc2/integration/tests/integration/tests/util/services.py` & `trove-8.0.1/integration/tests/integration/tests/util/services.py`

 * *Files 1% similar despite different names*

```diff
@@ -116,17 +116,17 @@
         """Returns how much memory the process is using according to pmap."""
         pid = self.find_proc_id()
         if not pid:
             raise RuntimeError("Can't find PID, so can't get memory.")
         proc = start_proc(["/usr/bin/pmap", "-d", str(pid)],
                           shell=False)
         for line in iter(proc.stdout.readline, ""):
-            m = re.search(r"mapped\:\s([0-9]+)K\s+"
-                          r"writeable/private:\s([0-9]+)K\s+"
-                          r"shared:\s+([0-9]+)K", line)
+            m = re.search("""mapped\:\s([0-9]+)K\s+"""
+                          """writeable/private:\s([0-9]+)K\s+"""
+                          """shared:\s+([0-9]+)K""", line)
             if m:
                 return MemoryInfo(int(m.group(1)), int(m.group(2)),
                                   int(m.group(3)))
         raise RuntimeError("Memory info not found.")
 
     def get_fd_count_from_proc_file(self):
         """Returns file descriptors according to /proc/<id>/status."""
@@ -185,20 +185,20 @@
         if not self.cmd:
             return False
         time.sleep(1)
         # The cmd[1] signifies the executable python script. It gets invoked
         # as python /path/to/executable args, so the entry is
         # /path/to/executable
         actual_command = self.cmd[proc_name_index].split("/")[-1]
-        print(actual_command)
+        print actual_command
         proc_command = ["/usr/bin/pgrep", "-f", actual_command]
-        print(proc_command)
+        print proc_command
         proc = start_proc(proc_command, shell=False)
         line = proc.stdout.readline()
-        print(line)
+        print line
         # pgrep only returns a pid. if there is no pid, it'll return nothing
         return len(line) != 0
 
     @property
     def is_running(self):
         """Returns true if the service has already been started.
```

### Comparing `trove-21.0.0.0rc2/releasenotes/notes/associate-volume-type-datastore-97defb9279b61c1f.yaml` & `trove-8.0.1/releasenotes/notes/associate-volume-type-datastore-97defb9279b61c1f.yaml`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/releasenotes/notes/cassandra-user-functions-041abfa4f4baa591.yaml` & `trove-8.0.1/releasenotes/notes/cassandra-user-functions-041abfa4f4baa591.yaml`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/releasenotes/notes/db2-online-backup-restore-3783afe752562e70.yaml` & `trove-8.0.1/releasenotes/notes/db2-online-backup-restore-3783afe752562e70.yaml`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/releasenotes/notes/pxc-grow-shrink-0b1ee689cbc77743.yaml` & `trove-8.0.1/releasenotes/notes/pxc-grow-shrink-0b1ee689cbc77743.yaml`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/releasenotes/source/conf.py` & `trove-8.0.1/releasenotes/source/conf.py`

 * *Files 7% similar despite different names*

```diff
@@ -39,18 +39,18 @@
 # ones.
 extensions = [
     'openstackdocstheme',
     'reno.sphinxext',
 ]
 
 # openstackdocstheme options
-openstackdocs_repo_name = 'openstack/trove'
-openstack_auto_name = False
-openstackdocs_bug_project = 'trove'
-openstackdocs_bug_tag = ''
+repository_name = 'openstack/trove'
+bug_project = 'trove'
+bug_tag = ''
+html_last_updated_fmt = '%Y-%m-%d %H:%M'
 html_theme = 'openstackdocs'
 
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ['_templates']
 
 # The suffix of source filenames.
 source_suffix = '.rst'
@@ -58,24 +58,27 @@
 # The encoding of source files.
 # source_encoding = 'utf-8-sig'
 
 # The master toctree document.
 master_doc = 'index'
 
 # General information about the project.
-project = 'Trove Release Notes'
-copyright = '2015, Trove Developers'
+project = u'Trove Release Notes'
+copyright = u'2015, Trove Developers'
 
-# Release notes are version independent.
+# The version info for the project you're documenting, acts as replacement for
+# |version| and |release|, also used in various other places throughout the
+# built documents.
+#
 # The short X.Y version.
-
+from trove.version import version_info as trove_version
 # The full version, including alpha/beta/rc tags.
-release = ''
+release = trove_version.version_string_with_vcs()
 # The short X.Y version.
-version = ''
+version = trove_version.canonical_version_string()
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 # language = None
 
 # There are two options for replacing |today|: either, you set today to some
 # non-false value, then it is used:
@@ -99,15 +102,15 @@
 # add_module_names = True
 
 # If true, sectionauthor and moduleauthor directives will be shown in the
 # output. They are ignored by default.
 # show_authors = False
 
 # The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'native'
+pygments_style = 'sphinx'
 
 # A list of ignored prefixes for module index sorting.
 # modindex_common_prefix = []
 
 # If true, keep warnings as "system message" paragraphs in the built documents.
 # keep_warnings = False
 
@@ -210,16 +213,16 @@
     # 'preamble': '',
 }
 
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title,
 #  author, documentclass [howto, manual, or own class]).
 latex_documents = [
-    ('index', 'TroveReleaseNotes.tex', 'Trove Release Notes Documentation',
-     'Trove Developers', 'manual'),
+    ('index', 'TroveReleaseNotes.tex', u'Trove Release Notes Documentation',
+     u'Trove Developers', 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
 # the title page.
 # latex_logo = None
 
 # For "manual" documents, if this is true, then toplevel headings are parts,
@@ -240,30 +243,30 @@
 
 
 # -- Options for manual page output ---------------------------------------
 
 # One entry per manual page. List of tuples
 # (source start file, name, description, authors, manual section).
 man_pages = [
-    ('index', 'trovereleasenotes', 'Trove Release Notes Documentation',
-     ['Trove Developers'], 1)
+    ('index', 'trovereleasenotes', u'Trove Release Notes Documentation',
+     [u'Trove Developers'], 1)
 ]
 
 # If true, show URL addresses after external links.
 # man_show_urls = False
 
 
 # -- Options for Texinfo output -------------------------------------------
 
 # Grouping the document tree into Texinfo files. List of tuples
 # (source start file, target name, title, author,
 #  dir menu entry, description, category)
 texinfo_documents = [
-    ('index', 'TroveReleaseNotes', 'Trove Release Notes Documentation',
-     'Trove Developers', 'TroveReleaseNotes',
+    ('index', 'TroveReleaseNotes', u'Trove Release Notes Documentation',
+     u'Trove Developers', 'TroveReleaseNotes',
      'One line description of project.',
      'Miscellaneous'),
 ]
 
 # Documents to append as an appendix to all manuals.
 # texinfo_appendices = []
```

### Comparing `trove-21.0.0.0rc2/run_tests.py` & `trove-8.0.1/run_tests.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,15 +21,16 @@
 import os
 import sys
 import traceback
 
 import eventlet
 from oslo_log import log as logging
 import proboscis
-import urllib
+import six
+from six.moves import urllib
 import wsgi_intercept
 from wsgi_intercept.httplib2_intercept import install as wsgi_install
 
 from trove.common import cfg
 from trove.common.rpc import service as rpc_service
 from trove.common.rpc import version as rpc_version
 from trove.common import utils
@@ -52,15 +53,18 @@
 
     """
     path = os.path.join(os.path.abspath(sys.argv[0]), os.pardir, os.pardir)
     possible_topdir = os.path.normpath(path)
     if os.path.exists(os.path.join(possible_topdir, 'nova', '__init__.py')):
         sys.path.insert(0, possible_topdir)
 
-    gettext.install('nova')
+    if six.PY2:
+        gettext.install('nova', unicode=1)
+    else:
+        gettext.install('nova')
 
 
 def initialize_trove(config_file):
     from trove.common import pastedeploy
 
     root_logger.DefaultRootLogger()
 
@@ -198,22 +202,32 @@
 
 def import_tests():
     # F401 unused imports needed for tox tests
     from trove.tests.api import backups  # noqa
     from trove.tests.api import configurations  # noqa
     from trove.tests.api import databases  # noqa
     from trove.tests.api import datastores  # noqa
+    from trove.tests.api import flavors  # noqa
+    from trove.tests.api import header  # noqa
     from trove.tests.api import instances as rd_instances  # noqa
     from trove.tests.api import instances_actions as rd_actions  # noqa
     from trove.tests.api import instances_delete  # noqa
+    from trove.tests.api import instances_mysql_down  # noqa
     from trove.tests.api import instances_resize  # noqa
     from trove.tests.api import limits  # noqa
+    from trove.tests.api.mgmt import accounts  # noqa
+    from trove.tests.api.mgmt import admin_required  # noqa
+    from trove.tests.api.mgmt import hosts  # noqa
+    from trove.tests.api.mgmt import instances as mgmt_instances  # noqa
     from trove.tests.api.mgmt import instances_actions as mgmt_actions  # noqa
+    from trove.tests.api.mgmt import malformed_json  # noqa
+    from trove.tests.api.mgmt import storage  # noqa
     from trove.tests.api import replication  # noqa
     from trove.tests.api import root  # noqa
+    from trove.tests.api import root_on_create  # noqa
     from trove.tests.api import user_access  # noqa
     from trove.tests.api import users  # noqa
     from trove.tests.api import versions  # noqa
     from trove.tests.db import migrations  # noqa
 
 
 def main(import_func):
```

### Comparing `trove-21.0.0.0rc2/setup.cfg` & `trove-8.0.1/setup.cfg`

 * *Files 24% similar despite different names*

```diff
@@ -1,65 +1,81 @@
 [metadata]
 name = trove
 summary = OpenStack DBaaS
-description_file = 
+description-file = 
 	README.rst
 author = OpenStack
-author_email = openstack-discuss@lists.openstack.org
-home_page = https://docs.openstack.org/trove/latest/
-python_requires = >= 3.8
+author-email = openstack-dev@lists.openstack.org
+home-page = https://wiki.openstack.org/wiki/Trove
 classifier = 
 	Environment :: OpenStack
 	Intended Audience :: Information Technology
 	Intended Audience :: System Administrators
 	License :: OSI Approved :: Apache Software License
 	Operating System :: POSIX :: Linux
 	Programming Language :: Python
+	Programming Language :: Python :: 2
+	Programming Language :: Python :: 2.7
 	Programming Language :: Python :: 3
-	Programming Language :: Python :: 3.8
-	Programming Language :: Python :: 3.9
-	Programming Language :: Python :: 3.10
-	Programming Language :: Python :: 3.11
+	Programming Language :: Python :: 3.5
 
 [files]
-data_files = 
-	etc/trove =
-	etc/trove/api-paste.ini
 packages = 
 	trove
 
 [entry_points]
-wsgi_scripts = 
-	trove-wsgi = trove.cmd.app_wsgi:wsgimain
 console_scripts = 
 	trove-api = trove.cmd.api:main
 	trove-taskmanager = trove.cmd.taskmanager:main
 	trove-mgmt-taskmanager = trove.cmd.taskmanager:mgmt_main
 	trove-conductor = trove.cmd.conductor:main
 	trove-manage = trove.cmd.manage:main
 	trove-guestagent = trove.cmd.guest:main
 	trove-fake-mode = trove.cmd.fakemode:main
-	trove-status = trove.cmd.status:main
-	trove-docker-plugin = trove.cmd.network_driver:main
 trove.api.extensions = 
+	account = trove.extensions.routes.account:Account
 	mgmt = trove.extensions.routes.mgmt:Mgmt
 	mysql = trove.extensions.routes.mysql:Mysql
+	security_group = trove.extensions.routes.security_group:Security_group
 trove.guestagent.module.drivers = 
 	ping = trove.guestagent.module.drivers.ping_driver:PingDriver
 	new_relic_license = trove.guestagent.module.drivers.new_relic_license_driver:NewRelicLicenseDriver
 oslo.messaging.notify.drivers = 
 	trove.openstack.common.notifier.log_notifier = oslo_messaging.notify._impl_log:LogDriver
 	trove.openstack.common.notifier.no_op_notifier = oslo_messaging.notify._impl_noop:NoOpDriver
 	trove.openstack.common.notifier.rpc_notifier2 = oslo_messaging.notify.messaging:MessagingV2Driver
 	trove.openstack.common.notifier.rpc_notifier = oslo_messaging.notify.messaging:MessagingDriver
 	trove.openstack.common.notifier.test_notifier = oslo_messaging.notify._impl_test:TestDriver
-oslo.policy.policies = 
-	trove = trove.common.policies:list_rules
-oslo.policy.enforcer = 
-	trove = trove.common.policy:get_enforcer
-oslo.config.opts = 
-	trove.config = trove.common.cfg:list_opts
+tempest.test_plugins = 
+	trove_tests = trove.tests.tempest.plugin:TroveTempestPlugin
+
+[global]
+setup-hooks = 
+	pbr.hooks.setup_hook
+
+[build_sphinx]
+all_files = 1
+build-dir = doc/build
+source-dir = doc/source
+warning-is-error = 1
+
+[compile_catalog]
+directory = trove/locale
+domain = trove
+
+[update_catalog]
+domain = trove
+output_dir = trove/locale
+input_file = trove/locale/trove.pot
+
+[extract_messages]
+keywords = _ gettext ngettext l_ lazy_gettext
+mapping_file = babel.cfg
+output_file = trove/locale/trove.pot
+
+[wheel]
+universal = 1
 
 [egg_info]
 tag_build = 
 tag_date = 0
```

### Comparing `trove-21.0.0.0rc2/setup.py` & `trove-8.0.1/setup.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/test-requirements.txt` & `trove-8.0.1/test-requirements.txt`

 * *Files 18% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 # The order of packages is significant, because pip processes them in the order
 # of appearance. Changing the order has an impact on the overall integration
 # process, which may cause wedges in the gate later.
 # Hacking already pins down pep8, pyflakes and flake8
-hacking>=3.0.1,<3.1.0 # Apache-2.0
-bandit[baseline]>=1.7.7 # Apache-2.0
+hacking!=0.13.0,<0.14,>=0.12.0 # Apache-2.0
+bandit>=1.1.0 # Apache-2.0
+sphinx>=1.6.2 # BSD
+os-api-ref>=1.0.0 # Apache-2.0
+reno!=2.3.1,>=1.8.0 # Apache-2.0
 coverage!=4.4,>=4.0 # Apache-2.0
-nose>=1.3.7 # LGPL
-nosexcover>=1.0.10 # BSD
+nose # LGPL
+nosexcover # BSD
+openstackdocstheme>=1.11.0 # Apache-2.0
 openstack.nose-plugin>=0.7 # Apache-2.0
-WebTest>=2.0.27 # MIT
+WebTest>=2.0 # MIT
 wsgi-intercept>=1.4.1 # MIT License
 proboscis>=1.2.5.3 # Apache-2.0
 python-troveclient>=2.2.0 # Apache-2.0
-testtools>=2.2.0 # MIT
+mock>=2.0 # BSD
+mox3!=0.19.0,>=0.7.0 # Apache-2.0
+testtools>=1.4.0 # MIT
+testrepository>=0.0.18 # Apache-2.0/BSD
 pymongo!=3.1,>=3.0.2 # Apache-2.0
 redis>=2.10.0 # MIT
+psycopg2>=2.5 # LGPL/ZPL
 cassandra-driver!=3.6.0,>=2.1.4 # Apache-2.0
 couchdb>=0.8 # Apache-2.0
-stestr>=1.1.0 # Apache-2.0
-doc8>=0.8.1 # Apache-2.0
-astroid==1.6.5 # LGPLv2.1
-pylint==1.9.2 # GPLv2
-oslotest>=3.2.0 # Apache-2.0
-tenacity>=4.9.0  # Apache-2.0
-# Docs building
-openstackdocstheme>=2.2.1 # Apache-2.0
-os-api-ref>=1.4.0 # Apache-2.0
-reno>=3.1.0 # Apache-2.0
+os-testr>=0.8.0 # Apache-2.0
+astroid<1.4.0 # LGPLv2.1 # breaks pylint 1.4.4
+pylint==1.4.5 # GPLv2
```

### Comparing `trove-21.0.0.0rc2/tools/install_venv.py` & `trove-8.0.1/tools/install_venv.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,14 +18,16 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 """
 Installation script for Trove's development virtualenv
 """
 
+from __future__ import print_function
+
 import os
 import subprocess
 import sys
 
 
 ROOT = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
 VENV = os.path.join(ROOT, '.venv')
@@ -35,14 +37,19 @@
 
 
 def die(message, *args):
     print(message % args, file=sys.stderr)
     sys.exit(1)
 
 
+def check_python_version():
+    if sys.version_info < (2, 7):
+        die("Need Python Version >= 2.7")
+
+
 def run_command(cmd, redirect_output=True, check_exit_code=True):
     """
     Runs a command in an out-of-process shell, returning the
     output of that command.  Working directory is ROOT.
     """
     if redirect_output:
         stdout = subprocess.PIPE
@@ -68,15 +75,15 @@
     if not HAS_VIRTUALENV:
         print('not found.')
         # Try installing it via easy_install...
         if HAS_EASY_INSTALL:
             print('Installing virtualenv via easy_install...'),
             if not (run_command(['which', 'easy_install']) and
                     run_command(['easy_install', 'virtualenv'])):
-                die('ERROR: virtualenv not found.\nTrove development'
+                die('ERROR: virtualenv not found.\n\Trove development'
                     ' requires virtualenv, please install it using your'
                     ' favorite package management tool')
             print('done.')
     print('done.')
 
 
 def create_virtualenv(venv=VENV):
@@ -115,27 +122,28 @@
 
     Trove development uses virtualenv to track and manage Python
     dependencies while in development and testing.
 
     To activate the Trove virtualenv for the extent of your current shell
     session you can run:
 
-    $ . .venv/bin/activate
+    $ source .venv/bin/activate
 
     Or, if you prefer, you can run commands in the virtualenv on a case by case
     basis by running:
 
     $ tools/with_venv.sh <your command>
 
     Also, make test will automatically use the virtualenv.
     """
     print(help)
 
 
 def main(argv):
+    check_python_version()
     check_dependencies()
     create_virtualenv()
     install_dependencies()
     print_help()
 
 if __name__ == '__main__':
     main(sys.argv)
```

### Comparing `trove-21.0.0.0rc2/tools/trove-pylint.README` & `trove-8.0.1/tools/trove-pylint.README`

 * *Files 2% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 (lintable) errors and fix them.
 
 How trove-pylint works
 ----------------------
 
 trove-pylint is driven by a configuration file which is by default,
 located in tools/trove-pylint.config. This file is a json dump of the
-configuration. A default configuration file looks like this.
+configuration. A default configuraiton file looks like this.
 
    {
       "include": ["*.py"],
       "folder": "trove",
       "options": ["--rcfile=./pylintrc", "-E"],
       "ignored_files": ['trove/tests'],
       "ignored_codes": [],
@@ -157,14 +157,17 @@
 ------------
 
 1. The tool appears to be very sensitive to the version(s) of pylint
 and astroid. In testing, I've found that if the version of either of
 these changes, you could either have a failure of the tool (exceptions
 thrown, ...) or a different set of errors reported.
 
-Refer to test-requirements.txt to see the versions currently being used.
+Currently, test-requirements.txt sets these versions in this way.
+
+astroid<1.4.0  # LGPLv2.1 # breaks pylint 1.4.4
+pylint==1.4.5  # GPLv2
 
 If you run the tool on your machine and find that there are no errors,
 but find that either the CI generates errors, or that the tool run
 through tox generates errors, check what versions of astroid and
 pylint are being run in each configuration.
```

### Comparing `trove-21.0.0.0rc2/tools/trove-pylint.config` & `trove-8.0.1/tools/trove-pylint.config`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9480918727915194%*

 * *Differences: {"'ignored_codes'": '[]',*

 * * "'ignored_file_code_messages'": "{60: {insert: [(0, 'trove/common/single_tenant_remote.py')], "*

 * *                                 'delete: [0]}, 61: {insert: [(0, '*

 * *                                 "'trove/common/single_tenant_remote.py')], delete: [0]}, insert: "*

 * *                                 "[(139, ['trove/extensions/common/service.py', 'E1101', "*

 * *                                 '"Instance of \'DefaultRootController\' has no '*

 * *                                 '\'_find_root_u […]*

```diff
@@ -3,17 +3,15 @@
         "Undefined variable '_'",
         "Undefined variable '_LC'",
         "Undefined variable '_LE'",
         "Undefined variable '_LI'",
         "Undefined variable '_LW'"
     ],
     "folder": "trove",
-    "ignored_codes": [
-        "not-callable"
-    ],
+    "ignored_codes": [],
     "ignored_file_code_messages": [
         [
             "trove/backup/models.py",
             "E1101",
             "Class 'DBBackup' has no 'deleted' member",
             "Backup.list"
         ],
@@ -368,21 +366,21 @@
         [
             "trove/common/models.py",
             "no-member",
             "Instance of 'ModelBase' has no 'id' member",
             "ModelBase.__hash__"
         ],
         [
-            "trove/common/clients_admin.py",
+            "trove/common/single_tenant_remote.py",
             "E0611",
             "No name 'v1_1' in module 'novaclient'",
             null
         ],
         [
-            "trove/common/clients_admin.py",
+            "trove/common/single_tenant_remote.py",
             "no-name-in-module",
             "No name 'v1_1' in module 'novaclient'",
             null
         ],
         [
             "trove/common/strategies/cluster/experimental/mongodb/api.py",
             "E1101",
@@ -770,38 +768,14 @@
         [
             "trove/db/sqlalchemy/migrate_repo/versions/042_add_cluster_configuration_id.py",
             "no-member",
             "Instance of 'Table' has no 'create_column' member",
             "upgrade"
         ],
         [
-            "trove/db/sqlalchemy/migrate_repo/versions/044_remove_datastore_configuration_parameters_deleted.py",
-            "E1101",
-            "Instance of 'Table' has no 'drop_column' member",
-            "upgrade"
-        ],
-        [
-            "trove/db/sqlalchemy/migrate_repo/versions/044_remove_datastore_configuration_parameters_deleted.py",
-            "no-member",
-            "Instance of 'Table' has no 'drop_column' member",
-            "upgrade"
-        ],
-        [
-            "trove/db/sqlalchemy/migrate_repo/versions/044_remove_datastore_configuration_parameters_deleted.py",
-            "E1120",
-            "No value for argument 'dml' in method call",
-            "upgrade"
-        ],
-        [
-            "trove/db/sqlalchemy/migrate_repo/versions/044_remove_datastore_configuration_parameters_deleted.py",
-            "no-value-for-parameter",
-            "No value for argument 'dml' in method call",
-            "upgrade"
-        ],
-        [
             "trove/db/sqlalchemy/migration.py",
             "E0611",
             "No name 'exceptions' in module 'migrate.versioning'",
             null
         ],
         [
             "trove/db/sqlalchemy/migration.py",
@@ -866,14 +840,26 @@
         [
             "trove/dns/designate/driver.py",
             "no-member",
             "Instance of 'Client' has no 'records' member",
             "DesignateDriver.delete_entry"
         ],
         [
+            "trove/extensions/common/service.py",
+            "E1101",
+            "Instance of 'DefaultRootController' has no '_find_root_user' member",
+            "DefaultRootController.root_delete"
+        ],
+        [
+            "trove/extensions/common/service.py",
+            "no-member",
+            "Instance of 'DefaultRootController' has no '_find_root_user' member",
+            "DefaultRootController.root_delete"
+        ],
+        [
             "trove/extensions/mgmt/instances/service.py",
             "E1101",
             "Instance of 'BuiltInstance' has no 'get_diagnostics' member",
             "MgmtInstanceController.diagnostics"
         ],
         [
             "trove/extensions/mgmt/instances/service.py",
@@ -1239,20 +1225,14 @@
             "trove/instance/models.py",
             "E1101",
             "Instance of 'DBInstance' has no 'encrypted_key' member",
             "DBInstance.key"
         ],
         [
             "trove/instance/models.py",
-            "E1101",
-            "Instance of 'InstanceServiceStatus' has no 'updated_at' member",
-            "InstanceServiceStatus.is_uptodate"
-        ],
-        [
-            "trove/instance/models.py",
             "no-member",
             "Class 'DBInstance' has no 'cluster_id' member",
             "module_instance_count"
         ],
         [
             "trove/instance/models.py",
             "no-member",
@@ -1328,20 +1308,14 @@
         [
             "trove/instance/models.py",
             "no-member",
             "Instance of 'DBInstance' has no 'encrypted_key' member",
             "DBInstance.key"
         ],
         [
-            "trove/instance/models.py",
-            "no-member",
-            "Instance of 'InstanceServiceStatus' has no 'updated_at' member",
-            "InstanceServiceStatus.is_uptodate"
-        ],
-        [
             "trove/instance/service.py",
             "E1101",
             "Instance of 'BuiltInstance' has no 'get_default_configuration_template' member",
             "InstanceController.configuration"
         ],
         [
             "trove/instance/service.py",
@@ -1509,20 +1483,14 @@
             "trove/taskmanager/manager.py",
             "E1101",
             "Instance of 'FreshInstance' has no 'get_replication_master_snapshot' member",
             "Manager._create_replication_slave"
         ],
         [
             "trove/taskmanager/manager.py",
-            "E1136",
-            "Value 'snapshot' is unsubscriptable",
-            "Manager._create_replication_slave"
-        ],
-        [
-            "trove/taskmanager/manager.py",
             "E1101",
             "Instance of 'FreshInstance' has no 'wait_for_instance' member",
             "Manager._create_instance"
         ],
         [
             "trove/taskmanager/manager.py",
             "no-member",
@@ -1686,20 +1654,14 @@
             "FreshInstanceTasks._create_server_volume_heat"
         ],
         [
             "trove/taskmanager/models.py",
             "unexpected-keyword-arg",
             "Unexpected keyword argument 'recover_func' in method call",
             "ResizeVolumeAction._resize_active_volume"
-        ],
-        [
-            "trove/common/context.py",
-            "E1101",
-            "Instance of 'TroveContext' has no 'notification' member",
-            "TroveContext.to_dict"
         ]
     ],
     "ignored_file_codes": [],
     "ignored_file_messages": [],
     "ignored_files": [
         "trove/tests"
     ],
```

### Comparing `trove-21.0.0.0rc2/tools/trove-pylint.py` & `trove-8.0.1/tools/trove-pylint.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,24 +8,27 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 # License for the specific language governing permissions and limitations
 # under the License.
 
+from __future__ import print_function
+
 import fnmatch
 import json
 from collections import OrderedDict
-import io
 import os
 import re
+import six
 import sys
 
 from pylint import lint
 from pylint.reporters import text
+from six.moves import cStringIO as csio
 
 DEFAULT_CONFIG_FILE = "tools/trove-pylint.config"
 DEFAULT_IGNORED_FILES = ['trove/tests']
 DEFAULT_IGNORED_CODES = []
 DEFAULT_IGNORED_MESSAGES = []
 DEFAULT_ALWAYS_ERROR = [
     "Undefined variable '_'",
@@ -55,15 +58,16 @@
 
         self.config = self.default_config
 
     def sort_config(self):
         sorted_config = OrderedDict()
         for key in sorted(self.config.keys()):
             value = self.get(key)
-            if isinstance(value, list) and not isinstance(value,str):
+            if isinstance(value, list) and not isinstance(value,
+                                                          six.string_types):
                 sorted_config[key] = sorted(value)
             else:
                 sorted_config[key] = value
 
         return sorted_config
 
     def save(self, filename=DEFAULT_CONFIG_FILE):
@@ -128,22 +132,21 @@
                 [filename, code] in self.config['ignored_file_codes']):
             return True
 
         if filename and codename and (
                 [filename, codename] in self.config['ignored_file_codes']):
             return True
 
+        fcm_ignore1 = [filename, code, message]
+        fcm_ignore2 = [filename, codename, message]
         for fcm in self.config['ignored_file_code_messages']:
-            if filename != fcm[0]:
-                # This ignore rule is for a different file.
-                continue
-            if fcm[1] not in (code, codename):
-                # This ignore rule is for a different code or codename.
-                continue
-            if message.startswith(fcm[2]):
+            if fcm_ignore1 == [fcm[0], fcm[1], fcm[2]]:
+                return True
+
+            if fcm_ignore2 == [fcm[0], fcm[1], fcm[2]]:
                 return True
 
         return False
 
     def ignore_code(self, c):
         _c = set(self.config['ignored_codes'])
         _c.add(c)
@@ -198,21 +201,20 @@
     # that's it folks
 
 
 class LintRunner(object):
     def __init__(self):
         self.config = Config()
         self.idline = re.compile("^[*]* Module .*")
-        self.detail = re.compile(r"(\S+):(\d+): \[(\S+)\((\S+)\),"
-                                 r" (\S+)?] (.*)")
+        self.detail = re.compile("(\S+):(\d+): \[(\S+)\((\S+)\), (\S+)?] (.*)")
 
     def dolint(self, filename):
         exceptions = set()
 
-        buffer = io.StringIO()
+        buffer = csio()
         reporter = ParseableTextReporter(output=buffer)
         options = list(self.config.get('options'))
         options.append(filename)
         lint.Run(options, reporter=reporter, exit=False)
 
         output = buffer.getvalue()
         buffer.close()
```

### Comparing `trove-21.0.0.0rc2/trove/backup/models.py` & `trove-8.0.1/trove/backup/models.py`

 * *Files 14% similar despite different names*

```diff
@@ -11,31 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Model classes that form the core of snapshots functionality."""
 
 from oslo_log import log as logging
-from requests.exceptions import ConnectionError
 from sqlalchemy import desc
 from swiftclient.client import ClientException
 
 from trove.backup.state import BackupState
 from trove.common import cfg
-from trove.common import clients
-from trove.common import constants
 from trove.common import exception
 from trove.common.i18n import _
-from trove.common import swift
+from trove.common.remote import create_swift_client
 from trove.common import utils
 from trove.datastore import models as datastore_models
 from trove.db.models import DatabaseModelBase
 from trove.quota.quota import run_with_quotas
 from trove.taskmanager import api
 
+
 CONF = cfg.CONF
 LOG = logging.getLogger(__name__)
 
 
 class Backup(object):
 
     @classmethod
@@ -47,80 +45,46 @@
         if not datastore_cfg or not (
                 datastore_cfg.get('backup_strategy', None)):
             raise exception.DatastoreOperationNotSupported(
                 operation=operation, datastore=instance.datastore.name)
 
     @classmethod
     def create(cls, context, instance, name, description=None,
-               parent_id=None, incremental=False, swift_container=None,
-               restore_from=None):
+               parent_id=None, incremental=False):
         """
         create db record for Backup
         :param cls:
         :param context: tenant_id included
         :param instance:
         :param name:
         :param description:
         :param parent_id:
         :param incremental: flag to indicate incremental backup
-                            based on previous backup
-        :param swift_container: Swift container name.
-        :param restore_from: A dict that contains backup information of another
-                             region.
+        based on previous backup
         :return:
         """
-        backup_state = BackupState.NEW
-        checksum = None
-        instance_id = None
-        parent = None
-        last_backup_id = None
-        location = None
-        backup_type = constants.BACKUP_TYPE_FULL
-        size = None
-
-        if restore_from:
-            # Check location and datastore version.
-            LOG.info(f"Restoring backup, restore_from: {restore_from}")
-            backup_state = BackupState.RESTORED
-
-            ds_version_id = restore_from.get('local_datastore_version_id')
-            ds_version = datastore_models.DatastoreVersion.load_by_uuid(
-                ds_version_id)
-
-            location = restore_from.get('remote_location')
-            swift_client = clients.create_swift_client(context)
-            try:
-                obj_meta = swift.get_metadata(swift_client, location,
-                                              extra_attrs=['etag'])
-            except Exception:
-                msg = f'Failed to restore backup from {location}'
-                LOG.exception(msg)
-                raise exception.BackupCreationError(msg)
-
-            checksum = obj_meta['etag']
-            if 'parent_location' in obj_meta:
-                backup_type = constants.BACKUP_TYPE_INC
 
-            size = restore_from['size']
-        else:
+        def _create_resources():
+            # parse the ID from the Ref
             instance_id = utils.get_id_from_href(instance)
-            # Import here to avoid circular imports.
-            from trove.instance import models as inst_model
-            instance_model = inst_model.Instance.load(context, instance_id)
+
+            # verify that the instance exists and can perform actions
+            from trove.instance.models import Instance
+            instance_model = Instance.load(context, instance_id)
             instance_model.validate_can_perform_action()
+            cls.validate_can_perform_action(
+                instance_model, 'backup_create')
+            cls.verify_swift_auth_token(context)
             if instance_model.cluster_id is not None:
                 raise exception.ClusterInstanceOperationNotSupported()
 
-            cls.validate_can_perform_action(instance_model, 'backup_create')
-
-            cls.verify_swift_auth_token(context)
-
             ds = instance_model.datastore
             ds_version = instance_model.datastore_version
-
+            parent = None
+            last_backup_id = None
             if parent_id:
                 # Look up the parent info or fail early if not found or if
                 # the user does not have access to the parent.
                 _parent = cls.get_by_id(context, parent_id)
                 parent = {
                     'location': _parent.location,
                     'checksum': _parent.checksum,
@@ -129,79 +93,60 @@
                 _parent = Backup.get_last_completed(context, instance_id)
                 if _parent:
                     parent = {
                         'location': _parent.location,
                         'checksum': _parent.checksum
                     }
                     last_backup_id = _parent.id
-
-            if parent:
-                backup_type = constants.BACKUP_TYPE_INC
-
-        def _create_resources():
             try:
-                db_info = DBBackup.create(
-                    name=name,
-                    description=description,
-                    tenant_id=context.project_id,
-                    state=backup_state,
-                    instance_id=instance_id,
-                    parent_id=parent_id or last_backup_id,
-                    datastore_version_id=ds_version.id,
-                    deleted=False,
-                    location=location,
-                    checksum=checksum,
-                    backup_type=backup_type,
-                    size=size
-                )
+                db_info = DBBackup.create(name=name,
+                                          description=description,
+                                          tenant_id=context.tenant,
+                                          state=BackupState.NEW,
+                                          instance_id=instance_id,
+                                          parent_id=parent_id or
+                                          last_backup_id,
+                                          datastore_version_id=ds_version.id,
+                                          deleted=False)
             except exception.InvalidModelError as ex:
-                LOG.exception("Unable to create backup record for "
-                              "instance: %s", instance_id)
+                LOG.exception(_("Unable to create backup record for "
+                                "instance: %s"), instance_id)
                 raise exception.BackupCreationError(str(ex))
 
-            if not restore_from:
-                backup_info = {
-                    'id': db_info.id,
-                    'name': name,
-                    'description': description,
-                    'instance_id': instance_id,
-                    'backup_type': db_info.backup_type,
-                    'checksum': db_info.checksum,
-                    'parent': parent,
-                    'datastore': ds.name,
-                    'datastore_version': ds_version.name,
-                    'swift_container': swift_container
-                }
-                api.API(context).create_backup(backup_info, instance_id)
-            else:
-                context.notification.payload.update(
-                    {'backup_id': db_info.id}
-                )
-
+            backup_info = {'id': db_info.id,
+                           'name': name,
+                           'description': description,
+                           'instance_id': instance_id,
+                           'backup_type': db_info.backup_type,
+                           'checksum': db_info.checksum,
+                           'parent': parent,
+                           'datastore': ds.name,
+                           'datastore_version': ds_version.name,
+                           }
+            api.API(context).create_backup(backup_info, instance_id)
             return db_info
-
-        return run_with_quotas(context.project_id, {'backups': 1},
+        return run_with_quotas(context.tenant,
+                               {'backups': 1},
                                _create_resources)
 
     @classmethod
     def running(cls, instance_id, exclude=None):
         """
         Returns the first running backup for instance_id
         :param instance_id: Id of the instance
         :param exclude: Backup ID to exclude from the query (any other running)
         """
-        with DBBackup.query() as query:
-            query = query.filter(DBBackup.instance_id == instance_id,
-                                 DBBackup.state.in_(
-                                     BackupState.RUNNING_STATES))
-            # filter out deleted backups, PEP8 does not like field == False!
-            query = query.filter_by(deleted=False)
-            if exclude:
-                query = query.filter(DBBackup.id != exclude)
-            return query.first()
+        query = DBBackup.query()
+        query = query.filter(DBBackup.instance_id == instance_id,
+                             DBBackup.state.in_(BackupState.RUNNING_STATES))
+        # filter out deleted backups, PEP8 does not like field == False!
+        query = query.filter_by(deleted=False)
+        if exclude:
+            query = query.filter(DBBackup.id != exclude)
+        return query.first()
 
     @classmethod
     def get_by_id(cls, context, backup_id, deleted=False):
         """
         get the backup for that id
         :param cls:
         :param backup_id: Id of the backup to return
@@ -230,57 +175,53 @@
         query = query.limit(limit)
         query = query.offset(marker)
         # check if we need to send a marker for the next page
         if query.count() < limit:
             marker = None
         else:
             marker += limit
-        res = query.all()
-        return res, marker
+        return query.all(), marker
 
     @classmethod
-    def list(cls, context, datastore=None, instance_id=None, project_id=None,
-             all_projects=False):
-        with DBBackup.query() as query:
-            filters = [DBBackup.deleted == 0]
-
-            if project_id:
-                filters.append(DBBackup.tenant_id == project_id)
-            elif not all_projects:
-                filters.append(DBBackup.tenant_id == context.project_id)
-
-            if instance_id:
-                filters.append(DBBackup.instance_id == instance_id)
-
-            if datastore:
-                ds = datastore_models.Datastore.load(datastore)
-                filters.append(datastore_models.DBDatastoreVersion.
-                               datastore_id == ds.id)
-                query = query.join(datastore_models.DBDatastoreVersion)
-
-            query = query.filter(*filters)
-            return cls._paginate(context, query)
+    def list(cls, context, datastore=None):
+        """
+        list all live Backups belong to given tenant
+        :param cls:
+        :param context: tenant_id included
+        :param datastore: datastore to filter by
+        :return:
+        """
+        query = DBBackup.query()
+        filters = [DBBackup.tenant_id == context.tenant,
+                   DBBackup.deleted == 0]
+        if datastore:
+            ds = datastore_models.Datastore.load(datastore)
+            filters.append(datastore_models.DBDatastoreVersion.
+                           datastore_id == ds.id)
+            query = query.join(datastore_models.DBDatastoreVersion)
+        query = query.filter(*filters)
+        return cls._paginate(context, query)
 
     @classmethod
     def list_for_instance(cls, context, instance_id):
         """
         list all live Backups associated with given instance
         :param cls:
         :param instance_id:
         :return:
         """
-        with DBBackup.query() as query:
-            if context.is_admin:
-                query = query.filter_by(instance_id=instance_id,
-                                        deleted=False)
-            else:
-                query = query.filter_by(instance_id=instance_id,
-                                        tenant_id=context.project_id,
-                                        deleted=False)
-            return cls._paginate(context, query)
+        query = DBBackup.query()
+        if context.is_admin:
+            query = query.filter_by(instance_id=instance_id,
+                                    deleted=False)
+        else:
+            query = query.filter_by(instance_id=instance_id,
+                                    tenant_id=context.tenant,
+                                    deleted=False)
+        return cls._paginate(context, query)
 
     @classmethod
     def get_last_completed(cls, context, instance_id,
                            include_incremental=True):
         """
         returns last completed backup
         :param cls:
@@ -298,158 +239,98 @@
                 if not last_backup or backup.updated > last_backup.updated:
                     last_backup = backup
 
         return last_backup
 
     @classmethod
     def fail_for_instance(cls, instance_id):
-        with DBBackup.query() as query:
-            query = query.filter(DBBackup.instance_id == instance_id,
-                                 DBBackup.state.in_(
-                                     BackupState.RUNNING_STATES))
-            query = query.filter_by(deleted=False)
-            for backup in query.all():
-                backup.state = BackupState.FAILED
-                backup.save()
+        query = DBBackup.query()
+        query = query.filter(DBBackup.instance_id == instance_id,
+                             DBBackup.state.in_(BackupState.RUNNING_STATES))
+        query = query.filter_by(deleted=False)
+        for backup in query.all():
+            backup.state = BackupState.FAILED
+            backup.save()
 
     @classmethod
     def delete(cls, context, backup_id):
         """
         update Backup table on deleted flag for given Backup
         :param cls:
         :param context: context containing the tenant id and token
         :param backup_id: Backup uuid
         :return:
         """
 
         # Recursively delete all children and grandchildren of this backup.
-        with DBBackup.query() as query:
-            query = query.filter_by(parent_id=backup_id, deleted=False)
-            for child in query.all():
-                try:
-                    cls.delete(context, child.id)
-                except exception.NotFound:
-                    LOG.warning("Backup %s cannot be found.", backup_id)
+        query = DBBackup.query()
+        query = query.filter_by(parent_id=backup_id, deleted=False)
+        for child in query.all():
+            cls.delete(context, child.id)
 
         def _delete_resources():
             backup = cls.get_by_id(context, backup_id)
             if backup.is_running:
                 msg = _("Backup %s cannot be deleted because it is running.")
                 raise exception.UnprocessableEntity(msg % backup_id)
             cls.verify_swift_auth_token(context)
             api.API(context).delete_backup(backup_id)
 
-        return run_with_quotas(context.project_id,
+        return run_with_quotas(context.tenant,
                                {'backups': -1},
                                _delete_resources)
 
     @classmethod
     def verify_swift_auth_token(cls, context):
         try:
-            client = clients.create_swift_client(context)
+            client = create_swift_client(context)
             client.get_account()
         except ClientException:
-            raise exception.SwiftAuthError(tenant_id=context.project_id)
+            raise exception.SwiftAuthError(tenant_id=context.tenant)
         except exception.NoServiceEndpoint:
-            raise exception.SwiftNotFound(tenant_id=context.project_id)
-        except ConnectionError:
-            raise exception.SwiftConnectionError()
-
-
-class BackupStrategy(object):
-    @classmethod
-    def create(cls, context, instance_id, swift_container):
-        try:
-            existing = DBBackupStrategy.find_by(tenant_id=context.project_id,
-                                                instance_id=instance_id)
-            existing.swift_container = swift_container
-            existing.save()
-            return existing
-        except exception.NotFound:
-            return DBBackupStrategy.create(
-                tenant_id=context.project_id,
-                instance_id=instance_id,
-                backend='swift',
-                swift_container=swift_container,
-            )
-
-    @classmethod
-    def list(cls, context, tenant_id, instance_id=None):
-        kwargs = {'tenant_id': tenant_id}
-        if instance_id:
-            kwargs['instance_id'] = instance_id
-        result = DBBackupStrategy.find_by_filter(**kwargs)
-        return result
-
-    @classmethod
-    def get(cls, context, instance_id):
-        try:
-            return DBBackupStrategy.find_by(tenant_id=context.project_id,
-                                            instance_id=instance_id)
-        except exception.NotFound:
-            try:
-                return DBBackupStrategy.find_by(tenant_id=context.project_id,
-                                                instance_id='')
-            except exception.NotFound:
-                return None
-
-    @classmethod
-    def delete(cls, context, tenant_id, instance_id):
-        try:
-            existing = DBBackupStrategy.find_by(tenant_id=tenant_id,
-                                                instance_id=instance_id)
-            existing.delete()
-        except exception.NotFound:
-            pass
+            raise exception.SwiftNotFound(tenant_id=context.tenant)
 
 
 def persisted_models():
-    return {'backups': DBBackup, 'backup_strategy': DBBackupStrategy}
+    return {'backups': DBBackup}
 
 
 class DBBackup(DatabaseModelBase):
     """A table for Backup records."""
-    _data_fields = ['name', 'description', 'location', 'backup_type',
+    _data_fields = ['id', 'name', 'description', 'location', 'backup_type',
                     'size', 'tenant_id', 'state', 'instance_id',
                     'checksum', 'backup_timestamp', 'deleted', 'created',
                     'updated', 'deleted_at', 'parent_id',
                     'datastore_version_id']
-    _table_name = 'backups'
+    preserve_on_delete = True
 
     @property
     def is_running(self):
         return self.state in BackupState.RUNNING_STATES
 
     @property
     def is_done(self):
         return self.state in BackupState.END_STATES
 
     @property
     def is_done_successfuly(self):
-        return self.state in [BackupState.COMPLETED, BackupState.RESTORED]
+        return self.state == BackupState.COMPLETED
 
     @property
     def filename(self):
         if self.location:
             last_slash = self.location.rfind("/")
             if last_slash < 0:
                 raise ValueError(_("Bad location for backup object: %s")
                                  % self.location)
             return self.location[last_slash + 1:]
         else:
             return None
 
     @property
-    def container_name(self):
-        if self.location:
-            return self.location.split('/')[-2]
-        else:
-            return None
-
-    @property
     def datastore(self):
         if self.datastore_version_id:
             return datastore_models.Datastore.load(
                 self.datastore_version.datastore_id)
 
     @property
     def datastore_version(self):
@@ -458,15 +339,15 @@
                 self.datastore_version_id)
 
     def check_swift_object_exist(self, context, verify_checksum=False):
         try:
             parts = self.location.split('/')
             obj = parts[-1]
             container = parts[-2]
-            client = clients.create_swift_client(context)
+            client = create_swift_client(context)
             LOG.debug("Checking if backup exists in %s", self.location)
             resp = client.head_object(container, obj)
             if verify_checksum:
                 LOG.debug("Checking if backup checksum matches swift "
                           "for backup %s", self.id)
                 # swift returns etag in double quotes
                 # e.g. '"dc3b0827f276d8d78312992cc60c2c3f"'
@@ -475,14 +356,8 @@
                     raise exception.RestoreBackupIntegrityError(
                         backup_id=self.id)
             return True
         except ClientException as e:
             if e.http_status == 404:
                 return False
             else:
-                raise exception.SwiftAuthError(tenant_id=context.project_id)
-
-
-class DBBackupStrategy(DatabaseModelBase):
-    """A table for backup strategy records."""
-    _data_fields = ['tenant_id', 'instance_id', 'backend', 'swift_container']
-    _table_name = 'backup_strategy'
+                raise exception.SwiftAuthError(tenant_id=context.tenant)
```

### Comparing `trove-21.0.0.0rc2/trove/backup/state.py` & `trove-8.0.1/trove/backup/state.py`

 * *Files 19% similar despite different names*

```diff
@@ -17,11 +17,10 @@
 
 class BackupState(object):
     NEW = "NEW"
     BUILDING = "BUILDING"
     SAVING = "SAVING"
     COMPLETED = "COMPLETED"
     FAILED = "FAILED"
-    RESTORED = "RESTORED"
     DELETE_FAILED = "DELETE_FAILED"
     RUNNING_STATES = [NEW, BUILDING, SAVING]
-    END_STATES = [COMPLETED, FAILED, DELETE_FAILED, RESTORED]
+    END_STATES = [COMPLETED, FAILED, DELETE_FAILED]
```

### Comparing `trove-21.0.0.0rc2/trove/cluster/models.py` & `trove-8.0.1/trove/cluster/models.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,37 +9,37 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+import six
+
 from oslo_log import log as logging
 
-from neutronclient.common import exceptions as neutron_exceptions
 from novaclient import exceptions as nova_exceptions
 from trove.cluster.tasks import ClusterTask
 from trove.cluster.tasks import ClusterTasks
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
-from trove.common import glance as common_glance
 from trove.common.i18n import _
 from trove.common.notification import (
     DBaaSClusterAttachConfiguration,
     DBaaSClusterDetachConfiguration,
     DBaaSClusterGrow,
     DBaaSClusterShrink,
     DBaaSClusterResetStatus,
     DBaaSClusterRestart)
 from trove.common.notification import DBaaSClusterUpgrade
 from trove.common.notification import DBaaSInstanceAttachConfiguration
 from trove.common.notification import DBaaSInstanceDetachConfiguration
 from trove.common.notification import EndNotification
 from trove.common.notification import StartNotification
+from trove.common import remote
 from trove.common import server_group as srv_grp
 from trove.common.strategies.cluster import strategy
 from trove.common import utils
 from trove.configuration import models as config_models
 from trove.datastore import models as datastore_models
 from trove.db import models as dbmodels
 from trove.instance import models as inst_models
@@ -54,18 +54,17 @@
 def persisted_models():
     return {
         'clusters': DBCluster,
     }
 
 
 class DBCluster(dbmodels.DatabaseModelBase):
-    _data_fields = ['created', 'updated', 'name', 'task_id',
+    _data_fields = ['id', 'created', 'updated', 'name', 'task_id',
                     'tenant_id', 'datastore_version_id', 'deleted',
                     'deleted_at', 'configuration_id']
-    _table_name = 'clusters'
 
     def __init__(self, task_status, **kwargs):
         """
         Creates a new persistable entity of the cluster.
         :param task_status: the current task of the cluster.
         :type task_status: trove.cluster.tasks.ClusterTask
         """
@@ -107,17 +106,17 @@
         self._db_instances = None
         self._server_group = None
         self._server_group_loaded = False
         self._locality = None
 
     @classmethod
     def get_guest(cls, instance):
-        return clients.create_guest_client(instance.context,
-                                           instance.db_info.id,
-                                           instance.datastore_version.manager)
+        return remote.create_guest_client(instance.context,
+                                          instance.db_info.id,
+                                          instance.datastore_version.manager)
 
     @classmethod
     def load_all(cls, context, tenant_id):
         db_infos = DBCluster.find_all(tenant_id=tenant_id,
                                       deleted=False)
         limit = utils.pagination_limit(context.limit, Cluster.DEFAULT_LIMIT)
         data_view = DBCluster.find_by_pagination('clusters', db_infos, "foo",
@@ -144,19 +143,19 @@
     def update_db(self, **values):
         self.db_info = DBCluster.find_by(id=self.id, deleted=False)
         for key in values:
             setattr(self.db_info, key, values[key])
         self.db_info.save()
 
     def reset_task(self):
-        LOG.info("Setting task to NONE on cluster %s", self.id)
+        LOG.info(_("Setting task to NONE on cluster %s"), self.id)
         self.update_db(task_status=ClusterTasks.NONE)
 
     def reset_status(self):
-        LOG.info("Resetting status to NONE on cluster %s", self.id)
+        LOG.info(_("Resetting status to NONE on cluster %s"), self.id)
         self.reset_task()
         instances = inst_models.DBInstance.find_all(cluster_id=self.id,
                                                     deleted=False).all()
         for inst in instances:
             instance = inst_models.load_any_instance(self.context, inst.id)
             instance.reset_status()
 
@@ -257,34 +256,30 @@
         """This is to facilitate the fact that the server group may not be
         set up before the create command returns.
         """
         self._locality = value
 
     @classmethod
     def create(cls, context, name, datastore, datastore_version,
-               instances, extended_properties, locality, configuration,
-               image_id=None):
+               instances, extended_properties, locality, configuration):
         locality = srv_grp.ServerGroup.build_scheduler_hint(
             context, locality, name)
         api_strategy = strategy.load_api_strategy(datastore_version.manager)
         return api_strategy.cluster_class.create(context, name, datastore,
                                                  datastore_version, instances,
                                                  extended_properties,
-                                                 locality, configuration,
-                                                 image_id)
+                                                 locality, configuration)
 
     def validate_cluster_available(self, valid_states=[ClusterTasks.NONE]):
         if self.db_info.task_status not in valid_states:
-            log_fmt = ("This action cannot be performed on the cluster while "
-                       "the current cluster task is '%s'.")
-            exc_fmt = _("This action cannot be performed on the cluster while "
-                        "the current cluster task is '%s'.")
-            LOG.error(log_fmt, self.db_info.task_status.name)
-            raise exception.UnprocessableEntity(
-                exc_fmt % self.db_info.task_status.name)
+            msg = (_("This action cannot be performed on the cluster while "
+                     "the current cluster task is '%s'.") %
+                   self.db_info.task_status.name)
+            LOG.error(msg)
+            raise exception.UnprocessableEntity(msg)
 
     def delete(self):
 
         self.validate_cluster_available([ClusterTasks.NONE,
                                          ClusterTasks.DELETING])
 
         db_insts = inst_models.DBInstance.find_all(cluster_id=self.id,
@@ -321,35 +316,22 @@
                     if 'nics' in node:
                         instance['nics'] = node['nics']
                     if 'availability_zone' in node:
                         instance['availability_zone'] = (
                             node['availability_zone'])
                     if 'type' in node:
                         instance_type = node['type']
-                        if isinstance(instance_type, str):
+                        if isinstance(instance_type, six.string_types):
                             instance_type = instance_type.split(',')
                         instance['instance_type'] = instance_type
                     instances.append(instance)
-
-                # Since Victoria, guest agent uses docker.
-                # Get image_id from glance if image_id in datastore_versions
-                # table is NULL.
-                image_id = self.ds_version.image_id
-                if not image_id:
-                    glance_client = clients.create_glance_client(context)
-                    image_id = common_glance.get_image_id(
-                        glance_client, self.ds_version.image_id,
-                        self.ds_version.image_tags)
-                return self.grow(instances, image_id)
-
+                return self.grow(instances)
         elif action == 'shrink':
             context.notification = DBaaSClusterShrink(context, request=req)
-            instance_ids = [instance['id'] for instance in param]
-            with StartNotification(context, cluster_id=self.id,
-                                   instance_ids=instance_ids):
+            with StartNotification(context, cluster_id=self.id):
                 instance_ids = [instance['id'] for instance in param]
                 return self.shrink(instance_ids)
         elif action == "reset-status":
             context.notification = DBaaSClusterResetStatus(context,
                                                            request=req)
             with StartNotification(context, cluster_id=self.id):
                 return self.reset_status()
@@ -381,15 +363,15 @@
                                                                    request=req)
             with StartNotification(context, cluster_id=self.id):
                 return self.configuration_detach()
 
         else:
             raise exception.BadRequest(_("Action %s not supported") % action)
 
-    def grow(self, instances, image_id=None):
+    def grow(self, instances):
         raise exception.BadRequest(_("Action 'grow' not supported"))
 
     def shrink(self, instance_ids):
         raise exception.BadRequest(_("Action 'shrink' not supported"))
 
     def rolling_restart(self):
         self.validate_cluster_available()
@@ -580,15 +562,15 @@
     for instance in instances:
         region_name = instance.get('region_name')
         flavor_id = instance['flavor_id']
         try:
             if region_name in nova_cli_cache:
                 nova_client = nova_cli_cache[region_name]
             else:
-                nova_client = clients.create_nova_client(
+                nova_client = remote.create_nova_client(
                     context, region_name)
                 nova_cli_cache[region_name] = nova_client
 
             flavor = nova_client.flavors.get(flavor_id)
             if (not volume_enabled and
                     (ephemeral_enabled and flavor.ephemeral == 0)):
                 raise exception.LocalStorageNotSpecified(
@@ -663,26 +645,7 @@
         raise exception.VolumeSizeNotSpecified()
     max_size = CONF.max_accepted_volume_size
     if int(size) > max_size:
         msg = ("Volume 'size' cannot exceed maximum "
                "of %d Gb, %s cannot be accepted."
                % (max_size, size))
         raise exception.VolumeQuotaExceeded(msg)
-
-
-def validate_instance_nics(context, instances):
-    """Checking networks are same for the cluster."""
-    instance_nics = []
-    for instance in instances:
-        nics = instance.get('nics')
-        if nics:
-            instance_nics.append(nics[0].get('network_id'))
-    if len(set(instance_nics)) > 1:
-        raise exception.ClusterNetworksNotEqual()
-    if not instance_nics:
-        return
-    instance_nic = instance_nics[0]
-    try:
-        neutron_client = clients.create_neutron_client(context)
-        neutron_client.find_resource('network', instance_nic)
-    except neutron_exceptions.NotFound:
-        raise exception.NetworkNotFound(uuid=instance_nic)
```

### Comparing `trove-21.0.0.0rc2/trove/cluster/service.py` & `trove-8.0.1/trove/cluster/service.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,17 +16,15 @@
 from oslo_config.cfg import NoSuchOptError
 from oslo_log import log as logging
 
 from trove.cluster import models
 from trove.cluster import views
 from trove.common import apischema
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
-from trove.common import glance as common_glance
 from trove.common.i18n import _
 from trove.common import notification
 from trove.common.notification import StartNotification
 from trove.common import pagination
 from trove.common import policy
 from trove.common import utils
 from trove.common import wsgi
@@ -141,18 +139,16 @@
         context = req.environ[wsgi.CONTEXT_KEY]
 
         # This theoretically allows the Admin tenant list clusters for
         # only one particular tenant as opposed to listing all clusters for
         # for all tenants.
         # * As far as I can tell this is the only call which actually uses the
         #   passed-in 'tenant_id' for anything.
-        if not context.is_admin and context.project_id != tenant_id:
-            raise exception.TroveOperationAuthError(
-                tenant_id=context.project_id
-            )
+        if not context.is_admin and context.tenant != tenant_id:
+            raise exception.TroveOperationAuthError(tenant_id=context.tenant)
 
         # The rule checks that the currently authenticated tenant can perform
         # the 'cluster-list' action.
         policy.authorize_on_tenant(context, 'cluster:index')
 
         # load all clusters and instances for the tenant
         clusters, marker = models.Cluster.load_all(context, tenant_id)
@@ -170,24 +166,15 @@
         policy.authorize_on_tenant(context, 'cluster:create')
 
         name = body['cluster']['name']
         datastore_args = body['cluster'].get('datastore', {})
         datastore, datastore_version = (
             datastore_models.get_datastore_version(**datastore_args))
 
-        # Since Victoria, guest agent uses docker.
-        # Get image_id from glance if image_id in datastore_versions table
-        # is NULL.
-        image_id = None
-        if not datastore_version.image_id:
-            glance_client = clients.create_glance_client(context)
-            image_id = common_glance.get_image_id(
-                glance_client, datastore_version.image_id,
-                datastore_version.image_tags)
-
+        # TODO(saurabhs): add extended_properties to apischema
         extended_properties = body['cluster'].get('extended_properties', {})
 
         try:
             clusters_enabled = (CONF.get(datastore_version.manager)
                                 .get('cluster_support'))
         except NoSuchOptError:
             clusters_enabled = False
@@ -225,23 +212,22 @@
         if locality:
             locality_domain = ['affinity', 'anti-affinity']
             locality_domain_msg = ("Invalid locality '%s'. "
                                    "Must be one of ['%s']" %
                                    (locality,
                                     "', '".join(locality_domain)))
             if locality not in locality_domain:
-                raise exception.BadRequest(message=locality_domain_msg)
+                raise exception.BadRequest(msg=locality_domain_msg)
 
         configuration = body['cluster'].get('configuration')
 
         context.notification = notification.DBaaSClusterCreate(context,
                                                                request=req)
         with StartNotification(context, name=name, datastore=datastore.name,
                                datastore_version=datastore_version.name):
             cluster = models.Cluster.create(context, name, datastore,
                                             datastore_version, instances,
                                             extended_properties,
-                                            locality, configuration,
-                                            image_id)
+                                            locality, configuration)
         cluster.locality = locality
         view = views.load_view(cluster, req=req, load_servers=False)
         return wsgi.Result(view.data(), 200)
```

### Comparing `trove-21.0.0.0rc2/trove/cluster/tasks.py` & `trove-8.0.1/trove/cluster/tasks.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/cluster/views.py` & `trove-8.0.1/trove/cluster/views.py`

 * *Files 1% similar despite different names*

```diff
@@ -89,14 +89,15 @@
             instance_ips = instance.get_visible_ip_addresses()
             if self.load_servers and instance_ips:
                 instance_dict["ip"] = instance_ips
                 if instance.type in ip_to_be_published_for:
                     ip_list.extend(instance_ips)
             if instance.type in instance_dict_to_be_published_for:
                 instances.append(instance_dict)
+        ip_list.sort()
         return instances, ip_list
 
     def build_instances(self):
         raise NotImplementedError()
 
     def get_extended_properties(self):
         return None
```

### Comparing `trove-21.0.0.0rc2/trove/cmd/__init__.py` & `trove-8.0.1/trove/cmd/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -22,13 +22,7 @@
 # It is not safe to leave monkey patching till later.
 
 import os
 
 if not os.environ.get('NO_EVENTLET_MONKEYPATCH'):
     import eventlet
     eventlet.monkey_patch(all=True)
-    # Monkey patch the original current_thread to use the up-to-date _active
-    # global variable. See https://bugs.launchpad.net/bugs/1863021 and
-    # https://github.com/eventlet/eventlet/issues/592
-    import __original_module_threading as orig_threading # noqa
-    import threading  # noqa
-    orig_threading.current_thread.__globals__['_active'] = threading._active
```

### Comparing `trove-21.0.0.0rc2/trove/cmd/api.py` & `trove-8.0.1/trove/cmd/api.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/cmd/app_wsgi.py` & `trove-8.0.1/trove/cmd/app.wsgi`

 * *Files 7% similar despite different names*

```diff
@@ -19,24 +19,22 @@
 from oslo_log import log as logging
 from trove.cmd.common import with_initialize
 from trove.common import pastedeploy
 from trove.common import profile
 
 LOG = logging.getLogger('trove.cmd.app')
 
-
 @with_initialize
 def wsgimain(CONF):
     from trove.common import cfg
     from trove.common import notification
     from trove.instance import models as inst_models
 
     notification.DBaaSAPINotification.register_notify_callback(
         inst_models.persist_instance_fault)
     cfg.set_api_config_defaults()
     profile.setup_profiler('api', CONF.host)
     conf_file = CONF.find_file(CONF.api_paste_config)
     LOG.debug("Trove started on %s", CONF.host)
     return pastedeploy.paste_deploy_app(conf_file, 'trove', {})
 
-
 application = wsgimain()
```

### Comparing `trove-21.0.0.0rc2/trove/cmd/common.py` & `trove-8.0.1/trove/cmd/common.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,14 +11,18 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 
 def initialize(extra_opts=None, pre_logging=None):
+    # Initialize localization support (the underscore character).
+    import gettext
+    gettext.install('trove', unicode=1)
+
     # Import only the modules necessary to initialize logging and determine if
     # debug_utils are enabled.
     import sys
 
     from oslo_log import log as logging
 
     from trove.common import cfg
```

### Comparing `trove-21.0.0.0rc2/trove/cmd/conductor.py` & `trove-8.0.1/trove/cmd/conductor.py`

 * *Files 11% similar despite different names*

```diff
@@ -30,10 +30,9 @@
         inst_models.persist_instance_fault)
     topic = conf.conductor_queue
     server = rpc_service.RpcService(
         key=None, manager=conf.conductor_manager, topic=topic,
         rpc_api_version=conductor_api.API.API_LATEST_VERSION,
         secure_serializer=sz.ConductorHostSerializer)
     workers = conf.trove_conductor_workers or processutils.get_worker_count()
-    launcher = openstack_service.launch(conf, server, workers=workers,
-                                        restart_method='mutate')
+    launcher = openstack_service.launch(conf, server, workers=workers)
     launcher.wait()
```

### Comparing `trove-21.0.0.0rc2/trove/cmd/fakemode.py` & `trove-8.0.1/trove/cmd/fakemode.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/cmd/manage.py` & `trove-8.0.1/trove/cmd/manage.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,17 +9,22 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+import gettext
 import inspect
 import sys
 
+
+gettext.install('trove', unicode=1)
+
+
 from oslo_log import log as logging
 
 from trove.common import cfg
 from trove.common import exception
 from trove.common.i18n import _
 from trove.common import utils
 from trove.configuration import models as config_models
@@ -42,15 +47,15 @@
         self.db_api.db_upgrade(CONF, version, repo_path=repo_path)
 
     def db_downgrade(self, version, repo_path=None):
         raise SystemExit(_("Database downgrade is no longer supported."))
 
     def execute(self):
         exec_method = getattr(self, CONF.action.name)
-        args = inspect.getfullargspec(exec_method)
+        args = inspect.getargspec(exec_method)
         args.args.remove('self')
         kwargs = {}
         for arg in args.args:
             kwargs[arg] = getattr(CONF.action, arg)
         exec_method(**kwargs)
 
     def datastore_update(self, datastore_name, default_version):
@@ -58,154 +63,107 @@
             datastore_models.update_datastore(datastore_name,
                                               default_version)
             print("Datastore '%s' updated." % datastore_name)
         except exception.DatastoreVersionNotFound as e:
             print(e)
 
     def datastore_version_update(self, datastore, version_name, manager,
-                                 image_id, packages, active, image_tags=None,
-                                 version=None):
+                                 image_id, packages, active):
         try:
             datastore_models.update_datastore_version(datastore,
                                                       version_name,
                                                       manager,
                                                       image_id,
-                                                      image_tags,
-                                                      packages, active,
-                                                      version=version)
-            print("Datastore version '%s(%s)' updated." %
-                  (version_name, version or version_name))
+                                                      packages, active)
+            print("Datastore version '%s' updated." % version_name)
         except exception.DatastoreNotFound as e:
             print(e)
 
     def db_recreate(self, repo_path):
         """Drops the database and recreates it."""
         self.db_api.drop_db(CONF)
         self.db_sync(repo_path)
 
     def db_load_datastore_config_parameters(self,
                                             datastore,
-                                            datastore_version_name,
-                                            config_file_location,
-                                            version=None):
+                                            datastore_version,
+                                            config_file_location):
         print("Loading config parameters for datastore (%s) version (%s)"
-              % (datastore, datastore_version_name))
+              % (datastore, datastore_version))
         config_models.load_datastore_configuration_parameters(
-            datastore, datastore_version_name, config_file_location,
-            version_number=version)
-
-    def db_remove_datastore_config_parameters(self, datastore,
-                                              datastore_version_name,
-                                              version=None):
-        print("Removing config parameters for datastore (%s) version (%s)"
-              % (datastore, datastore_version_name))
-        config_models.remove_datastore_configuration_parameters(
-            datastore, datastore_version_name, version_number=version)
+            datastore, datastore_version, config_file_location)
 
     def datastore_version_flavor_add(self, datastore_name,
-                                     datastore_version_name, flavor_ids,
-                                     version=None):
+                                     datastore_version_name, flavor_ids):
         """Adds flavors for a given datastore version id."""
-        dsmetadata = datastore_models.DatastoreVersionMetadata
         try:
-            datastore_version_id = dsmetadata.datastore_version_find(
-                datastore_name,
-                datastore_version_name,
-                version_number=version)
-
+            dsmetadata = datastore_models.DatastoreVersionMetadata
             dsmetadata.add_datastore_version_flavor_association(
-                datastore_version_id, flavor_ids.split(","))
+                datastore_name, datastore_version_name, flavor_ids.split(","))
             print("Added flavors '%s' to the '%s' '%s'."
                   % (flavor_ids, datastore_name, datastore_version_name))
-        except Exception as e:
+        except exception.DatastoreVersionNotFound as e:
             print(e)
 
     def datastore_version_flavor_delete(self, datastore_name,
-                                        datastore_version_name, flavor_id,
-                                        version=None):
+                                        datastore_version_name, flavor_id):
         """Deletes a flavor's association with a given datastore."""
         try:
             dsmetadata = datastore_models.DatastoreVersionMetadata
-
-            datastore_version_id = dsmetadata.datastore_version_find(
-                datastore_name,
-                datastore_version_name,
-                version_number=version)
-
             dsmetadata.delete_datastore_version_flavor_association(
-                datastore_version_id, flavor_id)
+                datastore_name, datastore_version_name, flavor_id)
             print("Deleted flavor '%s' from '%s' '%s'."
                   % (flavor_id, datastore_name, datastore_version_name))
-        except Exception as e:
+        except exception.DatastoreVersionNotFound as e:
             print(e)
 
     def datastore_version_volume_type_add(self, datastore_name,
                                           datastore_version_name,
-                                          volume_type_ids, version=None):
+                                          volume_type_ids):
         """Adds volume type assiciation for a given datastore version id."""
         try:
             dsmetadata = datastore_models.DatastoreVersionMetadata
-
-            datastore_version_id = dsmetadata.datastore_version_find(
-                datastore_name,
-                datastore_version_name,
-                version_number=version)
-
             dsmetadata.add_datastore_version_volume_type_association(
-                datastore_version_id,
+                datastore_name, datastore_version_name,
                 volume_type_ids.split(","))
             print("Added volume type '%s' to the '%s' '%s'."
                   % (volume_type_ids, datastore_name, datastore_version_name))
-        except Exception as e:
+        except exception.DatastoreVersionNotFound as e:
             print(e)
 
     def datastore_version_volume_type_delete(self, datastore_name,
                                              datastore_version_name,
-                                             volume_type_id, version=None):
+                                             volume_type_id):
         """Deletes a volume type association with a given datastore."""
         try:
             dsmetadata = datastore_models.DatastoreVersionMetadata
-
-            datastore_version_id = dsmetadata.datastore_version_find(
-                datastore_name,
-                datastore_version_name,
-                version_number=version)
-
             dsmetadata.delete_datastore_version_volume_type_association(
-                datastore_version_id, volume_type_id)
+                datastore_name, datastore_version_name, volume_type_id)
             print("Deleted volume type '%s' from '%s' '%s'."
                   % (volume_type_id, datastore_name, datastore_version_name))
-        except Exception as e:
+        except exception.DatastoreVersionNotFound as e:
             print(e)
 
     def datastore_version_volume_type_list(self, datastore_name,
-                                           datastore_version_name,
-                                           version=None):
+                                           datastore_version_name):
         """Lists volume type association with a given datastore."""
         try:
             dsmetadata = datastore_models.DatastoreVersionMetadata
-
-            datastore_version_id = dsmetadata.datastore_version_find(
-                datastore_name,
-                datastore_version_name,
-                version_number=version)
-
-            vtlist = dsmetadata. \
-                list_datastore_version_volume_type_associations(
-                    datastore_version_id)
+            vtlist = dsmetadata.list_datastore_volume_type_associations(
+                datastore_name, datastore_version_name)
             if vtlist.count() > 0:
                 for volume_type in vtlist:
-                    print("Datastore: %s, Version: %s, Volume Type: %s" %
-                          (datastore_name, datastore_version_name,
-                           volume_type.value))
+                    print ("Datastore: %s, Version: %s, Volume Type: %s" %
+                           (datastore_name, datastore_version_name,
+                            volume_type.value))
             else:
                 print("No Volume Type Associations found for Datastore: %s, "
                       "Version: %s." %
                       (datastore_name, datastore_version_name))
-        except Exception as e:
+        except exception.DatastoreVersionNotFound as e:
             print(e)
 
     def params_of(self, command_name):
         if Commands.has(command_name):
             return utils.MethodInspector(getattr(self, command_name))
 
 
@@ -244,147 +202,86 @@
         parser.add_argument('datastore', help='Name of the datastore.')
         parser.add_argument(
             'version_name', help='Name of the datastore version.')
         parser.add_argument(
             'manager', help='Name of the manager that will administer the '
             'datastore version.')
         parser.add_argument(
-            'image_id',
-            help='ID of the image used to create an instance of '
-                 'the datastore version.')
+            'image_id', help='ID of the image used to create an instance of '
+            'the datastore version.')
         parser.add_argument(
             'packages', help='Packages required by the datastore version that '
             'are installed on the guest image.')
         parser.add_argument(
-            'active', type=int,
-            help='Whether the datastore version is active or not. '
+            'active', help='Whether the datastore version is active or not. '
             'Accepted values are 0 and 1.')
-        parser.add_argument(
-            '--image-tags',
-            help='List of image tags separated by comma used for getting '
-                 'guest image.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <version_name> as default value.')
 
         parser = subparser.add_parser(
             'db_recreate', description='Drop the database and recreate it.')
         parser.add_argument('--repo_path', help=repo_path_help)
 
         parser = subparser.add_parser(
             'db_load_datastore_config_parameters',
             description='Loads configuration group parameter validation rules '
             'for a datastore version into the database.')
         parser.add_argument(
             'datastore',
             help='Name of the datastore.')
         parser.add_argument(
-            'datastore_version_name',
+            'datastore_version',
             help='Name of the datastore version.')
         parser.add_argument(
             'config_file_location',
             help='Fully qualified file path to the configuration group '
             'parameter validation rules.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
 
         parser = subparser.add_parser(
-            'db_remove_datastore_config_parameters',
-            description='Remove configuration group parameter validation '
-                        'rules for a datastore version from the database.')
-        parser.add_argument(
-            'datastore',
-            help='Name of the datastore.')
-        parser.add_argument(
-            'datastore_version_name',
-            help='Name of the datastore version.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
-
-        parser = subparser.add_parser(
-            'datastore_version_flavor_add',
-            help='Adds flavor association to a given datastore and datastore '
-                 'version.')
+            'datastore_version_flavor_add', help='Adds flavor association to '
+            'a given datastore and datastore version.')
         parser.add_argument('datastore_name', help='Name of the datastore.')
         parser.add_argument('datastore_version_name', help='Name of the '
                             'datastore version.')
         parser.add_argument('flavor_ids', help='Comma separated list of '
                             'flavor ids.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
 
         parser = subparser.add_parser(
-            'datastore_version_flavor_delete',
-            help='Deletes a flavor associated with a given datastore and '
-                 'datastore version.')
+            'datastore_version_flavor_delete', help='Deletes a flavor '
+            'associated with a given datastore and datastore version.')
         parser.add_argument('datastore_name', help='Name of the datastore.')
         parser.add_argument('datastore_version_name', help='Name of the '
                             'datastore version.')
         parser.add_argument('flavor_id', help='The flavor to be deleted for '
                             'a given datastore and datastore version.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
-
         parser = subparser.add_parser(
-            'datastore_version_volume_type_add',
-            help='Adds volume_type association to a given datastore and '
-                 'datastore version.')
+            'datastore_version_volume_type_add', help='Adds volume_type '
+            'association to a given datastore and datastore version.')
         parser.add_argument('datastore_name', help='Name of the datastore.')
         parser.add_argument('datastore_version_name', help='Name of the '
                             'datastore version.')
         parser.add_argument('volume_type_ids', help='Comma separated list of '
                             'volume_type ids.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
 
         parser = subparser.add_parser(
             'datastore_version_volume_type_delete',
             help='Deletes a volume_type '
-                 'associated with a given datastore and datastore version.')
+            'associated with a given datastore and datastore version.')
         parser.add_argument('datastore_name', help='Name of the datastore.')
         parser.add_argument('datastore_version_name', help='Name of the '
                             'datastore version.')
         parser.add_argument('volume_type_id', help='The volume_type to be '
                             'deleted for a given datastore and datastore '
                             'version.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
 
         parser = subparser.add_parser(
             'datastore_version_volume_type_list',
             help='Lists the volume_types '
-                 'associated with a given datastore and datastore version.')
+            'associated with a given datastore and datastore version.')
         parser.add_argument('datastore_name', help='Name of the datastore.')
         parser.add_argument('datastore_version_name', help='Name of the '
                             'datastore version.')
-        parser.add_argument(
-            '--version',
-            help='The version number of the datastore version, e.g. 5.7.30. '
-                 'If not specified, use <datastore_version_name> as default '
-                 'value.')
-
     cfg.custom_parser('action', actions)
     cfg.parse_args(sys.argv)
 
     try:
         logging.setup(CONF, None)
 
         Commands().execute()
```

### Comparing `trove-21.0.0.0rc2/trove/cmd/taskmanager.py` & `trove-8.0.1/trove/cmd/taskmanager.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,42 +8,45 @@
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+from oslo_config import cfg as openstack_cfg
 from oslo_service import service as openstack_service
 
 from trove.cmd.common import with_initialize
+from trove.taskmanager import api as task_api
+
+
+extra_opts = [openstack_cfg.StrOpt('taskmanager_manager')]
 
 
 def startup(conf, topic):
     from trove.common import notification
     from trove.common.rpc import service as rpc_service
     from trove.instance import models as inst_models
-    from trove.taskmanager import api as task_api
 
     notification.DBaaSAPINotification.register_notify_callback(
         inst_models.persist_instance_fault)
 
     if conf.enable_secure_rpc_messaging:
         key = conf.taskmanager_rpc_encr_key
     else:
         key = None
 
     server = rpc_service.RpcService(
         key=key, manager=conf.taskmanager_manager, topic=topic,
         rpc_api_version=task_api.API.API_LATEST_VERSION)
-    launcher = openstack_service.launch(conf, server,
-                                        restart_method='mutate')
+    launcher = openstack_service.launch(conf, server)
     launcher.wait()
 
 
-@with_initialize
+@with_initialize(extra_opts=extra_opts)
 def main(conf):
     startup(conf, conf.taskmanager_queue)
 
 
-@with_initialize
+@with_initialize(extra_opts=extra_opts)
 def mgmt_main(conf):
     startup(conf, "mgmt-taskmanager")
```

### Comparing `trove-21.0.0.0rc2/trove/common/api.py` & `trove-8.0.1/trove/common/api.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,40 +11,40 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import routes
 
 from trove.backup.service import BackupController
-from trove.backup.service import BackupStrategyController
 from trove.cluster.service import ClusterController
 from trove.common import wsgi
 from trove.configuration.service import ConfigurationsController
 from trove.configuration.service import ParametersController
 from trove.datastore.service import DatastoreController
 from trove.flavor.service import FlavorController
 from trove.instance.service import InstanceController
 from trove.limits.service import LimitsController
 from trove.module.service import ModuleController
 from trove.versions import VersionsController
+from trove.volume_type.service import VolumeTypesController
 
 
 class API(wsgi.Router):
     """Defines the API routes."""
     def __init__(self):
         mapper = routes.Mapper()
         super(API, self).__init__(mapper)
         self._instance_router(mapper)
         self._cluster_router(mapper)
         self._datastore_router(mapper)
         self._flavor_router(mapper)
+        self._volume_type_router(mapper)
         self._versions_router(mapper)
         self._limits_router(mapper)
         self._backups_router(mapper)
-        self._backup_strategy_router(mapper)
         self._configurations_router(mapper)
         self._modules_router(mapper)
 
     def _versions_router(self, mapper):
         versions_resource = VersionsController().create_resource()
         mapper.connect("/",
                        controller=versions_resource,
@@ -81,18 +81,14 @@
 
     def _instance_router(self, mapper):
         instance_resource = InstanceController().create_resource()
         mapper.connect("/{tenant_id}/instances",
                        controller=instance_resource,
                        action="index",
                        conditions={'method': ['GET']})
-        mapper.connect("/{tenant_id}/instances/detail",
-                       controller=instance_resource,
-                       action="detail",
-                       conditions={'method': ['GET']})
         mapper.connect("/{tenant_id}/instances",
                        controller=instance_resource,
                        action="create",
                        conditions={'method': ['POST']})
         mapper.connect("/{tenant_id}/instances/{id}",
                        controller=instance_resource,
                        action="show",
@@ -177,14 +173,25 @@
                        action="index",
                        conditions={'method': ['GET']})
         mapper.connect("/{tenant_id}/flavors/{id}",
                        controller=flavor_resource,
                        action="show",
                        conditions={'method': ['GET']})
 
+    def _volume_type_router(self, mapper):
+        volume_type_resource = VolumeTypesController().create_resource()
+        mapper.connect("/{tenant_id}/volume-types",
+                       controller=volume_type_resource,
+                       action="index",
+                       conditions={'method': ['GET']})
+        mapper.connect("/{tenant_id}/volume-types/{id}",
+                       controller=volume_type_resource,
+                       action="show",
+                       conditions={'method': ['GET']})
+
     def _limits_router(self, mapper):
         limits_resource = LimitsController().create_resource()
         mapper.connect("/{tenant_id}/limits",
                        controller=limits_resource,
                        action="index",
                        conditions={'method': ['GET']})
 
@@ -200,29 +207,18 @@
                        conditions={'method': ['POST']})
         mapper.connect("/{tenant_id}/backups/{id}",
                        controller=backups_resource,
                        action="show",
                        conditions={'method': ['GET']})
         mapper.connect("/{tenant_id}/backups/{id}",
                        controller=backups_resource,
-                       action="delete",
-                       conditions={'method': ['DELETE']})
-
-    def _backup_strategy_router(self, mapper):
-        backup_strategy_resource = BackupStrategyController().create_resource()
-        mapper.connect("/{tenant_id}/backup_strategies",
-                       controller=backup_strategy_resource,
-                       action="create",
+                       action="action",
                        conditions={'method': ['POST']})
-        mapper.connect("/{tenant_id}/backup_strategies",
-                       controller=backup_strategy_resource,
-                       action="index",
-                       conditions={'method': ['GET']})
-        mapper.connect("/{tenant_id}/backup_strategies",
-                       controller=backup_strategy_resource,
+        mapper.connect("/{tenant_id}/backups/{id}",
+                       controller=backups_resource,
                        action="delete",
                        conditions={'method': ['DELETE']})
 
     def _modules_router(self, mapper):
 
         modules_resource = ModuleController().create_resource()
         mapper.resource("modules", "/{tenant_id}/modules",
```

### Comparing `trove-21.0.0.0rc2/trove/common/apischema.py` & `trove-8.0.1/trove/common/apischema.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,16 +13,16 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 url_ref = {
     "type": "string",
     "minLength": 8,
-    "pattern": r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]'
-               r'|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
+    "pattern": 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]'
+               '|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
 }
 
 boolean_string = {
     "type": "integer",
     "minimum": 0,
     "maximum": 1
 }
@@ -46,15 +46,15 @@
     "pattern": "[0-9]+"
 }
 
 configuration_positive_integer = {
     "type": "string",
     "maxLength": 40,
     "minLength": 1,
-    "pattern": "^0*[1-9]+[0-9]*$"
+    "pattern": "^[0-9]+$"
 }
 
 configuration_non_empty_string = {
     "type": "string",
     "minLength": 1,
     "maxLength": 128,
     "pattern": "^.*[0-9a-zA-Z]+.*$"
@@ -68,32 +68,23 @@
         }]
 }
 
 volume_size = {
     "oneOf": [
         {
             "type": "integer",
-            "minimum": 1
-        },
-        configuration_positive_integer]
-}
-
-number_of_nodes = {
-    "oneOf": [
-        {
-            "type": "integer",
-            "minimum": 1
+            "minimum": 0
         },
         configuration_positive_integer]
 }
 
 host_string = {
     "type": "string",
     "minLength": 1,
-    "pattern": r"^[%]?[\w(-).]*[%]?$"
+    "pattern": "^[%]?[\w(-).]*[%]?$"
 }
 
 name_string = {
     "type": "string",
     "minLength": 1,
     "pattern": "^.*[0-9a-zA-Z]+.*$"
 }
@@ -102,47 +93,27 @@
     "type": "string",
     "minLength": 1,
     "maxLength": 64,
     "pattern": "^([0-9a-fA-F]){8}-([0-9a-fA-F]){4}-([0-9a-fA-F]){4}"
                "-([0-9a-fA-F]){4}-([0-9a-fA-F]){12}$"
 }
 
-ip_address_v4 = {
-    "type": "string",
-    "minLength": 7,
-    "maxLength": 15,
-    "pattern": r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$"
-}
-
 volume = {
     "type": "object",
     "required": ["size"],
     "properties": {
         "size": volume_size,
-        "type": {
-            "oneOf": [
-                non_empty_string,
-                {"type": "null"}
-            ]
-        }
+        "required": True
     }
 }
 
 nics = {
     "type": "array",
-    "maxItems": 1,
     "items": {
         "type": "object",
-        "additionalProperties": False,
-        "properties": {
-            "net-id": uuid,
-            "network_id": uuid,
-            "subnet_id": uuid,
-            "ip_address": ip_address_v4
-        }
     }
 }
 
 databases_ref_list = {
     "type": "array",
     "minItems": 0,
     "uniqueItems": True,
@@ -279,113 +250,99 @@
                                 "nics": nics,
                                 "availability_zone": non_empty_string,
                                 "modules": module_list,
                                 "region_name": non_empty_string
                             }
                         }
                     },
-                    "locality": non_empty_string,
-                    "extended_properties": {
-                        "type": "object",
-                        "additionalProperties": True,
-                        "properties": {
-                            "num_configsvr": number_of_nodes,
-                            "num_mongos": number_of_nodes,
-                            "configsvr_volume_size": volume_size,
-                            "configsvr_volume_type": non_empty_string,
-                            "mongos_volume_size": volume_size,
-                            "mongos_volume_type": non_empty_string
-                        }
-                    }
+                    "locality": non_empty_string
                 }
             }
         }
     },
-    "action": {
-        "add_shard": {
-            "type": "object",
-            "required": ["add_shard"],
-            "additionalProperties": True,
-            "properties": {
-                "add_shard": {
-                    "type": "object"
-                }
-            }
-        },
-        "grow": {
-            "type": "object",
-            "required": ["grow"],
-            "additionalProperties": True,
-            "properties": {
-                "grow": {
-                    "type": "array",
-                    "items": {
-                        "type": "object",
-                        "required": ["flavorRef"],
-                        "additionalProperties": True,
-                        "properties": {
-                            "name": non_empty_string,
-                            "flavorRef": flavorref,
-                            "volume": volume,
-                            "nics": nics,
-                            "availability_zone": non_empty_string,
-                            "modules": module_list,
-                            "related_to": non_empty_string,
-                            "type": non_empty_string,
-                            "region_name": non_empty_string
-                        }
-                    }
-                }
+    "add_shard": {
+        "type": "object",
+        "required": ["add_shard"],
+        "additionalProperties": True,
+        "properties": {
+            "add_shard": {
+                "type": "object"
             }
-        },
-        "shrink": {
-            "type": "object",
-            "required": ["shrink"],
-            "additionalProperties": True,
-            "properties": {
-                "shrink": {
-                    "type": "array",
-                    "items": {
-                        "type": "object",
-                        "required": ["id"],
-                        "additionalProperties": True,
-                        "properties": {
-                            "id": uuid
-                        }
+        }
+    },
+    "grow": {
+        "type": "object",
+        "required": ["grow"],
+        "additionalProperties": True,
+        "properties": {
+            "grow": {
+                "type": "array",
+                "items": {
+                    "type": "object",
+                    "required": ["flavorRef"],
+                    "additionalProperties": True,
+                    "properties": {
+                        "name": non_empty_string,
+                        "flavorRef": flavorref,
+                        "volume": volume,
+                        "nics": nics,
+                        "availability_zone": non_empty_string,
+                        "modules": module_list,
+                        "related_to": non_empty_string,
+                        "type": non_empty_string,
+                        "region_name": non_empty_string
                     }
                 }
             }
-        },
-        "upgrade": {
-            "type": "object",
-            "required": ["upgrade"],
-            "additionalProperties": True,
-            "properties": {
-                "upgrade": {
+        }
+    },
+    "shrink": {
+        "type": "object",
+        "required": ["shrink"],
+        "additionalProperties": True,
+        "properties": {
+            "shrink": {
+                "type": "array",
+                "items": {
                     "type": "object",
-                    "required": ["datastore_version"],
+                    "required": ["id"],
                     "additionalProperties": True,
                     "properties": {
-                        "datastore_version": non_empty_string
+                        "id": uuid
                     }
                 }
             }
         }
+    },
+    "upgrade": {
+        "type": "object",
+        "required": ["upgrade"],
+        "additionalProperties": True,
+        "properties": {
+            "upgrade": {
+                "type": "object",
+                "required": ["datastore_version"],
+                "additionalProperties": True,
+                "properties": {
+                    "datastore_version": non_empty_string
+                }
+            }
+        }
     }
 }
 
 instance = {
     "create": {
         "type": "object",
         "required": ["instance"],
         "additionalProperties": True,
         "properties": {
             "instance": {
                 "type": "object",
-                "required": ["name"],
+                "required": ["name", "flavorRef"],
                 "additionalProperties": True,
                 "properties": {
                     "name": non_empty_string,
                     "configuration_id": configuration_id,
                     "flavorRef": flavorref,
                     "volume": volume,
                     "databases": databases_def,
@@ -394,50 +351,27 @@
                         "type": "object",
                         "required": ["backupRef"],
                         "additionalProperties": True,
                         "properties": {
                             "backupRef": uuid
                         }
                     },
-                    "replica_of": uuid,
-                    "replica_count": {
-                        "type": "integer",
-                        "minimum": 1,
-                        "maximum": 3
-                    },
                     "availability_zone": non_empty_string,
                     "datastore": {
                         "type": "object",
                         "additionalProperties": True,
                         "properties": {
                             "type": non_empty_string,
-                            "version": non_empty_string,
-                            "version_number": non_empty_string
+                            "version": non_empty_string
                         }
                     },
                     "nics": nics,
                     "modules": module_list,
                     "region_name": non_empty_string,
-                    "locality": non_empty_string,
-                    "access": {
-                        "type": "object",
-                        "properties": {
-                            "is_public": {"type": "boolean"},
-                            "allowed_cidrs": {
-                                "type": "array",
-                                "uniqueItems": True,
-                                "items": {
-                                    "type": "string",
-                                    "pattern": "^([0-9]{1,3}\\.){3}[0-9]{1,3}"
-                                               "(\\/([0-9]|[1-2][0-9]|3[0-2]))"
-                                               "?$"
-                                }
-                            }
-                        }
-                    }
+                    "locality": non_empty_string
                 }
             }
         }
     },
     "edit": {
         "name": "instance:edit",
         "type": "object",
@@ -453,49 +387,14 @@
                     "name": non_empty_string,
                     "configuration": configuration_id,
                     "datastore_version": non_empty_string,
                 }
             }
         }
     },
-    "update": {
-        "name": "instance:update",
-        "type": "object",
-        "required": ["instance"],
-        "properties": {
-            "instance": {
-                "type": "object",
-                "required": [],
-                "additionalProperties": False,
-                "properties": {
-                    "name": non_empty_string,
-                    "replica_of": {},
-                    "configuration": configuration_id,
-                    "datastore_version": non_empty_string,
-                    "access": {
-                        "type": "object",
-                        "additionalProperties": False,
-                        "properties": {
-                            "is_public": {"type": "boolean"},
-                            "allowed_cidrs": {
-                                "type": "array",
-                                "uniqueItems": True,
-                                "items": {
-                                    "type": "string",
-                                    "pattern": "^([0-9]{1,3}\\.){3}[0-9]{1,3}"
-                                               "(\\/([0-9]|[1-2][0-9]|3[0-2]))"
-                                               "?$"
-                                }
-                            }
-                        }
-                    }
-                }
-            }
-        }
-    },
     "action": {
         "resize": {
             "volume": {
                 "type": "object",
                 "required": ["resize"],
                 "additionalProperties": True,
                 "properties": {
@@ -580,29 +479,14 @@
             "required": ["stop"],
             "additionalProperties": True,
             "properties": {
                 "stop": {
                     "type": "object"
                 }
             }
-        },
-        "rebuild": {
-            "type": "object",
-            "required": ["rebuild"],
-            "additionalProperties": True,
-            "properties": {
-                "rebuild": {
-                    "type": "object",
-                    "required": ["image_id"],
-                    "additionalProperties": False,
-                    "properties": {
-                        "image_id": uuid
-                    }
-                }
-            }
         }
     }
 }
 
 user = {
     "create": {
         "name": "users:create",
@@ -648,60 +532,27 @@
     "create": {
         "name": "backup:create",
         "type": "object",
         "required": ["backup"],
         "properties": {
             "backup": {
                 "type": "object",
-                "required": ["name"],
+                "required": ["instance", "name"],
                 "properties": {
                     "description": non_empty_string,
                     "instance": uuid,
                     "name": non_empty_string,
                     "parent_id": uuid,
-                    "incremental": boolean_string,
-                    "swift_container": non_empty_string,
-                    "restore_from": {
-                        "type": "object",
-                        "required": [
-                            "remote_location",
-                            "local_datastore_version_id",
-                            "size"
-                        ],
-                        "properties": {
-                            "remote_location": non_empty_string,
-                            "local_datastore_version_id": uuid,
-                            "size": {"type": "number"}
-                        }
-                    }
+                    "incremental": boolean_string
                 }
             }
         }
     }
 }
 
-backup_strategy = {
-    "create": {
-        "name": "backup_strategy:create",
-        "type": "object",
-        "required": ["backup_strategy"],
-        "properties": {
-            "backup_strategy": {
-                "type": "object",
-                "additionalProperties": False,
-                "required": ["swift_container"],
-                "properties": {
-                    "instance_id": uuid,
-                    "swift_container": non_empty_string
-                }
-            }
-        },
-    }
-}
-
 guest_log = {
     "action": {
         "name": "guest_log:action",
         "type": "object",
         "required": ["name"],
         "properties": {
             "name": non_empty_string,
@@ -830,16 +681,15 @@
                     },
                     "name": non_empty_string,
                     "datastore": {
                         "type": "object",
                         "additionalProperties": True,
                         "properties": {
                             "type": non_empty_string,
-                            "version": non_empty_string,
-                            "version_number": non_empty_string
+                            "version": non_empty_string
                         }
                     }
                 }
             }
         }
     },
     "update": {
@@ -958,61 +808,43 @@
         "type": "string",
         "minLength": 1,
         "maxLength": 255,
         "pattern": "^.*[0-9a-zA-Z]+.*$"
     }
 }
 
-image_tags = {
-    "type": "array",
-    "minItems": 0,
-    "maxItems": 5,
-    "uniqueItems": True,
-    "items": {
-        "type": "string",
-        "minLength": 1,
-        "maxLength": 20,
-        "pattern": "^.*[0-9a-zA-Z]+.*$"
-    }
-}
-
 mgmt_datastore_version = {
     "create": {
         "name": "mgmt_datastore_version:create",
         "type": "object",
         "required": ["version"],
         "properties": {
             "version": {
                 "type": "object",
-                "required": ["name", "datastore_name", "datastore_manager",
-                             "active"],
-                "additionalProperties": False,
+                "required": ["name", "datastore_name", "image", "active"],
+                "additionalProperties": True,
                 "properties": {
                     "name": non_empty_string,
                     "datastore_name": non_empty_string,
                     "datastore_manager": non_empty_string,
                     "packages": package_list,
                     "image": uuid,
-                    "image_tags": image_tags,
                     "active": {"enum": [True, False]},
-                    "default": {"enum": [True, False]},
-                    "version": non_empty_string
+                    "default": {"enum": [True, False]}
                 }
             }
         }
     },
     "edit": {
         "name": "mgmt_datastore_version:edit",
         "type": "object",
         "required": [],
-        "additionalProperties": False,
+        "additionalProperties": True,
         "properties": {
             "datastore_manager": non_empty_string,
             "packages": package_list,
             "image": uuid,
-            "image_tags": image_tags,
             "active": {"enum": [True, False]},
             "default": {"enum": [True, False]},
-            "name": non_empty_string
         }
     }
 }
```

### Comparing `trove-21.0.0.0rc2/trove/common/auth.py` & `trove-8.0.1/trove/common/auth.py`

 * *Files 14% similar despite different names*

```diff
@@ -17,15 +17,14 @@
 
 from oslo_log import log as logging
 from oslo_utils import strutils
 import webob.exc
 
 from trove.common import exception
 from trove.common.i18n import _
-from trove.common.utils import req_to_text
 from trove.common import wsgi
 
 LOG = logging.getLogger(__name__)
 
 
 class AuthorizationMiddleware(wsgi.Middleware):
 
@@ -59,24 +58,24 @@
     tenant_scoped_url = re.compile("/(?P<tenant_id>.*?)/.*")
 
     def authorize(self, request, tenant_id, roles):
         match_for_tenant = self.tenant_scoped_url.match(request.path_info)
         if (match_for_tenant and
                 tenant_id == match_for_tenant.group('tenant_id')):
             LOG.debug(strutils.mask_password(
-                ("Authorized tenant '%(tenant_id)s' request: "
-                 "%(request)s") %
-                {'tenant_id': tenant_id, 'request': req_to_text(request)}))
+                      _("Authorized tenant '%(tenant_id)s' request: "
+                        "%(request)s") %
+                      {'tenant_id': tenant_id, 'request': request}))
             return True
 
-        log_fmt = "User with tenant id %s cannot access this resource."
-        exc_fmt = _("User with tenant id %s cannot access this resource.")
+        msg = _(
+            "User with tenant id %s cannot access this resource.") % tenant_id
 
-        LOG.error(log_fmt, tenant_id)
-        raise webob.exc.HTTPForbidden(exc_fmt % tenant_id)
+        LOG.error(msg)
+        raise webob.exc.HTTPForbidden(msg)
 
 
 def admin_context(f):
     """
     Verify that the current context has administrative access,
     or throw an exception. Trove API functions typically take the form
     function(self, req), or function(self, req, id).
```

### Comparing `trove-21.0.0.0rc2/trove/common/base_exception.py` & `trove-8.0.1/trove/common/base_exception.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,14 +15,16 @@
 
 """
 Exceptions common to OpenStack projects
 """
 
 from oslo_log import log as logging
 
+from trove.common.i18n import _
+
 _FATAL_EXCEPTION_FORMAT_ERRORS = False
 
 LOG = logging.getLogger(__name__)
 
 
 class Error(Exception):
     def __init__(self, message=None):
@@ -94,15 +96,15 @@
 
 def wrap_exception(f):
     def _wrap(*args, **kw):
         try:
             return f(*args, **kw)
         except Exception as e:
             if not isinstance(e, Error):
-                LOG.exception('Uncaught exception')
+                LOG.exception(_('Uncaught exception'))
                 raise Error(str(e))
             raise
     _wrap.func_name = f.func_name
     return _wrap
 
 
 class OpenstackException(Exception):
```

### Comparing `trove-21.0.0.0rc2/trove/common/base_wsgi.py` & `trove-8.0.1/trove/common/base_wsgi.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,14 +11,16 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 """Utility methods for working with WSGI servers."""
 
+from __future__ import print_function
+
 import eventlet
 eventlet.patcher.monkey_patch(all=False, socket=True)
 
 import datetime
 import errno
 import socket
 import sys
@@ -35,15 +37,14 @@
 import webob.dec
 import webob.exc
 from xml.dom import minidom
 from xml.parsers import expat
 
 from trove.common import base_exception
 from trove.common.i18n import _
-from trove.common.utils import req_to_text
 from trove.common import xmlutils
 
 socket_opts = [
     cfg.IntOpt('backlog',
                default=4096,
                help="Number of backlog requests to configure the socket with"),
     cfg.IntOpt('tcp_keepidle',
@@ -327,16 +328,14 @@
         allowed_content_types = (allowed_content_types or
                                  self.default_request_content_types)
 
         if content_type not in allowed_content_types:
             raise base_exception.InvalidContentType(content_type=content_type)
         return content_type
 
-    __str__ = req_to_text
-
 
 class Resource(object):
     """
     WSGI app that handles (de)serialization and controller dispatch.
 
     Reads routing information supplied by RoutesMiddleware and calls
     the requested action method upon its deserializer, controller,
@@ -449,15 +448,16 @@
 
     def default(self, data):
         def sanitizer(obj):
             if isinstance(obj, datetime.datetime):
                 _dtime = obj - datetime.timedelta(microseconds=obj.microsecond)
                 return _dtime.isoformat()
             return obj
-        return jsonutils.dump_as_bytes(data, default=sanitizer)
+#            return six.text_type(obj)
+        return jsonutils.dumps(data, default=sanitizer)
 
 
 class XMLDictSerializer(DictSerializer):
 
     def __init__(self, metadata=None, xmlns=None):
         """
         :param metadata: information needed to deserialize xml into
```

### Comparing `trove-21.0.0.0rc2/trove/common/cache.py` & `trove-8.0.1/trove/guestagent/backup/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,57 +1,45 @@
-# Copyright 2021 Catalyst Cloud
+# Copyright 2013 OpenStack Foundation
+# Copyright 2013 Rackspace Hosting
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+
+from trove.guestagent.backup.backupagent import BackupAgent
+
+AGENT = BackupAgent()
 
-"""The code related to integration between oslo.cache module and trove."""
 
-from oslo_cache import core
-from oslo_config import cfg
+def backup(context, backup_info):
+    """
+    Main entry point for starting a backup based on the given backup id.  This
+    will create a backup for this DB instance and will then store the backup
+    in a configured repository (e.g. Swift)
 
+    :param context:     the context token which contains the users details
+    :param backup_id:   the id of the persisted backup object
+    """
+    return AGENT.execute_backup(context, backup_info)
 
-def register_cache_configurations(conf):
-    """Register all configurations required for oslo.cache.
 
-    The procedure registers all configurations required for oslo.cache.
-    It should be called before configuring of cache region
+def restore(context, backup_info, restore_location):
     """
-    core.configure(conf)
+    Main entry point for restoring a backup based on the given backup id.  This
+    will transfer backup data to this instance an will carry out the
+    appropriate restore procedure (eg. mysqldump)
 
-    ports_cache_group = cfg.OptGroup('instance_ports_cache')
-    ports_cache_opts = [
-        cfg.IntOpt('expiration_time', default=86400,
-                   help='TTL, in seconds, for any cached item in the '
-                        'dogpile.cache region used for caching of the '
-                        'instance ports.'),
-        cfg.BoolOpt("caching", default=True,
-                    help='Toggle to enable/disable caching when getting trove '
-                         'instance ports. Please note that the global toggle '
-                         'for oslo.cache(enabled=True in [cache] group) '
-                         'must be enabled to use this feature.')
-    ]
-    conf.register_group(ports_cache_group)
-    conf.register_opts(ports_cache_opts, group=ports_cache_group)
-
-    return conf
-
-
-# variable that stores an initialized cache region for trove
-_REGION = None
-
-
-def get_cache_region():
-    global _REGION
-    if not _REGION:
-        _REGION = core.configure_cache_region(
-            conf=register_cache_configurations(cfg.CONF),
-            region=core.create_region())
-    return _REGION
+    :param context:     the context token which contains the users details
+    :param backup_id:   the id of the persisted backup object
+    """
+    return AGENT.execute_restore(context, backup_info, restore_location)
```

### Comparing `trove-21.0.0.0rc2/trove/common/cfg.py` & `trove-8.0.1/trove/common/cfg.py`

 * *Files 18% similar despite different names*

```diff
@@ -13,22 +13,18 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 """Routines for configuring Trove."""
 
 import os.path
 
-from keystoneauth1 import loading
-from keystonemiddleware import auth_token
 from oslo_config import cfg
 from oslo_config.cfg import NoSuchOptError
 from oslo_config import types
-from oslo_db import options as db_options
 from oslo_log import log as logging
-from oslo_log import versionutils
 from oslo_middleware import cors
 from osprofiler import opts as profiler
 
 from trove.common.i18n import _
 from trove.version import version_info as version
 
 ListOfPortsType = types.Range(1, 65535)
@@ -44,86 +40,62 @@
 path_opts = [
     cfg.StrOpt('pybasedir',
                default=os.path.abspath(os.path.join(os.path.dirname(__file__),
                                                     '../')),
                help='Directory where the Trove python module is installed.'),
 ]
 
-versions_opts = [
-    cfg.StrOpt('public_endpoint', default=None,
-               help='Public URL to use for versions endpoint. The default '
-                    'is None, which will use the request\'s host_url '
-                    'attribute to populate the URL base. If Trove is '
-                    'operating behind a proxy, you will want to change '
-                    'this to represent the proxy\'s URL.')
-]
-
 common_opts = [
     cfg.IPOpt('bind_host', default='0.0.0.0',
               help='IP address the API server will listen on.'),
     cfg.PortOpt('bind_port', default=8779,
                 help='Port the API server will listen on.'),
     cfg.StrOpt('api_paste_config', default="api-paste.ini",
                help='File name for the paste.deploy config for trove-api.'),
-    cfg.BoolOpt('trove_volume_support', default=False,
+    cfg.BoolOpt('trove_volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
-    cfg.BoolOpt('volume_rootdisk_support', default=False,
-                help='Whether to provision a Cinder volume for rootdisk.'),
-    cfg.IntOpt('volume_rootdisk_size', default=10,
-               help='Size of volume rootdisk for Database instance'),
     cfg.ListOpt('admin_roles', default=['admin'],
                 help='Roles to add to an admin user.'),
     cfg.BoolOpt('update_status_on_fail', default=True,
                 help='Set the service and instance task statuses to ERROR '
                      'when an instance fails to become active within the '
                      'configured usage_timeout.'),
+    cfg.StrOpt('os_region_name', default='RegionOne',
+               help='Region name of this node. Used when searching catalog.'),
     cfg.URIOpt('nova_compute_url', help='URL without the tenant segment.'),
     cfg.StrOpt('nova_compute_service_type', default='compute',
                help='Service type to use when searching catalog.'),
     cfg.StrOpt('nova_compute_endpoint_type', default='publicURL',
                help='Service endpoint type to use when searching catalog.'),
     cfg.StrOpt('nova_client_version', default='2.12',
                help="The version of the compute service client."),
     cfg.StrOpt('glance_client_version', default='2',
                help="The version of the image service client."),
-    cfg.BoolOpt('nova_api_insecure', default=False,
-                help="Allow to perform insecure SSL requests to nova."),
-    cfg.StrOpt('nova_keypair', default=None,
-               help="Name of a Nova keypair to inject into a database "
-                    "instance to enable SSH access. The keypair should be "
-                    "prior created by the cloud operator."),
     cfg.URIOpt('neutron_url', help='URL without the tenant segment.'),
     cfg.StrOpt('neutron_service_type', default='network',
                help='Service type to use when searching catalog.'),
     cfg.StrOpt('neutron_endpoint_type', default='publicURL',
                help='Service endpoint type to use when searching catalog.'),
-    cfg.BoolOpt('neutron_api_insecure', default=False,
-                help="Allow to perform insecure SSL requests to neutron."),
     cfg.URIOpt('cinder_url', help='URL without the tenant segment.'),
-    cfg.StrOpt('cinder_service_type', default='volumev3',
+    cfg.StrOpt('cinder_service_type', default='volumev2',
                help='Service type to use when searching catalog.'),
     cfg.StrOpt('cinder_endpoint_type', default='publicURL',
                help='Service endpoint type to use when searching catalog.'),
-    cfg.BoolOpt('cinder_api_insecure', default=False,
-                help="Allow to perform insecure SSL requests to cinder."),
     cfg.URIOpt('swift_url', help='URL ending in ``AUTH_``.'),
     cfg.StrOpt('swift_service_type', default='object-store',
                help='Service type to use when searching catalog.'),
     cfg.StrOpt('swift_endpoint_type', default='publicURL',
                help='Service endpoint type to use when searching catalog.'),
-    cfg.BoolOpt('swift_api_insecure', default=False,
-                help="Allow to perform insecure SSL requests to swift."),
     cfg.URIOpt('glance_url', help='URL ending in ``AUTH_``.'),
     cfg.StrOpt('glance_service_type', default='image',
                help='Service type to use when searching catalog.'),
     cfg.StrOpt('glance_endpoint_type', default='publicURL',
                help='Service endpoint type to use when searching catalog.'),
-    cfg.StrOpt('taskmanager_manager',
-               default='trove.taskmanager.manager.Manager',
-               help='Driver for taskmanager'),
+    cfg.URIOpt('trove_auth_url', default='http://0.0.0.0/identity/v2.0',
+               help='Trove authentication URL.'),
     cfg.StrOpt('trove_url', help='URL without the tenant segment.'),
     cfg.StrOpt('trove_service_type', default='database',
                help='Service type to use when searching catalog.'),
     cfg.StrOpt('trove_endpoint_type', default='publicURL',
                help='Service endpoint type to use when searching catalog.'),
     cfg.IPOpt('host', default='0.0.0.0',
               help='Host to listen for RPC messages.'),
@@ -147,18 +119,14 @@
                help='Endpoint URL for DNSaaS.'),
     cfg.StrOpt('dns_service_type', default="",
                help='Service Type for DNSaaS.'),
     cfg.StrOpt('dns_region', default="",
                help='Region name for DNSaaS.'),
     cfg.URIOpt('dns_auth_url', default="http://0.0.0.0",
                help='Authentication URL for DNSaaS.'),
-    cfg.StrOpt('dns_user_domain_id', default="default",
-               help='Keystone user domain ID used for auth'),
-    cfg.StrOpt('dns_project_domain_id', default="default",
-               help='Keystone project domain ID used for auth'),
     cfg.StrOpt('dns_domain_name', default="",
                help='Domain name used for adding DNS entries.'),
     cfg.StrOpt('dns_username', default="", secret=True,
                help='Username for DNSaaS.'),
     cfg.StrOpt('dns_passkey', default="", secret=True,
                help='Passkey for DNSaaS.'),
     cfg.URIOpt('dns_management_base_url', default="http://0.0.0.0",
@@ -179,65 +147,51 @@
     cfg.IntOpt('backups_page_size', default=20,
                help='Page size for listing backups.'),
     cfg.IntOpt('configurations_page_size', default=20,
                help='Page size for listing configurations.'),
     cfg.IntOpt('modules_page_size', default=20,
                help='Page size for listing modules.'),
     cfg.IntOpt('agent_call_low_timeout', default=15,
-               help="Maximum time (in seconds) to wait for Guest Agent "
-                    "'quick' requests (such as retrieving a list of "
-                    "users or databases)."),
-    cfg.IntOpt('agent_call_high_timeout', default=60 * 3,
+               help="Maximum time (in seconds) to wait for Guest Agent 'quick'"
+                    "requests (such as retrieving a list of users or "
+                    "databases)."),
+    cfg.IntOpt('agent_call_high_timeout', default=60 * 10,
                help="Maximum time (in seconds) to wait for Guest Agent 'slow' "
                     "requests (such as restarting the database)."),
-    cfg.IntOpt('agent_replication_snapshot_timeout', default=60 * 30,
+    cfg.IntOpt('agent_replication_snapshot_timeout', default=36000,
                help='Maximum time (in seconds) to wait for taking a Guest '
                     'Agent replication snapshot.'),
-    cfg.IntOpt('command_process_timeout', default=30,
-               help='Maximum time (in seconds) to wait for out of process '
-                    'commands to complete.'),
     # The guest_id opt definition must match the one in cmd/guest.py
     cfg.StrOpt('guest_id', default=None, help="ID of the Guest Instance."),
-    cfg.StrOpt('controller_address',
-               help='The address used to download Trove code by guest agent '
-                    'in developer mode. This address is inserted into the '
-                    'file /etc/trove/controller.conf inside the guest.'),
-    cfg.IntOpt('state_change_wait_time', default=180,
-               help='Maximum time (in seconds) to wait for database state '
-                    'change.'),
+    cfg.IntOpt('state_change_wait_time', default=60 * 10,
+               help='Maximum time (in seconds) to wait for a state change.'),
     cfg.IntOpt('state_change_poll_time', default=3,
                help='Interval between state change poll requests (seconds).'),
-    cfg.IntOpt('state_healthy_counts', default=5,
-               help='consecutive success db connections for status HEALTHY'),
     cfg.IntOpt('agent_heartbeat_time', default=10,
                help='Maximum time (in seconds) for the Guest Agent to reply '
                     'to a heartbeat request.'),
-    cfg.IntOpt('agent_heartbeat_expiry', default=90,
+    cfg.IntOpt('agent_heartbeat_expiry', default=60,
                help='Time (in seconds) after which a guest is considered '
                     'unreachable'),
     cfg.IntOpt('num_tries', default=3,
                help='Number of times to check if a volume exists.'),
     cfg.StrOpt('volume_fstype', default='ext3',
-               choices=['ext3', 'ext4', 'xfs'],
                help='File system type used to format a volume.'),
     cfg.StrOpt('cinder_volume_type', default=None,
                help='Volume type to use when provisioning a Cinder volume.'),
     cfg.StrOpt('format_options', default='-m 5',
                help='Options to use when formatting a volume.'),
     cfg.IntOpt('volume_format_timeout', default=120,
                help='Maximum time (in seconds) to wait for a volume format.'),
     cfg.StrOpt('mount_options', default='defaults,noatime',
                help='Options to use when mounting a volume.'),
     cfg.IntOpt('max_instances_per_tenant',
                default=10,
                help='Default maximum number of instances per tenant.',
                deprecated_name='max_instances_per_user'),
-    cfg.IntOpt('max_ram_per_tenant',
-               default=-1,
-               help='Default maximum total amount of RAM in MB per tenant.'),
     cfg.IntOpt('max_accepted_volume_size', default=10,
                help='Default maximum volume size (in GB) for an instance.'),
     cfg.IntOpt('max_volumes_per_tenant', default=40,
                help='Default maximum volume capacity (in GB) spanning across '
                     'all Trove volumes per tenant.',
                deprecated_name='max_volumes_per_user'),
     cfg.IntOpt('max_backups_per_tenant', default=50,
@@ -248,29 +202,29 @@
     cfg.StrOpt('taskmanager_queue', default='taskmanager',
                help='Message queue name the Taskmanager will listen to.'),
     cfg.StrOpt('conductor_queue', default='trove-conductor',
                help='Message queue name the Conductor will listen on.'),
     cfg.IntOpt('trove_conductor_workers',
                help='Number of workers for the Conductor service. The default '
                'will be the number of CPUs available.'),
-    cfg.BoolOpt('use_nova_server_config_drive', default=False,
+    cfg.BoolOpt('use_nova_server_config_drive', default=True,
                 help='Use config drive for file injection when booting '
                 'instance.'),
+    cfg.BoolOpt('use_nova_server_volume', default=False,
+                help='Whether to provision a Cinder volume for the '
+                     'Nova instance.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
     cfg.StrOpt('default_datastore', default=None,
                help='The default datastore id or name to use if one is not '
                'provided by the user. If the default value is None, the field '
                'becomes required in the instance create request.'),
     cfg.StrOpt('datastore_manager', default=None,
                help='Manager class in the Guest Agent, set up by the '
-                    'Taskmanager on instance provision.'),
-    cfg.StrOpt('datastore_version', default=None,
-               help='The guest datastore version that is set by the '
-                    'Taskmanager during instance provision.'),
+               'Taskmanager on instance provision.'),
     cfg.StrOpt('block_device_mapping', default='vdb',
                help='Block device to map onto the created instance.'),
     cfg.IntOpt('server_delete_time_out', default=60,
                help='Maximum time (in seconds) to wait for a server delete.'),
     cfg.IntOpt('volume_time_out', default=60,
                help='Maximum time (in seconds) to wait for a volume attach.'),
     cfg.IntOpt('reboot_time_out', default=60 * 2,
@@ -300,111 +254,81 @@
                help="Maximum number of management HTTP 'POST' requests "
                     "(per minute)."),
     cfg.BoolOpt('hostname_require_valid_ip', default=True,
                 help='Require user hostnames to be valid IP addresses.',
                 deprecated_name='hostname_require_ipv4'),
     cfg.BoolOpt('trove_security_groups_support', default=True,
                 help='Whether Trove should add Security Groups on create.'),
-    cfg.StrOpt('trove_security_group_name_prefix', default='trove_sg',
+    cfg.StrOpt('trove_security_group_name_prefix', default='SecGroup',
                help='Prefix to use when creating Security Groups.'),
     cfg.StrOpt('trove_security_group_rule_cidr', default='0.0.0.0/0',
                help='CIDR to use when creating Security Group Rules.'),
     cfg.IntOpt('trove_api_workers',
                help='Number of workers for the API service. The default will '
                'be the number of CPUs available.'),
     cfg.IntOpt('usage_sleep_time', default=5,
                help='Time to sleep during the check for an active Guest.'),
     cfg.StrOpt('region', default='LOCAL_DEV',
                help='The region this service is located.'),
     cfg.StrOpt('backup_runner',
                default='trove.guestagent.backup.backup_types.InnoBackupEx',
-               help='Runner to use for backups.',
-               deprecated_for_removal=True),
+               help='Runner to use for backups.'),
     cfg.DictOpt('backup_runner_options', default={},
-                help='Additional options to be passed to the backup runner.',
-                deprecated_for_removal=True),
+                help='Additional options to be passed to the backup runner.'),
     cfg.BoolOpt('verify_swift_checksum_on_restore', default=True,
                 help='Enable verification of Swift checksum before starting '
                 'restore. Makes sure the checksum of original backup matches '
                 'the checksum of the Swift backup file.'),
-    cfg.BoolOpt('verify_replica_volume_size', default=True,
-                help='Require the replica volume size to be greater than '
-                'or equal to the size of the master volume '
-                'during replica creation.'),
-    cfg.StrOpt('storage_strategy', default='swift',
+    cfg.StrOpt('storage_strategy', default='SwiftStorage',
                help="Default strategy to store backups."),
     cfg.StrOpt('storage_namespace',
                default='trove.common.strategies.storage.swift',
-               help='Namespace to load the default storage strategy from.',
-               deprecated_for_removal=True),
+               help='Namespace to load the default storage strategy from.'),
     cfg.StrOpt('backup_swift_container', default='database_backups',
                help='Swift container to put backups in.'),
     cfg.BoolOpt('backup_use_gzip_compression', default=True,
-                help='Compress backups using gzip.',
-                deprecated_for_removal=True,
-                deprecated_since=versionutils.deprecated.VICTORIA,
-                deprecated_reason='Backup data compression is enabled by '
-                                  'default. This option is ignored.'),
-    cfg.BoolOpt(
-        'backup_use_openssl_encryption', default=True,
-        help='Encrypt backups using OpenSSL.',
-        deprecated_for_removal=True,
-        deprecated_since=versionutils.deprecated.VICTORIA,
-        deprecated_reason='Trove should not encrypt backup data on '
-                          'behalf of the user. This option is ignored.'
-    ),
-    cfg.StrOpt(
-        'backup_aes_cbc_key', default='', secret=True,
-        help='Default OpenSSL aes_cbc key for decrypting backup data created '
-             'prior to Victoria.',
-        deprecated_for_removal=True,
-        deprecated_since=versionutils.deprecated.VICTORIA,
-        deprecated_reason='This option is only for backward compatibility. '
-                          'Backups created after Victoria are not encrypted '
-                          'any more.'
-    ),
-    cfg.BoolOpt(
-        'backup_use_snet', default=False,
-        help='Send backup files over snet.',
-        deprecated_for_removal=True,
-        deprecated_since=versionutils.deprecated.VICTORIA,
-        deprecated_reason='This option is not supported any more.'
-    ),
+                help='Compress backups using gzip.'),
+    cfg.BoolOpt('backup_use_openssl_encryption', default=True,
+                help='Encrypt backups using OpenSSL.'),
+    cfg.StrOpt('backup_aes_cbc_key', default='default_aes_cbc_key',
+               help='Default OpenSSL aes_cbc key.'),
+    cfg.BoolOpt('backup_use_snet', default=False,
+                help='Send backup files over snet.'),
     cfg.IntOpt('backup_chunk_size', default=2 ** 16,
                help='Chunk size (in bytes) to stream to the Swift container. '
                'This should be in multiples of 128 bytes, since this is the '
                'size of an md5 digest block allowing the process to update '
                'the file checksum during streaming. '
                'See: http://stackoverflow.com/questions/1131220/'),
     cfg.IntOpt('backup_segment_max_size', default=2 * (1024 ** 3),
                help='Maximum size (in bytes) of each segment of the backup '
                'file.'),
     cfg.StrOpt('remote_dns_client',
-               default='trove.common.clients.dns_client',
+               default='trove.common.remote.dns_client',
                help='Client to send DNS calls to.'),
     cfg.StrOpt('remote_guest_client',
-               default='trove.common.clients.guest_client',
+               default='trove.common.remote.guest_client',
                help='Client to send Guest Agent calls to.'),
     cfg.StrOpt('remote_nova_client',
-               default='trove.common.clients_admin.nova_client_trove_admin',
+               default='trove.common.remote.nova_client',
                help='Client to send Nova calls to.'),
     cfg.StrOpt('remote_neutron_client',
-               default='trove.common.clients_admin.neutron_client_trove_admin',
+               default='trove.common.remote.neutron_client',
                help='Client to send Neutron calls to.'),
     cfg.StrOpt('remote_cinder_client',
-               default='trove.common.clients_admin.cinder_client_trove_admin',
+               default='trove.common.remote.cinder_client',
                help='Client to send Cinder calls to.'),
     cfg.StrOpt('remote_swift_client',
-               default='trove.common.clients.swift_client',
+               default='trove.common.remote.swift_client',
                help='Client to send Swift calls to.'),
     cfg.StrOpt('remote_trove_client',
                default='trove.common.trove_remote.trove_client',
                help='Client to send Trove calls to.'),
     cfg.StrOpt('remote_glance_client',
-               default='trove.common.clients_admin.glance_client_trove_admin',
+               default='trove.common.glance_remote.glance_client',
                help='Client to send Glance calls to.'),
     cfg.StrOpt('exists_notification_transformer',
                help='Transformer for exists notifications.'),
     cfg.IntOpt('exists_notification_interval', default=3600,
                help='Seconds to wait between pushing events.'),
     cfg.IntOpt('quota_notification_interval',
                help='Seconds to wait between pushing events.'),
@@ -418,87 +342,82 @@
                          'mongodb': 'c8c907af-7375-456f-b929-b637ff9209ee',
                          'postgresql': 'ac277e0d-4f21-40aa-b347-1ea31e571720',
                          'couchdb': 'f0a9ab7b-66f7-4352-93d7-071521d44c7c',
                          'vertica': 'a8d805ae-a3b2-c4fd-gb23-b62cee5201ae',
                          'db2': 'e040cd37-263d-4869-aaa6-c62aa97523b5',
                          'mariadb': '7a4f82cc-10d2-4bc6-aadc-d9aacc2a3cb5'},
                 help='Unique ID to tag notification events.'),
+    cfg.StrOpt('nova_proxy_admin_user', default='',
+               help="Admin username used to connect to Nova.", secret=True),
+    cfg.StrOpt('nova_proxy_admin_pass', default='',
+               help="Admin password used to connect to Nova.", secret=True),
+    cfg.StrOpt('nova_proxy_admin_tenant_id', default='',
+               help="Admin tenant ID used to connect to Nova.", secret=True),
+    cfg.StrOpt('nova_proxy_admin_tenant_name', default='',
+               help="Admin tenant name used to connect to Nova.", secret=True),
     cfg.StrOpt('network_label_regex', default='^private$',
-               help='Regular expression to match Trove network labels.',
-               deprecated_for_removal=True),
+               help='Regular expression to match Trove network labels.'),
     cfg.StrOpt('ip_regex', default=None,
                help='List IP addresses that match this regular expression.'),
     cfg.StrOpt('black_list_regex', default=None,
                help='Exclude IP addresses that match this regular '
                     'expression.'),
     cfg.StrOpt('cloudinit_location', default='/etc/trove/cloudinit',
                help='Path to folder with cloudinit scripts.'),
     cfg.StrOpt('injected_config_location', default='/etc/trove/conf.d',
                help='Path to folder on the Guest where config files will be '
                     'injected during instance creation.'),
-    cfg.StrOpt('injected_config_owner', default='ubuntu',
-               help='Owner of the Guest Agent directory and files to be '
-                    'injected during instance creation.'),
-    cfg.StrOpt('injected_config_group', default='root',
-               help='Group of the Guest Agent directory and files to be '
-                    'injected during instance creation.'),
     cfg.StrOpt('guest_config',
                default='/etc/trove/trove-guestagent.conf',
                help='Path to the Guest Agent config file to be injected '
                     'during instance creation.'),
     cfg.StrOpt('guest_info',
                default='guest_info.conf',
                help='The guest info filename found in the injected config '
                     'location.  If a full path is specified then it will '
                     'be used as the path to the guest info file'),
     cfg.DictOpt('datastore_registry_ext', default=dict(),
                 help='Extension for default datastore managers. '
                      'Allows the use of custom managers for each of '
                      'the datastores supported by Trove.'),
-    cfg.StrOpt('docker_bridge_network_ip', default=None,
-               help='Docker bridge network IP.'),
-    cfg.ListOpt('docker_insecure_registries', default=[],
-                help='Docker insecure registries for image development.'),
     cfg.StrOpt('template_path', default='/etc/trove/templates/',
                help='Path which leads to datastore templates.'),
     cfg.BoolOpt('sql_query_logging', default=False,
                 help='Allow insecure logging while '
                      'executing queries through SQLAlchemy.'),
     cfg.ListOpt('expected_filetype_suffixes', default=['json'],
                 help='Filetype endings not to be reattached to an ID '
                      'by the utils method correct_id_with_req.'),
-    cfg.ListOpt('management_networks', default=[],
-                deprecated_name='default_neutron_networks',
+    cfg.ListOpt('default_neutron_networks', default=[],
                 help='List of IDs for management networks which should be '
                      'attached to the instance regardless of what NICs '
-                     'are specified in the create API call. Currently only '
-                     'one management network is allowed.'),
-    cfg.ListOpt('management_security_groups', default=[],
-                help='List of the security group IDs that are applied on the '
-                     'management port of the database instance.'),
+                     'are specified in the create API call.'),
     cfg.IntOpt('max_header_line', default=16384,
                help='Maximum line size of message headers to be accepted. '
                     'max_header_line may need to be increased when using '
                     'large tokens (typically those generated by the '
                     'Keystone v3 API with big service catalogs).'),
     cfg.StrOpt('conductor_manager', default='trove.conductor.manager.Manager',
                help='Qualified class name to use for conductor manager.'),
     cfg.StrOpt('network_driver', default='trove.network.nova.NovaNetwork',
                help="Describes the actual network manager used for "
                     "the management of network attributes "
                     "(security groups, floating IPs, etc.)."),
     cfg.IntOpt('usage_timeout', default=60 * 30,
                help='Maximum time (in seconds) to wait for a Guest to become '
                     'active.'),
-    cfg.IntOpt('restore_usage_timeout', default=60 * 60,
+    cfg.IntOpt('restore_usage_timeout', default=36000,
                help='Maximum time (in seconds) to wait for a Guest instance '
                     'restored from a backup to become active.'),
     cfg.IntOpt('cluster_usage_timeout', default=36000,
                help='Maximum time (in seconds) to wait for a cluster to '
                     'become active.'),
+    cfg.IntOpt('timeout_wait_for_service', default=120,
+               help='Maximum time (in seconds) to wait for a service to '
+                    'become alive.'),
     cfg.StrOpt('module_aes_cbc_key', default='module_aes_cbc_key',
                help='OpenSSL aes_cbc key for module encryption.'),
     cfg.ListOpt('module_types', default=['ping', 'new_relic_license'],
                 help='A list of module types supported. A module type '
                      'corresponds to the name of a ModuleDriver.'),
     cfg.IntOpt('module_reapply_max_batch_size', default=50,
                help='The maximum number of instances to reapply a module to '
@@ -519,45 +438,92 @@
                default='bzH6y0SGmjuoY0FNSTptrhgieGXNDX6PIhvz',
                help='Key (OpenSSL aes_cbc) for taskmanager RPC encryption.'),
     cfg.StrOpt('inst_rpc_key_encr_key',
                default='emYjgHFqfXNB1NGehAFIUeoyw4V4XwWHEaKP',
                help='Key (OpenSSL aes_cbc) to encrypt instance keys in DB.'),
     cfg.StrOpt('instance_rpc_encr_key',
                help='Key (OpenSSL aes_cbc) for instance RPC encryption.'),
-    cfg.StrOpt('database_service_uid', default='1001',
-               help='The UID(GID) of database service user.'),
-    cfg.ListOpt('reserved_network_cidrs', default=[],
-                help='Network CIDRs reserved for Trove guest instance '
-                     'management.'),
-    cfg.BoolOpt(
-        'online_volume_resize', default=True,
-        help='If online volume resize is supported.'),
-    cfg.BoolOpt(
-        'enable_volume_az', default=False,
-        help='If true create the volume in the same availability-zone as the '
-             'instance'),
 ]
 
+
+database_opts = [
+    cfg.StrOpt('connection',
+               default='sqlite:///trove_test.sqlite',
+               help='SQL Connection.',
+               secret=True,
+               deprecated_name='sql_connection',
+               deprecated_group='DEFAULT'),
+    cfg.IntOpt('idle_timeout',
+               default=3600,
+               deprecated_name='sql_idle_timeout',
+               deprecated_group='DEFAULT'),
+    cfg.BoolOpt('query_log',
+                default=False,
+                deprecated_name='sql_query_log',
+                deprecated_group='DEFAULT',
+                deprecated_for_removal=True),
+    cfg.BoolOpt('sqlite_synchronous',
+                default=True,
+                help='If True, SQLite uses synchronous mode.'),
+    cfg.StrOpt('slave_connection',
+               secret=True,
+               help='The SQLAlchemy connection string to use to connect to the'
+                    ' slave database.'),
+    cfg.StrOpt('mysql_sql_mode',
+               default='TRADITIONAL',
+               help='The SQL mode to be used for MySQL sessions. '
+                    'This option, including the default, overrides any '
+                    'server-set SQL mode. To use whatever SQL mode '
+                    'is set by the server configuration, '
+                    'set this to no value. Example: mysql_sql_mode='),
+    cfg.IntOpt('max_pool_size',
+               help='Maximum number of SQL connections to keep open in a '
+                    'pool.'),
+    cfg.IntOpt('max_retries',
+               default=10,
+               help='Maximum number of database connection retries '
+                    'during startup. Set to -1 to specify an infinite '
+                    'retry count.'),
+    cfg.IntOpt('retry_interval',
+               default=10,
+               help='Interval between retries of opening a SQL connection.'),
+    cfg.IntOpt('max_overflow',
+               help='If set, use this value for max_overflow with '
+                    'SQLAlchemy.'),
+    cfg.IntOpt('connection_debug',
+               default=0,
+               help='Verbosity of SQL debugging information: 0=None, '
+                    '100=Everything.'),
+    cfg.BoolOpt('connection_trace',
+                default=False,
+                help='Add Python stack traces to SQL as comment strings.'),
+    cfg.IntOpt('pool_timeout',
+               help='If set, use this value for pool_timeout with '
+                    'SQLAlchemy.'),
+]
+
+
+# Datastore specific option groups
+
 # Mysql
 mysql_group = cfg.OptGroup(
     'mysql', title='MySQL options',
     help="Oslo option group designed for MySQL datastore")
 mysql_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["3306"], item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
-    cfg.StrOpt('backup_strategy', default='innobackupex',
+    cfg.StrOpt('backup_strategy', default='InnoBackupEx',
                help='Default strategy to perform backups.',
                deprecated_name='backup_strategy',
                deprecated_group='DEFAULT'),
     cfg.StrOpt('replication_strategy', default='MysqlGTIDReplication',
                help='Default strategy for replication.'),
     cfg.StrOpt('replication_namespace',
                default='trove.guestagent.strategies.replication.mysql_gtid',
@@ -569,63 +535,69 @@
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
     cfg.IntOpt('usage_timeout', default=400,
                help='Maximum time (in seconds) to wait for a Guest to become '
                     'active.'),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.mysql_impl',
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.mysql_impl',
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
+    cfg.DictOpt('backup_incremental_strategy',
+                default={'InnoBackupEx': 'InnoBackupExIncremental'},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental backup, the runner will use the default full '
+                'backup.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('root_controller',
-               default='trove.extensions.common.service.DefaultRootController',
+               default='trove.extensions.mysql.service.MySQLRootController',
                help='Root controller implementation for mysql.'),
     cfg.ListOpt('ignore_users', default=['os_admin', 'root'],
                 help='Users to exclude when listing users.',
                 deprecated_name='ignore_users',
                 deprecated_group='DEFAULT'),
     cfg.ListOpt('ignore_dbs',
-                default=['mysql', 'information_schema', 'performance_schema',
-                         'sys'],
+                default=['mysql', 'information_schema', 'performance_schema'],
                 help='Databases to exclude when listing databases.',
                 deprecated_name='ignore_dbs',
                 deprecated_group='DEFAULT'),
     cfg.StrOpt('guest_log_exposed_logs', default='general,slow_query',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('guest_log_long_query_time', default=1000,
                help='The time in milliseconds that a statement must take in '
                     'in order to be logged in the slow_query log.',
                deprecated_for_removal=True,
                deprecated_reason='Will be replaced by a configuration group '
                'option: long_query_time'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.'),
-    cfg.StrOpt(
-        'docker_image', default='mysql',
-        help='Database docker image.'
-    ),
-    cfg.StrOpt(
-        'backup_docker_image',
-        sample_default='your-registry/your-repo/db-backup-mysql',
-        help='The docker image used for backup and restore. Trove will uses'
-             'datastore version as the image tag, for example: '
-             'your-registry/your-repo/db-backup-mysql:5.7 is used for mysql'
-             'datastore with version 5.7'
-    ),
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # Percona
 percona_group = cfg.OptGroup(
     'percona', title='Percona options',
     help="Oslo option group designed for Percona datastore")
 percona_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["3306"], item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
@@ -650,18 +622,36 @@
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
     cfg.IntOpt('usage_timeout', default=450,
                help='Maximum time (in seconds) to wait for a Guest to become '
                     'active.'),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.mysql_impl',
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.mysql_impl',
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
+    cfg.DictOpt('backup_incremental_strategy',
+                default={'InnoBackupEx': 'InnoBackupExIncremental'},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental backup, the runner will use the default full '
+                'backup.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('root_controller',
                default='trove.extensions.common.service.DefaultRootController',
                help='Root controller implementation for percona.'),
     cfg.ListOpt('ignore_users', default=['os_admin', 'root'],
                 help='Users to exclude when listing users.',
                 deprecated_name='ignore_users',
                 deprecated_group='DEFAULT'),
@@ -676,25 +666,26 @@
                help='The time in milliseconds that a statement must take in '
                     'in order to be logged in the slow_query log.',
                deprecated_for_removal=True,
                deprecated_reason='Will be replaced by a configuration group '
                'option: long_query_time'),
     cfg.IntOpt('default_password_length',
                default='${mysql.default_password_length}',
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # Percona XtraDB Cluster
 pxc_group = cfg.OptGroup(
     'pxc', title='Percona XtraDB Cluster options',
     help="Oslo option group designed for Percona XtraDB Cluster datastore")
 pxc_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["3306", "4444", "4567", "4568"],
                 item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
@@ -716,18 +707,30 @@
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
     cfg.IntOpt('usage_timeout', default=450,
                help='Maximum time (in seconds) to wait for a Guest to become '
                     'active.'),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.mysql_impl',
+               help='Namespace to load backup strategies from.'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.mysql_impl',
+               help='Namespace to load restore strategies from.'),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
+    cfg.DictOpt('backup_incremental_strategy',
+                default={'InnoBackupEx': 'InnoBackupExIncremental'},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental backup, the runner will use the default full '
+                'backup.'),
     cfg.ListOpt('ignore_users', default=['os_admin', 'root', 'clusterrepuser'],
                 help='Users to exclude when listing users.'),
     cfg.ListOpt('ignore_dbs',
                 default=['mysql', 'information_schema', 'performance_schema'],
                 help='Databases to exclude when listing databases.'),
     cfg.BoolOpt('cluster_support', default=True,
                 help='Enable clusters to be created and managed.'),
@@ -756,52 +759,71 @@
                help='The time in milliseconds that a statement must take in '
                     'in order to be logged in the slow_query log.',
                deprecated_for_removal=True,
                deprecated_reason='Will be replaced by a configuration group '
                'option: long_query_time'),
     cfg.IntOpt('default_password_length',
                default='${mysql.default_password_length}',
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 
 # Redis
 redis_group = cfg.OptGroup(
     'redis', title='Redis options',
     help="Oslo option group designed for Redis datastore")
 redis_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["6379", "16379"],
                 item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.StrOpt('backup_strategy', default='RedisBackup',
                help='Default strategy to perform backups.',
                deprecated_name='backup_strategy',
                deprecated_group='DEFAULT'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('replication_strategy', default='RedisSyncReplication',
                help='Default strategy for replication.'),
     cfg.StrOpt('replication_namespace',
                default='trove.guestagent.strategies.replication.experimental.'
                        'redis_sync',
                help='Namespace to load replication strategies from.'),
     cfg.StrOpt('mount_point', default='/var/lib/redis',
                help="Filesystem path for mounting "
                "volumes if volume support is enabled."),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
+    cfg.StrOpt('backup_namespace',
+               default="trove.guestagent.strategies.backup.experimental."
+                       "redis_impl",
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default="trove.guestagent.strategies.restore.experimental."
+                       "redis_impl",
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.BoolOpt('cluster_support', default=True,
                 help='Enable clusters to be created and managed.'),
     cfg.StrOpt('api_strategy',
                default='trove.common.strategies.cluster.experimental.'
                'redis.api.RedisAPIStrategy',
                help='Class that implements datastore-specific API logic.'),
     cfg.StrOpt('taskmanager_strategy',
@@ -811,54 +833,74 @@
                     'logic.'),
     cfg.StrOpt('guestagent_strategy',
                default='trove.common.strategies.cluster.experimental.'
                'redis.guestagent.RedisGuestAgentStrategy',
                help='Class that implements datastore-specific Guest Agent API '
                     'logic.'),
     cfg.StrOpt('root_controller',
-               default='trove.extensions.redis.service.RedisRootController',
+               default='trove.extensions.common.service.DefaultRootController',
                help='Root controller implementation for redis.'),
     cfg.StrOpt('guest_log_exposed_logs', default='',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # Cassandra
 cassandra_group = cfg.OptGroup(
     'cassandra', title='Cassandra options',
     help="Oslo option group designed for Cassandra datastore")
 cassandra_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["7000", "7001", "7199", "9042", "9160"],
                 item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental strategy based on the default backup '
+                'strategy. For strategies that do not implement incremental '
+                'backups, the runner performs full backup instead.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('backup_strategy', default="NodetoolSnapshot",
                help='Default strategy to perform backups.',
                deprecated_name='backup_strategy',
                deprecated_group='DEFAULT'),
     cfg.StrOpt('replication_strategy', default=None,
                help='Default strategy for replication.'),
     cfg.StrOpt('mount_point', default='/var/lib/cassandra',
                help="Filesystem path for mounting "
                "volumes if volume support is enabled."),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
+    cfg.StrOpt('backup_namespace',
+               default="trove.guestagent.strategies.backup.experimental."
+               "cassandra_impl",
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default="trove.guestagent.strategies.restore.experimental."
+               "cassandra_impl",
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.StrOpt('root_controller',
-               default='trove.extensions.common.service.DefaultRootController',
+               default='trove.extensions.cassandra.service'
+               '.CassandraRootController',
                help='Root controller implementation for Cassandra.'),
     cfg.ListOpt('ignore_users', default=['os_admin'],
                 help='Users to exclude when listing users.'),
     cfg.ListOpt('ignore_dbs', default=['system', 'system_auth',
                                        'system_traces'],
                 help='Databases to exclude when listing databases.'),
     cfg.StrOpt('guest_log_exposed_logs', default='system',
@@ -880,15 +922,17 @@
                     'logic.'),
     cfg.StrOpt('guestagent_strategy',
                default='trove.common.strategies.cluster.experimental'
                '.cassandra.guestagent.CassandraGuestAgentStrategy',
                help='Class that implements datastore-specific Guest Agent API '
                     'logic.'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.'),
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
     cfg.BoolOpt('enable_cluster_instance_backup',
                 default=False,
                 help='Allows backup of single instance in the cluster.'),
     cfg.BoolOpt('enable_saslauthd', default=False,
                 help='Enable the saslauth daemon.'),
     cfg.StrOpt('user_controller',
                default='trove.extensions.cassandra.service.'
@@ -909,92 +953,112 @@
 
 # Couchbase
 couchbase_group = cfg.OptGroup(
     'couchbase', title='Couchbase options',
     help="Oslo option group designed for Couchbase datastore")
 couchbase_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', item_type=ListOfPortsType,
                 default=["8091", "8092", "4369", "11209-11211",
                          "21100-21199"],
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.StrOpt('backup_strategy', default='CbBackup',
                help='Default strategy to perform backups.',
                deprecated_name='backup_strategy',
                deprecated_group='DEFAULT'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('replication_strategy', default=None,
                help='Default strategy for replication.'),
     cfg.StrOpt('mount_point', default='/var/lib/couchbase',
                help="Filesystem path for mounting "
                "volumes if volume support is enabled."),
     cfg.BoolOpt('root_on_create', default=False,
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.experimental.'
+               'couchbase_impl',
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.experimental.'
+               'couchbase_impl',
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
     cfg.StrOpt('root_controller',
                default='trove.extensions.common.service.DefaultRootController',
                help='Root controller implementation for couchbase.'),
     cfg.StrOpt('guest_log_exposed_logs', default='',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('default_password_length', default=24, min=6, max=24,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # MongoDB
 mongodb_group = cfg.OptGroup(
     'mongodb', title='MongoDB options',
     help="Oslo option group designed for MongoDB datastore")
 mongodb_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["2500", "27017", "27019"],
                 item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.StrOpt('backup_strategy', default='MongoDump',
                help='Default strategy to perform backups.',
                deprecated_name='backup_strategy',
                deprecated_group='DEFAULT'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('replication_strategy', default=None,
                help='Default strategy for replication.'),
     cfg.StrOpt('mount_point', default='/var/lib/mongodb',
                help="Filesystem path for mounting "
                "volumes if volume support is enabled."),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
     cfg.IntOpt('num_config_servers_per_cluster', default=3,
                help='The number of config servers to create per cluster.'),
     cfg.IntOpt('num_query_routers_per_cluster', default=1,
                help='The number of query routers (mongos) to create '
                     'per cluster.'),
-    cfg.IntOpt('query_routers_volume_size', default=10,
-               help='Default volume_size (in GB) for query routers (mongos).'),
-    cfg.IntOpt('config_servers_volume_size', default=10,
-               help='Default volume_size (in GB) for config_servers.'),
     cfg.BoolOpt('cluster_support', default=True,
                 help='Enable clusters to be created and managed.'),
     cfg.BoolOpt('cluster_secure', default=True,
                 help='Create secure clusters. If False then the '
                      'Role-Based Access Control will be disabled.'),
     cfg.StrOpt('api_strategy',
                default='trove.common.strategies.cluster.experimental.'
@@ -1006,14 +1070,26 @@
                help='Class that implements datastore-specific task manager '
                     'logic.'),
     cfg.StrOpt('guestagent_strategy',
                default='trove.common.strategies.cluster.experimental.'
                'mongodb.guestagent.MongoDbGuestAgentStrategy',
                help='Class that implements datastore-specific Guest Agent API '
                     'logic.'),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.experimental.'
+                       'mongo_impl',
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.experimental.'
+                       'mongo_impl',
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.PortOpt('mongodb_port', default=27017,
                 help='Port for mongod and mongos instances.'),
     cfg.PortOpt('configsvr_port', default=27019,
                 help='Port for instances running as config servers.'),
     cfg.ListOpt('ignore_dbs', default=['admin', 'local', 'config'],
                 help='Databases to exclude when listing databases.'),
     cfg.ListOpt('ignore_users', default=['admin.os_admin', 'admin.root'],
@@ -1024,110 +1100,102 @@
     cfg.StrOpt('root_controller',
                default='trove.extensions.mongodb.service.'
                        'MongoDBRootController',
                help='Root controller implementation for mongodb.'),
     cfg.StrOpt('guest_log_exposed_logs', default='',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # PostgreSQL
 postgresql_group = cfg.OptGroup(
     'postgresql', title='PostgreSQL options',
     help="Oslo option group for the PostgreSQL datastore.")
 postgresql_opts = [
-    cfg.BoolOpt(
-        'enable_clean_wal_archives',
-        default=True,
-        help='Enable the periodic job to clean up WAL archive folder.'
-    ),
-    cfg.StrOpt(
-        'docker_image', default='postgres',
-        help='Database docker image.'
-    ),
-    cfg.StrOpt(
-        'backup_docker_image',
-        sample_default='your-registry/your-repo/db-backup-postgresql',
-        help='The docker image used for backup and restore. Trove will uses'
-             'datastore version as the image tag, for example: '
-             'your-registry/your-repo/db-backup-postgresql:12 is used for'
-             'postgresql datastore with version 12'
-    ),
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["5432"], item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.PortOpt('postgresql_port', default=5432,
                 help='The TCP port the server listens on.'),
-    cfg.StrOpt('backup_strategy', default='pg_basebackup',
+    cfg.StrOpt('backup_strategy', default='PgBaseBackup',
                help='Default strategy to perform backups.'),
-    cfg.StrOpt(
-        'replication_strategy',
-        default='PostgresqlReplicationStreaming',
-        help='Default strategy for replication.'
-    ),
-    cfg.StrOpt(
-        'replication_namespace',
-        default='trove.guestagent.strategies.replication.postgresql',
-        help='Namespace to load replication strategies from.'
-    ),
+    cfg.DictOpt('backup_incremental_strategy',
+                default={'PgBaseBackup': 'PgBaseBackupIncremental'},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.'),
+    cfg.StrOpt('replication_strategy',
+               default='PostgresqlReplicationStreaming',
+               help='Default strategy for replication.'),
+    cfg.StrOpt('replication_namespace',
+               default='trove.guestagent.strategies.replication.experimental.'
+                       'postgresql_impl',
+               help='Namespace to load replication strategies from.'),
     cfg.StrOpt('mount_point', default='/var/lib/postgresql',
                help="Filesystem path for mounting "
                "volumes if volume support is enabled."),
     cfg.StrOpt('wal_archive_location', default='/mnt/wal_archive',
                help="Filesystem path storing WAL archive files when "
                     "WAL-shipping based backups or replication "
-                    "is enabled.",
-               deprecated_for_removal=True,
-               deprecated_reason='Option is not used any more, will be '
-                                 'removed in future release.'
-               ),
+                    "is enabled."),
     cfg.BoolOpt('root_on_create', default=False,
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.experimental.'
+               'postgresql_impl',
+               help='Namespace to load backup strategies from.'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.experimental.'
+               'postgresql_impl',
+               help='Namespace to load restore strategies from.'),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb'),
-    cfg.ListOpt('ignore_users', default=['os_admin', 'postgres']),
+    cfg.ListOpt('ignore_users', default=['os_admin', 'postgres', 'root']),
     cfg.ListOpt('ignore_dbs', default=['os_admin', 'postgres']),
     cfg.StrOpt('root_controller',
-               default='trove.extensions.common.service.DefaultRootController',
+               default='trove.extensions.postgresql.service'
+               '.PostgreSQLRootController',
                help='Root controller implementation for postgresql.'),
     cfg.StrOpt('guest_log_exposed_logs', default='general',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('guest_log_long_query_time', default=0,
                help="The time in milliseconds that a statement must take in "
                     "in order to be logged in the 'general' log.  A value of "
                     "'0' logs all statements, while '-1' turns off "
                     "statement logging.",
                deprecated_for_removal=True,
                deprecated_reason='Will be replaced by configuration group '
-                                 'option: log_min_duration_statement'),
+               'option: log_min_duration_statement'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # Apache CouchDB
 couchdb_group = cfg.OptGroup(
     'couchdb', title='CouchDB options',
     help="Oslo option group designed for CouchDB datastore")
 couchdb_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports',
                 default=["5984"], item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
@@ -1140,14 +1208,24 @@
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
     cfg.StrOpt('backup_strategy', default='CouchDBBackup',
                help='Default strategy to perform backups.'),
     cfg.StrOpt('replication_strategy', default=None,
                help='Default strategy for replication.'),
+    cfg.StrOpt('backup_namespace', default='trove.guestagent.strategies'
+               '.backup.experimental.couchdb_impl',
+               help='Namespace to load backup strategies from.'),
+    cfg.StrOpt('restore_namespace', default='trove.guestagent.strategies'
+               '.restore.experimental.couchdb_impl',
+               help='Namespace to load restore strategies from.'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.'),
     cfg.BoolOpt('root_on_create', default=False,
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 'instance-create as the "password" field.'),
     cfg.StrOpt('root_controller',
                default='trove.extensions.common.service.DefaultRootController',
@@ -1160,52 +1238,55 @@
                 deprecated_group='DEFAULT'),
     cfg.ListOpt('ignore_dbs',
                 default=['_users', '_replicator'],
                 help='Databases to exclude when listing databases.',
                 deprecated_name='ignore_dbs',
                 deprecated_group='DEFAULT'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # Vertica
 vertica_group = cfg.OptGroup(
     'vertica', title='Vertica options',
     help="Oslo option group designed for Vertica datastore")
 vertica_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', item_type=ListOfPortsType,
-                default=["5433", "5434", "5444", "5450", "4803"],
+                default=["5433", "5434", "22", "5444", "5450", "4803"],
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', item_type=ListOfPortsType,
                 default=["5433", "4803", "4804", "6453"],
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.StrOpt('backup_strategy', default=None,
                help='Default strategy to perform backups.'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.'),
     cfg.StrOpt('replication_strategy', default=None,
                help='Default strategy for replication.'),
     cfg.StrOpt('mount_point', default='/var/lib/vertica',
                help="Filesystem path for mounting "
                "volumes if volume support is enabled."),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
     cfg.StrOpt('backup_namespace', default=None,
-               help='Namespace to load backup strategies from.',
-               deprecated_for_removal=True),
+               help='Namespace to load backup strategies from.'),
     cfg.StrOpt('restore_namespace', default=None,
-               help='Namespace to load restore strategies from.',
-               deprecated_for_removal=True),
+               help='Namespace to load restore strategies from.'),
     cfg.IntOpt('readahead_size', default=2048,
                help='Size(MB) to be set as readahead_size for data volume'),
     cfg.BoolOpt('cluster_support', default=True,
                 help='Enable clusters to be created and managed.'),
     cfg.IntOpt('cluster_member_count', default=3,
                help='Number of members in Vertica cluster.'),
     cfg.StrOpt('api_strategy',
@@ -1227,25 +1308,26 @@
                        'VerticaRootController',
                help='Root controller implementation for Vertica.'),
     cfg.StrOpt('guest_log_exposed_logs', default='',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('min_ksafety', default=0,
                help='Minimum k-safety setting permitted for vertica clusters'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # DB2
 db2_group = cfg.OptGroup(
     'db2', title='DB2 options',
     help="Oslo option group designed for DB2 datastore")
 db2_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports',
                 default=["50000"], item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                 'in the security group (only applicable '
                 'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
@@ -1263,65 +1345,104 @@
     cfg.StrOpt('replication_strategy', default=None,
                help='Default strategy for replication.'),
     cfg.BoolOpt('root_on_create', default=False,
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.experimental.'
+                       'db2_impl',
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.experimental.'
+                       'db2_impl',
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.DictOpt('backup_incremental_strategy', default={},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental, the runner will use the default full backup.'),
     cfg.ListOpt('ignore_users', default=['PUBLIC', 'DB2INST1']),
     cfg.StrOpt('root_controller',
                default='trove.extensions.common.service.DefaultRootController',
                help='Root controller implementation for db2.'),
     cfg.StrOpt('guest_log_exposed_logs', default='',
                help='List of Guest Logs to expose for publishing.'),
     cfg.IntOpt('default_password_length', default=36,
-               help='Character length of generated passwords.')
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # MariaDB
 mariadb_group = cfg.OptGroup(
     'mariadb', title='MariaDB options',
     help="Oslo option group designed for MariaDB datastore")
 mariadb_opts = [
     cfg.BoolOpt('icmp', default=False,
-                help='Whether to permit ICMP.',
-                deprecated_for_removal=True),
+                help='Whether to permit ICMP.'),
     cfg.ListOpt('tcp_ports', default=["3306", "4444", "4567", "4568"],
                 item_type=ListOfPortsType,
                 help='List of TCP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
     cfg.ListOpt('udp_ports', default=[], item_type=ListOfPortsType,
                 help='List of UDP ports and/or port ranges to open '
                      'in the security group (only applicable '
                      'if trove_security_groups_support is True).'),
-    cfg.StrOpt('backup_strategy', default='mariabackup',
+    cfg.StrOpt('backup_strategy', default='MariaDBInnoBackupEx',
                help='Default strategy to perform backups.',
                deprecated_name='backup_strategy',
                deprecated_group='DEFAULT'),
     cfg.StrOpt('replication_strategy', default='MariaDBGTIDReplication',
                help='Default strategy for replication.'),
     cfg.StrOpt('replication_namespace',
-               default='trove.guestagent.strategies.replication.mariadb_gtid',
+               default='trove.guestagent.strategies.replication.experimental'
+               '.mariadb_gtid',
                help='Namespace to load replication strategies from.'),
     cfg.StrOpt('mount_point', default='/var/lib/mysql',
                help="Filesystem path for mounting "
                     "volumes if volume support is enabled."),
     cfg.BoolOpt('root_on_create', default=False,
                 help='Enable the automatic creation of the root user for the '
                 'service during instance-create. The generated password for '
                 'the root user is immediately returned in the response of '
                 "instance-create as the 'password' field."),
     cfg.IntOpt('usage_timeout', default=400,
                help='Maximum time (in seconds) to wait for a Guest to become '
                     'active.'),
+    cfg.StrOpt('backup_namespace',
+               default='trove.guestagent.strategies.backup.experimental'
+                       '.mariadb_impl',
+               help='Namespace to load backup strategies from.',
+               deprecated_name='backup_namespace',
+               deprecated_group='DEFAULT'),
+    cfg.StrOpt('restore_namespace',
+               default='trove.guestagent.strategies.restore.experimental'
+                       '.mariadb_impl',
+               help='Namespace to load restore strategies from.',
+               deprecated_name='restore_namespace',
+               deprecated_group='DEFAULT'),
     cfg.BoolOpt('volume_support', default=True,
                 help='Whether to provision a Cinder volume for datadir.'),
     cfg.StrOpt('device_path', default='/dev/vdb',
                help='Device path for volume if volume support is enabled.'),
+    cfg.DictOpt('backup_incremental_strategy',
+                default={'MariaDBInnoBackupEx':
+                         'MariaDBInnoBackupExIncremental'},
+                help='Incremental Backup Runner based on the default '
+                'strategy. For strategies that do not implement an '
+                'incremental backup, the runner will use the default full '
+                'backup.',
+                deprecated_name='backup_incremental_strategy',
+                deprecated_group='DEFAULT'),
     cfg.StrOpt('root_controller',
                default='trove.extensions.common.service.DefaultRootController',
                help='Root controller implementation for mysql.'),
     cfg.ListOpt('ignore_users', default=['os_admin', 'root'],
                 help='Users to exclude when listing users.',
                 deprecated_name='ignore_users',
                 deprecated_group='DEFAULT'),
@@ -1354,27 +1475,17 @@
     cfg.StrOpt('guestagent_strategy',
                default='trove.common.strategies.cluster.experimental.'
                'galera_common.guestagent.GaleraCommonGuestAgentStrategy',
                help='Class that implements datastore-specific Guest Agent API '
                     'logic.'),
     cfg.IntOpt('default_password_length',
                default='${mysql.default_password_length}',
-               help='Character length of generated passwords.'),
-    cfg.StrOpt(
-        'docker_image', default='mariadb',
-        help='Database docker image.'
-    ),
-    cfg.StrOpt(
-        'backup_docker_image',
-        sample_default='your-registry/your-repo/db-backup-mariadb',
-        help='The docker image used for backup and restore. Trove will uses'
-             'datastore version as the image tag, for example: '
-             'your-registry/your-repo/db-backup-mariadb:10.3 is used for '
-             'mariadb datastore with version 10.3'
-    ),
+               help='Character length of generated passwords.',
+               deprecated_name='default_password_length',
+               deprecated_group='DEFAULT'),
 ]
 
 # RPC version groups
 upgrade_levels = cfg.OptGroup(
     'upgrade_levels',
     title='RPC upgrade levels group for handling versions',
     help='Contains the support version caps (Openstack Release) for '
@@ -1388,164 +1499,53 @@
         'guestagent', default='latest',
         help='Set a version cap for messages sent to guestagent services'),
     cfg.StrOpt(
         'conductor', default='latest',
         help='Set Openstack Release compatibility for conductor services'),
 ]
 
-network_group = cfg.OptGroup(
-    'network',
-    title='Networking options',
-    help="Options related to the trove instance networking."
-)
-network_opts = [
-    cfg.StrOpt(
-        'public_network_id',
-        default=None,
-        help='ID of the Neutron public network to create floating IP for the '
-             'public trove instance. If not given, Trove will try to query '
-             'all the public networks and use the first one in the list.'
-    ),
-    cfg.BoolOpt(
-        'enable_access_check', default=True,
-        help='Check if the user provided network is associated with router. '
-             'This is needed for the instance initialization. The check is '
-             'also necessary when creating public facing instance. A scenario '
-             'to set this option False is when using Neutron provider '
-             'network.'
-    ),
-    cfg.BoolOpt(
-        'network_isolation', default=False,
-        help='whether to plug user defined port to database container.'
-             'This would be useful to isolate user traffic from management'
-             'traffic and to avoid network address conflicts.'
-    )
-]
-
-service_credentials_group = cfg.OptGroup(
-    'service_credentials',
-    help="Options related to Trove service credentials."
-)
-service_credentials_opts = [
-    cfg.URIOpt('auth_url', default='https://0.0.0.0/identity/v3',
-               deprecated_name='trove_auth_url',
-               deprecated_group='DEFAULT',
-               help='Keystone authentication URL.'),
-    cfg.StrOpt('username', default='',
-               help="Trove service user name.",
-               deprecated_name='nova_proxy_admin_user',
-               deprecated_group='DEFAULT'),
-    cfg.StrOpt('password', default='', secret=True,
-               help="Trove service user password.",
-               deprecated_name='nova_proxy_admin_pass',
-               deprecated_group='DEFAULT'),
-    cfg.StrOpt('project_id', default='',
-               deprecated_name='nova_proxy_admin_tenant_id',
-               deprecated_group='DEFAULT',
-               help="Trove service project ID."),
-    cfg.StrOpt('project_name', default='',
-               deprecated_name='nova_proxy_admin_tenant_name',
-               deprecated_group='DEFAULT',
-               help="Trove service project name."),
-    cfg.StrOpt('user_domain_name', default='Default',
-               deprecated_name='nova_proxy_admin_user_domain_name',
-               deprecated_group='DEFAULT',
-               help="Keystone domain name of the Trove service user."),
-    cfg.StrOpt('project_domain_name', default='Default',
-               deprecated_name='nova_proxy_admin_project_domain_name',
-               deprecated_group='DEFAULT',
-               help="Keystone domain name of the Trove service project."),
-    cfg.StrOpt('region_name', default='RegionOne',
-               deprecated_name='os_region_name',
-               deprecated_group='DEFAULT',
-               help="Keystone region name of the Trove service project."),
-]
-
-guest_agent_group = cfg.OptGroup(
-    'guest_agent', title='Guest Agent options',
-    help="Config options used by guest agent.")
-guest_agent_opts = [
-    cfg.StrOpt(
-        'container_registry',
-        help='URL to the registry. E.g. https://index.docker.io/v1/'
-    ),
-    cfg.StrOpt(
-        'container_registry_username',
-        help='The registry username.'
-    ),
-    cfg.StrOpt(
-        'container_registry_password',
-        help='The plaintext registry password.',
-        secret=True,
-    ),
-]
-
 CONF = cfg.CONF
 
 CONF.register_opts(path_opts)
-CONF.register_opts(versions_opts)
 CONF.register_opts(common_opts)
+
+CONF.register_opts(database_opts, 'database')
+
 CONF.register_group(mysql_group)
 CONF.register_group(percona_group)
 CONF.register_group(pxc_group)
 CONF.register_group(redis_group)
 CONF.register_group(cassandra_group)
 CONF.register_group(couchbase_group)
 CONF.register_group(mongodb_group)
 CONF.register_group(postgresql_group)
 CONF.register_group(couchdb_group)
 CONF.register_group(vertica_group)
 CONF.register_group(db2_group)
 CONF.register_group(mariadb_group)
-CONF.register_group(network_group)
-CONF.register_group(service_credentials_group)
-CONF.register_group(guest_agent_group)
 
 CONF.register_opts(mysql_opts, mysql_group)
 CONF.register_opts(percona_opts, percona_group)
 CONF.register_opts(pxc_opts, pxc_group)
 CONF.register_opts(redis_opts, redis_group)
 CONF.register_opts(cassandra_opts, cassandra_group)
 CONF.register_opts(couchbase_opts, couchbase_group)
 CONF.register_opts(mongodb_opts, mongodb_group)
 CONF.register_opts(postgresql_opts, postgresql_group)
 CONF.register_opts(couchdb_opts, couchdb_group)
 CONF.register_opts(vertica_opts, vertica_group)
 CONF.register_opts(db2_opts, db2_group)
 CONF.register_opts(mariadb_opts, mariadb_group)
-CONF.register_opts(network_opts, network_group)
-CONF.register_opts(service_credentials_opts, service_credentials_group)
-CONF.register_opts(guest_agent_opts, guest_agent_group)
 
 CONF.register_opts(rpcapi_cap_opts, upgrade_levels)
-db_options.set_defaults(CONF, connection='sqlite://')
 
 profiler.set_defaults(CONF)
 logging.register_options(CONF)
 
 
-def list_opts():
-    keystone_middleware_opts = auth_token.list_opts()
-    keystone_loading_opts = [(
-        'keystone_authtoken', loading.get_auth_plugin_conf_options('password')
-    )]
-
-    trove_opts = [
-        (None, path_opts + versions_opts + common_opts),
-        (mysql_group, mysql_opts),
-        (postgresql_group, postgresql_opts),
-        (mariadb_group, mariadb_opts),
-        (network_group, network_opts),
-        (service_credentials_group, service_credentials_opts),
-        (guest_agent_group, guest_agent_opts),
-    ]
-
-    return keystone_middleware_opts + keystone_loading_opts + trove_opts
-
-
 def custom_parser(parsername, parser):
     CONF.register_cli_opt(cfg.SubCommandOpt(parsername, handler=parser))
 
 
 def parse_args(argv, default_config_files=None):
     cfg.CONF(args=argv[1:],
              project='trove',
@@ -1576,16 +1576,16 @@
 
     # Fake-integration tests do not define 'CONF.datastore_manager'.
     # *MySQL* options will
     # be loaded. This should never occur in a production environment.
     datastore_manager = CONF.datastore_manager
     if not datastore_manager:
         datastore_manager = 'mysql'
-        LOG.warning("Manager name ('datastore_manager') not defined, "
-                    "using '%s' options instead.", datastore_manager)
+        LOG.warning(_("Manager name ('datastore_manager') not defined, "
+                      "using '%s' options instead."), datastore_manager)
 
     try:
         return CONF.get(datastore_manager).get(property_name)
     except NoSuchOptError:
         return CONF.get(property_name)
```

### Comparing `trove-21.0.0.0rc2/trove/common/configurations.py` & `trove-8.0.1/trove/common/configurations.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/constants.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/018_datastore_versions_fix.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,22 +1,25 @@
-# Copyright 2021 Catalyst Cloud Ltd.
+# Copyright 2012 OpenStack Foundation
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
 
-BACKUP_TYPE_FULL = 'full'
-BACKUP_TYPE_INC = 'incremental'
-ETH1_CONFIG_PATH = "/etc/trove/eth1.json"
-DOCKER_NETWORK_NAME = "database-network"
-DOCKER_HOST_NIC_MODE = "docker-hostnic"
-DOCKER_BRIDGE_MODE = "bridge"
-MYSQL_HOST_SOCKET_PATH = "/var/lib/mysqld"
-POSTGRESQL_HOST_SOCKET_PATH = "/var/lib/postgresql-socket"
+from sqlalchemy.schema import MetaData
+
+from trove.db.sqlalchemy.migrate_repo.schema import Table
+
+
+def upgrade(migrate_engine):
+    meta = MetaData()
+    meta.bind = migrate_engine
+    datastore_versions = Table('datastore_versions', meta, autoload=True)
+    # modify column
+    datastore_versions.c.name.alter(unique=False)
```

### Comparing `trove-21.0.0.0rc2/trove/common/context.py` & `trove-8.0.1/trove/guestagent/strategies/restore/experimental/db2_impl.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,80 +1,91 @@
-# Copyright 2011 OpenStack Foundation
+# Copyright 2016 IBM Corp
 # All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License"); you may
-#    not use this file except in compliance with the License. You may obtain
-#    a copy of the License at
+# Licensed under the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License. You may obtain
+# a copy of the License at
 #
-#         http://www.apache.org/licenses/LICENSE-2.0
+# http://www.apache.org/licenses/LICENSE-2.0
 #
-#    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-#    License for the specific language governing permissions and limitations
-#    under the License.
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+# License for the specific language governing permissions and limitations
+# under the License.
 
-"""
-Simple class that stores security context information in the web request.
+from oslo_log import log as logging
 
-Projects should subclass this class if they wish to enhance the request
-context or provide additional information in their specific WSGI pipeline.
-"""
+from trove.common import exception
+from trove.common.i18n import _
+from trove.common import utils
+from trove.guestagent.datastore.experimental.db2 import service
+from trove.guestagent.datastore.experimental.db2 import system
+from trove.guestagent.strategies.restore import base
 
+LOG = logging.getLogger(__name__)
 
-from oslo_context import context
-from oslo_log import log as logging
 
-from trove.common import local
-from trove.common.serializable_notification import SerializableNotification
+class DB2Backup(base.RestoreRunner):
+    """
+    Base class implementation of Restore strategy for DB2
+    """
+    base_restore_cmd = 'sudo tar xPf -'
 
-LOG = logging.getLogger(__name__)
+    def __init__(self, *args, **kwargs):
+        super(DB2Backup, self).__init__(*args, **kwargs)
+        self.appStatus = service.DB2AppStatus()
+        self.app = service.DB2App(self.appStatus)
+        self.admin = service.DB2Admin()
+        self.restore_location = system.DB2_BACKUP_DIR
+
+    def _post_restore(self, restore_command, rollforward_command=None):
+        """
+        Restore from the directory that we untarred into
+        """
+        out = ""
+        try:
+            out, err = utils.execute_with_timeout(system.GET_DB_NAMES,
+                                                  shell=True)
+        except exception.ProcessExecutionError:
+            LOG.exception(_("Couldn't find any databases."))
+
+        dbNames = out.split()
+        for dbName in dbNames:
+            service.run_command(restore_command % {'dbname': dbName})
+            if rollforward_command:
+                service.run_command(system.ROLL_FORWARD_DB % {'dbname':
+                                                              dbName})
+
+        LOG.info(_("Cleaning out restore location: %s."),
+                 system.DB2_BACKUP_DIR)
+        service.remove_db2_dir(system.DB2_BACKUP_DIR)
 
 
-class TroveContext(context.RequestContext):
+class DB2OfflineBackup(DB2Backup):
     """
-    Stores information about the security context under which the user
-    accesses the system, as well as additional request information.
+    Implementation of Restore Strategy for full offline backups
+    using the default circular logging
     """
-    def __init__(self, limit=None, marker=None, service_catalog=None,
-                 user_identity=None, instance_id=None, timeout=None,
-                 **kwargs):
-        self.limit = limit
-        self.marker = marker
-        self.service_catalog = service_catalog
-        self.user_identity = user_identity
-        self.instance_id = instance_id
-        self.timeout = timeout
-        super(TroveContext, self).__init__(**kwargs)
-
-        if not hasattr(local.store, 'context'):
-            self.update_store()
-
-    def to_dict(self):
-        parent_dict = super(TroveContext, self).to_dict()
-        parent_dict.update({'limit': self.limit,
-                            'marker': self.marker,
-                            'service_catalog': self.service_catalog
-                            })
-        if hasattr(self, 'notification'):
-            serialized = SerializableNotification.serialize(self,
-                                                            self.notification)
-            parent_dict['trove_notification'] = serialized
-        return parent_dict
-
-    def update_store(self):
-        super(TroveContext, self).update_store()
-        local.store.context = self
-
-    @classmethod
-    def from_dict(cls, values):
-        n_values = values.pop('trove_notification', None)
-        ctx = super(TroveContext, cls).from_dict(
-            values,
-            limit=values.get('limit'),
-            marker=values.get('marker'),
-            service_catalog=values.get('service_catalog'))
-
-        if n_values:
-            ctx.notification = SerializableNotification.deserialize(
-                ctx, n_values)
-        return ctx
+    __strategy_name__ = 'db2offlinebackup'
+
+    def post_restore(self):
+        self._post_restore(system.RESTORE_OFFLINE_DB)
+
+
+class DB2OnlineBackup(DB2Backup):
+    """
+    Implementation of restore strategy for full online backups using
+    archived logging.
+    """
+    __strategy_name__ = 'db2onlinebackup'
+
+    def post_restore(self):
+        """
+        Once the databases are restored from a backup, we have to roll
+        forward the logs to the point of where the backup was taken. This
+        brings the database to a state were it can used, otherwise it
+        remains in a BACKUP PENDING state. After roll forwarding the logs,
+        we can delete the archived logs.
+        """
+        self._post_restore(system.RESTORE_ONLINE_DB, system.ROLL_FORWARD_DB)
+        service.remove_db2_dir(system.DB2_ARCHIVE_LOGS_DIR + '/*')
```

### Comparing `trove-21.0.0.0rc2/trove/common/crypto_utils.py` & `trove-8.0.1/trove/common/crypto_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -12,88 +12,67 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 # Encryption/decryption handling
 
+from Crypto.Cipher import AES
+from Crypto import Random
 import hashlib
-import os
 from oslo_utils import encodeutils
 import random
+import six
 import string
 
-from cryptography.hazmat.backends import default_backend
-from cryptography.hazmat.primitives.ciphers import algorithms
-from cryptography.hazmat.primitives.ciphers import Cipher
-from cryptography.hazmat.primitives.ciphers import modes
 from trove.common import stream_codecs
 
 
-IV_BYTE_COUNT = 16
-_CRYPT_BACKEND = None
-
-
-def _get_cipher(key, iv):
-    global _CRYPT_BACKEND
-    if not _CRYPT_BACKEND:
-        _CRYPT_BACKEND = default_backend()
-
-    return Cipher(algorithms.AES(key), modes.CBC(iv),
-                  backend=_CRYPT_BACKEND)
-
-
-def _encrypt(key, iv, data):
-    encryptor = _get_cipher(key, iv).encryptor()
-    return encryptor.update(data) + encryptor.finalize()
-
-
-def _decrypt(key, iv, data):
-    decryptor = _get_cipher(key, iv).decryptor()
-    return decryptor.update(data) + decryptor.finalize()
+IV_BIT_COUNT = 16
 
 
 def encode_data(data):
-    # NOTE(zhaochao) No need to encoding string object any more,
-    # as Base64Codec is now using oslo_serialization.base64 which
-    # could take care of this.
+    if isinstance(data, six.text_type):
+        data = data.encode('utf-8')
     return stream_codecs.Base64Codec().serialize(data)
 
 
 def decode_data(data):
     return stream_codecs.Base64Codec().deserialize(data)
 
 
 # Pad the data string to an multiple of pad_size
-def pad_for_encryption(data, pad_size=IV_BYTE_COUNT):
+def pad_for_encryption(data, pad_size=IV_BIT_COUNT):
     pad_count = pad_size - (len(data) % pad_size)
-    return data + bytes((pad_count,)) * pad_count
+    return data + six.int2byte(pad_count) * pad_count
 
 
 # Unpad the data string by stripping off excess characters
 def unpad_after_decryption(data):
-    return data[:len(data) - data[-1]]
+    return data[:len(data) - six.indexbytes(data, -1)]
 
 
-def encrypt_data(data, key, iv_byte_count=IV_BYTE_COUNT):
+def encrypt_data(data, key, iv_bit_count=IV_BIT_COUNT):
     data = encodeutils.to_utf8(data)
     key = encodeutils.to_utf8(key)
-    md5_key = encodeutils.safe_encode(hashlib.md5(key).hexdigest())
-    iv = os.urandom(iv_byte_count)
-    iv = iv[:iv_byte_count]
-    data = pad_for_encryption(data, iv_byte_count)
-    encrypted = _encrypt(md5_key, bytes(iv), data)
+    md5_key = hashlib.md5(key).hexdigest()
+    iv = Random.new().read(iv_bit_count)
+    iv = iv[:iv_bit_count]
+    aes = AES.new(md5_key, AES.MODE_CBC, iv)
+    data = pad_for_encryption(data, iv_bit_count)
+    encrypted = aes.encrypt(data)
     return iv + encrypted
 
 
-def decrypt_data(data, key, iv_byte_count=IV_BYTE_COUNT):
+def decrypt_data(data, key, iv_bit_count=IV_BIT_COUNT):
     key = encodeutils.to_utf8(key)
-    md5_key = encodeutils.safe_encode(hashlib.md5(key).hexdigest())
-    iv = data[:iv_byte_count]
-    decrypted = _decrypt(md5_key, bytes(iv), bytes(data[iv_byte_count:]))
+    md5_key = hashlib.md5(key).hexdigest()
+    iv = data[:iv_bit_count]
+    aes = AES.new(md5_key, AES.MODE_CBC, bytes(iv))
+    decrypted = aes.decrypt(bytes(data[iv_bit_count:]))
     return unpad_after_decryption(decrypted)
 
 
 def generate_random_key(length=32, chars=None):
     chars = chars if chars else (string.ascii_uppercase +
                                  string.ascii_lowercase + string.digits)
     return ''.join(random.choice(chars) for _ in range(length))
```

### Comparing `trove-21.0.0.0rc2/trove/common/db/cassandra/models.py` & `trove-8.0.1/trove/common/db/cassandra/models.py`

 * *Files 10% similar despite different names*

```diff
@@ -24,15 +24,15 @@
     """
 
     @property
     def _max_schema_name_length(self):
         return 32
 
     def _is_valid_schema_name(self, value):
-        return not any(c in value for c in r'/\. "$')
+        return not any(c in value for c in '/\. "$')
 
 
 class CassandraUser(models.DatastoreUser):
     """Represents a Cassandra user and its associated properties."""
 
     root_username = 'cassandra'
```

### Comparing `trove-21.0.0.0rc2/trove/common/db/couchdb/models.py` & `trove-8.0.1/trove/common/db/couchdb/models.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/db/models.py` & `trove-8.0.1/trove/common/db/models.py`

 * *Files 0% similar despite different names*

```diff
@@ -80,15 +80,16 @@
     @staticmethod
     def check_string(value, desc):
         """Check if the value is a string/unicode.
         :param value:        Value to check.
         :param desc:         Description for exception message.
         :raises:             ValueError if not a string/unicode.
         """
-        if not isinstance(value, str):
+        if not (isinstance(value, str) or
+                isinstance(value, unicode)):
             raise ValueError(_("%(desc)s is not a string. Type = %(t)s.")
                              % {'desc': desc, 't': type(value)})
 
 
 class DatastoreSchema(DatastoreModelsBase):
     """Represents a database schema."""
 
@@ -164,15 +165,15 @@
 
     def check_reserved(self):
         """Check if the name is on the ignore_dbs list, meaning it is
         reserved.
         :raises:             ValueError if name is on the reserved list.
         """
         if self.is_ignored():
-            raise ValueError(_('Database name "%(name)s" is on the reserved '
+            raise ValueError(_('Database name "%(name)s" is on the reserved'
                                'list: %(reserved)s.')
                              % {'name': self.name,
                                 'reserved': self.ignored_dbs})
 
     def _create_checks(self):
         """Checks to be performed before database can be created."""
         self.check_reserved()
```

### Comparing `trove-21.0.0.0rc2/trove/common/db/mongodb/models.py` & `trove-8.0.1/trove/common/db/mongodb/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,15 +23,15 @@
     @property
     def _max_schema_name_length(self):
         return 64
 
     def _is_valid_schema_name(self, value):
         # check against the invalid character set from
         # http://docs.mongodb.org/manual/reference/limits
-        return not any(c in value for c in r'/\. "$')
+        return not any(c in value for c in '/\. "$')
 
 
 class MongoDBUser(models.DatastoreUser):
     """Represents a MongoDB user and its associated properties.
     MongoDB users are identified using their name and database.
     Trove stores this as <database>.<username>
     """
```

### Comparing `trove-21.0.0.0rc2/trove/common/db/mysql/models.py` & `trove-8.0.1/trove/common/db/mysql/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,17 +25,17 @@
 CONF = cfg.CONF
 
 
 class MySQLSchema(models.DatastoreSchema):
     """Represents a MySQL database and its properties."""
 
     # Defaults
-    __charset__ = "utf8mb3"
-    __collation__ = "utf8mb3_general_ci"
-    dbname = re.compile(r"^[A-Za-z0-9_-]+[\s\?\#\@]*[A-Za-z0-9_-]+$")
+    __charset__ = "utf8"
+    __collation__ = "utf8_general_ci"
+    dbname = re.compile("^[A-Za-z0-9_-]+[\s\?\#\@]*[A-Za-z0-9_-]+$")
 
     # Complete list of acceptable values
     collation = mysql_settings.collation
     charset = mysql_settings.charset
 
     def __init__(self, name=None, collate=None, character_set=None,
                  deserializing=False):
@@ -117,15 +117,15 @@
         else:
             self._character_set = None
 
 
 class MySQLUser(models.DatastoreUser):
     """Represents a MySQL User and its associated properties."""
 
-    not_supported_chars = re.compile(r"""^\s|\s$|'|"|;|`|,|/|\\""")
+    not_supported_chars = re.compile("^\s|\s$|'|\"|;|`|,|/|\\\\")
 
     def _is_valid_string(self, value):
         if (not value or
                 self.not_supported_chars.search(value) or
                 ("%r" % value).find("\\") != -1):
             return False
         else:
```

### Comparing `trove-21.0.0.0rc2/trove/common/db/postgresql/models.py` & `trove-8.0.1/trove/common/db/postgresql/models.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,21 +11,23 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import re
 
+from six import u
+
 from trove.common.db import models
 
 
 class PostgreSQLSchema(models.DatastoreSchema):
     """Represents a PostgreSQL schema and its associated properties."""
 
-    name_regex = re.compile(str(r'^[\u0001-\u007F\u0080-\uFFFF]+[^\s]$'))
+    name_regex = re.compile(u(r'^[\u0001-\u007F\u0080-\uFFFF]+[^\s]$'))
 
     def __init__(self, name=None, collate=None, character_set=None,
                  deserializing=False):
         super(PostgreSQLSchema, self).__init__(name=name,
                                                deserializing=deserializing)
         self.collate = collate
         self.character_set = character_set
```

### Comparing `trove-21.0.0.0rc2/trove/common/db/redis/models.py` & `trove-8.0.1/trove/common/rpc/version.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,26 @@
-# Copyright 2017 Eayun, Inc.
+# Copyright 2011 OpenStack Foundation.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-#
-
-from trove.common.db import models
-
 
-class RedisRootUser(models.DatastoreModelsBase):
+# based on configured release version
+RPC_API_VERSION = "1.0"
 
-    def verify_dict(self):
-        pass
-
-    def __init__(self, password=None):
-        self._name = '-'
-        self._password = password
-        super(RedisRootUser, self).__init__()
+# API version history:
+#
+# 1.0 - Initial version.  (We started keeping track at icehouse-3)
+# 1.1 -
+# 1.2 - ...
+VERSION_ALIASES = {
+    'icehouse': '1.0'
+}
```

### Comparing `trove-21.0.0.0rc2/trove/common/debug_utils.py` & `trove-8.0.1/trove/common/debug_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 """Help utilities for debugging"""
 
 import sys
 
 from oslo_config import cfg
 from oslo_log import log as logging
 
+from trove.common.i18n import _
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
 __debug_state = None
 
 pydev_debug_opts = [
@@ -92,16 +93,16 @@
     # http://stackoverflow.com/questions/32452110/
     #     does-eventlet-do-monkey-patch-for-threading-module
     #
     # note multi-line URL
     if __debug_state:
         import threading
         if threading.current_thread.__module__ == 'eventlet.green.threading':
-            LOG.warning("Enabling debugging with eventlet monkey"
-                        " patched produce unexpected behavior.")
+            LOG.warning(_("Enabling debugging with eventlet monkey"
+                          " patched produce unexpected behavior."))
 
     return __debug_state
 
 
 def __setup_remote_pydev_debug_safe(pydev_debug_host=None,
                                     pydev_debug_port=5678, pydev_path=None):
     """
@@ -113,17 +114,17 @@
 
     try:
         return __setup_remote_pydev_debug(
             pydev_debug_host=pydev_debug_host,
             pydev_debug_port=pydev_debug_port,
             pydev_path=pydev_path)
     except Exception as e:
-        LOG.warning("Can't connect to remote debug server."
-                    " Continuing to work in standard mode."
-                    " Error: %s.", e)
+        LOG.warning(_("Can't connect to remote debug server."
+                      " Continuing to work in standard mode."
+                      " Error: %s."), e)
         return False
 
 
 def __setup_remote_pydev_debug(pydev_debug_host=None, pydev_debug_port=None,
                                pydev_path=None):
     """
     Method connects to remote debug server, and attach current thread trace
```

### Comparing `trove-21.0.0.0rc2/trove/common/exception.py` & `trove-8.0.1/trove/common/exception.py`

 * *Files 7% similar despite different names*

```diff
@@ -57,29 +57,19 @@
 
 
 class InvalidRPCConnectionReuse(TroveError):
 
     message = _("Invalid RPC Connection Reuse.")
 
 
-class InvalidValue(TroveError):
-    message = _("The value is not allowed: %(value)s.")
-
-
 class NotFound(TroveError):
 
     message = _("Resource %(uuid)s cannot be found.")
 
 
-class BadRequest(TroveError):
-
-    message = _("The server could not comply with the request since it is "
-                "either malformed or otherwise incorrect.")
-
-
 class CapabilityNotFound(NotFound):
 
     message = _("Capability '%(capability)s' cannot be found.")
 
 
 class CapabilityDisabled(TroveError):
 
@@ -92,29 +82,23 @@
 
 
 class UserNotFound(NotFound):
 
     message = _("User %(uuid)s cannot be found on the instance.")
 
 
-class RootHistoryNotFound(NotFound):
-
-    message = _("Root user has never been enabled on the instance.")
-
-
 class DatabaseNotFound(NotFound):
 
     message = _("Database %(uuid)s cannot be found on the instance.")
 
 
 class ComputeInstanceNotFound(NotFound):
 
-    # internal_message is used for log, stop translating.
-    internal_message = ("Cannot find compute instance %(server_id)s for "
-                        "instance %(instance_id)s.")
+    internal_message = _("Cannot find compute instance %(server_id)s for "
+                         "instance %(instance_id)s.")
 
     message = _("Resource %(instance_id)s can not be retrieved.")
 
 
 class DnsRecordNotFound(NotFound):
 
     message = _("DnsRecord with name= %(name)s not found.")
@@ -134,81 +118,58 @@
 
     message = _("Datastores cannot be found.")
 
 
 class DatastoreFlavorAssociationNotFound(NotFound):
 
     message = _("Flavor %(id)s is not supported for datastore "
-                "version %(datastore_version_id)s")
+                "%(datastore)s version %(datastore_version)s")
 
 
 class DatastoreFlavorAssociationAlreadyExists(TroveError):
 
     message = _("Flavor %(id)s is already associated with "
-                "datastore version %(datastore_version_id)s")
+                "datastore %(datastore)s version %(datastore_version)s")
 
 
 class DatastoreVolumeTypeAssociationNotFound(NotFound):
 
     message = _("The volume type %(id)s is not valid for datastore "
-                "version %(datastore_version_id)s.")
+                "%(datastore)s and version %(version_id)s.")
 
 
 class DatastoreVolumeTypeAssociationAlreadyExists(TroveError):
 
-    message = _("Datastore version %(datastore_version_id)s "
+    message = _("Datastore '%(datastore)s' version %(datastore_version)s "
                 "and volume-type %(id)s mapping already exists.")
 
 
 class DataStoreVersionVolumeTypeRequired(TroveError):
 
     message = _("Only specific volume types are allowed for a "
-                "datastore version %(datastore_version_id)s. "
+                "datastore %(datastore)s version %(datastore_version)s. "
                 "You must specify a valid volume type.")
 
 
 class DatastoreVersionNoVolumeTypes(TroveError):
 
     message = _("No valid volume types could be found for datastore "
-                "version %(datastore_version_id)s.")
+                "%(datastore)s and version %(datastore_version)s.")
 
 
 class DatastoreNoVersion(TroveError):
 
     message = _("Datastore '%(datastore)s' has no version '%(version)s'.")
 
 
 class DatastoreVersionInactive(TroveError):
 
     message = _("Datastore version '%(version)s' is not active.")
 
 
-class DatastoreVersionAlreadyExists(BadRequest):
-
-    message = _("The datastore version '%(name)s(%(version)s)' already "
-                "exists.")
-
-
-class DatastoreVersionsExist(BadRequest):
-
-    message = _("Datastore versions exist for datastore %(datastore)s.")
-
-
-class DatastoreVersionsInUse(BadRequest):
-
-    message = _("Datastore version is in use by %(resource)s.")
-
-
-class DatastoreVersionsNoUniqueMatch(TroveError):
-
-    message = _("Multiple datastore versions found for '%(name)s', "
-                "use an UUID or specify both the name and version number to "
-                "be more specific.")
-
-
 class DatastoreDefaultDatastoreNotFound(TroveError):
 
     message = _("Please specify datastore. Default datastore "
                 "'%(datastore)s' cannot be found.")
 
 
 class DatastoreDefaultDatastoreNotDefined(TroveError):
@@ -229,25 +190,24 @@
 
 class DatastoreOperationNotSupported(TroveError):
 
     message = _("The '%(operation)s' operation is not supported for "
                 "the '%(datastore)s' datastore.")
 
 
-class OverLimit(TroveError):
+class NoUniqueMatch(TroveError):
 
-    # internal_message is used for log, stop translating.
-    internal_message = ("The server rejected the request due to its size or "
-                        "rate.")
+    message = _("Multiple matches found for '%(name)s', "
+                "use an UUID to be more specific.")
 
 
-class QuotaLimitTooSmall(TroveError):
+class OverLimit(TroveError):
 
-    message = _("Quota limit '%(limit)s' for '%(resource)s' is too small"
-                " - must be at least '-1'.")
+    internal_message = _("The server rejected the request due to its size or "
+                         "rate.")
 
 
 class QuotaExceeded(TroveError):
 
     message = _("Quota exceeded for resources: %(overs)s.")
 
 
@@ -263,14 +223,20 @@
 
 
 class GuestTimeout(TroveError):
 
     message = _("Timeout trying to connect to the Guest Agent.")
 
 
+class BadRequest(TroveError):
+
+    message = _("The server could not comply with the request since it is "
+                "either malformed or otherwise incorrect.")
+
+
 class MissingKey(BadRequest):
 
     message = _("Required element/key - %(key)s was not specified.")
 
 
 class DatabaseAlreadyExists(BadRequest):
 
@@ -309,15 +275,15 @@
 
     message = _("Cannot find the volumes attached to compute "
                 "instance %(server_id)s.")
 
 
 class VolumeCreationFailure(TroveError):
 
-    message = _("Failed to create volume.")
+    message = _("Failed to create a volume in Nova.")
 
 
 class VolumeSizeNotSpecified(BadRequest):
 
     message = _("Volume size was not specified.")
 
 
@@ -343,24 +309,14 @@
 
 
 class ReplicationSlaveAttachError(TroveError):
 
     message = _("Exception encountered attaching slave to new replica source.")
 
 
-class SlaveOperationNotSupported(TroveError):
-    message = _("The '%(operation)s' operation is not supported for slaves in "
-                "replication.")
-
-
-class UnableToDetermineLastMasterGTID(TroveError):
-    message = _("Unable to determine last GTID executed on master "
-                "(from file %(binlog_file)s).")
-
-
 class TaskManagerError(TroveError):
 
     message = _("An error occurred communicating with the task manager: "
                 "%(original_message)s.")
 
 
 class BadValue(TroveError):
@@ -432,24 +388,41 @@
     message = _("Unable to create Backup.")
 
 
 class BackupUpdateError(TroveError):
     message = _("Unable to update Backup table in database.")
 
 
+class SecurityGroupCreationError(TroveError):
+
+    message = _("Failed to create Security Group.")
+
+
 class SecurityGroupDeletionError(TroveError):
 
     message = _("Failed to delete Security Group.")
 
 
+class SecurityGroupRuleCreationError(TroveError):
+
+    message = _("Failed to create Security Group Rule.")
+
+
 class SecurityGroupRuleDeletionError(TroveError):
 
     message = _("Failed to delete Security Group Rule.")
 
 
+class MalformedSecurityGroupRuleError(TroveError):
+
+    message = _("Error creating security group rules."
+                " Malformed port(s). Port must be an integer."
+                " FromPort = %(from)s greater than ToPort = %(to)s.")
+
+
 class BackupNotCompleteError(TroveError):
 
     message = _("Unable to create instance because backup %(backup_id)s is "
                 "not completed. Actual state: %(state)s.")
 
 
 class BackupFileNotFound(NotFound):
@@ -459,36 +432,22 @@
 
 class BackupDatastoreMismatchError(TroveError):
     message = _("The datastore from which the backup was taken, "
                 "%(datastore1)s, does not match the destination"
                 " datastore of %(datastore2)s.")
 
 
-class BackupTooLarge(TroveError):
-    message = _("Backup is too large for given flavor or volume. "
-                "Backup size: %(backup_size)s GBs. "
-                "Available size: %(disk_size)s GBs.")
-
-
-class ReplicaCreateWithUsersDatabasesError(TroveError):
-    message = _("Cannot create a replica with users or databases.")
-
-
 class SwiftAuthError(TroveError):
     message = _("Swift account not accessible for tenant %(tenant_id)s.")
 
 
 class SwiftNotFound(TroveError):
     message = _("Swift is disabled for tenant %(tenant_id)s.")
 
 
-class SwiftConnectionError(TroveError):
-    message = _("Cannot connect to Swift.")
-
-
 class DatabaseForUserNotInDatabaseListError(TroveError):
     message = _("The request indicates that user %(user)s should have access "
                 "to database %(database)s, but database %(database)s is not "
                 "included in the initial databases list.")
 
 
 class DatabaseInitialDatabaseDuplicateError(TroveError):
@@ -625,31 +584,14 @@
     message = _("The network for each instance in a cluster must be the same.")
 
 
 class NetworkNotFound(TroveError):
     message = _("Network Resource %(uuid)s cannot be found.")
 
 
-class PublicNetworkNotFound(TroveError):
-    message = _("Public network cannot be found.")
-
-
-class NetworkConflict(BadRequest):
-    message = _("User network conflicts with the reserved network.")
-
-
-class NetworkNotProvided(BadRequest):
-    message = _("Instance %(resource)s needs to be specified.")
-
-
-class SubnetNotFound(BadRequest):
-    message = _("Subnet %(subnet_id)s not found in the network "
-                "%(network_id)s.")
-
-
 class ClusterVolumeSizeRequired(TroveError):
     message = _("A volume size is required for each instance in the cluster.")
 
 
 class ClusterVolumeSizesNotEqual(TroveError):
     message = _("The volume size for each instance in a cluster must be "
                 "the same.")
@@ -663,15 +605,15 @@
 class ClusterNumInstancesNotLargeEnough(TroveError):
     message = _("The number of instances for your initial cluster must "
                 "be at least %(num_instances)s.")
 
 
 class ClusterNumInstancesBelowSafetyThreshold(TroveError):
     message = _("The number of instances in your cluster cannot "
-                "safely be lowered below the current level based "
+                "safely be lowered below the current level based"
                 "on your current fault-tolerance settings.")
 
 
 class ClusterShrinkMustNotLeaveClusterEmpty(TroveError):
     message = _("Must leave at least one instance in the cluster when "
                 "shrinking.")
 
@@ -696,25 +638,26 @@
 
 
 class ClusterDatastoreNotSupported(TroveError):
     message = _("Clusters not supported for "
                 "%(datastore)s-%(datastore_version)s.")
 
 
+class BackupTooLarge(TroveError):
+    message = _("Backup is too large for given flavor or volume. "
+                "Backup size: %(backup_size)s GBs. "
+                "Available size: %(disk_size)s GBs.")
+
+
 class ImageNotFound(NotFound):
 
     message = _("Image %(uuid)s cannot be found.")
 
 
-class ImageNotFoundByTags(NotFound):
+class DatastoreVersionAlreadyExists(BadRequest):
 
-    message = _("Failed to retrieve image with tags: %(tags)s.")
+    message = _("A datastore version with the name '%(name)s' already exists.")
 
 
 class LogAccessForbidden(Forbidden):
 
     message = _("You must be admin to %(action)s log '%(log)s'.")
-
-
-class LogsNotAvailable(Forbidden):
-
-    message = _("Log actions are not supported.")
```

### Comparing `trove-21.0.0.0rc2/trove/common/extensions.py` & `trove-8.0.1/trove/common/extensions.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,29 +15,31 @@
 
 import abc
 
 from lxml import etree
 from oslo_log import log as logging
 from oslo_utils import encodeutils
 import routes
+import six
 import stevedore
 import webob.dec
 import webob.exc
 
 from trove.common import base_exception as exception
 from trove.common import base_wsgi
 from trove.common.i18n import _
 from trove.common import wsgi
 
 LOG = logging.getLogger(__name__)
 DEFAULT_XMLNS = "http://docs.openstack.org/trove"
 XMLNS_ATOM = "http://www.w3.org/2005/Atom"
 
 
-class ExtensionDescriptor(object, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class ExtensionDescriptor(object):
     """Base class that defines the contract for extensions.
 
     Note that you don't have to derive from this class to have a valid
     extension; it is purely a convenience.
 
     """
     @abc.abstractmethod
@@ -391,15 +393,15 @@
         try:
             LOG.debug('Ext name: %s', extension.get_name())
             LOG.debug('Ext alias: %s', extension.get_alias())
             LOG.debug('Ext description: %s', extension.get_description())
             LOG.debug('Ext namespace: %s', extension.get_namespace())
             LOG.debug('Ext updated: %s', extension.get_updated())
         except AttributeError as ex:
-            LOG.exception("Exception loading extension: %s",
+            LOG.exception(_("Exception loading extension: %s"),
                           encodeutils.exception_to_unicode(ex))
             return False
         return True
 
     def _check_load_extension(self, ext):
         LOG.debug('Ext: %s', ext.obj)
         return isinstance(ext.obj, ExtensionDescriptor)
```

### Comparing `trove-21.0.0.0rc2/trove/common/i18n.py` & `trove-8.0.1/trove/tests/unittests/util/util.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,31 +1,33 @@
-#    Copyright 2014 Tesora, Inc.
+#    Copyright 2012 OpenStack Foundation
+#
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-"""oslo.i18n integration module.
-
-See https://docs.openstack.org/oslo.i18n/latest/user/index.html
-
-"""
-
-import oslo_i18n
-
-
-# NOTE(dhellmann): This reference to o-s-l-o will be replaced by the
-# application name when this module is synced into the separate
-# repository. It is OK to have more than one translation function
-# using the same domain, since there will still only be one message
-# catalog.
-_translators = oslo_i18n.TranslatorFactory(domain='trove')
+import threading
 
-# The primary translation function using the well-known name "_"
-_ = _translators.primary
+from trove.common import cfg
+from trove.db import get_db_api
+from trove.db.sqlalchemy import session
+
+CONF = cfg.CONF
+DB_SETUP = None
+LOCK = threading.Lock()
+
+
+def init_db():
+    with LOCK:
+        global DB_SETUP
+        if not DB_SETUP:
+            db_api = get_db_api()
+            db_api.db_sync(CONF)
+            session.configure_db(CONF)
+            DB_SETUP = True
```

### Comparing `trove-21.0.0.0rc2/trove/common/limits.py` & `trove-8.0.1/trove/common/limits.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,17 +19,17 @@
 
 import collections
 import copy
 import math
 import re
 import time
 
-from http import client as http_client
 from oslo_serialization import jsonutils
 from oslo_utils import importutils
+from six.moves import http_client
 import webob.dec
 import webob.exc
 
 from trove.common import base_wsgi
 from trove.common import cfg
 from trove.common.i18n import _
 from trove.common import wsgi
@@ -142,15 +142,14 @@
             "regex": self.regex,
             "value": self.value,
             "remaining": int(self.remaining),
             "unit": self.display_unit(),
             "resetTime": int(self.next_request or self._get_time()),
         }
 
-
 # "Limit" format is a dictionary with the HTTP verb, human-readable URI,
 # a regular-expression to match, value and unit of measure (PER_DAY, etc.)
 DEFAULT_LIMITS = [
     Limit("POST", "*", ".*", CONF.http_post_rate, PER_MINUTE),
     Limit("PUT", "*", ".*", CONF.http_put_rate, PER_MINUTE),
     Limit("DELETE", "*", ".*", CONF.http_delete_rate, PER_MINUTE),
     Limit("GET", "*", ".*", CONF.http_get_rate, PER_MINUTE),
@@ -201,15 +200,15 @@
         """
         verb = req.method
         url = req.url
         context = req.environ.get(wsgi.CONTEXT_KEY)
 
         tenant_id = None
         if context:
-            tenant_id = context.project_id
+            tenant_id = context.tenant
 
         delay, error = self._limiter.check_for_delay(verb, url, tenant_id)
 
         if delay and self.enabled():
             msg = _("This request was rate-limited.")
             retry = time.time() + delay
             return wsgi.OverLimitFault(msg, error, retry)
```

### Comparing `trove-21.0.0.0rc2/trove/common/local.py` & `trove-8.0.1/trove/common/local.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/models.py` & `trove-8.0.1/trove/common/models.py`

 * *Files 3% similar despite different names*

```diff
@@ -14,16 +14,16 @@
 #    under the License.
 
 """Model classes that form the core of instances functionality."""
 
 from oslo_utils.importutils import import_class
 
 from trove.common import cfg
-from trove.common import clients
 from trove.common.i18n import _
+from trove.common import remote
 
 CONF = cfg.CONF
 
 
 class ModelBase(object):
     """
     An object which can be stored in the database.
@@ -110,22 +110,22 @@
         return cls.network_driver(context, region_name)
 
 
 class NovaRemoteModelBase(RemoteModelBase):
 
     @classmethod
     def get_client(cls, context, region_name):
-        return clients.create_nova_client(context, region_name)
+        return remote.create_nova_client(context, region_name)
 
 
 class SwiftRemoteModelBase(RemoteModelBase):
 
     @classmethod
     def get_client(cls, context, region_name):
-        return clients.create_swift_client(context, region_name)
+        return remote.create_swift_client(context, region_name)
 
 
 class CinderRemoteModelBase(RemoteModelBase):
 
     @classmethod
     def get_client(cls, context):
-        return clients.create_cinder_client(context)
+        return remote.create_cinder_client(context)
```

### Comparing `trove-21.0.0.0rc2/trove/common/notification.py` & `trove-8.0.1/trove/common/notification.py`

 * *Files 4% similar despite different names*

```diff
@@ -111,15 +111,15 @@
                 'instance_type_id': instance.flavor_id,
                 'launched_at': created_time,
                 'nova_instance_id': instance.server_id,
                 'region': CONF.region,
                 'state_description': instance.status.lower(),
                 'state': instance.status.lower(),
                 'tenant_id': instance.tenant_id,
-                'user_id': instance.context.user_id,
+                'user_id': instance.context.user,
             })
 
         self.payload.update(kwargs)
 
     def serialize(self, ctxt):
 
         if hasattr(self, 'instance'):
@@ -166,23 +166,19 @@
 
     def serialize(self, ctxt):
         if hasattr(self, 'instance'):
             instance = self.instance
             if 'instance_type' not in self.payload:
                 flavor = instance.nova_client.flavors.get(instance.flavor_id)
                 self.payload['instance_size'] = flavor.ram
-            if self.server is None and instance.server_id:
-                try:
-                    self.server = instance.nova_client.servers.get(
-                        instance.server_id)
-                except Exception:
-                    pass
-            if self.server:
-                self.payload['availability_zone'] = getattr(
-                    self.server, 'OS-EXT-AZ:availability_zone', None)
+            if self.server is None:
+                self.server = instance.nova_client.servers.get(
+                    instance.server_id)
+            self.payload['availability_zone'] = getattr(
+                self.server, 'OS-EXT-AZ:availability_zone', None)
             if CONF.get(instance.datastore_version.manager).volume_support:
                 self.payload.update({
                     'volume_size': instance.volume_size,
                     'nova_volume_id': instance.volume_id
                 })
 
         return TroveBaseTraits.serialize(self, ctxt)
@@ -201,14 +197,31 @@
     def __init__(self, **kwargs):
         super(TroveInstanceCreate, self).__init__(**kwargs)
 
     def notify(self):
         super(TroveInstanceCreate, self).notify('create')
 
 
+class TroveInstanceModifyVolume(TroveCommonTraits):
+
+    '''
+    Additional traits for trove.instance.create notifications that describe
+    instance action events
+
+    This class should correspond to trove_instance_modify_volume in
+    ceilometer/event_definitions.yaml
+    '''
+
+    def __init__(self, **kwargs):
+        super(TroveInstanceModifyVolume, self).__init__(**kwargs)
+
+    def notify(self):
+        super(TroveInstanceModifyVolume, self).notify('modify_volume')
+
+
 class TroveInstanceModifyFlavor(TroveCommonTraits):
 
     '''
     Additional traits for trove.instance.create notifications that describe
     instance action events
 
     This class should correspond to trove_instance_modify_flavor in
@@ -347,15 +360,15 @@
         if 'request' in kwargs:
             request = kwargs.pop('request')
             self.payload.update({
                                 'request_id': context.request_id,
                                 'server_type': 'api',
                                 'client_ip': request.remote_addr,
                                 'server_ip': request.host,
-                                'tenant_id': context.project_id,
+                                'tenant_id': context.tenant,
                                 })
         elif 'request_id' not in kwargs:
             raise TroveError(_("Notification %s must include 'request'"
                                " property") % self.__class__.__name__)
 
         self.payload.update(kwargs)
 
@@ -430,23 +443,14 @@
                 'replica_of', 'replica_count', 'cluster_id', 'backup_id',
                 'nics']
 
     def required_end_traits(self):
         return ['instance_id']
 
 
-class DBaaSInstanceReboot(DBaaSAPINotification):
-
-    def event_type(self):
-        return 'instance_reboot'
-
-    def required_start_traits(self):
-        return ['instance_id']
-
-
 class DBaaSInstanceRestart(DBaaSAPINotification):
 
     def event_type(self):
         return 'instance_restart'
 
     def required_start_traits(self):
         return ['instance_id']
@@ -641,33 +645,25 @@
     def event_type(self):
         return 'cluster_grow'
 
     @abc.abstractmethod
     def required_start_traits(self):
         return ['cluster_id']
 
-    @abc.abstractmethod
-    def required_end_traits(self):
-        return ['cluster_id']
-
 
 class DBaaSClusterShrink(DBaaSAPINotification):
 
     @abc.abstractmethod
     def event_type(self):
         return 'cluster_shrink'
 
     @abc.abstractmethod
     def required_start_traits(self):
         return ['cluster_id']
 
-    @abc.abstractmethod
-    def required_end_traits(self):
-        return ['cluster_id']
-
 
 class DBaaSBackupCreate(DBaaSAPINotification):
 
     @abc.abstractmethod
     def event_type(self):
         return 'backup_create'
```

### Comparing `trove-21.0.0.0rc2/trove/common/pagination.py` & `trove-8.0.1/trove/common/pagination.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import bisect
 import collections
-import urllib.parse as urllib_parse
+import six.moves.urllib.parse as urllib_parse
 
 
 def url_quote(s):
     if s is None:
         return s
     return urllib_parse.quote(str(s))
```

### Comparing `trove-21.0.0.0rc2/trove/common/pastedeploy.py` & `trove-8.0.1/trove/common/pastedeploy.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/policies/flavors.py` & `trove-8.0.1/trove/extensions/cassandra/service.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,42 +1,28 @@
+# Copyright 2015 Tesora Inc.
+# All Rights Reserved.
+#
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from oslo_policy import policy
-
-from trove.common.policies.base import PATH_FLAVORS, PATH_FLAVOR
+from trove.common.db.cassandra import models as guest_models
+from trove.extensions.common.service import DefaultRootController
+from trove.extensions.mysql import models
 
-rules = [
-    policy.DocumentedRuleDefault(
-        name='flavor:index',
-        check_str='',
-        description='List all flavors.',
-        operations=[
-            {
-                'path': PATH_FLAVORS,
-                'method': 'GET'
-            }
-        ]),
-    policy.DocumentedRuleDefault(
-        name='flavor:show',
-        check_str='',
-        description='Get information of a flavor.',
-        operations=[
-            {
-                'path': PATH_FLAVOR,
-                'method': 'GET'
-            }
-        ])
-]
 
+class CassandraRootController(DefaultRootController):
 
-def list_rules():
-    return rules
+    def _find_root_user(self, context, instance_id):
+        user = guest_models.CassandraUser.root()
+        # TODO(pmalik): Using MySQL model until we have datastore specific
+        # extensions (bug/1498573).
+        return models.User.load(
+            context, instance_id, user.name, user.host, root_user=True)
```

### Comparing `trove-21.0.0.0rc2/trove/common/policies/limits.py` & `trove-8.0.1/integration/tests/examples/setup.py`

 * *Files 17% similar despite different names*

```diff
@@ -6,27 +6,25 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from oslo_policy import policy
+import os
+from setuptools import setup
 
-from trove.common.policies.base import PATH_LIMITS
 
-rules = [
-    policy.DocumentedRuleDefault(
-        name='limits:index',
-        check_str='rule:admin_or_owner',
-        description='List all absolute and rate limit informations.',
-        operations=[
-            {
-                'path': PATH_LIMITS,
-                'method': 'GET'
-            }
-        ])
-]
+def read(fname):
+    return open(os.path.join(os.path.dirname(__file__), fname)).read()
 
 
-def list_rules():
-    return rules
+setup(
+    name="Trove Example Generator",
+    version="0.0.9.9",
+    author='OpenStack',
+    description="Generates documentation examples.",
+    license='Apache',
+    py_modules=[],
+    packages=['examples'],
+    scripts=[]
+)
```

### Comparing `trove-21.0.0.0rc2/trove/common/profile.py` & `trove-8.0.1/trove/common/profile.py`

 * *Files 10% similar despite different names*

```diff
@@ -16,32 +16,33 @@
 from oslo_context import context
 from oslo_log import log as logging
 import oslo_messaging as messaging
 from osprofiler import notifier
 from osprofiler import web
 
 from trove.common import cfg
+from trove.common.i18n import _LW
 from trove import rpc
 
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
 
 def setup_profiler(binary, host):
     if CONF.profiler.enabled:
         _notifier = notifier.create(
             "Messaging", messaging, context.get_admin_context().to_dict(),
             rpc.TRANSPORT, "trove", binary, host)
         notifier.set(_notifier)
         web.enable(CONF.profiler.hmac_keys)
-        LOG.warning("The OpenStack Profiler is enabled. Using one"
-                    " of the hmac_keys specified in the trove.conf file "
-                    "(typically in /etc/trove), a trace can be made of "
-                    "all requests. Only an admin user can retrieve "
-                    "the trace information, however.\n"
-                    "To disable the profiler, add the following to the "
-                    "configuration file:\n"
-                    "[profiler]\n"
-                    "enabled=false")
+        LOG.warning(_LW("The OpenStack Profiler is enabled. Using one"
+                        " of the hmac_keys specified in the trove.conf file "
+                        "(typically in /etc/trove), a trace can be made of "
+                        "all requests. Only an admin user can retrieve "
+                        "the trace information, however.\n"
+                        "To disable the profiler, add the following to the "
+                        "configuration file:\n"
+                        "[profiler]\n"
+                        "enabled=false"))
     else:
         web.disable()
```

### Comparing `trove-21.0.0.0rc2/trove/common/rpc/conductor_guest_serializer.py` & `trove-8.0.1/trove/common/rpc/conductor_guest_serializer.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/rpc/conductor_host_serializer.py` & `trove-8.0.1/trove/common/rpc/conductor_host_serializer.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/rpc/secure_serializer.py` & `trove-8.0.1/trove/common/rpc/secure_serializer.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/rpc/serializer.py` & `trove-8.0.1/trove/common/rpc/serializer.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 import oslo_messaging as messaging
 from osprofiler import profiler
 
 from trove.common.context import TroveContext
 
 
 class TroveSerializer(messaging.Serializer):
-    """The Trove serializer class that handles class inheritance and base
+    """The Trove serializer class that handles class inheritence and base
        serializers.
     """
 
     def __init__(self, base):
         self._base = base
 
     def _serialize_entity(self, context, entity):
```

### Comparing `trove-21.0.0.0rc2/trove/common/rpc/service.py` & `trove-8.0.1/trove/common/rpc/service.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 import oslo_messaging as messaging
 from oslo_service import loopingcall
 from oslo_service import service
 from oslo_utils import importutils
 from osprofiler import profiler
 
 from trove.common import cfg
+from trove.common.i18n import _
 from trove.common import profile
 from trove.common.rpc import secure_serializer as ssz
 from trove import rpc
 
 
 CONF = cfg.CONF
 LOG = logging.getLogger(__name__)
@@ -78,11 +79,11 @@
 
     def stop(self):
         # Try to shut the connection down, but if we get any sort of
         # errors, go ahead and ignore them.. as we're shutting down anyway
         try:
             self.rpcserver.stop()
         except Exception:
-            LOG.info("Failed to stop RPC server before shutdown. ")
+            LOG.info(_("Failed to stop RPC server before shutdown. "))
             pass
 
         super(RpcService, self).stop()
```

### Comparing `trove-21.0.0.0rc2/trove/common/rpc/version.py` & `trove-8.0.1/integration/tests/integration/setup.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,26 +1,30 @@
-# Copyright 2011 OpenStack Foundation.
-# All Rights Reserved.
-#
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-# based on configured release version
-RPC_API_VERSION = "1.0"
+import os
+from setuptools import setup
 
-# API version history:
-#
-# 1.0 - Initial version.  (We started keeping track at icehouse-3)
-# 1.1 -
-# 1.2 - ...
-VERSION_ALIASES = {
-    'icehouse': '1.0'
-}
+
+def read(fname):
+    return open(os.path.join(os.path.dirname(__file__), fname)).read()
+
+
+setup(
+    name="Trove Integration Tests",
+    version="0.0.9.9",
+    author='OpenStack',
+    description="Runs integration tests on Ridley.",
+    license='Apache',
+    py_modules=[],
+    packages=['tests'],
+    scripts=[]
+)
```

### Comparing `trove-21.0.0.0rc2/trove/common/serializable_notification.py` & `trove-8.0.1/trove/common/serializable_notification.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/server_group.py` & `trove-8.0.1/trove/common/server_group.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,40 +10,38 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
+import six
+
 from oslo_log import log as logging
 
-from trove.common.clients import create_nova_client
+from trove.common.i18n import _
+from trove.common.remote import create_nova_client
 
 
 LOG = logging.getLogger(__name__)
 
 
 class ServerGroup(object):
 
     @classmethod
-    def load(cls, context, instance_id):
+    def load(cls, context, compute_id):
         client = create_nova_client(context)
         server_group = None
-        expected_name = "locality_%s" % instance_id
         try:
             for sg in client.server_groups.list():
-                if sg.name == expected_name:
+                if compute_id in sg.members:
                     server_group = sg
         except Exception:
-            LOG.exception("Could not load server group for instance %s",
-                          instance_id)
-
-        if not server_group:
-            LOG.info('No server group found for instance %s', instance_id)
-
+            LOG.exception(_("Could not load server group for compute %s"),
+                          compute_id)
         return server_group
 
     @classmethod
     def create(cls, context, locality, name_suffix):
         client = create_nova_client(context)
         server_group_name = "%s_%s" % ('locality', name_suffix)
         server_group = client.server_groups.create(
@@ -57,17 +55,17 @@
 
     @classmethod
     def delete(cls, context, server_group, force=False):
         # Only delete the server group if we're the last member in it, or if
         # it has no members
         if server_group:
             if force or len(server_group.members) <= 1:
-                LOG.info("Deleting server group %s", server_group.id)
                 client = create_nova_client(context)
                 client.server_groups.delete(server_group.id)
+                LOG.debug("Deleted server group %s.", server_group.id)
             else:
                 LOG.debug("Skipping delete of server group %(id)s "
                           "(members: %(members)s).",
                           {'id': server_group.id,
                            'members': server_group.members})
 
     @classmethod
@@ -78,15 +76,15 @@
         return hints
 
     @classmethod
     def build_scheduler_hint(cls, context, locality, name_suffix):
         scheduler_hint = None
         if locality:
             # Build the scheduler hint, but only if locality's a string
-            if isinstance(locality, str):
+            if isinstance(locality, six.string_types):
                 server_group = cls.create(
                     context, locality, name_suffix)
                 scheduler_hint = cls.convert_to_hint(
                     server_group)
             else:
                 # otherwise assume it's already in hint form (i.e. a dict)
                 scheduler_hint = locality
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/base.py` & `trove-8.0.1/trove/common/strategies/cluster/base.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/api.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/api.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,15 +17,16 @@
 
 from trove.cluster import models
 from trove.cluster.tasks import ClusterTasks
 from trove.cluster.views import ClusterView
 from trove.common import cfg
 from trove.common import server_group as srv_grp
 from trove.common.strategies.cluster import base
-from trove.common.strategies.cluster.experimental.cassandra import taskmanager
+from trove.common.strategies.cluster.experimental.cassandra.taskmanager import(
+    CassandraClusterTasks)
 from trove.common import utils
 from trove.extensions.mgmt.clusters.views import MgmtClusterView
 from trove.instance import models as inst_models
 from trove.quota.quota import check_quotas
 from trove.taskmanager import api as task_api
 
 
@@ -77,21 +78,20 @@
 class CassandraCluster(models.Cluster):
 
     DEFAULT_DATA_CENTER = "dc1"
     DEFAULT_RACK = "rack1"
 
     @classmethod
     def create(cls, context, name, datastore, datastore_version,
-               instances, extended_properties, locality, configuration,
-               image_id=None):
+               instances, extended_properties, locality, configuration):
         LOG.debug("Processing a request for creating a new cluster.")
 
         # Updating Cluster Task.
         db_info = models.DBCluster.create(
-            name=name, tenant_id=context.project_id,
+            name=name, tenant_id=context.tenant,
             datastore_version_id=datastore_version.id,
             task_status=ClusterTasks.BUILDING_INITIAL,
             configuration_id=configuration)
 
         cls._create_cluster_instances(
             context, db_info.id, db_info.name,
             datastore, datastore_version, instances, extended_properties,
@@ -121,24 +121,19 @@
         # Compute the total volume allocation.
         req_volume_size = models.get_required_volume_size(instances,
                                                           vol_enabled)
 
         # Check requirements against quota.
         num_new_instances = len(instances)
         deltas = {'instances': num_new_instances, 'volumes': req_volume_size}
-        models.assert_homogeneous_cluster(instances)
-        check_quotas(context.project_id, deltas)
-
-        # Checking networks are same for the cluster
-        models.validate_instance_nics(context, instances)
+        check_quotas(context.tenant, deltas)
 
         # Creating member instances.
         num_instances = len(
-            taskmanager.CassandraClusterTasks.find_cluster_node_ids(cluster_id)
-        )
+            CassandraClusterTasks.find_cluster_node_ids(cluster_id))
         new_instances = []
         for instance_idx, instance in enumerate(instances, num_instances + 1):
             instance_az = instance.get('availability_zone', None)
 
             member_config = {"id": cluster_id,
                              "instance_type": "member",
                              "dc": cls.DEFAULT_DATA_CENTER,
@@ -157,15 +152,14 @@
                 [], [],
                 datastore, datastore_version,
                 instance['volume_size'], None,
                 nics=instance.get('nics', None),
                 availability_zone=instance_az,
                 configuration_id=configuration_id,
                 cluster_config=member_config,
-                volume_type=instance.get('volume_type', None),
                 modules=instance.get('modules'),
                 locality=locality,
                 region_name=instance.get('region_name'))
 
             new_instances.append(new_instance)
 
         return new_instances
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/guestagent.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/guestagent.py`

 * *Files 14% similar despite different names*

```diff
@@ -41,59 +41,59 @@
     appropriate in this file
     """
 
     def get_data_center(self):
         LOG.debug("Retrieving the data center for node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_data_center", self.agent_low_timeout,
+        return self._call("get_data_center", guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def get_rack(self):
         LOG.debug("Retrieving the rack for node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_rack", self.agent_low_timeout,
+        return self._call("get_rack", guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def set_seeds(self, seeds):
         LOG.debug("Configuring the gossip seeds for node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("set_seeds", self.agent_low_timeout,
+        return self._call("set_seeds", guest_api.AGENT_LOW_TIMEOUT,
                           version=version, seeds=seeds)
 
     def get_seeds(self):
         LOG.debug("Retrieving the gossip seeds for node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_seeds", self.agent_low_timeout,
+        return self._call("get_seeds", guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def set_auto_bootstrap(self, enabled):
         LOG.debug("Setting the auto-bootstrap to '%(enabled)s' "
                   "for node: %(id)s", {'enabled': enabled, 'id': self.id})
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("set_auto_bootstrap", self.agent_low_timeout,
+        return self._call("set_auto_bootstrap", guest_api.AGENT_LOW_TIMEOUT,
                           version=version, enabled=enabled)
 
     def cluster_complete(self):
         LOG.debug("Sending a setup completion notification for node: %s",
                   self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("cluster_complete", self.agent_high_timeout,
+        return self._call("cluster_complete", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def node_cleanup_begin(self):
         LOG.debug("Signaling the node to prepare for cleanup: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("node_cleanup_begin", self.agent_low_timeout,
+        return self._call("node_cleanup_begin", guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def node_cleanup(self):
         LOG.debug("Running cleanup on node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
         return self._cast('node_cleanup', version=version)
@@ -105,25 +105,25 @@
         return self._cast("node_decommission", version=version)
 
     def cluster_secure(self, password):
         LOG.debug("Securing the cluster via node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
         return self._call(
-            "cluster_secure", self.agent_high_timeout,
+            "cluster_secure", guest_api.AGENT_HIGH_TIMEOUT,
             version=version, password=password)
 
     def get_admin_credentials(self):
         LOG.debug("Retrieving the admin credentials from node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_admin_credentials", self.agent_low_timeout,
+        return self._call("get_admin_credentials", guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def store_admin_credentials(self, admin_credentials):
         LOG.debug("Storing the admin credentials on node: %s", self.id)
         version = guest_api.API.API_BASE_VERSION
 
         return self._call("store_admin_credentials",
-                          self.agent_low_timeout,
+                          guest_api.AGENT_LOW_TIMEOUT,
                           version=version,
                           admin_credentials=admin_credentials)
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/cassandra/taskmanager.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/cassandra/taskmanager.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,25 +13,26 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from eventlet.timeout import Timeout
 from oslo_log import log as logging
 
 from trove.common import cfg
+from trove.common.i18n import _
 from trove.common.strategies.cluster import base
 from trove.common import utils
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
-from trove.instance import tasks as inst_tasks
 from trove.taskmanager import api as task_api
 import trove.taskmanager.models as task_models
 
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+USAGE_SLEEP_TIME = CONF.usage_sleep_time  # seconds.
 
 
 class CassandraTaskManagerStrategy(base.BaseTaskManagerStrategy):
 
     @property
     def task_manager_api_class(self):
         return CassandraTaskManagerAPI
@@ -93,35 +94,34 @@
                         admin_creds = node['guest'].cluster_secure(key)
                     else:
                         node['guest'].store_admin_credentials(admin_creds)
                     node['guest'].cluster_complete()
 
                 LOG.debug("Cluster configuration finished successfully.")
             except Exception:
-                LOG.exception("Error creating cluster.")
+                LOG.exception(_("Error creating cluster."))
                 self.update_statuses_on_failure(cluster_id)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _create_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for building cluster.")
+            LOG.exception(_("Timeout for building cluster."))
             self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End create_cluster for id: %s.", cluster_id)
 
     @classmethod
     def find_cluster_node_ids(cls, cluster_id):
-        db_instances = DBInstance.find_all(cluster_id=cluster_id,
-                                           deleted=False).all()
+        db_instances = DBInstance.find_all(cluster_id=cluster_id).all()
         return [db_instance.id for db_instance in db_instances]
 
     @classmethod
     def load_cluster_nodes(cls, context, node_ids):
         return [cls.build_node_info(Instance.load(context, node_id))
                 for node_id in node_ids]
 
@@ -234,34 +234,32 @@
                 LOG.debug("Cleaning up orphan data on old cluster nodes.")
                 for node in old_nodes:
                     nid = node['id']
                     node['guest'].node_cleanup_begin()
                     node['guest'].node_cleanup()
                     LOG.debug("Waiting for node to finish its "
                               "cleanup: %s", nid)
-                    if not self._all_instances_healthy([nid], cluster_id):
-                        LOG.warning("Node did not complete cleanup "
-                                    "successfully: %s", nid)
+                    if not self._all_instances_running([nid], cluster_id):
+                        LOG.warning(_("Node did not complete cleanup "
+                                      "successfully: %s"), nid)
 
                 LOG.debug("Cluster configuration finished successfully.")
             except Exception:
-                LOG.exception("Error growing cluster.")
-                self.update_statuses_on_failure(
-                    cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+                LOG.exception(_("Error growing cluster."))
+                self.update_statuses_on_failure(cluster_id)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _grow_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for growing cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+            LOG.exception(_("Timeout for growing cluster."))
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End grow_cluster for id: %s.", cluster_id)
 
     def shrink_cluster(self, context, cluster_id, removal_ids):
         LOG.debug("Begin shrink_cluster for id: %s.", cluster_id)
@@ -277,15 +275,19 @@
 
             # Update the list of seeds on remaining nodes if necessary.
             # Once all nodes are configured, decommission the removed nodes.
             # Cassandra will stream data from decommissioned nodes to the
             # remaining ones.
             try:
 
-                current_seeds = self._get_current_seeds(context, cluster_id)
+                # All nodes should have the same seeds.
+                # We retrieve current seeds from the first node.
+                test_node = self.load_cluster_nodes(
+                    context, cluster_node_ids[:1])[0]
+                current_seeds = test_node['guest'].get_seeds()
                 # The seeds will have to be updated on all remaining instances
                 # if any of the seed nodes is going to be removed.
                 update_seeds = any(node['ip'] in current_seeds
                                    for node in removed_nodes)
 
                 LOG.debug("Decommissioning removed nodes.")
                 for node in removed_nodes:
@@ -318,54 +320,34 @@
                 # consistent state.
                 LOG.debug("Deleting decommissioned instances.")
                 for node in removed_nodes:
                     Instance.delete(node['instance'])
 
                 LOG.debug("Cluster configuration finished successfully.")
             except Exception:
-                LOG.exception("Error shrinking cluster.")
-                self.update_statuses_on_failure(
-                    cluster_id,
-                    status=inst_tasks.InstanceTasks.SHRINKING_ERROR)
+                LOG.exception(_("Error shrinking cluster."))
+                self.update_statuses_on_failure(cluster_id)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _shrink_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for shrinking cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.SHRINKING_ERROR)
+            LOG.exception(_("Timeout for shrinking cluster."))
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End shrink_cluster for id: %s.", cluster_id)
 
     def restart_cluster(self, context, cluster_id):
         self.rolling_restart_cluster(
             context, cluster_id, delay_sec=CONF.cassandra.node_sync_time)
 
     def upgrade_cluster(self, context, cluster_id, datastore_version):
-        current_seeds = self._get_current_seeds(context, cluster_id)
-
-        def ordering_function(instance):
-
-            if self.get_ip(instance) in current_seeds:
-                return -1
-            return 0
-
-        self.rolling_upgrade_cluster(context, cluster_id,
-                                     datastore_version, ordering_function)
-
-    def _get_current_seeds(self, context, cluster_id):
-        # All nodes should have the same seeds.
-        # We retrieve current seeds from the first node.
-        cluster_node_ids = self.find_cluster_node_ids(cluster_id)
-        test_node = self.load_cluster_nodes(context,
-                                            cluster_node_ids[:1])[0]
-        return test_node['guest'].get_seeds()
+        self.rolling_upgrade_cluster(context, cluster_id, datastore_version)
 
 
 class CassandraTaskManagerAPI(task_api.API):
     pass
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/api.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/api.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,23 +8,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from novaclient import exceptions as nova_exceptions
 from oslo_log import log as logging
 
-import time
 
 from trove.cluster import models as cluster_models
 from trove.cluster.tasks import ClusterTasks
 from trove.cluster.views import ClusterView
 from trove.common import cfg
 from trove.common import exception
+from trove.common import remote
 from trove.common import server_group as srv_grp
 from trove.common.strategies.cluster import base as cluster_base
 from trove.extensions.mgmt.clusters.views import MgmtClusterView
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
 from trove.quota.quota import check_quotas
 from trove.taskmanager import api as task_api
@@ -54,115 +55,122 @@
     @staticmethod
     def _validate_cluster_instances(context, instances, datastore,
                                     datastore_version):
         """Validate the flavor and volume"""
         ds_conf = CONF.get(datastore_version.manager)
         num_instances = len(instances)
 
+        # Check number of instances is at least min_cluster_member_count
+        if num_instances < ds_conf.min_cluster_member_count:
+            raise exception.ClusterNumInstancesNotLargeEnough(
+                num_instances=ds_conf.min_cluster_member_count)
+
         # Checking volumes and get delta for quota check
         cluster_models.validate_instance_flavors(
             context, instances, ds_conf.volume_support, ds_conf.device_path)
 
         req_volume_size = cluster_models.get_required_volume_size(
             instances, ds_conf.volume_support)
 
         cluster_models.assert_homogeneous_cluster(instances)
 
         deltas = {'instances': num_instances, 'volumes': req_volume_size}
 
         # quota check
-        check_quotas(context.project_id, deltas)
+        check_quotas(context.tenant, deltas)
 
         # Checking networks are same for the cluster
-        cluster_models.validate_instance_nics(context, instances)
+        instance_nics = []
+        for instance in instances:
+            nics = instance.get('nics')
+            if nics:
+                instance_nics.append(nics[0].get('net-id'))
+        if len(set(instance_nics)) > 1:
+            raise exception.ClusterNetworksNotEqual()
+        if not instance_nics:
+            return
+        instance_nic = instance_nics[0]
+        try:
+            nova_client = remote.create_nova_client(context)
+            nova_client.networks.get(instance_nic)
+        except nova_exceptions.NotFound:
+            raise exception.NetworkNotFound(uuid=instance_nic)
 
     @staticmethod
     def _create_instances(context, db_info, datastore, datastore_version,
                           instances, extended_properties, locality,
-                          configuration_id, image_id):
+                          configuration_id):
         member_config = {"id": db_info.id,
                          "instance_type": "member"}
-        name_index = int(time.time())
+        name_index = 1
         for instance in instances:
             if not instance.get("name"):
                 instance['name'] = "%s-member-%s" % (db_info.name,
                                                      str(name_index))
                 name_index += 1
 
         return [Instance.create(context,
                                 instance['name'],
                                 instance['flavor_id'],
-                                datastore_version.image_id
-                                if datastore_version.image_id else image_id,
+                                datastore_version.image_id,
                                 [], [],
                                 datastore, datastore_version,
                                 instance.get('volume_size', None),
                                 None,
                                 availability_zone=instance.get(
                                     'availability_zone', None),
                                 nics=instance.get('nics', None),
                                 configuration_id=configuration_id,
                                 cluster_config=member_config,
-                                volume_type=instance.get(
-                                    'volume_type', None),
                                 modules=instance.get('modules'),
                                 locality=locality,
                                 region_name=instance.get('region_name')
                                 )
                 for instance in instances]
 
     @classmethod
     def create(cls, context, name, datastore, datastore_version,
-               instances, extended_properties, locality, configuration,
-               image_id=None):
+               instances, extended_properties, locality, configuration):
         LOG.debug("Initiating Galera cluster creation.")
-        ds_conf = CONF.get(datastore_version.manager)
-        # Check number of instances is at least min_cluster_member_count
-        if len(instances) < ds_conf.min_cluster_member_count:
-            raise exception.ClusterNumInstancesNotLargeEnough(
-                num_instances=ds_conf.min_cluster_member_count)
         cls._validate_cluster_instances(context, instances, datastore,
                                         datastore_version)
         # Updating Cluster Task
         db_info = cluster_models.DBCluster.create(
-            name=name, tenant_id=context.project_id,
+            name=name, tenant_id=context.tenant,
             datastore_version_id=datastore_version.id,
             task_status=ClusterTasks.BUILDING_INITIAL,
             configuration_id=configuration)
 
         cls._create_instances(context, db_info, datastore, datastore_version,
                               instances, extended_properties, locality,
-                              configuration, image_id)
+                              configuration)
 
         # Calling taskmanager to further proceed for cluster-configuration
         task_api.load(context, datastore_version.manager).create_cluster(
             db_info.id)
 
         return cls(context, db_info, datastore, datastore_version)
 
-    def grow(self, instances, image_id=None):
+    def grow(self, instances):
         LOG.debug("Growing cluster %s.", self.id)
 
         self.validate_cluster_available()
 
         context = self.context
         db_info = self.db_info
         datastore = self.ds
         datastore_version = self.ds_version
 
-        self._validate_cluster_instances(context, instances, datastore,
-                                         datastore_version)
-
         db_info.update(task_status=ClusterTasks.GROWING_CLUSTER)
         try:
             locality = srv_grp.ServerGroup.convert_to_hint(self.server_group)
             configuration_id = self.db_info.configuration_id
             new_instances = self._create_instances(
                 context, db_info, datastore, datastore_version, instances,
-                None, locality, configuration_id, image_id)
+                None, locality, configuration_id)
 
             task_api.load(context, datastore_version.manager).grow_cluster(
                 db_info.id, [instance.id for instance in new_instances])
         except Exception:
             db_info.update(task_status=ClusterTasks.NONE)
             raise
 
@@ -172,16 +180,15 @@
     def shrink(self, instances):
         """Removes instances from a cluster."""
         LOG.debug("Shrinking cluster %s.", self.id)
 
         self.validate_cluster_available()
         removal_instances = [Instance.load(self.context, inst_id)
                              for inst_id in instances]
-        db_instances = DBInstance.find_all(
-            cluster_id=self.db_info.id, deleted=False).all()
+        db_instances = DBInstance.find_all(cluster_id=self.db_info.id).all()
         if len(db_instances) - len(removal_instances) < 1:
             raise exception.ClusterShrinkMustNotLeaveClusterEmpty()
 
         self.db_info.update(task_status=ClusterTasks.SHRINKING_CLUSTER)
         try:
             task_api.load(self.context, self.ds_version.manager
                           ).shrink_cluster(self.db_info.id,
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/guestagent.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/guestagent.py`

 * *Files 3% similar despite different names*

```diff
@@ -63,27 +63,27 @@
                    admin_password=admin_password)
 
     def cluster_complete(self):
         """Set the status that the cluster is build is complete."""
         LOG.debug("Notifying cluster install completion.")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("cluster_complete", self.agent_high_timeout,
+        return self._call("cluster_complete", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def get_cluster_context(self):
         """Get the context of the cluster."""
         LOG.debug("Getting the cluster context.")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_cluster_context", self.agent_high_timeout,
+        return self._call("get_cluster_context", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def write_cluster_configuration_overrides(self, cluster_configuration):
         """Write an updated the cluster configuration."""
         LOG.debug("Writing an updated the cluster configuration.")
         version = guest_api.API.API_BASE_VERSION
 
         self._call("write_cluster_configuration_overrides",
-                   self.agent_high_timeout,
+                   guest_api.AGENT_HIGH_TIMEOUT,
                    version=version,
                    cluster_configuration=cluster_configuration)
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/galera_common/taskmanager.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/galera_common/taskmanager.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,18 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from eventlet.timeout import Timeout
 from oslo_log import log as logging
 
 from trove.common import cfg
-from trove.common.clients import create_nova_client
 from trove.common.exception import PollTimeOut
 from trove.common.exception import TroveError
 from trove.common.i18n import _
+from trove.common.remote import create_nova_client
 from trove.common.strategies.cluster import base as cluster_base
 from trove.common.template import ClusterConfigTemplate
 from trove.common import utils
 from trove.extensions.common import models as ext_models
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
 from trove.instance import tasks as inst_tasks
@@ -82,15 +82,16 @@
                                    "ACTIVE"))
 
             LOG.debug("All members ready, proceeding for cluster setup.")
             instances = [Instance.load(context, instance_id) for instance_id
                          in instance_ids]
 
             cluster_ips = [self.get_ip(instance) for instance in instances]
-            instance_guests = []
+            instance_guests = [self.get_guest(instance)
+                               for instance in instances]
 
             # Create replication user and password for synchronizing the
             # galera cluster
             replication_user = {
                 "name": self.CLUSTER_REPLICATION_USER,
                 "password": utils.generate_random_password(),
             }
@@ -103,20 +104,21 @@
 
             LOG.debug("Configuring cluster configuration.")
             try:
                 # Set the admin password for all the instances because the
                 # password in the my.cnf will be wrong after the joiner
                 # instances syncs with the donor instance.
                 admin_password = str(utils.generate_random_password())
+                for guest in instance_guests:
+                    guest.reset_admin_password(admin_password)
 
                 bootstrap = True
                 for instance in instances:
                     guest = self.get_guest(instance)
-                    instance_guests.append(guest)
-                    guest.reset_admin_password(admin_password)
+
                     # render the conf.d/cluster.cnf configuration
                     cluster_configuration = self._render_cluster_config(
                         context,
                         instance,
                         ",".join(cluster_ips),
                         cluster_name,
                         replication_user)
@@ -127,41 +129,42 @@
                                           bootstrap)
                     bootstrap = False
 
                 LOG.debug("Finalizing cluster configuration.")
                 for guest in instance_guests:
                     guest.cluster_complete()
             except Exception:
-                LOG.exception("Error creating cluster.")
+                LOG.exception(_("Error creating cluster."))
                 self.update_statuses_on_failure(cluster_id)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _create_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for building cluster.")
+            LOG.exception(_("Timeout for building cluster."))
             self.update_statuses_on_failure(cluster_id)
         except TroveError:
-            LOG.exception("Error creating cluster %s.", cluster_id)
+            LOG.exception(_("Error creating cluster %s."), cluster_id)
             self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End create_cluster for id: %s.", cluster_id)
 
     def _check_cluster_for_root(self, context, existing_instances,
                                 new_instances):
         """Check for existing instances root enabled"""
         for instance in existing_instances:
             if ext_models.Root.load(context, instance.id):
                 for new_instance in new_instances:
-                    ext_models.RootHistory.create(context, new_instance.id)
+                    ext_models.RootHistory.create(context, new_instance.id,
+                                                  context.user)
                 return
 
     def grow_cluster(self, context, cluster_id, new_instance_ids):
         LOG.debug("Begin Galera grow_cluster for id: %s.", cluster_id)
 
         def _grow_cluster():
 
@@ -238,19 +241,19 @@
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _grow_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for growing cluster.")
+            LOG.exception(_("Timeout for growing cluster."))
             self.update_statuses_on_failure(
                 cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
         except Exception:
-            LOG.exception("Error growing cluster %s.", cluster_id)
+            LOG.exception(_("Error growing cluster %s."), cluster_id)
             self.update_statuses_on_failure(
                 cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
         finally:
             timeout.cancel()
 
         LOG.debug("End grow_cluster for id: %s.", cluster_id)
 
@@ -270,24 +273,23 @@
                 non_deleted_ids = [db_instance.id for db_instance
                                    in non_deleted_instances]
                 return not bool(
                     set(removal_instance_ids).intersection(
                         set(non_deleted_ids))
                 )
             try:
-                LOG.info("Deleting instances (%s)", removal_instance_ids)
+                LOG.info(_("Deleting instances (%s)"), removal_instance_ids)
                 utils.poll_until(all_instances_marked_deleted,
                                  sleep_time=2,
                                  time_out=CONF.cluster_delete_time_out)
             except PollTimeOut:
-                LOG.error("timeout for instances to be marked as deleted.")
+                LOG.error(_("timeout for instances to be marked as deleted."))
                 return
 
-            db_instances = DBInstance.find_all(
-                cluster_id=cluster_id, deleted=False).all()
+            db_instances = DBInstance.find_all(cluster_id=cluster_id).all()
             leftover_instances = [Instance.load(context, db_inst.id)
                                   for db_inst in db_instances
                                   if db_inst.id not in removal_instance_ids]
             leftover_cluster_ips = [self.get_ip(instance) for instance in
                                     leftover_instances]
 
             # Get config changes for left over instances
@@ -310,19 +312,19 @@
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _shrink_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for shrinking cluster.")
+            LOG.exception(_("Timeout for shrinking cluster."))
             self.update_statuses_on_failure(
                 cluster_id, status=inst_tasks.InstanceTasks.SHRINKING_ERROR)
         except Exception:
-            LOG.exception("Error shrinking cluster %s.", cluster_id)
+            LOG.exception(_("Error shrinking cluster %s."), cluster_id)
             self.update_statuses_on_failure(
                 cluster_id, status=inst_tasks.InstanceTasks.SHRINKING_ERROR)
         finally:
             timeout.cancel()
 
         LOG.debug("End shrink_cluster for id: %s.", cluster_id)
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/api.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/api.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,19 +16,19 @@
 from novaclient import exceptions as nova_exceptions
 from oslo_log import log as logging
 
 from trove.cluster import models
 from trove.cluster.tasks import ClusterTasks
 from trove.cluster.views import ClusterView
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
 from trove.common.i18n import _
 from trove.common.notification import DBaaSClusterGrow
 from trove.common.notification import StartNotification
+from trove.common import remote
 from trove.common import server_group as srv_grp
 from trove.common.strategies.cluster import base
 from trove.common import utils
 from trove.datastore import models as datastore_models
 from trove.extensions.mgmt.clusters.views import MgmtClusterView
 from trove.instance import models as inst_models
 from trove.quota.quota import check_quotas
@@ -54,79 +54,57 @@
         return MongoDbMgmtClusterView
 
 
 class MongoDbCluster(models.Cluster):
 
     @classmethod
     def create(cls, context, name, datastore, datastore_version,
-               instances, extended_properties, locality, configuration,
-               image_id=None):
+               instances, extended_properties, locality, configuration):
 
         if configuration:
             raise exception.ConfigurationNotSupported()
 
         # TODO(amcreynolds): consider moving into CONF and even supporting
         # TODO(amcreynolds): an array of values, e.g. [3, 5, 7]
         # TODO(amcreynolds): or introduce a min/max num_instances and set
         # TODO(amcreynolds): both to 3
         num_instances = len(instances)
         if num_instances != 3:
             raise exception.ClusterNumInstancesNotSupported(num_instances=3)
 
         mongo_conf = CONF.get(datastore_version.manager)
-
-        num_configsvr = int(extended_properties.get(
-            'num_configsvr', mongo_conf.num_config_servers_per_cluster))
-        num_mongos = int(extended_properties.get(
-            'num_mongos', mongo_conf.num_query_routers_per_cluster))
-
+        num_configsvr = mongo_conf.num_config_servers_per_cluster
+        num_mongos = mongo_conf.num_query_routers_per_cluster
         delta_instances = num_instances + num_configsvr + num_mongos
 
         models.validate_instance_flavors(
             context, instances, mongo_conf.volume_support,
             mongo_conf.device_path)
         models.assert_homogeneous_cluster(instances)
 
-        flavor_id = instances[0]['flavor_id']
-
-        volume_size = instances[0].get('volume_size', None)
-        volume_type = instances[0].get('volume_type', None)
-
-        configsvr_vsize = int(extended_properties.get(
-            'configsvr_volume_size', mongo_conf.config_servers_volume_size))
-        configsvr_vtype = extended_properties.get('configsvr_volume_type',
-                                                  volume_type)
-
-        mongos_vsize = int(extended_properties.get(
-            'mongos_volume_size', mongo_conf.query_routers_volume_size))
-        mongos_vtype = extended_properties.get('mongos_volume_type',
-                                               volume_type)
-
-        all_instances = (instances
-                         + [{'volume_size': configsvr_vsize}] * num_configsvr
-                         + [{'volume_size': mongos_vsize}] * num_mongos)
         req_volume_size = models.get_required_volume_size(
-            all_instances, mongo_conf.volume_support)
+            instances, mongo_conf.volume_support)
 
         deltas = {'instances': delta_instances, 'volumes': req_volume_size}
-        check_quotas(context.project_id, deltas)
 
-        # Checking networks are same for the cluster
-        models.validate_instance_nics(context, instances)
+        check_quotas(context.tenant, deltas)
 
-        nics = instances[0].get('nics', None)
+        flavor_id = instances[0]['flavor_id']
+        volume_size = instances[0].get('volume_size', None)
+
+        nics = [instance.get('nics', None) for instance in instances]
 
         azs = [instance.get('availability_zone', None)
                for instance in instances]
 
         regions = [instance.get('region_name', None)
                    for instance in instances]
 
         db_info = models.DBCluster.create(
-            name=name, tenant_id=context.project_id,
+            name=name, tenant_id=context.tenant,
             datastore_version_id=datastore_version.id,
             task_status=ClusterTasks.BUILDING_INITIAL)
 
         replica_set_name = "rs1"
 
         member_config = {"id": db_info.id,
                          "shard_id": utils.generate_uuid(),
@@ -150,55 +128,50 @@
             inst_models.Instance.create(context, instance_name,
                                         flavor_id,
                                         datastore_version.image_id,
                                         [], [], datastore,
                                         datastore_version,
                                         volume_size, None,
                                         availability_zone=azs[i],
-                                        nics=nics,
+                                        nics=nics[i],
                                         configuration_id=None,
                                         cluster_config=member_config,
-                                        volume_type=volume_type,
                                         modules=instances[i].get('modules'),
                                         locality=locality,
                                         region_name=regions[i])
 
         for i in range(1, num_configsvr + 1):
             instance_name = "%s-%s-%s" % (name, "configsvr", str(i))
             inst_models.Instance.create(context, instance_name,
                                         flavor_id,
                                         datastore_version.image_id,
                                         [], [], datastore,
                                         datastore_version,
-                                        configsvr_vsize, None,
+                                        volume_size, None,
                                         availability_zone=None,
-                                        nics=nics,
+                                        nics=None,
                                         configuration_id=None,
                                         cluster_config=configsvr_config,
-                                        volume_type=configsvr_vtype,
                                         locality=locality,
-                                        region_name=regions[i % num_instances]
-                                        )
+                                        region_name=regions[i])
 
         for i in range(1, num_mongos + 1):
             instance_name = "%s-%s-%s" % (name, "mongos", str(i))
             inst_models.Instance.create(context, instance_name,
                                         flavor_id,
                                         datastore_version.image_id,
                                         [], [], datastore,
                                         datastore_version,
-                                        mongos_vsize, None,
+                                        volume_size, None,
                                         availability_zone=None,
-                                        nics=nics,
+                                        nics=None,
                                         configuration_id=None,
                                         cluster_config=mongos_config,
-                                        volume_type=mongos_vtype,
                                         locality=locality,
-                                        region_name=regions[i % num_instances]
-                                        )
+                                        region_name=regions[i])
 
         task_api.load(context, datastore_version.manager).create_cluster(
             db_info.id)
 
         return MongoDbCluster(context, db_info, datastore, datastore_version)
 
     def _parse_grow_item(self, item):
@@ -265,44 +238,40 @@
         else:
             super(MongoDbCluster, self).action(context, req, action, param)
 
     def add_shard(self):
 
         if self.db_info.task_status != ClusterTasks.NONE:
             current_task = self.db_info.task_status.name
-            log_fmt = ("This action cannot be performed on the cluster while "
-                       "the current cluster task is '%s'.")
-            exc_fmt = _("This action cannot be performed on the cluster while "
-                        "the current cluster task is '%s'.")
-            LOG.error(log_fmt, current_task)
-            raise exception.UnprocessableEntity(exc_fmt % current_task)
+            msg = _("This action cannot be performed on the cluster while "
+                    "the current cluster task is '%s'.") % current_task
+            LOG.error(msg)
+            raise exception.UnprocessableEntity(msg)
 
         db_insts = inst_models.DBInstance.find_all(cluster_id=self.id,
-                                                   deleted=False,
                                                    type='member').all()
         num_unique_shards = len(set([db_inst.shard_id for db_inst
                                      in db_insts]))
         if num_unique_shards == 0:
-            LOG.error("This action cannot be performed on the cluster as no "
-                      "reference shard exists.")
-            raise exception.UnprocessableEntity(
-                _("This action cannot be performed on the cluster as no "
-                  "reference shard exists."))
+            msg = _("This action cannot be performed on the cluster as no "
+                    "reference shard exists.")
+            LOG.error(msg)
+            raise exception.UnprocessableEntity(msg)
 
         arbitrary_shard_id = db_insts[0].shard_id
         members_in_shard = [db_inst for db_inst in db_insts
                             if db_inst.shard_id == arbitrary_shard_id]
         num_members_per_shard = len(members_in_shard)
         a_member = inst_models.load_any_instance(self.context,
                                                  members_in_shard[0].id)
         deltas = {'instances': num_members_per_shard}
         volume_size = a_member.volume_size
         if volume_size:
             deltas['volumes'] = volume_size * num_members_per_shard
-        check_quotas(self.context.project_id, deltas)
+        check_quotas(self.context.tenant, deltas)
         new_replica_set_name = "rs" + str(num_unique_shards + 1)
         new_shard_id = utils.generate_uuid()
         dsv_manager = (datastore_models.DatastoreVersion.
                        load_by_uuid(db_insts[0].datastore_version_id).manager)
         manager = task_api.load(self.context, dsv_manager)
         key = manager.get_key(a_member)
         member_config = {"id": self.id,
@@ -479,20 +448,18 @@
         return self._create_instances(instances, cluster_config,
                                       'mongos', locality, key=key)
 
     def _prep_resize(self):
         """Get information about the cluster's current state."""
         if self.db_info.task_status != ClusterTasks.NONE:
             current_task = self.db_info.task_status.name
-            log_fmt = ("This action cannot be performed on the cluster while "
-                       "the current cluster task is '%s'.")
-            exc_fmt = _("This action cannot be performed on the cluster while "
-                        "the current cluster task is '%s'.")
-            LOG.error(log_fmt, current_task)
-            raise exception.UnprocessableEntity(exc_fmt % current_task)
+            msg = _("This action cannot be performed on the cluster while "
+                    "the current cluster task is '%s'.") % current_task
+            LOG.error(msg)
+            raise exception.UnprocessableEntity(msg)
 
         def _instances_of_type(instance_type):
             return [db_inst for db_inst in self.db_instances
                     if db_inst.type == instance_type]
 
         self.config_svrs = _instances_of_type('config_server')
         self.query_routers = _instances_of_type('query_router')
@@ -506,15 +473,15 @@
 
     def _group_instances(self, instances):
         """Group the instances into logical sets (type, shard, etc)."""
         replicas = []
         query_routers = []
         for item in instances:
             if item['instance_type'] == 'replica':
-                replica_requirements = ['name']
+                replica_requirements = ['related_to', 'name']
                 if not all(key in item for key in replica_requirements):
                     raise exception.TroveError(
                         _('Replica instance does not have required field(s) '
                           '%s.') % replica_requirements
                     )
                 replicas.append(item)
             elif item['instance_type'] == 'query_router':
@@ -619,30 +586,30 @@
             )
 
     @staticmethod
     def _check_quotas(context, instances):
         deltas = {'instances': len(instances),
                   'volumes': sum([instance['volume_size']
                                   for instance in instances])}
-        check_quotas(context.project_id, deltas)
+        check_quotas(context.tenant, deltas)
 
     @staticmethod
     def _check_instances(context, instances, datastore_version,
                          allowed_instance_count=None):
         instance_count = len(instances)
         if allowed_instance_count:
             if instance_count not in allowed_instance_count:
                 raise exception.ClusterNumInstancesNotSupported(
                     num_instances=allowed_instance_count
                 )
         flavor_ids = [instance['flavor_id'] for instance in instances]
         if len(set(flavor_ids)) != 1:
             raise exception.ClusterFlavorsNotEqual()
         flavor_id = flavor_ids[0]
-        nova_client = clients.create_nova_client(context)
+        nova_client = remote.create_nova_client(context)
         try:
             flavor = nova_client.flavors.get(flavor_id)
         except nova_exceptions.NotFound:
             raise exception.FlavorNotFound(uuid=flavor_id)
         mongo_conf = CONF.get(datastore_version.manager)
         volume_sizes = [instance['volume_size'] for instance in instances
                         if instance.get('volume_size', None)]
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/guestagent.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/guestagent.py`

 * *Files 13% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from trove.common import cfg
 from trove.common.strategies.cluster import base
 from trove.guestagent import api as guest_api
 
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+ADD_MEMBERS_TIMEOUT = CONF.mongodb.add_members_timeout
 
 
 class MongoDbGuestAgentStrategy(base.BaseGuestAgentStrategy):
 
     @property
     def guest_client_class(self):
         return MongoDbGuestAgentAPI
@@ -47,91 +48,91 @@
         LOG.debug("Adding shard with replSet %(replica_set_name)s and member "
                   "%(replica_set_member)s for instance "
                   "%(id)s", {'replica_set_name': replica_set_name,
                              'replica_set_member': replica_set_member,
                              'id': self.id})
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("add_shard", self.agent_high_timeout,
+        return self._call("add_shard", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version,
                           replica_set_name=replica_set_name,
                           replica_set_member=replica_set_member)
 
     def add_members(self, members):
         LOG.debug("Adding members %(members)s on instance %(id)s", {
             'members': members, 'id': self.id})
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("add_members", CONF.mongodb.add_members_timeout,
+        return self._call("add_members", ADD_MEMBERS_TIMEOUT,
                           version=version, members=members)
 
     def add_config_servers(self, config_servers):
         LOG.debug("Adding config servers %(config_servers)s for instance "
                   "%(id)s", {'config_servers': config_servers,
                              'id': self.id})
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("add_config_servers", self.agent_high_timeout,
+        return self._call("add_config_servers", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version,
                           config_servers=config_servers)
 
     def cluster_complete(self):
         LOG.debug("Notify regarding cluster install completion")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("cluster_complete", self.agent_high_timeout,
+        return self._call("cluster_complete", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def get_key(self):
         LOG.debug("Requesting cluster key from guest")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_key", self.agent_low_timeout,
+        return self._call("get_key", guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def prep_primary(self):
         LOG.debug("Preparing member to be primary member.")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("prep_primary", self.agent_high_timeout,
+        return self._call("prep_primary", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def create_admin_user(self, password):
         LOG.debug("Creating admin user")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("create_admin_user", self.agent_high_timeout,
+        return self._call("create_admin_user", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version, password=password)
 
     def store_admin_password(self, password):
         LOG.debug("Storing admin password")
         version = guest_api.API.API_BASE_VERSION
 
         return self._call("store_admin_password",
-                          self.agent_low_timeout,
+                          guest_api.AGENT_LOW_TIMEOUT,
                           version=version,
                           password=password)
 
     def get_replica_set_name(self):
         LOG.debug("Querying member for its replica set name")
         version = guest_api.API.API_BASE_VERSION
 
         return self._call("get_replica_set_name",
-                          self.agent_high_timeout,
+                          guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def get_admin_password(self):
         LOG.debug("Querying instance for its admin password")
         version = guest_api.API.API_BASE_VERSION
 
         return self._call("get_admin_password",
-                          self.agent_low_timeout,
+                          guest_api.AGENT_LOW_TIMEOUT,
                           version=version)
 
     def is_shard_active(self, replica_set_name):
         LOG.debug("Checking if replica set %s is active", replica_set_name)
         version = guest_api.API.API_BASE_VERSION
 
         return self._call("is_shard_active",
-                          self.agent_high_timeout,
+                          guest_api.AGENT_HIGH_TIMEOUT,
                           version=version,
                           replica_set_name=replica_set_name)
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/mongodb/taskmanager.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/mongodb/taskmanager.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,27 +14,28 @@
 #    under the License.
 
 from eventlet.timeout import Timeout
 from oslo_log import log as logging
 
 from trove.common import cfg
 from trove.common.exception import PollTimeOut
+from trove.common.i18n import _
+from trove.common.instance import ServiceStatuses
 from trove.common.strategies.cluster import base
 from trove.common import utils
 from trove.instance import models
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
-from trove.instance.service_status import ServiceStatuses
-from trove.instance import tasks as inst_tasks
 from trove.taskmanager import api as task_api
-from trove.taskmanager import models as task_models
+import trove.taskmanager.models as task_models
 
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+USAGE_SLEEP_TIME = CONF.usage_sleep_time  # seconds.
 
 
 class MongoDbTaskManagerStrategy(base.BaseTaskManagerStrategy):
 
     @property
     def task_manager_api_class(self):
         return MongoDbTaskManagerAPI
@@ -116,15 +117,15 @@
         timeout = Timeout(cluster_usage_timeout)
         try:
             _create_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("timeout for building cluster.")
+            LOG.exception(_("timeout for building cluster."))
             self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("end create_cluster for id: %s", cluster_id)
 
     def add_shard_cluster(self, context, cluster_id, shard_id,
@@ -133,15 +134,14 @@
         LOG.debug("begin add_shard_cluster for cluster %(cluster_id)s "
                   "shard %(shard_id)s", {'cluster_id': cluster_id,
                                          'shard_id': shard_id})
 
         def _add_shard_cluster():
 
             db_instances = DBInstance.find_all(cluster_id=cluster_id,
-                                               deleted=False,
                                                shard_id=shard_id).all()
             instance_ids = [db_instance.id for db_instance in db_instances]
             LOG.debug("instances in shard %(shard_id)s: %(instance_ids)s",
                       {'shard_id': shard_id, 'instance_ids': instance_ids})
             if not self._all_instances_ready(instance_ids, cluster_id,
                                              shard_id):
                 return
@@ -165,15 +165,15 @@
         timeout = Timeout(cluster_usage_timeout)
         try:
             _add_shard_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("timeout for building shard.")
+            LOG.exception(_("timeout for building shard."))
             self.update_statuses_on_failure(cluster_id, shard_id)
         finally:
             timeout.cancel()
 
         LOG.debug("end add_shard_cluster for cluster %(cluster_id)s "
                   "shard %(shard_id)s", {'cluster_id': cluster_id,
                                          'shard_id': shard_id})
@@ -245,17 +245,16 @@
         timeout = Timeout(cluster_usage_timeout)
         try:
             _grow_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("timeout for growing cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+            LOG.exception(_("timeout for growing cluster."))
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("end grow_cluster for MongoDB cluster %s", self.id)
 
     def shrink_cluster(self, context, cluster_id, instance_ids):
         LOG.debug("begin shrink_cluster for MongoDB cluster %s", cluster_id)
@@ -270,28 +269,27 @@
                     set(instance_ids).intersection(set(non_deleted_ids))
                 )
             try:
                 utils.poll_until(all_instances_marked_deleted,
                                  sleep_time=2,
                                  time_out=CONF.cluster_delete_time_out)
             except PollTimeOut:
-                LOG.error("timeout for instances to be marked as deleted.")
+                LOG.error(_("timeout for instances to be marked as deleted."))
                 return
 
         cluster_usage_timeout = CONF.cluster_usage_timeout
         timeout = Timeout(cluster_usage_timeout)
         try:
             _shrink_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("timeout for shrinking cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.SHRINKING_ERROR)
+            LOG.exception(_("timeout for shrinking cluster."))
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("end shrink_cluster for MongoDB cluster %s", self.id)
 
     def get_cluster_admin_password(self, context):
         """The cluster admin's user credentials are stored on all query
@@ -309,15 +307,15 @@
         try:
             for member in other_members:
                 other_members_ips.append(self.get_ip(member))
                 self.get_guest(member).restart()
             self.get_guest(primary_member).prep_primary()
             self.get_guest(primary_member).add_members(other_members_ips)
         except Exception:
-            LOG.exception("error initializing replica set")
+            LOG.exception(_("error initializing replica set"))
             self.update_statuses_on_failure(self.id,
                                             shard_id=primary_member.shard_id)
             return False
         return True
 
     def _create_shard(self, query_router, members):
         """Create a replica set out of the given member instances and add it as
@@ -332,29 +330,29 @@
                   'to cluster %(cluster_id)s',
                   {'replica_set': replica_set,
                    'shard_id': primary_member.shard_id, 'cluster_id': self.id})
         try:
             self.get_guest(query_router).add_shard(
                 replica_set, self.get_ip(primary_member))
         except Exception:
-            LOG.exception("error adding shard")
+            LOG.exception(_("error adding shard"))
             self.update_statuses_on_failure(self.id,
                                             shard_id=primary_member.shard_id)
             return False
         return True
 
     def _get_running_query_router_id(self):
         """Get a query router in this cluster that is in the RUNNING state."""
         for instance_id in [db_instance.id for db_instance in self.db_instances
                             if db_instance.type == 'query_router']:
             status = models.InstanceServiceStatus.find_by(
                 instance_id=instance_id).get_status()
             if status == ServiceStatuses.RUNNING:
                 return instance_id
-        LOG.exception("no query routers ready to accept requests")
+        LOG.exception(_("no query routers ready to accept requests"))
         self.update_statuses_on_failure(self.id)
         return False
 
     def _add_query_routers(self, query_routers, config_server_ips,
                            admin_password=None):
         """Configure the given query routers for the cluster.
         If this is a new_cluster an admin user will be created with a randomly
@@ -373,15 +371,15 @@
                 if not admin_password:
                     LOG.debug("creating cluster admin user")
                     admin_password = utils.generate_random_password()
                     guest.create_admin_user(admin_password)
                 else:
                     guest.store_admin_password(admin_password)
             except Exception:
-                LOG.exception("error adding config servers")
+                LOG.exception(_("error adding config servers"))
                 self.update_statuses_on_failure(self.id)
                 return False
         return True
 
 
 class MongoDbTaskManagerAPI(task_api.API):
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/api.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/redis/api.py`

 * *Files 3% similar despite different names*

```diff
@@ -57,28 +57,24 @@
 
         models.validate_instance_flavors(
             context, instances, volume_enabled, ephemeral_enabled)
 
         total_volume_allocation = models.get_required_volume_size(
             instances, volume_enabled)
 
-        models.assert_homogeneous_cluster(instances)
-
-        models.validate_instance_nics(context, instances)
-
         name_index = 1
         for instance in instances:
             if not instance.get('name'):
                 instance['name'] = "%s-member-%s" % (db_info.name, name_index)
                 name_index += 1
 
         # Check quotas
         quota_request = {'instances': num_instances,
                          'volumes': total_volume_allocation}
-        check_quotas(context.project_id, quota_request)
+        check_quotas(context.tenant, quota_request)
 
         # Creating member instances
         return [inst_models.Instance.create(context,
                                             instance['name'],
                                             instance['flavor_id'],
                                             datastore_version.image_id,
                                             [], [],
@@ -88,51 +84,45 @@
                                             instance.get(
                                                 'availability_zone', None),
                                             instance.get('nics', None),
                                             configuration_id=None,
                                             cluster_config={
                                                 "id": db_info.id,
                                                 "instance_type": "member"},
-                                            volume_type=instance.get(
-                                                'volume_type', None),
                                             modules=instance.get('modules'),
                                             locality=locality,
                                             region_name=instance.get(
                                                 'region_name')
                                             )
                 for instance in instances]
 
     @classmethod
     def create(cls, context, name, datastore, datastore_version,
-               instances, extended_properties, locality, configuration,
-               image_id=None):
+               instances, extended_properties, locality, configuration):
         LOG.debug("Initiating cluster creation.")
 
         if configuration:
             raise exception.ConfigurationNotSupported()
 
         # Updating Cluster Task
 
         db_info = models.DBCluster.create(
-            name=name, tenant_id=context.project_id,
+            name=name, tenant_id=context.tenant,
             datastore_version_id=datastore_version.id,
             task_status=ClusterTasks.BUILDING_INITIAL)
 
         cls._create_instances(context, db_info, datastore, datastore_version,
                               instances, extended_properties, locality)
 
         # Calling taskmanager to further proceed for cluster-configuration
         task_api.load(context, datastore_version.manager).create_cluster(
             db_info.id)
 
         return RedisCluster(context, db_info, datastore, datastore_version)
 
-    def upgrade(self, datastore_version):
-        self.rolling_upgrade(datastore_version)
-
     def grow(self, instances):
         LOG.debug("Growing cluster.")
 
         self.validate_cluster_available()
 
         context = self.context
         db_info = self.db_info
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/guestagent.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/guestagent.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,78 +9,85 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from oslo_log import log as logging
 
+from trove.common import cfg
 from trove.common.strategies.cluster import base
 from trove.guestagent import api as guest_api
 
 
 LOG = logging.getLogger(__name__)
+CONF = cfg.CONF
 
 
-class RedisGuestAgentStrategy(base.BaseGuestAgentStrategy):
+class VerticaGuestAgentStrategy(base.BaseGuestAgentStrategy):
 
     @property
     def guest_client_class(self):
-        return RedisGuestAgentAPI
+        return VerticaGuestAgentAPI
 
 
-class RedisGuestAgentAPI(guest_api.API):
+class VerticaGuestAgentAPI(guest_api.API):
     """Cluster Specific Datastore Guest API
 
     **** VERSION CONTROLLED API ****
 
     The methods in this class are subject to version control as
     coordinated by guestagent/api.py.  Whenever a change is made to
     any API method in this class, add a version number and comment
     to the top of guestagent/api.py and use the version number as
     appropriate in this file
     """
 
-    def get_node_ip(self):
-        LOG.debug("Retrieve ip info from node.")
+    def get_public_keys(self, user):
+        LOG.debug("Getting public keys for user: %s.", user)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_node_ip",
-                          self.agent_high_timeout,
-                          version=version)
+        return self._call("get_public_keys", guest_api.AGENT_HIGH_TIMEOUT,
+                          version=version, user=user)
 
-    def get_node_id_for_removal(self):
-        LOG.debug("Validating cluster node removal.")
+    def authorize_public_keys(self, user, public_keys):
+        LOG.debug("Authorizing public keys for user: %s.", user)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("get_node_id_for_removal",
-                          self.agent_high_timeout,
-                          version=version)
+        return self._call("authorize_public_keys",
+                          guest_api.AGENT_HIGH_TIMEOUT,
+                          version=version,
+                          user=user, public_keys=public_keys)
 
-    def remove_nodes(self, node_ids):
-        LOG.debug("Removing nodes from cluster.")
+    def install_cluster(self, members):
+        LOG.debug("Installing Vertica cluster on members: %s.", members)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("remove_nodes", self.agent_high_timeout,
-                          version=version, node_ids=node_ids)
+        return self._call("install_cluster", CONF.cluster_usage_timeout,
+                          version=version, members=members)
 
-    def cluster_meet(self, ip, port):
-        LOG.debug("Joining node to cluster.")
+    def grow_cluster(self, members):
+        LOG.debug("Growing Vertica cluster with members: %s.", members)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("cluster_meet", self.agent_high_timeout,
-                          version=version, ip=ip, port=port)
+        return self._call("grow_cluster", CONF.cluster_usage_timeout,
+                          version=version, members=members)
 
-    def cluster_addslots(self, first_slot, last_slot):
-        LOG.debug("Adding slots %s-%s to cluster.", first_slot, last_slot)
+    def shrink_cluster(self, members):
+        LOG.debug("Shrinking Vertica cluster with members: %s.", members)
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("cluster_addslots",
-                          self.agent_high_timeout,
-                          version=version,
-                          first_slot=first_slot, last_slot=last_slot)
+        return self._call("shrink_cluster", CONF.cluster_usage_timeout,
+                          version=version, members=members)
+
+    def mark_design_ksafe(self, k):
+        LOG.debug("Setting vertica k-safety level to : %s.", k)
+        version = guest_api.API.API_BASE_VERSION
+
+        return self._call("mark_design_ksafe", CONF.cluster_usage_timeout,
+                          version=version, k=k)
 
     def cluster_complete(self):
         LOG.debug("Notifying cluster install completion.")
         version = guest_api.API.API_BASE_VERSION
 
-        return self._call("cluster_complete", self.agent_high_timeout,
+        return self._call("cluster_complete", guest_api.AGENT_HIGH_TIMEOUT,
                           version=version)
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/redis/taskmanager.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/redis/taskmanager.py`

 * *Files 14% similar despite different names*

```diff
@@ -16,21 +16,21 @@
 
 from trove.common import cfg
 from trove.common.exception import TroveError
 from trove.common.i18n import _
 from trove.common.strategies.cluster import base
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
-from trove.instance import tasks as inst_tasks
 from trove.taskmanager import api as task_api
 import trove.taskmanager.models as task_models
 
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+USAGE_SLEEP_TIME = CONF.usage_sleep_time  # seconds.
 
 
 class RedisTaskManagerStrategy(base.BaseTaskManagerStrategy):
 
     @property
     def task_manager_api_class(self):
         return RedisTaskManagerAPI
@@ -81,38 +81,37 @@
                         last_slot -= 1
                     guest.cluster_addslots(first_slot, last_slot)
                     first_slot = last_slot + 1
 
                 for guest in guests:
                     guest.cluster_complete()
             except Exception:
-                LOG.exception("Error creating cluster.")
+                LOG.exception(_("Error creating cluster."))
                 self.update_statuses_on_failure(cluster_id)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _create_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for building cluster.")
+            LOG.exception(_("Timeout for building cluster."))
             self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End create_cluster for id: %s.", cluster_id)
 
     def grow_cluster(self, context, cluster_id, new_instance_ids):
         LOG.debug("Begin grow_cluster for id: %s.", cluster_id)
 
         def _grow_cluster():
 
-            db_instances = DBInstance.find_all(cluster_id=cluster_id,
-                                               deleted=False).all()
+            db_instances = DBInstance.find_all(cluster_id=cluster_id).all()
             cluster_head = next(Instance.load(context, db_inst.id)
                                 for db_inst in db_instances
                                 if db_inst.id not in new_instance_ids)
             if not cluster_head:
                 raise TroveError(_("Unable to determine existing Redis cluster"
                                    " member"))
 
@@ -138,26 +137,21 @@
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _grow_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for growing cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+            LOG.exception(_("Timeout for growing cluster."))
+            self.update_statuses_on_failure(cluster_id)
         except Exception:
-            LOG.exception("Error growing cluster %s.", cluster_id)
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+            LOG.exception(_("Error growing cluster %s."), cluster_id)
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End grow_cluster for id: %s.", cluster_id)
 
-    def upgrade_cluster(self, context, cluster_id, datastore_version):
-        self.rolling_upgrade_cluster(context, cluster_id, datastore_version)
-
 
 class RedisTaskManagerAPI(task_api.API):
 
     pass
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/api.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/api.py`

 * *Files 6% similar despite different names*

```diff
@@ -70,16 +70,15 @@
     @staticmethod
     def _create_instances(context, db_info, datastore, datastore_version,
                           instances, extended_properties, locality,
                           new_cluster=True):
         vertica_conf = CONF.get(datastore_version.manager)
         num_instances = len(instances)
 
-        existing = inst_models.DBInstance.find_all(cluster_id=db_info.id,
-                                                   deleted=False).all()
+        existing = inst_models.DBInstance.find_all(cluster_id=db_info.id).all()
         num_existing = len(existing)
 
         # Matching number of instances with configured cluster_member_count
         if (new_cluster and
                 num_instances != vertica_conf.cluster_member_count):
             raise exception.ClusterNumInstancesNotSupported(
                 num_instances=vertica_conf.cluster_member_count)
@@ -88,23 +87,20 @@
             context, instances, vertica_conf.volume_support,
             vertica_conf.device_path)
 
         req_volume_size = models.get_required_volume_size(
             instances, vertica_conf.volume_support)
         models.assert_homogeneous_cluster(instances)
 
-        models.validate_instance_nics(context, instances)
-
         deltas = {'instances': num_instances, 'volumes': req_volume_size}
 
-        check_quotas(context.project_id, deltas)
+        check_quotas(context.tenant, deltas)
 
         flavor_id = instances[0]['flavor_id']
         volume_size = instances[0].get('volume_size', None)
-        volume_type = instances[0].get('volume_type', None)
 
         nics = [instance.get('nics', None) for instance in instances]
 
         azs = [instance.get('availability_zone', None)
                for instance in instances]
 
         regions = [instance.get('region_name', None)
@@ -122,39 +118,37 @@
             minstances.append(
                 inst_models.Instance.create(
                     context, instance_name, flavor_id,
                     datastore_version.image_id, [], [], datastore,
                     datastore_version, volume_size, None,
                     nics=nics[i], availability_zone=azs[i],
                     configuration_id=None, cluster_config=member_config,
-                    volume_type=volume_type,
                     modules=instances[i].get('modules'), locality=locality,
                     region_name=regions[i])
             )
         return minstances
 
     @classmethod
     def create(cls, context, name, datastore, datastore_version,
-               instances, extended_properties, locality, configuration,
-               image_id=None):
+               instances, extended_properties, locality, configuration):
         LOG.debug("Initiating cluster creation.")
 
         if configuration:
             raise exception.ConfigurationNotSupported()
 
         vertica_conf = CONF.get(datastore_version.manager)
         num_instances = len(instances)
 
         # Matching number of instances with configured cluster_member_count
         if num_instances != vertica_conf.cluster_member_count:
             raise exception.ClusterNumInstancesNotSupported(
                 num_instances=vertica_conf.cluster_member_count)
 
         db_info = models.DBCluster.create(
-            name=name, tenant_id=context.project_id,
+            name=name, tenant_id=context.tenant,
             datastore_version_id=datastore_version.id,
             task_status=ClusterTasks.BUILDING_INITIAL)
 
         cls._create_instances(context, db_info, datastore, datastore_version,
                               instances, extended_properties, locality,
                               new_cluster=True)
         # Calling taskmanager to further proceed for cluster-configuration
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/experimental/vertica/taskmanager.py` & `trove-8.0.1/trove/common/strategies/cluster/experimental/vertica/taskmanager.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,21 +17,21 @@
 from trove.common import cfg
 from trove.common.i18n import _
 from trove.common.strategies.cluster import base
 from trove.common.strategies.cluster.experimental.vertica.api import \
     VerticaCluster
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
-from trove.instance import tasks as inst_tasks
 from trove.taskmanager import api as task_api
 import trove.taskmanager.models as task_models
 
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+USAGE_SLEEP_TIME = CONF.usage_sleep_time  # seconds.
 
 
 class VerticaTaskManagerStrategy(base.BaseTaskManagerStrategy):
 
     @property
     def task_manager_api_class(self):
         return VerticaTaskManagerAPI
@@ -88,25 +88,25 @@
                             member_ips)
                         break
 
                 LOG.debug("Finalizing cluster configuration.")
                 for guest in guests:
                     guest.cluster_complete()
             except Exception:
-                LOG.exception("Error creating cluster.")
+                LOG.exception(_("Error creating cluster."))
                 self.update_statuses_on_failure(cluster_id)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _create_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for building cluster.")
+            LOG.exception(_("Timeout for building cluster."))
             self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("End create_cluster for id: %s.", cluster_id)
 
     def grow_cluster(self, context, cluster_id, new_instance_ids):
@@ -158,21 +158,19 @@
 
         try:
             _grow_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for growing cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+            LOG.exception(_("Timeout for growing cluster."))
+            self.update_statuses_on_failure(cluster_id)
         except Exception:
-            LOG.exception("Error growing cluster %s.", cluster_id)
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.GROWING_ERROR)
+            LOG.exception(_("Error growing cluster %s."), cluster_id)
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
     def shrink_cluster(self, context, cluster_id, instance_ids):
         def _shrink_cluster():
             db_instances = DBInstance.find_all(cluster_id=cluster_id,
                                                deleted=False).all()
@@ -210,17 +208,16 @@
         timeout = Timeout(CONF.cluster_usage_timeout)
         try:
             _shrink_cluster()
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise
-            LOG.exception("Timeout for shrinking cluster.")
-            self.update_statuses_on_failure(
-                cluster_id, status=inst_tasks.InstanceTasks.SHRINKING_ERROR)
+            LOG.exception(_("Timeout for shrinking cluster."))
+            self.update_statuses_on_failure(cluster_id)
         finally:
             timeout.cancel()
 
         LOG.debug("end shrink_cluster for Vertica cluster id %s", self.id)
 
 
 class VerticaTaskManagerAPI(task_api.API):
```

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/cluster/strategy.py` & `trove-8.0.1/trove/common/strategies/cluster/strategy.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/strategies/strategy.py` & `trove-8.0.1/trove/common/strategies/strategy.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,23 +13,25 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import abc
 
 from oslo_log import log as logging
+import six
 
 from trove.common.i18n import _
 from trove.common import utils
 
 
 LOG = logging.getLogger(__name__)
 
 
-class Strategy(object, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class Strategy(object):
 
     __strategy_ns__ = None
 
     __strategy_name__ = None
     __strategy_type__ = None
 
     def __init__(self):
```

### Comparing `trove-21.0.0.0rc2/trove/common/stream_codecs.py` & `trove-8.0.1/trove/common/stream_codecs.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,26 +11,25 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import abc
 import ast
-import configparser
+import base64
 import csv
-import io
+import json
 import re
+import sys
 
+import six
+from six.moves import configparser
 import xmltodict
 import yaml
 
-from oslo_serialization import base64
-from oslo_serialization import jsonutils
-from oslo_utils import strutils
-
 from trove.common import utils as trove_utils
 
 
 class StringConverter(object):
     """A passthrough string-to-object converter.
     """
 
@@ -68,25 +67,26 @@
 
         return str(value)
 
     def _to_object(self, value):
         # Return known mappings and quoted strings right away.
         if value in self._object_mappings:
             return self._object_mappings[value]
-        elif (isinstance(value, str) and
+        elif (isinstance(value, six.string_types) and
               re.match("^'(.*)'|\"(.*)\"$", value)):
             return value
 
         try:
             return ast.literal_eval(value)
         except Exception:
             return value
 
 
-class StreamCodec(object, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class StreamCodec(object):
 
     @abc.abstractmethod
     def serialize(self, data):
         """Serialize a Python object into a stream.
         """
 
     @abc.abstractmethod
@@ -195,44 +195,51 @@
         :type default_value:   object
         """
         self._default_value = default_value
         self._comment_markers = comment_markers
 
     def serialize(self, dict_data):
         parser = self._init_config_parser(dict_data)
-        output = io.StringIO()
+        output = six.StringIO()
         parser.write(output)
 
         return output.getvalue()
 
     def deserialize(self, stream):
         parser = self._init_config_parser()
-        parser.read_file(self._pre_parse(stream))
+        if sys.version_info >= (3, 2):
+            parser.read_file(self._pre_parse(stream))
+        else:
+            parser.readfp(self._pre_parse(stream))
 
         return {s: {k:
                     StringConverter({None: self._default_value}).to_objects(v)
                     for k, v in parser.items(s, raw=True)}
                 for s in parser.sections()}
 
     def _pre_parse(self, stream):
-        buf = io.StringIO()
-        for line in io.StringIO(stream):
+        buf = six.StringIO()
+        for line in six.StringIO(stream):
             # Ignore commented lines.
             if not line.startswith(self._comment_markers):
                 # Strip leading and trailing whitespaces from each line.
                 buf.write(line.strip() + '\n')
 
         # Rewind the output buffer.
         buf.flush()
         buf.seek(0)
 
         return buf
 
     def _init_config_parser(self, sections=None):
-        parser = configparser.ConfigParser(allow_no_value=True)
+        # SafeConfigParser was deprecated in Python 3.2
+        if sys.version_info >= (3, 2):
+            parser = configparser.ConfigParser(allow_no_value=True)
+        else:
+            parser = configparser.SafeConfigParser(allow_no_value=True)
         if sections:
             for section in sections:
                 parser.add_section(section)
                 for key, value in sections[section].items():
                     str_val = StringConverter(
                         {self._default_value: None}).to_strings(value)
                     parser.set(section, key,
@@ -273,79 +280,67 @@
         :param empty_value:       Value to represent None in the output.
         :type empty_value:        object
 
         :param comment_markers:   List of comment markers.
         :type comment_markers:    list
 
         :param unpack_singletons: Whether to unpack singleton collections
-                                  (collections with only a single item).
+                                  (collections with only a single value).
         :type unpack_singletons:  boolean
 
         :param string_mappings:   User-defined string representations of
                                   Python objects.
         :type string_mappings:    dict
         """
         self._delimiter = delimiter
         self._comment_markers = comment_markers
         self._string_converter = StringConverter(string_mappings or {})
         self._unpack_singletons = unpack_singletons
 
     def serialize(self, dict_data):
-        output = io.StringIO()
+        output = six.StringIO()
         writer = csv.writer(output, delimiter=self._delimiter,
                             quoting=self.QUOTING_MODE,
                             strict=self.STRICT_MODE,
                             skipinitialspace=self.SKIP_INIT_SPACE)
 
         for key, value in dict_data.items():
             writer.writerows(self._to_rows(key, value))
 
         return output.getvalue()
 
     def deserialize(self, stream):
-        reader = csv.reader(io.StringIO(stream),
+        reader = csv.reader(six.StringIO(stream),
                             delimiter=self._delimiter,
                             quoting=self.QUOTING_MODE,
                             strict=self.STRICT_MODE,
                             skipinitialspace=self.SKIP_INIT_SPACE)
 
         return self._to_dict(reader)
 
     def _to_dict(self, reader):
         data_dict = {}
         for row in reader:
             if row:
                 key = row[0].strip()
                 # Ignore comment lines.
                 if not key.strip().startswith(self._comment_markers):
-                    # NOTE(zhaochao): a list object is expected for
-                    # trove_utils.unpack_singleton, however in python3
-                    # map objects won't be treated as lists, so we
-                    # convert the result of StringConverter.to_objects
-                    # to a list explicitly.
-                    items = list(self._string_converter.to_objects(
+                    items = self._string_converter.to_objects(
                         [v if v else None for v in
-                         map(self._strip_comments, row[1:])]))
+                         map(self._strip_comments, row[1:])])
                     current = data_dict.get(key)
                     if current is not None:
                         current.append(trove_utils.unpack_singleton(items)
                                        if self._unpack_singletons else items)
                     else:
                         data_dict.update({key: [items]})
 
         if self._unpack_singletons:
             # Unpack singleton values.
-            # NOTE(zhaochao): In Python 3.x, dict.items() returns a view
-            # object, which will reflect the changes of the dict itself:
-            # https://docs.python.org/3/library/stdtypes.html#dict-views
-            # This means as we're changing the dict, dict.items() cannot
-            # guarantee we're safely iterating all entries in the dict.
-            # Manually converting the result of dict.items() to a list will
-            # fix.
-            for k, v in list(data_dict.items()):
+            for k, v in data_dict.items():
                 data_dict.update({k: trove_utils.unpack_singleton(v)})
 
         return data_dict
 
     def _strip_comments(self, value):
         # Strip in-line comments.
         for marker in self._comment_markers:
@@ -361,22 +356,17 @@
                     rows.extend(self._to_rows(header, item))
             else:
                 # This is a single-row property with multiple arguments.
                 rows.append(self._to_list(
                     header, self._string_converter.to_strings(items)))
         else:
             # This is a single-row property with only one argument.
-            # Note(zhaochao): csv.writerows expects a list object before
-            # python 3.5, but map objects won't be treated as lists in
-            # python 3, so we explicitly convert the result of
-            # StringConverter.to_strings to a list here to support py34
-            # unittests.
             rows.append(
-                list(self._string_converter.to_strings(
-                    self._to_list(header, items))))
+                self._string_converter.to_strings(
+                    self._to_list(header, items)))
 
         return rows
 
     def _to_list(self, *items):
         container = []
         for item in items:
             if trove_utils.is_collection(item):
@@ -440,33 +430,27 @@
     def serialize(self, dict_data):
         lines = []
         for k, v in dict_data.items():
             lines.append(k + self._delimeter + self.serialize_value(v))
         return self._line_terminator.join(lines)
 
     def deserialize(self, stream):
-        # Note(zhaochao): In Python 3, when files are opened in text mode,
-        # newlines will be translated to '\n' by default, so we just split
-        # the stream by '\n'.
-        lines = stream.split('\n')
-
+        lines = stream.split(self._line_terminator)
         result = {}
         for line in lines:
             line = line.lstrip().rstrip()
             if line == '' or line.startswith(self._comment_marker):
                 continue
             k, v = re.split(re.escape(self._delimeter), line, 1)
             if self._value_quoting and v.startswith(self._value_quote_char):
                 # remove trailing comments
                 v = re.sub(r'%s *%s.*$' % ("'", '#'), '', v)
                 v = v.lstrip(
                     self._value_quote_char).rstrip(
                     self._value_quote_char)
-            elif v.lower() in ['true', 'false']:
-                v = strutils.bool_from_string(v.lower())
             else:
                 # remove trailing comments
                 v = re.sub('%s.*$' % self._comment_marker, '', v)
             if self._hidden_marker and v.startswith(self._hidden_marker):
                 continue
             result[k.strip()] = v
         return result
@@ -499,38 +483,43 @@
             return False
         return True
 
 
 class JsonCodec(StreamCodec):
 
     def serialize(self, dict_data):
-        return jsonutils.dumps(dict_data)
+        return json.dumps(dict_data)
 
     def deserialize(self, stream):
-        if type(stream) == str:
-            return jsonutils.load(io.StringIO(stream))
-        if type(stream) == bytes:
-            return jsonutils.load(io.BytesIO(stream))
+        return json.load(six.StringIO(stream))
 
 
 class Base64Codec(StreamCodec):
     """Serialize (encode) and deserialize (decode) using the base64 codec.
     To read binary data from a file and b64encode it, used the decode=False
     flag on operating_system's read calls.  Use encode=False to decode
     binary data before writing to a file as well.
     """
 
-    # NOTE(zhaochao): migrate to oslo_serialization.base64 to serialize(return
-    # a text object) and deserialize(return a bytes object) data.
-
     def serialize(self, data):
-        return base64.encode_as_text(data)
+
+        try:
+            # py27str - if we've got text data, this should encode it
+            # py27aa/py34aa - if we've got a bytearray, this should work too
+            encoded = str(base64.b64encode(data).decode('utf-8'))
+        except TypeError:
+            # py34str - convert to bytes first, then we can encode
+            data_bytes = bytes([ord(item) for item in data])
+            encoded = base64.b64encode(data_bytes).decode('utf-8')
+        return encoded
 
     def deserialize(self, stream):
-        return base64.decode_as_bytes(stream)
+
+        # py27 & py34 seem to understand bytearray the same
+        return bytearray([item for item in base64.b64decode(stream)])
 
 
 class XmlCodec(StreamCodec):
 
     def __init__(self, encoding='utf-8'):
         self._encoding = encoding
```

### Comparing `trove-21.0.0.0rc2/trove/common/template.py` & `trove-8.0.1/trove/common/template.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/timeutils.py` & `trove-8.0.1/trove/common/timeutils.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/common/trove_remote.py` & `trove-8.0.1/trove/common/trove_remote.py`

 * *Files 22% similar despite different names*

```diff
@@ -12,45 +12,43 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from oslo_utils.importutils import import_class
 
 from trove.common import cfg
-from trove.common.clients import get_endpoint
-from trove.common.clients import normalize_url
+from trove.common.remote import get_endpoint
+from trove.common.remote import normalize_url
 
 from troveclient.v1 import client as TroveClient
 
 CONF = cfg.CONF
 
 
 """
 NOTE(mwj, Apr 2016):
-This module is separated from clients.py because clients.py is used
+This module is separated from remote.py because remote.py is used
 on the Trove guest, but the trove client is not installed on the guest,
 so the imports here would fail.
 """
 
 
 def trove_client(context, region_name=None):
     if CONF.trove_url:
         url = '%(url)s%(tenant)s' % {
             'url': normalize_url(CONF.trove_url),
-            'tenant': context.project_id}
+            'tenant': context.tenant}
     else:
-        region = region_name or CONF.service_credentials.region_name
         url = get_endpoint(context.service_catalog,
                            service_type=CONF.trove_service_type,
-                           endpoint_region=region,
+                           endpoint_region=region_name or CONF.os_region_name,
                            endpoint_type=CONF.trove_endpoint_type)
 
     client = TroveClient.Client(context.user, context.auth_token,
-                                project_id=context.project_id,
-                                auth_url=CONF.service_credentials.auth_url)
+                                project_id=context.tenant,
+                                auth_url=CONF.trove_auth_url)
     client.client.auth_token = context.auth_token
     client.client.management_url = url
     return client
 
 
-def create_trove_client(*arg, **kwargs):
-    return import_class(CONF.remote_trove_client)(*arg, **kwargs)
+create_trove_client = import_class(CONF.remote_trove_client)
```

### Comparing `trove-21.0.0.0rc2/trove/common/utils.py` & `trove-8.0.1/trove/common/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,31 +10,31 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 """I totally stole most of this from melange, thx guys!!!"""
 
-from collections import abc
+import collections
 import inspect
 import os
-import shlex
 import shutil
-import urllib.parse as urlparse
+import time
 import uuid
 
 from eventlet.timeout import Timeout
 import jinja2
 from oslo_concurrency import processutils
 from oslo_log import log as logging
 from oslo_service import loopingcall
-from oslo_utils.encodeutils import safe_encode
 from oslo_utils import importutils
 from oslo_utils import strutils
 from passlib import pwd
+import six
+import six.moves.urllib.parse as urlparse
 
 from trove.common import cfg
 from trove.common import exception
 from trove.common.i18n import _
 
 
 CONF = cfg.CONF
@@ -54,15 +54,14 @@
             jinja2.PackageLoader("trove", "templates")
         ]))
     # Add some basic operation not built-in.
     env.globals['max'] = max
     env.globals['min'] = min
     return env
 
-
 ENV = build_jinja_environment()
 
 
 def pagination_limit(limit, default_limit):
     limit = int(limit or default_limit)
     return min(limit, default_limit)
 
@@ -171,61 +170,51 @@
         args = self.argspec.args
         if inspect.ismethod(self._func):
             args.pop(0)
         return args
 
     @cached_property
     def argspec(self):
-        return inspect.getfullargspec(self._func)
+        return inspect.getargspec(self._func)
 
     def __str__(self):
         optionals = ["[{0}=<{0}>]".format(k) for k, v in self.optional_args]
         required = ["{0}=<{0}>".format(arg) for arg in self.required_args]
         args_str = ' '.join(required + optionals)
         return "%s %s" % (self._func.__name__, args_str)
 
 
 def build_polling_task(retriever, condition=lambda value: value,
-                       sleep_time=1, time_out=0, initial_delay=0):
-    """Run a function in a loop with backoff on error.
-
-    The condition function runs based on the retriever function result.
-    """
+                       sleep_time=1, time_out=None):
+    start_time = time.time()
 
     def poll_and_check():
         obj = retriever()
         if condition(obj):
             raise loopingcall.LoopingCallDone(retvalue=obj)
+        if time_out is not None and time.time() - start_time > time_out:
+            raise exception.PollTimeOut
 
-    call = loopingcall.BackOffLoopingCall(f=poll_and_check)
-    return call.start(initial_delay=initial_delay,
-                      starting_interval=sleep_time,
-                      max_interval=30, timeout=time_out)
-
-
-def wait_for_task(polling_task):
-    """Waits for the task until it is finished"""
-    try:
-        return polling_task.wait()
-    except loopingcall.LoopingCallTimeOut:
-        raise exception.PollTimeOut
+    return loopingcall.BackOffLoopingCall(
+        f=poll_and_check).start(initial_delay=False,
+                                starting_interval=sleep_time,
+                                max_interval=30, timeout=time_out)
 
 
 def poll_until(retriever, condition=lambda value: value,
-               sleep_time=3, time_out=0, initial_delay=0):
+               sleep_time=1, time_out=None):
     """Retrieves object until it passes condition, then returns it.
 
     If time_out_limit is passed in, PollTimeOut will be raised once that
     amount of time is eclipsed.
 
     """
-    task = build_polling_task(retriever, condition=condition,
-                              sleep_time=sleep_time, time_out=time_out,
-                              initial_delay=initial_delay)
-    return wait_for_task(task)
+
+    return build_polling_task(retriever, condition=condition,
+                              sleep_time=sleep_time, time_out=time_out).wait()
 
 
 # Copied from nova.api.openstack.common in the old code.
 def get_id_from_href(href):
     """Return the id or uuid portion of a url.
 
     Given: 'http://www.foo.com/bar/123?q=4'
@@ -235,46 +224,41 @@
     Returns: 'abc123'
 
     """
     return urlparse.urlsplit("%s" % href).path.split('/')[-1]
 
 
 def execute_with_timeout(*args, **kwargs):
-    time = kwargs.pop('timeout', CONF.command_process_timeout)
+    time = kwargs.pop('timeout', 30)
     log_output_on_error = kwargs.pop('log_output_on_error', False)
 
     timeout = Timeout(time)
     try:
         return execute(*args, **kwargs)
     except exception.ProcessExecutionError as e:
         if log_output_on_error:
             LOG.error(
-                ("Command '%(cmd)s' failed. %(description)s "
-                 "Exit code: %(exit_code)s\nstderr: %(stderr)s\n"
-                 "stdout: %(stdout)s"),
+                _("Command '%(cmd)s' failed. %(description)s "
+                  "Exit code: %(exit_code)s\nstderr: %(stderr)s\n"
+                  "stdout: %(stdout)s"),
                 {'cmd': e.cmd, 'description': e.description or '',
                  'exit_code': e.exit_code, 'stderr': e.stderr,
                  'stdout': e.stdout})
         raise
     except Timeout as t:
         if t is not timeout:
-            LOG.error("Got a timeout but not the one expected.")
+            LOG.error(_("Got a timeout but not the one expected."))
             raise
         else:
-            log_fmt = ("Time out after waiting "
-                       "%(time)s seconds when running proc: %(args)s"
-                       " %(kwargs)s.")
-            exc_fmt = _("Time out after waiting "
-                        "%(time)s seconds when running proc: %(args)s"
-                        " %(kwargs)s.")
-            msg_content = {
-                'time': time, 'args': args,
-                'kwargs': kwargs}
-            LOG.error(log_fmt, msg_content)
-            raise exception.ProcessExecutionError(exc_fmt % msg_content)
+            msg = (_("Time out after waiting "
+                     "%(time)s seconds when running proc: %(args)s"
+                     " %(kwargs)s.") % {'time': time, 'args': args,
+                                        'kwargs': kwargs})
+            LOG.error(msg)
+            raise exception.ProcessExecutionError(msg)
     finally:
         timeout.cancel()
 
 
 def correct_id_with_req(id, request):
     # Due to a shortcoming with the way Trove uses routes.mapper,
     # URL entities right of the last slash that contain at least
@@ -328,16 +312,16 @@
 
     return container
 
 
 def is_collection(item):
     """Return True is a given item is an iterable collection, but not a string.
     """
-    return (isinstance(item, abc.Iterable) and
-            not isinstance(item, (bytes, str)))
+    return (isinstance(item, collections.Iterable) and
+            not isinstance(item, (bytes, six.text_type)))
 
 
 def format_output(message, format_len=79, truncate_len=None, replace_index=0):
     """Recursive function to try and keep line lengths below a certain amount,
     so they can be displayed nicely on the command-line or UI.
     Tries replacement patterns one at a time (in round-robin fashion)
     that insert \n at strategic spots.
@@ -395,42 +379,7 @@
     this were also moved out from test_dbaas.py to test_utils.py.
     """
     if bytes == 0:
         return 0.0
     size = bytes / 1024.0 ** 2
     # Make sure we don't return 0.0 if the size is greater than 0
     return max(round(size, 2), 0.01)
-
-
-def req_to_text(req):
-    """
-    We do a lot request logging for debug, but if the value of one
-    request header is encoded in utf-8, an UnicodeEncodeError will
-    be raised. So we should carefully encode request headers.
-
-    To be consitent with webob, main procedures are copied from
-    webob.Request.as_bytes.
-    """
-    url = req.url
-    host = req.host_url
-    assert url.startswith(host)
-    url = url[len(host):]
-    parts = [safe_encode('%s %s %s' % (req.method, url, req.http_version))]
-
-    for k, v in sorted(req.headers.items()):
-        header = safe_encode('%s: %s' % (k, v))
-        parts.append(header)
-
-    if req.body:
-        parts.extend([b'', safe_encode(req.body)])
-
-    return b'\r\n'.join(parts).decode(req.charset)
-
-
-def validate_command(string):
-    """
-    Check if the string is legal for command
-
-    raise invalidvalue if illegal
-    """
-    if string != shlex.quote(string):
-        raise exception.InvalidValue(value=string)
```

### Comparing `trove-21.0.0.0rc2/trove/common/views.py` & `trove-8.0.1/trove/common/views.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 
 def create_links(resource_path, request, id):
     """Creates the links dictionary in the format typical of most resources."""
     context = request.environ[wsgi.CONTEXT_KEY]
     link_info = {
         'host': request.host,
         'version': request.url_version,
-        'tenant_id': context.project_id,
+        'tenant_id': context.tenant,
         'resource_path': resource_path,
         'id': id,
     }
     return [
         {
             "href": "https://%(host)s/v%(version)s/%(tenant_id)s"
                     "/%(resource_path)s/%(id)s" % link_info,
```

### Comparing `trove-21.0.0.0rc2/trove/common/wsgi.py` & `trove-8.0.1/trove/common/wsgi.py`

 * *Files 1% similar despite different names*

```diff
@@ -76,15 +76,15 @@
         launcher.wait()
 
     """
     LOG.debug("Trove started on %s", host)
     app = pastedeploy.paste_deploy_app(paste_config_file, app_name, data)
     server = base_wsgi.Service(app, port, host=host,
                                backlog=backlog, threads=threads)
-    return service.launch(CONF, server, workers, restart_method='mutate')
+    return service.launch(CONF, server, workers)
 
 
 # Note: taken from Nova
 def serializers(**serializers):
     """Attaches serializers to a method.
 
     This decorator associates a dictionary of serializers with a
@@ -193,24 +193,24 @@
         bm = self.accept.best_match(ctypes.keys())
 
         return ctypes.get(bm, 'application/json')
 
     @utils.cached_property
     def accept_version(self):
         accept_header = self.headers.get('ACCEPT', "")
-        accept_version_re = re.compile(r".*?application/vnd.openstack.trove"
-                                       r"(\+.+?)?;"
-                                       r"version=(?P<version_no>\d+\.?\d*)")
+        accept_version_re = re.compile(".*?application/vnd.openstack.trove"
+                                       "(\+.+?)?;"
+                                       "version=(?P<version_no>\d+\.?\d*)")
 
         match = accept_version_re.search(accept_header)
         return match.group("version_no") if match else None
 
     @utils.cached_property
     def url_version(self):
-        versioned_url_re = re.compile(r"/v(?P<version_no>\d+\.?\d*)")
+        versioned_url_re = re.compile("/v(?P<version_no>\d+\.?\d*)")
         match = versioned_url_re.search(self.path)
         return match.group("version_no") if match else None
 
 
 class Result(object):
     """A result whose serialization is compatible with JSON."""
 
@@ -297,15 +297,15 @@
             return super(Resource, self).serialize_response(
                 action, action_result, accept)
         except Exception:
             # execute_action either returns the results or a Fault object.
             # If action_result is not a Fault then there really was a
             # serialization error which we log. Otherwise return the Fault.
             if not isinstance(action_result, Fault):
-                LOG.exception("Unserializable result detected.")
+                LOG.exception(_("Unserializable result detected."))
                 raise
             return action_result
 
 
 class Controller(object):
     """Base controller that creates a Resource with default serializers."""
 
@@ -320,15 +320,14 @@
         webob.exc.HTTPForbidden: [
             exception.ReplicaSourceDeleteForbidden,
             exception.BackupTooLarge,
             exception.ModuleAccessForbidden,
             exception.ModuleAppliedToInstance,
             exception.PolicyNotAuthorized,
             exception.LogAccessForbidden,
-            exception.TroveOperationAuthError,
         ],
         webob.exc.HTTPBadRequest: [
             exception.InvalidModelError,
             exception.BadRequest,
             exception.CannotResizeToSameSize,
             exception.BadValue,
             exception.DatabaseAlreadyExists,
@@ -344,37 +343,34 @@
             exception.DatabaseNotFound,
             exception.QuotaResourceUnknown,
             exception.BackupFileNotFound,
             exception.ClusterNotFound,
             exception.DatastoreNotFound,
             exception.SwiftNotFound,
             exception.ModuleTypeNotFound,
-            exception.RootHistoryNotFound,
         ],
         webob.exc.HTTPConflict: [
             exception.BackupNotCompleteError,
             exception.RestoreBackupIntegrityError,
         ],
         webob.exc.HTTPRequestEntityTooLarge: [
             exception.OverLimit,
             exception.QuotaExceeded,
             exception.VolumeQuotaExceeded,
         ],
         webob.exc.HTTPServerError: [
             exception.VolumeCreationFailure,
             exception.UpdateGuestError,
-            exception.SwiftConnectionError,
         ],
         webob.exc.HTTPNotImplemented: [
             exception.VolumeNotSupported,
             exception.LocalStorageNotSupported,
             exception.DatastoreOperationNotSupported,
             exception.ClusterInstanceOperationNotSupported,
-            exception.ClusterDatastoreNotSupported,
-            exception.LogsNotAvailable
+            exception.ClusterDatastoreNotSupported
         ],
     }
 
     schemas = {}
 
     @classmethod
     def get_schema(cls, action, body):
@@ -453,15 +449,15 @@
 
 class Fault(webob.exc.HTTPException):
     """Error codes for API faults."""
 
     code_wrapper = {
         400: webob.exc.HTTPBadRequest,
         401: webob.exc.HTTPUnauthorized,
-        403: webob.exc.HTTPForbidden,
+        403: webob.exc.HTTPUnauthorized,
         404: webob.exc.HTTPNotFound,
     }
 
     resp_codes = [int(code) for code in code_wrapper.keys()]
 
     def __init__(self, exception):
         """Create a Fault for the given webob.exc.exception."""
@@ -507,15 +503,15 @@
         if self.wrapped_exc.detail:
             fault_data[fault_name]['message'] = self.wrapped_exc.detail
         else:
             fault_data[fault_name]['message'] = self.wrapped_exc.explanation
 
         content_type = req.best_match_content_type()
         serializer = {
-            'application/json': JSONDictSerializer(),
+            'application/json': base_wsgi.JSONDictSerializer(),
         }[content_type]
 
         self.wrapped_exc.body = serializer.serialize(fault_data, content_type)
         self.wrapped_exc.content_type = content_type
         return self.wrapped_exc
 
 
@@ -544,16 +540,16 @@
         is_admin = False
         for role in roles:
             if role.lower() in self.admin_roles:
                 is_admin = True
                 break
         limits = self._extract_limits(request.params)
         context = rd_context.TroveContext(auth_token=auth_token,
-                                          project_id=tenant_id,
-                                          user_id=user_id,
+                                          tenant=tenant_id,
+                                          user=user_id,
                                           is_admin=is_admin,
                                           limit=limits.get('limit'),
                                           marker=limits.get('marker'),
                                           service_catalog=service_catalog,
                                           roles=roles)
         request.environ[CONTEXT_KEY] = context
 
@@ -578,15 +574,15 @@
                 for (header, value) in resp._headerlist:
                     if header == "Content-Type" and \
                             value == "text/plain; charset=UTF-8":
                         return Fault(Fault.code_wrapper[resp.status_int]())
                 return resp
             return resp
         except Exception as ex:
-            LOG.exception("Caught error: %s.",
+            LOG.exception(_("Caught error: %s."),
                           encodeutils.exception_to_unicode(ex))
             exc = webob.exc.HTTPInternalServerError()
             return Fault(exc)
 
     @classmethod
     def factory(cls, global_config, **local_config):
         def _factory(app):
@@ -655,7 +651,14 @@
     """Default request body serialization."""
 
     def serialize(self, data, action='default'):
         return self.dispatch(data, action=action)
 
     def default(self, data):
         return ""
+
+
+class JSONDictSerializer(DictSerializer):
+    """Default JSON request body serialization."""
+
+    def default(self, data):
+        return jsonutils.dump_as_bytes(data)
```

### Comparing `trove-21.0.0.0rc2/trove/common/xmlutils.py` & `trove-8.0.1/trove/common/xmlutils.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/conductor/api.py` & `trove-8.0.1/trove/conductor/api.py`

 * *Files 1% similar despite different names*

```diff
@@ -88,21 +88,22 @@
         cctxt = self.client.prepare(version=version)
         cctxt.cast(self.context, "update_backup",
                    instance_id=instance_id,
                    backup_id=backup_id,
                    sent=sent,
                    **backup_fields)
 
-    def report_root(self, instance_id):
+    def report_root(self, instance_id, user):
         LOG.debug("Making async call to cast report_root for instance: %s",
                   instance_id)
         version = self.API_BASE_VERSION
         cctxt = self.client.prepare(version=version)
         cctxt.cast(self.context, "report_root",
-                   instance_id=instance_id)
+                   instance_id=instance_id,
+                   user=user)
 
     def notify_end(self, **notification_args):
         LOG.debug("Making async call to cast end notification")
         version = self.API_BASE_VERSION
         cctxt = self.client.prepare(version=version)
         context = self.context
         serialized = SerializableNotification.serialize(context,
```

### Comparing `trove-21.0.0.0rc2/trove/conductor/manager.py` & `trove-8.0.1/trove/conductor/manager.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,20 +15,21 @@
 from oslo_log import log as logging
 import oslo_messaging as messaging
 from oslo_service import periodic_task
 
 from trove.backup import models as bkup_models
 from trove.common import cfg
 from trove.common import exception as trove_exception
+from trove.common.i18n import _
+from trove.common.instance import ServiceStatus
 from trove.common.rpc import version as rpc_version
 from trove.common.serializable_notification import SerializableNotification
 from trove.conductor.models import LastSeen
 from trove.extensions.mysql import models as mysql_models
 from trove.instance import models as inst_models
-from trove.instance import service_status as svc_status
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
 
 class Manager(periodic_task.PeriodicTasks):
 
@@ -41,16 +42,16 @@
         fields = {
             "instance": instance_id,
             "method": method_name,
             "sent": sent,
         }
 
         if sent is None:
-            LOG.error("[Instance %s] sent field not present. Cannot "
-                      "compare.", instance_id)
+            LOG.error(_("[Instance %s] sent field not present. Cannot "
+                        "compare."), instance_id)
             return False
 
         LOG.debug("Instance %(instance)s sent %(method)s at %(sent)s ", fields)
 
         seen = None
         try:
             seen = LastSeen.load(instance_id=instance_id,
@@ -72,38 +73,29 @@
         if last_sent < sent:
             LOG.debug("[Instance %s] Rec'd message is younger than last "
                       "seen. Updating.", instance_id)
             seen.sent = sent
             seen.save()
             return False
 
-        LOG.info("[Instance %s] Rec'd message is older than last seen. "
-                 "Discarding.", instance_id)
+        LOG.info(_("[Instance %s] Rec'd message is older than last seen. "
+                   "Discarding."), instance_id)
         return True
 
     def heartbeat(self, context, instance_id, payload, sent=None):
         LOG.debug("Instance ID: %(instance)s, Payload: %(payload)s",
                   {"instance": str(instance_id),
                    "payload": str(payload)})
         status = inst_models.InstanceServiceStatus.find_by(
             instance_id=instance_id)
-
         if self._message_too_old(instance_id, 'heartbeat', sent):
             return
-
-        if status.get_status() == svc_status.ServiceStatuses.RESTART_REQUIRED:
-            LOG.debug("Instance %s service status is RESTART_REQUIRED, "
-                      "skip heartbeat", instance_id)
-            return
-
         if payload.get('service_status') is not None:
-            status.set_status(
-                svc_status.ServiceStatus.from_description(
-                    payload['service_status'])
-            )
+            status.set_status(ServiceStatus.from_description(
+                payload['service_status']))
         status.save()
 
     def update_backup(self, context, instance_id, backup_id,
                       sent=None, **backup_fields):
         LOG.debug("Instance ID: %(instance)s, Backup ID: %(backup)s",
                   {"instance": str(instance_id),
                    "backup": str(backup_id)})
@@ -116,51 +108,46 @@
         # Some verification based on IDs
         if backup_id != backup.id:
             fields = {
                 'expected': backup_id,
                 'found': backup.id,
                 'instance': str(instance_id),
             }
-            LOG.error("[Instance: %(instance)s] Backup IDs mismatch! "
-                      "Expected %(expected)s, found %(found)s", fields)
+            LOG.error(_("[Instance: %(instance)s] Backup IDs mismatch! "
+                        "Expected %(expected)s, found %(found)s"), fields)
             return
         if instance_id != backup.instance_id:
             fields = {
                 'expected': instance_id,
                 'found': backup.instance_id,
                 'instance': str(instance_id),
             }
-            LOG.error("[Instance: %(instance)s] Backup instance IDs "
-                      "mismatch! Expected %(expected)s, found "
-                      "%(found)s", fields)
+            LOG.error(_("[Instance: %(instance)s] Backup instance IDs "
+                        "mismatch! Expected %(expected)s, found "
+                        "%(found)s"), fields)
             return
 
         for k, v in backup_fields.items():
             if hasattr(backup, k):
                 fields = {
                     'key': k,
                     'value': v,
                 }
                 LOG.debug("Backup %(key)s: %(value)s", fields)
                 setattr(backup, k, v)
         backup.save()
 
-    # NOTE(zhaochao): the 'user' argument is left here to keep
-    # compatible with existing instances.
-    def report_root(self, context, instance_id, user=None):
-        if user is not None:
-            LOG.debug("calling report_root with a username: %s, "
-                      "is deprecated now!" % user)
-        mysql_models.RootHistory.create(context, instance_id)
+    def report_root(self, context, instance_id, user):
+        mysql_models.RootHistory.create(context, instance_id, user)
 
     def notify_end(self, context, serialized_notification, notification_args):
         notification = SerializableNotification.deserialize(
             context, serialized_notification)
         notification.notify_end(**notification_args)
 
     def notify_exc_info(self, context, serialized_notification,
                         message, exception):
         notification = SerializableNotification.deserialize(
             context, serialized_notification)
-        LOG.error("Guest exception on request %(req)s:\n%(exc)s",
+        LOG.error(_("Guest exception on request %(req)s:\n%(exc)s"),
                   {'req': notification.request_id, 'exc': exception})
         notification.notify_exc_info(message, exception)
```

### Comparing `trove-21.0.0.0rc2/trove/conductor/models.py` & `trove-8.0.1/trove/conductor/models.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/configuration/models.py` & `trove-8.0.1/trove/configuration/models.py`

 * *Files 8% similar despite different names*

```diff
@@ -44,19 +44,19 @@
             raise TypeError(_("Argument is not defined."))
 
         if context.is_admin:
             db_info = DBConfiguration.find_all(deleted=False)
             if db_info.count() == 0:
                 LOG.debug("No configurations found for admin user")
         else:
-            db_info = DBConfiguration.find_all(tenant_id=context.project_id,
+            db_info = DBConfiguration.find_all(tenant_id=context.tenant,
                                                deleted=False)
             if db_info.count() == 0:
                 LOG.debug("No configurations found for tenant %s",
-                          context.project_id)
+                          context.tenant)
 
         limit = utils.pagination_limit(context.limit,
                                        Configurations.DEFAULT_LIMIT)
         data_view = DBConfiguration.find_by_pagination('configurations',
                                                        db_info,
                                                        "foo",
                                                        limit=limit,
@@ -129,15 +129,15 @@
     @staticmethod
     def load(context, id):
         try:
             if context.is_admin:
                 return DBConfiguration.find_by(id=id, deleted=False)
             else:
                 return DBConfiguration.find_by(id=id,
-                                               tenant_id=context.project_id,
+                                               tenant_id=context.tenant,
                                                deleted=False)
         except ModelNotFoundError:
             msg = _("Configuration group with ID %s could not be found.") % id
             raise ModelNotFoundError(msg)
 
     @staticmethod
     def find_parameter_details(name, detail_list):
@@ -162,16 +162,14 @@
                 str(item.configuration_key), detail_list)
             if not rule:
                 continue
             if rule.data_type == 'boolean':
                 item.configuration_value = bool(int(item.configuration_value))
             elif rule.data_type == 'integer':
                 item.configuration_value = int(item.configuration_value)
-            elif rule.data_type == 'float':
-                item.configuration_value = float(item.configuration_value)
             else:
                 item.configuration_value = str(item.configuration_value)
         return config_items
 
     def get_configuration_overrides(self):
         """Gets the overrides dictionary to apply to an instance."""
         overrides = {}
@@ -187,15 +185,15 @@
         datastore_v = Configuration.load_configuration_datastore_version(
             self.context,
             self.configuration_id)
         config_items = Configuration.load_items(self.context,
                                                 id=self.configuration_id)
         LOG.debug("config_items: %s", config_items)
         detail_list = DatastoreConfigurationParameters.load_parameters(
-            datastore_v.id)
+            datastore_v.id, show_deleted=True)
 
         for i in config_items:
             LOG.debug("config item: %s", i)
             details = Configuration.find_parameter_details(
                 i.configuration_key, detail_list)
             LOG.debug("parameter details: %s", details)
             if not details:
@@ -226,15 +224,14 @@
             config_datastore_version=info.datastore_version_id,
             instance_datastore_version=datastore_version_id)
 
 
 class DBConfiguration(dbmodels.DatabaseModelBase):
     _data_fields = ['name', 'description', 'tenant_id', 'datastore_version_id',
                     'deleted', 'deleted_at', 'created', 'updated']
-    _table_name = 'configurations'
 
     @property
     def datastore(self):
         datastore_version = dstore_models.DatastoreVersion.load_by_uuid(
             self.datastore_version_id)
         datastore = dstore_models.Datastore.load(
             datastore_version.datastore_id)
@@ -244,35 +241,37 @@
     def datastore_version(self):
         datastore_version = dstore_models.DatastoreVersion.load_by_uuid(
             self.datastore_version_id)
         return datastore_version
 
 
 class DBConfigurationParameter(dbmodels.DatabaseModelBase):
-    _auto_generated_attrs = []
     _data_fields = ['configuration_id', 'configuration_key',
                     'configuration_value', 'deleted',
                     'deleted_at']
-    _table_name = 'configuration_parameters'
 
     def __hash__(self):
         return self.configuration_key.__hash__()
 
 
 class DBDatastoreConfigurationParameters(dbmodels.DatabaseModelBase):
     """Model for storing the configuration parameters on a datastore."""
+    _auto_generated_attrs = ['id']
     _data_fields = [
         'name',
         'datastore_version_id',
         'restart_required',
         'max_size',
         'min_size',
         'data_type',
+        'deleted',
+        'deleted_at',
     ]
     _table_name = "datastore_configuration_parameters"
+    preserve_on_delete = True
 
 
 class DatastoreConfigurationParameters(object):
 
     def __init__(self, db_info):
         self.db_info = db_info
 
@@ -283,53 +282,88 @@
         # Do we already have a parameter in the db?
         # yes: and its deleted then modify the param
         # yes: and its not deleted then error on create.
         # no: then just create the new param
         ds_v_id = kwargs.get('datastore_version_id')
         config_param_name = kwargs.get('name')
         try:
-            DatastoreConfigurationParameters.load_parameter_by_name(
+            param = DatastoreConfigurationParameters.load_parameter_by_name(
                 ds_v_id,
-                config_param_name)
-            raise exception.ConfigurationParameterAlreadyExists(
-                parameter_name=config_param_name,
-                datastore_version=ds_v_id)
+                config_param_name,
+                show_deleted=True)
+            if param.deleted == 1:
+                param.restart_required = kwargs.get('restart_required')
+                param.data_type = kwargs.get('data_type')
+                param.max_size = kwargs.get('max_size')
+                param.min_size = kwargs.get('min_size')
+                param.deleted = 0
+                param.save()
+                return param
+            else:
+                raise exception.ConfigurationParameterAlreadyExists(
+                    parameter_name=config_param_name,
+                    datastore_version=ds_v_id)
         except exception.NotFound:
             pass
         config_param = DBDatastoreConfigurationParameters.create(
             **kwargs)
         return config_param
 
     @staticmethod
     def delete(version_id, config_param_name):
         config_param = DatastoreConfigurationParameters.load_parameter_by_name(
             version_id, config_param_name)
-        config_param.delete()
+        config_param.deleted = True
+        config_param.deleted_at = timeutils.utcnow()
+        config_param.save()
 
     @classmethod
-    def load_parameters(cls, datastore_version_id):
-        return DBDatastoreConfigurationParameters.find_all(
-            datastore_version_id=datastore_version_id)
+    def load_parameters(cls, datastore_version_id, show_deleted=False):
+        try:
+            if show_deleted:
+                return DBDatastoreConfigurationParameters.find_all(
+                    datastore_version_id=datastore_version_id
+                )
+            else:
+                return DBDatastoreConfigurationParameters.find_all(
+                    datastore_version_id=datastore_version_id,
+                    deleted=False
+                )
+        except exception.NotFound:
+            raise exception.NotFound(uuid=datastore_version_id)
 
     @classmethod
-    def load_parameter(cls, config_id):
+    def load_parameter(cls, config_id, show_deleted=False):
         try:
-            return DBDatastoreConfigurationParameters.find_by(
-                id=config_id, deleted=False
-            )
+            if show_deleted:
+                return DBDatastoreConfigurationParameters.find_by(
+                    id=config_id
+                )
+            else:
+                return DBDatastoreConfigurationParameters.find_by(
+                    id=config_id, deleted=False
+                )
         except exception.NotFound:
             raise exception.NotFound(uuid=config_id)
 
     @classmethod
-    def load_parameter_by_name(cls, datastore_version_id, config_param_name):
+    def load_parameter_by_name(cls, datastore_version_id, config_param_name,
+                               show_deleted=False):
         try:
-            return DBDatastoreConfigurationParameters.find_by(
-                datastore_version_id=datastore_version_id,
-                name=config_param_name
-            )
+            if show_deleted:
+                return DBDatastoreConfigurationParameters.find_by(
+                    datastore_version_id=datastore_version_id,
+                    name=config_param_name
+                )
+            else:
+                return DBDatastoreConfigurationParameters.find_by(
+                    datastore_version_id=datastore_version_id,
+                    name=config_param_name,
+                    deleted=False
+                )
         except exception.NotFound:
             raise exception.NotFound(uuid=config_param_name)
 
 
 def create_or_update_datastore_configuration_parameter(name,
                                                        datastore_version_id,
                                                        restart_required,
@@ -337,63 +371,53 @@
                                                        max_size,
                                                        min_size):
     get_db_api().configure_db(CONF)
     datastore_version = dstore_models.DatastoreVersion.load_by_uuid(
         datastore_version_id)
     try:
         config = DatastoreConfigurationParameters.load_parameter_by_name(
-            datastore_version_id, name)
+            datastore_version_id, name, show_deleted=True)
         config.restart_required = restart_required
         config.max_size = max_size
         config.min_size = min_size
         config.data_type = data_type
         get_db_api().save(config)
     except exception.NotFound:
         config = DBDatastoreConfigurationParameters(
             id=utils.generate_uuid(),
             name=name,
             datastore_version_id=datastore_version.id,
             restart_required=restart_required,
             data_type=data_type,
             max_size=max_size,
             min_size=min_size,
+            deleted=False,
         )
         get_db_api().save(config)
 
 
-def load_datastore_configuration_parameters(datastore, datastore_version,
-                                            config_file, version_number=None):
+def load_datastore_configuration_parameters(datastore,
+                                            datastore_version,
+                                            config_file):
     get_db_api().configure_db(CONF)
     (ds, ds_v) = dstore_models.get_datastore_version(
-        type=datastore, version=datastore_version, return_inactive=True,
-        version_number=version_number)
+        type=datastore, version=datastore_version, return_inactive=True)
     with open(config_file) as f:
         config = json.load(f)
         for param in config['configuration-parameters']:
             create_or_update_datastore_configuration_parameter(
                 param['name'],
                 ds_v.id,
                 param['restart_required'],
                 param['type'],
                 param.get('max'),
                 param.get('min'),
             )
 
 
-def remove_datastore_configuration_parameters(datastore, datastore_version,
-                                              version_number=None):
-    get_db_api().configure_db(CONF)
-    (ds, ds_version) = dstore_models.get_datastore_version(
-        type=datastore, version=datastore_version, return_inactive=True,
-        version_number=version_number)
-    db_params = DatastoreConfigurationParameters.load_parameters(ds_version.id)
-    for db_param in db_params:
-        db_param.delete()
-
-
 def persisted_models():
     return {
         'configurations': DBConfiguration,
         'configuration_parameters': DBConfigurationParameter,
         'datastore_configuration_parameters':
-            DBDatastoreConfigurationParameters,
+        DBDatastoreConfigurationParameters,
     }
```

### Comparing `trove-21.0.0.0rc2/trove/configuration/service.py` & `trove-8.0.1/trove/configuration/service.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from oslo_log import log as logging
+import six
 
 from trove.cluster import models as cluster_models
 import trove.common.apischema as apischema
 from trove.common import cfg
 from trove.common import exception
 from trove.common.i18n import _
 from trove.common import notification
@@ -60,41 +61,31 @@
         LOG.debug("Showing configuration group %(id)s on tenant %(tenant)s",
                   {"tenant": tenant_id, "id": id})
         context = req.environ[wsgi.CONTEXT_KEY]
         configuration = models.Configuration.load(context, id)
         self.authorize_config_action(context, 'show', configuration)
         configuration_items = models.Configuration.load_items(context, id)
 
-        find_instance = {
-            'configuration_id': configuration.id,
-            'deleted': False
-        }
-        if not context.is_admin:
-            find_instance['tenant_id'] = context.project_id
-
         configuration.instance_count = instances_models.DBInstance.find_all(
-            **find_instance).count()
+            tenant_id=context.tenant,
+            configuration_id=configuration.id,
+            deleted=False).count()
 
         return wsgi.Result(views.DetailedConfigurationView(
                            configuration,
                            configuration_items).data(), 200)
 
     def instances(self, req, tenant_id, id):
         context = req.environ[wsgi.CONTEXT_KEY]
         configuration = models.Configuration.load(context, id)
         self.authorize_config_action(context, 'instances', configuration)
-
-        kwargs = {
-            'configuration_id': configuration.id,
-            'deleted': False
-        }
-        if not context.is_admin:
-            kwargs['tenant_id'] = context.project_id
-        instances = instances_models.DBInstance.find_all(**kwargs)
-
+        instances = instances_models.DBInstance.find_all(
+            tenant_id=context.tenant,
+            configuration_id=configuration.id,
+            deleted=False)
         limit = int(context.limit or CONF.instances_page_size)
         if limit > CONF.instances_page_size:
             limit = CONF.instances_page_size
         data_view = instances_models.DBInstance.find_by_pagination(
             'instances', instances, "foo",
             limit=limit,
             marker=context.marker)
@@ -111,16 +102,16 @@
         policy.authorize_on_tenant(context, 'configuration:create')
         context.notification = notification.DBaaSConfigurationCreate(
             context, request=req)
         name = body['configuration']['name']
         description = body['configuration'].get('description')
         values = body['configuration']['values']
 
-        msg = ("Creating configuration group on tenant "
-               "%(tenant_id)s with name: %(cfg_name)s")
+        msg = _("Creating configuration group on tenant "
+                "%(tenant_id)s with name: %(cfg_name)s")
         LOG.info(msg, {"tenant_id": tenant_id, "cfg_name": name})
 
         datastore_args = body['configuration'].get('datastore', {})
         datastore, datastore_version = (
             ds_models.get_datastore_version(**datastore_args))
 
         with StartNotification(context, name=name, datastore=datastore.name,
@@ -148,36 +139,36 @@
                     cfg_group.id, values)
 
         view_data = views.DetailedConfigurationView(cfg_group,
                                                     cfg_group_items)
         return wsgi.Result(view_data.data(), 200)
 
     def delete(self, req, tenant_id, id):
-        msg = ("Deleting configuration group %(cfg_id)s on tenant: "
-               "%(tenant_id)s")
+        msg = _("Deleting configuration group %(cfg_id)s on tenant: "
+                "%(tenant_id)s")
         LOG.info(msg, {"tenant_id": tenant_id, "cfg_id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         group = models.Configuration.load(context, id)
         self.authorize_config_action(context, 'delete', group)
         context.notification = notification.DBaaSConfigurationDelete(
             context, request=req)
         with StartNotification(context, configuration_id=id):
             instances = instances_models.DBInstance.find_all(
-                tenant_id=context.project_id,
+                tenant_id=context.tenant,
                 configuration_id=id,
                 deleted=False).all()
             if instances:
                 raise exception.InstanceAssignedToConfiguration()
             models.Configuration.delete(context, group)
         return wsgi.Result(None, 202)
 
     def update(self, req, body, tenant_id, id):
-        msg = ("Updating configuration group %(cfg_id)s for tenant "
-               "id %(tenant_id)s")
+        msg = _("Updating configuration group %(cfg_id)s for tenant "
+                "id %(tenant_id)s")
         LOG.info(msg, {"tenant_id": tenant_id, "cfg_id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         group = models.Configuration.load(context, id)
         # Note that changing the configuration group will also
         # indirectly affect all the instances which attach it.
         #
@@ -223,34 +214,36 @@
             models.Configuration.save(group, items)
             self._refresh_on_all_instances(context, id)
             self._refresh_on_all_clusters(context, id)
 
     def _refresh_on_all_instances(self, context, configuration_id):
         """Refresh a configuration group on all single instances.
         """
+        LOG.debug("Re-applying configuration group '%s' to all instances.",
+                  configuration_id)
         single_instances = instances_models.DBInstance.find_all(
-            tenant_id=context.project_id,
+            tenant_id=context.tenant,
             configuration_id=configuration_id,
             cluster_id=None,
             deleted=False).all()
 
         config = models.Configuration(context, configuration_id)
         for dbinstance in single_instances:
-            LOG.info("Re-applying configuration %s to instance: %s",
-                     configuration_id, dbinstance.id)
+            LOG.debug("Re-applying configuration to instance: %s",
+                      dbinstance.id)
             instance = instances_models.Instance.load(context, dbinstance.id)
             instance.update_configuration(config)
 
     def _refresh_on_all_clusters(self, context, configuration_id):
         """Refresh a configuration group on all clusters.
         """
         LOG.debug("Re-applying configuration group '%s' to all clusters.",
                   configuration_id)
         clusters = cluster_models.DBCluster.find_all(
-            tenant_id=context.project_id,
+            tenant_id=context.tenant,
             configuration_id=configuration_id,
             deleted=False).all()
 
         for dbcluster in clusters:
             LOG.debug("Re-applying configuration to cluster: %s", dbcluster.id)
             cluster = cluster_models.Cluster.load(context, dbcluster.id)
             cluster.configuration_attach(configuration_id)
@@ -272,15 +265,15 @@
                     configuration_key=k,
                     configuration_value=v,
                     deleted=False))
         return items
 
     @staticmethod
     def _validate_configuration(values, datastore_version, config_rules):
-        LOG.info("Validating configuration values")
+        LOG.info(_("Validating configuration values"))
 
         # create rules dictionary based on parameter name
         rules_lookup = {}
         for item in config_rules:
             rules_lookup[item.name.lower()] = item
 
         # checking if there are any rules for the datastore
@@ -312,15 +305,15 @@
                     value_type)):
                 output = {"key": k, "type": value_type}
                 msg = _("The value provided for the configuration "
                         "parameter %(key)s is not of type %(type)s.") % output
                 raise exception.UnprocessableEntity(message=msg)
 
             # integer min/max checking
-            if isinstance(v, int) and not isinstance(v, bool):
+            if isinstance(v, six.integer_types) and not isinstance(v, bool):
                 if rule.min_size is not None:
                     try:
                         min_value = int(rule.min_size)
                     except ValueError:
                         raise exception.TroveError(_(
                             "Invalid or unsupported min value defined in the "
                             "configuration-parameters configuration file. "
@@ -350,17 +343,17 @@
                         raise exception.UnprocessableEntity(message=message)
 
     @staticmethod
     def _find_type(value_type):
         if value_type == "boolean":
             return bool
         elif value_type == "string":
-            return str
+            return six.string_types
         elif value_type == "integer":
-            return int
+            return six.integer_types
         elif value_type == "float":
             return float
         else:
             raise exception.TroveError(_(
                 "Invalid or unsupported type defined in the "
                 "configuration-parameters configuration file."))
```

### Comparing `trove-21.0.0.0rc2/trove/configuration/views.py` & `trove-8.0.1/trove/configuration/views.py`

 * *Files 2% similar despite different names*

```diff
@@ -95,22 +95,20 @@
             "id": self.configuration.id,
             "name": self.configuration.name,
             "description": self.configuration.description,
             "values": strutils.mask_dict_password(values),
             "created": self.configuration.created,
             "updated": self.configuration.updated,
             "instance_count":
-                getattr(self.configuration, "instance_count", 0),
+            getattr(self.configuration, "instance_count", 0),
             "datastore_name": self.configuration.datastore.name,
             "datastore_version_id":
-                self.configuration.datastore_version_id,
+            self.configuration.datastore_version_id,
             "datastore_version_name":
-                self.configuration.datastore_version.name,
-            "datastore_version_number":
-                self.configuration.datastore_version.version
+            self.configuration.datastore_version.name
         }
 
         return {"configuration": configuration_dict}
 
 
 class ConfigurationParameterView(object):
```

### Comparing `trove-21.0.0.0rc2/trove/datastore/models.py` & `trove-8.0.1/trove/datastore/models.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,71 +12,67 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from oslo_log import log as logging
-from oslo_utils import uuidutils
 
 from trove.common import cfg
-from trove.common.clients import create_nova_client
 from trove.common import exception
 from trove.common.i18n import _
+from trove.common.remote import create_nova_client
 from trove.common import timeutils
 from trove.common import utils
 from trove.db import get_db_api
 from trove.db import models as dbmodels
 from trove.flavor.models import Flavor as flavor_model
 from trove.volume_type import models as volume_type_models
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 db_api = get_db_api()
 
 
 def persisted_models():
     return {
-        'datastores': DBDatastore,
+        'datastore': DBDatastore,
         'capabilities': DBCapabilities,
-        'datastore_versions': DBDatastoreVersion,
+        'datastore_version': DBDatastoreVersion,
         'capability_overrides': DBCapabilityOverrides,
         'datastore_version_metadata': DBDatastoreVersionMetadata
     }
 
 
 class DBDatastore(dbmodels.DatabaseModelBase):
 
-    _data_fields = ['name', 'default_version_id']
-    _table_name = 'datastores'
+    _data_fields = ['id', 'name', 'default_version_id']
 
 
 class DBCapabilities(dbmodels.DatabaseModelBase):
 
-    _data_fields = ['name', 'description', 'enabled']
-    _table_name = 'capabilities'
+    _data_fields = ['id', 'name', 'description', 'enabled']
 
 
 class DBCapabilityOverrides(dbmodels.DatabaseModelBase):
 
-    _data_fields = ['capability_id', 'datastore_version_id', 'enabled']
-    _table_name = 'capability_overrides'
+    _data_fields = ['id', 'capability_id', 'datastore_version_id', 'enabled']
 
 
 class DBDatastoreVersion(dbmodels.DatabaseModelBase):
-    _data_fields = ['datastore_id', 'name', 'image_id', 'image_tags',
-                    'packages', 'active', 'manager', 'version']
-    _table_name = 'datastore_versions'
+
+    _data_fields = ['id', 'datastore_id', 'name', 'manager', 'image_id',
+                    'packages', 'active']
 
 
 class DBDatastoreVersionMetadata(dbmodels.DatabaseModelBase):
 
-    _data_fields = ['datastore_version_id', 'key', 'value',
+    _data_fields = ['id', 'datastore_version_id', 'key', 'value',
                     'created', 'deleted', 'deleted_at', 'updated_at']
-    _table_name = 'datastore_version_metadata'
+    preserve_on_delete = True
 
 
 class Capabilities(object):
 
     def __init__(self, datastore_version_id=None):
         self.capabilities = []
         self.datastore_version_id = datastore_version_id
@@ -396,38 +392,26 @@
         self.db_info = db_info
         self._datastore_name = None
 
     def __repr__(self, *args, **kwargs):
         return "%s(%s)" % (self.name, self.id)
 
     @classmethod
-    def load(cls, datastore, id_or_name, version=None):
-        if uuidutils.is_uuid_like(id_or_name):
+    def load(cls, datastore, id_or_name):
+        try:
             return cls(DBDatastoreVersion.find_by(datastore_id=datastore.id,
                                                   id=id_or_name))
-
-        if not version:
+        except exception.ModelNotFoundError:
             versions = DBDatastoreVersion.find_all(datastore_id=datastore.id,
                                                    name=id_or_name)
             if versions.count() == 0:
                 raise exception.DatastoreVersionNotFound(version=id_or_name)
             if versions.count() > 1:
-                raise exception.DatastoreVersionsNoUniqueMatch(name=id_or_name)
-
-            db_version = versions.first()
-        else:
-            try:
-                db_version = DBDatastoreVersion.find_by(
-                    datastore_id=datastore.id,
-                    name=id_or_name,
-                    version=version)
-            except exception.ModelNotFoundError:
-                raise exception.DatastoreVersionNotFound(version=version)
-
-        return cls(db_version)
+                raise exception.NoUniqueMatch(name=id_or_name)
+            return cls(versions.first())
 
     @classmethod
     def load_by_uuid(cls, uuid):
         try:
             return cls(DBDatastoreVersion.find_by(id=uuid))
         except exception.ModelNotFoundError:
             raise exception.DatastoreVersionNotFound(version=uuid)
@@ -456,18 +440,14 @@
         return self.db_info.name
 
     @property
     def image_id(self):
         return self.db_info.image_id
 
     @property
-    def image_tags(self):
-        return self.db_info.image_tags
-
-    @property
     def packages(self):
         return self.db_info.packages
 
     @property
     def active(self):
         return (True if self.db_info.active else False)
 
@@ -483,18 +463,14 @@
     @property
     def capabilities(self):
         if self._capabilities is None:
             self._capabilities = Capabilities.load(self.db_info.id)
 
         return self._capabilities
 
-    @property
-    def version(self):
-        return self.db_info.version
-
 
 class DatastoreVersions(object):
 
     def __init__(self, db_info):
         self.db_info = db_info
 
     @classmethod
@@ -514,33 +490,31 @@
         return cls(DBDatastoreVersion.find_all())
 
     def __iter__(self):
         for item in self.db_info:
             yield item
 
 
-def get_datastore_version(type=None, version=None, return_inactive=False,
-                          version_number=None):
+def get_datastore_version(type=None, version=None, return_inactive=False):
     datastore = type or CONF.default_datastore
     if not datastore:
         raise exception.DatastoreDefaultDatastoreNotDefined()
     try:
         datastore = Datastore.load(datastore)
     except exception.DatastoreNotFound:
         if not type:
             raise exception.DatastoreDefaultDatastoreNotFound(
                 datastore=datastore)
         raise
 
-    version_id = version or datastore.default_version_id
-    if not version_id:
+    version = version or datastore.default_version_id
+    if not version:
         raise exception.DatastoreDefaultVersionNotFound(
             datastore=datastore.name)
-    datastore_version = DatastoreVersion.load(datastore, version_id,
-                                              version=version_number)
+    datastore_version = DatastoreVersion.load(datastore, version)
     if datastore_version.datastore_id != datastore.id:
         raise exception.DatastoreNoVersion(datastore=datastore.name,
                                            version=datastore_version.name)
     if not datastore_version.active and not return_inactive:
         raise exception.DatastoreVersionInactive(
             version=datastore_version.name)
     return (datastore, datastore_version)
@@ -595,78 +569,58 @@
         datastore.default_version_id = version.id
     else:
         datastore.default_version_id = None
 
     db_api.save(datastore)
 
 
-def update_datastore_version(datastore, name, manager, image_id, image_tags,
-                             packages, active, version=None, new_name=None):
-    """Create or update datastore version."""
-    version = version or name
+def update_datastore_version(datastore, name, manager, image_id, packages,
+                             active):
     db_api.configure_db(CONF)
     datastore = Datastore.load(datastore)
     try:
-        ds_version = DBDatastoreVersion.find_by(datastore_id=datastore.id,
-                                                name=name,
-                                                version=version)
+        version = DBDatastoreVersion.find_by(datastore_id=datastore.id,
+                                             name=name)
     except exception.ModelNotFoundError:
         # Create a new one
-        ds_version = DBDatastoreVersion()
-        ds_version.id = utils.generate_uuid()
-        ds_version.version = version
-        ds_version.datastore_id = datastore.id
-    ds_version.name = new_name or name
-    ds_version.manager = manager
-    ds_version.image_id = image_id
-    ds_version.image_tags = (",".join(image_tags)
-                             if type(image_tags) is list else image_tags)
-    ds_version.packages = packages
-    ds_version.active = active
+        version = DBDatastoreVersion()
+        version.id = utils.generate_uuid()
+        version.name = name
+        version.datastore_id = datastore.id
+    version.manager = manager
+    version.image_id = image_id
+    version.packages = packages
+    version.active = active
 
-    db_api.save(ds_version)
+    db_api.save(version)
 
 
 class DatastoreVersionMetadata(object):
     @classmethod
-    def datastore_version_find(cls, datastore_name,
-                               datastore_version_name, version_number=None):
+    def _datastore_version_find(cls, datastore_name,
+                                datastore_version_name):
         """
         Helper to find a datastore version id for a given
         datastore and datastore version name.
         """
         db_api.configure_db(CONF)
         db_ds_record = DBDatastore.find_by(
             name=datastore_name
         )
-
-        if not version_number:
-            db_dsv_records = DBDatastoreVersion.find_all(
-                datastore_id=db_ds_record.id,
-                name=datastore_version_name,
-            )
-            if db_dsv_records.count() == 0:
-                raise exception.DatastoreVersionNotFound(
-                    version=datastore_version_name)
-            if db_dsv_records.count() > 1:
-                raise exception.DatastoreVersionsNoUniqueMatch(
-                    name=datastore_version_name)
-
-            db_dsv_record = db_dsv_records.first()
-        else:
-            db_dsv_record = DBDatastoreVersion.find_by(
-                datastore_id=db_ds_record.id,
-                name=datastore_version_name,
-                version=version_number
-            )
+        db_dsv_record = DBDatastoreVersion.find_by(
+            datastore_id=db_ds_record.id,
+            name=datastore_version_name
+        )
 
         return db_dsv_record.id
 
     @classmethod
-    def _datastore_version_metadata_add(cls, datastore_version_id,
+    def _datastore_version_metadata_add(cls, datastore_name,
+                                        datastore_version_name,
+                                        datastore_version_id,
                                         key, value, exception_class):
         """
         Create a record of the specified key and value in the
         metadata table.
         """
         # if an association does not exist, create a new one.
         # if a deleted association exists, undelete it.
@@ -679,106 +633,140 @@
             if db_record.deleted == 1:
                 db_record.deleted = 0
                 db_record.updated_at = timeutils.utcnow()
                 db_record.save()
                 return
             else:
                 raise exception_class(
-                    datastore_version_id=datastore_version_id,
+                    datastore=datastore_name,
+                    datastore_version=datastore_version_name,
                     id=value)
         except exception.NotFound:
             pass
 
         # the record in the database only contains the datastore_verion_id
         DBDatastoreVersionMetadata.create(
             datastore_version_id=datastore_version_id,
             key=key, value=value)
 
     @classmethod
-    def _datastore_version_metadata_delete(cls, datastore_version_id,
+    def _datastore_version_metadata_delete(cls, datastore_name,
+                                           datastore_version_name,
                                            key, value, exception_class):
         """
         Delete a record of the specified key and value in the
         metadata table.
         """
         # if an association does not exist, raise an exception
         # if a deleted association exists, raise an exception
         # if an un-deleted association exists, delete it
+
+        datastore_version_id = cls._datastore_version_find(
+            datastore_name,
+            datastore_version_name)
+
         try:
             db_record = DBDatastoreVersionMetadata.find_by(
                 datastore_version_id=datastore_version_id,
                 key=key, value=value)
             if db_record.deleted == 0:
                 db_record.delete()
                 return
             else:
                 raise exception_class(
-                    datastore_version_id=datastore_version_id,
+                    datastore=datastore_name,
+                    datastore_version=datastore_version_name,
                     id=value)
         except exception.ModelNotFoundError:
-            raise exception_class(datastore_version_id=datastore_version_id,
+            raise exception_class(datastore=datastore_name,
+                                  datastore_version=datastore_version_name,
                                   id=value)
 
     @classmethod
-    def add_datastore_version_flavor_association(cls, datastore_version_id,
+    def add_datastore_version_flavor_association(cls, datastore_name,
+                                                 datastore_version_name,
                                                  flavor_ids):
+        datastore_version_id = cls._datastore_version_find(
+            datastore_name,
+            datastore_version_name)
+
         for flavor_id in flavor_ids:
             cls._datastore_version_metadata_add(
+                datastore_name, datastore_version_name,
                 datastore_version_id, 'flavor', flavor_id,
                 exception.DatastoreFlavorAssociationAlreadyExists)
 
     @classmethod
-    def delete_datastore_version_flavor_association(cls, datastore_version_id,
+    def delete_datastore_version_flavor_association(cls, datastore_name,
+                                                    datastore_version_name,
                                                     flavor_id):
         cls._datastore_version_metadata_delete(
-            datastore_version_id, 'flavor', flavor_id,
+            datastore_name, datastore_version_name, 'flavor', flavor_id,
             exception.DatastoreFlavorAssociationNotFound)
 
     @classmethod
     def list_datastore_version_flavor_associations(cls, context,
+                                                   datastore_type,
                                                    datastore_version_id):
-        """Get allowed flavors for a given datastore version.
-
-        All nova flavors are permitted for a datastore_version unless
-        one or more entries are found in datastore_version_metadata,
-        in which case only those are permitted.
-        """
-        nova_flavors = create_nova_client(context).flavors.list()
-        bound_flavors = DBDatastoreVersionMetadata.find_all(
-            datastore_version_id=datastore_version_id,
-            key='flavor', deleted=False
-        )
-        if (bound_flavors.count() != 0):
-            bound_flavors = tuple(f.value for f in bound_flavors)
-            # Generate a filtered list of nova flavors
-            ds_nova_flavors = (f for f in nova_flavors
-                               if f.id in bound_flavors)
-            associated_flavors = tuple(flavor_model(flavor=item)
-                                       for item in ds_nova_flavors)
+        if datastore_type and datastore_version_id:
+            """
+            All nova flavors are permitted for a datastore_version unless
+            one or more entries are found in datastore_version_metadata,
+            in which case only those are permitted.
+            """
+            (datastore, datastore_version) = get_datastore_version(
+                type=datastore_type, version=datastore_version_id)
+            # If datastore_version_id and flavor key exists in the
+            # metadata table return all the associated flavors for
+            # that datastore version.
+            nova_flavors = create_nova_client(context).flavors.list()
+            bound_flavors = DBDatastoreVersionMetadata.find_all(
+                datastore_version_id=datastore_version.id,
+                key='flavor', deleted=False
+            )
+            if (bound_flavors.count() != 0):
+                bound_flavors = tuple(f.value for f in bound_flavors)
+                # Generate a filtered list of nova flavors
+                ds_nova_flavors = (f for f in nova_flavors
+                                   if f.id in bound_flavors)
+                associated_flavors = tuple(flavor_model(flavor=item)
+                                           for item in ds_nova_flavors)
+            else:
+                # Return all nova flavors if no flavor metadata found
+                # for datastore_version.
+                associated_flavors = tuple(flavor_model(flavor=item)
+                                           for item in nova_flavors)
+            return associated_flavors
         else:
-            # Return all nova flavors if no flavor metadata found
-            # for datastore_version.
-            associated_flavors = tuple(flavor_model(flavor=item)
-                                       for item in nova_flavors)
-        return associated_flavors
+            msg = _("Specify both the datastore and datastore_version_id.")
+            raise exception.BadRequest(msg)
 
     @classmethod
-    def add_datastore_version_volume_type_association(cls,
-                                                      datastore_version_id,
+    def add_datastore_version_volume_type_association(cls, datastore_name,
+                                                      datastore_version_name,
                                                       volume_type_names):
+        datastore_version_id = cls._datastore_version_find(
+            datastore_name,
+            datastore_version_name)
+
+        # the database record will contain
+        # datastore_version_id, 'volume_type', volume_type_name
         for volume_type_name in volume_type_names:
             cls._datastore_version_metadata_add(
+                datastore_name, datastore_version_name,
                 datastore_version_id, 'volume_type', volume_type_name,
                 exception.DatastoreVolumeTypeAssociationAlreadyExists)
 
     @classmethod
     def delete_datastore_version_volume_type_association(
-            cls, datastore_version_id, volume_type_name):
+            cls, datastore_name,
+            datastore_version_name,
+            volume_type_name):
         cls._datastore_version_metadata_delete(
-            datastore_version_id, 'volume_type',
+            datastore_name, datastore_version_name, 'volume_type',
             volume_type_name,
             exception.DatastoreVolumeTypeAssociationNotFound)
 
     @classmethod
     def list_datastore_version_volume_type_associations(cls,
                                                         datastore_version_id):
         """
@@ -799,71 +787,90 @@
     def list_datastore_volume_type_associations(cls,
                                                 datastore_name,
                                                 datastore_version_name):
         """
         List the datastore associations for a given datastore and version.
         """
         if datastore_name and datastore_version_name:
-            datastore_version_id = cls.datastore_version_find(
+            datastore_version_id = cls._datastore_version_find(
                 datastore_name, datastore_version_name)
             return cls.list_datastore_version_volume_type_associations(
                 datastore_version_id)
         else:
             msg = _("Specify the datastore_name and datastore_version_name.")
             raise exception.BadRequest(msg)
 
     @classmethod
-    def datastore_volume_type_associations_exist(cls, datastore_version_id):
-        return cls.list_datastore_version_volume_type_associations(
-            datastore_version_id).count() > 0
+    def datastore_volume_type_associations_exist(cls,
+                                                 datastore_name,
+                                                 datastore_version_name):
+        return cls.list_datastore_volume_type_associations(
+            datastore_name,
+            datastore_version_name).count() > 0
 
     @classmethod
     def allowed_datastore_version_volume_types(cls, context,
-                                               datastore_version_id):
+                                               datastore_name,
+                                               datastore_version_name):
         """
         List all allowed volume types for a given datastore and
         datastore version. If datastore version metadata is
         provided, then the valid volume types in that list are
         allowed. If datastore version metadata is not provided
         then all volume types known to cinder are allowed.
         """
-        metadata = cls.list_datastore_version_volume_type_associations(
-            datastore_version_id)
+        if datastore_name and datastore_version_name:
+            # first obtain the list in the dsvmetadata
+            datastore_version_id = cls._datastore_version_find(
+                datastore_name, datastore_version_name)
 
-        # then get the list of all volume types
-        all_volume_types = volume_type_models.VolumeTypes(context)
+            metadata = cls.list_datastore_version_volume_type_associations(
+                datastore_version_id)
 
-        # if there's metadata: intersect,
-        # else, whatever cinder has.
-        if (metadata.count() != 0):
-            # the volume types from metadata first
-            ds_volume_types = tuple(f.value for f in metadata)
-
-            # Cinder volume type names are unique, intersect
-            allowed_volume_types = tuple(
-                f for f in all_volume_types
-                if ((f.name in ds_volume_types) or
-                    (f.id in ds_volume_types)))
-        else:
-            allowed_volume_types = tuple(all_volume_types)
+            # then get the list of all volume types
+            all_volume_types = volume_type_models.VolumeTypes(context)
+
+            # if there's metadata: intersect,
+            # else, whatever cinder has.
+            if (metadata.count() != 0):
+                # the volume types from metadata first
+                ds_volume_types = tuple(f.value for f in metadata)
+
+                # Cinder volume type names are unique, intersect
+                allowed_volume_types = tuple(
+                    f for f in all_volume_types
+                    if ((f.name in ds_volume_types) or
+                        (f.id in ds_volume_types)))
+            else:
+                allowed_volume_types = tuple(all_volume_types)
 
-        return allowed_volume_types
+            return allowed_volume_types
+        else:
+            msg = _("Specify the datastore_name and datastore_version_name.")
+            raise exception.BadRequest(msg)
 
     @classmethod
-    def validate_volume_type(cls, context, volume_type, datastore_version_id):
-        if cls.datastore_volume_type_associations_exist(datastore_version_id):
+    def validate_volume_type(cls, context, volume_type,
+                             datastore_name, datastore_version_name):
+        if cls.datastore_volume_type_associations_exist(
+                datastore_name, datastore_version_name):
             allowed = cls.allowed_datastore_version_volume_types(
-                context, datastore_version_id)
+                context, datastore_name, datastore_version_name)
             if len(allowed) == 0:
                 raise exception.DatastoreVersionNoVolumeTypes(
-                    datastore_version_id=datastore_version_id)
+                    datastore=datastore_name,
+                    datastore_version=datastore_version_name)
             if volume_type is None:
                 raise exception.DataStoreVersionVolumeTypeRequired(
-                    datastore_version_id=datastore_version_id)
+                    datastore=datastore_name,
+                    datastore_version=datastore_version_name)
 
             allowed_names = tuple(f.name for f in allowed)
-            LOG.debug(f"Allowed volume types: {allowed_names}")
-
+            for n in allowed_names:
+                LOG.debug("Volume Type: %s is allowed for datastore "
+                          "%s, version %s." %
+                          (n, datastore_name, datastore_version_name))
             if volume_type not in allowed_names:
                 raise exception.DatastoreVolumeTypeAssociationNotFound(
-                    datastore_version_id=datastore_version_id,
+                    datastore=datastore_name,
+                    version_id=datastore_version_name,
                     id=volume_type)
```

### Comparing `trove-21.0.0.0rc2/trove/datastore/service.py` & `trove-8.0.1/trove/datastore/service.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,25 +11,21 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from oslo_log import log as logging
 
-from trove.common import exception
 from trove.common import policy
 from trove.common import wsgi
 from trove.datastore import models, views
 from trove.flavor import views as flavor_views
 from trove.volume_type import views as volume_type_view
 
-LOG = logging.getLogger(__name__)
-
 
 class DatastoreController(wsgi.Controller):
 
     @classmethod
     def authorize_request(cls, req, rule_name):
         """Datastores are not owned by any particular tenant so we only check
         the current tenant is allowed to perform the action.
@@ -89,37 +85,23 @@
         one or more entries are found in datastore_version_metadata,
         in which case only those are returned.
         """
         self.authorize_request(req, 'list_associated_flavors')
         context = req.environ[wsgi.CONTEXT_KEY]
         flavors = (models.DatastoreVersionMetadata.
                    list_datastore_version_flavor_associations(
-                       context, version_id))
+                       context, datastore, version_id))
         return wsgi.Result(flavor_views.FlavorsView(flavors, req).data(), 200)
 
     def list_associated_volume_types(self, req, tenant_id, datastore,
                                      version_id):
         """
         Return all known volume types if no restrictions have been
         established in datastore_version_metadata, otherwise return
         that restricted set.
         """
         context = req.environ[wsgi.CONTEXT_KEY]
         volume_types = (models.DatastoreVersionMetadata.
                         allowed_datastore_version_volume_types(
-                            context, version_id))
+                            context, datastore, version_id))
         return wsgi.Result(volume_type_view.VolumeTypesView(
             volume_types, req).data(), 200)
-
-    def delete(self, req, tenant_id, id):
-        """Remove an existing datastore."""
-        self.authorize_request(req, 'delete')
-
-        ds_versions = models.DatastoreVersions.load(id, only_active=False)
-        if len(ds_versions.db_info.all()) > 0:
-            raise exception.DatastoreVersionsExist(datastore=id)
-
-        LOG.info("Deleting datastore %s", id)
-
-        datastore = models.Datastore.load(id)
-        datastore.delete()
-        return wsgi.Result(None, 202)
```

### Comparing `trove-21.0.0.0rc2/trove/datastore/views.py` & `trove-8.0.1/trove/datastore/views.py`

 * *Files 2% similar despite different names*

```diff
@@ -76,30 +76,24 @@
         self.req = req
         self.context = req.environ[wsgi.CONTEXT_KEY]
 
     def data(self, include_datastore_id=True):
         datastore_version_dict = {
             "id": self.datastore_version.id,
             "name": self.datastore_version.name,
-            "version": self.datastore_version.version,
             "links": self._build_links(),
         }
         if include_datastore_id:
             datastore_version_dict["datastore"] = (self.datastore_version.
                                                    datastore_id)
         if self.context.is_admin:
             datastore_version_dict['active'] = self.datastore_version.active
             datastore_version_dict['packages'] = (self.datastore_version.
                                                   packages)
             datastore_version_dict['image'] = self.datastore_version.image_id
-
-            image_tags = []
-            if self.datastore_version.image_tags:
-                image_tags = self.datastore_version.image_tags.split(',')
-            datastore_version_dict['image_tags'] = image_tags
         return {"version": datastore_version_dict}
 
     def _build_links(self):
         return create_links("datastores/versions",
                             self.req, self.datastore_version.id)
```

### Comparing `trove-21.0.0.0rc2/trove/db/__init__.py` & `trove-8.0.1/trove/db/sqlalchemy/api.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,89 +1,144 @@
-# Copyright 2010-2011 OpenStack Foundation
+# Copyright 2011 OpenStack Foundation
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from trove.common import cfg
-from trove.common import utils
+import sqlalchemy.exc
 
-CONF = cfg.CONF
+from trove.common import exception
+from trove.db.sqlalchemy import migration
+from trove.db.sqlalchemy import session
 
 
-def get_db_api():
-    return utils.import_module(CONF.db_api_implementation)
+def list(query_func, *args, **kwargs):
+    return query_func(*args, **kwargs).all()
 
 
-class Query(object):
-    """Mimics sqlalchemy query object.
+def count(query, *args, **kwargs):
+    return query(*args, **kwargs).count()
 
-    This class allows us to store query conditions and use them with
-    bulk updates and deletes just like sqlalchemy query object.
-    Using this class makes the models independent of sqlalchemy
 
-    """
-    def __init__(self, model, query_func, **conditions):
-        self._query_func = query_func
-        self._model = model
-        self._conditions = conditions
-        self.db_api = get_db_api()
+def first(query, *args, **kwargs):
+    return query(*args, **kwargs).first()
 
-    def all(self):
-        return self.db_api.list(self._query_func, self._model,
-                                **self._conditions)
 
-    def count(self):
-        return self.db_api.count(self._query_func, self._model,
-                                 **self._conditions)
+def join(query, model, *args):
+    return query(model).join(*args)
 
-    def first(self):
-        return self.db_api.first(self._query_func, self._model,
-                                 **self._conditions)
 
-    def join(self, *args):
-        return self.db_api.join(self._query_func, self._model, *args)
+def find_all(model, **conditions):
+    return _query_by(model, **conditions)
 
-    def __iter__(self):
-        return iter(self.all())
 
-    def update(self, **values):
-        self.db_api.update_all(self._query_func, self._model, self._conditions,
-                               values)
+def find_all_by_limit(query_func, model, conditions, limit, marker=None,
+                      marker_column=None):
+    return _limits(query_func, model, conditions, limit, marker,
+                   marker_column).all()
 
-    def delete(self):
-        self.db_api.delete_all(self._query_func, self._model,
-                               **self._conditions)
 
-    def limit(self, limit=200, marker=None, marker_column=None):
-        return self.db_api.find_all_by_limit(
-            self._query_func,
-            self._model,
-            self._conditions,
-            limit=limit,
-            marker=marker,
-            marker_column=marker_column)
+def find_by(model, **kwargs):
+    return _query_by(model, **kwargs).first()
 
-    def paginated_collection(self, limit=200, marker=None, marker_column=None):
-        collection = self.limit(int(limit) + 1, marker, marker_column)
-        if len(collection) > int(limit):
-            return (collection[0:-1], collection[-2]['id'])
-        return (collection, None)
 
+def find_by_filter(model, **kwargs):
+    filters = kwargs.pop('filters', [])
+    return _query_by_filter(model, *filters, **kwargs)
 
-class Queryable(object):
 
-    def __getattr__(self, item):
-        return lambda model, **conditions: Query(
-            model, query_func=getattr(get_db_api(), item), **conditions)
+def save(model):
+    try:
+        db_session = session.get_session()
+        model = db_session.merge(model)
+        db_session.flush()
+        return model
+    except sqlalchemy.exc.IntegrityError as error:
+        raise exception.DBConstraintError(model_name=model.__class__.__name__,
+                                          error=str(error.orig))
 
 
-db_query = Queryable()
+def delete(model):
+    db_session = session.get_session()
+    model = db_session.merge(model)
+    db_session.delete(model)
+    db_session.flush()
+
+
+def delete_all(query_func, model, **conditions):
+    query_func(model, **conditions).delete()
+
+
+def update(model, **values):
+    for k, v in values.items():
+        model[k] = v
+
+
+def update_all(query_func, model, conditions, values):
+    query_func(model, **conditions).update(values)
+
+
+def configure_db(options, *plugins):
+    session.configure_db(options)
+    configure_db_for_plugins(options, *plugins)
+
+
+def configure_db_for_plugins(options, *plugins):
+    for plugin in plugins:
+        session.configure_db(options, models_mapper=plugin.mapper)
+
+
+def drop_db(options):
+    session.drop_db(options)
+
+
+def clean_db():
+    session.clean_db()
+
+
+def db_sync(options, version=None, repo_path=None):
+    migration.db_sync(options, version, repo_path)
+
+
+def db_upgrade(options, version=None, repo_path=None):
+    migration.upgrade(options, version, repo_path)
+
+
+def db_reset(options, *plugins):
+    drop_db(options)
+    db_sync(options)
+    configure_db(options)
+
+
+def _base_query(cls):
+    return session.get_session().query(cls)
+
+
+def _query_by(cls, **conditions):
+    query = _base_query(cls)
+    if conditions:
+        query = query.filter_by(**conditions)
+    return query
+
+
+def _query_by_filter(cls, *filters, **conditions):
+    query = _query_by(cls, **conditions)
+    if filters:
+        query = query.filter(*filters)
+    return query
+
+
+def _limits(query_func, model, conditions, limit, marker, marker_column=None):
+    query = query_func(model, **conditions)
+    marker_column = marker_column or model.id
+    if marker:
+        query = query.filter(marker_column > marker)
+    return query.order_by(marker_column).limit(limit)
```

### Comparing `trove-21.0.0.0rc2/trove/db/models.py` & `trove-8.0.1/trove/db/models.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,16 +8,14 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from contextlib import contextmanager
-
 from oslo_log import log as logging
 from oslo_utils import strutils
 
 from trove.common import exception
 from trove.common.i18n import _
 from trove.common import models
 from trove.common import pagination
@@ -51,19 +49,16 @@
         return get_db_api()
 
     @property
     def preserve_on_delete(self):
         return hasattr(self, 'deleted') and hasattr(self, 'deleted_at')
 
     @classmethod
-    @contextmanager
     def query(cls):
-        query = get_db_api()._base_query(cls)
-        yield query
-        query.session.commit()
+        return get_db_api()._base_query(cls)
 
     def save(self):
         if not self.is_valid():
             raise exception.InvalidModelError(errors=self.errors)
         self['updated'] = timeutils.utcnow()
         LOG.debug("Saving %(name)s: %(dict)s",
                   {'name': self.__class__.__name__,
@@ -105,25 +100,22 @@
         model = cls.get_by(**conditions)
 
         if model is None:
             raise exception.ModelNotFoundError(_("%(s_name)s Not Found") %
                                                {"s_name": cls.__name__})
 
         if ((context and not context.is_admin and hasattr(model, 'tenant_id')
-             and model.tenant_id != context.project_id)):
-            log_fmt = ("Tenant %(s_tenant)s tried to access "
-                       "%(s_name)s, owned by %(s_owner)s.")
-            exc_fmt = _("Tenant %(s_tenant)s tried to access "
-                        "%(s_name)s, owned by %(s_owner)s.")
-            msg_content = {
-                "s_tenant": context.project_id,
-                "s_name": cls.__name__,
+             and model.tenant_id != context.tenant)):
+            msg = _("Tenant %(s_tenant)s tried to access "
+                    "%(s_name)s, owned by %(s_owner)s.") % {
+                "s_tenant": context.tenant, "s_name": cls.__name__,
                 "s_owner": model.tenant_id}
-            LOG.error(log_fmt, msg_content)
-            raise exception.ModelNotFoundError(exc_fmt % msg_content)
+
+            LOG.error(msg)
+            raise exception.ModelNotFoundError(msg)
 
         return model
 
     @classmethod
     def find_by_filter(cls, **kwargs):
         return db_query.find_by_filter(cls, **cls._process_conditions(kwargs))
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/mappers.py` & `trove-8.0.1/trove/db/sqlalchemy/mappers.py`

 * *Files 8% similar despite different names*

```diff
@@ -18,25 +18,25 @@
 from sqlalchemy.orm import exc as orm_exc
 from sqlalchemy import Table
 
 
 def map(engine, models):
     meta = MetaData()
     meta.bind = engine
-    if mapping_exists(models['instances']):
+    if mapping_exists(models['instance']):
         return
 
-    orm.mapper(models['instances'], Table('instances', meta, autoload=True))
+    orm.mapper(models['instance'], Table('instances', meta, autoload=True))
     orm.mapper(models['instance_faults'],
                Table('instance_faults', meta, autoload=True))
     orm.mapper(models['root_enabled_history'],
                Table('root_enabled_history', meta, autoload=True))
-    orm.mapper(models['datastores'],
+    orm.mapper(models['datastore'],
                Table('datastores', meta, autoload=True))
-    orm.mapper(models['datastore_versions'],
+    orm.mapper(models['datastore_version'],
                Table('datastore_versions', meta, autoload=True))
     orm.mapper(models['datastore_version_metadata'],
                Table('datastore_version_metadata', meta, autoload=True))
     orm.mapper(models['capabilities'],
                Table('capabilities', meta, autoload=True))
     orm.mapper(models['capability_overrides'],
                Table('capability_overrides', meta, autoload=True))
@@ -50,21 +50,19 @@
                Table('quotas', meta, autoload=True))
     orm.mapper(models['quota_usages'],
                Table('quota_usages', meta, autoload=True))
     orm.mapper(models['reservations'],
                Table('reservations', meta, autoload=True))
     orm.mapper(models['backups'],
                Table('backups', meta, autoload=True))
-    orm.mapper(models['backup_strategy'],
-               Table('backup_strategy', meta, autoload=True))
-    orm.mapper(models['security_groups'],
+    orm.mapper(models['security_group'],
                Table('security_groups', meta, autoload=True))
-    orm.mapper(models['security_group_rules'],
+    orm.mapper(models['security_group_rule'],
                Table('security_group_rules', meta, autoload=True))
-    orm.mapper(models['security_group_instance_associations'],
+    orm.mapper(models['security_group_instance_association'],
                Table('security_group_instance_associations', meta,
                      autoload=True))
     orm.mapper(models['configurations'],
                Table('configurations', meta, autoload=True))
     orm.mapper(models['configuration_parameters'],
                Table('configuration_parameters', meta, autoload=True))
     orm.mapper(models['conductor_lastseen'],
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/manage.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/manage.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/migrate.cfg` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/migrate.cfg`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/schema.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/schema.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 #    under the License.
 
 """Various conveniences used for migration scripts."""
 
 from oslo_log import log as logging
 import sqlalchemy.types
 
+
 logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')
 
 
 class String(sqlalchemy.types.String):
     def __init__(self, length, *args, **kwargs):
         super(String, self).__init__(*args, length=length, **kwargs)
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/001_base_schema.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/001_base_schema.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/002_service_images.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/002_service_images.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/003_service_statuses.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/003_service_statuses.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/004_root_enabled.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/004_root_enabled.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/005_heartbeat.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/005_heartbeat.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/006_dns_records.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/006_dns_records.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/007_add_volume_flavor.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/007_add_volume_flavor.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/008_add_instance_fields.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/008_add_instance_fields.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/009_add_deleted_flag_to_instances.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/009_add_deleted_flag_to_instances.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/010_add_usage.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/010_add_usage.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/011_quota.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/011_quota.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/012_backup.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/012_backup.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/013_add_security_group_artifacts.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/013_add_security_group_artifacts.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/014_update_instance_flavor_id.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/014_update_instance_flavor_id.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/015_add_service_type.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/015_add_service_type.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/016_add_datastore_type.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/016_add_datastore_type.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/017_update_datastores.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/017_update_datastores.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/018_datastore_versions_fix.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/029_add_backup_datastore.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,30 @@
-# Copyright 2012 OpenStack Foundation
+# Copyright 2013 OpenStack Foundation
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+from sqlalchemy import ForeignKey
+from sqlalchemy.schema import Column
 from sqlalchemy.schema import MetaData
 
+from trove.db.sqlalchemy.migrate_repo.schema import String
 from trove.db.sqlalchemy.migrate_repo.schema import Table
 
 
 def upgrade(migrate_engine):
     meta = MetaData()
     meta.bind = migrate_engine
-    datastore_versions = Table('datastore_versions', meta, autoload=True)
-    # modify column
-    datastore_versions.c.name.alter(unique=False)
+    backups = Table('backups', meta, autoload=True)
+    Table('datastore_versions', meta, autoload=True)
+    datastore_version_id = Column('datastore_version_id', String(36),
+                                  ForeignKey('datastore_versions.id'))
+    backups.create_column(datastore_version_id)
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/019_datastore_fix.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/019_datastore_fix.py`

 * *Files 12% similar despite different names*

```diff
@@ -12,15 +12,14 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from sqlalchemy.schema import MetaData
 from sqlalchemy.sql.expression import insert
 from sqlalchemy.sql.expression import select
 from sqlalchemy.sql.expression import update
-from sqlalchemy import text
 
 from trove.common import cfg
 from trove.db.sqlalchemy.migrate_repo.schema import Table
 from trove.db.sqlalchemy import utils as db_utils
 
 CONF = cfg.CONF
 LEGACY_IMAGE_ID = "00000000-0000-0000-0000-000000000000"
@@ -50,41 +49,41 @@
 
     return LEGACY_VERSION_ID
 
 
 def find_image(service_name):
     image_table = Table('service_images', meta, autoload=True)
     image = select(
-        columns=[text("id"), text("image_id"), text("service_name")],
+        columns=["id", "image_id", "service_name"],
         from_obj=image_table,
-        whereclause=text("service_name='%s'" % service_name),
+        whereclause="service_name='%s'" % service_name,
         limit=1
     ).execute().fetchone()
 
     if image:
         return image.id
     return LEGACY_IMAGE_ID
 
 
 def has_instances_wo_datastore_version(instances_table):
     instance = select(
-        columns=[text("id")],
+        columns=["id"],
         from_obj=instances_table,
-        whereclause=text("datastore_version_id is NULL"),
+        whereclause="datastore_version_id is NULL",
         limit=1
     ).execute().fetchone()
 
     return instance is not None
 
 
 def find_all_instances_wo_datastore_version(instances_table):
     instances = select(
-        columns=[text("id")],
+        columns=["id"],
         from_obj=instances_table,
-        whereclause=text("datastore_version_id is NULL")
+        whereclause="datastore_version_id is NULL"
     ).execute()
 
     return instances
 
 
 def upgrade(migrate_engine):
     meta.bind = migrate_engine
@@ -104,24 +103,24 @@
 
         version_id = create_legacy_version(datastores_table,
                                            datastore_versions_table,
                                            image_id)
         for instance in instances:
             update(
                 table=instance_table,
-                whereclause=text("id='%s'" % instance.id),
+                whereclause="id='%s'" % instance.id,
                 values=dict(datastore_version_id=version_id)
             ).execute()
 
     constraint_names = db_utils.get_foreign_key_constraint_names(
         engine=migrate_engine,
         table='instances',
-        columns=[text('datastore_version_id')],
+        columns=['datastore_version_id'],
         ref_table='datastore_versions',
-        ref_columns=[text('id')])
+        ref_columns=['id'])
     db_utils.drop_foreign_key_constraints(
         constraint_names=constraint_names,
         columns=[instance_table.c.datastore_version_id],
         ref_columns=[datastore_versions_table.c.id])
 
     instance_table.c.datastore_version_id.alter(nullable=False)
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/020_configurations.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/020_configurations.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/021_conductor_last_seen.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/021_conductor_last_seen.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/022_add_backup_parent_id.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/022_add_backup_parent_id.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/023_add_instance_indexes.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/023_add_instance_indexes.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/024_add_backup_indexes.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/024_add_backup_indexes.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/025_add_service_statuses_indexes.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/025_add_service_statuses_indexes.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/026_datastore_versions_unique_fix.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/026_datastore_versions_unique_fix.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/027_add_datastore_capabilities.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/027_add_datastore_capabilities.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/028_recreate_agent_heartbeat.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/028_recreate_agent_heartbeat.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/029_add_backup_datastore.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/042_add_cluster_configuration_id.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,30 +1,38 @@
-# Copyright 2013 OpenStack Foundation
+# Copyright 2016 Tesora, Inc.
+# All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+from oslo_log import log as logging
 from sqlalchemy import ForeignKey
 from sqlalchemy.schema import Column
 from sqlalchemy.schema import MetaData
 
+from trove.common import cfg
 from trove.db.sqlalchemy.migrate_repo.schema import String
 from trove.db.sqlalchemy.migrate_repo.schema import Table
 
 
+CONF = cfg.CONF
+logger = logging.getLogger('trove.db.sqlalchemy.migrate_repo.schema')
+
+meta = MetaData()
+
+
 def upgrade(migrate_engine):
-    meta = MetaData()
     meta.bind = migrate_engine
-    backups = Table('backups', meta, autoload=True)
-    Table('datastore_versions', meta, autoload=True)
-    datastore_version_id = Column('datastore_version_id', String(36),
-                                  ForeignKey('datastore_versions.id'))
-    backups.create_column(datastore_version_id)
+    # Load 'configurations' table to MetaData.
+    Table('configurations', meta, autoload=True, autoload_with=migrate_engine)
+    instances = Table('clusters', meta, autoload=True)
+    instances.create_column(Column('configuration_id', String(36),
+                                   ForeignKey("configurations.id")))
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/030_add_master_slave.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/030_add_master_slave.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/031_add_timestamps_to_configurations.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/031_add_timestamps_to_configurations.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/032_clusters.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/032_clusters.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/033_datastore_parameters.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/033_datastore_parameters.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/034_change_task_description.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/034_change_task_description.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/035_flavor_id_int_to_string.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/035_flavor_id_int_to_string.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/036_add_datastore_version_metadata.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/036_add_datastore_version_metadata.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/037_modules.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/037_modules.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/038_instance_faults.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/038_instance_faults.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/039_region.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/039_region.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,9 +28,8 @@
 meta = MetaData()
 
 
 def upgrade(migrate_engine):
     meta.bind = migrate_engine
     instances = Table('instances', meta, autoload=True)
     instances.create_column(Column('region_id', String(255)))
-    instances.update().values(
-        region_id=CONF.service_credentials.region_name).execute()
+    instances.update().values(region_id=CONF.os_region_name).execute()
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/040_module_priority.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/040_module_priority.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 from sqlalchemy.schema import Column
 from sqlalchemy.schema import MetaData
 from sqlalchemy.sql.expression import update
-from sqlalchemy import text
 
 from trove.db.sqlalchemy.migrate_repo.schema import Boolean
 from trove.db.sqlalchemy.migrate_repo.schema import Integer
 from trove.db.sqlalchemy.migrate_repo.schema import Table
 from trove.db.sqlalchemy.migrate_repo.schema import Text
 
 
@@ -41,9 +40,9 @@
     modules.create_column(column)
     column = Column(COLUMN_NAME_3, Boolean(), nullable=is_nullable, default=0)
     modules.create_column(column)
     modules.c.contents.alter(Text(length=4294967295))
     # mark all non-visible, auto-apply and all-tenant modules as is_admin
     update(table=modules,
            values=dict(is_admin=1),
-           whereclause=text("visible=0 or auto_apply=1 or tenant_id is null")
+           whereclause="visible=0 or auto_apply=1 or tenant_id is null"
            ).execute()
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/041_instance_keys.py` & `trove-8.0.1/trove/db/sqlalchemy/migrate_repo/versions/041_instance_keys.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/046_add_access_to_instance.py` & `trove-8.0.1/trove/guestagent/strategies/backup/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,28 +1,26 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
 
-from sqlalchemy.schema import Column
-from sqlalchemy.schema import MetaData
+from oslo_log import log as logging
 
-from trove.db.sqlalchemy.migrate_repo.schema import Table
-from trove.db.sqlalchemy.migrate_repo.schema import Text
+from trove.common.strategies.strategy import Strategy
 
+LOG = logging.getLogger(__name__)
 
-def upgrade(migrate_engine):
-    meta = MetaData()
-    meta.bind = migrate_engine
 
-    instances = Table('instances', meta, autoload=True)
-    instances.create_column(Column('access', Text(), nullable=True))
+def get_backup_strategy(backup_driver, ns=__name__):
+    LOG.debug("Getting backup strategy: %s.", backup_driver)
+    return Strategy.get_strategy(backup_driver, ns)
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migrate_repo/versions/047_image_tag_in_datastore_version.py` & `trove-8.0.1/trove/guestagent/datastore/experimental/redis/system.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,29 +1,37 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright (c) 2013 Rackspace
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from sqlalchemy.schema import Column
-from sqlalchemy.schema import MetaData
+"""
+Determines operating system version and OS dependent commands.
+"""
 
-from trove.db.sqlalchemy.migrate_repo.schema import String
-from trove.db.sqlalchemy.migrate_repo.schema import Table
+from trove.guestagent.common.operating_system import get_os
 
 
-def upgrade(migrate_engine):
-    meta = MetaData()
-    meta.bind = migrate_engine
+REDIS_OWNER = 'redis'
+REDIS_CONFIG = '/etc/redis/redis.conf'
+REDIS_PID_FILE = '/var/run/redis/redis-server.pid'
+REDIS_LOG_FILE = '/var/log/redis/server.log'
+REDIS_CONF_DIR = '/etc/redis'
+REDIS_DATA_DIR = '/var/lib/redis'
+REDIS_PORT = '6379'
+REDIS_INIT = '/etc/init/redis-server.conf'
+REDIS_PACKAGE = ''
+SERVICE_CANDIDATES = ['redis-server', 'redis']
 
-    ds_version = Table('datastore_versions', meta, autoload=True)
-    ds_version.create_column(Column('image_tags', String(255), nullable=True))
-    ds_version.c.image_id.alter(nullable=True)
+OS = get_os()
+if OS is 'redhat':
+    REDIS_CONFIG = '/etc/redis.conf'
+    REDIS_PACKAGE = 'redis'
```

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/migration.py` & `trove-8.0.1/trove/db/sqlalchemy/migration.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/db/sqlalchemy/utils.py` & `trove-8.0.1/trove/db/sqlalchemy/utils.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/dns/designate/driver.py` & `trove-8.0.1/trove/dns/designate/driver.py`

 * *Files 18% similar despite different names*

```diff
@@ -16,104 +16,145 @@
 """
 Dns Driver that uses Designate DNSaaS.
 """
 
 import base64
 import hashlib
 
-from designateclient import client
-from keystoneauth1 import loading
-from keystoneauth1 import session
+from designateclient.v1 import Client
+from designateclient.v1.records import Record
 from oslo_log import log as logging
 from oslo_utils import encodeutils
+import six
 
 from trove.common import cfg
 from trove.common import exception
 from trove.common.i18n import _
 from trove.dns import driver
 
 
 CONF = cfg.CONF
 
 DNS_TENANT_ID = CONF.dns_account_id
 DNS_AUTH_URL = CONF.dns_auth_url
+DNS_ENDPOINT_URL = CONF.dns_endpoint_url
+DNS_SERVICE_TYPE = CONF.dns_service_type
+DNS_REGION = CONF.dns_region
 DNS_USERNAME = CONF.dns_username
 DNS_PASSKEY = CONF.dns_passkey
 DNS_TTL = CONF.dns_ttl
 DNS_DOMAIN_ID = CONF.dns_domain_id
 DNS_DOMAIN_NAME = CONF.dns_domain_name
-DNS_USER_DOMAIN_ID = CONF.dns_user_domain_id
-DNS_PROJECT_DOMAIN_ID = CONF.dns_project_domain_id
+
 
 LOG = logging.getLogger(__name__)
 
 
-def create_designate_client(api_version='2'):
+class DesignateObjectConverter(object):
+
+    def domain_to_zone(self, domain):
+        return DesignateDnsZone(id=domain.id, name=domain.name)
+
+    def record_to_entry(self, record, dns_zone):
+        return driver.DnsEntry(name=record.name, content=record.data,
+                               type=record.type, ttl=record.ttl,
+                               priority=record.priority, dns_zone=dns_zone)
+
+
+def create_designate_client():
     """Creates a Designate DNSaaS client."""
-    loader = loading.get_plugin_loader('password')
-    auth = loader.load_from_options(auth_url=DNS_AUTH_URL,
-                                    username=DNS_USERNAME,
-                                    password=DNS_PASSKEY,
-                                    project_id=DNS_TENANT_ID,
-                                    user_domain_id=DNS_USER_DOMAIN_ID,
-                                    project_domain_id=DNS_PROJECT_DOMAIN_ID)
-    sesh = session.Session(auth=auth)
-    return client.Client(api_version, session=sesh)
+    client = Client(auth_url=DNS_AUTH_URL,
+                    username=DNS_USERNAME,
+                    password=DNS_PASSKEY,
+                    tenant_id=DNS_TENANT_ID,
+                    endpoint=DNS_ENDPOINT_URL,
+                    service_type=DNS_SERVICE_TYPE,
+                    region_name=DNS_REGION)
+    return client
 
 
-class DesignateDriverV2(driver.DnsDriver):
+class DesignateDriver(driver.DnsDriver):
 
     def __init__(self):
         self.dns_client = create_designate_client()
+        self.converter = DesignateObjectConverter()
         self.default_dns_zone = DesignateDnsZone(id=DNS_DOMAIN_ID,
                                                  name=DNS_DOMAIN_NAME)
 
     def create_entry(self, entry, content):
         """Creates the entry in the driver at the given dns zone."""
         dns_zone = entry.dns_zone or self.default_dns_zone
         if not dns_zone.id:
             raise TypeError(_("The entry's dns_zone must have an ID "
                               "specified."))
         name = entry.name
         LOG.debug("Creating DNS entry %s.", name)
         client = self.dns_client
         # Record name has to end with a '.' by dns standard
-        client.recordsets.create(DNS_DOMAIN_ID, entry.name + '.', entry.type,
-                                 records=[content])
+        record = Record(name=entry.name + '.',
+                        type=entry.type,
+                        data=content,
+                        ttl=entry.ttl,
+                        priority=entry.priority)
+        client.records.create(dns_zone.id, record)
 
     def delete_entry(self, name, type, dns_zone=None):
         """Deletes an entry with the given name and type from a dns zone."""
         dns_zone = dns_zone or self.default_dns_zone
         records = self._get_records(dns_zone)
         matching_record = [rec for rec in records
-                           if rec['name'] == name + '.'
-                           and rec['type'] == type]
+                           if rec.name == name + '.' and rec.type == type]
         if not matching_record:
             raise exception.DnsRecordNotFound(name)
         LOG.debug("Deleting DNS entry %s.", name)
-        self.dns_client.recordsets.delete(dns_zone.id,
-                                          matching_record[0]['id'])
+        self.dns_client.records.delete(dns_zone.id, matching_record[0].id)
+
+    def get_entries_by_content(self, content, dns_zone=None):
+        """Retrieves all entries in a DNS zone with matching content field."""
+        records = self._get_records(dns_zone)
+        return [self.converter.record_to_entry(record, dns_zone)
+                for record in records if record.data == content]
+
+    def get_entries_by_name(self, name, dns_zone):
+        records = self._get_records(dns_zone)
+        return [self.converter.record_to_entry(record, dns_zone)
+                for record in records if record.name == name]
+
+    def get_dns_zones(self, name=None):
+        """Returns all dns zones (optionally filtered by the name argument."""
+        domains = self.dns_client.domains.list()
+        return [self.converter.domain_to_zone(domain)
+                for domain in domains if not name or domain.name == name]
+
+    def modify_content(self, name, content, dns_zone):
+        # We dont need this in trove for now
+        raise NotImplementedError(_("Not implemented for Designate DNS."))
+
+    def rename_entry(self, content, name, dns_zone):
+        # We dont need this in trove for now
+        raise NotImplementedError(_("Not implemented for Designate DNS."))
 
     def _get_records(self, dns_zone):
         dns_zone = dns_zone or self.default_dns_zone
         if not dns_zone:
             raise TypeError(_('DNS domain is must be specified'))
-        return self.dns_client.recordsets.list(dns_zone.id)
+        return self.dns_client.records.list(dns_zone.id)
 
 
 class DesignateInstanceEntryFactory(driver.DnsInstanceEntryFactory):
     """Defines how instance DNS entries are created for instances."""
 
     def create_entry(self, instance_id):
         zone = DesignateDnsZone(id=DNS_DOMAIN_ID, name=DNS_DOMAIN_NAME)
         # Constructing the hostname by hashing the instance ID.
         name = encodeutils.to_utf8(instance_id)
         name = hashlib.md5(name).digest()
         name = base64.b32encode(name)[:11].lower()
-        name = name.decode('ascii')
+        if six.PY3:
+            name = name.decode('ascii')
         hostname = ("%s.%s" % (name, zone.name))
         # Removing the leading dot if present
         if hostname.endswith('.'):
             hostname = hostname[:-1]
 
         return driver.DnsEntry(name=hostname, content=None, type="A",
                                ttl=DNS_TTL, dns_zone=zone)
```

### Comparing `trove-21.0.0.0rc2/trove/dns/driver.py` & `trove-8.0.1/trove/dns/driver.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/dns/manager.py` & `trove-8.0.1/trove/dns/manager.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/dns/models.py` & `trove-8.0.1/trove/dns/models.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/common/models.py` & `trove-8.0.1/trove/extensions/common/models.py`

 * *Files 26% similar despite different names*

```diff
@@ -11,108 +11,92 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from oslo_log import log as logging
 
-from trove.common.clients import create_guest_client
 from trove.common.db import models as guest_models
 from trove.common import exception
+from trove.common.remote import create_guest_client
 from trove.common import timeutils
 from trove.db import get_db_api
 from trove.instance import models as base_models
 
 
 LOG = logging.getLogger(__name__)
 
 
-def load_and_verify(context, instance_id,
-                    enabled_datastore=['mysql', 'mariadb']):
-    """Check instance datastore.
-
-    Some API operations are only supported for some specific datastores.
-    """
+def load_and_verify(context, instance_id):
+    # Load InstanceServiceStatus to verify if its running
     instance = base_models.Instance.load(context, instance_id)
-
-    if instance.datastore_version.manager not in enabled_datastore:
-        raise exception.UnprocessableEntity(
-            "Operation not supported for datastore "
-            f"{instance.datastore_version.manager}."
-        )
-
     if not instance.is_datastore_running:
         raise exception.UnprocessableEntity(
-            "Instance %s is not ready, status: %s." %
-            (instance.id, instance.datastore_status.status)
-        )
-
-    return instance
+            "Instance %s is not ready." % instance.id)
+    else:
+        return instance
 
 
 class Root(object):
 
     @classmethod
     def load(cls, context, instance_id):
-        load_and_verify(context, instance_id,
-                        enabled_datastore=['mysql', 'mariadb', 'postgresql'])
+        load_and_verify(context, instance_id)
         # TODO(pdmars): remove the is_root_enabled call from the guest agent,
         # just check the database for this information.
         # If the root history returns null or raises an exception, the root
         # user hasn't been enabled.
         try:
             root_history = RootHistory.load(context, instance_id)
         except exception.NotFound:
             return False
         if not root_history:
             return False
         return True
 
     @classmethod
-    def create(cls, context, instance_id, root_password,
+    def create(cls, context, instance_id, user, root_password,
                cluster_instances_list=None):
-        load_and_verify(context, instance_id,
-                        enabled_datastore=['mysql', 'mariadb', 'postgresql'])
+        load_and_verify(context, instance_id)
         if root_password:
             root = create_guest_client(context,
                                        instance_id).enable_root_with_password(
                 root_password)
         else:
             root = create_guest_client(context, instance_id).enable_root()
 
         root_user = guest_models.DatastoreUser.deserialize(root,
                                                            verify=False)
         root_user.make_root()
 
         # if cluster_instances_list none, then root create is called for
         # single instance, adding an RootHistory entry for the instance_id
         if cluster_instances_list is None:
-            RootHistory.create(context, instance_id)
+            RootHistory.create(context, instance_id, user)
 
         return root_user
 
     @classmethod
     def delete(cls, context, instance_id):
-        load_and_verify(context, instance_id,
-                        enabled_datastore=['mysql', 'mariadb', 'postgresql'])
+        load_and_verify(context, instance_id)
         create_guest_client(context, instance_id).disable_root()
 
 
 class ClusterRoot(Root):
 
     @classmethod
-    def create(cls, context, instance_id, root_password,
+    def create(cls, context, instance_id, user, root_password,
                cluster_instances_list=None):
         root_user = super(ClusterRoot, cls).create(context, instance_id,
-                                                   root_password,
+                                                   user, root_password,
                                                    cluster_instances_list=None)
 
         if cluster_instances_list:
             for instance in cluster_instances_list:
-                RootHistory.create(context, instance)
+                RootHistory.create(context, instance, user)
 
         return root_user
 
 
 class RootHistory(object):
 
     _auto_generated_attrs = ['id']
@@ -131,13 +115,13 @@
 
     @classmethod
     def load(cls, context, instance_id):
         history = get_db_api().find_by(cls, id=instance_id)
         return history
 
     @classmethod
-    def create(cls, context, instance_id):
+    def create(cls, context, instance_id, user):
         history = cls.load(context, instance_id)
         if history is not None:
             return history
-        history = RootHistory(instance_id, context.user_id)
+        history = RootHistory(instance_id, user)
         return history.save()
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/common/service.py` & `trove-8.0.1/trove/extensions/common/service.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,20 +15,22 @@
 
 
 import abc
 
 from oslo_config.cfg import NoSuchOptError
 from oslo_log import log as logging
 from oslo_utils import importutils
+import six
 
 from trove.cluster import models as cluster_models
 from trove.cluster.models import DBCluster
 from trove.common import cfg
 from trove.common import exception
 from trove.common.i18n import _
+from trove.common.i18n import _LI
 from trove.common import policy
 from trove.common import wsgi
 from trove.datastore import models as datastore_models
 from trove.extensions.common import models
 from trove.extensions.common import views
 from trove.instance import models as instance_models
 from trove.instance.models import DBInstance
@@ -57,15 +59,16 @@
 
         target_type = 'cluster' if is_cluster else 'instance'
         policy.authorize_on_target(
             context, '%s:extension:%s' % (target_type, target_rule_name),
             {'tenant': target.tenant_id})
 
 
-class BaseDatastoreRootController(ExtensionController, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class BaseDatastoreRootController(ExtensionController):
     """Base class that defines the contract for root controllers."""
 
     @abc.abstractmethod
     def root_index(self, req, tenant_id, instance_id, is_cluster):
         pass
 
     @abc.abstractmethod
@@ -86,67 +89,72 @@
 class DefaultRootController(BaseDatastoreRootController):
 
     def root_index(self, req, tenant_id, instance_id, is_cluster):
         """Returns True if root is enabled; False otherwise."""
         if is_cluster:
             raise exception.ClusterOperationNotSupported(
                 operation='show_root')
-        LOG.info("Getting root enabled for instance '%s'.", instance_id)
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_LI("Getting root enabled for instance '%s'."), instance_id)
+        LOG.info(_LI("req : '%s'\n\n"), req)
         context = req.environ[wsgi.CONTEXT_KEY]
         is_root_enabled = models.Root.load(context, instance_id)
         return wsgi.Result(views.RootEnabledView(is_root_enabled).data(), 200)
 
     def root_create(self, req, body, tenant_id, instance_id, is_cluster):
         if is_cluster:
             raise exception.ClusterOperationNotSupported(
                 operation='enable_root')
-        LOG.info("Enabling root for instance '%s'.", instance_id)
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_LI("Enabling root for instance '%s'."), instance_id)
+        LOG.info(_LI("req : '%s'\n\n"), req)
         context = req.environ[wsgi.CONTEXT_KEY]
+        user_name = context.user
         password = DefaultRootController._get_password_from_body(body)
-        root = models.Root.create(context, instance_id, password)
+        root = models.Root.create(context, instance_id,
+                                  user_name, password)
         return wsgi.Result(views.RootCreatedView(root).data(), 200)
 
     def root_delete(self, req, tenant_id, instance_id, is_cluster):
         if is_cluster:
             raise exception.ClusterOperationNotSupported(
                 operation='disable_root')
-        LOG.info("Disabling root for instance '%s'.", instance_id)
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_LI("Disabling root for instance '%s'."), instance_id)
+        LOG.info(_LI("req : '%s'\n\n"), req)
         context = req.environ[wsgi.CONTEXT_KEY]
-        is_root_enabled = models.Root.load(context, instance_id)
-        if not is_root_enabled:
-            raise exception.RootHistoryNotFound()
+        try:
+            found_user = self._find_root_user(context, instance_id)
+        except (ValueError, AttributeError) as e:
+            raise exception.BadRequest(msg=str(e))
+        if not found_user:
+            raise exception.UserNotFound(uuid="root")
         models.Root.delete(context, instance_id)
-        return wsgi.Result(None, 204)
+        return wsgi.Result(None, 200)
 
 
 class ClusterRootController(DefaultRootController):
 
     def root_index(self, req, tenant_id, instance_id, is_cluster):
         """Returns True if root is enabled; False otherwise."""
         if is_cluster:
             return self.cluster_root_index(req, tenant_id, instance_id)
         else:
             return self.instance_root_index(req, tenant_id, instance_id)
 
     def instance_root_index(self, req, tenant_id, instance_id):
-        LOG.info("Getting root enabled for instance '%s'.", instance_id)
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_LI("Getting root enabled for instance '%s'."), instance_id)
+        LOG.info(_LI("req : '%s'\n\n"), req)
         context = req.environ[wsgi.CONTEXT_KEY]
         try:
             is_root_enabled = models.ClusterRoot.load(context, instance_id)
         except exception.UnprocessableEntity:
             raise exception.UnprocessableEntity(
                 _("Cluster %s is not ready.") % instance_id)
         return wsgi.Result(views.RootEnabledView(is_root_enabled).data(), 200)
 
     def cluster_root_index(self, req, tenant_id, cluster_id):
-        LOG.info("Getting root enabled for cluster '%s'.", cluster_id)
+        LOG.info(_LI("Getting root enabled for cluster '%s'."), cluster_id)
         single_instance_id, cluster_instances = self._get_cluster_instance_id(
             tenant_id, cluster_id)
         return self.instance_root_index(req, tenant_id, single_instance_id)
 
     def _block_cluster_instance_actions(self):
         return False
 
@@ -161,33 +169,32 @@
             return self.cluster_root_create(req, body, tenant_id, instance_id)
         else:
             self.check_cluster_instance_actions(instance_id)
             return self.instance_root_create(req, body, instance_id)
 
     def instance_root_create(self, req, body, instance_id,
                              cluster_instances=None):
-        LOG.info("Enabling root for instance '%s'.", instance_id)
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_LI("Enabling root for instance '%s'."), instance_id)
+        LOG.info(_LI("req : '%s'\n\n"), req)
         context = req.environ[wsgi.CONTEXT_KEY]
+        user_name = context.user
         password = ClusterRootController._get_password_from_body(body)
-        root = models.ClusterRoot.create(context, instance_id,
+        root = models.ClusterRoot.create(context, instance_id, user_name,
                                          password, cluster_instances)
         return wsgi.Result(views.RootCreatedView(root).data(), 200)
 
     def cluster_root_create(self, req, body, tenant_id, cluster_id):
-        LOG.info("Enabling root for cluster '%s'.", cluster_id)
+        LOG.info(_LI("Enabling root for cluster '%s'."), cluster_id)
         single_instance_id, cluster_instances = self._get_cluster_instance_id(
             tenant_id, cluster_id)
         return self.instance_root_create(req, body, single_instance_id,
                                          cluster_instances)
 
     def _find_cluster_node_ids(self, tenant_id, cluster_id):
-        args = {'tenant_id': tenant_id,
-                'cluster_id': cluster_id,
-                'deleted': False}
+        args = {'tenant_id': tenant_id, 'cluster_id': cluster_id}
         cluster_instances = DBInstance.find_all(**args).all()
         return [db_instance.id for db_instance in cluster_instances]
 
     def _get_cluster_instance_id(self, tenant_id, cluster_id):
         instance_ids = self._find_cluster_node_ids(tenant_id, cluster_id)
         single_instance_id = instance_ids[0]
         return single_instance_id, instance_ids
@@ -229,16 +236,15 @@
         self.authorize_target_action(context, 'root:delete', instance_id,
                                      is_cluster=is_cluster)
         root_controller = self.load_root_controller(datastore_manager)
         if root_controller is not None:
             return root_controller.root_delete(req, tenant_id,
                                                instance_id, is_cluster)
         else:
-            opt = 'root_controller'
-            raise NoSuchOptError(opt, group='datastore_manager')
+            raise NoSuchOptError
 
     def _get_datastore(self, tenant_id, instance_or_cluster_id):
         """
         Returns datastore manager and a boolean
         showing if instance_or_cluster_id is a cluster id
         """
         args = {'id': instance_or_cluster_id, 'tenant_id': tenant_id}
@@ -254,19 +260,11 @@
         ds_manager = ds_version.manager
         return (ds_manager, is_cluster)
 
     def load_root_controller(self, manager):
         try:
             clazz = CONF.get(manager).get('root_controller')
             LOG.debug("Loading Root Controller class %s.", clazz)
-
-            if not clazz:
-                raise exception.DatastoreOperationNotSupported(
-                    datastore=manager, operation='root')
-
             root_controller = import_class(clazz)
             return root_controller()
         except NoSuchOptError:
-            LOG.warning(
-                f"root_controller not configured for datastore {manager}")
-            raise exception.DatastoreOperationNotSupported(
-                datastore=manager, operation='root')
+            return None
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/common/views.py` & `trove-8.0.1/trove/extensions/common/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/models.py` & `trove-8.0.1/trove/extensions/mgmt/clusters/models.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/service.py` & `trove-8.0.1/trove/extensions/mgmt/clusters/service.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,46 +37,46 @@
         action_type = list(body.keys())[0]
         return action_schema.get(action_type, {})
 
     @admin_context
     def index(self, req, tenant_id):
         """Return a list of clusters."""
         LOG.debug("Showing a list of clusters for tenant '%s'.", tenant_id)
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_("req : '%s'\n\n"), req)
         context = req.environ[wsgi.CONTEXT_KEY]
         deleted = None
         deleted_q = req.GET.get('deleted', '').lower()
         if deleted_q in ['true']:
             deleted = True
         elif deleted_q in ['false']:
             deleted = False
         clusters = models.MgmtCluster.load_all(context, deleted=deleted)
         view_cls = views.MgmtClustersView
         return wsgi.Result(view_cls(clusters, req=req).data(), 200)
 
     @admin_context
     def show(self, req, tenant_id, id):
         """Return a single cluster."""
-        LOG.info("Showing cluster for tenant '%(tenant_id)s'.\n"
-                 "req : '%(req)s'\n"
-                 "id : '%(id)s'", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Showing cluster for tenant '%(tenant_id)s'.\n"
+                   "req : '%(req)s'\n"
+                   "id : '%(id)s'"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         cluster = models.MgmtCluster.load(context, id)
         return wsgi.Result(
             views.load_mgmt_view(cluster, req=req).data(),
             200)
 
     @admin_context
     def action(self, req, body, tenant_id, id):
         LOG.debug("Committing an action against cluster %(cluster)s for "
                   "tenant '%(tenant)s'.", {'cluster': id,
                                            'tenant': tenant_id})
-        LOG.info("req : '%s'\n\n", req)
+        LOG.info(_("req : '%s'\n\n"), req)
         if not body:
             raise exception.BadRequest(_("Invalid request body."))
         context = req.environ[wsgi.CONTEXT_KEY]
         cluster = models.MgmtCluster.load(context=context, id=id)
 
         if 'reset-task' in body:
             return self._action_reset_task(context, cluster, body)
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/clusters/views.py` & `trove-8.0.1/trove/extensions/mgmt/clusters/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/configuration/service.py` & `trove-8.0.1/trove/extensions/mgmt/configuration/service.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,24 +34,26 @@
     schemas = apischema.mgmt_configuration
 
     @admin_context
     def index(self, req, tenant_id, version_id):
         """List all configuration parameters."""
         ds_version = ds_models.DatastoreVersion.load_by_uuid(version_id)
         config_params = config_models.DatastoreConfigurationParameters
-        rules = config_params.load_parameters(ds_version.id)
+        rules = config_params.load_parameters(
+            ds_version.id, show_deleted=True)
         return wsgi.Result(views.MgmtConfigurationParametersView(rules).data(),
                            200)
 
     @admin_context
     def show(self, req, tenant_id, version_id, id):
         """Show a configuration parameter."""
         ds_models.DatastoreVersion.load_by_uuid(version_id)
         config_params = config_models.DatastoreConfigurationParameters
-        rule = config_params.load_parameter_by_name(version_id, id)
+        rule = config_params.load_parameter_by_name(
+            version_id, id, show_deleted=True)
         return wsgi.Result(views.MgmtConfigurationParameterView(rule).data(),
                            200)
 
     def _validate_data_type(self, parameter):
         min_size = None
         max_size = None
         data_type = parameter['data_type']
@@ -68,15 +70,15 @@
                 raise exception.BadRequest(
                     _("max_size must be greater than or equal to min_size."))
         return data_type, min_size, max_size
 
     @admin_context
     def create(self, req, body, tenant_id, version_id):
         """Create configuration parameter for datastore version."""
-        LOG.info("Creating configuration parameter for datastore")
+        LOG.info(_("Creating configuration parameter for datastore"))
         LOG.debug("req : '%s'\n\n", req)
         LOG.debug("body : '%s'\n\n", body)
         if not body:
             raise exception.BadRequest(_("Invalid request body."))
 
         parameter = body['configuration-parameter']
         name = parameter['name']
@@ -95,15 +97,15 @@
         return wsgi.Result(
             views.MgmtConfigurationParameterView(rule).data(),
             200)
 
     @admin_context
     def update(self, req, body, tenant_id, version_id, id):
         """Updating configuration parameter for datastore version."""
-        LOG.info("Updating configuration parameter for datastore")
+        LOG.info(_("Updating configuration parameter for datastore"))
         LOG.debug("req : '%s'\n\n", req)
         LOG.debug("body : '%s'\n\n", body)
         if not body:
             raise exception.BadRequest(_("Invalid request body."))
 
         parameter = body['configuration-parameter']
         restart_required = bool(parameter['restart_required'])
@@ -120,15 +122,15 @@
         return wsgi.Result(
             views.MgmtConfigurationParameterView(param).data(),
             200)
 
     @admin_context
     def delete(self, req, tenant_id, version_id, id):
         """Delete configuration parameter for datastore version."""
-        LOG.info("Deleting configuration parameter for datastore")
+        LOG.info(_("Deleting configuration parameter for datastore"))
         LOG.debug("req : '%s'\n\n", req)
         ds_config_params = config_models.DatastoreConfigurationParameters
         try:
             ds_config_params.delete(version_id, id)
         except exception.NotFound:
             raise exception.BadRequest(_("Parameter %s does not exist in the "
                                          "database.") % id)
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/configuration/views.py` & `trove-8.0.1/trove/extensions/mgmt/configuration/views.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,14 +27,16 @@
         # v1 api is to be a 'true' or 'false' json boolean instead of 1/0
         restart_required = True if self.config.restart_required else False
         ret = {
             "name": self.config.name,
             "datastore_version_id": self.config.datastore_version_id,
             "restart_required": restart_required,
             "type": self.config.data_type,
+            "deleted": self.config.deleted,
+            "deleted_at": self.config.deleted_at,
         }
         if self.config.max_size:
             ret["max_size"] = int(self.config.max_size)
         if self.config.min_size:
             ret["min_size"] = int(self.config.min_size)
         return ret
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/datastores/service.py` & `trove-8.0.1/trove/extensions/mgmt/datastores/service.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,28 +8,27 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+
+from glanceclient import exc as glance_exceptions
 from oslo_log import log as logging
 
-from trove.backup import models as backup_model
-from trove.common import apischema
+from trove.common import apischema as apischema
 from trove.common.auth import admin_context
-from trove.common import clients
 from trove.common import exception
-from trove.common import glance as common_glance
+from trove.common import glance_remote
+from trove.common.i18n import _
 from trove.common import utils
 from trove.common import wsgi
-from trove.configuration import models as config_model
 from trove.datastore import models
 from trove.extensions.mgmt.datastores import views
-from trove.instance import models as instance_model
 
 LOG = logging.getLogger(__name__)
 
 
 class DatastoreVersionController(wsgi.Controller):
     """Controller for datastore version registration functionality."""
 
@@ -38,56 +37,49 @@
     @admin_context
     def create(self, req, body, tenant_id):
         """Adds a new datastore version."""
         context = req.environ[wsgi.CONTEXT_KEY]
         datastore_name = body['version']['datastore_name']
         version_name = body['version']['name']
         manager = body['version']['datastore_manager']
-        image_id = body['version'].get('image')
-        image_tags = body['version'].get('image_tags')
-        packages = body['version'].get('packages')
+        image_id = body['version']['image']
+        packages = body['version']['packages']
         if type(packages) is list:
             packages = ','.join(packages)
         active = body['version']['active']
-        default = body['version'].get('default', False)
-        # For backward compatibility, use name as default value for version if
-        # not specified
-        version_str = body['version'].get('version', version_name)
+        default = body['version']['default']
 
-        LOG.info("Tenant: '%(tenant)s' is adding the datastore "
-                 "version: '%(version)s' to datastore: '%(datastore)s'",
+        LOG.info(_("Tenant: '%(tenant)s' is adding the datastore "
+                   "version: '%(version)s' to datastore: '%(datastore)s'"),
                  {'tenant': tenant_id, 'version': version_name,
                   'datastore': datastore_name})
 
-        if not image_id and not image_tags:
-            raise exception.BadRequest("Image must be specified.")
-
-        client = clients.create_glance_client(context)
-        common_glance.get_image_id(client, image_id, image_tags)
+        client = glance_remote.create_glance_client(context)
+        try:
+            client.images.get(image_id)
+        except glance_exceptions.HTTPNotFound:
+            raise exception.ImageNotFound(uuid=image_id)
 
         try:
             datastore = models.Datastore.load(datastore_name)
         except exception.DatastoreNotFound:
             # Create the datastore if datastore_name does not exists.
-            LOG.info("Creating datastore %s", datastore_name)
+            LOG.info(_("Creating datastore %s"), datastore_name)
             datastore = models.DBDatastore()
             datastore.id = utils.generate_uuid()
             datastore.name = datastore_name
             datastore.save()
 
         try:
-            models.DatastoreVersion.load(datastore, version_name,
-                                         version=version_str)
-            raise exception.DatastoreVersionAlreadyExists(
-                name=version_name, version=version_str)
+            models.DatastoreVersion.load(datastore, version_name)
+            raise exception.DatastoreVersionAlreadyExists(name=version_name)
         except exception.DatastoreVersionNotFound:
             models.update_datastore_version(datastore.name, version_name,
-                                            manager, image_id, image_tags,
-                                            packages, active,
-                                            version=version_str)
+                                            manager, image_id, packages,
+                                            active)
 
         if default:
             models.update_datastore(datastore.name, version_name)
 
         return wsgi.Result(None, 202)
 
     @admin_context
@@ -110,92 +102,54 @@
 
     @admin_context
     def edit(self, req, body, tenant_id, id):
         """Updates the attributes of a datastore version."""
         context = req.environ[wsgi.CONTEXT_KEY]
         datastore_version = models.DatastoreVersion.load_by_uuid(id)
 
-        LOG.info("Tenant: '%(tenant)s' is updating the datastore "
-                 "version: '%(id)s' for datastore: '%(datastore)s'",
-                 {'tenant': tenant_id, 'id': id,
+        LOG.info(_("Tenant: '%(tenant)s' is updating the datastore "
+                   "version: '%(version)s' for datastore: '%(datastore)s'"),
+                 {'tenant': tenant_id, 'version': datastore_version.name,
                   'datastore': datastore_version.datastore_name})
 
-        name = body.get('name', datastore_version.name)
         manager = body.get('datastore_manager', datastore_version.manager)
-        image_id = body.get('image')
-        image_tags = body.get('image_tags')
+        image_id = body.get('image', datastore_version.image_id)
         active = body.get('active', datastore_version.active)
         default = body.get('default', None)
         packages = body.get('packages', datastore_version.packages)
         if type(packages) is list:
             packages = ','.join(packages)
 
-        if image_id or image_tags:
-            client = clients.create_glance_client(context)
-            common_glance.get_image_id(client, image_id, image_tags)
-
-        if not image_id and image_tags:
-            # Remove the image ID from the datastore version.
-            image_id = ""
-
-        if image_id is None:
-            image_id = datastore_version.image_id
-        if image_tags is None:
-            image_tags = datastore_version.image_tags
-            if type(image_tags) is str:
-                image_tags = image_tags.split(',')
-
-        if not image_id and not image_tags:
-            raise exception.BadRequest("Image must be specified.")
+        client = glance_remote.create_glance_client(context)
+        try:
+            client.images.get(image_id)
+        except glance_exceptions.HTTPNotFound:
+            raise exception.ImageNotFound(uuid=image_id)
 
         models.update_datastore_version(datastore_version.datastore_name,
                                         datastore_version.name,
-                                        manager, image_id, image_tags,
-                                        packages, active,
-                                        version=datastore_version.version,
-                                        new_name=name)
+                                        manager, image_id, packages,
+                                        active)
 
         if default:
             models.update_datastore(datastore_version.datastore_name,
                                     datastore_version.name)
         elif (default is False and datastore_version.default is True):
             models.update_datastore(datastore_version.datastore_name, None)
 
         return wsgi.Result(None, 202)
 
     @admin_context
     def delete(self, req, tenant_id, id):
         """Remove an existing datastore version."""
-        instances = instance_model.DBInstance.find_all(
-            datastore_version_id=id, deleted=0).all()
-        if len(instances) > 0:
-            raise exception.DatastoreVersionsInUse(resource='instance')
-
-        backups = backup_model.DBBackup.find_all(
-            datastore_version_id=id, deleted=0).all()
-        if len(backups) > 0:
-            raise exception.DatastoreVersionsInUse(resource='backup')
-
-        configs = config_model.DBConfiguration.find_all(
-            datastore_version_id=id, deleted=0).all()
-        if len(configs) > 0:
-            raise exception.DatastoreVersionsInUse(resource='configuration')
-
         datastore_version = models.DatastoreVersion.load_by_uuid(id)
         datastore = models.Datastore.load(datastore_version.datastore_id)
 
-        LOG.info("Tenant: '%(tenant)s' is removing the datastore "
-                 "version: '%(version)s' for datastore: '%(datastore)s'",
+        LOG.info(_("Tenant: '%(tenant)s' is removing the datastore "
+                   "version: '%(version)s' for datastore: '%(datastore)s'"),
                  {'tenant': tenant_id, 'version': datastore_version.name,
                   'datastore': datastore.name})
 
-        # Remove the config parameters associated with the datastore version
-        LOG.debug(f"Deleting config parameters for datastore version {id}")
-        db_params = config_model.DatastoreConfigurationParameters. \
-            load_parameters(id)
-        for db_param in db_params:
-            db_param.delete()
-
         if datastore.default_version_id == datastore_version.id:
             models.update_datastore(datastore.name, None)
         datastore_version.delete()
         return wsgi.Result(None, 202)
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/datastores/views.py` & `trove-8.0.1/trove/extensions/mgmt/datastores/views.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,21 +18,18 @@
     def __init__(self, datastore_version):
         self.datastore_version = datastore_version
 
     def data(self):
         datastore_version_dict = {
             "id": self.datastore_version.id,
             "name": self.datastore_version.name,
-            "version": self.datastore_version.version,
             "datastore_id": self.datastore_version.datastore_id,
             "datastore_name": self.datastore_version.datastore_name,
             "datastore_manager": self.datastore_version.manager,
             "image": self.datastore_version.image_id,
-            "image_tags": (self.datastore_version.image_tags.split(',')
-                           if self.datastore_version.image_tags else ['']),
             "packages": (self.datastore_version.packages.split(
                 ',') if self.datastore_version.packages else ['']),
             "active": self.datastore_version.active,
             "default": self.datastore_version.default}
 
         return {'version': datastore_version_dict}
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/instances/models.py` & `trove-8.0.1/trove/extensions/mgmt/instances/models.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,60 +12,54 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 import datetime
 
 from oslo_log import log as logging
 
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
+from trove.common.i18n import _
+from trove.common import remote
 from trove.common import timeutils
 from trove.extensions.mysql import models as mysql_models
 from trove.instance import models as instance_models
 from trove import rpc
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
 
 def load_mgmt_instances(context, deleted=None, client=None,
-                        include_clustered=None, project_id=None):
+                        include_clustered=None):
     if not client:
-        client = clients.create_nova_client(
-            context, CONF.service_credentials.region_name
-        )
-
-    search_opts = {'all_tenants': False}
-    mgmt_servers = client.servers.list(search_opts=search_opts, limit=-1)
-    LOG.info("Found %d servers in Nova",
+        client = remote.create_nova_client(context, CONF.os_region_name)
+    try:
+        mgmt_servers = client.rdservers.list()
+    except AttributeError:
+        mgmt_servers = client.servers.list(search_opts={'all_tenants': 1})
+    LOG.info(_("Found %d servers in Nova"),
              len(mgmt_servers if mgmt_servers else []))
-
     args = {}
     if deleted is not None:
         args['deleted'] = deleted
     if not include_clustered:
         args['cluster_id'] = None
-    if project_id:
-        args['tenant_id'] = project_id
-
     db_infos = instance_models.DBInstance.find_all(**args)
 
     instances = MgmtInstances.load_status_from_existing(context, db_infos,
                                                         mgmt_servers)
     return instances
 
 
 def load_mgmt_instance(cls, context, id, include_deleted):
     try:
         instance = instance_models.load_instance(
             cls, context, id, needs_server=True,
             include_deleted=include_deleted)
-        client = clients.create_nova_client(
-            context, CONF.service_credentials.region_name
-        )
+        client = remote.create_nova_client(context, CONF.os_region_name)
         try:
             server = client.rdservers.get(instance.server_id)
         except AttributeError:
             server = client.servers.get(instance.server_id)
         if hasattr(server, 'host'):
             instance.server.host = server.host
         elif hasattr(server, 'hostId'):
@@ -120,15 +114,15 @@
         self.volume_used = None
         self.volume_total = None
         self.root_history = None
 
     @classmethod
     def load(cls, context, id, include_deleted=False):
         instance = load_mgmt_instance(cls, context, id, include_deleted)
-        client = clients.create_cinder_client(context)
+        client = remote.create_cinder_client(context)
         try:
             instance.volume = client.volumes.get(instance.volume_id)
         except Exception:
             instance.volume = None
             # Populate the volume_used attribute from the guest agent.
         instance_models.load_guest_info(instance, context, id)
         instance.root_history = mysql_models.RootHistory.load(context=context,
@@ -152,32 +146,32 @@
 
 class MgmtInstances(instance_models.Instances):
     @staticmethod
     def load_status_from_existing(context, db_infos, servers):
         def load_instance(context, db, status, server=None):
             return SimpleMgmtInstance(context, db, server, status)
 
+        if context is None:
+            raise TypeError(_("Argument context not defined."))
         find_server = instance_models.create_server_list_matcher(servers)
-
         instances = instance_models.Instances._load_servers_status(
             load_instance, context, db_infos, find_server)
-
         _load_servers(instances, find_server)
         return instances
 
 
 def _load_servers(instances, find_server):
     for instance in instances:
         db = instance.db_info
         instance.server = None
         try:
             server = find_server(db.id, db.compute_instance_id)
             instance.server = server
         except Exception as ex:
-            LOG.warning(ex)
+            LOG.exception(ex)
     return instances
 
 
 def publish_exist_events(transformer, admin_context):
     notifier = rpc.get_notifier("taskmanager")
     notifications = transformer()
     # clear out admin_context.auth_token so it does not get logged
@@ -200,15 +194,15 @@
         return audit_start, audit_end
 
     def _get_service_id(self, datastore_manager, id_map):
         if datastore_manager in id_map:
             datastore_manager_id = id_map[datastore_manager]
         else:
             datastore_manager_id = cfg.UNKNOWN_SERVICE_ID
-            LOG.error("Datastore ID for Manager (%s) is not configured",
+            LOG.error(_("Datastore ID for Manager (%s) is not configured"),
                       datastore_manager)
         return datastore_manager_id
 
     def transform_instance(self, instance, audit_start, audit_end):
         payload = {
             'audit_period_beginning': audit_start,
             'audit_period_ending': audit_end,
@@ -251,23 +245,23 @@
         return messages
 
 
 class NovaNotificationTransformer(NotificationTransformer):
     def __init__(self, **kwargs):
         super(NovaNotificationTransformer, self).__init__(**kwargs)
         self.context = kwargs['context']
-        self.nova_client = clients.create_admin_nova_client(self.context)
+        self.nova_client = remote.create_admin_nova_client(self.context)
         self._flavor_cache = {}
 
     def _lookup_flavor(self, flavor_id):
         if flavor_id in self._flavor_cache:
             LOG.debug("Flavor cache hit for %s", flavor_id)
             return self._flavor_cache[flavor_id]
         # fetch flavor resource from nova
-        LOG.info("Flavor cache miss for %s", flavor_id)
+        LOG.info(_("Flavor cache miss for %s"), flavor_id)
         flavor = self.nova_client.flavors.get(flavor_id)
         self._flavor_cache[flavor_id] = flavor.name if flavor else 'unknown'
         return self._flavor_cache[flavor_id]
 
     def __call__(self):
         audit_start, audit_end = NotificationTransformer._get_audit_period()
         instances = load_mgmt_instances(self.context, deleted=False,
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/instances/service.py` & `trove-8.0.1/trove/extensions/mgmt/instances/service.py`

 * *Files 10% similar despite different names*

```diff
@@ -45,45 +45,43 @@
     def get_action_schema(cls, body, action_schema):
         action_type = list(body.keys())[0]
         return action_schema.get(action_type, {})
 
     @admin_context
     def index(self, req, tenant_id, detailed=False):
         """Return all instances."""
-        LOG.info("Indexing a database instance for tenant '%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {"tenant_id": tenant_id, "req": req})
+        LOG.info(_("Indexing a database instance for tenant '%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         deleted = None
         deleted_q = req.GET.get('deleted', '').lower()
         if deleted_q in ['true']:
             deleted = True
         elif deleted_q in ['false']:
             deleted = False
         clustered_q = req.GET.get('include_clustered', '').lower()
         include_clustered = clustered_q == 'true'
-        project_id = req.GET.get('project_id')
-
         try:
             instances = models.load_mgmt_instances(
-                context, deleted=deleted, include_clustered=include_clustered,
-                project_id=project_id)
+                context, deleted=deleted, include_clustered=include_clustered)
         except nova_exceptions.ClientException as e:
             LOG.exception(e)
             return wsgi.Result(str(e), 403)
 
         view_cls = views.MgmtInstancesView
         return wsgi.Result(view_cls(instances, req=req).data(), 200)
 
     @admin_context
     def show(self, req, tenant_id, id):
         """Return a single instance."""
-        LOG.info("Showing a database instance %(id)s for tenant "
-                 "'%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Showing a database instance %(id)s for tenant "
+                   "'%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
         context = req.environ[wsgi.CONTEXT_KEY]
         deleted_q = req.GET.get('deleted', '').lower()
         include_deleted = deleted_q == 'true'
         server = models.DetailedMgmtInstance.load(context, id,
                                                   include_deleted)
         root_history = mysql_models.RootHistory.load(context=context,
                                                      instance_id=id)
@@ -92,28 +90,27 @@
                 server,
                 req=req,
                 root_history=root_history).data(),
             200)
 
     @admin_context
     def action(self, req, body, tenant_id, id):
-        LOG.info("Committing an ACTION against a database "
-                 "instance %(id)s for tenant '%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Committing an ACTION against a database "
+                   "instance %(id)s for tenant '%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
         if not body:
             raise exception.BadRequest(_("Invalid request body."))
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.MgmtInstance.load(context=context, id=id)
         _actions = {
             'stop': self._action_stop,
             'reboot': self._action_reboot,
             'migrate': self._action_migrate,
-            'reset-task-status': self._action_reset_task_status,
-            'rebuild': self._action_rebuild
+            'reset-task-status': self._action_reset_task_status
         }
         selected_action = None
         for key in body:
             if key in _actions:
                 if selected_action is not None:
                     msg = _("Only one action can be specified per request.")
                     raise exception.BadRequest(msg)
@@ -130,22 +127,15 @@
     def _action_stop(self, context, instance, req, body):
         LOG.debug("Stopping MySQL on instance %s.", instance.id)
         instance.stop_db()
         return wsgi.Result(None, 202)
 
     def _action_reboot(self, context, instance, req, body):
         LOG.debug("Rebooting instance %s.", instance.id)
-
-        context.notification = notification.DBaaSInstanceReboot(
-            context,
-            request=req
-        )
-        with StartNotification(context, instance_id=instance.id):
-            instance.reboot()
-
+        instance.reboot()
         return wsgi.Result(None, 202)
 
     def _action_migrate(self, context, instance, req, body):
         LOG.debug("Migrating instance %s.", instance.id)
         LOG.debug("body['migrate']= %s", body['migrate'])
         host = body['migrate'].get('host', None)
 
@@ -160,31 +150,23 @@
         instance.reset_task_status()
 
         LOG.debug("Failing backups for instance %s.", instance.id)
         Backup.fail_for_instance(instance.id)
 
         return wsgi.Result(None, 202)
 
-    def _action_rebuild(self, context, instance, req, body):
-        LOG.info("Rebuild instance %s.", instance.id)
-        req_body = body['rebuild']
-        image_id = req_body['image_id']
-
-        instance.rebuild(image_id)
-        return wsgi.Result(None, 202)
-
     @admin_context
     def root(self, req, tenant_id, id):
         """Return the date and time root was enabled on an instance,
             if ever.
         """
-        LOG.info("Showing root history for a database "
-                 "instance %(id)s for tenant '%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Showing root history for a database "
+                   "instance %(id)s for tenant '%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         try:
             instance_models.Instance.load(context=context, id=id)
         except exception.TroveError as e:
             LOG.exception(e)
             return wsgi.Result(str(e), 404)
@@ -194,45 +176,45 @@
             rhv = views.RootHistoryView(reh.id, enabled=reh.created,
                                         user_id=reh.user)
         return wsgi.Result(rhv.data(), 200)
 
     @admin_context
     def hwinfo(self, req, tenant_id, id):
         """Return a single instance hardware info."""
-        LOG.info("Showing hardware info for a database "
-                 "instance %(id)s for tenant '%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Showing hardware info for a database "
+                   "instance %(id)s for tenant '%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.MgmtInstance.load(context=context, id=id)
 
         hwinfo = instance.get_hwinfo()
         return wsgi.Result(HwInfoView(id, hwinfo).data(), 200)
 
     @admin_context
     def diagnostics(self, req, tenant_id, id):
         """Return instance diagnostics for a single instance."""
-        LOG.info("Showing diagnostic info for a database "
-                 "instance %(id)s for tenant '%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Showing diagnostic info for a database "
+                   "instance %(id)s for tenant '%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.MgmtInstance.load(context=context, id=id)
 
         diagnostics = instance.get_diagnostics()
         return wsgi.Result(DiagnosticsView(id, diagnostics).data(), 200)
 
     @admin_context
     def rpc_ping(self, req, tenant_id, id):
         """Checks if instance is reachable via rpc."""
-        LOG.info("Sending RPC PING for a database "
-                 "instance %(id)s for tenant '%(tenant_id)s'\n"
-                 "req : '%(req)s'\n\n", {
-                     "tenant_id": tenant_id, "req": req, "id": id})
+        LOG.info(_("Sending RPC PING for a database "
+                   "instance %(id)s for tenant '%(tenant_id)s'\n"
+                   "req : '%(req)s'\n\n"), {
+                       "tenant_id": tenant_id, "req": req, "id": id})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.MgmtInstance.load(context=context, id=id)
 
         instance.rpc_ping()
         return wsgi.Result(None, 204)
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/instances/views.py` & `trove-8.0.1/trove/extensions/mgmt/instances/views.py`

 * *Files 2% similar despite different names*

```diff
@@ -102,17 +102,16 @@
 
     def __init__(self, instances, req=None):
         self.instances = instances
         self.req = req
 
     def data(self):
         data = []
-        # Return instances in the order of 'created'
-        for instance in sorted(self.instances, key=lambda ins: ins.created,
-                               reverse=True):
+        # These are model instances
+        for instance in self.instances:
             data.append(self.data_for_instance(instance))
         return {'instances': data}
 
     def data_for_instance(self, instance):
         view = MgmtInstanceView(instance, req=self.req)
         return view.data()['instance']
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/quota/service.py` & `trove-8.0.1/trove/extensions/mgmt/quota/service.py`

 * *Files 8% similar despite different names*

```diff
@@ -25,52 +25,40 @@
 
 LOG = logging.getLogger(__name__)
 
 
 class QuotaController(wsgi.Controller):
     """Controller for quota functionality."""
 
+    @admin_context
     def show(self, req, tenant_id, id):
-        """Return all quotas for this tenant.
-
-        Regular tenant can get his own resource quota.
-        Admin user can get quota for any tenant.
-        """
-        LOG.info("Indexing quota info for tenant '%(id)s'\n"
-                 "req : '%(req)s'\n\n", {"id": id, "req": req})
-
-        context = req.environ[wsgi.CONTEXT_KEY]
-        if id != tenant_id and not context.is_admin:
-            raise exception.TroveOperationAuthError(
-                tenant_id=tenant_id
-            )
+        """Return all quotas for this tenant."""
+        LOG.info(_("Indexing quota info for tenant '%(id)s'\n"
+                   "req : '%(req)s'\n\n"), {"id": id, "req": req})
 
         usages = quota_engine.get_all_quota_usages_by_tenant(id)
         limits = quota_engine.get_all_quotas_by_tenant(id)
-        for key in usages.keys():
-            setattr(usages[key], "limit", limits[key].hard_limit)
+        map(lambda r: setattr(usages[r], "limit", limits[r].hard_limit),
+            usages.keys())
         return wsgi.Result(views.QuotaUsageView(usages).data(), 200)
 
     @admin_context
     def update(self, req, body, tenant_id, id):
-        LOG.info("Updating quota limits for tenant '%(id)s'\n"
-                 "req : '%(req)s'\n\n", {"id": id, "req": req})
+        LOG.info(_("Updating quota limits for tenant '%(id)s'\n"
+                   "req : '%(req)s'\n\n"), {"id": id, "req": req})
 
         if not body:
             raise exception.BadRequest(_("Invalid request body."))
 
         quotas = {}
         quota = None
         registered_resources = quota_engine.resources
         for resource, limit in body['quotas'].items():
             if limit is None:
                 continue
-            elif limit < -1:
-                raise exception.QuotaLimitTooSmall(limit=limit,
-                                                   resource=resource)
             if resource == "xmlns":
                 continue
             if resource not in registered_resources:
                 raise exception.QuotaResourceUnknown(unknown=resource)
             try:
                 quota = Quota.find_by(tenant_id=id, resource=resource)
                 quota.hard_limit = limit
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/quota/views.py` & `trove-8.0.1/trove/extensions/mgmt/quota/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/upgrade/models.py` & `trove-8.0.1/trove/extensions/mgmt/upgrade/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from trove.common.clients import guest_client
+from trove.common.remote import guest_client
 
 
 class UpgradeMessageSender(object):
     """
     This class handles the business logic for sending
     an rpc message to the guest
     """
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mgmt/upgrade/service.py` & `trove-8.0.1/trove/extensions/mgmt/upgrade/service.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from oslo_log import log as logging
 
 import trove.common.apischema as apischema
 from trove.common.auth import admin_context
+from trove.common.i18n import _
 from trove.common import wsgi
 from trove.extensions.mgmt.upgrade.models import UpgradeMessageSender
 
 
 LOG = logging.getLogger(__name__)
 
 
@@ -28,16 +29,16 @@
     """
     Controller for guest agent upgrade
     """
     schemas = apischema.upgrade
 
     @admin_context
     def create(self, req, body, tenant_id, instance_id):
-        LOG.info("Sending upgrade notifications\nreq : '%(req)s'\n"
-                 "Admin tenant_id: %(tenant_id)s",
+        LOG.info(_("Sending upgrade notifications\nreq : '%(req)s'\n"
+                   "Admin tenant_id: %(tenant_id)s"),
                  {"tenant_id": tenant_id, "req": req})
 
         context = req.environ.get(wsgi.CONTEXT_KEY)
         upgrade = body['upgrade']
 
         instance_version = upgrade.get('instance_version')
         location = upgrade.get('location')
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mongodb/service.py` & `trove-8.0.1/trove/extensions/mongodb/service.py`

 * *Files 13% similar despite different names*

```diff
@@ -29,15 +29,15 @@
             operation='disable_root', datastore=MANAGER)
 
     def _block_cluster_instance_actions(self):
         return True
 
     def _find_query_router_ids(self, tenant_id, cluster_id):
         args = {'tenant_id': tenant_id, 'cluster_id': cluster_id,
-                'deleted': False, 'type': 'query_router'}
+                'type': 'query_router'}
         query_router_instances = DBInstance.find_all(**args).all()
         return [db_instance.id for db_instance in query_router_instances]
 
     def _get_cluster_instance_id(self, tenant_id, cluster_id):
         instance_ids = self._find_cluster_node_ids(tenant_id, cluster_id)
         single_instance_id = self._find_query_router_ids(tenant_id,
                                                          cluster_id)[0]
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mysql/common.py` & `trove-8.0.1/trove/extensions/mysql/common.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from urllib.parse import unquote
+from six.moves.urllib.parse import unquote
 
 from trove.common.db.mysql import models as guest_models
 from trove.common import exception
 
 
 def populate_validated_databases(dbs):
     """
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mysql/models.py` & `trove-8.0.1/trove/extensions/mysql/models.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,18 +14,18 @@
 #    under the License.
 
 """
 Model classes that extend the instances functionality for MySQL instances.
 """
 
 from trove.common import cfg
-from trove.common.clients import create_guest_client
 from trove.common.db.mysql import models as guest_models
 from trove.common import exception
 from trove.common.notification import StartNotification
+from trove.common.remote import create_guest_client
 from trove.common import utils
 from trove.extensions.common.models import load_and_verify
 from trove.extensions.common.models import RootHistory
 
 CONF = cfg.CONF
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mysql/service.py` & `trove-8.0.1/trove/extensions/mysql/service.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 from trove.common import exception
 from trove.common.i18n import _
 from trove.common import notification
 from trove.common.notification import StartNotification
 from trove.common import pagination
 from trove.common.utils import correct_id_with_req
 from trove.common import wsgi
+from trove.extensions.common.service import DefaultRootController
 from trove.extensions.common.service import ExtensionController
 from trove.extensions.mysql.common import populate_users
 from trove.extensions.mysql.common import populate_validated_databases
 from trove.extensions.mysql.common import unquote_user_host
 from trove.extensions.mysql import models
 from trove.extensions.mysql import views
 
@@ -52,30 +53,30 @@
         if 'update_all' == action:
             update_type = list(body.keys())[0]
             action_schema = action_schema.get(update_type, {})
         return action_schema
 
     def index(self, req, tenant_id, instance_id):
         """Return all users."""
-        LOG.info("Listing users for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Listing users for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(context, 'user:index', instance_id)
         users, next_marker = models.Users.load(context, instance_id)
         view = views.UsersView(users)
         paged = pagination.SimplePaginatedDataView(req.url, 'users', view,
                                                    next_marker)
         return wsgi.Result(paged.data(), 200)
 
     def create(self, req, body, tenant_id, instance_id):
         """Creates a set of users."""
-        LOG.info("Creating users for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n"
-                 "body: '%(body)s'\n'n",
+        LOG.info(_("Creating users for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"
+                   "body: '%(body)s'\n'n"),
                  {"id": instance_id,
                   "req": strutils.mask_password(req),
                   "body": strutils.mask_password(body)})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(context, 'user:create', instance_id)
         context.notification = notification.DBaaSUserCreate(context,
                                                             request=req)
@@ -88,16 +89,16 @@
                 models.User.create(context, instance_id, model_users)
             except (ValueError, AttributeError) as e:
                 raise exception.BadRequest(_("User create error: %(e)s")
                                            % {'e': e})
         return wsgi.Result(None, 202)
 
     def delete(self, req, tenant_id, instance_id, id):
-        LOG.info("Delete instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Delete instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(context, 'user:delete', instance_id)
         id = correct_id_with_req(id, req)
         username, host = unquote_user_host(id)
         user = None
         context.notification = notification.DBaaSUserDelete(context,
@@ -117,16 +118,16 @@
             if not user:
                 raise exception.UserNotFound(uuid=id)
             models.User.delete(context, instance_id, user.serialize())
         return wsgi.Result(None, 202)
 
     def show(self, req, tenant_id, instance_id, id):
         """Return a single user."""
-        LOG.info("Showing a user for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Showing a user for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(context, 'user:show', instance_id)
         id = correct_id_with_req(id, req)
         username, host = unquote_user_host(id)
         user = None
         try:
@@ -137,16 +138,16 @@
         if not user:
             raise exception.UserNotFound(uuid=id)
         view = views.UserView(user)
         return wsgi.Result(view.data(), 200)
 
     def update(self, req, body, tenant_id, instance_id, id):
         """Change attributes for one user."""
-        LOG.info("Updating user attributes for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Updating user attributes for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": strutils.mask_password(req)})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(context, 'user:update', instance_id)
         id = correct_id_with_req(id, req)
         username, hostname = unquote_user_host(id)
         user = None
         user_attrs = body['user']
@@ -168,16 +169,16 @@
             except (ValueError, AttributeError) as e:
                 raise exception.BadRequest(_("User update error: %(e)s")
                                            % {'e': e})
         return wsgi.Result(None, 202)
 
     def update_all(self, req, body, tenant_id, instance_id):
         """Change the password of one or more users."""
-        LOG.info("Updating user password for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Updating user password for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": strutils.mask_password(req)})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(context, 'user:update_all', instance_id)
         context.notification = notification.DBaaSUserChangePassword(
             context, request=req)
         users = body['users']
         model_users = []
@@ -229,69 +230,69 @@
                                        % {'e': e})
         if not user:
             raise exception.UserNotFound(uuid=user_id)
         return user
 
     def index(self, req, tenant_id, instance_id, user_id):
         """Show permissions for the given user."""
-        LOG.info("Showing user access for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Showing user access for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'user_access:index', instance_id)
         # Make sure this user exists.
         user_id = correct_id_with_req(user_id, req)
         user = self._get_user(context, instance_id, user_id)
         if not user:
-            LOG.error("No such user: %(user)s ", {'user': user})
+            LOG.error(_("No such user: %(user)s "), {'user': user})
             raise exception.UserNotFound(uuid=user)
         username, hostname = unquote_user_host(user_id)
         access = models.User.access(context, instance_id, username, hostname)
         view = views.UserAccessView(access.databases)
         return wsgi.Result(view.data(), 200)
 
     def update(self, req, body, tenant_id, instance_id, user_id):
         """Grant access for a user to one or more databases."""
-        LOG.info("Granting user access for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Granting user access for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'user_access:update', instance_id)
         context.notification = notification.DBaaSUserGrant(
             context, request=req)
         user_id = correct_id_with_req(user_id, req)
         user = self._get_user(context, instance_id, user_id)
         if not user:
-            LOG.error("No such user: %(user)s ", {'user': user})
+            LOG.error(_("No such user: %(user)s "), {'user': user})
             raise exception.UserNotFound(uuid=user)
         username, hostname = unquote_user_host(user_id)
         databases = [db['name'] for db in body['databases']]
         with StartNotification(context, instance_id=instance_id,
                                username=username, database=databases):
             models.User.grant(context, instance_id, username, hostname,
                               databases)
         return wsgi.Result(None, 202)
 
     def delete(self, req, tenant_id, instance_id, user_id, id):
         """Revoke access for a user."""
-        LOG.info("Revoking user access for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Revoking user access for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'user_access:delete', instance_id)
         context.notification = notification.DBaaSUserRevoke(
             context, request=req)
         user_id = correct_id_with_req(user_id, req)
         user = self._get_user(context, instance_id, user_id)
         if not user:
-            LOG.error("No such user: %(user)s ", {'user': user})
+            LOG.error(_("No such user: %(user)s "), {'user': user})
             raise exception.UserNotFound(uuid=user)
         username, hostname = unquote_user_host(user_id)
         access = models.User.access(context, instance_id, username, hostname)
         databases = [db.name for db in access.databases]
         with StartNotification(context, instance_id=instance_id,
                                username=username, database=databases):
             if id not in databases:
@@ -302,32 +303,32 @@
 
 class SchemaController(ExtensionController):
     """Controller for instance functionality."""
     schemas = apischema.dbschema
 
     def index(self, req, tenant_id, instance_id):
         """Return all schemas."""
-        LOG.info("Listing schemas for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Listing schemas for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'database:index', instance_id)
         schemas, next_marker = models.Schemas.load(context, instance_id)
         view = views.SchemasView(schemas)
         paged = pagination.SimplePaginatedDataView(req.url, 'databases', view,
                                                    next_marker)
         return wsgi.Result(paged.data(), 200)
 
     def create(self, req, body, tenant_id, instance_id):
         """Creates a set of schemas."""
-        LOG.info("Creating schema for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n"
-                 "body: '%(body)s'\n'n",
+        LOG.info(_("Creating schema for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"
+                   "body: '%(body)s'\n'n"),
                  {"id": instance_id,
                   "req": req,
                   "body": body})
 
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'database:create', instance_id)
@@ -342,16 +343,16 @@
                 models.Schema.create(context, instance_id, model_schemas)
             except (ValueError, AttributeError) as e:
                 raise exception.BadRequest(_("Database create error: %(e)s")
                                            % {'e': e})
         return wsgi.Result(None, 202)
 
     def delete(self, req, tenant_id, instance_id, id):
-        LOG.info("Deleting schema for instance '%(id)s'\n"
-                 "req : '%(req)s'\n\n",
+        LOG.info(_("Deleting schema for instance '%(id)s'\n"
+                   "req : '%(req)s'\n\n"),
                  {"id": instance_id, "req": req})
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'database:delete', instance_id)
         context.notification = notification.DBaaSDatabaseDelete(
             context, request=req)
         with StartNotification(context, instance_id=instance_id, dbname=id):
@@ -367,7 +368,16 @@
         return wsgi.Result(None, 202)
 
     def show(self, req, tenant_id, instance_id, id):
         context = req.environ[wsgi.CONTEXT_KEY]
         self.authorize_target_action(
             context, 'database:show', instance_id)
         raise webob.exc.HTTPNotImplemented()
+
+
+class MySQLRootController(DefaultRootController):
+
+    def _find_root_user(self, context, instance_id):
+        user = guest_models.MySQLUser.root()
+        return models.User.load(context, instance_id,
+                                user.name, user.host,
+                                root_user=True)
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/mysql/views.py` & `trove-8.0.1/trove/extensions/mysql/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/pxc/service.py` & `trove-8.0.1/trove/extensions/pxc/service.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/redis/models.py` & `trove-8.0.1/trove/tests/fakes/limits.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2017 Eayun, Inc.
+# Copyright 2014 Rackspace Hosting
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
@@ -10,19 +10,17 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from trove.common.clients import create_guest_client
-from trove.extensions.common.models import load_and_verify
-from trove.extensions.common.models import Root
+from trove.common import limits
 
 
-class RedisRoot(Root):
-    @classmethod
-    def get_auth_password(cls, context, instance_id):
-        load_and_verify(context, instance_id)
-        password = create_guest_client(context,
-                                       instance_id).get_root_password()
-        return password
+ENABLED = False
+
+
+class FakeRateLimitingMiddleware(limits.RateLimitingMiddleware):
+
+    def enabled(self):
+        return ENABLED
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/redis/views.py` & `trove-8.0.1/trove/extensions/account/views.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,37 @@
-# Copyright 2017 Eayun, Inc.
+# Copyright 2012 OpenStack Foundation
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-#
 
-from trove.extensions.common.views import UserView
 
+class AccountsView(object):
+
+    def __init__(self, accounts_summary):
+        self.accounts_summary = accounts_summary
+
+    def data(self):
+        return {'accounts': self.accounts_summary.accounts}
+
+
+class AccountView(object):
 
-class RedisRootCreatedView(UserView):
-    def __init__(self, user, failed_slaves):
-        self.failed_slaves = failed_slaves
-        super(RedisRootCreatedView, self).__init__(user)
+    def __init__(self, account):
+        self.account = account
 
     def data(self):
-        user_dict = {
-            "name": self.user.name,
-            "password": self.user.password
+        return {
+            'account': {
+                'id': self.account.id,
+                'instance_ids': self.account.instance_ids,
+            }
         }
-        return {"user": user_dict, "failed_slaves": self.failed_slaves}
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/routes/mgmt.py` & `trove-8.0.1/trove/extensions/routes/mgmt.py`

 * *Files 18% similar despite different names*

```diff
@@ -13,17 +13,20 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from trove.common import extensions
 from trove.extensions.mgmt.clusters.service import MgmtClusterController
 from trove.extensions.mgmt.configuration import service as conf_service
 from trove.extensions.mgmt.datastores.service import DatastoreVersionController
+from trove.extensions.mgmt.host.instance import service as hostservice
+from trove.extensions.mgmt.host.service import HostController
 from trove.extensions.mgmt.instances.service import MgmtInstanceController
 from trove.extensions.mgmt.quota.service import QuotaController
 from trove.extensions.mgmt.upgrade.service import UpgradeController
+from trove.extensions.mgmt.volume.service import StorageController
 
 
 class Mgmt(extensions.ExtensionDescriptor):
 
     def get_name(self):
         return "Mgmt"
 
@@ -54,20 +57,40 @@
 
         clusters = extensions.ResourceExtension(
             '{tenant_id}/mgmt/clusters',
             MgmtClusterController(),
             member_actions={'action': 'POST'})
         resources.append(clusters)
 
+        hosts = extensions.ResourceExtension(
+            '{tenant_id}/mgmt/hosts',
+            HostController(),
+            member_actions={})
+        resources.append(hosts)
+
         quota = extensions.ResourceExtension(
             '{tenant_id}/mgmt/quotas',
             QuotaController(),
             member_actions={})
         resources.append(quota)
 
+        storage = extensions.ResourceExtension(
+            '{tenant_id}/mgmt/storage',
+            StorageController(),
+            member_actions={})
+        resources.append(storage)
+
+        host_instances = extensions.ResourceExtension(
+            'instances',
+            hostservice.HostInstanceController(),
+            parent={'member_name': 'host',
+                    'collection_name': '{tenant_id}/mgmt/hosts'},
+            collection_actions={'action': 'POST'})
+        resources.append(host_instances)
+
         upgrade = extensions.ResourceExtension(
             '{tenant_id}/mgmt/instances/{instance_id}/upgrade',
             UpgradeController(),
             member_actions={})
         resources.append(upgrade)
 
         datastore_configuration_parameters = extensions.ResourceExtension(
```

### Comparing `trove-21.0.0.0rc2/trove/extensions/routes/mysql.py` & `trove-8.0.1/trove/extensions/routes/mysql.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/extensions/vertica/service.py` & `trove-8.0.1/trove/extensions/vertica/service.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/flavor/models.py` & `trove-8.0.1/trove/flavor/models.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,32 +13,32 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 """Model classes that form the core of instance flavor functionality."""
 
 
 from novaclient import exceptions as nova_exceptions
-from trove.common.clients import create_nova_client
 from trove.common import exception
 from trove.common.models import NovaRemoteModelBase
+from trove.common.remote import create_nova_client
 
 
 class Flavor(object):
 
     _data_fields = ['id', 'links', 'name', 'ram', 'vcpus', 'disk', 'ephemeral']
 
     def __init__(self, flavor=None, context=None, flavor_id=None):
         if flavor:
             self.flavor = flavor
             return
         if flavor_id and context:
             try:
                 client = create_nova_client(context)
                 self.flavor = client.flavors.get(flavor_id)
-            except nova_exceptions.NotFound:
+            except nova_exceptions.NotFound as e:
                 raise exception.NotFound(uuid=flavor_id)
             except nova_exceptions.ClientException as e:
                 raise exception.TroveError(str(e))
             return
         msg = ("Flavor is not defined, and"
                " context and flavor_id were not specified.")
         raise exception.InvalidModelError(errors=msg)
```

### Comparing `trove-21.0.0.0rc2/trove/flavor/service.py` & `trove-8.0.1/trove/flavor/service.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,14 +9,17 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+
+import six
+
 from trove.common import exception
 from trove.common import policy
 from trove.common import wsgi
 from trove.flavor import models
 from trove.flavor import views
 
 
@@ -38,14 +41,14 @@
         """Return all flavors."""
         context = req.environ[wsgi.CONTEXT_KEY]
         policy.authorize_on_tenant(context, 'flavor:index')
         flavors = models.Flavors(context=context)
         return wsgi.Result(views.FlavorsView(flavors, req).data(), 200)
 
     def _validate_flavor_id(self, id):
-        if isinstance(id, str):
+        if isinstance(id, six.string_types):
             return
         try:
             if int(id) != float(id):
                 raise exception.NotFound(uuid=id)
         except ValueError:
             raise exception.NotFound(uuid=id)
```

### Comparing `trove-21.0.0.0rc2/trove/flavor/views.py` & `trove-8.0.1/trove/flavor/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/guestagent/api.py` & `trove-8.0.1/trove/guestagent/api.py`

 * *Files 22% similar despite different names*

```diff
@@ -20,63 +20,57 @@
 from eventlet import Timeout
 from oslo_log import log as logging
 import oslo_messaging as messaging
 from oslo_messaging.rpc.client import RemoteError
 
 from trove.common import cfg
 from trove.common import exception
+from trove.common.i18n import _
 from trove.common.notification import NotificationCastWrapper
 from trove import rpc
 
 CONF = cfg.CONF
 LOG = logging.getLogger(__name__)
+AGENT_LOW_TIMEOUT = CONF.agent_call_low_timeout
+AGENT_HIGH_TIMEOUT = CONF.agent_call_high_timeout
+AGENT_SNAPSHOT_TIMEOUT = CONF.agent_replication_snapshot_timeout
 
 
 class API(object):
     """API for interacting with the guest manager.
 
     API version history:
         * 1.0 - Initial version.
-        * 1.1 - Added argement ds_version to prepare and
-                start_db_with_conf_changes
-              - Remove do_not_start_on_reboot from stop_db
-              - Added online argument to resize_fs
 
     When updating this API, also update API_LATEST_VERSION
     """
 
     # API_LATEST_VERSION should bump the minor number each time
     # a method signature is added or changed
-    API_LATEST_VERSION = '1.1'
+    API_LATEST_VERSION = '1.0'
 
     # API_BASE_VERSION should only change on major version upgrade
     API_BASE_VERSION = '1.0'
 
     VERSION_ALIASES = {
         'icehouse': '1.0',
         'juno': '1.0',
         'kilo': '1.0',
         'liberty': '1.0',
         'mitaka': '1.0',
         'newton': '1.0',
-        'ussuri': '1.0',
-        'victoria': '1.1',
 
         'latest': API_LATEST_VERSION
     }
 
     def __init__(self, context, id):
         self.context = context
         self.id = id
         super(API, self).__init__()
 
-        self.agent_low_timeout = CONF.agent_call_low_timeout
-        self.agent_high_timeout = CONF.agent_call_high_timeout
-        self.agent_snapshot_timeout = CONF.agent_replication_snapshot_timeout
-
         version_cap = self.VERSION_ALIASES.get(
             CONF.upgrade_levels.guestagent, CONF.upgrade_levels.guestagent)
         self.target = messaging.Target(topic=self._get_routing_key(),
                                        version=version_cap)
 
         self.client = self.get_client(self.target, version_cap)
 
@@ -94,33 +88,33 @@
         try:
             cctxt = self.client.prepare(version=version, timeout=timeout_sec)
             result = cctxt.call(self.context, method_name, **kwargs)
 
             LOG.debug("Result is %s.", result)
             return result
         except RemoteError as r:
-            LOG.exception("Error calling %s", method_name)
+            LOG.exception(_("Error calling %s"), method_name)
             raise exception.GuestError(original_message=r.value)
         except Exception as e:
-            LOG.exception("Error calling %s", method_name)
+            LOG.exception(_("Error calling %s"), method_name)
             raise exception.GuestError(original_message=str(e))
         except Timeout:
             raise exception.GuestTimeout()
 
     def _cast(self, method_name, version, **kwargs):
-        LOG.debug("Calling %s asynchronously", method_name)
+        LOG.debug("Casting %s", method_name)
         try:
             with NotificationCastWrapper(self.context, 'guest'):
                 cctxt = self.client.prepare(version=version)
                 cctxt.cast(self.context, method_name, **kwargs)
         except RemoteError as r:
-            LOG.exception("Error calling %s", method_name)
+            LOG.exception(_("Error calling %s"), method_name)
             raise exception.GuestError(original_message=r.value)
         except Exception as e:
-            LOG.exception("Error calling %s", method_name)
+            LOG.exception(_("Error calling %s"), method_name)
             raise exception.GuestError(original_message=str(e))
 
     def _get_routing_key(self):
         """Create the routing key based on the container id."""
         return "guestagent.%s" % self.id
 
     def change_passwords(self, users):
@@ -145,66 +139,66 @@
         """Make an asynchronous call to create a new database user"""
         LOG.debug("Creating Users for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
         self._cast("create_user", version=version, users=users)
 
     def get_user(self, username, hostname):
-        """Make a synchronous call to get a single database user."""
+        """Make an asynchronous call to get a single database user."""
         LOG.debug("Getting a user %(username)s on instance %(id)s.",
                   {'username': username, 'id': self.id})
         version = self.API_BASE_VERSION
 
         return self._call("get_user",
-                          self.agent_low_timeout, version=version,
+                          AGENT_LOW_TIMEOUT, version=version,
                           username=username, hostname=hostname)
 
     def list_access(self, username, hostname):
         """Show all the databases to which a user has more than USAGE."""
         LOG.debug("Showing user %(username)s grants on instance %(id)s.",
                   {'username': username, 'id': self.id})
         version = self.API_BASE_VERSION
 
         return self._call("list_access",
-                          self.agent_low_timeout, version=version,
+                          AGENT_LOW_TIMEOUT, version=version,
                           username=username, hostname=hostname)
 
     def grant_access(self, username, hostname, databases):
         """Grant a user permission to use a given database."""
         LOG.debug("Granting access to databases %(databases)s for user "
                   "%(username)s on instance %(id)s.", {'username': username,
                                                        'databases': databases,
                                                        'id': self.id})
         version = self.API_BASE_VERSION
 
         return self._call("grant_access",
-                          self.agent_low_timeout, version=version,
+                          AGENT_LOW_TIMEOUT, version=version,
                           username=username, hostname=hostname,
                           databases=databases)
 
     def revoke_access(self, username, hostname, database):
         """Remove a user's permission to use a given database."""
         LOG.debug("Revoking access from database %(database)s for user "
                   "%(username)s on instance %(id)s.", {'username': username,
                                                        'database': database,
                                                        'id': self.id})
         version = self.API_BASE_VERSION
 
         return self._call("revoke_access",
-                          self.agent_low_timeout, version=version,
+                          AGENT_LOW_TIMEOUT, version=version,
                           username=username, hostname=hostname,
                           database=database)
 
     def list_users(self, limit=None, marker=None, include_marker=False):
-        """Make a synchronous call to list database users."""
+        """Make an asynchronous call to list database users."""
         LOG.debug("Listing Users for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("list_users",
-                          self.agent_high_timeout, version=version,
+        return self._call("list_users", AGENT_HIGH_TIMEOUT,
+                          version=version,
                           limit=limit, marker=marker,
                           include_marker=include_marker)
 
     def delete_user(self, user):
         """Make an asynchronous call to delete an existing database user."""
         LOG.debug("Deleting user %(user)s for instance %(instance_id)s.",
                   {'user': user, 'instance_id': self.id})
@@ -219,139 +213,122 @@
         LOG.debug("Creating databases for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
         self._cast("create_database", version=version,
                    databases=databases)
 
     def list_databases(self, limit=None, marker=None, include_marker=False):
-        """Make a synchronous call to list databases."""
+        """Make an asynchronous call to list databases."""
         LOG.debug("Listing databases for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("list_databases", self.agent_low_timeout,
+        return self._call("list_databases", AGENT_LOW_TIMEOUT,
                           version=version, limit=limit, marker=marker,
                           include_marker=include_marker)
 
     def delete_database(self, database):
         """Make an asynchronous call to delete an existing database
            within the specified container
         """
         LOG.debug("Deleting database %(database)s for "
                   "instance %(instance_id)s.", {'database': database,
                                                 'instance_id': self.id})
         version = self.API_BASE_VERSION
 
         self._cast("delete_database", version=version, database=database)
 
-    def get_root_password(self):
-        """Make a synchronous call to get root password of instance.
-        """
-        LOG.debug("Get root password of instance %s.", self.id)
-        version = self.API_BASE_VERSION
-
-        return self._call("get_root_password", self.agent_high_timeout,
-                          version=version)
-
     def enable_root(self):
         """Make a synchronous call to enable the root user for
            access from anywhere
         """
         LOG.debug("Enable root user for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("enable_root", self.agent_high_timeout,
+        return self._call("enable_root", AGENT_HIGH_TIMEOUT,
                           version=version)
 
     def enable_root_with_password(self, root_password=None):
         """Make a synchronous call to enable the root user for
            access from anywhere
         """
         LOG.debug("Enable root user for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("enable_root_with_password",
-                          self.agent_high_timeout,
+        return self._call("enable_root_with_password", AGENT_HIGH_TIMEOUT,
                           version=version, root_password=root_password)
 
     def disable_root(self):
         """Make a synchronous call to disable the root user for
            access from anywhere
         """
         LOG.debug("Disable root user for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("disable_root", self.agent_low_timeout,
+        return self._call("disable_root", AGENT_LOW_TIMEOUT,
                           version=version)
 
     def is_root_enabled(self):
         """Make a synchronous call to check if root access is
            available for the container
         """
         LOG.debug("Check root access for instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("is_root_enabled", self.agent_low_timeout,
+        return self._call("is_root_enabled", AGENT_LOW_TIMEOUT,
                           version=version)
 
     def get_hwinfo(self):
         """Make a synchronous call to get hardware info for the container"""
         LOG.debug("Check hwinfo on instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("get_hwinfo", self.agent_low_timeout,
+        return self._call("get_hwinfo", AGENT_LOW_TIMEOUT,
                           version=version)
 
     def get_diagnostics(self):
         """Make a synchronous call to get diagnostics for the container"""
         LOG.debug("Check diagnostics on instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("get_diagnostics",
-                          self.agent_low_timeout, version=version)
+        return self._call("get_diagnostics", AGENT_LOW_TIMEOUT,
+                          version=version)
 
     def rpc_ping(self):
         """Make a synchronous RPC call to check if we can ping the instance."""
         LOG.debug("Check RPC ping on instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("rpc_ping",
-                          self.agent_low_timeout, version=version)
+        return self._call("rpc_ping", AGENT_LOW_TIMEOUT, version=version)
 
     def prepare(self, memory_mb, packages, databases, users,
                 device_path='/dev/vdb', mount_point='/mnt/volume',
                 backup_info=None, config_contents=None, root_password=None,
                 overrides=None, cluster_config=None, snapshot=None,
-                modules=None, ds_version=None):
+                modules=None):
         """Make an asynchronous call to prepare the guest
            as a database container optionally includes a backup id for restores
         """
         LOG.debug("Sending the call to prepare the Guest.")
 
-        version = '1.1'
+        version = self.API_BASE_VERSION
 
         # Taskmanager is a publisher, guestagent is a consumer. Usually
         # consumer creates a queue, but in this case we have to make sure
         # "prepare" doesn't get lost if for some reason guest was delayed and
         # didn't create a queue on time.
         self._create_guest_queue()
 
         packages = packages.split()
-
-        prepare_args = dict(
-            packages=packages, databases=databases, memory_mb=memory_mb,
-            users=users, device_path=device_path, mount_point=mount_point,
+        self._cast(
+            "prepare", version=version, packages=packages,
+            databases=databases, memory_mb=memory_mb, users=users,
+            device_path=device_path, mount_point=mount_point,
             backup_info=backup_info, config_contents=config_contents,
             root_password=root_password, overrides=overrides,
-            cluster_config=cluster_config, snapshot=snapshot, modules=modules,
-            ds_version=ds_version)
-
-        if not self.client.can_send_version(version):
-            prepare_args.pop('ds_version')
-            version = '1.0'
-        self._cast("prepare", version=version, **prepare_args)
+            cluster_config=cluster_config, snapshot=snapshot, modules=modules)
 
     def _create_guest_queue(self):
         """Call to construct, start and immediately stop rpc server in order
            to create a queue to communicate with the guestagent. This is
            method do nothing in case a queue is already created by
            the guest
         """
@@ -371,108 +348,90 @@
                 server.wait()
 
     def pre_upgrade(self):
         """Prepare the guest for upgrade."""
         LOG.debug("Sending the call to prepare the guest for upgrade.")
         version = self.API_BASE_VERSION
 
-        return self._call("pre_upgrade",
-                          self.agent_high_timeout, version=version)
+        return self._call("pre_upgrade", AGENT_HIGH_TIMEOUT,
+                          version=version)
 
     def post_upgrade(self, upgrade_info):
         """Recover the guest after upgrading the guest's image."""
         LOG.debug("Recover the guest after upgrading the guest's image.")
         version = self.API_BASE_VERSION
         LOG.debug("Recycling the client ...")
         version_cap = self.VERSION_ALIASES.get(
             CONF.upgrade_levels.guestagent, CONF.upgrade_levels.guestagent)
         self.client = self.get_client(self.target, version_cap)
 
-        self._call("post_upgrade",
-                   self.agent_high_timeout, version=version,
+        self._call("post_upgrade", AGENT_HIGH_TIMEOUT, version=version,
                    upgrade_info=upgrade_info)
 
-    def upgrade(self, upgrade_info):
-        """Upgrade database service."""
-        LOG.debug("Sending the call to upgrade database service.")
-        version = self.API_BASE_VERSION
-
-        return self._cast("upgrade", version=version,
-                          upgrade_info=upgrade_info)
-
     def restart(self):
         """Restart the database server."""
         LOG.debug("Sending the call to restart the database process "
                   "on the Guest.")
         version = self.API_BASE_VERSION
 
-        self._call("restart", self.agent_high_timeout, version=version)
-
-    def start_db_with_conf_changes(self, config_contents, ds_version):
-        """Start the database with given configuration.
+        self._call("restart", AGENT_HIGH_TIMEOUT, version=version)
 
-        This method is called after resize.
-        """
+    def start_db_with_conf_changes(self, config_contents):
+        """Start the database server."""
         LOG.debug("Sending the call to start the database process on "
-                  "the Guest with a timeout of %s.",
-                  self.agent_high_timeout)
-        start_args = dict(config_contents=config_contents,
-                          ds_version=ds_version)
-
-        version = '1.1'
-        if not self.client.can_send_version(version):
-            start_args.pop('ds_version')
-            version = '1.0'
+                  "the Guest with a timeout of %s.", AGENT_HIGH_TIMEOUT)
+        version = self.API_BASE_VERSION
 
-        self._call("start_db_with_conf_changes", self.agent_high_timeout,
-                   version=version, **start_args)
+        self._call("start_db_with_conf_changes", AGENT_HIGH_TIMEOUT,
+                   version=version, config_contents=config_contents)
 
     def reset_configuration(self, configuration):
-        """Reset the database base configuration.
-
-        Ignore running state of the database server, just change the config
-        file to a new flavor.
+        """Ignore running state of the database server; just change
+           the config file to a new flavor.
         """
         LOG.debug("Sending the call to change the database conf file on the "
-                  "Guest with a timeout of %s.",
-                  self.agent_high_timeout)
+                  "Guest with a timeout of %s.", AGENT_HIGH_TIMEOUT)
         version = self.API_BASE_VERSION
 
-        self._call("reset_configuration", self.agent_high_timeout,
+        self._call("reset_configuration", AGENT_HIGH_TIMEOUT,
                    version=version, configuration=configuration)
 
     def stop_db(self, do_not_start_on_reboot=False):
         """Stop the database server."""
         LOG.debug("Sending the call to stop the database process "
                   "on the Guest.")
+        version = self.API_BASE_VERSION
 
-        version = '1.1'
-        stop_args = {}
-        if not self.client.can_send_version(version):
-            stop_args['do_not_start_on_reboot'] = do_not_start_on_reboot
-            version = '1.0'
+        self._call("stop_db", AGENT_HIGH_TIMEOUT, version=version,
+                   do_not_start_on_reboot=do_not_start_on_reboot)
+
+    def upgrade(self, instance_version, location, metadata=None):
+        """Make an asynchronous call to self upgrade the guest agent."""
+        LOG.debug("Sending an upgrade call to nova-guest.")
+        version = self.API_BASE_VERSION
 
-        self._call("stop_db", self.agent_low_timeout,
-                   version=version, **stop_args)
+        self._cast("upgrade", version=version,
+                   instance_version=instance_version,
+                   location=location,
+                   metadata=metadata)
 
     def get_volume_info(self):
         """Make a synchronous call to get volume info for the container."""
         LOG.debug("Check Volume Info on instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("get_filesystem_stats", self.agent_low_timeout,
+        return self._call("get_filesystem_stats", AGENT_LOW_TIMEOUT,
                           version=version, fs_path=None)
 
     def update_guest(self):
         """Make a synchronous call to update the guest agent."""
         LOG.debug("Updating guest agent on instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        self._call("update_guest",
-                   self.agent_high_timeout, version=version)
+        self._call("update_guest", AGENT_HIGH_TIMEOUT, version=version)
 
     def create_backup(self, backup_info):
         """Make async call to create a full backup of this instance."""
         LOG.debug("Create Backup %(backup_id)s "
                   "for instance %(instance_id)s.",
                   {'backup_id': backup_info['id'], 'instance_id': self.id})
         version = self.API_BASE_VERSION
@@ -482,77 +441,66 @@
 
     def mount_volume(self, device_path=None, mount_point=None):
         """Mount the volume."""
         LOG.debug("Mount volume %(mount)s on instance %(id)s.", {
             'mount': mount_point, 'id': self.id})
         version = self.API_BASE_VERSION
 
-        self._call("mount_volume",
-                   self.agent_low_timeout, version=version,
+        self._call("mount_volume", AGENT_LOW_TIMEOUT, version=version,
                    device_path=device_path, mount_point=mount_point)
 
     def unmount_volume(self, device_path=None, mount_point=None):
         """Unmount the volume."""
         LOG.debug("Unmount volume %(device)s on instance %(id)s.", {
             'device': device_path, 'id': self.id})
         version = self.API_BASE_VERSION
 
-        self._call("unmount_volume",
-                   self.agent_low_timeout, version=version,
+        self._call("unmount_volume", AGENT_LOW_TIMEOUT, version=version,
                    device_path=device_path, mount_point=mount_point)
 
-    def resize_fs(self, device_path=None, mount_point=None, online=False):
+    def resize_fs(self, device_path=None, mount_point=None):
         """Resize the filesystem."""
         LOG.debug("Resize device %(device)s on instance %(id)s.", {
             'device': device_path, 'id': self.id})
+        version = self.API_BASE_VERSION
 
-        resize_args = dict(device_path=device_path,
-                           mount_point=mount_point,
-                           online=online)
-
-        version = '1.1'
-        if not self.client.can_send_version(version):
-            resize_args.pop('online')
-            version = '1.0'
-
-        self._call("resize_fs",
-                   self.agent_high_timeout, version=version, **resize_args)
+        self._call("resize_fs", AGENT_HIGH_TIMEOUT, version=version,
+                   device_path=device_path, mount_point=mount_point)
 
     def update_overrides(self, overrides, remove=False):
         """Update the overrides."""
         LOG.debug("Updating overrides values %(overrides)s on instance "
                   "%(id)s.", {'overrides': overrides, 'id': self.id})
         version = self.API_BASE_VERSION
 
-        self._call("update_overrides", self.agent_high_timeout,
+        self._call("update_overrides", AGENT_HIGH_TIMEOUT,
                    version=version, overrides=overrides, remove=remove)
 
     def apply_overrides(self, overrides):
         LOG.debug("Applying overrides values %(overrides)s on instance "
                   "%(id)s.", {'overrides': overrides, 'id': self.id})
         version = self.API_BASE_VERSION
 
-        self._call("apply_overrides", self.agent_high_timeout,
+        self._call("apply_overrides", AGENT_HIGH_TIMEOUT,
                    version=version, overrides=overrides)
 
     def backup_required_for_replication(self):
         LOG.debug("Checking backup requirement for replication")
         version = self.API_BASE_VERSION
 
         return self._call("backup_required_for_replication",
-                          self.agent_low_timeout,
+                          AGENT_LOW_TIMEOUT,
                           version=version)
 
     def get_replication_snapshot(self, snapshot_info=None,
                                  replica_source_config=None):
         LOG.debug("Retrieving replication snapshot from instance %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("get_replication_snapshot",
-                          self.agent_snapshot_timeout,
+        return self._call("get_replication_snapshot", AGENT_SNAPSHOT_TIMEOUT,
                           version=version, snapshot_info=snapshot_info,
                           replica_source_config=replica_source_config)
 
     def attach_replication_slave(self, snapshot, replica_config=None):
         LOG.debug("Configuring instance %s to replicate from %s.",
                   self.id, snapshot.get('master').get('id'))
         version = self.API_BASE_VERSION
@@ -560,142 +508,123 @@
         self._cast("attach_replication_slave", version=version,
                    snapshot=snapshot, slave_config=replica_config)
 
     def detach_replica(self, for_failover=False):
         LOG.debug("Detaching replica %s from its replication source.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("detach_replica", self.agent_high_timeout,
+        return self._call("detach_replica", AGENT_HIGH_TIMEOUT,
                           version=version, for_failover=for_failover)
 
     def get_replica_context(self):
         LOG.debug("Getting replica context.")
         version = self.API_BASE_VERSION
 
         return self._call("get_replica_context",
-                          self.agent_high_timeout, version=version)
+                          AGENT_HIGH_TIMEOUT, version=version)
 
-    def attach_replica(self, replica_info, slave_config, restart=False):
+    def attach_replica(self, replica_info, slave_config):
         LOG.debug("Attaching replica %s.", replica_info)
         version = self.API_BASE_VERSION
 
-        self._call("attach_replica",
-                   self.agent_high_timeout, version=version,
-                   replica_info=replica_info, slave_config=slave_config,
-                   restart=restart)
+        self._call("attach_replica", AGENT_HIGH_TIMEOUT, version=version,
+                   replica_info=replica_info, slave_config=slave_config)
 
     def make_read_only(self, read_only):
         LOG.debug("Executing make_read_only(%s)", read_only)
         version = self.API_BASE_VERSION
 
-        self._call("make_read_only",
-                   self.agent_high_timeout, version=version,
+        self._call("make_read_only", AGENT_HIGH_TIMEOUT, version=version,
                    read_only=read_only)
 
     def enable_as_master(self, replica_source_config):
         LOG.debug("Executing enable_as_master")
         version = self.API_BASE_VERSION
 
-        self._call("enable_as_master", self.agent_high_timeout,
+        self._call("enable_as_master", AGENT_HIGH_TIMEOUT,
                    version=version,
                    replica_source_config=replica_source_config)
 
     # DEPRECATED: Maintain for API Compatibility
     def get_txn_count(self):
         LOG.debug("Executing get_txn_count.")
         version = self.API_BASE_VERSION
 
         return self._call("get_txn_count",
-                          self.agent_high_timeout, version=version)
+                          AGENT_HIGH_TIMEOUT, version=version)
 
     def get_last_txn(self):
         LOG.debug("Executing get_last_txn.")
         version = self.API_BASE_VERSION
 
         return self._call("get_last_txn",
-                          self.agent_high_timeout, version=version)
+                          AGENT_HIGH_TIMEOUT, version=version)
 
     def get_latest_txn_id(self):
         LOG.debug("Executing get_latest_txn_id.")
         version = self.API_BASE_VERSION
 
         return self._call("get_latest_txn_id",
-                          self.agent_high_timeout, version=version)
+                          AGENT_HIGH_TIMEOUT, version=version)
 
     def wait_for_txn(self, txn):
         LOG.debug("Executing wait_for_txn.")
         version = self.API_BASE_VERSION
 
-        self._call("wait_for_txn",
-                   self.agent_high_timeout, version=version, txn=txn)
+        self._call("wait_for_txn", AGENT_HIGH_TIMEOUT, version=version,
+                   txn=txn)
 
     def cleanup_source_on_replica_detach(self, replica_info):
         LOG.debug("Cleaning up master %s on detach of replica.", self.id)
         version = self.API_BASE_VERSION
 
-        self._call("cleanup_source_on_replica_detach",
-                   self.agent_high_timeout,
+        self._call("cleanup_source_on_replica_detach", AGENT_HIGH_TIMEOUT,
                    version=version, replica_info=replica_info)
 
     def demote_replication_master(self):
         LOG.debug("Demoting instance %s to non-master.", self.id)
         version = self.API_BASE_VERSION
 
-        self._call("demote_replication_master", self.agent_high_timeout,
+        self._call("demote_replication_master", AGENT_HIGH_TIMEOUT,
                    version=version)
 
     def guest_log_list(self):
         LOG.debug("Retrieving guest log list for %s.", self.id)
         version = self.API_BASE_VERSION
 
-        result = self._call("guest_log_list", self.agent_high_timeout,
+        result = self._call("guest_log_list", AGENT_HIGH_TIMEOUT,
                             version=version)
         LOG.debug("guest_log_list returns %s", result)
         return result
 
     def guest_log_action(self, log_name, enable, disable, publish, discard):
         LOG.debug("Processing guest log '%s' for %s.", log_name, self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("guest_log_action", self.agent_high_timeout,
+        return self._call("guest_log_action", AGENT_HIGH_TIMEOUT,
                           version=version, log_name=log_name,
                           enable=enable, disable=disable,
                           publish=publish, discard=discard)
 
     def module_list(self, include_contents):
         LOG.debug("Querying modules on %s (contents: %s).",
                   self.id, include_contents)
         version = self.API_BASE_VERSION
 
-        result = self._call("module_list", self.agent_high_timeout,
+        result = self._call("module_list", AGENT_HIGH_TIMEOUT,
                             version=version,
                             include_contents=include_contents)
         return result
 
     def module_apply(self, modules):
         LOG.debug("Applying modules to %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("module_apply", self.agent_high_timeout,
+        return self._call("module_apply", AGENT_HIGH_TIMEOUT,
                           version=version, modules=modules)
 
     def module_remove(self, module):
         LOG.debug("Removing modules from %s.", self.id)
         version = self.API_BASE_VERSION
 
-        return self._call("module_remove", self.agent_high_timeout,
+        return self._call("module_remove", AGENT_HIGH_TIMEOUT,
                           version=version, module=module)
-
-    def rebuild(self, ds_version, config_contents=None, config_overrides=None):
-        """Make an asynchronous call to rebuild the database service."""
-        LOG.debug("Sending the call to rebuild database service in the guest.")
-        version = self.API_BASE_VERSION
-
-        # Taskmanager is a publisher, guestagent is a consumer. Usually
-        # consumer creates a queue, but in this case we have to make sure
-        # "prepare" doesn't get lost if for some reason guest was delayed and
-        # didn't create a queue on time.
-        self._create_guest_queue()
-
-        self._cast("rebuild", version=version,
-                   ds_version=ds_version, config_contents=config_contents,
-                   config_overrides=config_overrides)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/common/configuration.py` & `trove-8.0.1/trove/guestagent/common/configuration.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,23 +12,20 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import abc
 import os
 import re
-
-from oslo_log import log as logging
+import six
 
 from trove.guestagent.common import guestagent_utils
 from trove.guestagent.common import operating_system
 from trove.guestagent.common.operating_system import FileMode
 
-LOG = logging.getLogger(__name__)
-
 
 class ConfigurationManager(object):
     """
     ConfigurationManager is responsible for management of
     datastore configuration.
     Its base functionality includes reading and writing configuration files.
     It is responsible for validating user inputs and requests.
@@ -75,19 +72,14 @@
                                     compatible with very much any datastore.
                                     It is recommended each datastore defines
                                     its strategy explicitly to avoid upgrade
                                     compatibility issues in case the default
                                     implementation changes in the future.
         :type override_strategy     ConfigurationOverrideStrategy
         """
-        base_config_dir = os.path.dirname(base_config_path)
-        operating_system.ensure_directory(
-            base_config_dir, user=owner, group=group, force=True, as_root=True
-        )
-
         self._base_config_path = base_config_path
         self._owner = owner
         self._group = group
         self._codec = codec
         self._requires_root = requires_root
         self._value_cache = None
 
@@ -100,62 +92,52 @@
             self._override_strategy = OneFileOverrideStrategy(revision_dir)
         else:
             self._override_strategy = override_strategy
 
         self._override_strategy.configure(
             base_config_path, owner, group, codec, requires_root)
 
-    def get_value(self, key, section=None, default=None):
+    def get_value(self, key, default=None):
         """Return the current value at a given key or 'default'.
         """
         if self._value_cache is None:
             self.refresh_cache()
 
-        if section:
-            return self._value_cache.get(section, {}).get(key, default)
-
         return self._value_cache.get(key, default)
 
     def parse_configuration(self):
         """Read contents of the configuration file (applying overrides if any)
         and parse it into a dict.
 
         :returns:        Configuration file as a Python dict.
         """
 
-        try:
-            base_options = operating_system.read_file(
-                self._base_config_path, codec=self._codec,
-                as_root=self._requires_root)
-        except Exception:
-            LOG.warning('File %s not found', self._base_config_path)
-            return None
+        base_options = operating_system.read_file(
+            self._base_config_path, codec=self._codec,
+            as_root=self._requires_root)
 
         updates = self._override_strategy.parse_updates()
         guestagent_utils.update_dict(updates, base_options)
 
         return base_options
 
-    def reset_configuration(self, options, remove_overrides=False):
+    def save_configuration(self, options):
         """Write given contents to the base configuration file.
+        Remove all existing overrides (both system and user).
 
-        Remove all existing overrides (both system and user) as required.
-
-        :param options: Contents of the configuration file (string or dict).
-        :param remove_overrides: Remove the overrides or not.
+        :param contents        Contents of the configuration file.
+        :type contents         string or dict
         """
         if isinstance(options, dict):
             # Serialize a dict of options for writing.
-            self.reset_configuration(self._codec.serialize(options),
-                                     remove_overrides=remove_overrides)
+            self.save_configuration(self._codec.serialize(options))
         else:
-            if remove_overrides:
-                self._override_strategy.remove(self.USER_GROUP)
-                self._override_strategy.remove(self.SYSTEM_PRE_USER_GROUP)
-                self._override_strategy.remove(self.SYSTEM_POST_USER_GROUP)
+            self._override_strategy.remove(self.USER_GROUP)
+            self._override_strategy.remove(self.SYSTEM_PRE_USER_GROUP)
+            self._override_strategy.remove(self.SYSTEM_POST_USER_GROUP)
 
             operating_system.write_file(
                 self._base_config_path, options, as_root=self._requires_root)
             operating_system.chown(
                 self._base_config_path, self._owner, self._group,
                 as_root=self._requires_root)
             operating_system.chmod(
@@ -225,15 +207,16 @@
         self._override_strategy.remove(group_name, change_id)
         self.refresh_cache()
 
     def refresh_cache(self):
         self._value_cache = self.parse_configuration()
 
 
-class ConfigurationOverrideStrategy(object, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class ConfigurationOverrideStrategy(object):
     """ConfigurationOverrideStrategy handles configuration files.
     The strategy provides functionality to enumerate, apply and remove
     configuration overrides.
     """
 
     @abc.abstractmethod
     def configure(self, *args, **kwargs):
@@ -308,15 +291,15 @@
 
     The name format of override files is: '<set prefix>-<n>-<group name>.<ext>'
     where 'set prefix' is to used to order user/system sets,
     'n' is an index used to keep track of the order in which overrides
     within their set got applied.
     """
 
-    FILE_NAME_PATTERN = r'%s-([0-9]+)-%s\.%s$'
+    FILE_NAME_PATTERN = '%s-([0-9]+)-%s\.%s$'
 
     def __init__(self, revision_dir, revision_ext):
         """
         :param revision_dir  Path to the directory for import files.
         :type revision_dir   string
 
         :param revision_ext  Extension of revision files.
@@ -348,16 +331,14 @@
         """
         self._base_config_path = base_config_path
         self._owner = owner
         self._group = group
         self._codec = codec
         self._requires_root = requires_root
 
-        self._initialize_import_directory()
-
     def exists(self, group_name, change_id):
         return self._find_revision_file(group_name, change_id) is not None
 
     def apply(self, group_name, change_id, options):
         self._initialize_import_directory()
         revision_file = self._find_revision_file(group_name, change_id)
         if revision_file is None:
@@ -383,15 +364,15 @@
         operating_system.chmod(
             revision_file, FileMode.ADD_READ_ALL, as_root=self._requires_root)
 
     def _initialize_import_directory(self):
         """Lazy-initialize the directory for imported revision files.
         """
         if not os.path.exists(self._revision_dir):
-            operating_system.ensure_directory(
+            operating_system.create_directory(
                 self._revision_dir, user=self._owner, group=self._group,
                 force=True, as_root=self._requires_root)
 
     def remove(self, group_name, change_id=None):
         removed = set()
         if change_id:
             # Remove a given file.
@@ -416,15 +397,14 @@
     def parse_updates(self):
         parsed_options = {}
         for path in self._collect_revision_files():
             options = operating_system.read_file(path, codec=self._codec,
                                                  as_root=self._requires_root)
             guestagent_utils.update_dict(options, parsed_options)
 
-        LOG.debug(f"Parsed overrides options: {parsed_options}")
         return parsed_options
 
     @property
     def has_revisions(self):
         """Return True if there currently are any revision files.
         """
         return (operating_system.exists(
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/common/guestagent_utils.py` & `trove-8.0.1/trove/guestagent/common/guestagent_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,28 +9,21 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from collections import abc
-import json
+import collections
 import os
 import re
 
-from pyroute2 import IPRoute
+import six
 
-from trove.common import cfg
-from trove.common import constants
 from trove.common import pagination
-from trove.common import utils
-from trove.guestagent.common import operating_system
-
-CONF = cfg.CONF
 
 
 def update_dict(updates, target):
     """Recursively update a target dictionary with given updates.
 
     Updates are provided as a dictionary of key-value pairs
     where a value can also be a nested dictionary in which case
@@ -46,15 +39,15 @@
     if isinstance(target, list):
         for index, item in enumerate(target):
             target[index] = update_dict(updates, item)
         return target
 
     if updates is not None:
         for k, v in updates.items():
-            if isinstance(v, abc.Mapping):
+            if isinstance(v, collections.Mapping):
                 target[k] = update_dict(v, target.get(k, {}))
             else:
                 target[k] = updates[k]
 
     return target
 
 
@@ -84,15 +77,15 @@
     {'ns1': {'ns2a': {'ns3a': True, 'ns3b': False}, 'ns2b': 10}}
 
     would be flattened to:
     {'ns1.ns2a.ns3a': True, 'ns1.ns2a.ns3b': False, 'ns1.ns2b': 10}
     """
     def flatten(target, keys, namespace_sep):
         flattened = {}
-        if isinstance(target, abc.Mapping):
+        if isinstance(target, collections.Mapping):
             for k, v in target.items():
                 flattened.update(
                     flatten(v, keys + [k], namespace_sep))
         else:
             ns = namespace_sep.join(keys)
             flattened[ns] = target
 
@@ -110,16 +103,16 @@
     file_name = os.extsep.join([base_name] + list(extensions))
     return os.path.expanduser(os.path.join(base_dir, file_name))
 
 
 def to_bytes(value):
     """Convert numbers with a byte suffix to bytes.
     """
-    if isinstance(value, str):
-        pattern = re.compile(r'^(\d+)([K,M,G]{1})$')
+    if isinstance(value, six.string_types):
+        pattern = re.compile('^(\d+)([K,M,G]{1})$')
         match = pattern.match(value)
         if match:
             value = match.group(1)
             suffix = match.group(2)
             factor = {
                 'K': 1024,
                 'M': 1024 ** 2,
@@ -143,56 +136,7 @@
     """
     Paginate (by name) and serialize a given object list.
     :returns:           A serialized and paginated version of a given list.
     """
     page, next_name = paginate_list(li, limit=limit, marker=marker,
                                     include_marker=include_marker)
     return [item.serialize() for item in page], next_name
-
-
-def get_filesystem_volume_stats(fs_path):
-    try:
-        stats = os.statvfs(fs_path)
-    except OSError:
-        raise RuntimeError("Filesystem not found (%s)" % fs_path)
-
-    total = stats.f_blocks * stats.f_bsize
-    free = stats.f_bfree * stats.f_bsize
-    # return the size in GB
-    used_gb = utils.to_gb(total - free)
-    total_gb = utils.to_gb(total)
-
-    output = {
-        'block_size': stats.f_bsize,
-        'total_blocks': stats.f_blocks,
-        'free_blocks': stats.f_bfree,
-        'total': total_gb,
-        'free': free,
-        'used': used_gb
-    }
-    return output
-
-
-def get_conf_dir():
-    """Get the config directory for the database related settings.
-
-    For now, the files inside the config dir are mainly for instance rebuild.
-    """
-    mount_point = CONF.get(CONF.datastore_manager).mount_point
-    conf_dir = os.path.join(mount_point, 'conf.d')
-    if not operating_system.exists(conf_dir, is_directory=True, as_root=True):
-        operating_system.ensure_directory(conf_dir, as_root=True)
-
-    return conf_dir
-
-
-def disable_user_defined_port():
-    with open(constants.ETH1_CONFIG_PATH) as fd:
-        eth1_config = json.load(fd)
-    ipr = IPRoute()
-    ifaces = ipr.get_links(address=eth1_config.get("mac_address"))
-    if not ifaces:
-        return
-    ifname = ifaces[0].get_attr('IFLA_IFNAME')
-    operating_system.execute_shell_cmd(f"ip link set {ifname} down", [],
-                                       shell=True,
-                                       as_root=True)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/common/operating_system.py` & `trove-8.0.1/trove/guestagent/common/operating_system.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,24 +9,22 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from functools import reduce
 import inspect
 import operator
 import os
-from pathlib import Path
-import pwd
 import re
 import stat
 import tempfile
 
+from functools import reduce
 from oslo_concurrency.processutils import UnknownArgumentError
 
 from trove.common import exception
 from trove.common.i18n import _
 from trove.common.stream_codecs import IdentityCodec
 from trove.common import utils
 
@@ -54,26 +52,21 @@
 
     :returns:               A dictionary of key-value pairs.
 
     :raises:                :class:`UnprocessableEntity` if file doesn't exist.
     :raises:                :class:`UnprocessableEntity` if codec not given.
     """
     if path and exists(path, is_directory=False, as_root=as_root):
-        if decode:
-            open_flag = 'r'
-            convert_func = codec.deserialize
-        else:
-            open_flag = 'rb'
-            convert_func = codec.serialize
-
         if as_root:
-            return _read_file_as_root(path, open_flag, convert_func)
+            return _read_file_as_root(path, codec, decode=decode)
 
-        with open(path, open_flag) as fp:
-            return convert_func(fp.read())
+        with open(path, 'r') as fp:
+            if decode:
+                return codec.deserialize(fp.read())
+            return codec.serialize(fp.read())
 
     raise exception.UnprocessableEntity(_("File does not exist: %s") % path)
 
 
 def exists(path, is_directory=False, as_root=False):
     """Check a given path exists.
 
@@ -100,55 +93,36 @@
             cmd, shell=True, check_exit_code=False,
             run_as_root=True, root_helper='sudo')
         found = bool(int(stdout))
 
     return found
 
 
-def find_executable(executable, path=None):
-    """Finds a location of an executable in the locations listed in 'path'
-
-    :param executable          File to search.
-    :type executable           string
-
-    :param path                Lookup directories separated by a path
-                               separartor.
-    :type path                 string
-    """
-    if path is None:
-        path = os.environ.get('PATH', os.defpath)
-    dirs = path.split(os.pathsep)
-    for directory in dirs:
-        exec_path = os.path.join(directory, executable)
-        if os.path.isfile(exec_path) and os.access(exec_path, os.X_OK):
-            return exec_path
-    return None
-
-
-def _read_file_as_root(path, open_flag, convert_func):
+def _read_file_as_root(path, codec, decode=True):
     """Read a file as root.
 
     :param path                Path to the written file.
     :type path                 string
 
-    :param open_flag:          The flag for opening a file
-    :type open_flag:           string
+    :param codec:              A codec used to transform the data.
+    :type codec:               StreamCodec
 
-    :param convert_func:       The function for converting data.
-    :type convert_func:        callable
+    :param decode:             Should the codec decode the data.
+    :type decode:              boolean
     """
-    with tempfile.NamedTemporaryFile(open_flag) as fp:
+    with tempfile.NamedTemporaryFile() as fp:
         copy(path, fp.name, force=True, dereference=True, as_root=True)
         chmod(fp.name, FileMode.ADD_READ_ALL(), as_root=True)
-        return convert_func(fp.read())
+        if decode:
+            return codec.deserialize(fp.read())
+        return codec.serialize(fp.read())
 
 
 def write_file(path, data, codec=IdentityCodec(), as_root=False, encode=True):
     """Write data into file using a given codec.
-
     Overwrite any existing contents.
     The written file can be read back into its original
     form by 'read_file'.
 
     :param path                Path to the written config file.
     :type path                 string
 
@@ -163,50 +137,49 @@
 
     :param encode:             Should the codec encode the data.
     :type encode:              boolean
 
     :raises:                   :class:`UnprocessableEntity` if path not given.
     """
     if path:
-        if encode:
-            open_flag = 'w'
-            convert_func = codec.serialize
-        else:
-            open_flag = 'wb'
-            convert_func = codec.deserialize
-
         if as_root:
-            _write_file_as_root(path, data, open_flag, convert_func)
+            _write_file_as_root(path, data, codec, encode=encode)
         else:
-            with open(path, open_flag) as fp:
-                fp.write(convert_func(data))
+            with open(path, 'w') as fp:
+                if encode:
+                    fp.write(codec.serialize(data))
+                else:
+                    fp.write(codec.deserialize(data))
                 fp.flush()
     else:
         raise exception.UnprocessableEntity(_("Invalid path: %s") % path)
 
 
-def _write_file_as_root(path, data, open_flag, convert_func):
+def _write_file_as_root(path, data, codec, encode=True):
     """Write a file as root. Overwrite any existing contents.
 
     :param path                Path to the written file.
     :type path                 string
 
     :param data:               An object representing the file contents.
     :type data:                StreamCodec
 
-    :param open_flag:          The flag for opening a file
-    :type open_flag:           string
+    :param codec:              A codec used to transform the data.
+    :type codec:               StreamCodec
 
-    :param convert_func:       The function for converting data.
-    :type convert_func:        callable
+    :param encode:             Should the codec encode the data.
+    :type encode:              boolean
     """
     # The files gets removed automatically once the managing object goes
     # out of scope.
-    with tempfile.NamedTemporaryFile(open_flag, delete=False) as fp:
-        fp.write(convert_func(data))
+    with tempfile.NamedTemporaryFile('w', delete=False) as fp:
+        if encode:
+            fp.write(codec.serialize(data))
+        else:
+            fp.write(codec.deserialize(data))
         fp.flush()
         fp.close()  # Release the resource before proceeding.
         copy(fp.name, path, force=True, as_root=True)
 
 
 class FileMode(object):
     """
@@ -477,54 +450,19 @@
                 result['cmd_enable'] = "sudo systemctl enable %s" % service
                 result['cmd_disable'] = "sudo systemctl disable %s" % service
             break
 
     return result
 
 
-def execute_shell_cmd(cmd, options, *args, **kwargs):
-    """Execute a given shell command passing it
-    given options (flags) and arguments.
-
-    Takes optional keyword arguments:
-    :param as_root:        Execute as root.
-    :type as_root:         boolean
-
-    :param timeout:        Number of seconds if specified,
-                           default if not.
-                           There is no timeout if set to None.
-    :type timeout:         integer
-
-    :raises:               class:`UnknownArgumentError` if passed unknown args.
-    """
-
-    exec_args = {}
-    if kwargs.pop('as_root', False):
-        exec_args['run_as_root'] = True
-        exec_args['root_helper'] = 'sudo'
-
-    if 'timeout' in kwargs:
-        exec_args['timeout'] = kwargs.pop('timeout')
-
-    exec_args['shell'] = kwargs.pop('shell', False)
-
-    if kwargs:
-        raise UnknownArgumentError(_("Got unknown keyword args: %r") % kwargs)
-
-    cmd_flags = _build_command_options(options)
-    cmd_args = cmd_flags + list(args)
-    stdout, stderr = utils.execute_with_timeout(cmd, *cmd_args, **exec_args)
-    return stdout
-
-
-def ensure_directory(dir_path, user=None, group=None, force=True, **kwargs):
+def create_directory(dir_path, user=None, group=None, force=True, **kwargs):
     """Create a given directory and update its ownership
     (recursively) to the given user and group if any.
 
-    seealso:: execute_shell_cmd for valid optional keyword arguments.
+    seealso:: _execute_shell_cmd for valid optional keyword arguments.
 
     :param dir_path:        Path to the created directory.
     :type dir_path:         string
 
     :param user:            Owner.
     :type user:             string
 
@@ -546,15 +484,15 @@
         raise exception.UnprocessableEntity(
             _("Cannot create a blank directory."))
 
 
 def chown(path, user, group, recursive=True, force=False, **kwargs):
     """Changes the owner and group of a given file.
 
-    seealso:: execute_shell_cmd for valid optional keyword arguments.
+    seealso:: _execute_shell_cmd for valid optional keyword arguments.
 
     :param path:         Path to the modified file.
     :type path:          string
 
     :param user:         Owner.
     :type user:          string
 
@@ -576,15 +514,15 @@
             _("Cannot change ownership of a blank file or directory."))
     if not user and not group:
         raise exception.UnprocessableEntity(
             _("Please specify owner or group, or both."))
 
     owner_group_modifier = _build_user_group_pair(user, group)
     options = (('f', force), ('R', recursive))
-    execute_shell_cmd('chown', options, owner_group_modifier, path, **kwargs)
+    _execute_shell_cmd('chown', options, owner_group_modifier, path, **kwargs)
 
 
 def _build_user_group_pair(user, group):
     return "%s:%s" % tuple((v if v else '') for v in (user, group))
 
 
 def _create_directory(dir_path, force=True, **kwargs):
@@ -592,26 +530,25 @@
 
     :param dir_path:        Path to the created directory.
     :type dir_path:         string
 
     :param force:           No error if existing, make parent directories
                             as needed.
     :type force:            boolean
-    :param as_root: Run as root user, default: False.
     """
 
     options = (('p', force),)
-    execute_shell_cmd('mkdir', options, dir_path, **kwargs)
+    _execute_shell_cmd('mkdir', options, dir_path, **kwargs)
 
 
 def chmod(path, mode, recursive=True, force=False, **kwargs):
     """Changes the mode of a given file.
 
     :seealso: Modes for more information on the representation of modes.
-    :seealso: execute_shell_cmd for valid optional keyword arguments.
+    :seealso: _execute_shell_cmd for valid optional keyword arguments.
 
     :param path:            Path to the modified file.
     :type path:             string
 
     :param mode:            File permissions (modes).
                             The modes will be applied in the following order:
                             reset (=), add (+), remove (-)
@@ -626,25 +563,25 @@
     :raises:                :class:`UnprocessableEntity` if path not given.
     :raises:                :class:`UnprocessableEntity` if no mode given.
     """
 
     if path:
         options = (('f', force), ('R', recursive))
         shell_modes = _build_shell_chmod_mode(mode)
-        execute_shell_cmd('chmod', options, shell_modes, path, **kwargs)
+        _execute_shell_cmd('chmod', options, shell_modes, path, **kwargs)
     else:
         raise exception.UnprocessableEntity(
             _("Cannot change mode of a blank file."))
 
 
 def change_user_group(user, group, append=True, add_group=True, **kwargs):
     """Adds a user to groups by using the usermod linux command with -a and
     -G options.
 
-    seealso:: execute_shell_cmd for valid optional keyword arguments.
+    seealso:: _execute_shell_cmd for valid optional keyword arguments.
 
     :param user:            Username.
     :type user:             string
 
     :param group:           Group names.
     :type group:            comma separated string
 
@@ -665,15 +602,15 @@
 
     if not user:
         raise exception.UnprocessableEntity(_("Missing user."))
     elif not group:
         raise exception.UnprocessableEntity(_("Missing group."))
 
     options = (('a', append), ('G', add_group))
-    execute_shell_cmd('usermod', options, group, user, **kwargs)
+    _execute_shell_cmd('usermod', options, group, user, **kwargs)
 
 
 def _build_shell_chmod_mode(mode):
     """
     Build a shell representation of given mode.
 
     :seealso: Modes for more information on the representation of modes.
@@ -701,15 +638,15 @@
     else:
         raise exception.UnprocessableEntity(_("No file mode specified."))
 
 
 def remove(path, force=False, recursive=True, **kwargs):
     """Remove a given file or directory.
 
-    :seealso: execute_shell_cmd for valid optional keyword arguments.
+    :seealso: _execute_shell_cmd for valid optional keyword arguments.
 
     :param path:            Path to the removed file.
     :type path:             string
 
     :param force:           Ignore nonexistent files.
     :type force:            boolean
 
@@ -717,25 +654,25 @@
     :type recursive:        boolean
 
     :raises:                :class:`UnprocessableEntity` if path not given.
     """
 
     if path:
         options = (('f', force), ('R', recursive))
-        execute_shell_cmd('rm', options, path, **kwargs)
+        _execute_shell_cmd('rm', options, path, **kwargs)
     else:
         raise exception.UnprocessableEntity(_("Cannot remove a blank file."))
 
 
 def move(source, destination, force=False, **kwargs):
     """Move a given file or directory to a new location.
     Move attempts to preserve the original ownership, permissions and
     timestamps.
 
-    :seealso: execute_shell_cmd for valid optional keyword arguments.
+    :seealso: _execute_shell_cmd for valid optional keyword arguments.
 
     :param source:          Path to the source location.
     :type source:           string
 
     :param destination:     Path to the destination location.
     :type destination:      string
 
@@ -748,25 +685,24 @@
 
     if not source:
         raise exception.UnprocessableEntity(_("Missing source path."))
     elif not destination:
         raise exception.UnprocessableEntity(_("Missing destination path."))
 
     options = (('f', force),)
-    execute_shell_cmd('mv', options, source, destination, **kwargs)
+    _execute_shell_cmd('mv', options, source, destination, **kwargs)
 
 
 def copy(source, destination, force=False, preserve=False, recursive=True,
          dereference=False, **kwargs):
     """Copy a given file or directory to another location.
-
     Copy does NOT attempt to preserve ownership, permissions and timestamps
     unless the 'preserve' option is enabled.
 
-    :seealso: execute_shell_cmd for valid optional keyword arguments.
+    :seealso: _execute_shell_cmd for valid optional keyword arguments.
 
     :param source:          Path to the source location.
     :type source:           string
 
     :param destination:     Path to the destination location.
     :type destination:      string
 
@@ -790,15 +726,15 @@
     if not source:
         raise exception.UnprocessableEntity(_("Missing source path."))
     elif not destination:
         raise exception.UnprocessableEntity(_("Missing destination path."))
 
     options = (('f', force), ('p', preserve), ('R', recursive),
                ('L', dereference))
-    execute_shell_cmd('cp', options, source, destination, **kwargs)
+    _execute_shell_cmd('cp', options, source, destination, **kwargs)
 
 
 def get_bytes_free_on_fs(path):
     """
     Returns the number of bytes free for the filesystem that path is on
     """
     v = os.statvfs(path)
@@ -827,36 +763,69 @@
         if not recursive:
             cmd_args.extend(['-maxdepth', '0'])
         if not include_dirs:
             cmd_args.extend(['-type', 'f'])
         if pattern:
             cmd_args.extend(['-regextype', 'posix-extended',
                              '-regex', os.path.join('.*', pattern) + '$'])
-        files = execute_shell_cmd('find', [], *cmd_args, as_root=True)
+        files = _execute_shell_cmd('find', [], *cmd_args, as_root=True)
         return {fp for fp in files.splitlines()}
 
     return {os.path.abspath(os.path.join(root, name))
             for (root, dirs, files) in os.walk(root_dir, topdown=True)
             if recursive or (root == root_dir)
             for name in (files + (dirs if include_dirs else []))
             if not pattern or re.match(pattern, name)}
 
 
+def _execute_shell_cmd(cmd, options, *args, **kwargs):
+    """Execute a given shell command passing it
+    given options (flags) and arguments.
+
+    Takes optional keyword arguments:
+    :param as_root:        Execute as root.
+    :type as_root:         boolean
+
+    :param timeout:        Number of seconds if specified,
+                           default if not.
+                           There is no timeout if set to None.
+    :type timeout:         integer
+
+    :raises:               class:`UnknownArgumentError` if passed unknown args.
+    """
+
+    exec_args = {}
+    if kwargs.pop('as_root', False):
+        exec_args['run_as_root'] = True
+        exec_args['root_helper'] = 'sudo'
+
+    if 'timeout' in kwargs:
+        exec_args['timeout'] = kwargs.pop('timeout')
+
+    if kwargs:
+        raise UnknownArgumentError(_("Got unknown keyword args: %r") % kwargs)
+
+    cmd_flags = _build_command_options(options)
+    cmd_args = cmd_flags + list(args)
+    stdout, stderr = utils.execute_with_timeout(cmd, *cmd_args, **exec_args)
+    return stdout
+
+
 def _build_command_options(options):
     """Build a list of flags from given pairs (option, is_enabled).
     Each option is prefixed with a single '-'.
     Include only options for which is_enabled=True.
     """
 
     return ['-' + item[0] for item in options if item[1]]
 
 
 def get_device(path, as_root=False):
     """Get the device that a given path exists on."""
-    stdout = execute_shell_cmd('df', [], path, as_root=as_root)
+    stdout = _execute_shell_cmd('df', [], path, as_root=as_root)
     return stdout.splitlines()[1].split()[0]
 
 
 def is_mount(path):
     """Check if the given directory path is a mountpoint. Try the standard
     ismount first. This fails if the path is not accessible though, so resort
     to checking as the root user (which is slower).
@@ -864,60 +833,7 @@
     if os.access(path, os.R_OK):
         return os.path.ismount(path)
     if not exists(path, is_directory=True, as_root=True):
         return False
     directory_dev = get_device(path, as_root=True)
     parent_dev = get_device(os.path.join(path, '..'), as_root=True)
     return directory_dev != parent_dev
-
-
-def get_current_user():
-    """Returns name of the current OS user"""
-    return pwd.getpwuid(os.getuid())[0]
-
-
-def create_user(user_name, user_id, group_name=None, group_id=None):
-    group_name = group_name or user_name
-    group_id = group_id or user_id
-
-    try:
-        execute_shell_cmd('groupadd', [], '--gid', group_id, group_name,
-                          as_root=True)
-    except exception.ProcessExecutionError as err:
-        if 'already exists' not in err.stderr:
-            raise exception.UnprocessableEntity(
-                'Failed to add group %s, error: %s' % (group_name, err.stderr)
-            )
-
-    try:
-        execute_shell_cmd('useradd', [], '--uid', user_id, '--gid', group_id,
-                          '-M', user_name, as_root=True)
-    except exception.ProcessExecutionError as err:
-        if 'already exists' not in err.stderr:
-            raise exception.UnprocessableEntity(
-                'Failed to add user %s, error: %s' % (user_name, err.stderr)
-            )
-
-
-def remove_dir_contents(folder):
-    """Remove all the files and sub-directories but keep the folder.
-
-    Use shell=True here because shell=False doesn't support '*'
-    """
-    path = os.path.join(folder, '*')
-    execute_shell_cmd(f'rm -rf {path}', [], shell=True, as_root=True)
-
-
-def get_dir_size(path):
-    """Get the directory size in bytes."""
-    root_directory = Path(path)
-    return sum(f.stat().st_size for f in root_directory.glob('**/*')
-               if f.is_file())
-
-
-def get_filesystem_size(path):
-    """Get size(bytes) of a mounted filesystem the given path locates.
-
-    path is the pathname of any file within the mounted filesystem.
-    """
-    ret = os.statvfs(path)
-    return ret.f_blocks * ret.f_frsize
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/common/sql_query.py` & `trove-8.0.1/trove/guestagent/common/sql_query.py`

 * *Files 18% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 """
 
 Intermediary class for building SQL queries for use by the guest agent.
 Do not hard-code strings into the guest agent; use this module to build
 them for you.
 
 """
-import semantic_version
 
 
 class Query(object):
 
     def __init__(self, columns=None, tables=None, where=None, order=None,
                  group=None, limit=None):
         self.columns = columns or []
@@ -157,14 +156,22 @@
         return "*"
 
     @property
     def _user(self):
         return self.user or ""
 
     @property
+    def _identity(self):
+        if self.clear:
+            return "IDENTIFIED BY '%s'" % self.clear
+        if self.hashed:
+            return "IDENTIFIED BY PASSWORD '%s'" % self.hashed
+        return ""
+
+    @property
     def _host(self):
         return self.host or "%"
 
     @property
     def _user_host(self):
         return "`%s`@`%s`" % (self._user, self._host)
 
@@ -176,15 +183,20 @@
     @property
     def _where(self):
         # Database and table to which the user is granted permissions.
         return "ON %s.%s" % (self._database, self._table)
 
     @property
     def _whom(self):
-        return f"TO {self._user_host}"
+        # User and host to be granted permission. Optionally, password, too.
+        whom = [("TO %s" % self._user_host),
+                self._identity,
+                ]
+        whom = [w for w in whom if w]
+        return " ".join(whom)
 
     @property
     def _with(self):
         clauses = []
 
         if self.grant_option:
             clauses.append("GRANT OPTION")
@@ -240,15 +252,20 @@
     def _what(self):
         # Permissions to be revoked from the user.
         return "REVOKE %s" % self._permissions
 
     @property
     def _whom(self):
         # User and host from whom to revoke permission.
-        return f"FROM {self._user_host}"
+        # Optionally, password, too.
+        whom = [("FROM %s" % self._user_host),
+                self._identity,
+                ]
+        whom = [w for w in whom if w]
+        return " ".join(whom)
 
 
 class CreateDatabase(object):
 
     def __init__(self, database, charset=None, collate=None):
         self.database = database
         self.charset = charset
@@ -347,40 +364,29 @@
                       'new_name': self.new_user or self.user,
                       'new_host': self.new_host or self.host}
         return ("RENAME USER '%(old_name)s'@'%(old_host)s' TO "
                 "'%(new_name)s'@'%(new_host)s';" % properties)
 
 
 class SetPassword(object):
-    def __init__(self, user, host=None, new_password=None, ds=None,
-                 ds_version=None):
+
+    def __init__(self, user, host=None, new_password=None):
         self.user = user
         self.host = host or '%'
         self.new_password = new_password or ''
-        self.ds = ds or 'mysql'
-        self.ds_version = ds_version or '5.7'
 
     def __repr__(self):
         return str(self)
 
     def __str__(self):
-        if self.ds == 'mysql':
-            cur_version = semantic_version.Version.coerce(self.ds_version)
-            mysql_575 = semantic_version.Version('5.7.5')
-            if cur_version <= mysql_575:
-                return (f"SET PASSWORD FOR '{self.user}'@'{self.host}' = "
-                        f"PASSWORD('{self.new_password}');")
-
-            return (f"ALTER USER '{self.user}'@'{self.host}' "
-                    f"IDENTIFIED WITH mysql_native_password "
-                    f"BY '{self.new_password}';")
-        elif self.ds == 'mariadb':
-            return (f"ALTER USER '{self.user}'@'{self.host}' IDENTIFIED VIA "
-                    f"mysql_native_password USING "
-                    f"PASSWORD('{self.new_password}');")
+        properties = {'user_name': self.user,
+                      'user_host': self.host,
+                      'new_password': self.new_password}
+        return ("SET PASSWORD FOR '%(user_name)s'@'%(user_host)s' = "
+                "PASSWORD('%(new_password)s');" % properties)
 
 
 class DropUser(object):
 
     def __init__(self, user, host='%'):
         self.user = user
         self.host = host
@@ -409,15 +415,15 @@
         elif self.value is None:
             return "SET GLOBAL %s" % (self.key)
         elif isinstance(self.value, str):
             return "SET GLOBAL %s='%s'" % (self.key, self.value)
         else:
             return "SET GLOBAL %s=%s" % (self.key, self.value)
 
-
 # Miscellaneous queries that need no parameters.
+
 FLUSH = "FLUSH PRIVILEGES;"
 ROOT_ENABLED = ("SELECT User FROM mysql.user "
                 "WHERE User = 'root' AND Host != 'localhost';")
 REMOVE_ANON = "DELETE FROM mysql.user WHERE User = '';"
 REMOVE_ROOT = ("DELETE FROM mysql.user "
                "WHERE User = 'root' AND Host != 'localhost';")
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/manager.py` & `trove-8.0.1/trove/guestagent/datastore/manager.py`

 * *Files 12% similar despite different names*

```diff
@@ -13,33 +13,34 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import abc
 import operator
 
-import docker
 from oslo_config import cfg as oslo_cfg
 from oslo_log import log as logging
 from oslo_service import periodic_task
+from oslo_utils import encodeutils
 
 from trove.common import cfg
 from trove.common import exception
 from trove.common.i18n import _
+from trove.common import instance
 from trove.common.notification import EndNotification
 from trove.guestagent.common import guestagent_utils
 from trove.guestagent.common import operating_system
 from trove.guestagent.common.operating_system import FileMode
 from trove.guestagent import dbaas
 from trove.guestagent import guest_log
 from trove.guestagent.module import driver_manager
 from trove.guestagent.module import module_manager
 from trove.guestagent.strategies import replication as repl_strategy
 from trove.guestagent import volume
-from trove.instance import service_status
+
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
 
 class Manager(periodic_task.PeriodicTasks):
     """This is the base class for all datastore managers.  Over time, common
@@ -60,24 +61,14 @@
     GUEST_LOG_DEFS_GUEST_LABEL = 'guest'
     GUEST_LOG_DEFS_GENERAL_LABEL = 'general'
     GUEST_LOG_DEFS_ERROR_LABEL = 'error'
     GUEST_LOG_DEFS_SLOW_QUERY_LABEL = 'slow_query'
 
     MODULE_APPLY_TO_ALL = module_manager.ModuleManager.MODULE_APPLY_TO_ALL
 
-    _docker_client = None
-
-    @property
-    def docker_client(self):
-        if self._docker_client:
-            return self._docker_client
-
-        self._docker_client = docker.from_env()
-        return self._docker_client
-
     def __init__(self, manager_name):
         super(Manager, self).__init__(CONF)
 
         # Manager properties
         self.__manager_name = manager_name
         self.__manager = None
         self.__prepare_error = False
@@ -87,30 +78,14 @@
         self._guest_log_loaded_context = None
         self._guest_log_cache = None
         self._guest_log_defs = None
 
         # Module
         self.module_driver_manager = driver_manager.ModuleDriverManager()
 
-        # Drivers should implement
-        self.adm = None
-        self.app = None
-        self.status = None
-
-        if CONF.guest_agent.container_registry:
-            try:
-                self.docker_client.login(
-                    CONF.guest_agent.container_registry_username,
-                    CONF.guest_agent.container_registry_password,
-                    email="",
-                    registry=CONF.guest_agent.container_registry)
-            except Exception as exc:
-                raise exception.GuestError(f"Failed to login the container "
-                                           f"registry, error: {str(exc)}")
-
     @property
     def manager_name(self):
         """This returns the passed-in name of the manager."""
         return self.__manager_name
 
     @property
     def manager(self):
@@ -124,196 +99,283 @@
         return self.__prepare_error
 
     @prepare_error.setter
     def prepare_error(self, prepare_error):
         self.__prepare_error = prepare_error
 
     @property
-    def configuration_manager(self):
-        """If the datastore supports the new-style configuration manager,
-        it should override this to return it.
-        """
-        return None
-
-    @property
     def replication(self):
         """If the datastore supports replication, return an instance of
         the strategy.
         """
         try:
             return repl_strategy.get_instance(self.manager)
         except Exception as ex:
-            LOG.warning("Cannot get replication instance for '%(manager)s': "
-                        "%(msg)s", {'manager': self.manager, 'msg': str(ex)})
+            LOG.debug("Cannot get replication instance for '%(manager)s': "
+                      "%(msg)s", {'manager': self.manager, 'msg': ex.message})
 
         return None
 
     @property
     def replication_strategy(self):
         """If the datastore supports replication, return the strategy."""
         try:
             return repl_strategy.get_strategy(self.manager)
         except Exception as ex:
             LOG.debug("Cannot get replication strategy for '%(manager)s': "
-                      "%(msg)s", {'manager': self.manager, 'msg': str(ex)})
+                      "%(msg)s", {'manager': self.manager, 'msg': ex.message})
+
+        return None
 
+    @abc.abstractproperty
+    def status(self):
+        """This should return an instance of a status class that has been
+        inherited from datastore.service.BaseDbStatus.  Each datastore
+        must implement this property.
+        """
         return None
 
     @property
+    def configuration_manager(self):
+        """If the datastore supports the new-style configuration manager,
+        it should override this to return it.
+        """
+        return None
+
+    @property
+    def datastore_log_defs(self):
+        """Any datastore-specific log files should be overridden in this dict
+        by the corresponding Manager class.
+
+        Format of a dict entry:
+
+        'name_of_log': {self.GUEST_LOG_TYPE_LABEL:
+                            Specified by the Enum in guest_log.LogType,
+                        self.GUEST_LOG_USER_LABEL:
+                            User that owns the file,
+                        self.GUEST_LOG_FILE_LABEL:
+                            Path on filesystem where the log resides,
+                        self.GUEST_LOG_SECTION_LABEL:
+                            Section where to put config (if ini style)
+                        self.GUEST_LOG_ENABLE_LABEL: {
+                            Dict of config_group settings to enable log},
+                        self.GUEST_LOG_DISABLE_LABEL: {
+                            Dict of config_group settings to disable log},
+
+        See guestagent_log_defs for an example.
+        """
+        return {}
+
+    @property
     def guestagent_log_defs(self):
         """These are log files that should be available on every Trove
         instance.  By definition, these should be of type LogType.SYS
         """
-        log_dir = CONF.log_dir or '/var/log/trove/'
-        log_file = CONF.log_file or 'trove-guestagent.log'
+        log_dir = CONF.get('log_dir', '/var/log/trove/')
+        log_file = CONF.get('log_file', 'trove-guestagent.log')
         guestagent_log = guestagent_utils.build_file_path(log_dir, log_file)
         return {
             self.GUEST_LOG_DEFS_GUEST_LABEL: {
                 self.GUEST_LOG_TYPE_LABEL: guest_log.LogType.SYS,
                 self.GUEST_LOG_USER_LABEL: None,
                 self.GUEST_LOG_FILE_LABEL: guestagent_log,
             },
         }
 
     @property
+    def guest_log_defs(self):
+        """Return all the guest log defs."""
+        if not self._guest_log_defs:
+            self._guest_log_defs = dict(self.datastore_log_defs)
+            self._guest_log_defs.update(self.guestagent_log_defs)
+        return self._guest_log_defs
+
+    @property
     def guest_log_context(self):
         return self._guest_log_context
 
     @guest_log_context.setter
     def guest_log_context(self, context):
         self._guest_log_context = context
 
-    @periodic_task.periodic_task
-    def update_status(self, context):
-        """Update the status of the trove instance."""
-        if not self.status.is_installed:
-            LOG.info("Database service is not installed, skip status check")
-            return
+    @property
+    def guest_log_cache(self):
+        """Make sure the guest_log_cache is loaded and return it."""
+        self._refresh_guest_log_cache()
+        return self._guest_log_cache
 
-        LOG.debug("Starting to check database service status")
+    def _refresh_guest_log_cache(self):
+        if self._guest_log_cache:
+            # Replace the context if it's changed
+            if self._guest_log_loaded_context != self.guest_log_context:
+                for log_name in self._guest_log_cache.keys():
+                    self._guest_log_cache[log_name].context = (
+                        self.guest_log_context)
+        else:
+            # Load the initial cache
+            self._guest_log_cache = {}
+            if self.guest_log_context:
+                gl_defs = self.guest_log_defs
+                try:
+                    exposed_logs = CONF.get(self.manager).get(
+                        'guest_log_exposed_logs')
+                except oslo_cfg.NoSuchOptError:
+                    exposed_logs = ''
+                LOG.debug("Available log defs: %s", ",".join(gl_defs.keys()))
+                exposed_logs = exposed_logs.lower().replace(',', ' ').split()
+                LOG.debug("Exposing log defs: %s", ",".join(exposed_logs))
+                expose_all = 'all' in exposed_logs
+                for log_name in gl_defs.keys():
+                    gl_def = gl_defs[log_name]
+                    exposed = expose_all or log_name in exposed_logs
+                    LOG.debug("Building guest log '%(name)s' from def: %(def)s"
+                              " (exposed: %(exposed)s)",
+                              {'name': log_name, 'def': gl_def,
+                               'exposed': exposed})
+                    self._guest_log_cache[log_name] = guest_log.GuestLog(
+                        self.guest_log_context, log_name,
+                        gl_def[self.GUEST_LOG_TYPE_LABEL],
+                        gl_def[self.GUEST_LOG_USER_LABEL],
+                        gl_def[self.GUEST_LOG_FILE_LABEL],
+                        exposed)
 
-        status = self.get_service_status()
-        self.status.set_status(status)
+        self._guest_log_loaded_context = self.guest_log_context
 
-    def get_service_status(self):
-        return self.status.get_actual_db_status()
+    ################
+    # Status related
+    ################
+    @periodic_task.periodic_task
+    def update_status(self, context):
+        """Update the status of the trove instance. It is decorated with
+        perodic_task so it is called automatically.
+        """
+        LOG.debug("Update status called.")
+        self.status.update()
 
     def rpc_ping(self, context):
         LOG.debug("Responding to RPC ping.")
         return True
 
     #################
     # Instance related
     #################
     def prepare(self, context, packages, databases, memory_mb, users,
                 device_path=None, mount_point=None, backup_info=None,
                 config_contents=None, root_password=None, overrides=None,
-                cluster_config=None, snapshot=None, modules=None,
-                ds_version=None):
+                cluster_config=None, snapshot=None, modules=None):
         """Set up datastore on a Guest Instance."""
         with EndNotification(context, instance_id=CONF.guest_id):
             self._prepare(context, packages, databases, memory_mb, users,
                           device_path, mount_point, backup_info,
                           config_contents, root_password, overrides,
-                          cluster_config, snapshot, modules,
-                          ds_version=ds_version)
+                          cluster_config, snapshot, modules)
 
     def _prepare(self, context, packages, databases, memory_mb, users,
                  device_path, mount_point, backup_info,
                  config_contents, root_password, overrides,
-                 cluster_config, snapshot, modules, ds_version=None):
-        LOG.info("Starting datastore prepare for '%s:%s'.", self.manager,
-                 ds_version)
+                 cluster_config, snapshot, modules):
+        LOG.info(_("Starting datastore prepare for '%s'."), self.manager)
         self.status.begin_install()
         post_processing = True if cluster_config else False
         try:
             # Since all module handling is common, don't pass it down to the
             # individual 'do_prepare' methods.
             self.do_prepare(context, packages, databases, memory_mb,
                             users, device_path, mount_point, backup_info,
                             config_contents, root_password, overrides,
-                            cluster_config, snapshot, ds_version=ds_version)
+                            cluster_config, snapshot)
+            if overrides:
+                LOG.info(_("Applying user-specified configuration "
+                           "(called from 'prepare')."))
+                self.apply_overrides_on_prepare(context, overrides)
         except Exception as ex:
             self.prepare_error = True
-            LOG.exception("Failed to prepare datastore: %s", ex)
+            LOG.exception(_("An error occurred preparing datastore: %s"),
+                          encodeutils.exception_to_unicode(ex))
             raise
         finally:
-            LOG.info("Ending datastore prepare for '%s'.", self.manager)
+            LOG.info(_("Ending datastore prepare for '%s'."), self.manager)
             self.status.end_install(error_occurred=self.prepare_error,
                                     post_processing=post_processing)
         # At this point critical 'prepare' work is done and the instance
         # is now in the correct 'ACTIVE' 'INSTANCE_READY' or 'ERROR' state.
         # Of cource if an error has occurred, none of the code that follows
         # will run.
-        LOG.info("Completed setup of '%s' datastore successfully.",
+        LOG.info(_("Completed setup of '%s' datastore successfully."),
                  self.manager)
 
         # The following block performs additional instance initialization.
         # Failures will be recorded, but won't stop the provisioning
         # or change the instance state.
         try:
             if modules:
-                LOG.info("Applying modules (called from 'prepare').")
+                LOG.info(_("Applying modules (called from 'prepare')."))
                 self.module_apply(context, modules)
-                LOG.info('Module apply completed.')
+                LOG.info(_('Module apply completed.'))
         except Exception as ex:
-            LOG.exception("An error occurred applying modules: "
-                          "%s", str(ex))
-
+            LOG.exception(_("An error occurred applying modules: "
+                            "%s"), ex.message)
         # The following block performs single-instance initialization.
         # Failures will be recorded, but won't stop the provisioning
         # or change the instance state.
         if not cluster_config:
             try:
                 if databases:
-                    LOG.info("Creating databases (called from 'prepare').")
+                    LOG.info(_("Creating databases (called from 'prepare')."))
                     self.create_database(context, databases)
-                    LOG.info('Databases created successfully.')
+                    LOG.info(_('Databases created successfully.'))
             except Exception as ex:
-                LOG.warning("An error occurred creating databases: %s",
-                            str(ex))
+                LOG.exception(_("An error occurred creating databases: "
+                                "%s"), ex.message)
             try:
                 if users:
-                    LOG.info("Creating users (called from 'prepare')")
+                    LOG.info(_("Creating users (called from 'prepare')"))
                     self.create_user(context, users)
-                    LOG.info('Users created successfully.')
+                    LOG.info(_('Users created successfully.'))
             except Exception as ex:
-                LOG.warning("An error occurred creating users: "
-                            "%s", str(ex))
+                LOG.exception(_("An error occurred creating users: "
+                                "%s"), ex.message)
 
             # We only enable-root automatically if not restoring a backup
             # that may already have root enabled in which case we keep it
             # unchanged.
             if root_password and not backup_info:
                 try:
-                    LOG.info("Enabling root user (with password).")
+                    LOG.info(_("Enabling root user (with password)."))
                     self.enable_root_on_prepare(context, root_password)
-                    LOG.info('Root enabled successfully.')
+                    LOG.info(_('Root enabled successfully.'))
                 except Exception as ex:
-                    LOG.exception("An error occurred enabling root user: "
-                                  "%s", str(ex))
+                    LOG.exception(_("An error occurred enabling root user: "
+                                    "%s"), ex.message)
 
         try:
-            LOG.info("Starting post prepare for '%s' datastore.", self.manager)
+            LOG.info(_("Calling post_prepare for '%s' datastore."),
+                     self.manager)
             self.post_prepare(context, packages, databases, memory_mb,
                               users, device_path, mount_point, backup_info,
                               config_contents, root_password, overrides,
                               cluster_config, snapshot)
-            LOG.info("Post prepare for '%s' datastore completed.",
+            LOG.info(_("Post prepare for '%s' datastore completed."),
                      self.manager)
         except Exception as ex:
-            LOG.exception("An error occurred in post prepare: %s",
-                          str(ex))
+            LOG.exception(_("An error occurred in post prepare: %s"),
+                          ex.message)
             raise
 
+    def apply_overrides_on_prepare(self, context, overrides):
+        self.update_overrides(context, overrides)
+        self.restart(context)
+
+    def enable_root_on_prepare(self, context, root_password):
+        self.enable_root_with_password(context, root_password)
+
     @abc.abstractmethod
     def do_prepare(self, context, packages, databases, memory_mb, users,
                    device_path, mount_point, backup_info, config_contents,
-                   root_password, overrides, cluster_config, snapshot,
-                   ds_version=None):
+                   root_password, overrides, cluster_config, snapshot):
         """This is called from prepare when the Trove instance first comes
         online.  'Prepare' is the first rpc message passed from the
         task manager.  do_prepare handles all the base configuration of
         the instance and is where the actual work is done.  Once this method
         completes, the datastore is considered either 'ready' for use (or
         for final connections to other datastores) or in an 'error' state,
         and the status is changed accordingly.  Each datastore must
@@ -328,51 +390,37 @@
         Processing done here should be limited to things that will not
         affect the actual 'running' status of the datastore (for example,
         creating databases and users, although these are now handled
         automatically).  Any exceptions are caught, logged and rethrown,
         however no status changes are made and the end-user will not be
         informed of the error.
         """
-        LOG.info('No post_prepare work has been defined.')
+        LOG.info(_('No post_prepare work has been defined.'))
         pass
 
-    def start_db_with_conf_changes(self, context, config_contents, ds_version):
-        """Start the database with given configuration.
-
-        This method is called after resize.
-        """
-        self.app.start_db_with_conf_changes(config_contents, ds_version)
-
-    def stop_db(self, context):
-        self.app.stop_db()
-
-    def restart(self, context):
-        self.app.restart()
-
-    def rebuild(self, context, ds_version, config_contents=None,
-                config_overrides=None):
-        raise exception.DatastoreOperationNotSupported(
-            operation='rebuild', datastore=self.manager)
-
     def pre_upgrade(self, context):
         """Prepares the guest for upgrade, returning a dict to be passed
         to post_upgrade
         """
         return {}
 
-    def upgrade(self, context, upgrade_info):
-        """Upgrade the database."""
-        pass
-
     def post_upgrade(self, context, upgrade_info):
         """Recovers the guest after the image is upgraded using information
         from the pre_upgrade step
         """
         pass
 
+    #################
+    # Service related
+    #################
+    @abc.abstractmethod
+    def restart(self, context):
+        """Restart the database service."""
+        pass
+
     #####################
     # File System related
     #####################
     def get_filesystem_stats(self, context, fs_path):
         """Gets the filesystem stats for the path given."""
         # TODO(peterstac) - note that fs_path is not used in this method.
         mount_point = CONF.get(self.manager).mount_point
@@ -390,202 +438,95 @@
     def unmount_volume(self, context, device_path=None, mount_point=None):
         LOG.debug("Unmounting the device %(path)s from the mount point "
                   "%(mount_point)s.", {'path': device_path,
                                        'mount_point': mount_point})
         device = volume.VolumeDevice(device_path)
         device.unmount(mount_point)
 
-    def resize_fs(self, context, device_path=None, mount_point=None,
-                  online=False):
-        LOG.info(f"Resizing the filesystem at {mount_point}, online: {online}")
+    def resize_fs(self, context, device_path=None, mount_point=None):
+        LOG.debug("Resizing the filesystem at %s.", mount_point)
         device = volume.VolumeDevice(device_path)
-        device.resize_fs(mount_point, online=online)
+        device.resize_fs(mount_point)
 
     ###############
     # Configuration
     ###############
     def reset_configuration(self, context, configuration):
-        """Reset database base configuration.
-
-        The default implementation should be sufficient if a
+        """The default implementation should be sufficient if a
         configuration_manager is provided. Even if one is not, this
         method needs to be implemented to allow the rollback of
         flavor-resize on the guestagent side.
         """
+        LOG.debug("Resetting configuration.")
         if self.configuration_manager:
-            LOG.info("Resetting configuration.")
             config_contents = configuration['config_contents']
-            self.configuration_manager.reset_configuration(config_contents)
-
-    def apply_overrides_on_prepare(self, context, overrides):
-        self.update_overrides(context, overrides)
-        self.restart(context)
-
-    def update_overrides(self, context, overrides, remove=False):
-        LOG.info(f"Updating config options: {overrides}, remove={remove}")
-        if remove:
-            self.app.remove_overrides()
-        self.app.update_overrides(overrides)
-
-    def apply_overrides(self, context, overrides):
-        raise exception.DatastoreOperationNotSupported(
-            operation='apply_overrides', datastore=self.manager)
+            self.configuration_manager.save_configuration(config_contents)
 
     #################
     # Cluster related
     #################
     def cluster_complete(self, context):
-        LOG.info("Cluster creation complete, starting status checks.")
+        LOG.debug("Cluster creation complete, starting status checks.")
         self.status.end_install()
 
     #############
     # Log related
     #############
-    def get_datastore_log_defs(self):
-        """Any datastore-specific log files should be overridden in this dict
-        by the corresponding Manager class.
-
-        Format of a dict entry:
-
-        'name_of_log': {self.GUEST_LOG_TYPE_LABEL:
-                            Specified by the Enum in guest_log.LogType,
-                        self.GUEST_LOG_USER_LABEL:
-                            User that owns the file,
-                        self.GUEST_LOG_FILE_LABEL:
-                            Path on filesystem where the log resides,
-                        self.GUEST_LOG_SECTION_LABEL:
-                            Section where to put config (if ini style)
-                        self.GUEST_LOG_ENABLE_LABEL: {
-                            Dict of config_group settings to enable log},
-                        self.GUEST_LOG_DISABLE_LABEL: {
-                            Dict of config_group settings to disable log},
-
-        See guestagent_log_defs for an example.
-        """
-        return {}
-
-    def is_log_enabled(self, logname):
-        return False
-
-    def get_guest_log_defs(self):
-        """Return all the guest log defs."""
-        if not self._guest_log_defs:
-            self._guest_log_defs = dict(self.get_datastore_log_defs())
-            self._guest_log_defs.update(self.guestagent_log_defs)
-        return self._guest_log_defs
-
-    def get_guest_log_cache(self):
-        """Make sure the guest_log_cache is loaded and return it."""
-        self._refresh_guest_log_cache()
-        return self._guest_log_cache
-
-    def _refresh_guest_log_cache(self):
-        if self._guest_log_cache:
-            # Replace the context if it's changed
-            if self._guest_log_loaded_context != self.guest_log_context:
-                for log_name in self._guest_log_cache.keys():
-                    self._guest_log_cache[log_name].context = (
-                        self.guest_log_context)
-        else:
-            # Load the initial cache
-            self._guest_log_cache = {}
-            if self.guest_log_context:
-                gl_defs = self.get_guest_log_defs()
-                try:
-                    exposed_logs = CONF.get(self.manager).get(
-                        'guest_log_exposed_logs')
-                except oslo_cfg.NoSuchOptError:
-                    exposed_logs = ''
-                LOG.debug("Available log defs: %s", ",".join(gl_defs.keys()))
-                exposed_logs = exposed_logs.lower().replace(',', ' ').split()
-                LOG.debug("Exposing log defs: %s", ",".join(exposed_logs))
-                expose_all = 'all' in exposed_logs
-
-                for log_name in gl_defs.keys():
-                    gl_def = gl_defs[log_name]
-                    exposed = expose_all or log_name in exposed_logs
-                    guestlog = guest_log.GuestLog(
-                        self.guest_log_context, log_name,
-                        gl_def[self.GUEST_LOG_TYPE_LABEL],
-                        gl_def[self.GUEST_LOG_USER_LABEL],
-                        gl_def[self.GUEST_LOG_FILE_LABEL],
-                        exposed)
-
-                    if (gl_def[self.GUEST_LOG_TYPE_LABEL] ==
-                            guest_log.LogType.USER):
-                        guestlog.enabled = self.is_log_enabled(log_name)
-                        guestlog.status = (guest_log.LogStatus.Enabled
-                                           if guestlog.enabled
-                                           else guest_log.LogStatus.Disabled)
-
-                    self._guest_log_cache[log_name] = guestlog
-
-        self._guest_log_loaded_context = self.guest_log_context
-
     def guest_log_list(self, context):
-        LOG.info("Getting list of guest logs.")
+        LOG.info(_("Getting list of guest logs."))
         self.guest_log_context = context
-        gl_cache = self.get_guest_log_cache()
+        gl_cache = self.guest_log_cache
         result = filter(None, [gl_cache[log_name].show()
                                if gl_cache[log_name].exposed else None
                                for log_name in gl_cache.keys()])
+        LOG.info(_("Returning list of logs: %s"), result)
         return result
 
     def guest_log_action(self, context, log_name, enable, disable,
                          publish, discard):
         if enable and disable:
             raise exception.BadRequest("Cannot enable and disable log '%s'." %
                                        log_name)
         # Enable if we are publishing, unless told to disable
         if publish and not disable:
             enable = True
-        LOG.info("Processing guest log '%(log)s' "
-                 "(enable=%(en)s, disable=%(dis)s, "
-                 "publish=%(pub)s, discard=%(disc)s).",
+        LOG.info(_("Processing guest log '%(log)s' "
+                   "(enable=%(en)s, disable=%(dis)s, "
+                   "publish=%(pub)s, discard=%(disc)s)."),
                  {'log': log_name, 'en': enable, 'dis': disable,
                   'pub': publish, 'disc': discard})
-
         self.guest_log_context = context
-        gl_cache = self.get_guest_log_cache()
-
+        gl_cache = self.guest_log_cache
         if log_name in gl_cache:
-            LOG.debug(f"Found log {log_name}, type={gl_cache[log_name].type}, "
-                      f"enable={gl_cache[log_name].enabled}")
-
-            # system log can only be published
             if ((gl_cache[log_name].type == guest_log.LogType.SYS) and
                     not publish):
                 if enable or disable:
                     if enable:
                         action_text = "enable"
                     else:
                         action_text = "disable"
                     raise exception.BadRequest("Cannot %s a SYSTEM log ('%s')."
                                                % (action_text, log_name))
-
             if gl_cache[log_name].type == guest_log.LogType.USER:
                 requires_change = (
                     (gl_cache[log_name].enabled and disable) or
                     (not gl_cache[log_name].enabled and enable))
                 if requires_change:
-                    self.guest_log_enable(context, log_name, disable)
+                    restart_required = self.guest_log_enable(
+                        context, log_name, disable)
+                    if restart_required:
+                        self.set_guest_log_status(
+                            guest_log.LogStatus.Restart_Required, log_name)
                     gl_cache[log_name].enabled = enable
-                    gl_cache[log_name].status = (
-                        guest_log.LogStatus.Enabled
-                        if enable
-                        else guest_log.LogStatus.Disabled
-                    )
-
             log_details = gl_cache[log_name].show()
             if discard:
                 log_details = gl_cache[log_name].discard_log()
             if publish:
                 log_details = gl_cache[log_name].publish_log()
-
-            LOG.info("Details for log '%(log)s': %(det)s",
+            LOG.info(_("Details for log '%(log)s': %(det)s"),
                      {'log': log_name, 'det': log_details})
             return log_details
 
         raise exception.NotFound("Log '%s' is not defined." % log_name)
 
     def guest_log_enable(self, context, log_name, disable):
         """This method can be overridden by datastore implementations to
@@ -596,15 +537,15 @@
         the logging to begin.
         """
         restart_required = False
         verb = ("Disabling" if disable else "Enabling")
         if self.configuration_manager:
             LOG.debug("%(verb)s log '%(log)s'", {'verb': verb,
                                                  'log': log_name})
-            gl_def = self.get_guest_log_defs()[log_name]
+            gl_def = self.guest_log_defs[log_name]
             enable_cfg_label = "%s_%s_log" % (self.GUEST_LOG_ENABLE_LABEL,
                                               log_name)
             disable_cfg_label = "%s_%s_log" % (self.GUEST_LOG_DISABLE_LABEL,
                                                log_name)
             restart_required = gl_def.get(self.GUEST_LOG_RESTART_LABEL,
                                           restart_required)
             if disable:
@@ -616,22 +557,19 @@
             else:
                 self._apply_log_overrides(
                     context, disable_cfg_label, enable_cfg_label,
                     gl_def.get(self.GUEST_LOG_ENABLE_LABEL),
                     gl_def.get(self.GUEST_LOG_SECTION_LABEL),
                     restart_required)
         else:
-            log_fmt = ("%(verb)s log '%(log)s' not supported - "
-                       "no configuration manager defined!")
-            exc_fmt = _("%(verb)s log '%(log)s' not supported - "
-                        "no configuration manager defined!")
-            msg_content = {'verb': verb, 'log': log_name}
-            LOG.error(log_fmt, msg_content)
-            raise exception.GuestError(
-                original_message=(exc_fmt % msg_content))
+            msg = (_("%(verb)s log '%(log)s' not supported - "
+                     "no configuration manager defined!") %
+                   {'verb': verb, 'log': log_name})
+            LOG.error(msg)
+            raise exception.GuestError(original_message=msg)
 
         return restart_required
 
     def _apply_log_overrides(self, context, remove_label,
                              apply_label, cfg_values, section_label,
                              restart_required):
         self.configuration_manager.remove_system_override(
@@ -639,37 +577,51 @@
         if cfg_values:
             config_man_values = cfg_values
             if section_label:
                 config_man_values = {section_label: cfg_values}
             self.configuration_manager.apply_system_override(
                 config_man_values, change_id=apply_label, pre_user=True)
         if restart_required:
-            self.status.set_status(
-                service_status.ServiceStatuses.RESTART_REQUIRED)
+            self.status.set_status(instance.ServiceStatuses.RESTART_REQUIRED)
         else:
             self.apply_overrides(context, cfg_values)
 
-    def get_log_status(self, label):
-        self.configuration_manager.get_value(label)
+    def set_guest_log_status(self, status, log_name=None):
+        """Sets the status of log_name to 'status' - if log_name is not
+        provided, sets the status on all logs.
+        """
+        gl_cache = self.guest_log_cache
+        names = [log_name]
+        if not log_name or log_name not in gl_cache:
+            names = gl_cache.keys()
+        for name in names:
+            # If we're already in restart mode and we're asked to set the
+            # status to restart, assume enable/disable has been flipped
+            # without a restart and set the status to restart done
+            if (gl_cache[name].status == guest_log.LogStatus.Restart_Required
+                    and status == guest_log.LogStatus.Restart_Required):
+                gl_cache[name].status = guest_log.LogStatus.Restart_Completed
+            else:
+                gl_cache[name].status = status
 
     def build_log_file_name(self, log_name, owner, datastore_dir=None):
         """Build a log file name based on the log_name and make sure the
         directories exist and are accessible by owner.
         """
         if datastore_dir is None:
             base_dir = self.GUEST_LOG_BASE_DIR
             if not operating_system.exists(base_dir, is_directory=True):
-                operating_system.ensure_directory(
+                operating_system.create_directory(
                     base_dir, user=owner, group=owner, force=True,
                     as_root=True)
             datastore_dir = guestagent_utils.build_file_path(
                 base_dir, self.GUEST_LOG_DATASTORE_DIRNAME)
 
         if not operating_system.exists(datastore_dir, is_directory=True):
-            operating_system.ensure_directory(
+            operating_system.create_directory(
                 datastore_dir, user=owner, group=owner, force=True,
                 as_root=True)
         log_file_name = guestagent_utils.build_file_path(
             datastore_dir, '%s-%s.log' % (self.manager, log_name))
 
         return self.validate_log_file(log_file_name, owner)
 
@@ -679,29 +631,29 @@
         if not operating_system.exists(log_file, as_root=True):
             operating_system.write_file(log_file, '', as_root=True)
 
         operating_system.chown(log_file, user=owner, group=owner,
                                as_root=True)
         operating_system.chmod(log_file, FileMode.ADD_USR_RW_GRP_RW_OTH_R,
                                as_root=True)
-
+        LOG.debug("Set log file '%s' as readable", log_file)
         return log_file
 
     ################
     # Module related
     ################
     def module_list(self, context, include_contents=False):
-        LOG.info("Getting list of modules.")
+        LOG.info(_("Getting list of modules."))
         results = module_manager.ModuleManager.read_module_results(
             is_admin=context.is_admin, include_contents=include_contents)
-        LOG.info("Returning list of modules: %s", results)
+        LOG.info(_("Returning list of modules: %s"), results)
         return results
 
     def module_apply(self, context, modules=None):
-        LOG.info("Applying modules.")
+        LOG.info(_("Applying modules."))
         results = []
         modules = [data['module'] for data in modules]
         try:
             # make sure the modules are applied in the correct order
             modules.sort(key=operator.itemgetter('apply_order'))
             modules.sort(key=operator.itemgetter('priority_apply'),
                          reverse=True)
@@ -743,19 +695,19 @@
                 reason = (_("Module not valid for datastore %s") %
                           CONF.datastore_manager)
                 raise exception.ModuleInvalid(reason=reason)
             result = module_manager.ModuleManager.apply_module(
                 driver, module_type, name, tenant, datastore, ds_version,
                 contents, id, md5, auto_apply, visible, is_admin)
             results.append(result)
-        LOG.info("Returning list of modules: %s", results)
+        LOG.info(_("Returning list of modules: %s"), results)
         return results
 
     def module_remove(self, context, module=None):
-        LOG.info("Removing module.")
+        LOG.info(_("Removing module."))
         module = module['module']
         id = module.get('id', None)
         module_type = module.get('type', None)
         name = module.get('name', None)
         datastore = module.get('datastore', None)
         ds_version = module.get('datastore_version', None)
         if not name:
@@ -763,181 +715,175 @@
         driver = self.module_driver_manager.get_driver(module_type)
         if not driver:
             raise exception.ModuleTypeNotFound(
                 _("No driver implemented for module type '%s'") %
                 module_type)
         module_manager.ModuleManager.remove_module(
             driver, module_type, id, name, datastore, ds_version)
-        LOG.info("Deleted module: %s", name)
+        LOG.info(_("Deleted module: %s"), name)
 
-    ################
-    # Backup and restore
-    ################
-    def create_backup(self, context, backup_info):
-        """Create backup for the database.
+    ###############
+    # Not Supported
+    ###############
+    def change_passwords(self, context, users):
+        LOG.debug("Changing passwords.")
+        with EndNotification(context):
+            raise exception.DatastoreOperationNotSupported(
+                operation='change_passwords', datastore=self.manager)
 
-        :param context: User context object.
-        :param backup_info: a dictionary containing the db instance id of the
-                            backup task, location, type, and other data.
-        """
-        pass
+    def enable_root(self, context):
+        LOG.debug("Enabling root.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='enable_root', datastore=self.manager)
 
-    def perform_restore(self, context, restore_location, backup_info):
-        LOG.info("Starting to restore database from backup %s, "
-                 "backup_info: %s", backup_info['id'], backup_info)
-
-        if (backup_info["location"].endswith('.enc') and
-                not CONF.backup_aes_cbc_key):
-            self.status.set_status(service_status.ServiceStatuses.FAILED)
-            raise exception.TroveError('Decryption key not configured for '
-                                       'encrypted backup.')
+    def enable_root_with_password(self, context, root_password=None):
+        LOG.debug("Enabling root with password.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='enable_root_with_password', datastore=self.manager)
 
-        try:
-            self.app.restore_backup(context, backup_info, restore_location)
-        except Exception:
-            LOG.error("Failed to restore from backup %s.", backup_info['id'])
-            self.status.set_status(service_status.ServiceStatuses.FAILED)
-            raise
+    def disable_root(self, context):
+        LOG.debug("Disabling root.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='disable_root', datastore=self.manager)
 
-        LOG.info("Finished restore data from backup %s", backup_info['id'])
+    def is_root_enabled(self, context):
+        LOG.debug("Checking if root was ever enabled.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='is_root_enabled', datastore=self.manager)
+
+    def create_backup(self, context, backup_info):
+        LOG.debug("Creating backup.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='create_backup', datastore=self.manager)
+
+    def _perform_restore(self, backup_info, context, restore_location, app):
+        LOG.debug("Performing restore.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='_perform_restore', datastore=self.manager)
 
-    ################
-    # Database and user management
-    ################
     def create_database(self, context, databases):
+        LOG.debug("Creating databases.")
         with EndNotification(context):
-            return self.adm.create_databases(databases)
+            raise exception.DatastoreOperationNotSupported(
+                operation='create_database', datastore=self.manager)
 
     def list_databases(self, context, limit=None, marker=None,
                        include_marker=False):
-        return self.adm.list_databases(limit, marker, include_marker)
+        LOG.debug("Listing databases.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='list_databases', datastore=self.manager)
 
     def delete_database(self, context, database):
+        LOG.debug("Deleting database.")
         with EndNotification(context):
-            return self.adm.delete_database(database)
-
-    def change_passwords(self, context, users):
-        with EndNotification(context):
-            self.adm.change_passwords(users)
-
-    def get_root_password(self, context):
-        raise exception.DatastoreOperationNotSupported(
-            operation='get_root_password', datastore=self.manager)
-
-    def enable_root(self, context):
-        LOG.info("Enabling root for the database.")
-        return self.adm.enable_root()
-
-    def enable_root_on_prepare(self, context, root_password):
-        self.enable_root_with_password(context, root_password)
-
-    def enable_root_with_password(self, context, root_password=None):
-        return self.adm.enable_root(root_password)
-
-    def disable_root(self, context):
-        LOG.info("Disabling root for the database.")
-        return self.adm.disable_root()
-
-    def is_root_enabled(self, context):
-        return self.adm.is_root_enabled()
+            raise exception.DatastoreOperationNotSupported(
+                operation='delete_database', datastore=self.manager)
 
     def create_user(self, context, users):
+        LOG.debug("Creating users.")
         with EndNotification(context):
-            self.adm.create_users(users)
+            raise exception.DatastoreOperationNotSupported(
+                operation='create_user', datastore=self.manager)
 
     def list_users(self, context, limit=None, marker=None,
                    include_marker=False):
-        return self.adm.list_users(limit, marker, include_marker)
+        LOG.debug("Listing users.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='list_users', datastore=self.manager)
 
     def delete_user(self, context, user):
+        LOG.debug("Deleting user.")
         with EndNotification(context):
-            self.adm.delete_user(user)
+            raise exception.DatastoreOperationNotSupported(
+                operation='delete_user', datastore=self.manager)
 
     def get_user(self, context, username, hostname):
-        return self.adm.get_user(username, hostname)
+        LOG.debug("Getting user.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='get_user', datastore=self.manager)
 
     def update_attributes(self, context, username, hostname, user_attrs):
+        LOG.debug("Updating user attributes.")
         with EndNotification(context):
-            self.adm.update_attributes(username, hostname, user_attrs)
+            raise exception.DatastoreOperationNotSupported(
+                operation='update_attributes', datastore=self.manager)
 
     def grant_access(self, context, username, hostname, databases):
-        return self.adm.grant_access(username, hostname, databases)
+        LOG.debug("Granting user access.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='grant_access', datastore=self.manager)
 
     def revoke_access(self, context, username, hostname, database):
-        return self.adm.revoke_access(username, hostname, database)
+        LOG.debug("Revoking user access.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='revoke_access', datastore=self.manager)
 
     def list_access(self, context, username, hostname):
-        return self.adm.list_access(username, hostname)
-
-    ################
-    # Replication related
-    ################
-    def backup_required_for_replication(self, context):
-        return self.replication.backup_required_for_replication()
-
-    def get_replication_snapshot(self, context, snapshot_info,
-                                 replica_source_config=None):
-        LOG.info("Getting replication snapshot, snapshot_info: %s",
-                 snapshot_info)
+        LOG.debug("Listing user access.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='list_access', datastore=self.manager)
 
-        self.replication.enable_as_master(self.app, replica_source_config)
-        LOG.info('Enabled as replication master')
+    def get_config_changes(self, cluster_config, mount_point=None):
+        LOG.debug("Get configuration changes.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='get_configuration_changes', datastore=self.manager)
 
-        snapshot_id, replica_conf = self.replication.snapshot_for_replication(
-            context, self.app, self.adm, None, snapshot_info)
+    def update_overrides(self, context, overrides, remove=False):
+        LOG.debug("Updating overrides.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='update_overrides', datastore=self.manager)
 
-        volume_stats = self.get_filesystem_stats(context, None)
-
-        replication_snapshot = {
-            'dataset': {
-                'datastore_manager': self.manager,
-                'dataset_size': volume_stats.get('used', 0.0),
-                'volume_size': volume_stats.get('total', 0.0),
-                'snapshot_id': snapshot_id
-            },
-            'replication_strategy': self.replication_strategy,
-            'master': self.replication.get_master_ref(self.app, snapshot_info),
-            'replica_conf': replica_conf
-        }
+    def apply_overrides(self, context, overrides):
+        LOG.debug("Applying overrides.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='apply_overrides', datastore=self.manager)
 
-        return replication_snapshot
+    def get_replication_snapshot(self, context, snapshot_info,
+                                 replica_source_config=None):
+        LOG.debug("Getting replication snapshot.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='get_replication_snapshot', datastore=self.manager)
 
-    def attach_replica(self, context, snapshot, slave_config, restart=False):
+    def attach_replication_slave(self, context, snapshot, slave_config):
+        LOG.debug("Attaching replication slave.")
         raise exception.DatastoreOperationNotSupported(
             operation='attach_replication_slave', datastore=self.manager)
 
     def detach_replica(self, context, for_failover=False):
-        """Running on replica, detach from the primary."""
-        LOG.info("Detaching replica.")
-        replica_info = self.replication.detach_slave(self.app, for_failover)
-        return replica_info
+        LOG.debug("Detaching replica.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='detach_replica', datastore=self.manager)
 
     def get_replica_context(self, context):
-        """Running on primary."""
-        LOG.info("Getting replica context.")
-        replica_info = self.replication.get_replica_context(self.app, self.adm)
-        return replica_info
+        LOG.debug("Getting replica context.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='get_replica_context', datastore=self.manager)
 
     def make_read_only(self, context, read_only):
+        LOG.debug("Making datastore read-only.")
         raise exception.DatastoreOperationNotSupported(
             operation='make_read_only', datastore=self.manager)
 
     def enable_as_master(self, context, replica_source_config):
-        LOG.info("Enable as master")
-        self.replication.enable_as_master(self.app, replica_source_config)
-
-    def demote_replication_master(self, context):
-        LOG.info("Demoting replication master.")
-        self.replication.demote_master(self.app)
+        LOG.debug("Enabling as master.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='enable_as_master', datastore=self.manager)
 
     def get_txn_count(self, context):
         LOG.debug("Getting transaction count.")
         raise exception.DatastoreOperationNotSupported(
             operation='get_txn_count', datastore=self.manager)
 
     def get_latest_txn_id(self, context):
+        LOG.debug("Getting latest transaction id.")
         raise exception.DatastoreOperationNotSupported(
             operation='get_latest_txn_id', datastore=self.manager)
 
     def wait_for_txn(self, context, txn):
+        LOG.debug("Waiting for transaction.")
         raise exception.DatastoreOperationNotSupported(
             operation='wait_for_txn', datastore=self.manager)
+
+    def demote_replication_master(self, context):
+        LOG.debug("Demoting replication master.")
+        raise exception.DatastoreOperationNotSupported(
+            operation='demote_replication_master', datastore=self.manager)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/mariadb/manager.py` & `trove-8.0.1/trove/tests/unittests/guestagent/test_datastore_manager.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,34 +1,24 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2016 Tesora Inc.
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-from trove.guestagent.datastore.mariadb import service
-from trove.guestagent.datastore.mysql_common import manager
-from trove.guestagent.datastore.mysql_common import service as mysql_service
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
 
+from trove.tests.unittests import trove_testtools
 
-class Manager(manager.MySqlManager):
-    def __init__(self):
-        status = mysql_service.BaseMySqlAppStatus(self.docker_client)
-        app = service.MariaDBApp(status, self.docker_client)
-        adm = service.MariaDBAdmin(app)
 
-        super(Manager, self).__init__(app, status, adm)
+class DatastoreManagerTest(trove_testtools.TestCase):
 
-    def get_start_db_params(self, data_dir):
-        """Get parameters for starting database.
-
-        Cinder volume initialization(after formatted) may leave a lost+found
-        folder.
-        """
-        return (f'--ignore-db-dir=lost+found --ignore-db-dir=conf.d '
-                f'--datadir={data_dir}')
+    def setUp(self, manager_name):
+        super(DatastoreManagerTest, self).setUp()
+        self.patch_datastore_manager(manager_name)
+        self.context = trove_testtools.TroveTestContext(self)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/mariadb/service.py` & `trove-8.0.1/trove/guestagent/datastore/experimental/mariadb/service.py`

 * *Files 22% similar despite different names*

```diff
@@ -12,81 +12,96 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 from oslo_log import log as logging
 
-from trove.common import exception
-from trove.common import utils
+from trove.common.i18n import _
+from trove.guestagent.common import operating_system
+from trove.guestagent.datastore.galera_common import service as galera_service
 from trove.guestagent.datastore.mysql_common import service as mysql_service
-from trove.guestagent.utils import mysql as mysql_util
 
 LOG = logging.getLogger(__name__)
 
 
-class MariaDBApp(mysql_service.BaseMySqlApp):
-    def __init__(self, status, docker_client):
-        super(MariaDBApp, self).__init__(status, docker_client)
-
-    def wait_for_slave_status(self, status, client, max_time):
-        def verify_slave_status():
-            actual_status = client.execute(
-                'SHOW GLOBAL STATUS like "Slave_running";').first()[1]
-            return actual_status.upper() == status.upper()
-
-        LOG.debug("Waiting for slave status %s with timeout %s",
-                  status, max_time)
-        try:
-            utils.poll_until(verify_slave_status, sleep_time=3,
-                             time_out=max_time)
-            LOG.info("Replication status: %s.", status)
-        except exception.PollTimeOut:
-            raise RuntimeError(
-                "Replication is not %(status)s after %(max)d seconds." %
-                {'status': status.lower(), 'max': max_time})
+class MariaDBApp(galera_service.GaleraApp):
+
+    OS = operating_system.get_os()
+
+    def __init__(self, status):
+        super(MariaDBApp, self).__init__(
+            status, mysql_service.BaseLocalSqlClient,
+            mysql_service.BaseKeepAliveConnection)
+
+    @property
+    def service_candidates(self):
+        service_candidates = super(MariaDBApp, self).service_candidates
+        return {
+            operating_system.DEBIAN: ["mariadb"] + service_candidates,
+            operating_system.REDHAT: ["mariadb"],
+            operating_system.SUSE: service_candidates
+        }[self.OS]
+
+    @property
+    def mysql_service(self):
+        result = super(MariaDBApp, self).mysql_service
+        if result['type'] == 'sysvinit':
+            result['cmd_bootstrap_galera_cluster'] = (
+                "sudo service %s bootstrap"
+                % result['service'])
+        elif result['type'] == 'systemd':
+            # TODO(mwj 2016/01/28): determine RHEL start for MariaDB Cluster
+            result['cmd_bootstrap_galera_cluster'] = (
+                "sudo systemctl start %s@bootstrap.service"
+                % result['service'])
+        return result
+
+    @property
+    def cluster_configuration(self):
+        return self.configuration_manager.get_value('galera')
 
     def _get_slave_status(self):
-        with mysql_util.SqlClient(self.get_engine()) as client:
+        with self.local_sql_client(self.get_engine()) as client:
             return client.execute('SHOW SLAVE STATUS').first()
 
     def _get_master_UUID(self):
         slave_status = self._get_slave_status()
         return slave_status and slave_status['Master_Server_Id'] or None
 
     def _get_gtid_executed(self):
-        with mysql_util.SqlClient(self.get_engine()) as client:
+        with self.local_sql_client(self.get_engine()) as client:
             return client.execute('SELECT @@global.gtid_binlog_pos').first()[0]
 
-    def _get_gtid_slave_executed(self):
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            return client.execute('SELECT @@global.gtid_slave_pos').first()[0]
-
     def get_last_txn(self):
         master_UUID = self._get_master_UUID()
         last_txn_id = '0'
-        gtid_executed = self._get_gtid_slave_executed()
+        gtid_executed = self._get_gtid_executed()
         for gtid_set in gtid_executed.split(','):
             uuid_set = gtid_set.split('-')
-            if str(uuid_set[1]) == str(master_UUID):
+            if uuid_set[1] == master_UUID:
                 last_txn_id = uuid_set[-1]
                 break
         return master_UUID, int(last_txn_id)
 
     def get_latest_txn_id(self):
+        LOG.info(_("Retrieving latest txn id."))
         return self._get_gtid_executed()
 
     def wait_for_txn(self, txn):
-        cmd = "SELECT MASTER_GTID_WAIT('%s')" % txn
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            client.execute(cmd)
+        LOG.info(_("Waiting on txn '%s'."), txn)
+        with self.local_sql_client(self.get_engine()) as client:
+            client.execute("SELECT MASTER_GTID_WAIT('%s')" % txn)
 
 
 class MariaDBRootAccess(mysql_service.BaseMySqlRootAccess):
-    def __init__(self, app):
-        super(MariaDBRootAccess, self).__init__(app)
+    def __init__(self):
+        super(MariaDBRootAccess, self).__init__(
+            mysql_service.BaseLocalSqlClient,
+            MariaDBApp(mysql_service.BaseMySqlAppStatus.get()))
 
 
 class MariaDBAdmin(mysql_service.BaseMySqlAdmin):
-    def __init__(self, app):
-        root_access = MariaDBRootAccess(app)
-        super(MariaDBAdmin, self).__init__(root_access, app)
+    def __init__(self):
+        super(MariaDBAdmin, self).__init__(
+            mysql_service.BaseLocalSqlClient, MariaDBRootAccess(),
+            MariaDBApp)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/mysql/manager.py` & `trove-8.0.1/trove/extensions/mgmt/volume/service.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,50 +1,40 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2011 OpenStack Foundation
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-import semantic_version
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
 
+
+from oslo_log import log as logging
+
+from trove.common.auth import admin_context
 from trove.common import cfg
-from trove.guestagent.datastore.mysql import service
-from trove.guestagent.datastore.mysql_common import manager
+from trove.common.i18n import _
+from trove.common import wsgi
+from trove.extensions.mgmt.volume import models
+from trove.extensions.mgmt.volume import views
 
 CONF = cfg.CONF
+LOG = logging.getLogger(__name__)
 
 
-class Manager(manager.MySqlManager):
-    def __init__(self):
-        status = service.MySqlAppStatus(self.docker_client)
-        app = service.MySqlApp(status, self.docker_client)
-        adm = service.MySqlAdmin(app)
-
-        super(Manager, self).__init__(app, status, adm)
-
-    def get_start_db_params(self, data_dir):
-        """Get parameters for starting database.
-
-        Cinder volume initialization(after formatted) may leave a lost+found
-        folder.
-
-        The --ignore-db-dir option is deprecated in MySQL 5.7. With the
-        introduction of the data dictionary in MySQL 8.0, it became
-        superfluous and was removed in that version.
-        """
-        params = f'--datadir={data_dir}'
-
-        mysql_8 = semantic_version.Version('8.0.0')
-        cur_ver = semantic_version.Version.coerce(CONF.datastore_version)
-        params = f'--datadir={data_dir}'
-        if cur_ver < mysql_8:
-            params = (f"{params} --ignore-db-dir=lost+found "
-                      f"--ignore-db-dir=conf.d")
+class StorageController(wsgi.Controller):
+    """Controller for storage device functionality."""
 
-        return params
+    @admin_context
+    def index(self, req, tenant_id):
+        """Return all storage devices."""
+        LOG.info(_("req : '%s'\n\n"), req)
+        LOG.info(_("Indexing storage info for tenant '%s'"), tenant_id)
+        context = req.environ[wsgi.CONTEXT_KEY]
+        storages = models.StorageDevices.load(context, CONF.os_region_name)
+        return wsgi.Result(views.StoragesView(storages).data(), 200)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/mysql/service.py` & `trove-8.0.1/trove/guestagent/datastore/mysql/service.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,105 +1,106 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright 2013 OpenStack Foundation
+# Copyright 2013 Rackspace Hosting
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-import semantic_version
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+#
+
+from oslo_log import log as logging
 
-from trove.common import cfg
+from trove.common.i18n import _
 from trove.guestagent.datastore.mysql_common import service
-from trove.guestagent.utils import mysql as mysql_util
 
-CONF = cfg.CONF
+LOG = logging.getLogger(__name__)
+CONF = service.CONF
+
+
+class KeepAliveConnection(service.BaseKeepAliveConnection):
+    pass
 
 
 class MySqlAppStatus(service.BaseMySqlAppStatus):
-    def __init__(self, docker_client):
-        super(MySqlAppStatus, self).__init__(docker_client)
+    pass
 
 
-class MySqlApp(service.BaseMySqlApp):
-    def __init__(self, status, docker_client):
-        super(MySqlApp, self).__init__(status, docker_client)
+class LocalSqlClient(service.BaseLocalSqlClient):
+    pass
 
-    def _get_gtid_executed(self):
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            return client.execute('SELECT @@global.gtid_executed').first()[0]
+
+class MySqlApp(service.BaseMySqlApp):
+    def __init__(self, status):
+        super(MySqlApp, self).__init__(status, LocalSqlClient,
+                                       KeepAliveConnection)
+
+    # DEPRECATED: Mantain for API Compatibility
+    def get_txn_count(self):
+        LOG.info(_("Retrieving latest txn id."))
+        txn_count = 0
+        with self.local_sql_client(self.get_engine()) as client:
+            result = client.execute('SELECT @@global.gtid_executed').first()
+            for uuid_set in result[0].split(','):
+                for interval in uuid_set.split(':')[1:]:
+                    if '-' in interval:
+                        iparts = interval.split('-')
+                        txn_count += int(iparts[1]) - int(iparts[0])
+                    else:
+                        txn_count += 1
+        return txn_count
 
     def _get_slave_status(self):
-        with mysql_util.SqlClient(self.get_engine()) as client:
+        with self.local_sql_client(self.get_engine()) as client:
             return client.execute('SHOW SLAVE STATUS').first()
 
     def _get_master_UUID(self):
         slave_status = self._get_slave_status()
         return slave_status and slave_status['Master_UUID'] or None
 
-    def get_latest_txn_id(self):
-        return self._get_gtid_executed()
+    def _get_gtid_executed(self):
+        with self.local_sql_client(self.get_engine()) as client:
+            return client.execute('SELECT @@global.gtid_executed').first()[0]
 
     def get_last_txn(self):
         master_UUID = self._get_master_UUID()
         last_txn_id = '0'
         gtid_executed = self._get_gtid_executed()
         for gtid_set in gtid_executed.split(','):
             uuid_set = gtid_set.split(':')
-            if str(uuid_set[0]) == str(master_UUID):
+            if uuid_set[0] == master_UUID:
                 last_txn_id = uuid_set[-1].split('-')[-1]
                 break
         return master_UUID, int(last_txn_id)
 
+    def get_latest_txn_id(self):
+        LOG.info(_("Retrieving latest txn id."))
+        return self._get_gtid_executed()
+
     def wait_for_txn(self, txn):
-        with mysql_util.SqlClient(self.get_engine()) as client:
+        LOG.info(_("Waiting on txn '%s'."), txn)
+        with self.local_sql_client(self.get_engine()) as client:
             client.execute("SELECT WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS('%s')"
                            % txn)
 
-    def get_backup_image(self):
-        """Get the actual container image based on datastore version.
-
-        For example, this method converts openstacktrove/db-backup-mysql:1.0.0
-        to openstacktrove/db-backup-mysql5.7:1.0.0
-
-        **deprecated**: this function is for backward compatibility.
-        """
-        image = cfg.get_configuration_property('backup_docker_image')
-        if not self._image_has_tag(image):
-            return super().get_backup_image()
-        else:
-            name, tag = image.rsplit(':', 1)
-            # Get minor version
-            cur_ver = semantic_version.Version.coerce(CONF.datastore_version)
-            minor_ver = f"{cur_ver.major}.{cur_ver.minor}"
-            return f"{name}{minor_ver}:{tag}"
-
-    def get_backup_strategy(self):
-        """Get backup strategy.
-
-        innobackupex was removed in Percona XtraBackup 8.0, use xtrabackup
-        instead.
-        """
-        strategy = cfg.get_configuration_property('backup_strategy')
-
-        mysql_8 = semantic_version.Version('8.0.0')
-        cur_ver = semantic_version.Version.coerce(CONF.datastore_version)
-        if cur_ver >= mysql_8:
-            strategy = 'xtrabackup'
-
-        return strategy
-
 
 class MySqlRootAccess(service.BaseMySqlRootAccess):
-    def __init__(self, app):
-        super(MySqlRootAccess, self).__init__(app)
+    def __init__(self):
+        super(MySqlRootAccess, self).__init__(LocalSqlClient,
+                                              MySqlApp(MySqlAppStatus.get()))
 
 
 class MySqlAdmin(service.BaseMySqlAdmin):
-    def __init__(self, app):
-        root_access = MySqlRootAccess(app)
-        super(MySqlAdmin, self).__init__(root_access, app)
+    def __init__(self):
+        super(MySqlAdmin, self).__init__(LocalSqlClient, MySqlRootAccess(),
+                                         MySqlApp)
+
+
+get_engine = MySqlApp.get_engine
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/mysql_common/service.py` & `trove-8.0.1/trove/guestagent/datastore/experimental/mongodb/service.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,911 +1,840 @@
-# Copyright 2020 Catalyst Cloud
+#   Copyright (c) 2014 Mirantis, Inc.
+#   All Rights Reserved.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
 
-import abc
 import os
-import re
 
 from oslo_log import log as logging
-from oslo_utils import encodeutils
-import sqlalchemy
-from sqlalchemy import event
-from sqlalchemy import exc
-from sqlalchemy.sql.expression import text
-import urllib
+from oslo_utils import netutils
+import pymongo
 
 from trove.common import cfg
-from trove.common.configurations import MySQLConfParser
-from trove.common import constants
-from trove.common.db.mysql import models
+from trove.common.db.mongodb import models
 from trove.common import exception
 from trove.common.i18n import _
-from trove.common import utils
+from trove.common import instance as ds_instance
+from trove.common.stream_codecs import JsonCodec, SafeYamlCodec
+from trove.common import utils as utils
 from trove.guestagent.common.configuration import ConfigurationManager
-from trove.guestagent.common.configuration import ImportOverrideStrategy
+from trove.guestagent.common.configuration import OneFileOverrideStrategy
 from trove.guestagent.common import guestagent_utils
 from trove.guestagent.common import operating_system
-from trove.guestagent.common import sql_query
+from trove.guestagent.datastore.experimental.mongodb import system
 from trove.guestagent.datastore import service
-from trove.guestagent.utils import docker as docker_util
-from trove.guestagent.utils import mysql as mysql_util
-from trove.instance import service_status
+
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
-ADMIN_USER_NAME = "os_admin"
-CONNECTION_STR_FORMAT = ("mysql+pymysql://%s:%s@localhost/?" +
-                         f"unix_socket={constants.MYSQL_HOST_SOCKET_PATH}"
-                         "/mysqld.sock")
-ENGINE = None
-INCLUDE_MARKER_OPERATORS = {
-    True: ">=",
-    False: ">"
-}
-MYSQL_CONFIG = "/etc/mysql/my.cnf"
-CNF_EXT = 'cnf'
-CNF_INCLUDE_DIR = '/etc/mysql/conf.d'
-CNF_MASTER = 'master-replication'
-CNF_SLAVE = 'slave-replication'
-
-BACKUP_LOG = re.compile(r'.*Backup successfully, checksum: (?P<checksum>.*), '
-                        r'location: (?P<location>.*)')
-
-
-class BaseMySqlAppStatus(service.BaseDbStatus):
-
-    def __init__(self, docker_client):
-        super(BaseMySqlAppStatus, self).__init__(docker_client)
-
-    def get_actual_db_status(self):
-        """Check database service status."""
-        status = docker_util.get_container_status(self.docker_client)
-        if status == "running":
-            root_pass = service.BaseDbApp.get_auth_password(file="root.cnf")
-            cmd = 'mysql -uroot -p%s -e "select 1;"' % root_pass
-            try:
-                docker_util.run_command(self.docker_client, cmd)
-                return service_status.ServiceStatuses.HEALTHY
-            except Exception as exc:
-                LOG.warning('Failed to run docker command, error: %s',
-                            str(exc))
-                container_log = docker_util.get_container_logs(
-                    self.docker_client, tail='all')
-                LOG.debug('container log: \n%s', '\n'.join(container_log))
-                return service_status.ServiceStatuses.RUNNING
-        elif status == "not running":
-            return service_status.ServiceStatuses.SHUTDOWN
-        elif status == "restarting":
-            return service_status.ServiceStatuses.SHUTDOWN
-        elif status == "paused":
-            return service_status.ServiceStatuses.PAUSED
-        elif status == "exited":
-            return service_status.ServiceStatuses.SHUTDOWN
-        elif status == "dead":
-            return service_status.ServiceStatuses.CRASHED
-        else:
-            return service_status.ServiceStatuses.UNKNOWN
+CONFIG_FILE = operating_system.file_discovery(system.CONFIG_CANDIDATES)
+MANAGER = CONF.datastore_manager if CONF.datastore_manager else 'mongodb'
 
+# Configuration group for clustering-related settings.
+CNF_CLUSTER = 'clustering'
 
-class BaseMySqlAdmin(object, metaclass=abc.ABCMeta):
-    """Handles administrative tasks on the MySQL database."""
+MONGODB_PORT = CONF.mongodb.mongodb_port
+CONFIGSVR_PORT = CONF.mongodb.configsvr_port
 
-    def __init__(self, mysql_root_access, mysql_app):
-        self.mysql_root_access = mysql_root_access
-        self.mysql_app = mysql_app
-
-    def _associate_dbs(self, user):
-        """Internal. Given a MySQLUser, populate its databases attribute."""
-        LOG.debug("Associating dbs to user %(name)s at %(host)s.",
-                  {'name': user.name, 'host': user.host})
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            q = sql_query.Query()
-            q.columns = ["grantee", "table_schema"]
-            q.tables = ["information_schema.SCHEMA_PRIVILEGES"]
-            q.group = ["grantee", "table_schema"]
-            q.where = ["privilege_type != 'USAGE'"]
-            t = text(str(q))
-            db_result = client.execute(t)
-            for db in db_result:
-                LOG.debug("\t db: %s.", db)
-                if db['grantee'] == "'%s'@'%s'" % (user.name, user.host):
-                    user.databases = db['table_schema']
 
-    def change_passwords(self, users):
-        """Change the passwords of one or more existing users."""
-        LOG.debug("Changing the password of some users.")
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            for item in users:
-                LOG.debug("Changing password for user %s.", item)
-                user_dict = {'_name': item['name'],
-                             '_host': item['host'],
-                             '_password': item['password']}
-                user = models.MySQLUser.deserialize(user_dict)
-                uu = sql_query.SetPassword(user.name, host=user.host,
-                                           new_password=user.password,
-                                           ds=CONF.datastore_manager,
-                                           ds_version=CONF.datastore_version)
-                t = text(str(uu))
-                client.execute(t)
-
-    def update_attributes(self, username, hostname, user_attrs):
-        """Change the attributes of an existing user."""
-        LOG.debug("Changing user attributes for user %s.", username)
-        user = self._get_user(username, hostname)
-
-        new_name = user_attrs.get('name')
-        new_host = user_attrs.get('host')
-        new_password = user_attrs.get('password')
-
-        if new_name or new_host or new_password:
-            with mysql_util.SqlClient(
-                    self.mysql_app.get_engine(), use_flush=True) as client:
-                if new_password is not None:
-                    uu = sql_query.SetPassword(
-                        user.name, host=user.host,
-                        new_password=new_password,
-                        ds=CONF.datastore_manager,
-                        ds_version=CONF.datastore_version)
-                    t = text(str(uu))
-                    client.execute(t)
-
-                if new_name or new_host:
-                    uu = sql_query.RenameUser(user.name, host=user.host,
-                                              new_user=new_name,
-                                              new_host=new_host)
-                    t = text(str(uu))
-                    client.execute(t)
-
-    def create_databases(self, databases):
-        """Create the list of specified databases."""
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            for item in databases:
-                mydb = models.MySQLSchema.deserialize(item)
-                mydb.check_create()
-                cd = sql_query.CreateDatabase(mydb.name,
-                                              mydb.character_set,
-                                              mydb.collate)
-                t = text(str(cd))
-                LOG.debug('Creating database, command: %s', str(cd))
-                client.execute(t)
+class MongoDBApp(object):
+    """Prepares DBaaS on a Guest container."""
+
+    def __init__(self):
+        self.state_change_wait_time = CONF.state_change_wait_time
+
+        revision_dir = guestagent_utils.build_file_path(
+            os.path.dirname(CONFIG_FILE),
+            ConfigurationManager.DEFAULT_STRATEGY_OVERRIDES_SUB_DIR)
+        self.configuration_manager = ConfigurationManager(
+            CONFIG_FILE, system.MONGO_USER, system.MONGO_USER,
+            SafeYamlCodec(default_flow_style=False),
+            requires_root=True,
+            override_strategy=OneFileOverrideStrategy(revision_dir))
+
+        self.is_query_router = False
+        self.is_cluster_member = False
+        self.status = MongoDBAppStatus()
+
+    def install_if_needed(self, packages):
+        """Prepare the guest machine with a MongoDB installation."""
+        LOG.info(_("Preparing Guest as MongoDB."))
+        if not system.PACKAGER.pkg_is_installed(packages):
+            LOG.debug("Installing packages: %s.", str(packages))
+            system.PACKAGER.pkg_install(packages, {}, system.TIME_OUT)
+        LOG.info(_("Finished installing MongoDB server."))
+
+    def _get_service_candidates(self):
+        if self.is_query_router:
+            return system.MONGOS_SERVICE_CANDIDATES
+        return system.MONGOD_SERVICE_CANDIDATES
+
+    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
+        self.status.stop_db_service(
+            self._get_service_candidates(), self.state_change_wait_time,
+            disable_on_boot=do_not_start_on_reboot, update_db=update_db)
 
-    def create_users(self, users):
-        """Create users and grant them privileges for the
-           specified databases.
-        """
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            for item in users:
-                user = models.MySQLUser.deserialize(item)
-                user.check_create()
+    def restart(self):
+        self.status.restart_db_service(
+            self._get_service_candidates(), self.state_change_wait_time)
 
-                cu = sql_query.CreateUser(user.name, host=user.host,
-                                          clear=user.password)
-                t = text(str(cu))
-                client.execute(t, **cu.keyArgs)
-
-                for database in user.databases:
-                    mydb = models.MySQLSchema.deserialize(database)
-                    g = sql_query.Grant(permissions='ALL', database=mydb.name,
-                                        user=user.name, host=user.host)
-                    t = text(str(g))
-                    LOG.debug('Creating user, command: %s', str(g))
-                    client.execute(t)
+    def start_db(self, update_db=False):
+        self.status.start_db_service(
+            self._get_service_candidates(), self.state_change_wait_time,
+            enable_on_boot=True, update_db=update_db)
 
-    def delete_database(self, database):
-        """Delete the specified database."""
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            mydb = models.MySQLSchema.deserialize(database)
-            mydb.check_delete()
-            dd = sql_query.DropDatabase(mydb.name)
-            t = text(str(dd))
-            client.execute(t)
+    def update_overrides(self, context, overrides, remove=False):
+        if overrides:
+            self.configuration_manager.apply_user_override(overrides)
 
-    def delete_user(self, user):
-        """Delete the specified user."""
-        mysql_user = models.MySQLUser.deserialize(user)
-        mysql_user.check_delete()
-        self.delete_user_by_name(mysql_user.name, mysql_user.host)
-
-    def delete_user_by_name(self, name, host='%'):
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            du = sql_query.DropUser(name, host=host)
-            t = text(str(du))
-            LOG.debug("delete_user_by_name: %s", t)
-            client.execute(t)
+    def remove_overrides(self):
+        self.configuration_manager.remove_user_override()
 
-    def get_user(self, username, hostname):
-        user = self._get_user(username, hostname)
-        if not user:
-            return None
-        return user.serialize()
+    def start_db_with_conf_changes(self, config_contents):
+        LOG.info(_('Starting MongoDB with configuration changes.'))
+        if self.status.is_running:
+            format = 'Cannot start_db_with_conf_changes because status is %s.'
+            LOG.debug(format, self.status)
+            raise RuntimeError(format % self.status)
+        LOG.info(_("Initiating config."))
+        self.configuration_manager.save_configuration(config_contents)
+        # The configuration template has to be updated with
+        # guestagent-controlled settings.
+        self.apply_initial_guestagent_configuration(
+            None, mount_point=system.MONGODB_MOUNT_POINT)
+        self.start_db(True)
+
+    def apply_initial_guestagent_configuration(
+            self, cluster_config, mount_point=None):
+        LOG.debug("Applying initial configuration.")
+
+        # Mongodb init scripts assume the PID-file path is writable by the
+        # database service.
+        # See: https://jira.mongodb.org/browse/SERVER-20075
+        self._initialize_writable_run_dir()
 
-    def _get_user(self, username, hostname):
-        """Return a single user matching the criteria."""
-        user = None
-        try:
-            # Could possibly throw a ValueError here.
-            user = models.MySQLUser(name=username)
-            user.check_reserved()
-        except ValueError as ve:
-            LOG.exception("Error Getting user information")
-            err_msg = encodeutils.exception_to_unicode(ve)
-            raise exception.BadRequest(_("Username %(user)s is not valid"
-                                         ": %(reason)s") %
-                                       {'user': username, 'reason': err_msg}
-                                       )
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            q = sql_query.Query()
-            q.columns = ['User', 'Host']
-            q.tables = ['mysql.user']
-            q.where = ["Host != 'localhost'",
-                       "User = '%s'" % username,
-                       "Host = '%s'" % hostname]
-            q.order = ['User', 'Host']
-            t = text(str(q))
-            result = client.execute(t).fetchall()
-            LOG.debug("Getting user information %s.", result)
-            if len(result) != 1:
-                return None
-            found_user = result[0]
-            user.host = found_user['Host']
-            self._associate_dbs(user)
-            return user
-
-    def grant_access(self, username, hostname, databases):
-        """Grant a user permission to use a given database."""
-        user = self._get_user(username, hostname)
-        mydb = None  # cache the model as we just want name validation
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            for database in databases:
-                try:
-                    if mydb:
-                        mydb.name = database
-                    else:
-                        mydb = models.MySQLSchema(name=database)
-                        mydb.check_reserved()
-                except ValueError:
-                    LOG.exception("Error granting access")
-                    raise exception.BadRequest(_(
-                        "Grant access to %s is not allowed") % database)
-
-                g = sql_query.Grant(permissions='ALL', database=mydb.name,
-                                    user=user.name, host=user.host,
-                                    hashed=user.password)
-                t = text(str(g))
-                client.execute(t)
+        self.configuration_manager.apply_system_override(
+            {'processManagement.fork': False,
+             'processManagement.pidFilePath': system.MONGO_PID_FILE,
+             'systemLog.destination': 'file',
+             'systemLog.path': system.MONGO_LOG_FILE,
+             'systemLog.logAppend': True
+             })
+
+        if mount_point:
+            self.configuration_manager.apply_system_override(
+                {'storage.dbPath': mount_point})
 
-    def is_root_enabled(self):
-        """Return True if root access is enabled; False otherwise."""
-        return self.mysql_root_access.is_root_enabled()
+        if cluster_config is not None:
+            self._configure_as_cluster_instance(cluster_config)
+        else:
+            self._configure_network(MONGODB_PORT)
 
-    def enable_root(self, root_password=None):
-        """Enable the root user global access and/or
-           reset the root password.
+    def _initialize_writable_run_dir(self):
+        """Create a writable directory for Mongodb's runtime data
+        (e.g. PID-file).
         """
-        return self.mysql_root_access.enable_root(root_password)
-
-    def disable_root(self):
-        """Disable the root user global access
+        mongodb_run_dir = os.path.dirname(system.MONGO_PID_FILE)
+        LOG.debug("Initializing a runtime directory: %s", mongodb_run_dir)
+        operating_system.create_directory(
+            mongodb_run_dir, user=system.MONGO_USER, group=system.MONGO_USER,
+            force=True, as_root=True)
+
+    def _configure_as_cluster_instance(self, cluster_config):
+        """Configure this guest as a cluster instance and return its
+        new status.
         """
-        return self.mysql_root_access.disable_root()
+        if cluster_config['instance_type'] == "query_router":
+            self._configure_as_query_router()
+        elif cluster_config["instance_type"] == "config_server":
+            self._configure_as_config_server()
+        elif cluster_config["instance_type"] == "member":
+            self._configure_as_cluster_member(
+                cluster_config['replica_set_name'])
+        else:
+            LOG.error(_("Bad cluster configuration; instance type "
+                        "given as %s."), cluster_config['instance_type'])
+            return ds_instance.ServiceStatuses.FAILED
+
+        if 'key' in cluster_config:
+            self._configure_cluster_security(cluster_config['key'])
+
+    def _configure_as_query_router(self):
+        LOG.info(_("Configuring instance as a cluster query router."))
+        self.is_query_router = True
+
+        # FIXME(pmalik): We should really have a separate configuration
+        # template for the 'mongos' process.
+        # Remove all storage configurations from the template.
+        # They apply only to 'mongod' processes.
+        # Already applied overrides will be integrated into the base file and
+        # their current groups removed.
+        config = guestagent_utils.expand_dict(
+            self.configuration_manager.parse_configuration())
+        if 'storage' in config:
+            LOG.debug("Removing 'storage' directives from the configuration "
+                      "template.")
+            del config['storage']
+            self.configuration_manager.save_configuration(
+                guestagent_utils.flatten_dict(config))
 
-    def list_databases(self, limit=None, marker=None, include_marker=False):
-        """List databases on this mysql instance."""
-        LOG.info("Listing Databases")
-        ignored_database_names = "'%s'" % "', '".join(cfg.get_ignored_dbs())
-        LOG.debug("The following database names are on ignore list and will "
-                  "be omitted from the listing: %s", ignored_database_names)
-        databases = []
-        with mysql_util.SqlClient(self.mysql_app.get_engine()) as client:
-            # If you have an external volume mounted at /var/lib/mysql
-            # the lost+found directory will show up in mysql as a database
-            # which will create errors if you try to do any database ops
-            # on it.  So we remove it here if it exists.
-            q = sql_query.Query()
-            q.columns = [
-                'schema_name as name',
-                'default_character_set_name as charset',
-                'default_collation_name as collation',
-            ]
-            q.tables = ['information_schema.schemata']
-            q.where = ["schema_name NOT IN (" + ignored_database_names + ")"]
-            q.order = ['schema_name ASC']
-            if limit:
-                q.limit = limit + 1
-            if marker:
-                q.where.append("schema_name %s '%s'" %
-                               (INCLUDE_MARKER_OPERATORS[include_marker],
-                                marker))
-            t = text(str(q))
-            database_names = client.execute(t)
-            next_marker = None
-            for count, database in enumerate(database_names):
-                if limit is not None and count >= limit:
-                    break
-                mysql_db = models.MySQLSchema(name=database[0],
-                                              character_set=database[1],
-                                              collate=database[2])
-                next_marker = mysql_db.name
-                databases.append(mysql_db.serialize())
-
-        LOG.info("databases = %s", str(databases))
-        if limit is not None and database_names.rowcount <= limit:
-            next_marker = None
-        return databases, next_marker
+        # Apply 'mongos' configuration.
+        self._configure_network(MONGODB_PORT)
+        self.configuration_manager.apply_system_override(
+            {'sharding.configDB': ''}, CNF_CLUSTER)
 
-    def list_users(self, limit=None, marker=None, include_marker=False):
-        """List users that have access to the database."""
-        '''
-        SELECT
-            User,
-            Host,
-            Marker
-        FROM
-            (SELECT
-                User,
-                Host,
-                CONCAT(User, '@', Host) as Marker
-            FROM mysql.user
-            ORDER BY 1, 2) as innerquery
-        WHERE
-            Marker > :marker
-        ORDER BY
-            Marker
-        LIMIT :limit;
-        '''
-        LOG.info("Listing Users")
-        ignored_user_names = "'%s'" % "', '".join(cfg.get_ignored_users())
-        LOG.debug("The following user names are on ignore list and will "
-                  "be omitted from the listing: %s", ignored_user_names)
-        users = []
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            iq = sql_query.Query()  # Inner query.
-            iq.columns = ['User', 'Host', "CONCAT(User, '@', Host) as Marker"]
-            iq.tables = ['mysql.user']
-            iq.order = ['User', 'Host']
-            innerquery = str(iq).rstrip(';')
-
-            oq = sql_query.Query()  # Outer query.
-            oq.columns = ['User', 'Host', 'Marker']
-            oq.tables = ['(%s) as innerquery' % innerquery]
-            oq.where = [
-                "Host != 'localhost'",
-                "User NOT IN (" + ignored_user_names + ")"]
-            oq.order = ['Marker']
-            if marker:
-                oq.where.append("Marker %s '%s'" %
-                                (INCLUDE_MARKER_OPERATORS[include_marker],
-                                 marker))
-            if limit:
-                oq.limit = limit + 1
-            t = text(str(oq))
-            result = client.execute(t)
-            next_marker = None
-            for count, row in enumerate(result):
-                if limit is not None and count >= limit:
-                    break
-                LOG.debug("user = %s", str(row))
-                mysql_user = models.MySQLUser(name=row['User'],
-                                              host=row['Host'])
-                mysql_user.check_reserved()
-                self._associate_dbs(mysql_user)
-                next_marker = row['Marker']
-                users.append(mysql_user.serialize())
-        if limit is not None and result.rowcount <= limit:
-            next_marker = None
-        LOG.info("users = %s", str(users))
-
-        return users, next_marker
-
-    def revoke_access(self, username, hostname, database):
-        """Revoke a user's permission to use a given database."""
-        user = self._get_user(username, hostname)
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            r = sql_query.Revoke(database=database,
-                                 user=user.name,
-                                 host=user.host)
-            t = text(str(r))
-            client.execute(t)
-
-    def list_access(self, username, hostname):
-        """Show all the databases to which the user has more than
-           USAGE granted.
-        """
-        user = self._get_user(username, hostname)
-        return user.databases
+    def _configure_as_config_server(self):
+        LOG.info(_("Configuring instance as a cluster config server."))
+        self._configure_network(CONFIGSVR_PORT)
+        self.configuration_manager.apply_system_override(
+            {'sharding.clusterRole': 'configsvr'}, CNF_CLUSTER)
 
+    def _configure_as_cluster_member(self, replica_set_name):
+        LOG.info(_("Configuring instance as a cluster member."))
+        self.is_cluster_member = True
+        self._configure_network(MONGODB_PORT)
+        # we don't want these thinking they are in a replica set yet
+        # as that would prevent us from creating the admin user,
+        # so start mongo before updating the config.
+        # mongo will be started by the cluster taskmanager
+        self.start_db()
+        self.configuration_manager.apply_system_override(
+            {'replication.replSetName': replica_set_name}, CNF_CLUSTER)
 
-class BaseMySqlApp(service.BaseDbApp):
-    _configuration_manager = None
+    def _configure_cluster_security(self, key_value):
+        """Force cluster key-file-based authentication.
 
-    @property
-    def configuration_manager(self):
-        if self._configuration_manager:
-            return self._configuration_manager
-
-        self._configuration_manager = ConfigurationManager(
-            MYSQL_CONFIG, CONF.database_service_uid, CONF.database_service_uid,
-            service.BaseDbApp.CFG_CODEC, requires_root=True,
-            override_strategy=ImportOverrideStrategy(CNF_INCLUDE_DIR, CNF_EXT)
-        )
-        return self._configuration_manager
+        This will enabled RBAC.
+        """
+        # Store the cluster member authentication key.
+        self.store_key(key_value)
 
-    def get_engine(self):
-        """Create the default engine with the updated admin user.
+        self.configuration_manager.apply_system_override(
+            {'security.clusterAuthMode': 'keyFile',
+             'security.keyFile': self.get_key_file()}, CNF_CLUSTER)
 
-        If admin user not created yet, use root instead.
+    def _configure_network(self, port=None):
+        """Make the service accessible at a given (or default if not) port.
         """
-        global ENGINE
-        if ENGINE:
-            return ENGINE
-
+        instance_ip = netutils.get_my_ipv4()
+        bind_interfaces_string = ','.join([instance_ip, '127.0.0.1'])
+        options = {'net.bindIp': bind_interfaces_string}
+        if port is not None:
+            guestagent_utils.update_dict({'net.port': port}, options)
+
+        self.configuration_manager.apply_system_override(options)
+        self.status.set_host(instance_ip, port=port)
+
+    def clear_storage(self):
+        mount_point = "/var/lib/mongodb/*"
+        LOG.debug("Clearing storage at %s.", mount_point)
         try:
-            user = ADMIN_USER_NAME
-            password = self.get_auth_password()
-        except exception.UnprocessableEntity:
-            # os_admin user not created yet
-            user = 'root'
-            password = self.get_auth_password(file="root.cnf")
-
-        ENGINE = sqlalchemy.create_engine(
-            CONNECTION_STR_FORMAT % (user,
-                                     urllib.parse.quote(password.strip())),
-            pool_recycle=120, echo=CONF.sql_query_logging
-        )
-        event.listen(ENGINE, 'checkout', mysql_util.connection_checkout)
-
-        return ENGINE
+            operating_system.remove(mount_point, force=True, as_root=True)
+        except exception.ProcessExecutionError:
+            LOG.exception(_("Error clearing storage."))
+
+    def _has_config_db(self):
+        value_string = self.configuration_manager.get_value(
+            'sharding', {}).get('configDB')
+
+        return value_string is not None
+
+    # FIXME(pmalik): This method should really be called 'set_config_servers'.
+    # The current name suggests it adds more config servers, but it
+    # rather replaces the existing ones.
+    def add_config_servers(self, config_server_hosts):
+        """Set config servers on a query router (mongos) instance.
+        """
+        config_servers_string = ','.join(['%s:%s' % (host, CONFIGSVR_PORT)
+                                          for host in config_server_hosts])
+        LOG.info(_("Setting config servers: %s"), config_servers_string)
+        self.configuration_manager.apply_system_override(
+            {'sharding.configDB': config_servers_string}, CNF_CLUSTER)
+        self.start_db(True)
 
-    def execute_sql(self, sql_statement, use_flush=False):
-        LOG.debug("Executing SQL: %s", sql_statement)
-        with mysql_util.SqlClient(
-                self.get_engine(), use_flush=use_flush) as client:
-            return client.execute(sql_statement)
+    def add_shard(self, replica_set_name, replica_set_member):
+        """
+        This method is used by query router (mongos) instances.
+        """
+        url = "%(rs)s/%(host)s:%(port)s"\
+              % {'rs': replica_set_name,
+                 'host': replica_set_member,
+                 'port': MONGODB_PORT}
+        MongoDBAdmin().add_shard(url)
 
-    def get_data_dir(self):
-        return self.configuration_manager.get_value(
-            'datadir', section=MySQLConfParser.SERVER_CONF_SECTION)
+    def add_members(self, members):
+        """
+        This method is used by a replica-set member instance.
+        """
+        def check_initiate_status():
+            """
+            This method is used to verify replica-set status.
+            """
+            status = MongoDBAdmin().get_repl_status()
+
+            if((status["ok"] == 1) and
+               (status["members"][0]["stateStr"] == "PRIMARY") and
+               (status["myState"] == 1)):
+                return True
+            else:
+                return False
 
-    def set_data_dir(self, value):
+        def check_rs_status():
+            """
+            This method is used to verify replica-set status.
+            """
+            status = MongoDBAdmin().get_repl_status()
+            primary_count = 0
+
+            if status["ok"] != 1:
+                return False
+            if len(status["members"]) != (len(members) + 1):
+                return False
+            for rs_member in status["members"]:
+                if rs_member["state"] not in [1, 2, 7]:
+                    return False
+                if rs_member["health"] != 1:
+                    return False
+                if rs_member["state"] == 1:
+                    primary_count += 1
+
+            return primary_count == 1
+
+        MongoDBAdmin().rs_initiate()
+        # TODO(ramashri) see if hardcoded values can be removed
+        utils.poll_until(check_initiate_status, sleep_time=30,
+                         time_out=CONF.mongodb.add_members_timeout)
+
+        # add replica-set members
+        MongoDBAdmin().rs_add_members(members)
+        # TODO(ramashri) see if hardcoded values can be removed
+        utils.poll_until(check_rs_status, sleep_time=10,
+                         time_out=CONF.mongodb.add_members_timeout)
+
+    def _set_localhost_auth_bypass(self, enabled):
+        """When active, the localhost exception allows connections from the
+        localhost interface to create the first user on the admin database.
+        The exception applies only when there are no users created in the
+        MongoDB instance.
+        """
         self.configuration_manager.apply_system_override(
-            {MySQLConfParser.SERVER_CONF_SECTION: {'datadir': value}})
+            {'setParameter': {'enableLocalhostAuthBypass': enabled}})
 
-    def _create_admin_user(self, client, password):
-        """
-        Create a os_admin user with a random password
-        with all privileges similar to the root user.
-        """
-        LOG.info("Creating Trove admin user '%s'.", ADMIN_USER_NAME)
-        host = "localhost"
-        try:
-            cu = sql_query.CreateUser(ADMIN_USER_NAME, host=host,
-                                      clear=password)
-            t = text(str(cu))
-            client.execute(t, **cu.keyArgs)
-        except (exc.OperationalError, exc.InternalError) as err:
-            # Ignore, user is already created, just reset the password
-            # (user will already exist in a restore from backup)
-            LOG.debug(err)
-            uu = sql_query.SetPassword(
-                ADMIN_USER_NAME, host=host, new_password=password,
-                ds=CONF.datastore_manager, ds_version=CONF.datastore_version
-            )
-            t = text(str(uu))
-            client.execute(t)
+    def list_all_dbs(self):
+        return MongoDBAdmin().list_database_names()
 
-        g = sql_query.Grant(permissions='ALL', user=ADMIN_USER_NAME,
-                            host=host, grant_option=True)
-        t = text(str(g))
-        client.execute(t)
-        LOG.info("Trove admin user '%s' created.", ADMIN_USER_NAME)
+    def db_data_size(self, db_name):
+        schema = models.MongoDBSchema(db_name)
+        return MongoDBAdmin().db_stats(schema.serialize())['dataSize']
+
+    def admin_cmd_auth_params(self):
+        return MongoDBAdmin().cmd_admin_auth_params
+
+    def get_key_file(self):
+        return system.MONGO_KEY_FILE
+
+    def get_key(self):
+        return operating_system.read_file(
+            system.MONGO_KEY_FILE, as_root=True).rstrip()
+
+    def store_key(self, key):
+        """Store the cluster key."""
+        LOG.debug('Storing key for MongoDB cluster.')
+        operating_system.write_file(system.MONGO_KEY_FILE, key, as_root=True)
+        operating_system.chmod(system.MONGO_KEY_FILE,
+                               operating_system.FileMode.SET_USR_RO,
+                               as_root=True)
+        operating_system.chown(system.MONGO_KEY_FILE,
+                               system.MONGO_USER, system.MONGO_USER,
+                               as_root=True)
+
+    def store_admin_password(self, password):
+        LOG.debug('Storing admin password.')
+        creds = MongoDBCredentials(username=system.MONGO_ADMIN_NAME,
+                                   password=password)
+        creds.write(system.MONGO_ADMIN_CREDS_FILE)
+        return creds
+
+    def create_admin_user(self, password):
+        """Create the admin user while the localhost exception is active."""
+        LOG.debug('Creating the admin user.')
+        creds = self.store_admin_password(password)
+        user = models.MongoDBUser(name='admin.%s' % creds.username,
+                                  password=creds.password)
+        user.roles = system.MONGO_ADMIN_ROLES
+        # the driver engine is already cached, but we need to change it it
+        with MongoDBClient(None, host='localhost',
+                           port=MONGODB_PORT) as client:
+            MongoDBAdmin().create_validated_user(user, client=client)
+        # now revert to the normal engine
+        self.status.set_host(host=netutils.get_my_ipv4(),
+                             port=MONGODB_PORT)
+        LOG.debug('Created admin user.')
 
     def secure(self):
-        LOG.info("Securing MySQL now.")
+        """Create the Trove admin user.
 
-        root_pass = self.get_auth_password(file="root.cnf")
-        admin_password = utils.generate_random_password()
+        The service should not be running at this point.
+        This will enable role-based access control (RBAC) by default.
+        """
+        if self.status.is_running:
+            raise RuntimeError(_("Cannot secure the instance. "
+                                 "The service is still running."))
 
-        engine = sqlalchemy.create_engine(
-            CONNECTION_STR_FORMAT % ('root', root_pass), echo=True)
-        with mysql_util.SqlClient(engine) as client:
-            self._create_admin_user(client, admin_password)
-
-        engine = sqlalchemy.create_engine(
-            CONNECTION_STR_FORMAT % (ADMIN_USER_NAME,
-                                     urllib.parse.quote(admin_password)),
-            echo=True)
-        with mysql_util.SqlClient(engine, use_flush=True) as client:
-            self._remove_anonymous_user(client)
-
-        self.save_password(ADMIN_USER_NAME, admin_password)
-        LOG.info("MySQL secure complete.")
-
-    def secure_root(self):
-        with mysql_util.SqlClient(self.get_engine(), use_flush=True) as client:
-            self._remove_remote_root_access(client)
-
-    def _remove_anonymous_user(self, client):
-        LOG.debug("Removing anonymous user.")
-        t = text(sql_query.REMOVE_ANON)
-        client.execute(t)
-        LOG.debug("Anonymous user removed.")
-
-    def _remove_remote_root_access(self, client):
-        LOG.debug("Removing remote root access.")
-        t = text(sql_query.REMOVE_ROOT)
-        client.execute(t)
-        LOG.debug("Root remote access removed.")
+        try:
+            self.configuration_manager.apply_system_override(
+                {'security.authorization': 'enabled'})
+            self._set_localhost_auth_bypass(True)
+            self.start_db(update_db=False)
+            password = utils.generate_random_password()
+            self.create_admin_user(password)
+            LOG.debug("MongoDB secure complete.")
+        finally:
+            self._set_localhost_auth_bypass(False)
+            self.stop_db()
 
-    def update_overrides(self, overrides):
-        if overrides:
-            self.configuration_manager.apply_user_override(
-                {MySQLConfParser.SERVER_CONF_SECTION: overrides})
+    def get_configuration_property(self, name, default=None):
+        """Return the value of a MongoDB configuration property.
+        """
+        return self.configuration_manager.get_value(name, default)
 
-    def apply_overrides(self, overrides):
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            for k, v in overrides.items():
-                byte_value = guestagent_utils.to_bytes(v)
-                q = sql_query.SetServerVariable(key=k, value=byte_value)
-                t = text(str(q))
-                try:
-                    client.execute(t)
-                except exc.OperationalError:
-                    output = {'key': k, 'value': byte_value}
-                    LOG.error("Unable to set %(key)s with value %(value)s.",
-                              output)
-
-    def start_db(self, update_db=False, ds_version=None, command=None,
-                 extra_volumes=None):
-        """Start and wait for database service."""
-        docker_image = CONF.get(CONF.datastore_manager).docker_image
-        image = (f'{docker_image}:latest' if not ds_version else
-                 f'{docker_image}:{ds_version}')
-        command = command if command else ''
+    def prep_primary(self):
+        # Prepare the primary member of a replica set.
+        password = utils.generate_random_password()
+        self.create_admin_user(password)
+        self.restart()
 
-        try:
-            root_pass = self.get_auth_password(file="root.cnf")
-        except exception.UnprocessableEntity:
-            root_pass = utils.generate_random_password()
-
-        # Get uid and gid
-        user = "%s:%s" % (CONF.database_service_uid, CONF.database_service_uid)
-
-        # Create folders for mysql on localhost
-        for folder in ['/etc/mysql', constants.MYSQL_HOST_SOCKET_PATH,
-                       '/etc/mysql/mysql.conf.d']:
-            operating_system.ensure_directory(
-                folder, user=CONF.database_service_uid,
-                group=CONF.database_service_uid, force=True,
-                as_root=True)
-
-        volumes = {
-            "/etc/mysql": {"bind": "/etc/mysql", "mode": "rw"},
-            constants.MYSQL_HOST_SOCKET_PATH: {"bind": "/var/run/mysqld",
-                                               "mode": "rw"},
-            "/var/lib/mysql": {"bind": "/var/lib/mysql", "mode": "rw"},
-        }
-        if extra_volumes:
-            volumes.update(extra_volumes)
-
-        # Expose ports
-        ports = {}
-        tcp_ports = cfg.get_configuration_property('tcp_ports')
-        for port_range in tcp_ports:
-            for port in port_range:
-                ports[f'{port}/tcp'] = port
-
-        if CONF.network_isolation and \
-                os.path.exists(constants.ETH1_CONFIG_PATH):
-            network_mode = constants.DOCKER_HOST_NIC_MODE
+    @property
+    def replica_set_name(self):
+        return MongoDBAdmin().get_repl_status()['set']
+
+    @property
+    def admin_password(self):
+        creds = MongoDBCredentials()
+        creds.read(system.MONGO_ADMIN_CREDS_FILE)
+        return creds.password
+
+    def is_shard_active(self, replica_set_name):
+        shards = MongoDBAdmin().list_active_shards()
+        if replica_set_name in [shard['_id'] for shard in shards]:
+            LOG.debug('Replica set %s is active.', replica_set_name)
+            return True
         else:
-            network_mode = constants.DOCKER_BRIDGE_MODE
+            LOG.debug('Replica set %s is not active.', replica_set_name)
+            return False
 
-        try:
-            docker_util.start_container(
-                self.docker_client,
-                image,
-                volumes=volumes,
-                network_mode=network_mode,
-                ports=ports,
-                user=user,
-                environment={
-                    "MYSQL_ROOT_PASSWORD": root_pass,
-                    "MYSQL_INITDB_SKIP_TZINFO": 1,
-                },
-                command=command
-            )
 
-            # Save root password
-            LOG.debug("Saving root credentials to local host.")
-            self.save_password('root', root_pass)
-        except Exception:
-            LOG.exception("Failed to start mysql")
-            raise exception.TroveError(_("Failed to start mysql"))
+class MongoDBAppStatus(service.BaseDbStatus):
 
-        if not self.status.wait_for_status(
-            service_status.ServiceStatuses.HEALTHY,
-            CONF.state_change_wait_time, update_db
-        ):
-            raise exception.TroveError(_("Failed to start mysql"))
-
-    def wipe_ib_logfiles(self):
-        """Destroys the iblogfiles.
-
-        If for some reason the selected log size in the conf changes from the
-        current size of the files MySQL will fail to start, so we delete the
-        files to be safe.
-        """
-        for index in range(2):
-            try:
-                # On restarts, sometimes these are wiped. So it can be a race
-                # to have MySQL start up before it's restarted and these have
-                # to be deleted. That's why its ok if they aren't found and
-                # that is why we use the "force" option to "remove".
-                operating_system.remove("%s/ib_logfile%d"
-                                        % (self.get_data_dir(), index),
-                                        force=True, as_root=True)
-            except exception.ProcessExecutionError:
-                LOG.exception("Could not delete logfile.")
-                raise
+    def __init__(self, host='localhost', port=None):
+        super(MongoDBAppStatus, self).__init__()
+        self.set_host(host, port=port)
 
-    def restart(self):
-        LOG.info("Restarting mysql")
-
-        # Ensure folders permission for database.
-        for folder in ['/etc/mysql', constants.MYSQL_HOST_SOCKET_PATH,
-                       '/etc/mysql/mysql.conf.d']:
-            operating_system.ensure_directory(
-                folder, user=CONF.database_service_uid,
-                group=CONF.database_service_uid, force=True,
-                as_root=True)
+    def set_host(self, host, port=None):
+        # This forces refresh of the 'pymongo' engine cached in the
+        # MongoDBClient class.
+        # Authentication is not required to check the server status.
+        MongoDBClient(None, host=host, port=port)
 
+    def _get_actual_db_status(self):
         try:
-            docker_util.restart_container(self.docker_client)
+            with MongoDBClient(None) as client:
+                client.server_info()
+            return ds_instance.ServiceStatuses.RUNNING
+        except (pymongo.errors.ServerSelectionTimeoutError,
+                pymongo.errors.AutoReconnect):
+            return ds_instance.ServiceStatuses.SHUTDOWN
         except Exception:
-            LOG.exception("Failed to restart mysql")
-            raise exception.TroveError("Failed to restart mysql")
+            LOG.exception(_("Error getting MongoDB status."))
 
-        if not self.status.wait_for_status(
-            service_status.ServiceStatuses.HEALTHY,
-            CONF.state_change_wait_time, update_db=True
-        ):
-            raise exception.TroveError("Failed to start mysql")
-
-        LOG.info("Finished restarting mysql")
-
-    def restore_backup(self, context, backup_info, restore_location):
-        backup_id = backup_info['id']
-        storage_driver = CONF.storage_strategy
-        backup_driver = self.get_backup_strategy()
-        user_token = context.auth_token
-        auth_url = CONF.service_credentials.auth_url
-        region_name = CONF.service_credentials.region_name
-        user_tenant = context.project_id
-        image = self.get_backup_image()
-        name = 'db_restore'
-        volumes = {'/var/lib/mysql': {'bind': '/var/lib/mysql', 'mode': 'rw'}}
-
-        command = (
-            f'/usr/bin/python3 main.py --nobackup '
-            f'--storage-driver={storage_driver} --driver={backup_driver} '
-            f'--os-token={user_token} --os-auth-url={auth_url} '
-            f'--os-tenant-id={user_tenant} --os-region-name={region_name} '
-            f'--restore-from={backup_info["location"]} '
-            f'--restore-checksum={backup_info["checksum"]}'
-        )
-        if CONF.backup_aes_cbc_key:
-            command = (f"{command} "
-                       f"--backup-encryption-key={CONF.backup_aes_cbc_key}")
-
-        LOG.debug('Stop the database and clean up the data before restore '
-                  'from %s', backup_id)
-        self.stop_db()
-        operating_system.chmod(restore_location,
-                               operating_system.FileMode.SET_FULL,
-                               as_root=True)
-        utils.clean_out(restore_location)
+        return ds_instance.ServiceStatuses.SHUTDOWN
 
-        # Start to run restore inside a separate docker container
-        LOG.info('Starting to restore backup %s, command: %s', backup_id,
-                 command)
-        output, ret = docker_util.run_container(
-            self.docker_client, image, name,
-            volumes=volumes, command=command)
-        result = output[-1]
-        if not ret:
-            msg = f'Failed to run restore container, error: {result}'
-            LOG.error(msg)
-            raise Exception(msg)
-
-        LOG.debug('Deleting ib_logfile files after restore from backup %s',
-                  backup_id)
-        operating_system.chown(restore_location, CONF.database_service_uid,
-                               CONF.database_service_uid, force=True,
-                               as_root=True)
-        self.wipe_ib_logfiles()
+    def cleanup_stalled_db_services(self):
+        pid, err = utils.execute_with_timeout(system.FIND_PID, shell=True)
+        utils.execute_with_timeout(system.MONGODB_KILL % pid, shell=True)
 
-    def exists_replication_source_overrides(self):
-        return self.configuration_manager.has_system_override(CNF_MASTER)
 
-    def write_replication_source_overrides(self, overrideValues):
-        self.configuration_manager.apply_system_override(overrideValues,
-                                                         CNF_MASTER)
-
-    def write_replication_replica_overrides(self, overrideValues):
-        self.configuration_manager.apply_system_override(overrideValues,
-                                                         CNF_SLAVE)
-
-    def remove_replication_source_overrides(self):
-        self.configuration_manager.remove_system_override(CNF_MASTER)
-
-    def remove_replication_replica_overrides(self):
-        self.configuration_manager.remove_system_override(CNF_SLAVE)
-
-    def grant_replication_privilege(self, replication_user):
-        LOG.info("Granting replication slave privilege for %s",
-                 replication_user['name'])
-
-        with mysql_util.SqlClient(self.get_engine(), use_flush=True) as client:
-            g = sql_query.Grant(permissions=['REPLICATION SLAVE'],
-                                user=replication_user['name'],
-                                clear=replication_user['password'])
-
-            t = text(str(g))
-            client.execute(t)
-
-    def get_port(self):
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            result = client.execute('SELECT @@port').first()
-            return result[0]
-
-    def wait_for_slave_status(self, status, client, max_time):
-        def verify_slave_status():
-            ret = client.execute(
-                "SELECT SERVICE_STATE FROM "
-                "performance_schema.replication_connection_status").first()
-            if not ret:
-                actual_status = 'OFF'
-            else:
-                actual_status = ret[0]
-            return actual_status.upper() == status.upper()
+class MongoDBAdmin(object):
+    """Handles administrative tasks on MongoDB."""
 
-        LOG.debug("Waiting for slave status %s with timeout %s",
-                  status, max_time)
-        try:
-            utils.poll_until(verify_slave_status, sleep_time=3,
-                             time_out=max_time)
-            LOG.info("Replication status: %s.", status)
-        except exception.PollTimeOut:
-            raise RuntimeError(
-                _("Replication is not %(status)s after %(max)d seconds.") % {
-                    'status': status.lower(), 'max': max_time})
-
-    def start_slave(self):
-        LOG.info("Starting slave replication.")
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            client.execute('START SLAVE')
-            self.wait_for_slave_status("ON", client, 180)
-
-    def stop_slave(self, for_failover):
-        LOG.info("Stopping slave replication.")
-
-        replication_user = None
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            result = client.execute('SHOW SLAVE STATUS')
-            replication_user = result.first()['Master_User']
-            client.execute('STOP SLAVE')
-            client.execute('RESET SLAVE ALL')
-            self.wait_for_slave_status('OFF', client, 180)
-            if not for_failover:
-                client.execute('DROP USER IF EXISTS ' + replication_user)
-
-        return {
-            'replication_user': replication_user
-        }
-
-    def stop_master(self):
-        LOG.info("Stopping replication master.")
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            client.execute('RESET MASTER')
-
-    def make_read_only(self, read_only):
-        with mysql_util.SqlClient(self.get_engine()) as client:
-            q = "set global read_only = %s" % read_only
-            client.execute(text(str(q)))
-
-    def upgrade(self, upgrade_info):
-        """Upgrade the database."""
-        new_version = upgrade_info.get('datastore_version')
-        if new_version == CONF.datastore_version:
-            return
-
-        LOG.info('Stopping db container for upgrade')
-        self.stop_db()
-
-        LOG.info('Deleting db container for upgrade')
-        docker_util.remove_container(self.docker_client)
-
-        LOG.info('Remove unused images before starting new db container')
-        docker_util.prune_images(self.docker_client)
-
-        LOG.info('Starting new db container with version %s for upgrade',
-                 new_version)
-        self.start_db(update_db=True, ds_version=new_version)
+    # user is cached by making it a class attribute
+    admin_user = None
 
+    def _admin_user(self):
+        if not type(self).admin_user:
+            creds = MongoDBCredentials()
+            creds.read(system.MONGO_ADMIN_CREDS_FILE)
+            user = models.MongoDBUser(
+                'admin.%s' % creds.username,
+                creds.password
+            )
+            type(self).admin_user = user
+        return type(self).admin_user
 
-class BaseMySqlRootAccess(object):
+    @property
+    def cmd_admin_auth_params(self):
+        """Returns a list of strings that constitute MongoDB command line
+        authentication parameters.
+        """
+        user = self._admin_user()
+        return ['--username', user.username,
+                '--password', user.password,
+                '--authenticationDatabase', user.database.name]
+
+    def _create_user_with_client(self, user, client):
+        """Run the add user command."""
+        client[user.database.name].add_user(
+            user.username, password=user.password, roles=user.roles
+        )
+
+    def create_validated_user(self, user, client=None):
+        """Creates a user on their database. The caller should ensure that
+        this action is valid.
+        :param user:   a MongoDBUser object
+        """
+        LOG.debug('Creating user %(user)s on database %(db)s with roles '
+                  '%(role)s.',
+                  {'user': user.username, 'db': user.database.name,
+                   'role': str(user.roles)})
+        if client:
+            self._create_user_with_client(user, client)
+        else:
+            with MongoDBClient(self._admin_user()) as admin_client:
+                self._create_user_with_client(user, admin_client)
 
-    def __init__(self, mysql_app):
-        self.mysql_app = mysql_app
+    def create_users(self, users):
+        """Create the given user(s).
+        :param users:   list of serialized user objects
+        """
+        with MongoDBClient(self._admin_user()) as client:
+            for item in users:
+                user = models.MongoDBUser.deserialize(item)
+                # this could be called to create multiple users at once;
+                # catch exceptions, log the message, and continue
+                try:
+                    user.check_create()
+                    if self._get_user_record(user.name, client=client):
+                        raise ValueError(_('User with name %(user)s already '
+                                           'exists.') % {'user': user.name})
+                    self.create_validated_user(user, client=client)
+                except (ValueError, pymongo.errors.PyMongoError) as e:
+                    LOG.error(e)
+                    LOG.warning(_('Skipping creation of user with name '
+                                  '%(user)s'), {'user': user.name})
+
+    def delete_validated_user(self, user):
+        """Deletes a user from their database. The caller should ensure that
+        this action is valid.
+        :param user:   a MongoDBUser object
+        """
+        LOG.debug('Deleting user %(user)s from database %(db)s.',
+                  {'user': user.username, 'db': user.database.name})
+        with MongoDBClient(self._admin_user()) as admin_client:
+            admin_client[user.database.name].remove_user(user.username)
+
+    def delete_user(self, user):
+        """Delete the given user.
+        :param user:   a serialized user object
+        """
+        user = models.MongoDBUser.deserialize(user)
+        user.check_delete()
+        self.delete_validated_user(user)
+
+    def _get_user_record(self, name, client=None):
+        """Get the user's record."""
+        user = models.MongoDBUser(name)
+        if user.is_ignored:
+            LOG.warning(_('Skipping retrieval of user with reserved '
+                          'name %(user)s'), {'user': user.name})
+            return None
+        if client:
+            user_info = client.admin.system.users.find_one(
+                {'user': user.username, 'db': user.database.name})
+        else:
+            with MongoDBClient(self._admin_user()) as admin_client:
+                user_info = admin_client.admin.system.users.find_one(
+                    {'user': user.username, 'db': user.database.name})
+        if not user_info:
+            return None
+        user.roles = user_info['roles']
+        return user
+
+    def get_existing_user(self, name):
+        """Check that a user exists."""
+        user = self._get_user_record(name)
+        if not user:
+            raise ValueError(_('User with name %(user)s does not'
+                               'exist.') % {'user': name})
+        return user
+
+    def get_user(self, name):
+        """Get information for the given user."""
+        LOG.debug('Getting user %s.', name)
+        user = self._get_user_record(name)
+        if not user:
+            return None
+        return user.serialize()
+
+    def list_users(self, limit=None, marker=None, include_marker=False):
+        """Get a list of all users."""
+        users = []
+        with MongoDBClient(self._admin_user()) as admin_client:
+            for user_info in admin_client.admin.system.users.find():
+                user = models.MongoDBUser(name=user_info['_id'])
+                user.roles = user_info['roles']
+                if not user.is_ignored:
+                    users.append(user)
+        LOG.debug('users = ' + str(users))
+        return guestagent_utils.serialize_list(
+            users,
+            limit=limit, marker=marker, include_marker=include_marker)
+
+    def change_passwords(self, users):
+        with MongoDBClient(self._admin_user()) as admin_client:
+            for item in users:
+                user = models.MongoDBUser.deserialize(item)
+                # this could be called to create multiple users at once;
+                # catch exceptions, log the message, and continue
+                try:
+                    user.check_create()
+                    self.get_existing_user(user.name)
+                    self.create_validated_user(user, admin_client)
+                    LOG.debug('Changing password for user %(user)s',
+                              {'user': user.name})
+                    self._create_user_with_client(user, admin_client)
+                except (ValueError, pymongo.errors.PyMongoError) as e:
+                    LOG.error(e)
+                    LOG.warning(_('Skipping password change for user with '
+                                  'name %(user)s'), {'user': user.name})
+
+    def update_attributes(self, name, user_attrs):
+        """Update user attributes."""
+        user = self.get_existing_user(name)
+        password = user_attrs.get('password')
+        if password:
+            user.password = password
+            self.change_passwords([user.serialize()])
+        if user_attrs.get('name'):
+            LOG.warning(_('Changing user name is not supported.'))
+        if user_attrs.get('host'):
+            LOG.warning(_('Changing user host is not supported.'))
+
+    def enable_root(self, password=None):
+        """Create a user 'root' with role 'root'."""
+        if not password:
+            LOG.debug('Generating root user password.')
+            password = utils.generate_random_password()
+        root_user = models.MongoDBUser.root(password=password)
+        root_user.roles = {'db': 'admin', 'role': 'root'}
+        root_user.check_create()
+        self.create_validated_user(root_user)
+        return root_user.serialize()
 
     def is_root_enabled(self):
-        """Return True if root access is enabled; False otherwise."""
-        with mysql_util.SqlClient(self.mysql_app.get_engine()) as client:
-            t = text(sql_query.ROOT_ENABLED)
-            result = client.execute(t)
-            LOG.debug("Found %s with remote root access.", result.rowcount)
-            return result.rowcount != 0
-
-    def enable_root(self, root_password=None):
-        """Enable the root user global access and/or
-           reset the root password.
-        """
-        user = models.MySQLUser.root(password=root_password)
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            try:
-                cu = sql_query.CreateUser(user.name, host=user.host)
-                t = text(str(cu))
-                client.execute(t, **cu.keyArgs)
-            except (exc.OperationalError, exc.InternalError) as err:
-                # Ignore, user is already created, just reset the password
-                # TODO(rnirmal): More fine grained error checking later on
-                LOG.debug(err)
-
-        with mysql_util.SqlClient(
-                self.mysql_app.get_engine(), use_flush=True) as client:
-            uu = sql_query.SetPassword(
-                user.name, host=user.host, new_password=user.password,
-                ds=CONF.datastore_manager, ds_version=CONF.datastore_version
+        """Check if user 'admin.root' exists."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            return bool(admin_client.admin.system.users.find_one(
+                {'roles.role': 'root'}
+            ))
+
+    def _update_user_roles(self, user):
+        with MongoDBClient(self._admin_user()) as admin_client:
+            admin_client[user.database.name].add_user(
+                user.username, roles=user.roles
             )
-            t = text(str(uu))
-            client.execute(t)
 
-            LOG.debug("CONF.root_grant: %(grant)s CONF.root_grant_option: "
-                      "%(grant_option)s.",
-                      {'grant': CONF.root_grant,
-                       'grant_option': CONF.root_grant_option})
-
-            g = sql_query.Grant(permissions=CONF.root_grant,
-                                user=user.name,
-                                host=user.host,
-                                grant_option=CONF.root_grant_option)
-
-            t = text(str(g))
-            client.execute(t)
-            return user.serialize()
+    def grant_access(self, username, databases):
+        """Adds the RW role to the user for each specified database."""
+        user = self.get_existing_user(username)
+        for db_name in databases:
+            # verify the database name
+            models.MongoDBSchema(db_name)
+            role = {'db': db_name, 'role': 'readWrite'}
+            if role not in user.roles:
+                LOG.debug('Adding role %(role)s to user %(user)s.',
+                          {'role': str(role), 'user': username})
+                user.roles = role
+            else:
+                LOG.debug('User %(user)s already has role %(role)s.',
+                          {'user': username, 'role': str(role)})
+        LOG.debug('Updating user %s.', username)
+        self._update_user_roles(user)
+
+    def revoke_access(self, username, database):
+        """Removes the RW role from the user for the specified database."""
+        user = self.get_existing_user(username)
+        # verify the database name
+        models.MongoDBSchema(database)
+        role = {'db': database, 'role': 'readWrite'}
+        LOG.debug('Removing role %(role)s from user %(user)s.',
+                  {'role': str(role), 'user': username})
+        user.revoke_role(role)
+        LOG.debug('Updating user %s.', username)
+        self._update_user_roles(user)
+
+    def list_access(self, username):
+        """Returns a list of all databases for which the user has the RW role.
+        """
+        user = self.get_existing_user(username)
+        return user.databases
+
+    def create_database(self, databases):
+        """Forces creation of databases.
+        For each new database creates a dummy document in a dummy collection,
+        then drops the collection.
+        """
+        tmp = 'dummy'
+        with MongoDBClient(self._admin_user()) as admin_client:
+            for item in databases:
+                schema = models.MongoDBSchema.deserialize(item)
+                schema.check_create()
+                LOG.debug('Creating MongoDB database %s', schema.name)
+                db = admin_client[schema.name]
+                db[tmp].insert({'dummy': True})
+                db.drop_collection(tmp)
+
+    def delete_database(self, database):
+        """Deletes the database."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            schema = models.MongoDBSchema.deserialize(database)
+            schema.check_delete()
+            admin_client.drop_database(schema.name)
+
+    def list_database_names(self):
+        """Get the list of database names."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            return admin_client.database_names()
 
-    def disable_root(self):
-        """Reset the root password to an unknown value.
+    def list_databases(self, limit=None, marker=None, include_marker=False):
+        """Lists the databases."""
+        databases = []
+        for db_name in self.list_database_names():
+            schema = models.MongoDBSchema(name=db_name)
+            if not schema.is_ignored():
+                databases.append(schema)
+        LOG.debug('databases = ' + str(databases))
+        return guestagent_utils.serialize_list(
+            databases,
+            limit=limit, marker=marker, include_marker=include_marker)
+
+    def add_shard(self, url):
+        """Runs the addShard command."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            admin_client.admin.command({'addShard': url})
+
+    def get_repl_status(self):
+        """Runs the replSetGetStatus command."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            status = admin_client.admin.command('replSetGetStatus')
+            LOG.debug('Replica set status: %s', status)
+            return status
+
+    def rs_initiate(self):
+        """Runs the replSetInitiate command."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            return admin_client.admin.command('replSetInitiate')
+
+    def rs_add_members(self, members):
+        """Adds the given members to the replication set."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            # get the current config, add the new members, then save it
+            config = admin_client.admin.command('replSetGetConfig')['config']
+            config['version'] += 1
+            next_id = max([m['_id'] for m in config['members']]) + 1
+            for member in members:
+                config['members'].append({'_id': next_id, 'host': member})
+                next_id += 1
+            admin_client.admin.command('replSetReconfig', config)
+
+    def db_stats(self, database, scale=1):
+        """Gets the stats for the given database."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            db_name = models.MongoDBSchema.deserialize(database).name
+            return admin_client[db_name].command('dbStats', scale=scale)
+
+    def list_active_shards(self):
+        """Get a list of shards active in this cluster."""
+        with MongoDBClient(self._admin_user()) as admin_client:
+            return [shard for shard in admin_client.config.shards.find()]
+
+
+class MongoDBClient(object):
+    """A wrapper to manage a MongoDB connection."""
+
+    # engine information is cached by making it a class attribute
+    engine = {}
+
+    def __init__(self, user, host=None, port=None):
+        """Get the client. Specifying host and/or port updates cached values.
+        :param user: MongoDBUser instance used to authenticate
+        :param host: server address, defaults to localhost
+        :param port: server port, defaults to 27017
+        :return:
         """
-        self.enable_root(root_password=None)
+        new_client = False
+        self._logged_in = False
+        if not type(self).engine:
+            # no engine cached
+            type(self).engine['host'] = (host if host else 'localhost')
+            type(self).engine['port'] = (port if port else MONGODB_PORT)
+            new_client = True
+        elif host or port:
+            LOG.debug("Updating MongoDB client.")
+            if host:
+                type(self).engine['host'] = host
+            if port:
+                type(self).engine['port'] = port
+            new_client = True
+        if new_client:
+            host = type(self).engine['host']
+            port = type(self).engine['port']
+            LOG.debug("Creating MongoDB client to %(host)s:%(port)s.",
+                      {'host': host, 'port': port})
+            type(self).engine['client'] = pymongo.MongoClient(host=host,
+                                                              port=port,
+                                                              connect=False)
+        self.session = type(self).engine['client']
+        if user:
+            db_name = user.database.name
+            LOG.debug("Authenticating MongoDB client on %s.", db_name)
+            self._db = self.session[db_name]
+            self._db.authenticate(user.username, password=user.password)
+            self._logged_in = True
+
+    def __enter__(self):
+        return self.session
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        LOG.debug("Disconnecting from MongoDB.")
+        if self._logged_in:
+            self._db.logout()
+        self.session.close()
+
+
+class MongoDBCredentials(object):
+    """Handles storing/retrieving credentials. Stored as json in files."""
+
+    def __init__(self, username=None, password=None):
+        self.username = username
+        self.password = password
+
+    def read(self, filename):
+        credentials = operating_system.read_file(filename, codec=JsonCodec())
+        self.username = credentials['username']
+        self.password = credentials['password']
+
+    def write(self, filename):
+        credentials = {'username': self.username,
+                       'password': self.password}
+
+        operating_system.write_file(filename, credentials, codec=JsonCodec())
+        operating_system.chmod(filename, operating_system.FileMode.SET_USR_RW)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/query.py` & `trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/pgsql_query.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright (c) 2013 OpenStack Foundation
+# Copyright (c) 2016 Tesora, Inc.
 #
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
@@ -12,89 +13,98 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 
 class DatabaseQuery(object):
+
     @classmethod
     def list(cls, ignore=()):
         """Query to list all databases."""
+
         statement = (
             "SELECT datname, pg_encoding_to_char(encoding), "
             "datcollate FROM pg_database "
             "WHERE datistemplate = false"
         )
 
         for name in ignore:
             statement += " AND datname != '{name}'".format(name=name)
 
         return statement
 
     @classmethod
     def create(cls, name, encoding=None, collation=None):
         """Query to create a database."""
+
         statement = "CREATE DATABASE \"{name}\"".format(name=name)
         if encoding is not None:
             statement += " ENCODING = '{encoding}'".format(
                 encoding=encoding,
             )
         if collation is not None:
             statement += " LC_COLLATE = '{collation}'".format(
                 collation=collation,
             )
 
         return statement
 
     @classmethod
     def drop(cls, name):
-        return f'DROP DATABASE IF EXISTS "{name}"'
+        """Query to drop a database."""
+
+        return "DROP DATABASE IF EXISTS \"{name}\"".format(name=name)
 
 
 class UserQuery(object):
 
     @classmethod
     def list(cls, ignore=()):
         """Query to list all users."""
+
         statement = (
             "SELECT usename, datname, pg_encoding_to_char(encoding), "
             "datcollate FROM pg_catalog.pg_user "
             "LEFT JOIN pg_catalog.pg_database "
-            "ON CONCAT(usename, '=CTc/postgres') = ANY(datacl::text[]) "
+            "ON CONCAT(usename, '=CTc/os_admin') = ANY(datacl::text[]) "
             "WHERE (datistemplate ISNULL OR datistemplate = false)")
         if ignore:
             for name in ignore:
-                statement += f" AND usename != '{name}'"
+                statement += " AND usename != '{name}'".format(name=name)
 
         return statement
 
     @classmethod
     def list_root(cls, ignore=()):
         """Query to list all superuser accounts."""
+
         statement = (
             "SELECT usename FROM pg_catalog.pg_user WHERE usesuper = true"
         )
 
         for name in ignore:
-            statement += f" AND usename != '{name}'"
+            statement += " AND usename != '{name}'".format(name=name)
 
         return statement
 
     @classmethod
     def get(cls, name):
         """Query to get a single user."""
-        return cls.list() + f" AND usename = '{name}'"
+
+        return cls.list() + " AND usename = '{name}'".format(name=name)
 
     @classmethod
     def create(cls, name, password, encrypt_password=None, *options):
         """Query to create a user with a password."""
+
         create_clause = "CREATE USER \"{name}\"".format(name=name)
         with_clause = cls._build_with_clause(
             password, encrypt_password, *options)
-        return ' '.join([create_clause, with_clause])
+        return ''.join([create_clause, with_clause])
 
     @classmethod
     def _build_with_clause(cls, password, encrypt_password=None, *options):
         tokens = ['WITH']
         if password:
             # Do not specify the encryption option if 'encrypt_password'
             # is None. PostgreSQL will use the configuration default.
@@ -118,37 +128,50 @@
 
         return cls.alter_user(name, password, encrypt_password)
 
     @classmethod
     def alter_user(cls, name, password, encrypt_password=None, *options):
         """Query to alter a user."""
 
-        alter_clause = f'ALTER USER "{name}"'
+        alter_clause = "ALTER USER \"{name}\"".format(name=name)
         with_clause = cls._build_with_clause(
             password, encrypt_password, *options)
         return ''.join([alter_clause, with_clause])
 
     @classmethod
     def update_name(cls, old, new):
         """Query to update the name of a user.
         This statement also results in an automatic permission transfer to the
         new username.
         """
-        return f'ALTER USER "{old}" RENAME TO "{new}"'
+
+        return "ALTER USER \"{old}\" RENAME TO \"{new}\"".format(
+            old=old,
+            new=new,
+        )
 
     @classmethod
     def drop(cls, name):
         """Query to drop a user."""
-        return f'DROP USER IF EXISTS "{name}"'
+
+        return "DROP USER \"{name}\"".format(name=name)
 
 
 class AccessQuery(object):
 
     @classmethod
     def grant(cls, user, database):
         """Query to grant user access to a database."""
-        return f'GRANT ALL ON DATABASE "{database}" TO "{user}"'
+
+        return "GRANT ALL ON DATABASE \"{database}\" TO \"{user}\"".format(
+            database=database,
+            user=user,
+        )
 
     @classmethod
     def revoke(cls, user, database):
         """Query to revoke user access to a database."""
-        return f'REVOKE ALL ON DATABASE "{database}" FROM "{user}"'
+
+        return "REVOKE ALL ON DATABASE \"{database}\" FROM \"{user}\"".format(
+            database=database,
+            user=user,
+        )
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/postgres/service.py` & `trove-8.0.1/trove/guestagent/datastore/experimental/postgresql/service.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,391 +1,484 @@
-# Copyright 2020 Catalyst Cloud
+# Copyright (c) 2013 OpenStack Foundation
+# Copyright (c) 2016 Tesora, Inc.
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+# All Rights Reserved.
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
 from collections import OrderedDict
 import os
+import re
 
 from oslo_log import log as logging
 import psycopg2
 
 from trove.common import cfg
-from trove.common import constants
 from trove.common.db.postgresql import models
 from trove.common import exception
-from trove.common import stream_codecs
+from trove.common.i18n import _
+from trove.common import instance
+from trove.common.stream_codecs import PropertiesCodec
 from trove.common import utils
-from trove.guestagent.common import configuration
+from trove.guestagent.common.configuration import ConfigurationManager
+from trove.guestagent.common.configuration import OneFileOverrideStrategy
 from trove.guestagent.common import guestagent_utils
 from trove.guestagent.common import operating_system
-from trove.guestagent.datastore.postgres import query
+from trove.guestagent.common.operating_system import FileMode
+from trove.guestagent.datastore.experimental.postgresql import pgsql_query
 from trove.guestagent.datastore import service
-from trove.guestagent.utils import docker as docker_util
-from trove.instance import service_status
+from trove.guestagent import pkg
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
-SUPER_USER_NAME = "postgres"
-CONFIG_FILE = "/etc/postgresql/postgresql.conf"
-CNF_EXT = 'conf'
-# The same with include_dir config option
-CNF_INCLUDE_DIR = '/etc/postgresql/conf.d'
-HBA_CONFIG_FILE = '/etc/postgresql/pg_hba.conf'
-# The same with the path in archive_command config option.
-WAL_ARCHIVE_DIR = '/var/lib/postgresql/data/wal_archive'
+BACKUP_CFG_OVERRIDE = 'PgBaseBackupConfig'
+DEBUG_MODE_OVERRIDE = 'DebugLevelOverride'
 
 
-class PgSqlAppStatus(service.BaseDbStatus):
-    def __init__(self, docker_client):
-        super(PgSqlAppStatus, self).__init__(docker_client)
+class PgSqlApp(object):
 
-    def get_actual_db_status(self):
-        """Check database service status."""
-        status = docker_util.get_container_status(self.docker_client)
-        if status == "running":
-            cmd = "psql -U postgres -c 'select 1;'"
-            try:
-                docker_util.run_command(self.docker_client, cmd)
-                return service_status.ServiceStatuses.HEALTHY
-            except Exception as exc:
-                LOG.warning('Failed to run docker command, error: %s',
-                            str(exc))
-                container_log = docker_util.get_container_logs(
-                    self.docker_client, tail='all')
-                LOG.debug('container log: \n%s', '\n'.join(container_log))
-                return service_status.ServiceStatuses.RUNNING
-        elif status == "not running":
-            return service_status.ServiceStatuses.SHUTDOWN
-        elif status == "paused":
-            return service_status.ServiceStatuses.PAUSED
-        elif status == "exited":
-            return service_status.ServiceStatuses.SHUTDOWN
-        elif status == "dead":
-            return service_status.ServiceStatuses.CRASHED
-        else:
-            return service_status.ServiceStatuses.UNKNOWN
+    OS = operating_system.get_os()
+    LISTEN_ADDRESSES = ['*']  # Listen on all available IP (v4/v6) interfaces.
+    ADMIN_USER = 'os_admin'  # Trove's administrative user.
+
+    def __init__(self):
+        super(PgSqlApp, self).__init__()
+
+        self._current_admin_user = None
+        self.status = PgSqlAppStatus(self.pgsql_extra_bin_dir)
+
+        revision_dir = guestagent_utils.build_file_path(
+            os.path.dirname(self.pgsql_config),
+            ConfigurationManager.DEFAULT_STRATEGY_OVERRIDES_SUB_DIR)
+        self.configuration_manager = ConfigurationManager(
+            self.pgsql_config, self.pgsql_owner, self.pgsql_owner,
+            PropertiesCodec(
+                delimiter='=',
+                string_mappings={'on': True, 'off': False, "''": None}),
+            requires_root=True,
+            override_strategy=OneFileOverrideStrategy(revision_dir))
 
+    @property
+    def service_candidates(self):
+        return ['postgresql']
 
-class PgSqlApp(service.BaseDbApp):
-    _configuration_manager = None
+    @property
+    def pgsql_owner(self):
+        return 'postgres'
 
     @property
-    def configuration_manager(self):
-        if self._configuration_manager:
-            return self._configuration_manager
+    def default_superuser_name(self):
+        return "postgres"
 
-        self._configuration_manager = configuration.ConfigurationManager(
-            CONFIG_FILE,
-            CONF.database_service_uid,
-            CONF.database_service_uid,
-            stream_codecs.KeyValueCodec(
-                value_quoting=True,
-                bool_case=stream_codecs.KeyValueCodec.BOOL_LOWER,
-                big_ints=True),
-            requires_root=True,
-            override_strategy=configuration.ImportOverrideStrategy(
-                CNF_INCLUDE_DIR, CNF_EXT)
-        )
-        return self._configuration_manager
+    @property
+    def pgsql_base_data_dir(self):
+        return '/var/lib/postgresql/'
 
-    def __init__(self, status, docker_client):
-        super(PgSqlApp, self).__init__(status, docker_client)
+    @property
+    def pgsql_pid_file(self):
+        return guestagent_utils.build_file_path(self.pgsql_run_dir,
+                                                'postgresql.pid')
 
-        # See
-        # https://github.com/docker-library/docs/blob/master/postgres/README.md#pgdata
-        mount_point = cfg.get_configuration_property('mount_point')
-        self.datadir = f"{mount_point}/data/pgdata"
-        self.adm = PgSqlAdmin(SUPER_USER_NAME)
+    @property
+    def pgsql_run_dir(self):
+        return '/var/run/postgresql/'
 
-    def get_data_dir(self):
-        return self.configuration_manager.get_value('data_directory')
+    @property
+    def pgsql_extra_bin_dir(self):
+        """Redhat and Ubuntu packages for PgSql do not place 'extra' important
+        binaries in /usr/bin, but rather in a directory like /usr/pgsql-9.4/bin
+        in the case of PostgreSQL 9.4 for RHEL/CentOS
+        """
+        return {
+            operating_system.DEBIAN: '/usr/lib/postgresql/%s/bin/',
+            operating_system.REDHAT: '/usr/pgsql-%s/bin/',
+            operating_system.SUSE: '/usr/bin/'
+        }[self.OS] % self.pg_version[1]
 
-    def set_data_dir(self, value):
-        self.configuration_manager.apply_system_override(
-            {'data_directory': value})
+    @property
+    def pgsql_config(self):
+        return self._find_config_file('postgresql.conf')
 
-    def reload(self):
-        cmd = f"pg_ctl reload -D {self.datadir}"
-        docker_util.run_command(self.docker_client, cmd)
+    @property
+    def pgsql_hba_config(self):
+        return self._find_config_file('pg_hba.conf')
 
-    def apply_access_rules(self):
-        """PostgreSQL Client authentication settings
+    @property
+    def pgsql_ident_config(self):
+        return self._find_config_file('pg_ident.conf')
 
-        The order of entries is important. The first failure to authenticate
-        stops the lookup. That is why the 'local' connections validate first.
-        The OrderedDict is necessary to guarantee the iteration order.
-        """
-        LOG.debug("Applying client authentication access rules.")
+    def _find_config_file(self, name_pattern):
+        version_base = guestagent_utils.build_file_path(self.pgsql_config_dir,
+                                                        self.pg_version[1])
+        return sorted(operating_system.list_files_in_directory(
+            version_base, recursive=True, pattern=name_pattern,
+            as_root=True), key=len)[0]
 
+    @property
+    def pgsql_config_dir(self):
+        return {
+            operating_system.DEBIAN: '/etc/postgresql/',
+            operating_system.REDHAT: '/var/lib/postgresql/',
+            operating_system.SUSE: '/var/lib/pgsql/'
+        }[self.OS]
+
+    @property
+    def pgsql_log_dir(self):
+        return "/var/log/postgresql/"
+
+    def build_admin(self):
+        return PgSqlAdmin(self.get_current_admin_user())
+
+    def update_overrides(self, context, overrides, remove=False):
+        if remove:
+            self.configuration_manager.remove_user_override()
+        elif overrides:
+            self.configuration_manager.apply_user_override(overrides)
+
+    def set_current_admin_user(self, user):
+        self._current_admin_user = user
+
+    def get_current_admin_user(self):
+        if self._current_admin_user is not None:
+            return self._current_admin_user
+
+        if self.status.is_installed:
+            return models.PostgreSQLUser(self.ADMIN_USER)
+
+        return models.PostgreSQLUser(self.default_superuser_name)
+
+    def apply_overrides(self, context, overrides):
+        self.reload_configuration()
+
+    def reload_configuration(self):
+        """Send a signal to the server, causing configuration files to be
+        reloaded by all server processes.
+        Active queries or connections to the database will not be
+        interrupted.
+
+        NOTE: Do not use the 'SET' command as it only affects the current
+        session.
+        """
+        self.build_admin().psql(
+            "SELECT pg_reload_conf()")
+
+    def reset_configuration(self, context, configuration):
+        """Reset the PgSql configuration to the one given.
+        """
+        config_contents = configuration['config_contents']
+        self.configuration_manager.save_configuration(config_contents)
+
+    def start_db_with_conf_changes(self, context, config_contents):
+        """Starts the PgSql instance with a new configuration."""
+        if self.status.is_running:
+            raise RuntimeError(_("The service is still running."))
+
+        self.configuration_manager.save_configuration(config_contents)
+        # The configuration template has to be updated with
+        # guestagent-controlled settings.
+        self.apply_initial_guestagent_configuration()
+        self.start_db()
+
+    def apply_initial_guestagent_configuration(self):
+        """Update guestagent-controlled configuration properties.
+        """
+        LOG.debug("Applying initial guestagent configuration.")
+        file_locations = {
+            'data_directory': self._quote(self.pgsql_data_dir),
+            'hba_file': self._quote(self.pgsql_hba_config),
+            'ident_file': self._quote(self.pgsql_ident_config),
+            'external_pid_file': self._quote(self.pgsql_pid_file),
+            'unix_socket_directories': self._quote(self.pgsql_run_dir),
+            'listen_addresses': self._quote(','.join(self.LISTEN_ADDRESSES)),
+            'port': cfg.get_configuration_property('postgresql_port')}
+        self.configuration_manager.apply_system_override(file_locations)
+        self._apply_access_rules()
+
+    @staticmethod
+    def _quote(value):
+        return "'%s'" % value
+
+    def _apply_access_rules(self):
+        LOG.debug("Applying database access rules.")
+
+        # Connections to all resources are granted.
+        #
+        # Local access from administrative users is implicitly trusted.
+        #
+        # Remote access from the Trove's account is always rejected as
+        # it is not needed and could be used by malicious users to hijack the
+        # instance.
+        #
+        # Connections from other accounts always require a double-MD5-hashed
+        # password.
+        #
+        # Make the rules readable only by the Postgres service.
+        #
+        # NOTE: The order of entries is important.
+        # The first failure to authenticate stops the lookup.
+        # That is why the 'local' connections validate first.
+        # The OrderedDict is necessary to guarantee the iteration order.
+        local_admins = ','.join([self.default_superuser_name, self.ADMIN_USER])
+        remote_admins = self.ADMIN_USER
         access_rules = OrderedDict(
-            [('local', [['all', SUPER_USER_NAME, None, 'trust'],
-                        ['replication', SUPER_USER_NAME, None, 'trust'],
+            [('local', [['all', local_admins, None, 'trust'],
+                        ['replication', local_admins, None, 'trust'],
                         ['all', 'all', None, 'md5']]),
-             ('host', [['all', SUPER_USER_NAME, '127.0.0.1/32', 'trust'],
-                       ['all', SUPER_USER_NAME, '::1/128', 'trust'],
-                       ['all', SUPER_USER_NAME, 'localhost', 'trust'],
-                       ['all', SUPER_USER_NAME, '0.0.0.0/0', 'reject'],
-                       ['all', SUPER_USER_NAME, '::/0', 'reject'],
+             ('host', [['all', local_admins, '127.0.0.1/32', 'trust'],
+                       ['all', local_admins, '::1/128', 'trust'],
+                       ['all', local_admins, 'localhost', 'trust'],
+                       ['all', remote_admins, '0.0.0.0/0', 'reject'],
+                       ['all', remote_admins, '::/0', 'reject'],
                        ['all', 'all', '0.0.0.0/0', 'md5'],
                        ['all', 'all', '::/0', 'md5']])
              ])
-        operating_system.write_file(
-            HBA_CONFIG_FILE, access_rules,
-            stream_codecs.PropertiesCodec(string_mappings={'\t': None}),
-            as_root=True)
-        operating_system.chown(HBA_CONFIG_FILE,
-                               CONF.database_service_uid,
-                               CONF.database_service_uid,
+        operating_system.write_file(self.pgsql_hba_config, access_rules,
+                                    PropertiesCodec(
+                                        string_mappings={'\t': None}),
+                                    as_root=True)
+        operating_system.chown(self.pgsql_hba_config,
+                               self.pgsql_owner, self.pgsql_owner,
                                as_root=True)
-        operating_system.chmod(HBA_CONFIG_FILE,
-                               operating_system.FileMode.SET_USR_RO,
+        operating_system.chmod(self.pgsql_hba_config, FileMode.SET_USR_RO,
                                as_root=True)
 
-    def update_overrides(self, overrides):
-        """Update config options in the include directory."""
-        if overrides:
-            self.configuration_manager.apply_user_override(overrides)
-
-    def apply_overrides(self, overrides):
-        """Reload config."""
-        cmd = "pg_ctl reload"
-        docker_util.run_command(self.docker_client, cmd)
-
-    def start_db(self, update_db=False, ds_version=None, command=None,
-                 extra_volumes=None):
-        """Start and wait for database service."""
-        docker_image = CONF.get(CONF.datastore_manager).docker_image
-        image = (f'{docker_image}:latest' if not ds_version else
-                 f'{docker_image}:{ds_version}')
-        command = command if command else ''
-
-        try:
-            postgres_pass = self.get_auth_password(file="postgres.cnf")
-        except exception.UnprocessableEntity:
-            postgres_pass = utils.generate_random_password()
-
-        # Get uid and gid
-        user = "%s:%s" % (CONF.database_service_uid, CONF.database_service_uid)
-
-        # Create folders for postgres on localhost
-        for folder in ['/etc/postgresql',
-                       constants.POSTGRESQL_HOST_SOCKET_PATH]:
-            operating_system.ensure_directory(
-                folder, user=CONF.database_service_uid,
-                group=CONF.database_service_uid, force=True,
-                as_root=True)
-
-        volumes = {
-            "/etc/postgresql": {"bind": "/etc/postgresql", "mode": "rw"},
-            constants.POSTGRESQL_HOST_SOCKET_PATH:
-                {"bind": "/var/run/postgresql", "mode": "rw"},
-            "/var/lib/postgresql": {"bind": "/var/lib/postgresql",
-                                    "mode": "rw"},
-            "/var/lib/postgresql/data": {"bind": "/var/lib/postgresql/data",
-                                         "mode": "rw"},
+    def disable_backups(self):
+        """Reverse overrides applied by PgBaseBackup strategy"""
+        if not self.configuration_manager.has_system_override(
+                BACKUP_CFG_OVERRIDE):
+            return
+        LOG.info(_("Removing configuration changes for backups"))
+        self.configuration_manager.remove_system_override(BACKUP_CFG_OVERRIDE)
+        self.remove_wal_archive_dir()
+        self.restart()
+
+    def enable_backups(self):
+        """Apply necessary changes to config to enable WAL-based backups
+        if we are using the PgBaseBackup strategy
+        """
+        LOG.info(_("Checking if we need to apply changes to WAL config"))
+        if 'PgBaseBackup' not in self.backup_strategy:
+            return
+        if self.configuration_manager.has_system_override(BACKUP_CFG_OVERRIDE):
+            return
+
+        LOG.info(_("Applying changes to WAL config for use by base backups"))
+        wal_arch_loc = self.wal_archive_location
+        if not os.path.isdir(wal_arch_loc):
+            raise RuntimeError(_("Cannot enable backup as WAL dir '%s' does "
+                                 "not exist.") % wal_arch_loc)
+        arch_cmd = "'test ! -f {wal_arch}/%f && cp %p {wal_arch}/%f'".format(
+            wal_arch=wal_arch_loc
+        )
+        opts = {
+            'wal_level': 'hot_standby',
+            'archive_mode': 'on',
+            'max_wal_senders': 8,
+            'checkpoint_segments': 8,
+            'wal_keep_segments': 8,
+            'archive_command': arch_cmd
         }
-        if extra_volumes:
-            volumes.update(extra_volumes)
+        if not self.pg_version[1] in ('9.3'):
+            opts['wal_log_hints'] = 'on'
 
-        # Expose ports
-        ports = {}
-        tcp_ports = cfg.get_configuration_property('tcp_ports')
-        for port_range in tcp_ports:
-            for port in port_range:
-                ports[f'{port}/tcp'] = port
-
-        if CONF.network_isolation and \
-                os.path.exists(constants.ETH1_CONFIG_PATH):
-            network_mode = constants.DOCKER_HOST_NIC_MODE
-        else:
-            network_mode = constants.DOCKER_BRIDGE_MODE
+        self.configuration_manager.apply_system_override(
+            opts, BACKUP_CFG_OVERRIDE)
+        self.restart()
 
-        try:
-            docker_util.start_container(
-                self.docker_client,
-                image,
-                volumes=volumes,
-                network_mode=network_mode,
-                ports=ports,
-                user=user,
-                environment={
-                    "POSTGRES_PASSWORD": postgres_pass,
-                    "PGDATA": self.datadir,
-                },
-                command=command
-            )
+    def disable_debugging(self, level=1):
+        """Disable debug-level logging in postgres"""
+        self.configuration_manager.remove_system_override(DEBUG_MODE_OVERRIDE)
 
-            # Save root password
-            LOG.debug("Saving root credentials to local host.")
-            self.save_password('postgres', postgres_pass)
-        except Exception:
-            LOG.exception("Failed to start database service")
-            raise exception.TroveError("Failed to start database service")
+    def enable_debugging(self, level=1):
+        """Enable debug-level logging in postgres"""
+        opt = {'log_min_messages': 'DEBUG%s' % level}
+        self.configuration_manager.apply_system_override(opt,
+                                                         DEBUG_MODE_OVERRIDE)
 
-        if not self.status.wait_for_status(
-            service_status.ServiceStatuses.HEALTHY,
-            CONF.state_change_wait_time, update_db
-        ):
-            raise exception.TroveError("Failed to start database service")
+    def install(self, context, packages):
+        """Install one or more packages that postgresql needs to run.
 
-    def restart(self):
-        LOG.info("Restarting database")
+        The packages parameter is a string representing the package names that
+        should be given to the system's package manager.
+        """
 
-        # Ensure folders permission for database.
-        for folder in ['/etc/postgresql',
-                       constants.POSTGRESQL_HOST_SOCKET_PATH]:
-            operating_system.ensure_directory(
-                folder, user=CONF.database_service_uid,
-                group=CONF.database_service_uid, force=True,
-                as_root=True)
+        LOG.debug(
+            "{guest_id}: Beginning PgSql package installation.".format(
+                guest_id=CONF.guest_id
+            )
+        )
+        self.recreate_wal_archive_dir()
 
-        try:
-            docker_util.restart_container(self.docker_client)
-        except Exception:
-            LOG.exception("Failed to restart database")
-            raise exception.TroveError("Failed to restart database")
+        packager = pkg.Package()
+        if not packager.pkg_is_installed(packages):
+            try:
+                LOG.info(
+                    _("{guest_id}: Installing ({packages}).").format(
+                        guest_id=CONF.guest_id,
+                        packages=packages,
+                    )
+                )
+                packager.pkg_install(packages, {}, 1000)
+            except (pkg.PkgAdminLockError, pkg.PkgPermissionError,
+                    pkg.PkgPackageStateError, pkg.PkgNotFoundError,
+                    pkg.PkgTimeout, pkg.PkgScriptletError,
+                    pkg.PkgDownloadError, pkg.PkgSignError,
+                    pkg.PkgBrokenError):
+                LOG.exception(
+                    _("{guest_id}: There was a package manager error while "
+                      "trying to install ({packages}).").format(
+                        guest_id=CONF.guest_id,
+                        packages=packages,
+                    )
+                )
+                raise
+            except Exception:
+                LOG.exception(
+                    _("{guest_id}: The package manager encountered an unknown "
+                      "error while trying to install ({packages}).").format(
+                        guest_id=CONF.guest_id,
+                        packages=packages,
+                    )
+                )
+                raise
+            else:
+                self.start_db()
+                LOG.debug(
+                    "{guest_id}: Completed package installation.".format(
+                        guest_id=CONF.guest_id,
+                    )
+                )
 
-        if not self.status.wait_for_status(
-            service_status.ServiceStatuses.HEALTHY,
-            CONF.state_change_wait_time, update_db=True
-        ):
-            raise exception.TroveError("Failed to start database")
-
-        LOG.info("Finished restarting database")
-
-    def restore_backup(self, context, backup_info, restore_location):
-        backup_id = backup_info['id']
-        storage_driver = CONF.storage_strategy
-        backup_driver = self.get_backup_strategy()
-        image = self.get_backup_image()
-        name = 'db_restore'
-        volumes = {
-            '/var/lib/postgresql/data': {
-                'bind': '/var/lib/postgresql/data',
-                'mode': 'rw'
-            }
-        }
+    @property
+    def pgsql_recovery_config(self):
+        return os.path.join(self.pgsql_data_dir, "recovery.conf")
 
-        os_cred = (f"--os-token={context.auth_token} "
-                   f"--os-auth-url={CONF.service_credentials.auth_url} "
-                   f"--os-tenant-id={context.project_id} "
-                   f"--os-region-name={CONF.service_credentials.region_name}")
-
-        command = (
-            f'/usr/bin/python3 main.py --nobackup '
-            f'--storage-driver={storage_driver} --driver={backup_driver} '
-            f'{os_cred} '
-            f'--restore-from={backup_info["location"]} '
-            f'--restore-checksum={backup_info["checksum"]} '
-            f'--pg-wal-archive-dir {WAL_ARCHIVE_DIR}'
-        )
-        if CONF.backup_aes_cbc_key:
-            command = (f"{command} "
-                       f"--backup-encryption-key={CONF.backup_aes_cbc_key}")
-
-        LOG.debug('Stop the database and clean up the data before restore '
-                  'from %s', backup_id)
-        self.stop_db()
-        for dir in [WAL_ARCHIVE_DIR, self.datadir]:
-            operating_system.remove_dir_contents(dir)
-
-        # Start to run restore inside a separate docker container
-        LOG.info('Starting to restore backup %s, command: %s', backup_id,
-                 command)
-        output, ret = docker_util.run_container(
-            self.docker_client, image, name,
-            volumes=volumes, command=command)
-        result = output[-1]
-        if not ret:
-            msg = f'Failed to run restore container, error: {result}'
-            LOG.error(msg)
-            raise Exception(msg)
-
-        for dir in [WAL_ARCHIVE_DIR, self.datadir]:
-            operating_system.chown(dir, CONF.database_service_uid,
-                                   CONF.database_service_uid, force=True,
-                                   as_root=True)
+    @property
+    def pgsql_data_dir(self):
+        return os.path.dirname(self.pg_version[0])
 
-    def is_replica(self):
-        """Wrapper for pg_is_in_recovery() for detecting a server in
-        standby mode
+    @property
+    def pg_version(self):
+        """Find the database version file stored in the data directory.
+
+        :returns: A tuple with the path to the version file
+                  (in the root of the data directory) and the version string.
         """
-        r = self.adm.query("SELECT pg_is_in_recovery()")
-        return r[0][0]
+        version_files = operating_system.list_files_in_directory(
+            self.pgsql_base_data_dir, recursive=True, pattern='PG_VERSION',
+            as_root=True)
+        version_file = sorted(version_files, key=len)[0]
+        version = operating_system.read_file(version_file, as_root=True)
+        return version_file, version.strip()
 
-    def get_current_wal_lsn(self):
-        """Wrapper for pg_current_wal_lsn()
+    def restart(self):
+        self.status.restart_db_service(
+            self.service_candidates, CONF.state_change_wait_time)
 
-        Cannot be used against a running replica
+    def start_db(self, enable_on_boot=True, update_db=False):
+        self.status.start_db_service(
+            self.service_candidates, CONF.state_change_wait_time,
+            enable_on_boot=enable_on_boot, update_db=update_db)
+
+    def stop_db(self, do_not_start_on_reboot=False, update_db=False):
+        self.status.stop_db_service(
+            self.service_candidates, CONF.state_change_wait_time,
+            disable_on_boot=do_not_start_on_reboot, update_db=update_db)
+
+    def secure(self, context):
+        """Create an administrative user for Trove.
+        Force password encryption.
+        Also disable the built-in superuser
+        """
+        password = utils.generate_random_password()
+
+        os_admin_db = models.PostgreSQLSchema(self.ADMIN_USER)
+        os_admin = models.PostgreSQLUser(self.ADMIN_USER, password)
+        os_admin.databases.append(os_admin_db.serialize())
+
+        postgres = models.PostgreSQLUser(self.default_superuser_name)
+        admin = PgSqlAdmin(postgres)
+        admin._create_database(context, os_admin_db)
+        admin._create_admin_user(context, os_admin,
+                                 encrypt_password=True)
+
+        PgSqlAdmin(os_admin).alter_user(context, postgres, None,
+                                        'NOSUPERUSER', 'NOLOGIN')
+
+        self.set_current_admin_user(os_admin)
+
+    def pg_current_xlog_location(self):
+        """Wrapper for pg_current_xlog_location()
+        Cannot be used against a running slave
         """
-        r = self.adm.query("SELECT pg_current_wal_lsn()")
+        r = self.build_admin().query("SELECT pg_current_xlog_location()")
         return r[0][0]
 
-    def get_last_wal_replay_lsn(self):
-        """Wrapper for pg_last_wal_replay_lsn()
-
-         For use on replica servers
+    def pg_last_xlog_replay_location(self):
+        """Wrapper for pg_last_xlog_replay_location()
+         For use on standby servers
          """
-        r = self.adm.query("SELECT pg_last_wal_replay_lsn()")
+        r = self.build_admin().query("SELECT pg_last_xlog_replay_location()")
         return r[0][0]
 
-    def pg_rewind(self, conn_info):
-        docker_image = CONF.get(CONF.datastore_manager).docker_image
-        image = f'{docker_image}:{CONF.datastore_version}'
-        user = "%s:%s" % (CONF.database_service_uid, CONF.database_service_uid)
-        volumes = {
-            constants.POSTGRESQL_HOST_SOCKET_PATH:
-                {"bind": "/var/run/postgresql", "mode": "rw"},
-            "/var/lib/postgresql": {"bind": "/var/lib/postgresql",
-                                    "mode": "rw"},
-            "/var/lib/postgresql/data": {"bind": "/var/lib/postgresql/data",
-                                         "mode": "rw"},
-        }
-        command = (f"pg_rewind --target-pgdata={self.datadir} "
-                   f"--source-server='{conn_info}'")
-
-        docker_util.remove_container(self.docker_client, name='pg_rewind')
-
-        LOG.info('Running pg_rewind in container')
-        output, ret = docker_util.run_container(
-            self.docker_client, image, 'pg_rewind',
-            volumes=volumes, command=command, user=user)
-        result = output[-1]
-        LOG.debug(f"Finished running pg_rewind, last output: {result}")
-        if not ret:
-            msg = f'Failed to run pg_rewind in container, error: {result}'
-            LOG.error(msg)
-            raise Exception(msg)
-
-
-class PgSqlAdmin(object):
-    # Default set of options of an administrative account.
-    ADMIN_OPTIONS = (
-        'SUPERUSER', 'CREATEDB', 'CREATEROLE', 'INHERIT', 'REPLICATION',
-        'BYPASSRLS', 'LOGIN'
-    )
+    def pg_is_in_recovery(self):
+        """Wrapper for pg_is_in_recovery() for detecting a server in
+        standby mode
+        """
+        r = self.build_admin().query("SELECT pg_is_in_recovery()")
+        return r[0][0]
 
-    def __init__(self, username):
-        port = cfg.get_configuration_property('postgresql_port')
-        self.connection = PostgresConnection(username, port=port)
+    def pg_primary_host(self):
+        """There seems to be no way to programmatically determine this
+        on a hot standby, so grab what we have written to the recovery
+        file
+        """
+        r = operating_system.read_file(self.pgsql_recovery_config,
+                                       as_root=True)
+        regexp = re.compile("host=(\d+.\d+.\d+.\d+) ")
+        m = regexp.search(r)
+        return m.group(1)
+
+    def recreate_wal_archive_dir(self):
+        wal_archive_dir = self.wal_archive_location
+        operating_system.remove(wal_archive_dir, force=True, recursive=True,
+                                as_root=True)
+        operating_system.create_directory(wal_archive_dir,
+                                          user=self.pgsql_owner,
+                                          group=self.pgsql_owner,
+                                          force=True, as_root=True)
+
+    def remove_wal_archive_dir(self):
+        wal_archive_dir = self.wal_archive_location
+        operating_system.remove(wal_archive_dir, force=True, recursive=True,
+                                as_root=True)
+
+    def is_root_enabled(self, context):
+        """Return True if there is a superuser account enabled.
+        """
+        results = self.build_admin().query(
+            pgsql_query.UserQuery.list_root(),
+            timeout=30,
+        )
 
-    def build_root_user(self, password=None):
-        return models.PostgreSQLUser.root(name='root', password=password)
+        # There should be only one superuser (Trove's administrative account).
+        return len(results) > 1 or (results[0][0] != self.ADMIN_USER)
 
-    def enable_root(self, root_password=None):
+    def enable_root(self, context, root_password=None):
         """Create a superuser user or reset the superuser password.
 
         The default PostgreSQL administration account is 'postgres'.
         This account always exists and cannot be removed.
         Its attributes and access can however be altered.
 
         Clients can connect from the localhost or remotely via TCP/IP:
@@ -395,404 +488,557 @@
         This system account has no password and is *locked* by default,
         so that it can be used by *local* users only.
         It should *never* be enabled (or its password set)!!!
         That would just open up a new attack vector on the system account.
 
         Remote clients should use a build-in *database* account of the same
         name. It's password can be changed using the "ALTER USER" statement.
-        """
-        root = self.build_root_user(root_password)
-        results = self.query(query.UserQuery.list_root(self.ignore_users))
-        cur_roots = [row[0] for row in results]
-        if 'root' not in cur_roots:
-            self.create_user(root)
 
-        self.alter_user(root, None, *PgSqlAdmin.ADMIN_OPTIONS)
-        return root.serialize()
+        Access to this account is disabled by Trove exposed only once the
+        superuser access is requested.
+        Trove itself creates its own administrative account.
+
+            {"_name": "postgres", "_password": "<secret>"}
+        """
+        user = self.build_root_user(root_password)
+        self.build_admin().alter_user(
+            context, user, None, *PgSqlAdmin.ADMIN_OPTIONS)
+        return user.serialize()
 
-    def disable_root(self):
-        """Generate a new random password for the public superuser account.
+    def build_root_user(self, password=None):
+        return models.PostgreSQLUser.root(password=password)
+
+    def pg_start_backup(self, backup_label):
+        r = self.build_admin().query(
+            "SELECT pg_start_backup('%s', true)" % backup_label)
+        return r[0][0]
+
+    def pg_xlogfile_name(self, start_segment):
+        r = self.build_admin().query(
+            "SELECT pg_xlogfile_name('%s')" % start_segment)
+        return r[0][0]
+
+    def pg_stop_backup(self):
+        r = self.build_admin().query("SELECT pg_stop_backup()")
+        return r[0][0]
 
+    def disable_root(self, context):
+        """Generate a new random password for the public superuser account.
         Do not disable its access rights. Once enabled the account should
         stay that way.
         """
-        self.enable_root()
+        self.enable_root(context)
 
-    def list_root(self, ignore=()):
-        """Query to list all superuser accounts."""
-        statement = (
-            "SELECT usename FROM pg_catalog.pg_user WHERE usesuper = true"
-        )
+    def enable_root_with_password(self, context, root_password=None):
+        return self.enable_root(context, root_password)
 
-        for name in ignore:
-            statement += " AND usename != '{name}'".format(name=name)
+    @property
+    def wal_archive_location(self):
+        return cfg.get_configuration_property('wal_archive_location')
+
+    @property
+    def backup_strategy(self):
+        return cfg.get_configuration_property('backup_strategy')
+
+    def save_files_pre_upgrade(self, mount_point):
+        LOG.debug('Saving files pre-upgrade.')
+        mnt_etc_dir = os.path.join(mount_point, 'save_etc')
+        if self.OS not in [operating_system.REDHAT]:
+            # No need to store the config files away for Redhat because
+            # they are already stored in the data volume.
+            operating_system.remove(mnt_etc_dir, force=True, as_root=True)
+            operating_system.copy(self.pgsql_config_dir, mnt_etc_dir,
+                                  preserve=True, recursive=True, as_root=True)
+        return {'save_etc': mnt_etc_dir}
+
+    def restore_files_post_upgrade(self, upgrade_info):
+        LOG.debug('Restoring files post-upgrade.')
+        if self.OS not in [operating_system.REDHAT]:
+            # No need to restore the config files for Redhat because
+            # they are already in the data volume.
+            operating_system.copy('%s/.' % upgrade_info['save_etc'],
+                                  self.pgsql_config_dir,
+                                  preserve=True, recursive=True,
+                                  force=True, as_root=True)
+            operating_system.remove(upgrade_info['save_etc'], force=True,
+                                    as_root=True)
+        self.configuration_manager.refresh_cache()
+        self.status.set_ready()
 
-        return statement
 
-    def grant_access(self, username, hostname, databases):
+class PgSqlAppStatus(service.BaseDbStatus):
+
+    HOST = 'localhost'
+
+    def __init__(self, tools_dir):
+        super(PgSqlAppStatus, self).__init__()
+        self._cmd = guestagent_utils.build_file_path(tools_dir, 'pg_isready')
+
+    def _get_actual_db_status(self):
+        try:
+            utils.execute_with_timeout(
+                self._cmd, '-h', self.HOST, log_output_on_error=True)
+            return instance.ServiceStatuses.RUNNING
+        except exception.ProcessExecutionError:
+            return instance.ServiceStatuses.SHUTDOWN
+        except utils.Timeout:
+            return instance.ServiceStatuses.BLOCKED
+        except Exception:
+            LOG.exception(_("Error getting Postgres status."))
+            return instance.ServiceStatuses.CRASHED
+
+        return instance.ServiceStatuses.SHUTDOWN
+
+
+class PgSqlAdmin(object):
+
+    # Default set of options of an administrative account.
+    ADMIN_OPTIONS = (
+        'SUPERUSER', 'CREATEDB', 'CREATEROLE', 'INHERIT', 'REPLICATION',
+        'LOGIN'
+    )
+
+    def __init__(self, user):
+        port = cfg.get_configuration_property('postgresql_port')
+        self.__connection = PostgresLocalhostConnection(user.name, port=port)
+
+    def grant_access(self, context, username, hostname, databases):
         """Give a user permission to use a given database.
 
         The username and hostname parameters are strings.
         The databases parameter is a list of strings representing the names of
         the databases to grant permission on.
         """
         for database in databases:
-            LOG.info(f"Granting user {username} access to database {database}")
+            LOG.info(
+                _("{guest_id}: Granting user ({user}) access to database "
+                    "({database}).").format(
+                        guest_id=CONF.guest_id,
+                        user=username,
+                        database=database,)
+            )
             self.psql(
-                query.AccessQuery.grant(
+                pgsql_query.AccessQuery.grant(
                     user=username,
                     database=database,
-                )
+                ),
+                timeout=30,
             )
 
-    def revoke_access(self, username, hostname, database):
+    def revoke_access(self, context, username, hostname, database):
         """Revoke a user's permission to use a given database.
 
         The username and hostname parameters are strings.
         The database parameter is a string representing the name of the
         database.
         """
-        LOG.info(f"Revoking user ({username}) access to database {database}")
+        LOG.info(
+            _("{guest_id}: Revoking user ({user}) access to database"
+                "({database}).").format(
+                    guest_id=CONF.guest_id,
+                    user=username,
+                    database=database,)
+        )
         self.psql(
-            query.AccessQuery.revoke(
+            pgsql_query.AccessQuery.revoke(
                 user=username,
                 database=database,
-            )
+            ),
+            timeout=30,
         )
 
-    def list_access(self, username, hostname):
+    def list_access(self, context, username, hostname):
         """List database for which the given user as access.
         Return a list of serialized Postgres databases.
         """
-        user = self._find_user(username)
-        return user.databases if user is not None else []
+        user = self._find_user(context, username)
+        if user is not None:
+            return user.databases
+
+        raise exception.UserNotFound(username)
 
-    def create_databases(self, databases):
+    def create_database(self, context, databases):
         """Create the list of specified databases.
 
         The databases parameter is a list of serialized Postgres databases.
         """
         for database in databases:
-            self.create_database(models.PostgreSQLSchema.deserialize(database))
+            self._create_database(
+                context,
+                models.PostgreSQLSchema.deserialize(database))
 
-    def create_database(self, database):
+    def _create_database(self, context, database):
         """Create a database.
 
         :param database:          Database to be created.
         :type database:           PostgreSQLSchema
         """
-        LOG.info(f"Creating database {database.name}")
+        LOG.info(
+            _("{guest_id}: Creating database {name}.").format(
+                guest_id=CONF.guest_id,
+                name=database.name,
+            )
+        )
         self.psql(
-            query.DatabaseQuery.create(
+            pgsql_query.DatabaseQuery.create(
                 name=database.name,
                 encoding=database.character_set,
                 collation=database.collate,
-            )
+            ),
+            timeout=30,
         )
 
-    def delete_database(self, database):
+    def delete_database(self, context, database):
         """Delete the specified database.
         """
         self._drop_database(
             models.PostgreSQLSchema.deserialize(database))
 
     def _drop_database(self, database):
         """Drop a given Postgres database.
 
         :param database:          Database to be dropped.
         :type database:           PostgreSQLSchema
         """
-        LOG.info(f"Dropping database {database.name}")
-        self.psql(query.DatabaseQuery.drop(name=database.name))
+        LOG.info(
+            _("{guest_id}: Dropping database {name}.").format(
+                guest_id=CONF.guest_id,
+                name=database.name,
+            )
+        )
+        self.psql(
+            pgsql_query.DatabaseQuery.drop(name=database.name),
+            timeout=30,
+        )
+
+    def list_databases(self, context, limit=None, marker=None,
+                       include_marker=False):
+        """List all databases on the instance.
+        Return a paginated list of serialized Postgres databases.
+        """
 
-    def list_databases(self, limit=None, marker=None, include_marker=False):
         return guestagent_utils.serialize_list(
             self._get_databases(),
             limit=limit, marker=marker, include_marker=include_marker)
 
     def _get_databases(self):
         """Return all non-system Postgres databases on the instance."""
         results = self.query(
-            query.DatabaseQuery.list(ignore=self.ignore_dbs)
+            pgsql_query.DatabaseQuery.list(ignore=self.ignore_dbs),
+            timeout=30,
         )
         return [models.PostgreSQLSchema(
             row[0].strip(), character_set=row[1], collate=row[2])
             for row in results]
 
-    def create_users(self, users):
+    def create_user(self, context, users):
         """Create users and grant privileges for the specified databases.
 
         The users parameter is a list of serialized Postgres users.
         """
         for user in users:
-            self.create_user(models.PostgreSQLUser.deserialize(user), None)
+            self._create_user(
+                context,
+                models.PostgreSQLUser.deserialize(user), None)
 
-    def create_user(self, user, encrypt_password=None, *options):
+    def _create_user(self, context, user, encrypt_password=None, *options):
         """Create a user and grant privileges for the specified databases.
 
         :param user:              User to be created.
         :type user:               PostgreSQLUser
 
         :param encrypt_password:  Store passwords encrypted if True.
                                   Fallback to configured default
                                   behavior if None.
         :type encrypt_password:   boolean
 
         :param options:           Other user options.
         :type options:            list
         """
-        with_clause = query.UserQuery._build_with_clause(
-            '<SANITIZED>',
-            encrypt_password,
-            *options
+        LOG.info(
+            _("{guest_id}: Creating user {user} {with_clause}.")
+            .format(
+                guest_id=CONF.guest_id,
+                user=user.name,
+                with_clause=pgsql_query.UserQuery._build_with_clause(
+                    '<SANITIZED>',
+                    encrypt_password,
+                    *options
+                ),
+            )
         )
-        LOG.info(f"Creating user {user.name} {with_clause}")
-
         self.psql(
-            query.UserQuery.create(
+            pgsql_query.UserQuery.create(
                 user.name,
                 user.password,
                 encrypt_password,
                 *options
-            )
+            ),
+            timeout=30,
         )
         self._grant_access(
-            user.name,
-            [models.PostgreSQLSchema.deserialize(db) for db in user.databases])
+            context, user.name,
+            [models.PostgreSQLSchema.deserialize(db)
+             for db in user.databases])
 
-    def create_admin_user(self, user, encrypt_password=None):
-        self.create_user(user, encrypt_password, *self.ADMIN_OPTIONS)
+    def _create_admin_user(self, context, user, encrypt_password=None):
+        self._create_user(context, user, encrypt_password, *self.ADMIN_OPTIONS)
 
-    def _grant_access(self, username, databases):
+    def _grant_access(self, context, username, databases):
         self.grant_access(
+            context,
             username,
             None,
             [db.name for db in databases],
         )
 
-    def list_users(self, limit=None, marker=None, include_marker=False):
+    def list_users(
+            self, context, limit=None, marker=None, include_marker=False):
         """List all users on the instance along with their access permissions.
         Return a paginated list of serialized Postgres users.
         """
         return guestagent_utils.serialize_list(
-            self._get_users(),
+            self._get_users(context),
             limit=limit, marker=marker, include_marker=include_marker)
 
-    def _get_users(self):
+    def _get_users(self, context):
         """Return all non-system Postgres users on the instance."""
         results = self.query(
-            query.UserQuery.list(ignore=self.ignore_users)
+            pgsql_query.UserQuery.list(ignore=self.ignore_users),
+            timeout=30,
         )
 
         names = set([row[0].strip() for row in results])
-        return [self._build_user(name, results) for name in names]
+        return [self._build_user(context, name, results) for name in names]
 
-    def _build_user(self, username, acl=None):
+    def _build_user(self, context, username, acl=None):
         """Build a model representation of a Postgres user.
-
         Include all databases it has access to.
         """
         user = models.PostgreSQLUser(username)
         if acl:
             dbs = [models.PostgreSQLSchema(row[1].strip(),
                                            character_set=row[2],
                                            collate=row[3])
                    for row in acl if row[0] == username and row[1] is not None]
             for d in dbs:
                 user.databases.append(d.serialize())
 
         return user
 
-    def delete_user(self, user):
+    def delete_user(self, context, user):
         """Delete the specified user.
         """
-        self._drop_user(models.PostgreSQLUser.deserialize(user))
+        self._drop_user(
+            context, models.PostgreSQLUser.deserialize(user))
 
-    def _drop_user(self, user):
+    def _drop_user(self, context, user):
         """Drop a given Postgres user.
 
         :param user:              User to be dropped.
         :type user:               PostgreSQLUser
         """
         # Postgresql requires that you revoke grants before dropping the user
-        databases = list(self.list_access(user.name, None))
-        for db in databases:
-            db_schema = models.PostgreSQLSchema.deserialize(db)
-            self.revoke_access(user.name, None, db_schema.name)
-
-        LOG.info(f"Dropping user {user.name}")
-        self.psql(query.UserQuery.drop(name=user.name))
+        dbs = self.list_access(context, user.name, None)
+        for d in dbs:
+            db = models.PostgreSQLSchema.deserialize(d)
+            self.revoke_access(context, user.name, None, db.name)
+
+        LOG.info(
+            _("{guest_id}: Dropping user {name}.").format(
+                guest_id=CONF.guest_id,
+                name=user.name,
+            )
+        )
+        self.psql(
+            pgsql_query.UserQuery.drop(name=user.name),
+            timeout=30,
+        )
 
-    def get_user(self, username, hostname):
+    def get_user(self, context, username, hostname):
         """Return a serialized representation of a user with a given name.
         """
-        user = self._find_user(username)
+        user = self._find_user(context, username)
         return user.serialize() if user is not None else None
 
-    def _find_user(self, username):
+    def _find_user(self, context, username):
         """Lookup a user with a given username.
-
         Return a new Postgres user instance or None if no match is found.
         """
-        results = self.query(query.UserQuery.get(name=username))
+        results = self.query(
+            pgsql_query.UserQuery.get(name=username),
+            timeout=30,
+        )
 
         if results:
-            return self._build_user(username, results)
+            return self._build_user(context, username, results)
 
         return None
 
     def user_exists(self, username):
         """Return whether a given user exists on the instance."""
-        results = self.query(query.UserQuery.get(name=username))
+        results = self.query(
+            pgsql_query.UserQuery.get(name=username),
+            timeout=30,
+        )
 
         return bool(results)
 
-    def change_passwords(self, users):
+    def change_passwords(self, context, users):
         """Change the passwords of one or more existing users.
-
         The users parameter is a list of serialized Postgres users.
         """
         for user in users:
             self.alter_user(
-                models.PostgreSQLUser.deserialize(user))
+                context,
+                models.PostgreSQLUser.deserialize(user), None)
 
-    def alter_user(self, user, encrypt_password=None, *options):
+    def alter_user(self, context, user, encrypt_password=None, *options):
         """Change the password and options of an existing users.
 
         :param user:              User to be altered.
         :type user:               PostgreSQLUser
 
         :param encrypt_password:  Store passwords encrypted if True.
                                   Fallback to configured default
                                   behavior if None.
         :type encrypt_password:   boolean
 
         :param options:           Other user options.
         :type options:            list
         """
-        with_clause = query.UserQuery._build_with_clause(
-            '<SANITIZED>',
-            encrypt_password,
-            *options
+        LOG.info(
+            _("{guest_id}: Altering user {user} {with_clause}.")
+            .format(
+                guest_id=CONF.guest_id,
+                user=user.name,
+                with_clause=pgsql_query.UserQuery._build_with_clause(
+                    '<SANITIZED>',
+                    encrypt_password,
+                    *options
+                ),
+            )
         )
-        LOG.info(f"Altering user {user.name} {with_clause}")
-
         self.psql(
-            query.UserQuery.alter_user(
+            pgsql_query.UserQuery.alter_user(
                 user.name,
                 user.password,
                 encrypt_password,
-                *options)
+                *options),
+            timeout=30,
         )
 
-    def update_attributes(self, username, hostname, user_attrs):
+    def update_attributes(self, context, username, hostname, user_attrs):
         """Change the attributes of one existing user.
 
         The username and hostname parameters are strings.
         The user_attrs parameter is a dictionary in the following form:
 
             {"password": "", "name": ""}
 
         Each key/value pair in user_attrs is optional.
         """
-        user = self._build_user(username)
+        user = self._build_user(context, username)
         new_username = user_attrs.get('name')
         new_password = user_attrs.get('password')
 
         if new_username is not None:
-            self._rename_user(user, new_username)
+            self._rename_user(context, user, new_username)
             # Make sure we can retrieve the renamed user.
-            user = self._find_user(new_username)
+            user = self._find_user(context, new_username)
             if user is None:
-                raise exception.TroveError(
-                    "Renamed user %s could not be found on the instance."
+                raise exception.TroveError(_(
+                    "Renamed user %s could not be found on the instance.")
                     % new_username)
 
         if new_password is not None:
             user.password = new_password
-            self.alter_user(user)
+            self.alter_user(context, user)
 
-    def _rename_user(self, user, new_username):
-        """Rename a Postgres user and transfer all access to the new name.
+    def _rename_user(self, context, user, new_username):
+        """Rename a given Postgres user and transfer all access to the
+        new name.
 
         :param user:              User to be renamed.
         :type user:               PostgreSQLUser
         """
-        LOG.info(f"Changing username for {user.name} to {new_username}")
+        LOG.info(
+            _("{guest_id}: Changing username for {old} to {new}.").format(
+                guest_id=CONF.guest_id,
+                old=user.name,
+                new=new_username,
+            )
+        )
         # PostgreSQL handles the permission transfer itself.
         self.psql(
-            query.UserQuery.update_name(
+            pgsql_query.UserQuery.update_name(
                 old=user.name,
                 new=new_username,
-            )
+            ),
+            timeout=30,
         )
 
-    def psql(self, statement):
+    def psql(self, statement, timeout=30):
         """Execute a non-returning statement (usually DDL);
         Turn autocommit ON (this is necessary for statements that cannot run
         within an implicit transaction, like CREATE DATABASE).
         """
-        return self.connection.execute(statement)
+        return self.__connection.execute(statement)
 
-    def query(self, query):
+    def query(self, query, timeout=30):
         """Execute a query and return the result set.
         """
-        return self.connection.query(query)
+        return self.__connection.query(query)
 
     @property
     def ignore_users(self):
         return cfg.get_ignored_users()
 
     @property
     def ignore_dbs(self):
         return cfg.get_ignored_dbs()
 
 
 class PostgresConnection(object):
-    def __init__(self, user, password=None,
-                 host=constants.POSTGRESQL_HOST_SOCKET_PATH,
-                 port=5432):
-        """Utility class to communicate with PostgreSQL.
-
-        Connect with socket rather than IP or localhost address to avoid
-        manipulation of pg_hba.conf when the database is running inside
-        container with bridge network.
-
-        This class is consistent with PostgresConnection in
-        backup/utils/postgresql.py
-        """
-        self.user = user
-        self.password = password
-        self.host = host
-        self.port = port
 
-        self.connect_str = (f"user='{self.user}' password='{self.password}' "
-                            f"host='{self.host}' port='{self.port}'")
+    def __init__(self, **connection_args):
+        self._connection_args = connection_args
 
     def execute(self, statement, identifiers=None, data_values=None):
         """Execute a non-returning statement.
         """
         self._execute_stmt(statement, identifiers, data_values, False,
                            autocommit=True)
 
     def query(self, query, identifiers=None, data_values=None):
         """Execute a query and return the result set.
         """
         return self._execute_stmt(query, identifiers, data_values, True)
 
     def _execute_stmt(self, statement, identifiers, data_values, fetch,
                       autocommit=False):
-        cmd = self._bind(statement, identifiers)
-        connection = psycopg2.connect(self.connect_str)
-        connection.autocommit = autocommit
-        try:
-            with connection.cursor() as cursor:
-                cursor.execute(cmd, data_values)
-                if fetch:
-                    return cursor.fetchall()
-        finally:
-            connection.close()
+        if statement:
+            with psycopg2.connect(**self._connection_args) as connection:
+                connection.autocommit = autocommit
+                with connection.cursor() as cursor:
+                    cursor.execute(
+                        self._bind(statement, identifiers), data_values)
+                    if fetch:
+                        return cursor.fetchall()
+        else:
+            raise exception.UnprocessableEntity(_("Invalid SQL statement: %s")
+                                                % statement)
 
     def _bind(self, statement, identifiers):
         if identifiers:
             return statement.format(*identifiers)
         return statement
+
+
+class PostgresLocalhostConnection(PostgresConnection):
+
+    HOST = 'localhost'
+
+    def __init__(self, user, password=None, port=5432):
+        super(PostgresLocalhostConnection, self).__init__(
+            user=user, password=password,
+            host=self.HOST, port=port)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/datastore/service.py` & `trove-8.0.1/trove/guestagent/datastore/experimental/db2/service.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,564 +1,632 @@
-# Copyright 2011 OpenStack Foundation
+# Copyright 2015 IBM Corp.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+
 import os
-import re
-import time
 
 from oslo_log import log as logging
-from oslo_utils import timeutils
+from oslo_utils import encodeutils
 
-from trove.backup.state import BackupState
 from trove.common import cfg
-from trove.common import context as trove_context
+from trove.common.db import models
 from trove.common import exception
 from trove.common.i18n import _
-from trove.common import stream_codecs
-from trove.conductor import api as conductor_api
+from trove.common import instance as rd_instance
+from trove.common.stream_codecs import PropertiesCodec
+from trove.common import utils as utils
+from trove.guestagent.common.configuration import ConfigurationManager
+from trove.guestagent.common.configuration import ImportOverrideStrategy
 from trove.guestagent.common import guestagent_utils
 from trove.guestagent.common import operating_system
-from trove.guestagent.utils import docker as docker_util
-from trove.instance import service_status
+from trove.guestagent.datastore.experimental.db2 import system
+from trove.guestagent.datastore import service
 
-LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
-BACKUP_LOG_RE = re.compile(r'.*Backup successfully, checksum: '
-                           r'(?P<checksum>.*), location: (?P<location>.*)')
+LOG = logging.getLogger(__name__)
+MOUNT_POINT = CONF.db2.mount_point
+FAKE_CFG = os.path.join(MOUNT_POINT, "db2.cfg.fake")
+DB2_DEFAULT_CFG = os.path.join(MOUNT_POINT, "db2_default_dbm.cfg")
 
 
-class BaseDbStatus(object):
+class DB2App(object):
     """
-    Answers the question "what is the status of the DB application on
-    this box?" The answer can be that the application is not installed, or
-    the state of the application is determined by calling a series of
-    commands.
-
-    This is a base class, subclasses must implement real logic for
-    determining current status of DB in get_actual_db_status()
+    Handles installation and configuration of DB2
+    on a Trove instance.
     """
-
-    GUESTAGENT_DIR = '/opt/trove-guestagent'
-    PREPARE_START_FILENAME = '.guestagent.prepare.start'
-    PREPARE_END_FILENAME = '.guestagent.prepare.end'
-
-    def __init__(self, docker_client):
-        self.status = None
-        self.docker_client = docker_client
-
-        self.__prepare_completed = None
-
-    @property
-    def prepare_completed(self):
-        if self.__prepare_completed is None:
-            # Force the file check
-            self.__refresh_prepare_completed()
-        return self.__prepare_completed
-
-    def __refresh_prepare_completed(self):
-        # Set the value of __prepared_completed based on the existence of
-        # the file.  This is required as the state is cached so this method
-        # must be called any time the existence of the file changes.
-        is_file = os.path.isfile(
-            guestagent_utils.build_file_path(
-                self.GUESTAGENT_DIR, self.PREPARE_END_FILENAME))
-        self.__prepare_completed = is_file if is_file else None
-
-    def begin_install(self):
-        """First call of the DB prepare."""
-        prepare_start_file = guestagent_utils.build_file_path(
-            self.GUESTAGENT_DIR, self.PREPARE_START_FILENAME)
-        operating_system.write_file(prepare_start_file, '')
-        self.__refresh_prepare_completed()
-
-        self.set_status(service_status.ServiceStatuses.BUILDING, True)
-
-    def set_ready(self):
-        prepare_end_file = guestagent_utils.build_file_path(
-            self.GUESTAGENT_DIR, self.PREPARE_END_FILENAME)
-        operating_system.write_file(prepare_end_file, '')
-        self.__refresh_prepare_completed()
-
-    def end_install(self, error_occurred=False, post_processing=False):
-        """Called after prepare has ended."""
-
-        # Set the "we're done" flag if there's no error and
-        # no post_processing is necessary
-        if not (error_occurred or post_processing):
-            self.set_ready()
-
-        final_status = None
-        if error_occurred:
-            final_status = service_status.ServiceStatuses.FAILED
-        elif post_processing:
-            final_status = service_status.ServiceStatuses.INSTANCE_READY
-
-        if final_status:
-            LOG.info("Set final status to %s.", final_status)
-            self.set_status(final_status, force=True)
-        else:
-            self._end_install(True)
-
-    def _end_install(self, force):
-        """Called after DB is installed.
-
-        Updates the database with the actual DB server status.
-        """
-        real_status = self.get_actual_db_status()
-        LOG.info("Current database status is '%s'.", real_status)
-        self.set_status(real_status, force=force)
-
-    def get_actual_db_status(self):
-        raise NotImplementedError()
-
-    @property
-    def is_installed(self):
-        """
-        True if DB app should be installed and attempts to ascertain
-        its status won't result in nonsense.
+    def __init__(self, status, state_change_wait_time=None):
+        LOG.debug("Initialize DB2App.")
+        self.state_change_wait_time = (
+            state_change_wait_time if state_change_wait_time else
+            CONF.state_change_wait_time
+        )
+        LOG.debug("state_change_wait_time = %s.", self.state_change_wait_time)
+        self.status = status
+        self.dbm_default_config = {}
+        self.init_config()
+        '''
+        If DB2 guest agent has been configured for online backups,
+        every database that is created will be configured for online
+        backups. Since online backups are done using archive logging,
+        we need to create a directory to store the archived logs.
+        '''
+        if CONF.db2.backup_strategy == 'DB2OnlineBackup':
+            create_db2_dir(system.DB2_ARCHIVE_LOGS_DIR)
+
+    def init_config(self):
+        if not operating_system.exists(MOUNT_POINT, True):
+            operating_system.create_directory(MOUNT_POINT,
+                                              system.DB2_INSTANCE_OWNER,
+                                              system.DB2_INSTANCE_OWNER,
+                                              as_root=True)
+        """
+        The database manager configuration file - db2systm is stored  under the
+        /home/db2inst1/sqllib directory. To update the configuration
+        parameters, DB2 recommends using the command - UPDATE DBM CONFIGURATION
+        commands instead of directly updating the config file.
+
+        The existing PropertiesCodec implementation has been reused to handle
+        text-file operations. Configuration overrides are implemented using
+        the ImportOverrideStrategy of the guestagent configuration manager.
         """
-        return self.prepare_completed
-
-    @property
-    def is_running(self):
-        """True if DB server is running."""
-        return (self.status is not None and
-                self.status in [service_status.ServiceStatuses.RUNNING,
-                                service_status.ServiceStatuses.HEALTHY])
-
-    def set_status(self, status, force=False):
-        """Use conductor to update the DB app status."""
-
-        if force or self.is_installed:
-            LOG.debug("Casting set_status message to conductor "
-                      "(status is '%s').", status.description)
-            context = trove_context.TroveContext()
-
-            heartbeat = {'service_status': status.description}
-            conductor_api.API(context).heartbeat(
-                CONF.guest_id, heartbeat,
-                sent=timeutils.utcnow_ts(microsecond=True))
-            LOG.debug("Successfully cast set_status.")
-            self.status = status
-        else:
-            LOG.debug("Prepare has not completed yet, skipping heartbeat.")
-
-    def update(self):
-        """Find and report status of DB on this machine.
-        The database is updated and the status is also returned.
+        LOG.debug("Initialize DB2 configuration")
+        revision_dir = (
+            guestagent_utils.build_file_path(
+                os.path.join(MOUNT_POINT,
+                             os.path.dirname(system.DB2_INSTANCE_OWNER)),
+                ConfigurationManager.DEFAULT_STRATEGY_OVERRIDES_SUB_DIR)
+        )
+        if not operating_system.exists(FAKE_CFG):
+            operating_system.write_file(FAKE_CFG, '', as_root=True)
+            operating_system.chown(FAKE_CFG, system.DB2_INSTANCE_OWNER,
+                                   system.DB2_INSTANCE_OWNER, as_root=True)
+        self.configuration_manager = (
+            ConfigurationManager(FAKE_CFG, system.DB2_INSTANCE_OWNER,
+                                 system.DB2_INSTANCE_OWNER,
+                                 PropertiesCodec(delimiter='='),
+                                 requires_root=True,
+                                 override_strategy=ImportOverrideStrategy(
+                                     revision_dir, "cnf"))
+        )
+        '''
+        Below we are getting the database manager default configuration and
+        saving it to the DB2_DEFAULT_CFG file. This is done to help with
+        correctly resetting the configurations to the original values when
+        user wants to detach a user-defined configuration group from an
+        instance. DB2 provides a command to reset the database manager
+        configuration parameters (RESET DBM CONFIGURATION) but this command
+        resets all the configuration parameters to the system defaults. When
+        we build a DB2 guest image there are certain configurations
+        parameters like SVCENAME which we set so that the instance can start
+        correctly. Hence resetting this value to the system default will
+        render the instance in an unstable state. Instead, the recommended
+        way for resetting a subset of configuration parameters is to save
+        the output of GET DBM CONFIGURATION of the original configuration
+        and then call UPDATE DBM CONFIGURATION to reset the value.
+          http://www.ibm.com/support/knowledgecenter/SSEPGG_10.5.0/
+        com.ibm.db2.luw.admin.cmd.doc/doc/r0001970.html
+        '''
+        if not operating_system.exists(DB2_DEFAULT_CFG):
+            run_command(system.GET_DBM_CONFIGURATION % {
+                "dbm_config": DB2_DEFAULT_CFG})
+        self.process_default_dbm_config()
+
+    def process_default_dbm_config(self):
+        """
+        Once the default database manager configuration is saved to
+        DB2_DEFAULT_CFG, we try to store the configuration parameters
+        and values into a dictionary object, dbm_default_config. For
+        example, a sample content of the database manager configuration
+        file looks like this:
+         Buffer pool                         (DFT_MON_BUFPOOL) = OFF
+        We need to process this so that we key it on the configuration
+        parameter DFT_MON_BUFPOOL.
+        """
+        with open(DB2_DEFAULT_CFG) as cfg_file:
+            for line in cfg_file:
+                if '=' in line:
+                    item = line.rstrip('\n').split(' = ')
+                    fIndex = item[0].rfind('(')
+                    lIndex = item[0].rfind(')')
+                    if fIndex > -1:
+                        param = item[0][fIndex + 1: lIndex]
+                        value = item[1]
+                        '''
+                        Some of the configuration parameters have the keyword
+                        AUTOMATIC to indicate that DB2 will automatically
+                        adjust the setting depending on system resources.
+                        For some configuration parameters, DB2 also allows
+                        setting a starting value along with the AUTOMATIC
+                        setting. In the configuration parameter listing,
+                        this is displayed as:
+                        MON_HEAP_SZ = AUTOMATIC(90)
+                        This can be set using the following command:
+                        db2 update dbm cfg using mon_heap_sz 90 automatic
+                        '''
+                        if not value:
+                            value = 'NULL'
+                        elif 'AUTOMATIC' in value:
+                            fIndex = item[1].rfind('(')
+                            lIndex = item[1].rfind(')')
+                            if fIndex > -1:
+                                default_value = item[1][fIndex + 1: lIndex]
+                                value = default_value + " AUTOMATIC"
+                        self.dbm_default_config.update({param: value})
+
+    def update_hostname(self):
+        """
+        When DB2 server is installed, it uses the hostname of the
+        instance were the image was built. This needs to be updated
+        to reflect the guest instance.
         """
-        if self.is_installed:
-            status = self.get_actual_db_status()
-            self.set_status(status)
-
-    def start_db_service(self, service_candidates, timeout,
-                         enable_on_boot=True, update_db=False):
-        """Start the database service and wait for the database to become
-        available.
-        The service auto-start will be updated only if the service command
-        succeeds.
-
-        :param service_candidates:   List of possible system service names.
-        :type service_candidates:    list
-
-        :param timeout:              Wait timeout in seconds.
-        :type timeout:               integer
-
-        :param enable_on_boot:       Enable service auto-start.
-                                     The auto-start setting will be updated
-                                     only if the service command succeeds.
-        :type enable_on_boot:        boolean
-
-        :param update_db:            Suppress the Trove instance heartbeat.
-        :type update_db:             boolean
-
-        :raises:              :class:`RuntimeError` on failure.
+        LOG.debug("Update the hostname of the DB2 instance.")
+        try:
+            run_command(system.UPDATE_HOSTNAME,
+                        superuser='root')
+        except exception.ProcessExecutionError:
+            raise RuntimeError(_("Command to update the hostname failed."))
+
+    def change_ownership(self, mount_point):
+        """
+        When DB2 server instance is installed, it does not have the
+        DB2 local database directory created (/home/db2inst1/db2inst1).
+        This gets created when we mount the cinder volume. So we need
+        to change ownership of this directory to the DB2 instance user
+        - db2inst1.
         """
-        LOG.info("Starting database service.")
-        operating_system.start_service(service_candidates, timeout=timeout)
-
-        self.wait_for_database_service_start(timeout, update_db=update_db)
+        LOG.debug("Changing ownership of the DB2 data directory.")
+        try:
+            operating_system.chown(mount_point,
+                                   system.DB2_INSTANCE_OWNER,
+                                   system.DB2_INSTANCE_OWNER,
+                                   recursive=False, as_root=True)
+        except exception.ProcessExecutionError:
+            raise RuntimeError(_(
+                "Command to change ownership of  DB2 data directory failed."))
 
-        if enable_on_boot:
-            LOG.info("Enable service auto-start on boot.")
-            operating_system.enable_service_on_boot(service_candidates)
+    def _enable_db_on_boot(self):
+        LOG.debug("Enable DB on boot.")
+        try:
+            run_command(system.ENABLE_AUTOSTART)
+        except exception.ProcessExecutionError:
+            raise RuntimeError(_(
+                "Command to enable DB2 server on boot failed."))
 
-    def wait_for_database_service_start(self, timeout, update_db=False):
-        """Wait for the database to become available.
+    def _disable_db_on_boot(self):
+        LOG.debug("Disable DB2 on boot.")
+        try:
+            run_command(system.DISABLE_AUTOSTART)
+        except exception.ProcessExecutionError:
+            raise RuntimeError(_(
+                "Command to disable DB2 server on boot failed."))
+
+    def start_db_with_conf_changes(self, config_contents):
+        LOG.info(_("Starting DB2 with configuration changes."))
+        self.configuration_manager.save_configuration(config_contents)
+        self.start_db(True)
+
+    def start_db(self, update_db=False):
+        LOG.debug("Start the DB2 server instance.")
+        self._enable_db_on_boot()
+        try:
+            run_command(system.START_DB2)
+        except exception.ProcessExecutionError:
+            pass
+
+        if not self.status.wait_for_real_status_to_change_to(
+                rd_instance.ServiceStatuses.RUNNING,
+                self.state_change_wait_time, update_db):
+            LOG.error(_("Start of DB2 server instance failed."))
+            self.status.end_restart()
+            raise RuntimeError(_("Could not start DB2."))
+
+    def stop_db(self, update_db=False, do_not_start_on_reboot=False):
+        LOG.debug("Stop the DB2 server instance.")
+        if do_not_start_on_reboot:
+            self._disable_db_on_boot()
+        try:
+            run_command(system.STOP_DB2)
+        except exception.ProcessExecutionError:
+            pass
+
+        if not (self.status.wait_for_real_status_to_change_to(
+                rd_instance.ServiceStatuses.SHUTDOWN,
+                self.state_change_wait_time, update_db)):
+            LOG.error(_("Could not stop DB2."))
+            self.status.end_restart()
+            raise RuntimeError(_("Could not stop DB2."))
 
-        :param timeout:              Wait timeout in seconds.
-        :type timeout:               integer
+    def restart(self):
+        LOG.debug("Restarting DB2 server instance.")
+        try:
+            self.status.begin_restart()
+            self.stop_db()
+            self.start_db()
+        finally:
+            self.status.end_restart()
 
-        :param update_db:            Suppress the Trove instance heartbeat.
-        :type update_db:             boolean
+    def update_overrides(self, context, overrides, remove=False):
+        if overrides:
+            self.apply_overrides(overrides)
 
-        :raises:              :class:`RuntimeError` on failure.
-        """
-        LOG.debug("Waiting for database to start up.")
-        if not self._wait_for_database_service_status(
-                service_status.ServiceStatuses.RUNNING, timeout, update_db):
-            raise RuntimeError(_("Database failed to start."))
-
-        LOG.info("Database has started successfully.")
+    def remove_overrides(self):
+        config = self.configuration_manager.get_user_override()
+        self._reset_config(config)
+        self.configuration_manager.remove_user_override()
 
-    def stop_db_service(self, service_candidates, timeout,
-                        disable_on_boot=False, update_db=False):
-        """Stop the database service and wait for the database to shutdown.
+    def apply_overrides(self, overrides):
+        self._apply_config(overrides)
+        self.configuration_manager.apply_user_override(overrides)
 
-        :param service_candidates:   List of possible system service names.
-        :type service_candidates:    list
+    def _update_dbm_config(self, param, value):
+        try:
+            run_command(
+                system.UPDATE_DBM_CONFIGURATION % {
+                    "parameter": param,
+                    "value": value})
+        except exception.ProcessExecutionError:
+            LOG.exception(_("Failed to update config %s"), param)
+            raise
 
-        :param timeout:              Wait timeout in seconds.
-        :type timeout:               integer
+    def _reset_config(self, config):
+        try:
+            for k, v in config.iteritems():
+                default_cfg_value = self.dbm_default_config[k]
+                self._update_dbm_config(k, default_cfg_value)
+        except Exception:
+            LOG.exception(_("DB2 configuration reset failed."))
+            raise RuntimeError(_("DB2 configuration reset failed."))
+        LOG.info(_("DB2 configuration reset completed."))
 
-        :param disable_on_boot:      Disable service auto-start.
-                                     The auto-start setting will be updated
-                                     only if the service command succeeds.
-        :type disable_on_boot:       boolean
+    def _apply_config(self, config):
+        try:
+            for k, v in config.items():
+                self._update_dbm_config(k, v)
+        except Exception:
+            LOG.exception(_("DB2 configuration apply failed"))
+            raise RuntimeError(_("DB2 configuration apply failed"))
+        LOG.info(_("DB2 config apply completed."))
 
-        :param update_db:            Suppress the Trove instance heartbeat.
-        :type update_db:             boolean
 
-        :raises:              :class:`RuntimeError` on failure.
-        """
-        LOG.info("Stopping database service.")
-        operating_system.stop_service(service_candidates, timeout=timeout)
+class DB2AppStatus(service.BaseDbStatus):
+    """
+    Handles all of the status updating for the DB2 guest agent.
+    """
+    def _get_actual_db_status(self):
+        LOG.debug("Getting the status of the DB2 server instance.")
+        try:
+            out, err = utils.execute_with_timeout(
+                system.DB2_STATUS, shell=True)
+            if "0" not in out:
+                return rd_instance.ServiceStatuses.RUNNING
+            else:
+                return rd_instance.ServiceStatuses.SHUTDOWN
+        except exception.ProcessExecutionError:
+            LOG.exception(_("Error getting the DB2 server status."))
+            return rd_instance.ServiceStatuses.CRASHED
 
-        LOG.debug("Waiting for database to shutdown.")
-        if not self._wait_for_database_service_status(
-                service_status.ServiceStatuses.SHUTDOWN, timeout, update_db):
-            raise RuntimeError(_("Database failed to stop."))
 
-        LOG.info("Database has stopped successfully.")
+def run_command(command, superuser=system.DB2_INSTANCE_OWNER,
+                timeout=system.TIMEOUT):
+    return utils.execute_with_timeout("sudo", "su", "-", superuser, "-c",
+                                      command, timeout=timeout)
 
-        if disable_on_boot:
-            LOG.info("Disable service auto-start on boot.")
-            operating_system.disable_service_on_boot(service_candidates)
 
-    def _wait_for_database_service_status(self, status, timeout, update_db):
-        """Wait for the given database status.
+def create_db2_dir(dir_name):
+    if not operating_system.exists(dir_name, True):
+        operating_system.create_directory(dir_name,
+                                          system.DB2_INSTANCE_OWNER,
+                                          system.DB2_INSTANCE_OWNER,
+                                          as_root=True)
 
-        :param status:          The status to wait for.
-        :type status:           BaseDbStatus
 
-        :param timeout:         Wait timeout in seconds.
-        :type timeout:          integer
+def remove_db2_dir(dir_name):
+    operating_system.remove(dir_name,
+                            force=True,
+                            as_root=True)
 
-        :param update_db:       Suppress the Trove instance heartbeat.
-        :type update_db:        boolean
 
-        :returns:               True on success, False otherwise.
-        """
-        if not self.wait_for_real_status_to_change_to(
-                status, timeout, update_db):
-            LOG.info("Service status did not change to %(status)s "
-                     "within the given timeout: %(timeout)ds",
-                     {'status': status, 'timeout': timeout})
-            LOG.debug("Attempting to cleanup stalled services.")
+class DB2Admin(object):
+    """
+    Handles administrative tasks on the DB2 instance.
+    """
+    def create_database(self, databases):
+        """Create the given database(s)."""
+        dbName = None
+        db_create_failed = []
+        LOG.debug("Creating DB2 databases.")
+        for item in databases:
+            mydb = models.DatastoreSchema.deserialize(item)
+            mydb.check_create()
+            dbName = mydb.name
+            LOG.debug("Creating DB2 database: %s.", dbName)
             try:
-                self.cleanup_stalled_db_services()
-            except Exception:
-                LOG.debug("Cleanup failed.", exc_info=True)
-            return False
-
-        return True
-
-    def wait_for_status(self, status, max_time, update_db=False):
-        """Waits the given time for the real status to change to the one
-        specified.
-
-        The internal status is always updated. The public instance
-        state stored in the Trove database is updated only if "update_db" is
-        True.
-        """
-        end_time = time.time() + max_time
-
-        # since python does not support a real do-while loop, we have
-        # to emulate one. Hence these shenanigans. We force at least
-        # one pass into the loop and therefore it is safe that
-        # actual_status is initialized in the loop while it is used
-        # outside.
-        loop = True
-
-        # We need 5 (by default) consecutive success db connections for status
-        # 'HEALTHY'
-        healthy_count = 0
-        state_healthy_counts = CONF.state_healthy_counts - 1
-        while loop:
-            self.status = self.get_actual_db_status()
-            if self.status == status:
-                if (status == service_status.ServiceStatuses.HEALTHY and
-                        healthy_count < state_healthy_counts):
-                    healthy_count += 1
-                    time.sleep(CONF.state_change_poll_time)
-                    continue
-
-                if update_db:
-                    self.set_status(self.status)
-                return True
-
-            # should we remain in this loop? this is the thing
-            # that emulates the do-while construct.
-            loop = (time.time() < end_time)
-            # reset the healthy_count
-            healthy_count = 0
-            # no point waiting if our time is up and we're
-            # just going to error out anyway.
-            if loop:
-                LOG.debug("Waiting for DB status to change from "
-                          "%(actual_status)s to %(status)s.",
-                          {"actual_status": self.status, "status": status})
-
-                time.sleep(CONF.state_change_poll_time)
-
-        LOG.error("Timeout while waiting for database status to change."
-                  "Expected state %(status)s, "
-                  "current state is %(actual_status)s",
-                  {"status": status, "actual_status": self.status})
-        return False
-
-    def cleanup_stalled_db_services(self):
-        """An optional datastore-specific code to cleanup stalled
-        database services and other resources after a status change timeout.
-        """
-        LOG.debug("No cleanup action specified for this datastore.")
-
-    def report_root(self, context):
-        """Use conductor to update the root-enable status."""
-        LOG.debug("Casting report_root message to conductor.")
-        conductor_api.API(context).report_root(CONF.guest_id)
-        LOG.debug("Successfully cast report_root.")
-
-
-class BaseDbApp(object):
-    CFG_CODEC = stream_codecs.IniCodec()
-
-    def __init__(self, status, docker_client):
-        self.status = status
-        self.docker_client = docker_client
-
-    @classmethod
-    def get_client_auth_file(cls, file="os_admin.cnf"):
-        # Save the password inside the mount point directory so we could
-        # restore everyting when rebuilding the instance.
-        conf_dir = guestagent_utils.get_conf_dir()
-        return guestagent_utils.build_file_path(conf_dir, file)
-
-    @classmethod
-    def get_auth_password(cls, file="os_admin.cnf"):
-        auth_config = operating_system.read_file(
-            cls.get_client_auth_file(file), codec=cls.CFG_CODEC, as_root=True)
-        return auth_config['client']['password']
-
-    @classmethod
-    def save_password(cls, user, password):
-        content = {
-            'client': {
-                'user': user,
-                'password': password,
-                'host': "localhost"
-            }
-        }
-
-        conf_dir = guestagent_utils.get_conf_dir()
-        operating_system.write_file(
-            f'{conf_dir}/{user}.cnf', content, codec=cls.CFG_CODEC,
-            as_root=True)
-
-    def remove_overrides(self):
-        self.configuration_manager.remove_user_override()
-
-    def reset_configuration(self, configuration):
-        LOG.info("Resetting configuration.")
-        self.configuration_manager.reset_configuration(configuration)
-
-    def stop_db(self, update_db=False):
-        LOG.info("Stopping database.")
-
+                run_command(system.CREATE_DB_COMMAND % {'dbname': dbName})
+            except exception.ProcessExecutionError:
+                LOG.exception(_(
+                    "There was an error creating database: %s."), dbName)
+                db_create_failed.append(dbName)
+                pass
+
+            '''
+            Configure each database to do archive logging for online
+            backups. Once the database is configured, it will go in to a
+            BACKUP PENDING state. In this state, the database will not
+            be accessible for any operations. To get the database back to
+            normal mode, we have to do a full offline backup as soon as we
+            configure it for archive logging.
+            '''
+            try:
+                if CONF.db2.backup_strategy == 'DB2OnlineBackup':
+                    run_command(system.UPDATE_DB_LOG_CONFIGURATION % {
+                        'dbname': dbName})
+                    run_command(system.RECOVER_FROM_BACKUP_PENDING_MODE % {
+                        'dbname': dbName})
+            except exception.ProcessExecutionError:
+                LOG.exception(_(
+                    "There was an error while configuring the database for "
+                    "online backup: %s."), dbName)
+
+        if len(db_create_failed) > 0:
+            LOG.exception(_("Creating the following databases failed: %s."),
+                          db_create_failed)
+
+    def delete_database(self, database):
+        """Delete the specified database."""
+        dbName = None
         try:
-            docker_util.stop_container(self.docker_client)
-        except Exception:
-            LOG.exception("Failed to stop database")
-            raise exception.TroveError("Failed to stop database")
-
-        if not self.status.wait_for_status(
-            service_status.ServiceStatuses.SHUTDOWN,
-            CONF.state_change_wait_time, update_db
-        ):
-            raise exception.TroveError("Failed to stop database")
-
-    def start_db(self, *args, **kwargs):
-        pass
+            mydb = models.DatastoreSchema.deserialize(database)
+            mydb.check_delete()
+            dbName = mydb.name
+            LOG.debug("Deleting DB2 database: %s.", dbName)
+            run_command(system.DELETE_DB_COMMAND % {'dbname': dbName})
+        except exception.ProcessExecutionError:
+            LOG.exception(_(
+                "There was an error while deleting database:%s."), dbName)
+            raise exception.GuestError(original_message=_(
+                "Unable to delete database: %s.") % dbName)
+
+    def list_databases(self, limit=None, marker=None, include_marker=False):
+        LOG.debug("Listing all the DB2 databases.")
+        databases = []
+        next_marker = None
 
-    def start_db_with_conf_changes(self, config_contents, ds_version):
-        """Start the database with given configuration.
+        try:
+            out, err = run_command(system.LIST_DB_COMMAND)
+            dblist = out.split()
+            result = iter(dblist)
+            count = 0
+
+            if marker is not None:
+                try:
+                    item = next(result)
+                    while item != marker:
+                        item = next(result)
+
+                    if item == marker:
+                        marker = None
+                except StopIteration:
+                    pass
 
-        This method is called after resize.
-        """
-        LOG.info(f"Starting database service with new configuration and "
-                 f"datastore version {ds_version}.")
+            try:
+                item = next(result)
+                while item:
+                    count = count + 1
+                    if (limit and count <= limit) or limit is None:
+                        db2_db = models.DatastoreSchema(name=item)
+                        LOG.debug("database = %s .", item)
+                        next_marker = db2_db.name
+                        databases.append(db2_db.serialize())
+                        item = next(result)
+                    else:
+                        next_marker = None
+                        break
+            except StopIteration:
+                next_marker = None
+            LOG.debug("databases = %s.", str(databases))
+        except exception.ProcessExecutionError as pe:
+            err_msg = encodeutils.exception_to_unicode(pe)
+            LOG.exception(_("An error occurred listing databases: %s."),
+                          err_msg)
+            pass
+        return databases, next_marker
 
-        if self.status.is_running:
-            LOG.info("Stopping database before applying changes.")
-            self.stop_db()
+    def create_user(self, users):
+        LOG.debug("Creating user(s) for accessing DB2 database(s).")
+        try:
+            for item in users:
+                user = models.DatastoreUser.deserialize(item)
+                user.check_create()
+                try:
+                    LOG.debug("Creating OS user: %s.", user.name)
+                    utils.execute_with_timeout(
+                        system.CREATE_USER_COMMAND % {
+                            'login': user.name, 'login': user.name,
+                            'passwd': user.password}, shell=True)
+                except exception.ProcessExecutionError as pe:
+                    LOG.exception(_("Error creating user: %s."), user.name)
+                    continue
 
-        self.reset_configuration(config_contents)
-        self.start_db(update_db=True, ds_version=ds_version)
+                for database in user.databases:
+                    mydb = models.DatastoreSchema.deserialize(database)
+                    try:
+                        LOG.debug("Granting user: %(user)s access to "
+                                  "database: %(db)s.",
+                                  {'user': user.name, 'db': mydb.name})
+                        run_command(system.GRANT_USER_ACCESS % {
+                            'dbname': mydb.name, 'login': user.name})
+                    except exception.ProcessExecutionError as pe:
+                        LOG.debug("Error granting user: %(user)s access to "
+                                  "database: %(db)s.",
+                                  {'user': user.name, 'db': mydb.name})
+                        LOG.debug(pe)
+                        pass
+        except exception.ProcessExecutionError as pe:
+            LOG.exception(_("An error occurred creating users: %s."),
+                          pe.message)
+            pass
+
+    def delete_user(self, user):
+        LOG.debug("Delete a given user.")
+        db2_user = models.DatastoreUser.deserialize(user)
+        db2_user.check_delete()
+        userName = db2_user.name
+        user_dbs = db2_user.databases
+        LOG.debug("For user %(user)s, databases to be deleted = %(dbs)r.",
+                  {'user': userName, 'dbs': user_dbs})
 
-    @staticmethod
-    def _image_has_tag(image):
-        """
-        Whether docker_image being config with tag
-            "example.domain:5000/repo/image_name:tag",
-            "example.domain:5000/repo/image-name:tag",
-            "example.domain:5000/repo/image-name:tag_tag",
-            "example.domain:5000/repo/image_name:tag-tag",
-            "example.domain:5000/repo/image-name",
-            "example.domain:5000/repo/image_name",
-            "example.domain:5000:5000/repo/image-name",
-            "example.domain/repo/image-name",
-            "example.domain/repo/image-name:tag"
+        if len(user_dbs) == 0:
+            databases = self.list_access(db2_user.name, None)
+        else:
+            databases = user_dbs
 
-        Returns:
-            - True if match
-        """
-        return image.split('/')[-1].find(':') > 0
+        LOG.debug("databases for user = %r.", databases)
+        for database in databases:
+            mydb = models.DatastoreSchema.deserialize(database)
+            try:
+                run_command(system.REVOKE_USER_ACCESS % {
+                    'dbname': mydb.name,
+                    'login': userName})
+                LOG.debug("Revoked access for user:%(user)s on "
+                          "database:%(db)s.",
+                          {'user': userName, 'db': mydb.name})
+            except exception.ProcessExecutionError as pe:
+                LOG.debug("Error occurred while revoking access to %s.",
+                          mydb.name)
+                pass
+            try:
+                utils.execute_with_timeout(system.DELETE_USER_COMMAND % {
+                    'login': db2_user.name.lower()}, shell=True)
+            except exception.ProcessExecutionError as pe:
+                LOG.exception(_(
+                    "There was an error while deleting user: %s."), pe)
+                raise exception.GuestError(original_message=_(
+                    "Unable to delete user: %s.") % userName)
+
+    def list_users(self, limit=None, marker=None, include_marker=False):
+        LOG.debug(
+            "List all users for all the databases in a DB2 server instance.")
+        users = []
+        user_map = {}
+        next_marker = None
+        count = 0
+
+        databases, marker = self.list_databases()
+        for database in databases:
+            db2_db = models.DatastoreSchema.deserialize(database)
+            out = None
+            try:
+                out, err = run_command(
+                    system.LIST_DB_USERS % {'dbname': db2_db.name})
+            except exception.ProcessExecutionError:
+                LOG.debug(
+                    "There was an error while listing users for database: %s.",
+                    db2_db.name)
+                continue
+
+            userlist = []
+            for item in out.split('\n'):
+                LOG.debug("item = %r", item)
+                user = item.split() if item != "" else None
+                LOG.debug("user = %r", user)
+                if (user is not None
+                    and (user[0] not in cfg.get_ignored_users()
+                         and user[1] == 'Y')):
+                    userlist.append(user[0])
+            result = iter(userlist)
+
+            if marker is not None:
+                try:
+                    item = next(result)
+                    while item != marker:
+                        item = next(result)
+
+                    if item == marker:
+                        marker = None
+                except StopIteration:
+                    pass
 
-    def get_backup_image(self):
-        image = cfg.get_configuration_property('backup_docker_image')
-        if not self._image_has_tag(image):
-            ds_version = CONF.datastore_version
-            image = (f'{image}:latest' if not ds_version else
-                     f'{image}:{ds_version}')
-        return image
-
-    def get_backup_strategy(self):
-        return cfg.get_configuration_property('backup_strategy')
-
-    def create_backup(self, context, backup_info, volumes_mapping={},
-                      need_dbuser=True, extra_params=''):
-        storage_driver = CONF.storage_strategy
-        backup_driver = self.get_backup_strategy()
-        incremental = ''
-        backup_type = 'full'
-        if backup_info.get('parent'):
-            incremental = (
-                f'--incremental '
-                f'--parent-location={backup_info["parent"]["location"]} '
-                f'--parent-checksum={backup_info["parent"]["checksum"]}')
-            backup_type = 'incremental'
-
-        name = 'db_backup'
-        backup_id = backup_info["id"]
-        image = self.get_backup_image()
-        os_cred = (f"--os-token={context.auth_token} "
-                   f"--os-auth-url={CONF.service_credentials.auth_url} "
-                   f"--os-tenant-id={context.project_id} "
-                   f"--os-region-name={CONF.service_credentials.region_name}")
-
-        db_userinfo = ''
-        if need_dbuser:
-            admin_pass = self.get_auth_password()
-            # Use localhost to avoid host access verification.
-            db_userinfo = (f"--db-host=localhost --db-user=os_admin "
-                           f"--db-password={admin_pass}")
-
-        swift_metadata = (
-            f'datastore:{backup_info["datastore"]},'
-            f'datastore_version:{backup_info["datastore_version"]}'
-        )
-        swift_container = (backup_info.get('swift_container') or
-                           CONF.backup_swift_container)
-        swift_params = (f'--swift-extra-metadata={swift_metadata} '
-                        f'--swift-container={swift_container}')
-
-        command = (
-            f'/usr/bin/python3 main.py --backup --backup-id={backup_id} '
-            f'--storage-driver={storage_driver} --driver={backup_driver} '
-            f'{os_cred} '
-            f'{db_userinfo} '
-            f'{swift_params} '
-            f'{incremental} '
-            f'{extra_params}'
-        )
+            try:
+                item = next(result)
 
-        # Update backup status in db
-        conductor = conductor_api.API(context)
-        mount_point = cfg.get_configuration_property('mount_point')
-        stats = guestagent_utils.get_filesystem_volume_stats(mount_point)
-        backup_state = {
-            'backup_id': backup_id,
-            'size': stats.get('used', 0.0),
-            'state': BackupState.BUILDING,
-            'backup_type': backup_type
-        }
-        conductor.update_backup(CONF.guest_id,
-                                sent=timeutils.utcnow_ts(microsecond=True),
-                                **backup_state)
-        LOG.debug(f"Updated state for backup {backup_id} to {backup_state}")
-
-        # Start to run backup inside a separate docker container
-        try:
-            LOG.info(f'Starting to create backup {backup_id}, '
-                     f'command: {command}')
-            output, ret = docker_util.run_container(
-                self.docker_client, image, name,
-                volumes=volumes_mapping, command=command)
-            result = output[-1]
-            if not ret:
-                msg = f'Failed to run backup container, error: {result}'
-                LOG.error(msg)
-                raise Exception(msg)
-
-            backup_result = BACKUP_LOG_RE.match(result)
-            if backup_result:
-                backup_state.update({
-                    'checksum': backup_result.group('checksum'),
-                    'location': backup_result.group('location'),
-                    'success': True,
-                    'state': BackupState.COMPLETED,
-                })
-            else:
-                msg = f'Cannot parse backup output: {result}'
-                LOG.error(msg)
-                backup_state.update({
-                    'success': False,
-                    'state': BackupState.FAILED,
-                })
-
-                # The exception message is visible to the user
-                user_msg = msg
-                ex_regex = re.compile(r'.+Exception: (.+)')
-                for line in output[-5:-1]:
-                    m = ex_regex.search(line)
-                    if m:
-                        user_msg = m.group(1)
+                while item:
+                    '''
+                    Check if the user has already been discovered. If so,
+                    add this database to the database list for this user.
+                    '''
+                    if item in user_map:
+                        db2user = user_map.get(item)
+                        db2user.databases = db2_db.name
+                        item = next(result)
+                        continue
+                    '''
+                     If this user was not previously discovered, then add
+                     this to the user's list.
+                    '''
+                    count = count + 1
+                    if (limit and count <= limit) or limit is None:
+                        db2_user = models.DatastoreUser(name=item,
+                                                        databases=db2_db.name)
+                        users.append(db2_user.serialize())
+                        user_map.update({item: db2_user})
+                        item = next(result)
+                    else:
+                        next_marker = None
                         break
-                raise Exception(user_msg)
-        except Exception as err:
-            LOG.error("Failed to create backup %s", backup_id)
-            backup_state.update({
-                'success': False,
-                'state': BackupState.FAILED,
-            })
-            raise exception.TroveError(
-                "Failed to create backup %s, error: %s" %
-                (backup_id, str(err))
-            )
-        finally:
-            LOG.info("Completed backup %s.", backup_id)
-            conductor.update_backup(
-                CONF.guest_id,
-                sent=timeutils.utcnow_ts(microsecond=True),
-                **backup_state)
-            LOG.debug("Updated state for %s to %s.", backup_id, backup_state)
+            except StopIteration:
+                next_marker = None
+
+            if count == limit:
+                break
+        return users, next_marker
+
+    def get_user(self, username, hostname):
+        LOG.debug("Get details of a given database user.")
+        user = self._get_user(username, hostname)
+        if not user:
+            return None
+        return user.serialize()
+
+    def _get_user(self, username, hostname):
+        LOG.debug("Get details of a given database user %s.", username)
+        user = models.DatastoreUser(name=username)
+        databases, marker = self.list_databases()
+        out = None
+        for database in databases:
+            db2_db = models.DatastoreSchema.deserialize(database)
+            try:
+                out, err = run_command(
+                    system.LIST_DB_USERS % {'dbname': db2_db.name})
+            except exception.ProcessExecutionError:
+                LOG.debug(
+                    "Error while trying to get the users for database: %s.",
+                    db2_db.name)
+                continue
+
+            for item in out.split('\n'):
+                user_access = item.split() if item != "" else None
+                if (user_access is not None and
+                        user_access[0].lower() == username.lower() and
+                        user_access[1] == 'Y'):
+                    user.databases = db2_db.name
+                    break
+        return user
+
+    def list_access(self, username, hostname):
+        """
+           Show all the databases to which the user has more than
+           USAGE granted.
+        """
+        LOG.debug("Listing databases that user: %s has access to.", username)
+        user = self._get_user(username, hostname)
+        return user.databases
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/dbaas.py` & `trove-8.0.1/trove/guestagent/dbaas.py`

 * *Files 12% similar despite different names*

```diff
@@ -28,40 +28,41 @@
 
 from oslo_log import log as logging
 
 from trove.common import cfg
 from trove.common.i18n import _
 from trove.common import utils
 
+
 LOG = logging.getLogger(__name__)
 defaults = {
     'mysql':
-        'trove.guestagent.datastore.mysql.manager.Manager',
-    'mariadb':
-        'trove.guestagent.datastore.mariadb.manager.Manager',
-    'postgresql':
-        'trove.guestagent.datastore.postgres.manager.PostgresManager',
+    'trove.guestagent.datastore.mysql.manager.Manager',
     'percona':
-        'trove.guestagent.datastore.experimental.percona.manager.Manager',
+    'trove.guestagent.datastore.experimental.percona.manager.Manager',
     'pxc':
-        'trove.guestagent.datastore.experimental.pxc.manager.Manager',
+    'trove.guestagent.datastore.experimental.pxc.manager.Manager',
     'redis':
-        'trove.guestagent.datastore.experimental.redis.manager.Manager',
+    'trove.guestagent.datastore.experimental.redis.manager.Manager',
     'cassandra':
-        'trove.guestagent.datastore.experimental.cassandra.manager.Manager',
+    'trove.guestagent.datastore.experimental.cassandra.manager.Manager',
     'couchbase':
-        'trove.guestagent.datastore.experimental.couchbase.manager.Manager',
+    'trove.guestagent.datastore.experimental.couchbase.manager.Manager',
     'mongodb':
-        'trove.guestagent.datastore.experimental.mongodb.manager.Manager',
+    'trove.guestagent.datastore.experimental.mongodb.manager.Manager',
+    'postgresql':
+    'trove.guestagent.datastore.experimental.postgresql.manager.Manager',
     'couchdb':
-        'trove.guestagent.datastore.experimental.couchdb.manager.Manager',
+    'trove.guestagent.datastore.experimental.couchdb.manager.Manager',
     'vertica':
-        'trove.guestagent.datastore.experimental.vertica.manager.Manager',
+    'trove.guestagent.datastore.experimental.vertica.manager.Manager',
     'db2':
-        'trove.guestagent.datastore.experimental.db2.manager.Manager',
+    'trove.guestagent.datastore.experimental.db2.manager.Manager',
+    'mariadb':
+    'trove.guestagent.datastore.experimental.mariadb.manager.Manager'
 }
 CONF = cfg.CONF
 
 
 def get_custom_managers():
     return CONF.datastore_registry_ext
 
@@ -71,15 +72,15 @@
                 get_custom_managers().items()))
 
 
 def get_filesystem_volume_stats(fs_path):
     try:
         stats = os.statvfs(fs_path)
     except OSError:
-        LOG.exception("Error getting volume stats.")
+        LOG.exception(_("Error getting volume stats."))
         raise RuntimeError(_("Filesystem not found (%s)") % fs_path)
 
     total = stats.f_blocks * stats.f_bsize
     free = stats.f_bfree * stats.f_bsize
     # return the size in GB
     used_gb = utils.to_gb(total - free)
     total_gb = utils.to_gb(total)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/guest_log.py` & `trove-8.0.1/trove/guestagent/guest_log.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,24 +11,23 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import enum
 import hashlib
 import os
-from pathlib import Path
 from requests.exceptions import ConnectionError
 
 from oslo_log import log as logging
 from swiftclient.client import ClientException
 
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
 from trove.common.i18n import _
+from trove.common.remote import create_swift_client
 from trove.common import stream_codecs
 from trove.common import timeutils
 from trove.guestagent.common import operating_system
 from trove.guestagent.common.operating_system import FileMode
 
 
 LOG = logging.getLogger(__name__)
@@ -127,15 +126,15 @@
     def type(self):
         return self._type
 
     @property
     def swift_client(self):
         if not self._cached_swift_client or (
                 self._cached_context != self.context):
-            self._cached_swift_client = clients.swift_client(self.context)
+            self._cached_swift_client = create_swift_client(self.context)
             self._cached_context = self.context
         return self._cached_swift_client
 
     @property
     def exposed(self):
         return self._exposed or self.context.is_admin
 
@@ -175,15 +174,15 @@
             except ClientException as ex:
                 if ex.http_status == 404:
                     LOG.debug("Container '%s' not found; creating now",
                               container_name)
                     self.swift_client.put_container(
                         container_name, headers=self._get_headers())
                 else:
-                    LOG.exception("Could not retrieve container '%s'",
+                    LOG.exception(_("Could not retrieve container '%s'"),
                                   container_name)
                     raise
             self._container_name = container_name
         return self._container_name
 
     def _set_status(self, use_first, first_status, second_status):
         if use_first:
@@ -212,29 +211,30 @@
                 'prefix': prefix,
                 'metafile': self._metafile_name()
             }
         else:
             raise exception.LogAccessForbidden(action='show', log=self._name)
 
     def _refresh_details(self):
+
         if self._published_size is None:
             # Initializing, so get all the values
             try:
                 meta_details = self._get_meta_details()
                 self._published_size = int(
                     meta_details[self.MF_LABEL_LOG_SIZE])
                 self._published_header_digest = (
                     meta_details[self.MF_LABEL_LOG_HEADER])
             except ClientException as ex:
                 if ex.http_status == 404:
                     LOG.debug("No published metadata found for log '%s'",
                               self._name)
                     self._published_size = 0
                 else:
-                    LOG.exception("Could not get meta details for log '%s'",
+                    LOG.exception(_("Could not get meta details for log '%s'"),
                                   self._name)
                     raise
             except ConnectionError as e:
                 # A bad endpoint will cause a ConnectionError
                 # This exception contains another exception that we want
                 exc = e.args[0]
                 raise exc
@@ -242,71 +242,75 @@
         self._update_details()
         LOG.debug("Log size for '%(name)s' set to %(size)d "
                   "(published %(published)d)",
                   {'name': self._name, 'size': self._size,
                    'published': self._published_size})
 
     def _update_details(self):
-        if operating_system.exists(self._file, as_root=True):
-            file_path = Path(self._file)
-
-            # Make sure guest agent can read the log file.
+        # Make sure we can read the file
+        if not self._file_readable or not os.access(self._file, os.R_OK):
             if not os.access(self._file, os.R_OK):
-                operating_system.chmod(self._file, FileMode.ADD_ALL_R,
-                                       as_root=True)
-                operating_system.chmod(str(file_path.parent),
-                                       FileMode.ADD_GRP_RX_OTH_RX,
-                                       as_root=True)
-
-            self._size = file_path.stat().st_size
+                if operating_system.exists(self._file, as_root=True):
+                    operating_system.chmod(
+                        self._file, FileMode.ADD_ALL_R, as_root=True)
+            self._file_readable = True
+
+        if os.path.isfile(self._file):
+            logstat = os.stat(self._file)
+            self._size = logstat.st_size
             self._update_log_header_digest(self._file)
 
-            if self.status != LogStatus.Disabled:
-                if self._log_rotated():
-                    self.status = LogStatus.Rotated
-                # See if we have stuff to publish
-                elif self._size > self._published_size:
-                    self._set_status(self._published_size,
-                                     LogStatus.Partial, LogStatus.Ready)
-                # We've published everything so far
-                elif self._size == self._published_size:
-                    self._set_status(self._published_size,
-                                     LogStatus.Published, LogStatus.Enabled)
-                # We've already handled this case (log rotated) so what gives?
-                else:
-                    raise Exception(_("Bug in _log_rotated ?"))
+            if self._log_rotated():
+                self.status = LogStatus.Rotated
+            # See if we have stuff to publish
+            elif logstat.st_size > self._published_size:
+                self._set_status(self._published_size,
+                                 LogStatus.Partial, LogStatus.Ready)
+            # We've published everything so far
+            elif logstat.st_size == self._published_size:
+                self._set_status(self._published_size,
+                                 LogStatus.Published, LogStatus.Enabled)
+            # We've already handled this case (log rotated) so what gives?
+            else:
+                raise Exception(_("Bug in _log_rotated ?"))
         else:
-            LOG.warning(f"File {self._file} does not exist")
             self._published_size = 0
             self._size = 0
 
+        if not self._size or not self.enabled:
+            user_status = LogStatus.Disabled
+            if self.enabled:
+                user_status = LogStatus.Enabled
+            self._set_status(self._type == LogType.USER,
+                             user_status, LogStatus.Unavailable)
+
     def _log_rotated(self):
         """If the file is smaller than the last reported size
         or the first line hash is different, we can probably assume
         the file changed under our nose.
         """
         if (self._published_size > 0 and
                 (self._size < self._published_size or
                  self._published_header_digest != self._header_digest)):
             return True
 
     def _update_log_header_digest(self, log_file):
-        with open(log_file, 'rb') as log:
+        with open(log_file, 'r') as log:
             self._header_digest = hashlib.md5(log.readline()).hexdigest()
 
     def _get_headers(self):
         return {'X-Delete-After': str(CONF.guest_log_expiry)}
 
     def publish_log(self):
         if self.exposed:
             if self._log_rotated():
                 LOG.debug("Log file rotation detected for '%s' - "
                           "discarding old log", self._name)
                 self._delete_log_components()
-            if operating_system.exists(self._file, as_root=True):
+            if os.path.isfile(self._file):
                 self._publish_to_container(self._file)
             else:
                 raise RuntimeError(_(
                     "Cannot publish log file '%s' as it does not exist.") %
                     self._file)
             return self.show()
         else:
@@ -326,14 +330,16 @@
         prefix = self._object_prefix()
         swift_files = [swift_file['name']
                        for swift_file in self.swift_client.get_container(
                        container_name, prefix=prefix)[1]]
         swift_files.append(self._metafile_name())
         for swift_file in swift_files:
             self.swift_client.delete_object(container_name, swift_file)
+        self._set_status(self._type == LogType.USER,
+                         LogStatus.Disabled, LogStatus.Enabled)
         self._published_size = 0
 
     def _publish_to_container(self, log_filename):
         log_component, log_lines = '', 0
         chunk_size = CONF.guest_log_limit
         container_name = self.get_container_name(force=True)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/models.py` & `trove-8.0.1/trove/guestagent/models.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,23 +15,26 @@
 from datetime import datetime
 from datetime import timedelta
 
 from oslo_log import log as logging
 
 from trove.common import cfg
 from trove.common import exception
+from trove.common.i18n import _
 from trove.common import timeutils
 from trove.common import utils
 from trove.db import get_db_api
 from trove.db import models as dbmodels
 
 LOG = logging.getLogger(__name__)
 
 CONF = cfg.CONF
 
+AGENT_HEARTBEAT = CONF.agent_heartbeat_time
+
 
 def persisted_models():
     return {'agent_heartbeats': AgentHeartBeat}
 
 
 class AgentHeartBeat(dbmodels.DatabaseModelBase):
     """Defines the state of a Guest Agent."""
@@ -78,14 +81,14 @@
         if instance_id is None:
             raise exception.ModelNotFoundError(instance_id=instance_id)
 
         try:
             return cls.find_by(instance_id=instance_id)
 
         except exception.NotFound:
-            LOG.exception("Error finding instance %s", instance_id)
+            LOG.exception(_("Error finding instance %s"), instance_id)
             raise exception.ModelNotFoundError(instance_id=instance_id)
 
     @staticmethod
     def is_active(agent):
         return (datetime.now() - agent.updated_at <
-                timedelta(seconds=CONF.agent_heartbeat_time))
+                timedelta(seconds=AGENT_HEARTBEAT))
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/module/driver_manager.py` & `trove-8.0.1/trove/guestagent/module/driver_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 
 
 class ModuleDriverManager(object):
 
     MODULE_DRIVER_NAMESPACE = 'trove.guestagent.module.drivers'
 
     def __init__(self):
-        LOG.info('Initializing module driver manager.')
+        LOG.info(_('Initializing module driver manager.'))
 
         self._drivers = {}
         self._module_types = [mt.lower() for mt in CONF.module_types]
 
         self._load_drivers()
 
     def _load_drivers(self):
@@ -43,51 +43,51 @@
             namespace=self.MODULE_DRIVER_NAMESPACE,
             check_func=self._check_extension,
             invoke_on_load=True,
             invoke_kwds={})
         try:
             manager.map(self.add_driver_extension)
         except stevedore.exception.NoMatches:
-            LOG.info("No module drivers loaded")
+            LOG.info(_("No module drivers loaded"))
 
     def _check_extension(self, extension):
         """Checks for required methods in driver objects."""
         driver = extension.obj
         supported = False
         try:
-            LOG.info('Loading Module driver: %s', driver.get_type())
+            LOG.info(_('Loading Module driver: %s'), driver.get_type())
             if driver.get_type() != driver.get_type().lower():
                 raise AttributeError(_("Driver 'type' must be lower-case"))
             LOG.debug('  description: %s', driver.get_description())
             LOG.debug('  updated    : %s', driver.get_updated())
             required_attrs = ['apply', 'remove']
             for attr in required_attrs:
                 if not hasattr(driver, attr):
                     raise AttributeError(
                         _("Driver '%(type)s' missing attribute: %(attr)s")
                         % {'type': driver.get_type(), 'attr': attr})
             if driver.get_type() in self._module_types:
                 supported = True
             else:
-                LOG.info("Driver '%s' not supported, skipping",
+                LOG.info(_("Driver '%s' not supported, skipping"),
                          driver.get_type())
         except AttributeError as ex:
-            LOG.exception("Exception loading module driver: %s",
+            LOG.exception(_("Exception loading module driver: %s"),
                           encodeutils.exception_to_unicode(ex))
 
         return supported
 
     def add_driver_extension(self, extension):
         # Add a module driver from the extension.
         # If the stevedore manager is changed to one that doesn't
         # check the extension driver, then it should be done manually here
         # by calling self._check_extension(extension)
         driver = extension.obj
         driver_type = driver.get_type()
-        LOG.info('Loaded module driver: %s', driver_type)
+        LOG.info(_('Loaded module driver: %s'), driver_type)
 
         if driver_type in self._drivers:
             raise exception.Error(_("Found duplicate driver: %s") %
                                   driver_type)
         self._drivers[driver_type] = driver
 
     def get_driver(self, driver_type):
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/module/drivers/module_driver.py` & `trove-8.0.1/trove/guestagent/module/drivers/module_driver.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,24 +13,27 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import abc
 import functools
 import re
+import six
 
 from oslo_log import log as logging
 
 from trove.common import exception
+from trove.common.i18n import _
 
 
 LOG = logging.getLogger(__name__)
 
 
-class ModuleDriver(object, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class ModuleDriver(object):
     """Base class that defines the contract for module drivers.
 
     Note that you don't have to derive from this class to have a valid
     driver; it is purely a convenience. Any class that adheres to the
     'interface' as dictated by this class' abstractmethod decorators
     (and other methods such as get_type, get_name and configure)
     will work.
@@ -161,15 +164,15 @@
                 if message_args:
                     try:
                         log_msg = log_msg % message_args
                         success_msg = success_msg % message_args
                         fail_msg = fail_msg % message_args
                     except Exception:
                         # if there's a problem, just log it and drive on
-                        LOG.warning("Could not apply message args: %s",
+                        LOG.warning(_("Could not apply message args: %s"),
                                     message_args)
                         pass
 
             if log_msg:
                 LOG.info(log_msg)
             success = False
             try:
@@ -177,27 +180,27 @@
                 if rv:
                     # Use the actual values, if there are some
                     success, message = rv
                 else:
                     success = True
                     message = success_msg
             except exception.ProcessExecutionError as ex:
-                message = ("%(msg)s: %(out)s\n%(err)s" %
+                message = (_("%(msg)s: %(out)s\n%(err)s") %
                            {'msg': fail_msg,
                             'out': ex.stdout,
                             'err': ex.stderr})
                 message = message.replace(': \n', ': ')
                 message = message.rstrip()
                 LOG.exception(message)
             except exception.TroveError as ex:
-                message = ("%(msg)s: %(err)s" %
+                message = (_("%(msg)s: %(err)s") %
                            {'msg': fail_msg, 'err': ex._error_string})
                 LOG.exception(message)
             except Exception as ex:
-                message = ("%(msg)s: %(err)s" %
-                           {'msg': fail_msg, 'err': str(ex)})
+                message = (_("%(msg)s: %(err)s") %
+                           {'msg': fail_msg, 'err': ex.message})
                 LOG.exception(message)
             return success, message
 
         return wrapper
 
     return output_decorator
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/module/drivers/new_relic_license_driver.py` & `trove-8.0.1/trove/guestagent/module/drivers/new_relic_license_driver.py`

 * *Files 14% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 #    under the License.
 #
 
 from datetime import date
 
 from oslo_log import log as logging
 
+from trove.common.i18n import _
 from trove.common import stream_codecs
 from trove.common import utils
 from trove.guestagent.common import operating_system
 from trove.guestagent.module.drivers import module_driver
 
 
 LOG = logging.getLogger(__name__)
@@ -37,17 +38,17 @@
     def get_description(self):
         return "New Relic License Module Driver"
 
     def get_updated(self):
         return date(2016, 4, 12)
 
     @module_driver.output(
-        log_message='Installing New Relic license key',
-        success_message='New Relic license key installed',
-        fail_message='New Relic license key not installed')
+        log_message=_('Installing New Relic license key'),
+        success_message=_('New Relic license key installed'),
+        fail_message=_('New Relic license key not installed'))
     def apply(self, name, datastore, ds_version, data_file, admin_module):
         license_key = None
         data = operating_system.read_file(
             data_file, codec=stream_codecs.KeyValueCodec())
         for key, value in data.items():
             if 'license_key' == key.lower():
                 license_key = value
@@ -63,30 +64,30 @@
             exec_args = {'timeout': 10,
                          'run_as_root': True,
                          'root_helper': 'sudo'}
             cmd = list(NR_ADD_LICENSE_CMD)
             cmd[-1] = cmd[-1] % license_key
             utils.execute_with_timeout(*cmd, **exec_args)
         except Exception:
-            LOG.exception("Could not install license key '%s'",
+            LOG.exception(_("Could not install license key '%s'"),
                           license_key)
             raise
 
     def _server_control(self, command):
         try:
             exec_args = {'timeout': 10,
                          'run_as_root': True,
                          'root_helper': 'sudo'}
             cmd = list(NR_SRV_CONTROL_CMD)
             cmd.append(command)
             utils.execute_with_timeout(*cmd, **exec_args)
         except Exception:
-            LOG.exception("Could not %s New Relic server", command)
+            LOG.exception(_("Could not %s New Relic server"), command)
             raise
 
     @module_driver.output(
-        log_message='Removing New Relic license key',
-        success_message='New Relic license key removed',
-        fail_message='New Relic license key not removed')
+        log_message=_('Removing New Relic license key'),
+        success_message=_('New Relic license key removed'),
+        fail_message=_('New Relic license key not removed'))
     def remove(self, name, datastore, ds_version, data_file):
         self._add_license_key("bad_key_that_is_exactly_40_characters_xx")
         self._server_control('stop')
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/module/drivers/ping_driver.py` & `trove-8.0.1/trove/guestagent/module/drivers/ping_driver.py`

 * *Files 13% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 from datetime import date
 
+from trove.common.i18n import _
 from trove.common import stream_codecs
 from trove.guestagent.common import operating_system
 from trove.guestagent.module.drivers import module_driver
 
 
 class PingDriver(module_driver.ModuleDriver):
     """Concrete module to show implementation and functionality. Responds
@@ -32,21 +33,21 @@
     def get_description(self):
         return "Ping Module Driver"
 
     def get_updated(self):
         return date(2016, 3, 4)
 
     @module_driver.output(
-        log_message='Extracting %(type)s message',
-        fail_message='Could not extract %(type)s message')
+        log_message=_('Extracting %(type)s message'),
+        fail_message=_('Could not extract %(type)s message'))
     def apply(self, name, datastore, ds_version, data_file, admin_module):
         data = operating_system.read_file(
             data_file, codec=stream_codecs.KeyValueCodec())
         for key, value in data.items():
             if 'message' == key.lower():
                 return True, value
         return False, 'Message not found in contents file'
 
     @module_driver.output(
-        log_message='Removing %(type)s module')
+        log_message=_('Removing %(type)s module'))
     def remove(self, name, datastore, ds_version, data_file):
         return True, ""
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/module/module_manager.py` & `trove-8.0.1/trove/guestagent/module/module_manager.py`

 * *Files 5% similar despite different names*

```diff
@@ -62,17 +62,17 @@
             auto_apply, visible, now, admin_module)
         result = cls.read_module_result(module_dir, default_result)
         try:
             driver.configure(name, datastore, ds_version, data_file)
             applied, message = driver.apply(
                 name, datastore, ds_version, data_file, admin_module)
         except Exception as ex:
-            LOG.exception("Could not apply module '%s'", name)
+            LOG.exception(_("Could not apply module '%s'"), name)
             applied = False
-            message = str(ex)
+            message = ex.message
         finally:
             status = 'OK' if applied else 'ERROR'
             result['removed'] = None
             result['status'] = status
             result['message'] = message
             result['updated'] = now
             result['id'] = module_id
@@ -90,15 +90,15 @@
 
     @classmethod
     def build_module_dir(cls, module_type, module_id):
         sub_dir = os.path.join(module_type, module_id)
         module_dir = guestagent_utils.build_file_path(
             cls.MODULE_BASE_DIR, sub_dir)
         if not operating_system.exists(module_dir, is_directory=True):
-            operating_system.ensure_directory(module_dir, force=True)
+            operating_system.create_directory(module_dir, force=True)
         return module_dir
 
     @classmethod
     def write_module_contents(cls, module_dir, contents, md5):
         contents_file = cls.build_contents_filename(module_dir)
         operating_system.write_file(contents_file, contents,
                                     codec=stream_codecs.Base64Codec(),
@@ -145,15 +145,15 @@
         result_file = cls.get_result_filename(result_file)
         result = default
         try:
             result = operating_system.read_file(
                 result_file, codec=stream_codecs.JsonCodec())
         except Exception:
             if not result:
-                LOG.exception("Could not find module result in %s",
+                LOG.exception(_("Could not find module result in %s"),
                               result_file)
                 raise
         return result
 
     @classmethod
     def get_result_filename(cls, file_or_dir):
         result_file = file_or_dir
@@ -213,15 +213,15 @@
                 _("Module '%s' has not been applied") % name)
         try:
             driver.configure(name, datastore, ds_version, contents_file)
             removed, message = driver.remove(
                 name, datastore, ds_version, contents_file)
             cls.remove_module_result(module_dir)
         except Exception:
-            LOG.exception("Could not remove module '%s'", name)
+            LOG.exception(_("Could not remove module '%s'"), name)
             raise
         return removed, message
 
     @classmethod
     def remove_module_result(cls, result_file):
         now = cls.get_current_timestamp()
         result = cls.read_module_result(result_file, None)
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/pkg.py` & `trove-8.0.1/trove/guestagent/pkg.py`

 * *Files 5% similar despite different names*

```diff
@@ -18,15 +18,14 @@
 """
 import os
 import re
 import subprocess
 from tempfile import NamedTemporaryFile
 
 from oslo_log import log as logging
-from oslo_utils import encodeutils
 import pexpect
 
 from trove.common import exception
 from trove.common.exception import ProcessExecutionError
 from trove.common.i18n import _
 from trove.common import utils
 from trove.guestagent.common import operating_system
@@ -47,18 +46,17 @@
 
     try:
         proc = subprocess.Popen(cmd,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.STDOUT)
     except OSError:
         # ignore errors like program not found
-        return str("")
-
+        return b''
     stdout = proc.communicate()[0]
-    return encodeutils.safe_decode(stdout)
+    return stdout
 
 
 class PkgAdminLockError(exception.TroveError):
     pass
 
 
 class PkgPermissionError(exception.TroveError):
@@ -132,15 +130,15 @@
         conflicted package without dependencies and hope it will replaced
         by another package
         """
         try:
             utils.execute("rpm", "-e", "--nodeps", package_name,
                           run_as_root=True, root_helper="sudo")
         except ProcessExecutionError:
-            LOG.exception("Error removing conflict %(package)s",
+            LOG.exception(_("Error removing conflict %(package)s"),
                           package_name)
 
     def _install(self, packages, time_out):
         """must be overridden by an RPM based PackagerMixin"""
         raise NotImplementedError()
 
     def _remove(self, package_name, time_out):
@@ -177,15 +175,15 @@
         for line in std_out.split("\n"):
             regex = re.compile("[0-9.]+-.*")
             matches = regex.match(line)
             if matches:
                 line = matches.group()
                 return line
 
-        LOG.error("Unexpected output from rpm command. (%(output)s)",
+        LOG.error(_("Unexpected output from rpm command. (%(output)s)"),
                   {'output': std_out})
 
     def pkg_remove(self, package_name, time_out):
         """Removes a package."""
         if self.pkg_version(package_name) is None:
             return
         result = self._remove(package_name, time_out)
@@ -200,15 +198,15 @@
 
         Returns OK if the packages are installed or a result code if a
         recoverable-error occurred.
         Raises an exception if a non-recoverable error or timeout occurs.
 
         """
         cmd = "sudo yum --color=never -y install %s" % " ".join(packages)
-        output_expects = [r'\[sudo\] password for .*:',
+        output_expects = ['\[sudo\] password for .*:',
                           'No package (.*) available.',
                           ('file .* from install of .* conflicts with file'
                            ' from package (.*?)\r\n'),
                           'Error: (.*?) conflicts with .*?\r\n',
                           'Processing Conflict: .* conflicts (.*?)\r\n',
                           '.*scriptlet failed*',
                           'HTTP Error',
@@ -241,15 +239,15 @@
         Returns OK if the package is removed successfully or a result code if a
         recoverable-error occurs.
         Raises an exception if a non-recoverable error or timeout occurs.
 
         """
         cmd = "sudo yum --color=never -y remove %s" % package_name
         LOG.debug("Running package remove command: %s", cmd)
-        output_expects = [r'\[sudo\] password for .*:',
+        output_expects = ['\[sudo\] password for .*:',
                           'No Packages marked for removal',
                           'Removed:']
         i, match = self.pexpect_run(cmd, output_expects, time_out)
         if i == 0:
             raise PkgPermissionError(_("Invalid permissions."))
         elif i == 1:
             raise PkgNotFoundError(_("Could not find package %s") %
@@ -263,15 +261,15 @@
         """Sometimes you have to run this command before a
             package will install.
         """
         try:
             utils.execute("dpkg", "--configure", "-a", run_as_root=True,
                           root_helper="sudo")
         except ProcessExecutionError:
-            LOG.exception("Error fixing dpkg")
+            LOG.exception(_("Error fixing dpkg"))
 
     def _fix_package_selections(self, packages, config_opts):
         """
         Sometimes you have to run this command before a package will install.
         This command sets package selections to configure package.
         """
         selections = ""
@@ -287,15 +285,15 @@
                     m = re.match(".* (.*/%s):.*" % selection, line)
                     if m:
                         selections += ("%s %s string '%s'\n" %
                                        (package_name, m.group(1), value))
         if selections:
             with NamedTemporaryFile(delete=False) as f:
                 fname = f.name
-                f.write(encodeutils.safe_encode(selections))
+                f.write(selections)
             try:
                 utils.execute("debconf-set-selections", fname,
                               run_as_root=True, root_helper="sudo")
                 utils.execute("dpkg", "--configure", "-a",
                               run_as_root=True, root_helper="sudo")
             except ProcessExecutionError:
                 raise PkgConfigureError(_("Error configuring package."))
@@ -375,15 +373,15 @@
 
     def pkg_install(self, packages, config_opts, time_out):
         """Installs packages."""
         try:
             utils.execute("apt-get", "update", run_as_root=True,
                           root_helper="sudo")
         except ProcessExecutionError:
-            LOG.exception("Error updating the apt sources")
+            LOG.exception(_("Error updating the apt sources"))
 
         result = self._install(packages, time_out)
         if result != OK:
             if result == RUN_DPKG_FIRST:
                 self._fix(time_out)
             result = self._install(packages, time_out)
             if result != OK:
@@ -393,15 +391,15 @@
         # interactive configure script
         if config_opts:
             self._fix_package_selections(packages, config_opts)
 
     def pkg_version(self, package_name):
         std_out = getoutput("apt-cache", "policy", package_name)
         for line in std_out.split("\n"):
-            m = re.match(r"\s+Installed: (.*)", line)
+            m = re.match("\s+Installed: (.*)", line)
             if m:
                 version = m.group(1)
                 if version == "(none)":
                     version = None
                 return version
 
     def pkg_is_installed(self, packages):
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/service.py` & `trove-8.0.1/trove/guestagent/service.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/guestagent/strategies/replication/__init__.py` & `trove-8.0.1/trove/guestagent/strategies/replication/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,15 +37,15 @@
     if not __replication_instance or manager != __replication_manager:
         replication_strategy = get_strategy(manager)
         __replication_namespace = CONF.get(manager).replication_namespace
         replication_strategy_cls = get_strategy_cls(
             replication_strategy, __replication_namespace)
         __replication_instance = replication_strategy_cls()
         __replication_manager = manager
-    LOG.debug('Replication instance from: %(namespace)s.%(strategy)s',
+    LOG.debug('Got replication instance from: %(namespace)s.%(strategy)s',
               {'namespace': __replication_namespace,
                'strategy': __replication_strategy})
     return __replication_instance
 
 
 def get_strategy(manager):
     global __replication_strategy
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/strategies/replication/base.py` & `trove-8.0.1/trove/guestagent/strategies/replication/base.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,18 +12,20 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import abc
 
+import six
 from trove.common.strategies.strategy import Strategy
 
 
-class Replication(Strategy, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class Replication(Strategy):
     """Base class for Replication Strategy implementation."""
 
     __strategy_type__ = 'replication'
     __strategy_ns__ = 'trove.guestagent.strategies.replication'
 
     def __init__(self):
         super(Replication, self).__init__()
@@ -33,24 +35,24 @@
         """Get reference to master site for replication strategy."""
 
     def backup_required_for_replication(self):
         """Indicates whether a backup is required for replication."""
         return True
 
     @abc.abstractmethod
-    def snapshot_for_replication(self, context, service, adm, location,
+    def snapshot_for_replication(self, context, service, location,
                                  snapshot_info):
         """Capture snapshot of master db."""
 
     @abc.abstractmethod
     def enable_as_master(self, service, master_config):
         """Configure underlying database to act as master for replication."""
 
     @abc.abstractmethod
-    def enable_as_slave(self, service, master_info, slave_config):
+    def enable_as_slave(self, service, snapshot, slave_config):
         """Configure underlying database as a slave of the given master."""
 
     @abc.abstractmethod
     def detach_slave(self, service, for_failover):
         """Turn off replication on a slave site."""
 
     @abc.abstractmethod
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/strategies/replication/mysql_base.py` & `trove-8.0.1/trove/guestagent/strategies/replication/mysql_base.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,54 +11,61 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import abc
-import json
-import os
 import uuid
 
 from oslo_log import log as logging
 from oslo_utils import netutils
 
 from trove.common import cfg
-from trove.common import constants
 from trove.common.db.mysql import models
-from trove.common import exception
+from trove.common.i18n import _
 from trove.common import utils
-from trove.guestagent.common import operating_system
+from trove.guestagent.backup.backupagent import BackupAgent
+from trove.guestagent.datastore.mysql.service import MySqlAdmin
+from trove.guestagent.strategies import backup
 from trove.guestagent.strategies.replication import base
 
-LOG = logging.getLogger(__name__)
+AGENT = BackupAgent()
 CONF = cfg.CONF
 
+REPL_BACKUP_NAMESPACE = 'trove.guestagent.strategies.backup.mysql_impl'
+
+LOG = logging.getLogger(__name__)
+
 
 class MysqlReplicationBase(base.Replication):
     """Base class for MySql Replication strategies."""
 
+    @property
+    def repl_backup_runner(self):
+        return backup.get_backup_strategy('InnoBackupEx',
+                                          REPL_BACKUP_NAMESPACE)
+
+    @property
+    def repl_incr_backup_runner(self):
+        return backup.get_backup_strategy('InnoBackupExIncremental',
+                                          REPL_BACKUP_NAMESPACE)
+
+    @property
+    def repl_backup_extra_opts(self):
+        return CONF.backup_runner_options.get('InnoBackupEx', '')
+
     def get_master_ref(self, service, snapshot_info):
-        ip_address = None
-        if CONF.network_isolation and \
-                os.path.exists(constants.ETH1_CONFIG_PATH):
-            # Get IP_address from eth1.json, ipv4 address was preferred.
-            with open(constants.ETH1_CONFIG_PATH) as fd:
-                eth1_config = json.load(fd)
-            ip_address = eth1_config.get("ipv4_address", None) or \
-                eth1_config.get("ipv6_address", None)
-        if not ip_address:
-            ip_address = netutils.get_my_ipv4()
         master_ref = {
-            'host': ip_address,
+            'host': netutils.get_my_ipv4(),
             'port': service.get_port()
         }
         return master_ref
 
-    def _create_replication_user(self, service, adm):
+    def _create_replication_user(self):
         replication_user = None
         replication_password = utils.generate_random_password(16)
 
         mysql_user = None  # cache the model as we just want name validation
 
         retry_count = 0
 
@@ -68,105 +75,88 @@
                 if mysql_user:
                     mysql_user.name = name
                 else:
                     mysql_user = models.MySQLUser(
                         name=name, password=replication_password
                     )
                     mysql_user.check_create()
-
-                LOG.debug("Trying to create replication user %s",
+                MySqlAdmin().create_user([mysql_user.serialize()])
+                LOG.debug("Trying to create replication user " +
                           mysql_user.name)
-                adm.create_users([mysql_user.serialize()])
-
                 replication_user = {
                     'name': mysql_user.name,
                     'password': replication_password
                 }
             except Exception:
                 retry_count += 1
                 if retry_count > 5:
-                    LOG.error("Replication user retry count exceeded")
+                    LOG.error(_("Replication user retry count exceeded"))
                     raise
 
         return replication_user
 
-    def snapshot_for_replication(self, context, service, adm, location,
-                                 snapshot_info):
-        LOG.info("Creating backup for replication")
-
-        volumes_mapping = {
-            '/var/lib/mysql': {'bind': '/var/lib/mysql', 'mode': 'rw'},
-            constants.MYSQL_HOST_SOCKET_PATH: {"bind": "/var/run/mysqld",
-                                               "mode": "ro"},
-            '/tmp': {'bind': '/tmp', 'mode': 'rw'}
-        }
-        service.create_backup(context, snapshot_info,
-                              volumes_mapping=volumes_mapping)
+    def snapshot_for_replication(self, context, service,
+                                 location, snapshot_info):
+        snapshot_id = snapshot_info['id']
+        replica_number = snapshot_info.get('replica_number', 1)
+
+        LOG.debug("Acquiring backup for replica number %d.", replica_number)
+        # Only create a backup if it's the first replica
+        if replica_number == 1:
+            AGENT.execute_backup(
+                context, snapshot_info, runner=self.repl_backup_runner,
+                extra_opts=self.repl_backup_extra_opts,
+                incremental_runner=self.repl_incr_backup_runner)
+        else:
+            LOG.debug("Using existing backup created for previous replica.")
+        LOG.debug("Replication snapshot %(snapshot_id)s used for replica "
+                  "number %(replica_number)d.",
+                  {'snapshot_id': snapshot_id,
+                   'replica_number': replica_number})
 
-        LOG.info('Creating replication user')
-        replication_user = self._create_replication_user(service, adm)
+        replication_user = self._create_replication_user()
         service.grant_replication_privilege(replication_user)
 
-        replica_conf = {
-            'log_position': {},
+        # With streamed InnobackupEx, the log position is in
+        # the stream and will be decoded by the slave
+        log_position = {
             'replication_user': replication_user
         }
-        return snapshot_info['id'], replica_conf
+        return snapshot_id, log_position
 
     def enable_as_master(self, service, master_config):
         if not service.exists_replication_source_overrides():
             service.write_replication_source_overrides(master_config)
             service.restart()
 
-    def read_last_master_gtid(self, service):
-        INFO_FILE = ('%s/xtrabackup_binlog_info' % service.get_data_dir())
-        operating_system.chmod(INFO_FILE,
-                               operating_system.FileMode.ADD_READ_ALL,
-                               as_root=True)
-
-        LOG.info("Reading last master GTID from %s", INFO_FILE)
-        try:
-            with open(INFO_FILE, 'r') as f:
-                content = f.read()
-                LOG.debug('Content in %s: "%s"', INFO_FILE, content)
-                ret = content.strip().split('\t')
-                return ret[2] if len(ret) == 3 else ''
-        except Exception as ex:
-            LOG.error('Failed to read last master GTID, error: %s', str(ex))
-            raise exception.UnableToDetermineLastMasterGTID(
-                {'binlog_file': INFO_FILE})
-
     @abc.abstractmethod
-    def connect_to_master(self, service, master_info):
+    def connect_to_master(self, service, snapshot):
         """Connects a slave to a master"""
 
-    def enable_as_slave(self, service, master_info, slave_config):
+    def enable_as_slave(self, service, snapshot, slave_config):
         try:
             service.write_replication_replica_overrides(slave_config)
             service.restart()
-            self.connect_to_master(service, master_info)
-        except Exception as err:
-            LOG.error("Exception enabling guest as replica, error: %s",
-                      str(err))
+            self.connect_to_master(service, snapshot)
+        except Exception:
+            LOG.exception(_("Exception enabling guest as replica"))
             raise
 
     def detach_slave(self, service, for_failover):
         replica_info = service.stop_slave(for_failover)
         service.remove_replication_replica_overrides()
         service.restart()
         return replica_info
 
-    def get_replica_context(self, service, adm):
-        """Get replication information as master."""
-        replication_user = self._create_replication_user(service, adm)
+    def get_replica_context(self, service):
+        replication_user = self._create_replication_user()
         service.grant_replication_privilege(replication_user)
         return {
             'master': self.get_master_ref(service, None),
-            'replica_conf': {
-                'log_position': {},
+            'log_position': {
                 'replication_user': replication_user
             }
         }
 
     def cleanup_source_on_replica_detach(self, admin_service, replica_info):
         admin_service.delete_user_by_name(replica_info['replication_user'])
```

### Comparing `trove-21.0.0.0rc2/trove/guestagent/volume.py` & `trove-8.0.1/trove/guestagent/volume.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,15 +9,14 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-import abc
 import os
 import shlex
 from tempfile import NamedTemporaryFile
 import traceback
 
 from oslo_log import log as logging
 
@@ -29,164 +28,24 @@
 
 TMP_MOUNT_POINT = "/mnt/volume"
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
 
 
-# We removed all translation for messages destinated to log file.
-# However we cannot use _(xxx) instead of _("xxxx") because of the
-# H701 pep8 checking, so we have to pass different message format
-# string and format content here.
-def log_and_raise(log_fmt, exc_fmt, fmt_content=None):
-    if fmt_content is not None:
-        LOG.exception(log_fmt, fmt_content)
-        raise_msg = exc_fmt % fmt_content
-    else:
-        # if fmt_content is not provided, log_fmt and
-        # exc_fmt are just plain string messages
-        LOG.exception(log_fmt)
-        raise_msg = exc_fmt
-    raise_msg += _("\nExc: %s") % traceback.format_exc()
+def log_and_raise(message):
+    LOG.exception(message)
+    raise_msg = message + _("\nExc: %s") % traceback.format_exc()
     raise exception.GuestError(original_message=raise_msg)
 
 
-class FSBase(object, metaclass=abc.ABCMeta):
-
-    def __init__(self, fstype, format_options):
-        self.fstype = fstype
-        self.format_options = format_options
-
-    @abc.abstractmethod
-    def format(self, device_path, timeout):
-        """
-        Format device
-        """
-
-    @abc.abstractmethod
-    def check_format(self, device_path):
-        """
-        Check if device is formatted
-        """
-
-    @abc.abstractmethod
-    def resize(self, device_path, online=False):
-        """
-        Resize the filesystem on device
-        """
-
-
-class FSExt(FSBase):
-
-    def __init__(self, fstype, format_options):
-        super(FSExt, self).__init__(fstype, format_options)
-
-    def format(self, device_path, timeout):
-        format_options = shlex.split(self.format_options)
-        format_options.append(device_path)
-        try:
-            utils.execute_with_timeout(
-                "mkfs", "--type", self.fstype, *format_options,
-                timeout=timeout, run_as_root=True, root_helper="sudo")
-        except exception.ProcessExecutionError:
-            log_fmt = "Could not format '%s'."
-            exc_fmt = _("Could not format '%s'.")
-            log_and_raise(log_fmt, exc_fmt, device_path)
-
-    def check_format(self, device_path):
-        try:
-            stdout, stderr = utils.execute(
-                "dumpe2fs", device_path, run_as_root=True, root_helper="sudo")
-            if 'has_journal' not in stdout:
-                msg = _("Volume '%s' does not appear to be formatted.") % (
-                    device_path)
-                raise exception.GuestError(original_message=msg)
-        except exception.ProcessExecutionError as pe:
-            if 'Wrong magic number' in pe.stderr:
-                volume_fstype = self.fstype
-                log_fmt = "'Device '%(dev)s' did not seem to be '%(type)s'."
-                exc_fmt = _("'Device '%(dev)s' did not seem to be '%(type)s'.")
-                log_and_raise(log_fmt, exc_fmt, {'dev': device_path,
-                                                 'type': volume_fstype})
-            log_fmt = "Volume '%s' was not formatted."
-            exc_fmt = _("Volume '%s' was not formatted.")
-            log_and_raise(log_fmt, exc_fmt, device_path)
-
-    def resize(self, device_path, online=False):
-        if not online:
-            utils.execute("e2fsck", "-f", "-p", device_path,
-                          run_as_root=True, root_helper="sudo")
-        utils.execute("resize2fs", device_path,
-                      run_as_root=True, root_helper="sudo")
-
-
-class FSExt3(FSExt):
-
-    def __init__(self, format_options):
-        super(FSExt3, self).__init__('ext3', format_options)
-
-
-class FSExt4(FSExt):
-
-    def __init__(self, format_options):
-        super(FSExt4, self).__init__('ext4', format_options)
-
-
-class FSXFS(FSBase):
-
-    def __init__(self, format_options):
-        super(FSXFS, self).__init__('xfs', format_options)
-
-    def format(self, device_path, timeout):
-        format_options = shlex.split(self.format_options)
-        format_options.append(device_path)
-        try:
-            utils.execute_with_timeout(
-                "mkfs.xfs", *format_options,
-                timeout=timeout, run_as_root=True, root_helper="sudo")
-        except exception.ProcessExecutionError:
-            log_fmt = "Could not format '%s'."
-            exc_fmt = _("Could not format '%s'.")
-            log_and_raise(log_fmt, exc_fmt, device_path)
-
-    def check_format(self, device_path):
-        stdout, stderr = utils.execute(
-            "xfs_admin", "-l", device_path,
-            run_as_root=True, root_helper="sudo")
-        if 'not a valid XFS filesystem' in stdout:
-            msg = _("Volume '%s' does not appear to be formatted.") % (
-                device_path)
-            raise exception.GuestError(original_message=msg)
-
-    def resize(self, device_path, online=False):
-        utils.execute("xfs_repair", device_path,
-                      run_as_root=True, root_helper="sudo")
-        utils.execute("mount", device_path,
-                      run_as_root=True, root_helper="sudo")
-        utils.execute("xfs_growfs", device_path,
-                      run_as_root=True, root_helper="sudo")
-        utils.execute("umount", device_path,
-                      run_as_root=True, root_helper="sudo")
-
-
-def VolumeFs(fstype, format_options=''):
-    supported_fs = {
-        'xfs': FSXFS,
-        'ext3': FSExt3,
-        'ext4': FSExt4
-    }
-    return supported_fs[fstype](format_options)
-
-
 class VolumeDevice(object):
 
     def __init__(self, device_path):
         self.device_path = device_path
-        self.volume_fs = VolumeFs(CONF.volume_fstype,
-                                  CONF.format_options)
 
     def migrate_data(self, source_dir, target_subdir=None):
         """Synchronize the data from the source directory to the new
         volume; optionally to a new sub-directory on the new volume.
         """
         self.mount(TMP_MOUNT_POINT, write_to_fstab=False)
         if not source_dir[-1] == '/':
@@ -196,17 +55,16 @@
             target_dir = target_dir + "/" + target_subdir
         try:
             utils.execute("rsync", "--safe-links", "--perms",
                           "--recursive", "--owner", "--group", "--xattrs",
                           "--sparse", source_dir, target_dir,
                           run_as_root=True, root_helper="sudo")
         except exception.ProcessExecutionError:
-            log_msg = "Could not migrate data."
-            exc_msg = _("Could not migrate date.")
-            log_and_raise(log_msg, exc_msg)
+            msg = _("Could not migrate data.")
+            log_and_raise(msg)
         self.unmount(TMP_MOUNT_POINT)
 
     def _check_device_exists(self):
         """Check that the device path exists.
 
         Verify that the device path has actually been created and can report
         its size, only then can it be available for formatting, retry
@@ -216,39 +74,58 @@
             num_tries = CONF.num_tries
             LOG.debug("Checking if %s exists.", self.device_path)
 
             utils.execute("blockdev", "--getsize64", self.device_path,
                           run_as_root=True, root_helper="sudo",
                           attempts=num_tries)
         except exception.ProcessExecutionError:
-            log_fmt = "Device '%s' is not ready."
-            exc_fmt = _("Device '%s' is not ready.")
-            log_and_raise(log_fmt, exc_fmt, self.device_path)
+            msg = _("Device '%s' is not ready.") % self.device_path
+            log_and_raise(msg)
 
     def _check_format(self):
         """Checks that a volume is formatted."""
         LOG.debug("Checking whether '%s' is formatted.", self.device_path)
-        self.volume_fs.check_format(self.device_path)
+        try:
+            stdout, stderr = utils.execute(
+                "dumpe2fs", self.device_path,
+                run_as_root=True, root_helper="sudo")
+            if 'has_journal' not in stdout:
+                msg = _("Volume '%s' does not appear to be formatted.") % (
+                    self.device_path)
+                raise exception.GuestError(original_message=msg)
+        except exception.ProcessExecutionError as pe:
+            if 'Wrong magic number' in pe.stderr:
+                volume_fstype = CONF.volume_fstype
+                msg = _("'Device '%(dev)s' did not seem to be '%(type)s'.") % (
+                    {'dev': self.device_path, 'type': volume_fstype})
+                log_and_raise(msg)
+            msg = _("Volume '%s' was not formatted.") % self.device_path
+            log_and_raise(msg)
 
     def _format(self):
         """Calls mkfs to format the device at device_path."""
+        volume_fstype = CONF.volume_fstype
+        format_options = shlex.split(CONF.format_options)
+        format_options.append(self.device_path)
+        volume_format_timeout = CONF.volume_format_timeout
         LOG.debug("Formatting '%s'.", self.device_path)
-        self.volume_fs.format(self.device_path, CONF.volume_format_timeout)
+        try:
+            utils.execute_with_timeout(
+                "mkfs", "--type", volume_fstype, *format_options,
+                run_as_root=True, root_helper="sudo",
+                timeout=volume_format_timeout)
+        except exception.ProcessExecutionError:
+            msg = _("Could not format '%s'.") % self.device_path
+            log_and_raise(msg)
 
     def format(self):
         """Formats the device at device_path and checks the filesystem."""
         self._check_device_exists()
-
-        try:
-            self._check_format()
-            LOG.debug(f"Device {self.device_path} already formatted.")
-            return
-        except exception.GuestError:
-            self._format()
-            self._check_format()
+        self._format()
+        self._check_format()
 
     def mount(self, mount_point, write_to_fstab=True):
         """Mounts, and writes to fstab."""
         LOG.debug("Will mount %(path)s at %(mount_point)s.",
                   {'path': self.device_path, 'mount_point': mount_point})
 
         mount_point = VolumeMountPoint(self.device_path, mount_point)
@@ -264,49 +141,51 @@
         try:
             utils.poll_until(wait_for_mount, sleep_time=1, time_out=timeout)
         except exception.PollTimeOut:
             return False
 
         return True
 
-    def resize_fs(self, mount_point, online=False):
+    def resize_fs(self, mount_point):
         """Resize the filesystem on the specified device."""
         self._check_device_exists()
         # Some OS's will mount a file systems after it's attached if
         # an entry is put in the fstab file (like Trove does).
         # Thus it may be necessary to wait for the mount and then unmount
         # the fs again (since the volume was just attached).
-        if not online and self._wait_for_mount(mount_point, timeout=2):
+        if self._wait_for_mount(mount_point, timeout=2):
             LOG.debug("Unmounting '%s' before resizing.", mount_point)
             self.unmount(mount_point)
         try:
-            self.volume_fs.resize(self.device_path, online=online)
+            utils.execute("e2fsck", "-f", "-p", self.device_path,
+                          run_as_root=True, root_helper="sudo")
+            utils.execute("resize2fs", self.device_path,
+                          run_as_root=True, root_helper="sudo")
         except exception.ProcessExecutionError:
-            log_fmt = "Error resizing the filesystem with device '%s'."
-            exc_fmt = _("Error resizing the filesystem with device '%s'.")
-            log_and_raise(log_fmt, exc_fmt, self.device_path)
+            msg = _("Error resizing the filesystem with device '%s'.") % (
+                self.device_path)
+            log_and_raise(msg)
 
     def unmount(self, mount_point):
         if operating_system.is_mount(mount_point):
             try:
                 utils.execute("umount", mount_point,
                               run_as_root=True, root_helper='sudo')
             except exception.ProcessExecutionError:
-                log_fmt = "Error unmounting '%s'."
-                exc_fmt = _("Error unmounting '%s'.")
-                log_and_raise(log_fmt, exc_fmt, mount_point)
+                msg = _("Error unmounting '%s'.") % mount_point
+                log_and_raise(msg)
         else:
             LOG.debug("'%s' is not a mounted fs, cannot unmount", mount_point)
 
     def unmount_device(self, device_path):
         # unmount if device is already mounted
         mount_points = self.mount_points(device_path)
         for mnt in mount_points:
-            LOG.info("Device '%(device)s' is mounted on "
-                     "'%(mount_point)s'. Unmounting now.",
+            LOG.info(_("Device '%(device)s' is mounted on "
+                       "'%(mount_point)s'. Unmounting now."),
                      {'device': device_path, 'mount_point': mnt})
             self.unmount(mnt)
 
     def mount_points(self, device_path):
         """Returns a list of mount points on the specified device."""
         stdout, stderr = utils.execute(
             "grep '^%s ' /etc/mtab" % device_path,
@@ -317,47 +196,44 @@
         """Set the readahead size of disk."""
         self._check_device_exists()
         try:
             utils.execute("blockdev", "--setra",
                           readahead_size, self.device_path,
                           run_as_root=True, root_helper="sudo")
         except exception.ProcessExecutionError:
-            log_fmt = ("Error setting readahead size to %(size)s "
-                       "for device %(device)s.")
-            exc_fmt = _("Error setting readahead size to %(size)s "
-                        "for device %(device)s.")
-            log_and_raise(log_fmt, exc_fmt, {'size': readahead_size,
-                                             'device': self.device_path})
+            msg = _("Error setting readahead size to %(size)s "
+                    "for device %(device)s.") % {
+                'size': readahead_size, 'device': self.device_path}
+            log_and_raise(msg)
 
 
 class VolumeMountPoint(object):
 
     def __init__(self, device_path, mount_point):
         self.device_path = device_path
         self.mount_point = mount_point
         self.volume_fstype = CONF.volume_fstype
         self.mount_options = CONF.mount_options
 
     def mount(self):
         if not operating_system.exists(self.mount_point, is_directory=True,
                                        as_root=True):
-            operating_system.ensure_directory(self.mount_point, as_root=True)
+            operating_system.create_directory(self.mount_point, as_root=True)
         LOG.debug("Mounting volume. Device path:{0}, mount_point:{1}, "
                   "volume_type:{2}, mount options:{3}".format(
                       self.device_path, self.mount_point, self.volume_fstype,
                       self.mount_options))
         try:
             utils.execute("mount", "-t", self.volume_fstype,
                           "-o", self.mount_options,
                           self.device_path, self.mount_point,
                           run_as_root=True, root_helper="sudo")
         except exception.ProcessExecutionError:
-            log_fmt = "Could not mount '%s'."
-            exc_fmt = _("Could not mount '%s'.")
-            log_and_raise(log_fmt, exc_fmt, self.mount_point)
+            msg = _("Could not mount '%s'.") % self.mount_point
+            log_and_raise(msg)
 
     def write_to_fstab(self):
         fstab_line = ("%s\t%s\t%s\t%s\t0\t0" %
                       (self.device_path, self.mount_point, self.volume_fstype,
                        self.mount_options))
         LOG.debug("Writing new line to fstab:%s", fstab_line)
         with open('/etc/fstab', "r") as fstab:
@@ -365,11 +241,10 @@
         with NamedTemporaryFile(mode='w', delete=False) as tempfstab:
             tempfstab.write(fstab_content + fstab_line)
         try:
             utils.execute("install", "-o", "root", "-g", "root",
                           "-m", "644", tempfstab.name, "/etc/fstab",
                           run_as_root=True, root_helper="sudo")
         except exception.ProcessExecutionError:
-            log_fmt = "Could not add '%s' to fstab."
-            exc_fmt = _("Could not add '%s' to fstab.")
-            log_and_raise(log_fmt, exc_fmt, self.mount_point)
+            msg = _("Could not add '%s' to fstab.") % self.mount_point
+            log_and_raise(msg)
         os.remove(tempfstab.name)
```

### Comparing `trove-21.0.0.0rc2/trove/hacking/checks.py` & `trove-8.0.1/trove/hacking/translation_checks.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,88 +8,103 @@
 # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 # License for the specific language governing permissions and limitations
 # under the License.
 
 import re
 
-import pycodestyle
+import pep8
 
-from hacking import core
+_all_log_levels = {
+    'critical': '_',
+    'error': '_',
+    'exception': '_',
+    'info': '_',
+    'reserved': '_',
+    'warning': '_',
+}
+_all_hints = set(_all_log_levels.values())
+
+
+def _regex_for_level(level, hint):
+    return r".*LOG\.%(level)s\(\s*((%(wrong_hints)s)\(|'|\")" % {
+        'level': level,
+        'wrong_hints': '|'.join(_all_hints - set([hint])),
+    }
+
+
+_log_translation_hint = re.compile(
+    '|'.join('(?:%s)' % _regex_for_level(level, hint)
+             for level, hint in _all_log_levels.items()))
 
-_all_log_levels = (
-    'critical',
-    'debug',
-    'error',
-    'exception',
-    'info',
-    'reserved',
-    'warning',
-)
-
-_translated_log = re.compile(
-    r".*LOG\.(%(levels)s)\(\s*_\(\s*('|\")" % {
-        'levels': '|'.join(_all_log_levels)})
+_log_string_interpolation = re.compile(
+    r".*LOG\.(error|warning|info|critical|exception|debug)\([^,]*%[^,]*[,)]")
 
 
 def _translation_is_not_expected(filename):
     # Do not do these validations on tests
     return any(pat in filename for pat in ["/tests/"])
 
 
-@core.flake8ext
-def check_raised_localized_exceptions(logical_line, filename):
-    """T103 - Untranslated exception message.
+def validate_log_translations(logical_line, physical_line, filename):
+    """T101 - Log messages require translation hints.
     :param logical_line: The logical line to check.
+    :param physical_line: The physical line to check.
     :param filename: The file name where the logical line exists.
     :returns: None if the logical line passes the check, otherwise a tuple
     is yielded that contains the offending index in logical line and a
     message describe the check validation failure.
     """
     if _translation_is_not_expected(filename):
         return
 
-    logical_line = logical_line.strip()
-    raised_search = re.compile(
-        r"raise (?:\w*)\((.*)\)").match(logical_line)
-    if raised_search:
-        exception_msg = raised_search.groups()[0]
-        if exception_msg.startswith("\"") or exception_msg.startswith("\'"):
-            msg = "T103: Untranslated exception message."
-            yield (logical_line.index(exception_msg), msg)
+    if pep8.noqa(physical_line):
+        return
 
+    msg = "T101: Untranslated Log message."
+    if _log_translation_hint.match(logical_line):
+        yield (0, msg)
 
-@core.flake8ext
-def no_translate_logs(physical_line, logical_line, filename):
-    """T105 - Log messages shouldn't be translated from the
-    Pike release.
+
+def no_translate_debug_logs(logical_line, filename):
+    """T102 - Don't translate debug level logs.
+    Check for 'LOG.debug(_(' and 'LOG.debug(_Lx('
+    As per our translation policy,
+    https://wiki.openstack.org/wiki/LoggingStandards#Log_Translation
+    we shouldn't translate debug level logs.
+    * This check assumes that 'LOG' is a logger.
     :param logical_line: The logical line to check.
-    :param physical_line: The physical line to check.
     :param filename: The file name where the logical line exists.
     :returns: None if the logical line passes the check, otherwise a tuple
     is yielded that contains the offending index in logical line and a
     message describe the check validation failure.
     """
-    if _translation_is_not_expected(filename):
-        return
-
-    if pycodestyle.noqa(physical_line):
-        return
-
-    msg = "T105: Log message shouldn't be translated."
-    if _translated_log.match(logical_line):
-        yield (0, msg)
+    for hint in _all_hints:
+        if logical_line.startswith("LOG.debug(%s(" % hint):
+            yield(0, "T102 Don't translate debug level logs")
 
 
-asse_raises_regexp = re.compile(r"assertRaisesRegexp\(")
+def check_raised_localized_exceptions(logical_line, filename):
+    """T103 - Untranslated exception message.
+    :param logical_line: The logical line to check.
+    :param filename: The file name where the logical line exists.
+    :returns: None if the logical line passes the check, otherwise a tuple
+    is yielded that contains the offending index in logical line and a
+    message describe the check validation failure.
+    """
+    if _translation_is_not_expected(filename):
+        return
 
+    logical_line = logical_line.strip()
+    raised_search = re.compile(
+        r"raise (?:\w*)\((.*)\)").match(logical_line)
+    if raised_search:
+        exception_msg = raised_search.groups()[0]
+        if exception_msg.startswith("\"") or exception_msg.startswith("\'"):
+            msg = "T103: Untranslated exception message."
+            yield (logical_line.index(exception_msg), msg)
 
-@core.flake8ext
-def assert_raises_regexp(logical_line):
-    """Check for usage of deprecated assertRaisesRegexp
 
-    N335
-    """
-    res = asse_raises_regexp.search(logical_line)
-    if res:
-        yield (0, "N335: assertRaisesRegex must be used instead "
-                  "of assertRaisesRegexp")
+def factory(register):
+    register(validate_log_translations)
+    register(no_translate_debug_logs)
+    register(check_raised_localized_exceptions)
```

### Comparing `trove-21.0.0.0rc2/trove/instance/models.py` & `trove-8.0.1/trove/instance/models.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,98 +11,92 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 """Model classes that form the core of instances functionality."""
-import base64
-import json
-import os.path
-import re
-
 from datetime import datetime
 from datetime import timedelta
+import os.path
+import re
+from sqlalchemy import func
 
 from novaclient import exceptions as nova_exceptions
 from oslo_config.cfg import NoSuchOptError
 from oslo_log import log as logging
-from oslo_utils import encodeutils
-from oslo_utils import netutils
-from sqlalchemy import func
 
 from trove.backup.models import Backup
 from trove.common import cfg
-from trove.common import clients
 from trove.common import crypto_utils as cu
 from trove.common import exception
-from trove.common.i18n import _
-from trove.common import neutron
-from trove.common import notification
+from trove.common.glance_remote import create_glance_client
+from trove.common.i18n import _, _LE, _LI, _LW
+import trove.common.instance as tr_instance
+from trove.common.notification import StartNotification
+from trove.common.remote import create_cinder_client
+from trove.common.remote import create_dns_client
+from trove.common.remote import create_guest_client
+from trove.common.remote import create_nova_client
 from trove.common import server_group as srv_grp
 from trove.common import template
 from trove.common import timeutils
 from trove.common.trove_remote import create_trove_client
 from trove.common import utils
 from trove.configuration.models import Configuration
 from trove.datastore import models as datastore_models
 from trove.datastore.models import DatastoreVersionMetadata as dvm
 from trove.datastore.models import DBDatastoreVersionMetadata
 from trove.db import get_db_api
 from trove.db import models as dbmodels
 from trove.extensions.security_group.models import SecurityGroup
-from trove.instance import service_status as srvstatus
 from trove.instance.tasks import InstanceTask
 from trove.instance.tasks import InstanceTasks
 from trove.module import models as module_models
 from trove.module import views as module_views
 from trove.quota.quota import run_with_quotas
 from trove.taskmanager import api as task_api
 
 CONF = cfg.CONF
 LOG = logging.getLogger(__name__)
 
-# Invalid states to contact the agent
-AGENT_INVALID_STATUSES = ["BUILD", "REBOOT", "RESIZE", "PROMOTE", "EJECT",
-                          "UPGRADE"]
-
 
-def ip_visible(ip, white_list_regex, black_list_regex):
-    if re.search(white_list_regex, ip) and not re.search(black_list_regex, ip):
-        return True
-
-    return False
+def filter_ips(ips, white_list_regex, black_list_regex):
+    """Return IPs matching white_list_regex and
+       Filter out IPs matching black_list_regex.
+    """
+    return [ip for ip in ips if re.search(white_list_regex, ip)
+            and not re.search(black_list_regex, ip)]
 
 
 def load_server(context, instance_id, server_id, region_name):
     """
     Loads a server or raises an exception.
     :param context: request context used to access nova
     :param instance_id: the trove instance id corresponding to the nova server
     (informational only)
     :param server_id: the compute instance id which will be retrieved from nova
     :type context: trove.common.context.TroveContext
     :type instance_id: unicode
     :type server_id: unicode
     :rtype: novaclient.v2.servers.Server
     """
-    client = clients.create_nova_client(context, region_name=region_name)
+    client = create_nova_client(context, region_name=region_name)
     try:
         server = client.servers.get(server_id)
     except nova_exceptions.NotFound:
-        LOG.error("Could not find nova server_id(%s).", server_id)
+        LOG.error(_LE("Could not find nova server_id(%s)."), server_id)
         raise exception.ComputeInstanceNotFound(instance_id=instance_id,
                                                 server_id=server_id)
     except nova_exceptions.ClientException as e:
         raise exception.TroveError(str(e))
     return server
 
 
 class InstanceStatus(object):
-    HEALTHY = "HEALTHY"
     ACTIVE = "ACTIVE"
     BLOCKED = "BLOCKED"
     BUILD = "BUILD"
     FAILED = "FAILED"
     REBOOT = "REBOOT"
     RESIZE = "RESIZE"
     BACKUP = "BACKUP"
@@ -126,59 +120,29 @@
         raise exception.VolumeQuotaExceeded(msg)
 
 
 def load_simple_instance_server_status(context, db_info):
     """Loads a server or raises an exception."""
     if 'BUILDING' == db_info.task_status.action:
         db_info.server_status = "BUILD"
+        db_info.addresses = {}
     else:
-        client = clients.create_nova_client(context, db_info.region_id)
+        client = create_nova_client(context, db_info.region_id)
         try:
             server = client.servers.get(db_info.compute_instance_id)
             db_info.server_status = server.status
+            db_info.addresses = server.addresses
         except nova_exceptions.NotFound:
             db_info.server_status = "SHUTDOWN"
+            db_info.addresses = {}
 
 
-def load_simple_instance_addresses(context, db_info):
-    """Get addresses of the instance from Neutron."""
-    if 'BUILDING' == db_info.task_status.action and not db_info.cluster_id:
-        db_info.addresses = []
-        return
-
-    addresses = []
-    user_ports = []
-    client = clients.create_neutron_client(context, db_info.region_id)
-    ports = neutron.get_instance_ports(client, db_info.compute_instance_id)
-    for port in ports:
-        if port['network_id'] not in CONF.management_networks:
-            LOG.debug('Found user port %s for instance %s', port['id'],
-                      db_info.id)
-
-            user_ports.append(port['id'])
-            for ip in port['fixed_ips']:
-                # TODO(lxkong): IPv6 is not supported
-                if netutils.is_valid_ipv4(ip.get('ip_address')):
-                    addresses.append(
-                        {
-                            'address': ip['ip_address'],
-                            'type': 'private',
-                            'network': port['network_id']
-                        }
-                    )
-
-            fips = neutron.get_port_fips(client, port['id'])
-            if len(fips) == 0:
-                continue
-            fip = fips[0]
-            addresses.append(
-                {'address': fip['floating_ip_address'], 'type': 'public'})
-
-    db_info.ports = user_ports
-    db_info.addresses = addresses
+# Invalid states to contact the agent
+AGENT_INVALID_STATUSES = ["BUILD", "REBOOT", "RESIZE", "PROMOTE", "EJECT",
+                          "UPGRADE"]
 
 
 class SimpleInstance(object):
     """A simple view of an instance.
     This gets loaded directly from the local database, so its cheaper than
     creating the fully loaded Instance.  As the name implies this class knows
     nothing of the underlying Nova Compute Instance (i.e. server)
@@ -206,78 +170,74 @@
         """
         self.context = context
         self.db_info = db_info
         self.datastore_status = datastore_status
         self.root_pass = root_password
         self._fault = None
         self._fault_loaded = False
-        self.ds_version = None
-        self.ds = None
-        self.locality = locality
-        self.slave_list = None
-
-        if ds_version is None and self.db_info.datastore_version_id:
+        if ds_version is None:
             self.ds_version = (datastore_models.DatastoreVersion.
                                load_by_uuid(self.db_info.datastore_version_id))
-        if ds is None and self.ds_version:
+        if ds is None:
             self.ds = (datastore_models.Datastore.
                        load(self.ds_version.datastore_id))
+        self.locality = locality
+
+        self.slave_list = None
 
     def __repr__(self, *args, **kwargs):
         return "%s(%s)" % (self.name, self.id)
 
     @property
     def addresses(self):
+        # TODO(tim.simpson): This code attaches two parts of the Nova server to
+        #                   db_info: "status" and "addresses". The idea
+        #                   originally was to listen to events to update this
+        #                   data and store it in the Trove database.
+        #                   However, it may have been unwise as a year and a
+        #                   half later we still have to load the server anyway
+        #                   and this makes the code confusing.
         if hasattr(self.db_info, 'addresses'):
             return self.db_info.addresses
         else:
             return None
 
     @property
-    def ports(self):
-        if hasattr(self.db_info, 'ports'):
-            return self.db_info.ports
-        else:
-            return None
-
-    @property
     def created(self):
         return self.db_info.created
 
     @property
     def dns_ip_address(self):
         """Returns the IP address to be used with DNS."""
         ips = self.get_visible_ip_addresses()
         if ips:
-            # FIXME
             return ips[0]
 
     @property
     def flavor_id(self):
         # Flavor ID is a str in the 1.0 API.
         return str(self.db_info.flavor_id)
 
     @property
     def hostname(self):
         return self.db_info.hostname
 
     def get_visible_ip_addresses(self):
         """Returns IPs that will be visible to the user."""
-        if not self.addresses:
+        if self.addresses is None:
             return None
-
         IPs = []
-        for address in self.addresses:
-            if CONF.ip_regex and CONF.black_list_regex:
-                if not ip_visible(address['address'], CONF.ip_regex,
-                                  CONF.black_list_regex):
-                    continue
-
-            IPs.append(address)
-
+        for label in self.addresses:
+            if (re.search(CONF.network_label_regex, label) and
+                    len(self.addresses[label]) > 0):
+                IPs.extend([addr.get('addr')
+                            for addr in self.addresses[label]])
+        # Includes ip addresses that match the regexp pattern
+        if CONF.ip_regex and CONF.black_list_regex:
+            IPs = filter_ips(IPs, CONF.ip_regex, CONF.black_list_regex)
         return IPs
 
     @property
     def id(self):
         return self.db_info.id
 
     @property
@@ -333,103 +293,87 @@
                                                InstanceServiceStatus):
             raise ValueError(_("datastore_status must be of type "
                                "InstanceServiceStatus. Got %s instead.") %
                              datastore_status.__class__.__name__)
         self.__datastore_status = datastore_status
 
     @property
-    def operating_status(self):
-        """operating_status is the database service status."""
-        task_status = self.db_info.task_status
-        server_status = self.db_info.server_status
-        ds_status = self.datastore_status.status
-
-        if (task_status != InstanceTasks.NONE or server_status != 'ACTIVE'):
-            return ""
-
-        return repr(ds_status)
-
-    @property
     def status(self):
-        """The server status of the database instance.
-
-        - The task action is considered first.
-        - If it's performing backup or not.
-        - Then server status
-        - Otherwise, unknown
-        """
-        LOG.debug(f"Getting instance status for {self.id}, "
-                  f"task status: {self.db_info.task_status}, "
-                  f"datastore status: {self.datastore_status.status}, "
-                  f"server status: {self.db_info.server_status}")
-
-        task_status = self.db_info.task_status
-        server_status = self.db_info.server_status
-
         # Check for taskmanager errors.
-        if task_status.is_error:
+        if self.db_info.task_status.is_error:
+            return InstanceStatus.ERROR
+
+        # If we've reset the status, show it as an error
+        if tr_instance.ServiceStatuses.UNKNOWN == self.datastore_status.status:
             return InstanceStatus.ERROR
 
-        action = task_status.action
         # Check for taskmanager status.
-        if InstanceTasks.BUILDING.action == action:
-            if 'ERROR' == server_status:
+        action = self.db_info.task_status.action
+        if 'BUILDING' == action:
+            if 'ERROR' == self.db_info.server_status:
                 return InstanceStatus.ERROR
             return InstanceStatus.BUILD
-        if InstanceTasks.REBOOTING.action == action:
+        if 'REBOOTING' == action:
             return InstanceStatus.REBOOT
-        if InstanceTasks.RESIZING.action == action:
+        if 'RESIZING' == action:
             return InstanceStatus.RESIZE
-        if InstanceTasks.UPGRADING.action == action:
+        if 'UPGRADING' == action:
             return InstanceStatus.UPGRADE
-        if InstanceTasks.RESTART_REQUIRED.action == action:
+        if 'RESTART_REQUIRED' == action:
             return InstanceStatus.RESTART_REQUIRED
         if InstanceTasks.PROMOTING.action == action:
             return InstanceStatus.PROMOTE
         if InstanceTasks.EJECTING.action == action:
             return InstanceStatus.EJECT
         if InstanceTasks.LOGGING.action == action:
             return InstanceStatus.LOGGING
         if InstanceTasks.DETACHING.action == action:
             return InstanceStatus.DETACH
-        # Report as Shutdown while deleting, unless there's an error.
-        if InstanceTasks.DELETING.action == action:
-            if server_status in ["ACTIVE", "SHUTDOWN", "DELETED"]:
-                return InstanceStatus.SHUTDOWN
-            else:
-                LOG.error("While shutting down instance (%(instance)s): "
-                          "server had status (%(status)s).",
-                          {'instance': self.id, 'status': server_status})
-                return InstanceStatus.ERROR
-
-        # Check if there is a backup running for this instance
-        if Backup.running(self.id):
-            return InstanceStatus.BACKUP
 
         # Check for server status.
-        if server_status in ["BUILD", "ERROR", "REBOOT", "RESIZE", "ACTIVE",
-                             "SHUTDOWN"]:
-            return server_status
+        if self.db_info.server_status in ["BUILD", "ERROR", "REBOOT",
+                                          "RESIZE"]:
+            return self.db_info.server_status
 
         # As far as Trove is concerned, Nova instances in VERIFY_RESIZE should
         # still appear as though they are in RESIZE.
-        if server_status in ["VERIFY_RESIZE"]:
+        if self.db_info.server_status in ["VERIFY_RESIZE"]:
             return InstanceStatus.RESIZE
 
-        return "UNKNOWN"
+        # Check if there is a backup running for this instance
+        if Backup.running(self.id):
+            return InstanceStatus.BACKUP
+
+        # Report as Shutdown while deleting, unless there's an error.
+        if 'DELETING' == action:
+            if self.db_info.server_status in ["ACTIVE", "SHUTDOWN", "DELETED"]:
+                return InstanceStatus.SHUTDOWN
+            else:
+                LOG.error(_LE("While shutting down instance (%(instance)s): "
+                              "server had status (%(status)s)."),
+                          {'instance': self.id,
+                           'status': self.db_info.server_status})
+                return InstanceStatus.ERROR
+
+        # Check against the service status.
+        # The service is only paused during a reboot.
+        if tr_instance.ServiceStatuses.PAUSED == self.datastore_status.status:
+            return InstanceStatus.REBOOT
+        # If the service status is NEW, then we are building.
+        if tr_instance.ServiceStatuses.NEW == self.datastore_status.status:
+            return InstanceStatus.BUILD
+
+        # For everything else we can look at the service status mapping.
+        return self.datastore_status.status.api_status
 
     @property
     def updated(self):
         return self.db_info.updated
 
     @property
-    def service_status_updated(self):
-        return self.datastore_status.updated_at
-
-    @property
     def volume_id(self):
         return self.db_info.volume_id
 
     @property
     def volume_size(self):
         return self.db_info.volume_size
 
@@ -439,23 +383,19 @@
 
     @property
     def datastore(self):
         return self.ds
 
     @property
     def volume_support(self):
-        if self.datastore_version:
-            return CONF.get(self.datastore_version.manager).volume_support
-        return None
+        return CONF.get(self.datastore_version.manager).volume_support
 
     @property
     def device_path(self):
-        if self.datastore_version:
-            return CONF.get(self.datastore_version.manager).device_path
-        return None
+        return CONF.get(self.datastore_version.manager).device_path
 
     @property
     def root_password(self):
         return self.root_pass
 
     @property
     def fault(self):
@@ -497,23 +437,14 @@
     def region_name(self):
         return self.db_info.region_id
 
     @property
     def encrypted_rpc_messaging(self):
         return True if self.db_info.encrypted_key is not None else False
 
-    @property
-    def access(self):
-        if hasattr(self.db_info, 'access'):
-            if type(self.db_info.access) == str:
-                return json.loads(self.db_info.access)
-            return self.db_info.access
-        else:
-            return None
-
 
 class DetailInstance(SimpleInstance):
     """A detailed view of an Instance.
 
     This loads a SimpleInstance and then adds additional data for the
     instance from the guest.
     """
@@ -574,95 +505,73 @@
 def load_any_instance(context, id, load_server=True):
     # Try to load an instance with a server.
     # If that fails, try to load it without the server.
     try:
         return load_instance(BuiltInstance, context, id,
                              needs_server=load_server)
     except exception.UnprocessableEntity:
-        LOG.warning("Could not load instance %s.", id)
+        LOG.warning(_LW("Could not load instance %s."), id)
         return load_instance(FreshInstance, context, id, needs_server=False)
 
 
 def load_instance(cls, context, id, needs_server=False,
                   include_deleted=False):
     db_info = get_db_info(context, id, include_deleted=include_deleted)
     if not needs_server:
         # TODO(tim.simpson): When we have notifications this won't be
         # necessary and instead we'll just use the server_status field from
         # the instance table.
         load_simple_instance_server_status(context, db_info)
-        load_simple_instance_addresses(context, db_info)
         server = None
     else:
         try:
             server = load_server(context, db_info.id,
                                  db_info.compute_instance_id,
                                  region_name=db_info.region_id)
+            # TODO(tim.simpson): Remove this hack when we have notifications!
             db_info.server_status = server.status
-
-            load_simple_instance_addresses(context, db_info)
+            db_info.addresses = server.addresses
         except exception.ComputeInstanceNotFound:
-            LOG.error("Could not load compute instance %s.",
+            LOG.error(_LE("Could not load compute instance %s."),
                       db_info.compute_instance_id)
             raise exception.UnprocessableEntity("Instance %s is not ready." %
                                                 id)
 
     service_status = InstanceServiceStatus.find_by(instance_id=id)
     LOG.debug("Instance %(instance_id)s service status is %(service_status)s.",
               {'instance_id': id, 'service_status': service_status.status})
-
     return cls(context, db_info, server, service_status)
 
 
-def update_service_status(task_status, service_status, ins_id):
-    """Update service status as needed."""
-    RESTART_REQUIRED = srvstatus.ServiceStatuses.RESTART_REQUIRED
-    if (task_status == InstanceTasks.NONE and
-            service_status.status != RESTART_REQUIRED and
-            not service_status.is_uptodate()):
-        LOG.warning('Guest agent heartbeat for instance %s has expried',
-                    ins_id)
-        service_status.status = \
-            srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT
-
-
-def load_instance_with_info(cls, context, ins_id, cluster_id=None):
-    db_info = get_db_info(context, ins_id, cluster_id)
-    service_status = InstanceServiceStatus.find_by(instance_id=ins_id)
-
-    update_service_status(db_info.task_status, service_status, ins_id)
-
+def load_instance_with_info(cls, context, id, cluster_id=None):
+    db_info = get_db_info(context, id, cluster_id)
     load_simple_instance_server_status(context, db_info)
-
-    load_simple_instance_addresses(context, db_info)
-
+    service_status = InstanceServiceStatus.find_by(instance_id=id)
+    LOG.debug("Instance %(instance_id)s service status is %(service_status)s.",
+              {'instance_id': id, 'service_status': service_status.status})
     instance = cls(context, db_info, service_status)
-
-    load_guest_info(instance, context, ins_id)
-
-    load_server_group_info(instance, context)
-
+    load_guest_info(instance, context, id)
+    load_server_group_info(instance, context, db_info.compute_instance_id)
     return instance
 
 
 def load_guest_info(instance, context, id):
     if instance.status not in AGENT_INVALID_STATUSES:
-        guest = clients.create_guest_client(context, id)
+        guest = create_guest_client(context, id)
         try:
             volume_info = guest.get_volume_info()
             instance.volume_used = volume_info['used']
             instance.volume_total = volume_info['total']
         except Exception as e:
             LOG.exception(e)
     return instance
 
 
-def load_server_group_info(instance, context):
-    instance_id = instance.slave_of_id if instance.slave_of_id else instance.id
-    server_group = srv_grp.ServerGroup.load(context, instance_id)
+def load_server_group_info(instance, context, compute_id):
+    server_group = srv_grp.ServerGroup.load(context, compute_id)
     if server_group:
         instance.locality = srv_grp.ServerGroup.get_locality(server_group)
 
 
 class BaseInstance(SimpleInstance):
     """Represents an instance.
     -----------
@@ -698,417 +607,177 @@
         :typdatastore_statusus: trove.instance.models.InstanceServiceStatus
         """
         super(BaseInstance, self).__init__(context, db_info, datastore_status)
         self.server = server
         self._guest = None
         self._nova_client = None
         self._volume_client = None
-        self._neutron_client = None
         self._server_group = None
         self._server_group_loaded = False
 
     def get_guest(self):
-        return clients.create_guest_client(self.context, self.db_info.id)
+        return create_guest_client(self.context, self.db_info.id)
 
     def delete(self):
         def _delete_resources():
             if self.is_building:
                 raise exception.UnprocessableEntity(
                     "Instance %s is not ready. (Status is %s)." %
                     (self.id, self.status))
             LOG.debug("Deleting instance with compute id = %s.",
                       self.db_info.compute_instance_id)
 
             from trove.cluster.models import is_cluster_deleting
             if (self.db_info.cluster_id is not None and not
-                    is_cluster_deleting(context=self.context,
-                                        cluster_id=self.db_info.cluster_id)):
+               is_cluster_deleting(self.context, self.db_info.cluster_id)):
                 raise exception.ClusterInstanceOperationNotSupported()
 
             if self.slaves:
-                LOG.warning("Detach replicas before deleting replica source.")
-                raise exception.ReplicaSourceDeleteForbidden(
-                    _("Detach replicas before deleting replica source."))
+                msg = _("Detach replicas before deleting replica source.")
+                LOG.warning(msg)
+                raise exception.ReplicaSourceDeleteForbidden(msg)
 
             self.update_db(task_status=InstanceTasks.DELETING,
                            configuration_id=None)
             task_api.API(self.context).delete_instance(self.id)
-        flavor = self.get_flavor()
-        deltas = {'instances': -1, 'ram': -flavor.ram}
+
+        deltas = {'instances': -1}
         if self.volume_support:
             deltas['volumes'] = -self.volume_size
         return run_with_quotas(self.tenant_id,
                                deltas,
                                _delete_resources)
 
-    def server_status_matches(self, expected_status, server=None):
-        if not server:
-            server = self.server
-
-        return server.status.upper() in (
-            status.upper() for status in expected_status)
-
     def _delete_resources(self, deleted_at):
-        """Delete the openstack resources related to an instance.
-
-        Deleting the instance should not break or raise exceptions because
-        the end users want their instances to be deleted anyway. Cloud operator
-        should consider the way to clean up orphan resources afterwards, e.g.
-        using the naming convention.
-        """
-        LOG.info("Starting to delete resources for instance %s", self.id)
-
-        old_server = None
-        if self.server_id:
-            # Stop db
-            try:
-                old_server = self.nova_client.servers.get(self.server_id)
-
-                # The server may have already been marked as 'SHUTDOWN'
-                # but check for 'ACTIVE' in case of any race condition
-                # We specifically don't want to attempt to stop db if
-                # the server is in 'ERROR' or 'FAILED" state, as it will
-                # result in a long timeout
-                if self.server_status_matches(['ACTIVE', 'SHUTDOWN'],
-                                              server=self):
-                    LOG.debug("Stopping datastore on instance %s before "
-                              "deleting any resources.", self.id)
-                    self.guest.stop_db()
-            except Exception as e:
-                LOG.warning("Failed to stop the database before attempting "
-                            "to delete trove instance %s, error: %s", self.id,
-                            str(e))
-
-            # Nova VM
-            if old_server:
-                try:
-                    LOG.info("Deleting server for instance %s", self.id)
-                    self.server.delete()
-                except Exception as e:
-                    LOG.warning("Failed to delete compute server %s",
-                                self.server_id, str(e))
-
-        # Neutron ports (floating IP)
-        try:
-            ret = self.neutron_client.list_ports(name='trove-%s' % self.id)
-            ports = ret.get("ports", [])
-            for port in ports:
-                LOG.info("Deleting port %s for instance %s", port["id"],
-                         self.id)
-                neutron.delete_port(self.neutron_client, port["id"])
-        except Exception as e:
-            LOG.warning("Failed to delete ports for instance %s, "
-                        "error: %s", self.id, str(e))
-
-        # Neutron security groups
-        try:
-            name = "%s-%s" % (CONF.trove_security_group_name_prefix, self.id)
-            ret = self.neutron_client.list_security_groups(name=name)
-            sgs = ret.get("security_groups", [])
-            for sg in sgs:
-                LOG.info("Deleting security group %s for instance %s",
-                         sg["id"], self.id)
-                self.neutron_client.delete_security_group(sg["id"])
-        except Exception as e:
-            LOG.warning("Failed to delete security groups for instance %s, "
-                        "error: %s", self.id, str(e))
-
-        # DNS resources, e.g. Designate
-        try:
-            dns_support = CONF.trove_dns_support
-            if dns_support:
-                dns_api = clients.create_dns_client(self.context)
-                dns_api.delete_instance_entry(instance_id=self.id)
-        except Exception as e:
-            LOG.warning("Failed to delete dns entry of instance %s, error: %s",
-                        self.id, str(e))
-
-        # Nova server group
-        try:
-            srv_grp.ServerGroup.delete(self.context, self.server_group)
-        except Exception as e:
-            LOG.warning("Failed to delete server group for %s, error: %s",
-                        self.id, str(e))
-
-        def server_is_finished():
-            try:
-                server = self.nova_client.servers.get(self.server_id)
-                LOG.debug(f"Compute server {self.server_id} status "
-                          f"{server.status}")
-                return False
-            except nova_exceptions.NotFound:
-                return True
-
-        if old_server:
-            try:
-                LOG.info("Waiting for compute server %s removal for "
-                         "instance %s", self.server_id, self.id)
-                utils.poll_until(server_is_finished, sleep_time=2,
-                                 time_out=CONF.server_delete_time_out)
-            except exception.PollTimeOut:
-                LOG.warning("Failed to delete instance %(instance_id)s: "
-                            "Timeout deleting compute server %(vm_id)s",
-                            {'instance_id': self.id, 'vm_id': self.server_id})
-
-        # Cinder volume.
-        vols = self.volume_client.volumes.list(
-            search_opts={'name': f'trove-{self.id}'})
-        for vol in vols:
-            LOG.info(f"Deleting volume {vol.id} for instance {self.id}")
-
-            try:
-                vol.delete()
-            except Exception as e:
-                LOG.warning(f"Failed to delete volume {vol.id}({vol.status}) "
-                            f"for instance {self.id}, error: {str(e)}")
-
-        notification.TroveInstanceDelete(
-            instance=self,
-            deleted_at=timeutils.isotime(deleted_at),
-            server=old_server
-        ).notify()
-
-        LOG.info("Finished to delete resources for instance %s", self.id)
+        """Implemented in subclass."""
+        pass
 
     def delete_async(self):
         deleted_at = timeutils.utcnow()
         self._delete_resources(deleted_at)
         LOG.debug("Setting instance %s to be deleted.", self.id)
-        # Also set FOREIGN KEY fields to NULL
         self.update_db(deleted=True, deleted_at=deleted_at,
-                       task_status=InstanceTasks.NONE,
-                       datastore_version_id=None,
-                       configuration_id=None,
-                       slave_of_id=None,
-                       cluster_id=None)
+                       task_status=InstanceTasks.NONE)
         self.set_servicestatus_deleted()
         self.set_instance_fault_deleted()
-
+        # Delete associated security group
         if CONF.trove_security_groups_support:
-            # Delete associated security group for backward compatibility
             SecurityGroup.delete_for_instance(self.db_info.id, self.context,
                                               self.db_info.region_id)
 
     @property
     def guest(self):
         if not self._guest:
             self._guest = self.get_guest()
         return self._guest
 
     @property
     def nova_client(self):
         if not self._nova_client:
-            self._nova_client = clients.create_nova_client(
+            self._nova_client = create_nova_client(
                 self.context, region_name=self.db_info.region_id)
         return self._nova_client
 
     def update_db(self, **values):
         self.db_info = DBInstance.find_by(id=self.id, deleted=False)
-
-        if 'access' in values and type(values['access'] != str):
-            values['access'] = json.dumps(values['access'])
-
         for key in values:
             setattr(self.db_info, key, values[key])
         self.db_info.save()
 
     def set_servicestatus_deleted(self):
         del_instance = InstanceServiceStatus.find_by(instance_id=self.id)
-        del_instance.set_status(srvstatus.ServiceStatuses.DELETED)
-        del_instance.save()
-
-    def set_servicestatus_restart(self):
-        del_instance = InstanceServiceStatus.find_by(instance_id=self.id)
-        del_instance.set_status(srvstatus.ServiceStatuses.RESTARTING)
+        del_instance.set_status(tr_instance.ServiceStatuses.DELETED)
         del_instance.save()
 
     def set_instance_fault_deleted(self):
         try:
             del_fault = DBInstanceFault.find_by(instance_id=self.id)
             del_fault.deleted = True
             del_fault.deleted_at = datetime.utcnow()
             del_fault.save()
         except exception.ModelNotFoundError:
             pass
 
-    def get_flavor(self):
-        return self.nova_client.flavors.get(self.flavor_id)
-
     @property
     def volume_client(self):
         if not self._volume_client:
-            self._volume_client = clients.create_cinder_client(
+            self._volume_client = create_cinder_client(
                 self.context, region_name=self.db_info.region_id)
         return self._volume_client
 
-    @property
-    def neutron_client(self):
-        if not self._neutron_client:
-            self._neutron_client = clients.create_neutron_client(
-                self.context, region_name=self.db_info.region_id)
-        return self._neutron_client
-
-    @property
-    def user_neutron_client(self):
-        if not self._user_neutron_client:
-            self._user_neutron_client = clients.neutron_client(
-                self.context, region_name=self.db_info.region_id)
-        return self._user_neutron_client
-
     def reset_task_status(self):
+        LOG.info(_LI("Resetting task status to NONE on instance %s."),
+                 self.id)
         self.update_db(task_status=InstanceTasks.NONE)
 
     @property
     def server_group(self):
         # The server group could be empty, so we need a flag to cache it
         if not self._server_group_loaded:
-            self._server_group = srv_grp.ServerGroup.load(self.context,
-                                                          self.id)
+            self._server_group = srv_grp.ServerGroup.load(
+                self.context, self.db_info.compute_instance_id)
             self._server_group_loaded = True
         return self._server_group
 
-    def prepare_cloud_config(self, files):
-        # This method returns None if the files argument is None
-        userdata = None
-
-        if files:
-            userdata = (
-                "#cloud-config\n"
-                "write_files:\n"
-            )
-
-            injected_config_owner = CONF.get('injected_config_owner')
-            injected_config_group = CONF.get('injected_config_group')
-            for filename, content in files.items():
-                ud = encodeutils.safe_encode(content)
-                body_userdata = (
-                    "- encoding: b64\n"
-                    "  owner: %s:%s\n"
-                    "  path: %s\n"
-                    "  content: %s\n" % (
-                        injected_config_owner, injected_config_group, filename,
-                        encodeutils.safe_decode(base64.b64encode(ud)))
-                )
-                userdata = userdata + body_userdata
-
-        return userdata if userdata else ""
-
-    def get_injected_files(self,
-                           datastore_manager,
-                           datastore_version,
-                           **kwargs):
+    def get_injected_files(self, datastore_manager):
         injected_config_location = CONF.get('injected_config_location')
         guest_info = CONF.get('guest_info')
 
         if ('/' in guest_info):
             # Set guest_info_file to exactly guest_info from the conf file.
             # This should be /etc/guest_info for pre-Kilo compatibility.
             guest_info_file = guest_info
         else:
             guest_info_file = os.path.join(injected_config_location,
                                            guest_info)
 
-        files = {
-            guest_info_file: (
-                "[DEFAULT]\n"
-                "guest_id=%s\n"
-                "datastore_manager=%s\n"
-                "datastore_version=%s\n"
-                "tenant_id=%s\n"
-                % (self.id, datastore_manager, datastore_version,
-                   self.tenant_id)
-            )
-        }
+        files = {guest_info_file: (
+            "[DEFAULT]\n"
+            "guest_id=%s\n"
+            "datastore_manager=%s\n"
+            "tenant_id=%s\n"
+            % (self.id, datastore_manager, self.tenant_id))}
 
-        # pass through the network_isolation to guest
-        files = {
-            guest_info_file: ("%snetwork_isolation=%s\n" %
-                              (files.get(guest_info_file),
-                               CONF.network.network_isolation))
-        }
         instance_key = get_instance_encryption_key(self.id)
         if instance_key:
-            files = {
-                guest_info_file: ("%sinstance_rpc_encr_key=%s\n" %
-                                  (files.get(guest_info_file), instance_key))
-            }
+            files = {guest_info_file: (
+                "%s"
+                "instance_rpc_encr_key=%s\n" % (
+                    files.get(guest_info_file),
+                    instance_key))}
 
         if os.path.isfile(CONF.get('guest_config')):
             with open(CONF.get('guest_config'), "r") as f:
                 files[os.path.join(injected_config_location,
                                    "trove-guestagent.conf")] = f.read()
 
-        # For trove guest agent service init in dev mode
-        # Before Nova version 2.57, userdata is not supported when doing
-        # rebuild, have to use injected files instead.
-        if CONF.controller_address:
-            files['/etc/trove/controller.conf'] = (
-                f"CONTROLLER={CONF.controller_address}"
-            )
-
-        # Since Victoria, guest agent uses docker.
-        # Configure docker's daemon.json if the directives exist in trove.conf
-        docker_daemon_values = {}
-
-        # In case that user enables network_isolation with management/bussiness
-        # network not set
-        if CONF.network.network_isolation and \
-                kwargs.get("disable_bridge", False):
-            docker_daemon_values["bridge"] = "none"
-            docker_daemon_values["ip-forward"] = False
-            docker_daemon_values["iptables"] = False
-        else:
-            # Configure docker_bridge_network_ip in order to change the docker
-            # default range(172.17.0.0/16) of bridge network
-            if CONF.docker_bridge_network_ip:
-                docker_daemon_values["bip"] = CONF.docker_bridge_network_ip
-        if CONF.docker_insecure_registries:
-            docker_daemon_values["insecure-registries"] = \
-                CONF.docker_insecure_registries
-
-        if docker_daemon_values:
-            files['/etc/docker/daemon.json'] = (
-                json.dumps(docker_daemon_values)
-            )
-
         return files
 
     def reset_status(self):
-        LOG.info("Resetting the status to ERROR on instance %s.",
-                 self.id)
-        self.reset_task_status()
-
-        reset_instance = InstanceServiceStatus.find_by(instance_id=self.id)
-        reset_instance.set_status(srvstatus.ServiceStatuses.UNKNOWN)
-        reset_instance.save()
-
-    def set_service_status(self, status):
-        reset_instance = InstanceServiceStatus.find_by(instance_id=self.id)
-        reset_instance.set_status(status)
-        reset_instance.save()
-
-    def prepare_userdata(self, datastore_manager):
-        userdata = None
-        cloudinit = os.path.join(CONF.get('cloudinit_location'),
-                                 "%s.cloudinit" % datastore_manager)
-        if os.path.isfile(cloudinit):
-            with open(cloudinit, "r") as f:
-                userdata = f.read()
-        return userdata
+        if self.is_building or self.is_error:
+            LOG.info(_LI("Resetting the status to ERROR on instance %s."),
+                     self.id)
+            self.reset_task_status()
+
+            reset_instance = InstanceServiceStatus.find_by(instance_id=self.id)
+            reset_instance.set_status(tr_instance.ServiceStatuses.UNKNOWN)
+            reset_instance.save()
+        else:
+            raise exception.UnprocessableEntity(
+                "Instance %s status can only be reset in BUILD or ERROR "
+                "state." % self.id)
 
 
 class FreshInstance(BaseInstance):
-
     @classmethod
     def load(cls, context, id):
         return load_instance(cls, context, id, needs_server=False)
 
 
 class BuiltInstance(BaseInstance):
-
     @classmethod
     def load(cls, context, id, needs_server=True):
         return load_instance(cls, context, id, needs_server=needs_server)
 
 
 class Instance(BuiltInstance):
     """Represents an instance.
@@ -1128,150 +797,127 @@
                       " hence defaulting the value to False.",
                       datastore_manager)
             return False
 
     @classmethod
     def _validate_remote_datastore(cls, context, region_name, flavor,
                                    datastore, datastore_version):
-        remote_nova_client = clients.create_nova_client(
-            context, region_name=region_name)
+        remote_nova_client = create_nova_client(context,
+                                                region_name=region_name)
         try:
             remote_flavor = remote_nova_client.flavors.get(flavor.id)
             if (flavor.ram != remote_flavor.ram or
                     flavor.vcpus != remote_flavor.vcpus):
                 raise exception.TroveError(
                     "Flavors differ between regions"
                     " %(local)s and %(remote)s." %
-                    {'local': CONF.service_credentials.region_name,
-                     'remote': region_name}
-                )
+                    {'local': CONF.os_region_name, 'remote': region_name})
         except nova_exceptions.NotFound:
             raise exception.TroveError(
                 "Flavors %(flavor)s not found in region %(remote)s."
                 % {'flavor': flavor.id, 'remote': region_name})
 
         remote_trove_client = create_trove_client(
             context, region_name=region_name)
         try:
             remote_ds_ver = remote_trove_client.datastore_versions.get(
                 datastore.name, datastore_version.name)
             if datastore_version.name != remote_ds_ver.name:
                 raise exception.TroveError(
                     "Datastore versions differ between regions "
                     "%(local)s and %(remote)s." %
-                    {'local': CONF.service_credentials.region_name,
-                     'remote': region_name}
-                )
+                    {'local': CONF.os_region_name, 'remote': region_name})
         except exception.NotFound:
             raise exception.TroveError(
                 "Datastore Version %(dsv)s not found in region %(remote)s."
                 % {'dsv': datastore_version.name, 'remote': region_name})
 
-        glance_client = clients.create_glance_client(context)
+        glance_client = create_glance_client(context)
         local_image = glance_client.images.get(datastore_version.image)
-        remote_glance_client = clients.create_glance_client(
+        remote_glance_client = create_glance_client(
             context, region_name=region_name)
         remote_image = remote_glance_client.images.get(
             remote_ds_ver.image)
         if local_image.checksum != remote_image.checksum:
             raise exception.TroveError(
-                "Images for Datastore %(ds)s do not match "
+                "Images for Datastore %(ds)s do not match"
                 "between regions %(local)s and %(remote)s." %
-                {'ds': datastore.name,
-                 'local': CONF.service_credentials.region_name,
+                {'ds': datastore.name, 'local': CONF.os_region_name,
                  'remote': region_name})
 
     @classmethod
     def create(cls, context, name, flavor_id, image_id, databases, users,
                datastore, datastore_version, volume_size, backup_id,
                availability_zone=None, nics=None,
                configuration_id=None, slave_of_id=None, cluster_config=None,
                replica_count=None, volume_type=None, modules=None,
-               locality=None, region_name=None, access=None):
-        nova_client = clients.create_nova_client(context)
-        cinder_client = clients.create_cinder_client(context)
-        datastore_cfg = CONF.get(datastore_version.manager)
-        volume_support = datastore_cfg.volume_support
+               locality=None, region_name=None):
+
+        region_name = region_name or CONF.os_region_name
 
         call_args = {
             'name': name,
             'flavor_id': flavor_id,
             'datastore': datastore.name if datastore else None,
             'datastore_version': datastore_version.name,
             'image_id': image_id,
             'availability_zone': availability_zone,
             'region_name': region_name,
-            'locality': locality
         }
-        if cluster_config:
-            call_args['cluster_id'] = cluster_config.get("id", None)
 
         # All nova flavors are permitted for a datastore-version unless one
         # or more entries are found in datastore_version_metadata,
         # in which case only those are permitted.
         bound_flavors = DBDatastoreVersionMetadata.find_all(
             datastore_version_id=datastore_version.id,
             key='flavor', deleted=False
         )
         if bound_flavors.count() > 0:
             valid_flavors = tuple(f.value for f in bound_flavors)
             if flavor_id not in valid_flavors:
                 raise exception.DatastoreFlavorAssociationNotFound(
-                    datastore_version_id=datastore_version.id,
-                    id=flavor_id)
+                    datastore=datastore.name,
+                    datastore_version=datastore_version.name,
+                    flavor_id=flavor_id)
+
+        datastore_cfg = CONF.get(datastore_version.manager)
+        client = create_nova_client(context)
         try:
-            flavor = nova_client.flavors.get(flavor_id)
+            flavor = client.flavors.get(flavor_id)
         except nova_exceptions.NotFound:
             raise exception.FlavorNotFound(uuid=flavor_id)
 
-        replica_source = None
-        if slave_of_id:
-            replica_source = DBInstance.find_by(
-                context, id=slave_of_id, deleted=False)
-
         # If a different region is specified for the instance, ensure
         # that the flavor and image are the same in both regions
-        if region_name and region_name != CONF.service_credentials.region_name:
+        if region_name and region_name != CONF.os_region_name:
             cls._validate_remote_datastore(context, region_name, flavor,
                                            datastore, datastore_version)
 
-        deltas = {'instances': 1, 'ram': flavor.ram}
+        deltas = {'instances': 1}
+        volume_support = datastore_cfg.volume_support
         if volume_support:
-            if replica_source:
-                try:
-                    volume = cinder_client.volumes.get(
-                        replica_source.volume_id)
-                except Exception as e:
-                    LOG.error(f'Failed to get volume from Cinder, error: '
-                              f'{str(e)}')
-                    raise exception.NotFound(uuid=replica_source.volume_id)
-                volume_type = volume.volume_type
-                volume_size = volume.size
-
-            dvm.validate_volume_type(context, volume_type,
-                                     datastore_version.id)
-            validate_volume_size(volume_size)
             call_args['volume_type'] = volume_type
+            dvm.validate_volume_type(context, volume_type,
+                                     datastore.name, datastore_version.name)
             call_args['volume_size'] = volume_size
+            validate_volume_size(volume_size)
             deltas['volumes'] = volume_size
             # Instance volume should have enough space for the backup
             # Backup, and volume sizes are in GBs
             target_size = volume_size
         else:
             target_size = flavor.disk  # local_storage
             if volume_size is not None:
                 raise exception.VolumeNotSupported()
             if datastore_cfg.device_path:
                 if flavor.ephemeral == 0:
                     raise exception.LocalStorageNotSpecified(flavor=flavor_id)
                 target_size = flavor.ephemeral  # ephemeral_Storage
 
         if backup_id:
-            Backup.verify_swift_auth_token(context)
-
             call_args['backup_id'] = backup_id
             backup_info = Backup.get_by_id(context, backup_id)
             if not backup_info.is_done_successfuly:
                 raise exception.BackupNotCompleteError(
                     backup_id=backup_id, state=backup_info.state)
 
             if backup_info.size > target_size:
@@ -1289,52 +935,64 @@
                 raise exception.BackupDatastoreMismatchError(
                     datastore1=backup_info.datastore.name,
                     datastore2=datastore.name)
 
         if slave_of_id:
             call_args['replica_of'] = slave_of_id
             call_args['replica_count'] = replica_count
-
             replication_support = datastore_cfg.replication_strategy
             if not replication_support:
                 raise exception.ReplicationNotSupported(
                     datastore=datastore.name)
-            if (CONF.verify_replica_volume_size
-                    and replica_source.volume_size > volume_size):
-                raise exception.Forbidden(
-                    _("Replica volume size should not be smaller than"
-                      " master's, replica volume size: %(replica_size)s"
-                      " and master volume size: %(master_size)s.")
-                    % {'replica_size': volume_size,
-                       'master_size': replica_source.volume_size})
-            # load the replica source status to check if
-            # source is available
-            load_simple_instance_server_status(
-                context,
-                replica_source)
-            replica_source_instance = Instance(
-                context, replica_source,
-                None,
-                InstanceServiceStatus.find_by(
+            try:
+                # looking for replica source
+                replica_source = DBInstance.find_by(
                     context,
-                    instance_id=slave_of_id))
-            replica_source_instance.validate_can_perform_action()
-
+                    id=slave_of_id,
+                    deleted=False)
+                if replica_source.slave_of_id:
+                    raise exception.Forbidden(
+                        _("Cannot create a replica of a replica %(id)s.")
+                        % {'id': slave_of_id})
+                # load the replica source status to check if
+                # source is available
+                load_simple_instance_server_status(
+                    context,
+                    replica_source)
+                replica_source_instance = Instance(
+                    context, replica_source,
+                    None,
+                    InstanceServiceStatus.find_by(
+                        context,
+                        instance_id=slave_of_id))
+                replica_source_instance.validate_can_perform_action()
+            except exception.ModelNotFoundError:
+                LOG.exception(
+                    _("Cannot create a replica of %(id)s "
+                      "as that instance could not be found."),
+                    {'id': slave_of_id})
+                raise exception.NotFound(uuid=slave_of_id)
+        elif replica_count and replica_count != 1:
+            raise exception.Forbidden(_(
+                "Replica count only valid when creating replicas. Cannot "
+                "create %(count)d instances.") % {'count': replica_count})
         multi_replica = slave_of_id and replica_count and replica_count > 1
         instance_count = replica_count if multi_replica else 1
+        if locality:
+            call_args['locality'] = locality
 
         if not nics:
             nics = []
-        if CONF.management_networks:
-            # Make sure management network interface is always configured after
-            # user defined instance.
-            nics = nics + [{"network_id": net_id}
-                           for net_id in CONF.management_networks]
+        if CONF.default_neutron_networks:
+            nics = [{"net-id": net_id}
+                    for net_id in CONF.default_neutron_networks] + nics
         if nics:
             call_args['nics'] = nics
+        if cluster_config:
+            call_args['cluster_id'] = cluster_config.get("id", None)
 
         if not modules:
             modules = []
         module_ids = [mod['id'] for mod in modules]
         modules = module_models.Modules.load_by_ids(context, module_ids)
         auto_apply_modules = module_models.Modules.load_auto_apply(
             context, datastore.id, datastore_version.id)
@@ -1342,244 +1000,227 @@
             if aa_module.id not in module_ids:
                 modules.append(aa_module)
         module_models.Modules.validate(
             modules, datastore.id, datastore_version.id)
         module_list = module_views.convert_modules_to_list(modules)
 
         def _create_resources():
+
             if cluster_config:
                 cluster_id = cluster_config.get("id", None)
                 shard_id = cluster_config.get("shard_id", None)
                 instance_type = cluster_config.get("instance_type", None)
             else:
                 cluster_id = shard_id = instance_type = None
 
             ids = []
             names = []
             root_passwords = []
             root_password = None
             for instance_index in range(0, instance_count):
                 db_info = DBInstance.create(
-                    name=name, flavor_id=flavor_id,
-                    tenant_id=context.project_id,
+                    name=name, flavor_id=flavor_id, tenant_id=context.tenant,
                     volume_size=volume_size,
                     datastore_version_id=datastore_version.id,
                     task_status=InstanceTasks.BUILDING,
                     configuration_id=configuration_id,
                     slave_of_id=slave_of_id, cluster_id=cluster_id,
                     shard_id=shard_id, type=instance_type,
-                    region_id=region_name, access=access)
+                    region_id=region_name)
+                LOG.debug("Tenant %(tenant)s created new Trove instance "
+                          "%(db)s in region %(region)s.",
+                          {'tenant': context.tenant, 'db': db_info.id,
+                           'region': region_name})
+
                 instance_id = db_info.id
+                cls.add_instance_modules(context, instance_id, modules)
                 instance_name = name
-                LOG.debug(f"Creating new instance {instance_id}")
                 ids.append(instance_id)
                 names.append(instance_name)
                 root_passwords.append(None)
-
-                cls.add_instance_modules(context, instance_id, modules)
-
                 # change the name to be name + replica_number if more than one
                 if multi_replica:
                     replica_number = instance_index + 1
                     names[instance_index] += '-' + str(replica_number)
                     setattr(db_info, 'name', names[instance_index])
                     db_info.save()
 
                 # if a configuration group is associated with an instance,
                 # generate an overrides dict to pass into the instance creation
                 # method
+
                 config = Configuration(context, configuration_id)
                 overrides = config.get_configuration_overrides()
-
                 service_status = InstanceServiceStatus.create(
                     instance_id=instance_id,
-                    status=srvstatus.ServiceStatuses.NEW)
+                    status=tr_instance.ServiceStatuses.NEW)
 
                 if CONF.trove_dns_support:
-                    dns_client = clients.create_dns_client(context)
+                    dns_client = create_dns_client(context)
                     hostname = dns_client.determine_hostname(instance_id)
                     db_info.hostname = hostname
                     db_info.save()
 
                 if cls.get_root_on_create(
                         datastore_version.manager) and not backup_id:
                     root_password = utils.generate_random_password()
                     root_passwords[instance_index] = root_password
 
             if instance_count > 1:
                 instance_id = ids
                 instance_name = names
                 root_password = root_passwords
-
             task_api.API(context).create_instance(
                 instance_id, instance_name, flavor, image_id, databases, users,
                 datastore_version.manager, datastore_version.packages,
                 volume_size, backup_id, availability_zone, root_password,
                 nics, overrides, slave_of_id, cluster_config,
                 volume_type=volume_type, modules=module_list,
-                locality=locality, access=access,
-                ds_version=datastore_version.version)
+                locality=locality)
 
             return SimpleInstance(context, db_info, service_status,
                                   root_password, locality=locality)
 
-        with notification.StartNotification(context, **call_args):
-            return run_with_quotas(context.project_id, deltas,
-                                   _create_resources)
+        with StartNotification(context, **call_args):
+            return run_with_quotas(context.tenant, deltas, _create_resources)
 
     @classmethod
     def add_instance_modules(cls, context, instance_id, modules):
         for module in modules:
             module_models.InstanceModule.create(
                 context, instance_id, module.id, module.md5)
 
+    def get_flavor(self):
+        return self.nova_client.flavors.get(self.flavor_id)
+
     def get_default_configuration_template(self):
         flavor = self.get_flavor()
         LOG.debug("Getting default config template for datastore version "
                   "%(ds_version)s and flavor %(flavor)s.",
                   {'ds_version': self.ds_version, 'flavor': flavor})
         config = template.SingleInstanceConfigTemplate(
             self.ds_version, flavor, self.id)
         return config.render_dict()
 
     def resize_flavor(self, new_flavor_id):
         self.validate_can_perform_action()
-        LOG.info("Resizing instance %(instance_id)s flavor to "
-                 "%(flavor_id)s.",
+        LOG.info(_LI("Resizing instance %(instance_id)s flavor to "
+                     "%(flavor_id)s."),
                  {'instance_id': self.id, 'flavor_id': new_flavor_id})
         if self.db_info.cluster_id is not None:
             raise exception.ClusterInstanceOperationNotSupported()
 
-        # Validate that the old and new flavor IDs are not the same, new
-        # flavor can be found and has ephemeral/volume support if required
-        # by the current flavor.
+        # Validate that the old and new flavor IDs are not the same, new flavor
+        # can be found and has ephemeral/volume support if required by the
+        # current flavor.
         if self.flavor_id == new_flavor_id:
-            raise exception.BadRequest(
-                _("The new flavor id must be different "
-                  "than the current flavor id of '%s'.") % self.flavor_id)
+            raise exception.BadRequest(_("The new flavor id must be different "
+                                         "than the current flavor id of '%s'.")
+                                       % self.flavor_id)
         try:
             new_flavor = self.nova_client.flavors.get(new_flavor_id)
         except nova_exceptions.NotFound:
             raise exception.FlavorNotFound(uuid=new_flavor_id)
 
         old_flavor = self.nova_client.flavors.get(self.flavor_id)
         if self.volume_support:
             if new_flavor.ephemeral != 0:
                 raise exception.LocalStorageNotSupported()
         elif self.device_path is not None:
             # ephemeral support enabled
             if new_flavor.ephemeral == 0:
-                raise exception.LocalStorageNotSpecified(
-                    flavor=new_flavor_id)
-
-        def _resize_flavor():
-            # Set the task to RESIZING and begin the async call before
-            # returning.
-            self.update_db(task_status=InstanceTasks.RESIZING)
-            LOG.debug("Instance %s set to RESIZING.", self.id)
-            task_api.API(self.context).resize_flavor(self.id, old_flavor,
-                                                     new_flavor)
+                raise exception.LocalStorageNotSpecified(flavor=new_flavor_id)
 
-        return run_with_quotas(self.tenant_id,
-                               {'ram': new_flavor.ram - old_flavor.ram},
-                               _resize_flavor)
+        # Set the task to RESIZING and begin the async call before returning.
+        self.update_db(task_status=InstanceTasks.RESIZING)
+        LOG.debug("Instance %s set to RESIZING.", self.id)
+        task_api.API(self.context).resize_flavor(self.id, old_flavor,
+                                                 new_flavor)
 
     def resize_volume(self, new_size):
-        """Resize instance volume.
-
-        If the instance is primary in a replication cluster, volumes of all the
-        replicas are also resized.
-        """
-
-        def _resize_resources(instance):
-            LOG.info("Resizing volume of instance %s.", instance.id)
-            instance.update_db(task_status=InstanceTasks.RESIZING)
-            task_api.API(self.context).resize_volume(new_size, instance.id)
-
-        new_size_l = int(new_size)
+        def _resize_resources():
+            self.validate_can_perform_action()
+            LOG.info(_LI("Resizing volume of instance %s."), self.id)
+            if self.db_info.cluster_id is not None:
+                raise exception.ClusterInstanceOperationNotSupported()
+            old_size = self.volume_size
+            if int(new_size) <= old_size:
+                raise exception.BadRequest(_("The new volume 'size' must be "
+                                             "larger than the current volume "
+                                             "size of '%s'.") % old_size)
+            # Set the task to Resizing before sending off to the taskmanager
+            self.update_db(task_status=InstanceTasks.RESIZING)
+            task_api.API(self.context).resize_volume(new_size, self.id)
 
-        if self.db_info.cluster_id is not None:
-            raise exception.ClusterInstanceOperationNotSupported()
         if not self.volume_size:
             raise exception.BadRequest(_("Instance %s has no volume.")
                                        % self.id)
-        if new_size_l <= self.volume_size:
-            raise exception.BadRequest(_("The new volume 'size' must be "
-                                         "larger than the current volume "
-                                         "size of '%s'.") % self.volume_size)
+        new_size_l = int(new_size)
         validate_volume_size(new_size_l)
-        self.validate_can_perform_action()
-
-        instances = [self]
-        for dbinfo in self.slaves:
-            replica = Instance.load(self.context, dbinfo.id, needs_server=True)
-            replica.validate_can_perform_action()
-            instances.append(replica)
-
-        for instance in instances:
-            run_with_quotas(
-                self.tenant_id, {'volumes': new_size_l - self.volume_size},
-                _resize_resources, instance)
+        return run_with_quotas(self.tenant_id,
+                               {'volumes': new_size_l - self.volume_size},
+                               _resize_resources)
 
     def reboot(self):
-        LOG.info("Rebooting instance %s.", self.id)
+        self.validate_can_perform_action()
+        LOG.info(_LI("Rebooting instance %s."), self.id)
         if self.db_info.cluster_id is not None and not self.context.is_admin:
             raise exception.ClusterInstanceOperationNotSupported()
-
         self.update_db(task_status=InstanceTasks.REBOOTING)
-        self.set_servicestatus_restart()
-
         task_api.API(self.context).reboot(self.id)
 
     def restart(self):
         self.validate_can_perform_action()
-        LOG.info("Restarting datastore on instance %s.", self.id)
+        LOG.info(_LI("Restarting datastore on instance %s."), self.id)
         if self.db_info.cluster_id is not None and not self.context.is_admin:
             raise exception.ClusterInstanceOperationNotSupported()
-
+        # Set our local status since Nova might not change it quick enough.
+        # TODO(tim.simpson): Possible bad stuff can happen if this service
+        #                   shuts down before it can set status to NONE.
+        #                   We need a last updated time to mitigate this;
+        #                   after some period of tolerance, we'll assume the
+        #                   status is no longer in effect.
         self.update_db(task_status=InstanceTasks.REBOOTING)
-        self.set_servicestatus_restart()
-
         task_api.API(self.context).restart(self.id)
 
     def detach_replica(self):
         self.validate_can_perform_action()
-        LOG.info("Detaching instance %s from its replication source.",
+        LOG.info(_LI("Detaching instance %s from its replication source."),
                  self.id)
         if not self.slave_of_id:
             raise exception.BadRequest(_("Instance %s is not a replica.")
                                        % self.id)
 
         self.update_db(task_status=InstanceTasks.DETACHING)
 
         task_api.API(self.context).detach_replica(self.id)
 
     def promote_to_replica_source(self):
         self.validate_can_perform_action()
-        LOG.info("Promoting instance %s to replication source.", self.id)
+        LOG.info(_LI("Promoting instance %s to replication source."), self.id)
         if not self.slave_of_id:
             raise exception.BadRequest(_("Instance %s is not a replica.")
                                        % self.id)
 
         # Update task status of master and all slaves
         master = BuiltInstance.load(self.context, self.slave_of_id)
         for dbinfo in [master.db_info] + master.slaves:
             setattr(dbinfo, 'task_status', InstanceTasks.PROMOTING)
             dbinfo.save()
 
         task_api.API(self.context).promote_to_replica_source(self.id)
 
     def eject_replica_source(self):
-        LOG.info("Ejecting replica source %s from its replication set.",
+        self.validate_can_perform_action()
+        LOG.info(_LI("Ejecting replica source %s from its replication set."),
                  self.id)
 
         if not self.slaves:
             raise exception.BadRequest(_("Instance %s is not a replica"
-                                         " source.") % self.id)
-
+                                       " source.") % self.id)
         service = InstanceServiceStatus.find_by(instance_id=self.id)
         last_heartbeat_delta = timeutils.utcnow() - service.updated_at
         agent_expiry_interval = timedelta(seconds=CONF.agent_heartbeat_expiry)
         if last_heartbeat_delta < agent_expiry_interval:
             raise exception.BadRequest(_("Replica Source %s cannot be ejected"
                                          " as it has a current heartbeat")
                                        % self.id)
@@ -1589,52 +1230,43 @@
             setattr(dbinfo, 'task_status', InstanceTasks.EJECTING)
             dbinfo.save()
 
         task_api.API(self.context).eject_replica_source(self.id)
 
     def migrate(self, host=None):
         self.validate_can_perform_action()
-        LOG.info("Migrating instance id = %(instance_id)s "
-                 "to host = %(host)s.",
+        LOG.info(_LI("Migrating instance id = %(instance_id)s "
+                     "to host = %(host)s."),
                  {'instance_id': self.id, 'host': host})
         self.update_db(task_status=InstanceTasks.MIGRATING)
         task_api.API(self.context).migrate(self.id, host)
 
     def validate_can_perform_action(self):
         """
         Raises exception if an instance action cannot currently be performed.
         """
         # cases where action cannot be performed
-        status_type = 'instance'
-        if self.db_info.server_status not in ['ACTIVE', 'HEALTHY']:
+        if self.db_info.server_status != 'ACTIVE':
             status = self.db_info.server_status
         elif (self.db_info.task_status != InstanceTasks.NONE and
               self.db_info.task_status != InstanceTasks.RESTART_REQUIRED):
-            status_type = 'task'
-            status = self.db_info.task_status.action
+            status = self.db_info.task_status
         elif not self.datastore_status.status.action_is_allowed:
             status = self.status
         elif Backup.running(self.id):
             status = InstanceStatus.BACKUP
         else:
             # action can be performed
             return
 
-        log_fmt = ("Instance %(instance_id)s is not currently available for "
-                   "an action to be performed (%(status_type)s status was "
-                   "%(action_status)s).")
-        exc_fmt = _("Instance %(instance_id)s is not currently available for "
-                    "an action to be performed (%(status_type)s status was "
-                    "%(action_status)s).")
-        msg_content = {
-            'instance_id': self.id,
-            'status_type': status_type,
-            'action_status': status}
-        LOG.error(log_fmt, msg_content)
-        raise exception.UnprocessableEntity(exc_fmt % msg_content)
+        msg = (_("Instance %(instance_id)s is not currently available for an "
+                 "action to be performed (status was %(action_status)s).") %
+               {'instance_id': self.id, 'action_status': status})
+        LOG.error(msg)
+        raise exception.UnprocessableEntity(msg)
 
     def _validate_can_perform_assign(self):
         """
         Raises exception if a configuration assign cannot
         currently be performed
         """
 
@@ -1646,18 +1278,18 @@
             status = self.db_info.task_status.action
 
         if status:
             raise exception.InvalidInstanceState(instance_id=self.id,
                                                  status=status)
 
     def attach_configuration(self, configuration_id):
-        LOG.info("Attaching configuration %s to instance: %s",
-                 configuration_id, self.id)
+        LOG.debug("Attaching configuration to instance: %s", self.id)
         if not self.db_info.configuration_id:
             self._validate_can_perform_assign()
+            LOG.debug("Attaching configuration: %s", configuration_id)
             config = Configuration.find(self.context, configuration_id,
                                         self.db_info.datastore_version_id)
             self.update_configuration(config)
         else:
             raise exception.ConfigurationAlreadyAttached(
                 instance_id=self.id,
                 configuration_id=self.db_info.configuration_id)
@@ -1669,41 +1301,40 @@
     def save_configuration(self, configuration):
         """Save configuration changes on the guest.
         Update Trove records if successful.
         This method does not update runtime values. It sets the instance task
         to RESTART_REQUIRED.
         """
 
-        LOG.info("Saving configuration %s on instance: %s",
-                 configuration.configuration_id, self.id)
+        LOG.debug("Saving configuration on instance: %s", self.id)
         overrides = configuration.get_configuration_overrides()
 
         # Always put the instance into RESTART_REQUIRED state after
-        # configuration update. The state may be released only once (and if)
+        # configuration update. The sate may be released only once (and if)
         # the configuration is successfully applied.
         # This ensures that the instance will always be in a consistent state
         # even if the apply never executes or fails.
         LOG.debug("Persisting new configuration on the guest.")
         self.guest.update_overrides(overrides)
         LOG.debug("Configuration has been persisted on the guest.")
 
-        # Configuration has now been persisted on the instance and can be
-        # safely attached. Update our records to reflect this change
-        # irrespective of results of any further operations.
+        # Configuration has now been persisted on the instance an can be safely
+        # detached. Update our records to reflect this change irrespective of
+        # results of any further operations.
         self.update_db(task_status=InstanceTasks.RESTART_REQUIRED,
                        configuration_id=configuration.configuration_id)
 
     def apply_configuration(self, configuration):
         """Apply runtime configuration changes and release the
         RESTART_REQUIRED task.
         Apply changes only if ALL values can be applied at once.
         Return True if the configuration has changed.
         """
-        LOG.info("Applying configuration %s on instance: %s",
-                 configuration.configuration_id, self.id)
+
+        LOG.debug("Applying configuration on instance: %s", self.id)
         overrides = configuration.get_configuration_overrides()
 
         if not configuration.does_configuration_need_restart():
             LOG.debug("Applying runtime configuration changes.")
             self.guest.apply_overrides(overrides)
             LOG.debug("Configuration has been applied.")
             self.update_db(task_status=InstanceTasks.NONE)
@@ -1713,15 +1344,15 @@
         LOG.debug(
             "Configuration changes include non-dynamic settings and "
             "will require restart to take effect.")
 
         return False
 
     def detach_configuration(self):
-        LOG.info("Detaching configuration from instance: %s", self.id)
+        LOG.debug("Detaching configuration from instance: %s", self.id)
         if self.configuration and self.configuration.id:
             self._validate_can_perform_assign()
             LOG.debug("Detaching configuration: %s", self.configuration.id)
             self.remove_configuration()
         else:
             LOG.debug("No configuration found on instance.")
 
@@ -1793,22 +1424,14 @@
 
     def upgrade(self, datastore_version):
         self.update_db(datastore_version_id=datastore_version.id,
                        task_status=InstanceTasks.UPGRADING)
         task_api.API(self.context).upgrade(self.id,
                                            datastore_version.id)
 
-    def rebuild(self, image_id):
-        self.update_db(task_status=InstanceTasks.BUILDING)
-        task_api.API(self.context).rebuild(self.id, image_id)
-
-    def update_access(self, access):
-        self.update_db(task_status=InstanceTasks.UPDATING)
-        task_api.API(self.context).update_access(self.id, access)
-
 
 def create_server_list_matcher(server_list):
     # Returns a method which finds a server from the given list.
     def find_server(instance_id, server_id):
         matches = [server for server in server_list if server.id == server_id]
         if len(matches) == 1:
             return matches[0]
@@ -1816,17 +1439,17 @@
             # The instance was not found in the list and
             # this can happen if the instance is deleted from
             # nova but still in trove database
             raise exception.ComputeInstanceNotFound(
                 instance_id=instance_id, server_id=server_id)
         else:
             # Should never happen, but never say never.
-            LOG.error("Server %(server)s for instance %(instance)s was "
-                      "found twice!", {'server': server_id,
-                                       'instance': instance_id})
+            LOG.error(_LE("Server %(server)s for instance %(instance)s was "
+                          "found twice!"), {'server': server_id,
+                                            'instance': instance_id})
             raise exception.TroveError(uuid=instance_id)
 
     return find_server
 
 
 class Instances(object):
     DEFAULT_LIMIT = CONF.instances_page_size
@@ -1835,17 +1458,17 @@
     def load(context, include_clustered, instance_ids=None):
 
         def load_simple_instance(context, db_info, status, **kwargs):
             return SimpleInstance(context, db_info, status)
 
         if context is None:
             raise TypeError(_("Argument context not defined."))
-        client = clients.create_nova_client(context)
-        servers = client.servers.list(limit=-1)
-        query_opts = {'tenant_id': context.project_id,
+        client = create_nova_client(context)
+        servers = client.servers.list()
+        query_opts = {'tenant_id': context.tenant,
                       'deleted': False}
         if not include_clustered:
             query_opts['cluster_id'] = None
         if instance_ids:
             if context.is_admin:
                 query_opts.pop('tenant_id')
             filters = [DBInstance.id.in_(instance_ids)]
@@ -1887,66 +1510,61 @@
 
     @staticmethod
     def _load_servers_status(load_instance, context, db_items, find_server):
         ret = []
         for db in db_items:
             server = None
             try:
+                # TODO(tim.simpson): Delete when we get notifications working!
                 if InstanceTasks.BUILDING == db.task_status:
                     db.server_status = "BUILD"
-                    db.addresses = []
+                    db.addresses = {}
                 else:
                     try:
-                        region = CONF.service_credentials.region_name
-                        if (not db.region_id or db.region_id == region):
+                        if (not db.region_id
+                                or db.region_id == CONF.os_region_name):
                             server = find_server(db.id, db.compute_instance_id)
                         else:
-                            nova_client = clients.create_nova_client(
+                            nova_client = create_nova_client(
                                 context, region_name=db.region_id)
                             server = nova_client.servers.get(
                                 db.compute_instance_id)
                         db.server_status = server.status
-
-                        load_simple_instance_addresses(context, db)
+                        db.addresses = server.addresses
                     except exception.ComputeInstanceNotFound:
                         db.server_status = "SHUTDOWN"  # Fake it...
-                        db.addresses = []
+                        db.addresses = {}
+                # TODO(tim.simpson): End of hack.
 
-                service_status = InstanceServiceStatus.find_by(
+                # volumes = find_volumes(server.id)
+                datastore_status = InstanceServiceStatus.find_by(
                     instance_id=db.id)
-                if not service_status.status:  # This should never happen.
-                    LOG.error("Server status could not be read for "
-                              "instance id(%s).", db.id)
+                if not datastore_status.status:  # This should never happen.
+                    LOG.error(_LE("Server status could not be read for "
+                                  "instance id(%s)."), db.id)
                     continue
-
-                # Get the real-time service status.
-                LOG.debug('Task status for instance %s: %s', db.id,
-                          db.task_status)
-                update_service_status(db.task_status, service_status, db.id)
+                LOG.debug("Server api_status(%s).",
+                          datastore_status.status.api_status)
             except exception.ModelNotFoundError:
-                LOG.error("Server status could not be read for "
-                          "instance id(%s).", db.id)
+                LOG.error(_LE("Server status could not be read for "
+                              "instance id(%s)."), db.id)
                 continue
-
-            ret.append(
-                load_instance(context, db, service_status, server=server)
-            )
+            ret.append(load_instance(context, db, datastore_status,
+                                     server=server))
         return ret
 
 
 class DBInstance(dbmodels.DatabaseModelBase):
 
-    _data_fields = ['created', 'updated', 'name', 'hostname',
-                    'compute_instance_id', 'task_id', 'task_description',
-                    'task_start_time', 'volume_id', 'flavor_id',
-                    'volume_size', 'tenant_id', 'server_status',
-                    'deleted', 'deleted_at', 'datastore_version_id',
-                    'configuration_id', 'slave_of_id', 'cluster_id',
-                    'shard_id', 'type', 'region_id', 'encrypted_key', 'access']
-    _table_name = 'instances'
+    _data_fields = ['name', 'created', 'compute_instance_id',
+                    'task_id', 'task_description', 'task_start_time',
+                    'volume_id', 'deleted', 'tenant_id',
+                    'datastore_version_id', 'configuration_id', 'slave_of_id',
+                    'cluster_id', 'shard_id', 'type', 'region_id',
+                    'encrypted_key']
 
     def __init__(self, task_status, **kwargs):
         """
         Creates a new persistable entity of the Trove Guest Instance for
         purposes of recording its current state and record of modifications
         :param task_status: the current state details of any activity or error
          that is running on this guest instance (e.g. resizing, deleting)
@@ -1972,20 +1590,14 @@
     def key(self):
         if self.encrypted_key is None:
             return None
 
         return cu.decrypt_data(cu.decode_data(self.encrypted_key),
                                CONF.inst_rpc_key_encr_key)
 
-    @classmethod
-    def create(cls, **values):
-        if 'access' in values and type(values['access'] != str):
-            values['access'] = json.dumps(values['access'])
-        return super(DBInstance, cls).create(**values)
-
     def _validate(self, errors):
         if InstanceTask.from_code(self.task_id) is None:
             errors['task_id'] = "Not valid."
         if self.task_status is None:
             errors['task_status'] = "Cannot be None."
 
     def get_task_status(self):
@@ -1995,15 +1607,14 @@
         self.task_id = value.code
         self.task_description = value.db_text
 
     task_status = property(get_task_status, set_task_status)
 
 
 class instance_encryption_key_cache(object):
-
     def __init__(self, func, lru_cache_size=10):
         self._table = {}
         self._lru = []
         self._lru_cache_size = lru_cache_size
         self._func = func
 
     def get(self, instance_id):
@@ -2016,18 +1627,14 @@
         else:
             val = self._func(instance_id)
 
             # BUG(1650518): Cleanup in the Pike release
             if val is None:
                 return val
 
-            # We need string anyway
-            if isinstance(val, bytes):
-                val = encodeutils.safe_decode(val)
-
             if len(self._lru) == self._lru_cache_size:
                 tail = self._lru.pop()
                 del self._table[tail]
 
             self._lru.insert(0, instance_id)
             self._table[instance_id] = val
             return self._table[instance_id]
@@ -2065,31 +1672,30 @@
                func.count(module_models.DBInstanceModule.md5),
                (module_models.DBInstanceModule.md5 ==
                 module_models.DBModule.md5),
                func.min(module_models.DBInstanceModule.updated),
                func.max(module_models.DBInstanceModule.updated)]
     filters = [module_models.DBInstanceModule.module_id == module_id,
                module_models.DBInstanceModule.deleted == 0]
-    with module_models.DBInstanceModule.query() as query:
-        query = query.join(
-            module_models.DBModule,
-            module_models.DBInstanceModule.module_id ==
-            module_models.DBModule.id)
-        query = query.join(
-            DBInstance,
-            module_models.DBInstanceModule.instance_id == DBInstance.id)
-        if not include_clustered:
-            filters.append(DBInstance.cluster_id.is_(None))
-        if not context.is_admin:
-            filters.append(DBInstance.tenant_id == context.project_id)
-        query = query.group_by(module_models.DBInstanceModule.md5)
-        query = query.add_columns(*columns)
-        query = query.filter(*filters)
-        query = query.order_by(module_models.DBInstanceModule.updated)
-        return query.all()
+    query = module_models.DBInstanceModule.query()
+    query = query.join(
+        module_models.DBModule,
+        module_models.DBInstanceModule.module_id == module_models.DBModule.id)
+    query = query.join(
+        DBInstance,
+        module_models.DBInstanceModule.instance_id == DBInstance.id)
+    if not include_clustered:
+        filters.append(DBInstance.cluster_id.is_(None))
+    if not context.is_admin:
+        filters.append(DBInstance.tenant_id == context.tenant)
+    query = query.group_by(module_models.DBInstanceModule.md5)
+    query = query.add_columns(*columns)
+    query = query.filter(*filters)
+    query = query.order_by(module_models.DBInstanceModule.updated)
+    return query.all()
 
 
 def persist_instance_fault(notification, event_qualifier):
     """This callback is registered to be fired whenever a
     notification is sent out.
     """
     if "error" == event_qualifier:
@@ -2134,81 +1740,67 @@
             # We don't need to save anything if the instance id isn't valid
             pass
 
 
 class DBInstanceFault(dbmodels.DatabaseModelBase):
     _data_fields = ['instance_id', 'message', 'details',
                     'created', 'updated', 'deleted', 'deleted_at']
-    _table_name = 'instance_faults'
 
     def __init__(self, **kwargs):
         super(DBInstanceFault, self).__init__(**kwargs)
 
     def set_info(self, message, details):
         self.message = message
         self.details = details
 
 
 class InstanceServiceStatus(dbmodels.DatabaseModelBase):
     _data_fields = ['instance_id', 'status_id', 'status_description',
                     'updated_at']
-    _table_name = 'service_statuses'
 
     def __init__(self, status, **kwargs):
         kwargs["status_id"] = status.code
         kwargs["status_description"] = status.description
         super(InstanceServiceStatus, self).__init__(**kwargs)
         self.set_status(status)
 
     def _validate(self, errors):
         if self.status is None:
             errors['status'] = "Cannot be None."
-        if srvstatus.ServiceStatus.from_code(self.status_id) is None:
+        if tr_instance.ServiceStatus.from_code(self.status_id) is None:
             errors['status_id'] = "Not valid."
 
     def get_status(self):
         """
         Returns the current enumerated status of the Service running on the
         instance
         :return: a ServiceStatus reference indicating the currently stored
         status of the service
         :rtype: trove.common.instance.ServiceStatus
         """
-        return srvstatus.ServiceStatus.from_code(self.status_id)
+        return tr_instance.ServiceStatus.from_code(self.status_id)
 
     def set_status(self, value):
         """
         Sets the status of the hosted service
         :param value: current state of the hosted service
         :type value: trove.common.instance.ServiceStatus
         """
         self.status_id = value.code
         self.status_description = value.description
 
     def save(self):
         self['updated_at'] = timeutils.utcnow()
         return get_db_api().save(self)
 
-    def is_uptodate(self):
-        """Check if the service status heartbeat is up to date."""
-        heartbeat_expiry = timedelta(seconds=CONF.agent_heartbeat_expiry)
-        last_update = (timeutils.utcnow() - self.updated_at)
-        if last_update < heartbeat_expiry:
-            return True
-
-        return False
-
     status = property(get_status, set_status)
 
 
 def persisted_models():
     return {
-        'instances': DBInstance,
+        'instance': DBInstance,
         'instance_faults': DBInstanceFault,
         'service_statuses': InstanceServiceStatus,
     }
 
 
-MYSQL_RESPONSIVE_STATUSES = [
-    srvstatus.ServiceStatuses.RUNNING,
-    srvstatus.ServiceStatuses.HEALTHY
-]
+MYSQL_RESPONSIVE_STATUSES = [tr_instance.ServiceStatuses.RUNNING]
```

### Comparing `trove-21.0.0.0rc2/trove/instance/service.py` & `trove-8.0.1/trove/instance/service.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,46 +9,45 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-import ipaddress
-
 from oslo_log import log as logging
 from oslo_utils import strutils
 
 from trove.backup.models import Backup as backup_model
 from trove.backup import views as backup_views
 import trove.common.apischema as apischema
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
-from trove.common import glance as common_glance
 from trove.common.i18n import _
-from trove.common import neutron
+from trove.common.i18n import _LI
 from trove.common import notification
 from trove.common.notification import StartNotification
 from trove.common import pagination
 from trove.common import policy
+from trove.common.remote import create_guest_client
 from trove.common import utils
 from trove.common import wsgi
-from trove.datastore import models as ds_models
+from trove.datastore import models as datastore_models
 from trove.extensions.mysql.common import populate_users
 from trove.extensions.mysql.common import populate_validated_databases
 from trove.instance import models, views
 from trove.module import models as module_models
 from trove.module import views as module_views
 
+
 CONF = cfg.CONF
 LOG = logging.getLogger(__name__)
 
 
 class InstanceController(wsgi.Controller):
+
     """Controller for instance functionality."""
     schemas = apischema.instance.copy()
 
     @classmethod
     def authorize_instance_action(cls, context, instance_rule_name, instance):
         policy.authorize_on_target(context, 'instance:%s' % instance_rule_name,
                                    {'tenant': instance.tenant_id})
@@ -84,29 +83,28 @@
         if not body:
             raise exception.BadRequest(_("Invalid request body."))
         context = req.environ[wsgi.CONTEXT_KEY]
         _actions = {
             'restart': self._action_restart,
             'resize': self._action_resize,
             'promote_to_replica_source':
-                self._action_promote_to_replica_source,
+            self._action_promote_to_replica_source,
             'eject_replica_source': self._action_eject_replica_source,
             'reset_status': self._action_reset_status,
         }
         selected_action = None
         action_name = None
         for key in body:
             if key in _actions:
                 selected_action = _actions[key]
                 action_name = key
-        LOG.info("Performing %(action_name)s action against "
-                 "instance %(instance_id)s for tenant %(tenant_id)s, "
-                 "body: %(body)s",
+        LOG.info(_LI("Performing %(action_name)s action against "
+                     "instance %(instance_id)s for tenant '%(tenant_id)s'"),
                  {'action_name': action_name, 'instance_id': id,
-                  'tenant_id': tenant_id, 'body': body})
+                  'tenant_id': tenant_id})
         needs_server = True
         if action_name in ['reset_status']:
             needs_server = False
         instance = models.Instance.load(context, id, needs_server=needs_server)
         return selected_action(context, req, instance, body)
 
     def _action_restart(self, context, req, instance, body):
@@ -193,47 +191,29 @@
             LOG.debug("Failing backups for instance %s.", instance.id)
             backup_model.fail_for_instance(instance.id)
 
         return wsgi.Result(None, 202)
 
     def index(self, req, tenant_id):
         """Return all instances."""
-        LOG.info("Listing database instances for tenant '%s'", tenant_id)
+        LOG.info(_LI("Listing database instances for tenant '%s'"), tenant_id)
         LOG.debug("req : '%s'\n\n", req)
         context = req.environ[wsgi.CONTEXT_KEY]
         policy.authorize_on_tenant(context, 'instance:index')
-        instances = self._get_instances(req, instance_view=views.InstanceView)
-        return wsgi.Result(instances, 200)
-
-    def detail(self, req, tenant_id):
-        """Return all instances with details."""
-        LOG.info("Listing database instances with details for tenant '%s'",
-                 tenant_id)
-        LOG.debug("req : '%s'\n\n", req)
-        context = req.environ[wsgi.CONTEXT_KEY]
-        policy.authorize_on_tenant(context, 'instance:detail')
-        instances = self._get_instances(req,
-                                        instance_view=views.InstanceDetailView)
-        return wsgi.Result(instances, 200)
-
-    def _get_instances(self, req, instance_view):
-        context = req.environ[wsgi.CONTEXT_KEY]
         clustered_q = req.GET.get('include_clustered', '').lower()
         include_clustered = clustered_q == 'true'
-        instances, marker = models.Instances.load(context, include_clustered)
-        view = views.InstancesView(instances,
-                                   item_view=instance_view,
-                                   req=req)
+        servers, marker = models.Instances.load(context, include_clustered)
+        view = views.InstancesView(servers, req=req)
         paged = pagination.SimplePaginatedDataView(req.url, 'instances', view,
                                                    marker)
-        return paged.data()
+        return wsgi.Result(paged.data(), 200)
 
     def backups(self, req, tenant_id, id):
         """Return all backups for the specified instance."""
-        LOG.info("Listing backups for instance '%s'",
+        LOG.info(_LI("Listing backups for instance '%s'"),
                  id)
         LOG.debug("req : '%s'\n\n", req)
         context = req.environ[wsgi.CONTEXT_KEY]
 
         instance = models.Instance.load(context, id)
         self.authorize_instance_action(context, 'backups', instance)
 
@@ -241,31 +221,30 @@
         view = backup_views.BackupViews(backups)
         paged = pagination.SimplePaginatedDataView(req.url, 'backups', view,
                                                    marker)
         return wsgi.Result(paged.data(), 200)
 
     def show(self, req, tenant_id, id):
         """Return a single instance."""
-        LOG.info("Showing database instance '%(instance_id)s' for tenant "
-                 "'%(tenant_id)s'",
+        LOG.info(_LI("Showing database instance '%(instance_id)s' for tenant "
+                     "'%(tenant_id)s'"),
                  {'instance_id': id, 'tenant_id': tenant_id})
         LOG.debug("req : '%s'\n\n", req)
 
         context = req.environ[wsgi.CONTEXT_KEY]
-        instance = models.load_instance_with_info(models.DetailInstance,
-                                                  context, id)
-        self.authorize_instance_action(context, 'show', instance)
-        return wsgi.Result(
-            views.InstanceDetailView(instance, req=req).data(), 200
-        )
+        server = models.load_instance_with_info(models.DetailInstance,
+                                                context, id)
+        self.authorize_instance_action(context, 'show', server)
+        return wsgi.Result(views.InstanceDetailView(server,
+                                                    req=req).data(), 200)
 
     def delete(self, req, tenant_id, id):
         """Delete a single instance."""
-        LOG.info("Deleting database instance '%(instance_id)s' for tenant "
-                 "'%(tenant_id)s'",
+        LOG.info(_LI("Deleting database instance '%(instance_id)s' for tenant "
+                     "'%(tenant_id)s'"),
                  {'instance_id': id, 'tenant_id': tenant_id})
         LOG.debug("req : '%s'\n\n", req)
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.load_any_instance(context, id)
         self.authorize_instance_action(context, 'delete', instance)
         context.notification = notification.DBaaSInstanceDelete(
             context, request=req)
@@ -279,167 +258,43 @@
                         context, instance_module['instance_id'],
                         instance_module['module_id'])
                     module_models.InstanceModule.delete(
                         context, instance_module)
             instance.delete()
         return wsgi.Result(None, 202)
 
-    def _check_nic(self, context, nic):
-        """Check user provided nic.
-
-        :param context: User context.
-        :param nic: A dict may contain network_id(net-id), subnet_id or
-            ip_address.
-        """
-        neutron_client = clients.create_neutron_client(context)
-        network_id = nic.get('network_id', nic.get('net-id'))
-        subnet_id = nic.get('subnet_id')
-        ip_address = nic.get('ip_address')
-
-        if not network_id and not subnet_id:
-            raise exception.NetworkNotProvided(resource='network or subnet')
-
-        if not subnet_id and ip_address:
-            raise exception.NetworkNotProvided(resource='subnet')
-
-        if subnet_id:
-            actual_network = neutron_client.show_subnet(
-                subnet_id)['subnet']['network_id']
-            if network_id and actual_network != network_id:
-                raise exception.SubnetNotFound(subnet_id=subnet_id,
-                                               network_id=network_id)
-            network_id = actual_network
-
-        nic['network_id'] = network_id
-        nic.pop('net-id', None)
-        if not CONF.network.network_isolation:
-            self._check_network_overlap(context, network_id, subnet_id)
-
-    def _check_network_overlap(self, context, user_network=None,
-                               user_subnet=None):
-        """Check if the network contains IP address belongs to reserved
-        network.
-
-        :param context: User context.
-        :param user_network: Network ID.
-        :param user_subnet: Subnet ID.
-        """
-        neutron_client = clients.create_neutron_client(context)
-        user_cidrs = neutron.get_subnet_cidrs(neutron_client, user_network,
-                                              user_subnet)
-
-        reserved_cidrs = CONF.reserved_network_cidrs
-        mgmt_cidrs = neutron.get_mamangement_subnet_cidrs(neutron_client)
-        reserved_cidrs.extend(mgmt_cidrs)
-
-        LOG.debug("Cidrs of the user network: %s, cidrs of the reserved "
-                  "network: %s", user_cidrs, reserved_cidrs)
-
-        for user_cidr in user_cidrs:
-            user_net = ipaddress.ip_network(user_cidr)
-            for reserved_cidr in reserved_cidrs:
-                res_net = ipaddress.ip_network(reserved_cidr)
-                if user_net.overlaps(res_net):
-                    raise exception.NetworkConflict()
-
     def create(self, req, body, tenant_id):
-        LOG.info("Creating a database instance for tenant '%s'",
+        # TODO(hub-cap): turn this into middleware
+        LOG.info(_LI("Creating a database instance for tenant '%s'"),
                  tenant_id)
         LOG.debug("req : '%s'\n\n", strutils.mask_password(req))
         LOG.debug("body : '%s'\n\n", strutils.mask_password(body))
         context = req.environ[wsgi.CONTEXT_KEY]
         policy.authorize_on_tenant(context, 'instance:create')
-        context.notification = notification.DBaaSInstanceCreate(
-            context, request=req)
-
-        name = body['instance']['name']
-        slave_of_id = body['instance'].get('replica_of')
-        replica_count = body['instance'].get('replica_count')
-        flavor_ref = body['instance'].get('flavorRef')
+        context.notification = notification.DBaaSInstanceCreate(context,
+                                                                request=req)
         datastore_args = body['instance'].get('datastore', {})
-        volume_info = body['instance'].get('volume', {})
-        availability_zone = body['instance'].get('availability_zone')
-        nics = body['instance'].get('nics', [])
-        locality = body['instance'].get('locality')
-        region_name = body['instance'].get(
-            'region_name', CONF.service_credentials.region_name
-        )
-        access = body['instance'].get('access', None)
-
-        if slave_of_id:
-            if flavor_ref:
-                msg = 'Cannot specify flavor when creating replicas.'
-                raise exception.BadRequest(message=msg)
-            if datastore_args:
-                msg = 'Cannot specify datastore when creating replicas.'
-                raise exception.BadRequest(message=msg)
-            if volume_info:
-                msg = 'Cannot specify volume when creating replicas.'
-                raise exception.BadRequest(message=msg)
-            if locality:
-                msg = 'Cannot specify locality when creating replicas.'
-                raise exception.BadRequest(message=msg)
-            backup_model.verify_swift_auth_token(context)
-        else:
-            if replica_count and replica_count > 1:
-                msg = (f"Replica count only valid when creating replicas. "
-                       f"Cannot create {replica_count} instances.")
-                raise exception.BadRequest(message=msg)
-
+        datastore, datastore_version = (
+            datastore_models.get_datastore_version(**datastore_args))
+        image_id = datastore_version.image_id
+        name = body['instance']['name']
+        flavor_ref = body['instance']['flavorRef']
         flavor_id = utils.get_id_from_href(flavor_ref)
 
-        if volume_info:
-            volume_size = int(volume_info.get('size'))
-            volume_type = volume_info.get('type')
-        else:
-            volume_size = None
-            volume_type = None
-
-        if slave_of_id:
-            try:
-                replica_source = models.DBInstance.find_by(
-                    context, id=slave_of_id, deleted=False)
-                flavor_id = replica_source.flavor_id
-            except exception.ModelNotFoundError:
-                LOG.error(f"Cannot create a replica of {slave_of_id} as that "
-                          f"instance could not be found.")
-                raise exception.NotFound(uuid=slave_of_id)
-            if replica_source.slave_of_id:
-                raise exception.Forbidden(
-                    f"Cannot create a replica of a replica {slave_of_id}")
-
-            datastore_version = ds_models.DatastoreVersion.load_by_uuid(
-                replica_source.datastore_version_id)
-            datastore = ds_models.Datastore.load(
-                datastore_version.datastore_id)
-        else:
-            datastore, datastore_version = ds_models.get_datastore_version(
-                **datastore_args)
-
-        # If only image_tags is configured in the datastore version, get
-        # the image ID using the tags.
-        glance_client = clients.create_glance_client(context)
-        image_id = common_glance.get_image_id(
-            glance_client, datastore_version.image_id,
-            datastore_version.image_tags)
-        LOG.info(f'Using image {image_id} for creating instance')
-
+        configuration = self._configuration_parse(context, body)
         databases = populate_validated_databases(
             body['instance'].get('databases', []))
         database_names = [database.get('_name', '') for database in databases]
         users = None
         try:
             users = populate_users(body['instance'].get('users', []),
                                    database_names)
         except ValueError as ve:
-            raise exception.BadRequest(message=str(ve))
-        if slave_of_id and (databases or users):
-            raise exception.ReplicaCreateWithUsersDatabasesError()
+            raise exception.BadRequest(msg=ve)
 
-        configuration = self._configuration_parse(context, body)
         modules = body['instance'].get('modules')
 
         # The following operations have their own API calls.
         # We need to make sure the same policies are enforced when
         # creating an instance.
         # i.e. if attaching configuration group to an existing instance is not
         # allowed, it should not be possible to create a new instance with the
@@ -451,208 +306,194 @@
         if users:
             policy.authorize_on_tenant(
                 context, 'instance:extension:user:create')
         if databases:
             policy.authorize_on_tenant(
                 context, 'instance:extension:database:create')
 
+        if 'volume' in body['instance']:
+            volume_info = body['instance']['volume']
+            volume_size = int(volume_info['size'])
+            volume_type = volume_info.get('type')
+        else:
+            volume_size = None
+            volume_type = None
+
         if 'restorePoint' in body['instance']:
             backupRef = body['instance']['restorePoint']['backupRef']
             backup_id = utils.get_id_from_href(backupRef)
         else:
             backup_id = None
 
-        # Only 1 nic is allowed as defined in API jsonschema.
-        # Use list just for backward compatibility.
-        if len(nics) > 0:
-            nic = nics[0]
-            LOG.info('Checking user provided instance network %s', nic)
-            if slave_of_id and nic.get('ip_address'):
-                msg = "Cannot specify IP address when creating replicas."
-                raise exception.BadRequest(message=msg)
-            self._check_nic(context, nic)
+        availability_zone = body['instance'].get('availability_zone')
+        nics = body['instance'].get('nics')
 
+        slave_of_id = body['instance'].get('replica_of',
+                                           # also check for older name
+                                           body['instance'].get('slave_of'))
+        replica_count = body['instance'].get('replica_count')
+        locality = body['instance'].get('locality')
         if locality:
             locality_domain = ['affinity', 'anti-affinity']
             locality_domain_msg = ("Invalid locality '%s'. "
                                    "Must be one of ['%s']" %
                                    (locality,
                                     "', '".join(locality_domain)))
             if locality not in locality_domain:
-                raise exception.BadRequest(message=locality_domain_msg)
+                raise exception.BadRequest(msg=locality_domain_msg)
+            if slave_of_id:
+                dupe_locality_msg = (
+                    'Cannot specify locality when adding replicas to existing '
+                    'master.')
+                raise exception.BadRequest(msg=dupe_locality_msg)
+        region_name = body['instance'].get('region_name', CONF.os_region_name)
 
         instance = models.Instance.create(context, name, flavor_id,
                                           image_id, databases, users,
                                           datastore, datastore_version,
                                           volume_size, backup_id,
                                           availability_zone, nics,
                                           configuration, slave_of_id,
                                           replica_count=replica_count,
                                           volume_type=volume_type,
                                           modules=modules,
                                           locality=locality,
-                                          region_name=region_name,
-                                          access=access)
+                                          region_name=region_name)
 
         view = views.InstanceDetailView(instance, req=req)
         return wsgi.Result(view.data(), 200)
 
     def _configuration_parse(self, context, body):
         if 'configuration' in body['instance']:
             configuration_ref = body['instance']['configuration']
             if configuration_ref:
                 configuration_id = utils.get_id_from_href(configuration_ref)
                 return configuration_id
-            return ""
 
     def _modify_instance(self, context, req, instance, **kwargs):
         if 'detach_replica' in kwargs and kwargs['detach_replica']:
+            LOG.debug("Detaching replica from source.")
             context.notification = notification.DBaaSInstanceDetach(
                 context, request=req)
             with StartNotification(context, instance_id=instance.id):
                 instance.detach_replica()
-        elif 'configuration_id' in kwargs:
+        if 'configuration_id' in kwargs:
             if kwargs['configuration_id']:
                 context.notification = (
                     notification.DBaaSInstanceAttachConfiguration(context,
                                                                   request=req))
                 configuration_id = kwargs['configuration_id']
                 with StartNotification(context, instance_id=instance.id,
                                        configuration_id=configuration_id):
                     instance.attach_configuration(configuration_id)
             else:
                 context.notification = (
                     notification.DBaaSInstanceDetachConfiguration(context,
                                                                   request=req))
                 with StartNotification(context, instance_id=instance.id):
                     instance.detach_configuration()
-        elif 'datastore_version' in kwargs:
-            datastore_version = ds_models.DatastoreVersion.load(
+        if 'datastore_version' in kwargs:
+            datastore_version = datastore_models.DatastoreVersion.load(
                 instance.datastore, kwargs['datastore_version'])
-
-            if datastore_version.name == instance.ds_version.name:
-                LOG.warning(f"Same datastore version {datastore_version.name} "
-                            f"for upgrading")
-                return
-
             context.notification = (
                 notification.DBaaSInstanceUpgrade(context, request=req))
             with StartNotification(context, instance_id=instance.id,
                                    datastore_version_id=datastore_version.id):
                 instance.upgrade(datastore_version)
-        elif 'access' in kwargs:
-            instance.update_access(kwargs['access'])
+        if kwargs:
+            instance.update_db(**kwargs)
 
     def update(self, req, id, body, tenant_id):
-        """Updates the instance.
-
-        - attach/detach configuration.
-        - detach from primary.
-        - access information.
-        """
-        LOG.info("Updating database instance '%(instance_id)s' for tenant "
-                 "'%(tenant_id)s'",
+        """Updates the instance to attach/detach configuration."""
+        LOG.info(_LI("Updating database instance '%(instance_id)s' for tenant "
+                     "'%(tenant_id)s'"),
                  {'instance_id': id, 'tenant_id': tenant_id})
         LOG.debug("req: %s", req)
         LOG.debug("body: %s", body)
         context = req.environ[wsgi.CONTEXT_KEY]
 
-        name = body['instance'].get('name')
-        if ((name and len(body['instance'].keys()) > 2) or
-                (not name and len(body['instance'].keys()) >= 2)):
-            raise exception.BadRequest("Only one attribute (except 'name') is "
-                                       "allowed to update.")
-
         instance = models.Instance.load(context, id)
         self.authorize_instance_action(context, 'update', instance)
 
+        # Make sure args contains a 'configuration_id' argument,
         args = {}
-        if name:
-            instance.update_db(name=name)
+        args['configuration_id'] = self._configuration_parse(context, body)
+        self._modify_instance(context, req, instance, **args)
+        return wsgi.Result(None, 202)
 
-        detach_replica = ('replica_of' in body['instance'] or
-                          'slave_of' in body['instance'])
-        if detach_replica:
-            args['detach_replica'] = detach_replica
-
-        configuration_id = self._configuration_parse(context, body)
-        if configuration_id is not None:
-            args['configuration_id'] = configuration_id
+    def edit(self, req, id, body, tenant_id):
+        """
+        Updates the instance to set or unset one or more attributes.
+        """
+        LOG.info(_LI("Editing instance for tenant id %s."), tenant_id)
+        LOG.debug("req: %s", strutils.mask_password(req))
+        LOG.debug("body: %s", strutils.mask_password(body))
+        context = req.environ[wsgi.CONTEXT_KEY]
+
+        instance = models.Instance.load(context, id)
+        self.authorize_instance_action(context, 'edit', instance)
 
-        if 'access' in body['instance']:
-            args['access'] = body['instance']['access']
+        args = {}
+        args['detach_replica'] = ('replica_of' in body['instance'] or
+                                  'slave_of' in body['instance'])
 
+        if 'name' in body['instance']:
+            args['name'] = body['instance']['name']
+        if 'configuration' in body['instance']:
+            args['configuration_id'] = self._configuration_parse(context, body)
         if 'datastore_version' in body['instance']:
-            args['datastore_version'] = body['instance']['datastore_version']
+            args['datastore_version'] = body['instance'].get(
+                'datastore_version')
 
         self._modify_instance(context, req, instance, **args)
         return wsgi.Result(None, 202)
 
-    def edit(self, req, id, body, tenant_id):
-        """Updates the instance to set or unset one or more attributes.
-
-        Deprecated. Use update method instead.
-        """
-        return self.update(req, id, body, tenant_id)
-
     def configuration(self, req, tenant_id, id):
         """
         Returns the default configuration template applied to the instance.
         """
-        LOG.info("Getting default configuration for instance %s", id)
+        LOG.info(_LI("Getting default configuration for instance %s"), id)
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.Instance.load(context, id)
         self.authorize_instance_action(context, 'configuration', instance)
 
         LOG.debug("Server: %s", instance)
         config = instance.get_default_configuration_template()
         LOG.debug("Default config for instance %(instance_id)s is %(config)s",
                   {'instance_id': id, 'config': config})
         return wsgi.Result(views.DefaultConfigurationView(
-            config).data(), 200)
+                           config).data(), 200)
 
     def guest_log_list(self, req, tenant_id, id):
         """Return all information about all logs for an instance."""
         LOG.debug("Listing logs for tenant %s", tenant_id)
         context = req.environ[wsgi.CONTEXT_KEY]
-
-        try:
-            backup_model.verify_swift_auth_token(context)
-        except exception.SwiftNotFound:
-            raise exception.LogsNotAvailable()
-
         instance = models.Instance.load(context, id)
         if not instance:
             raise exception.NotFound(uuid=id)
         self.authorize_instance_action(context, 'guest_log_list', instance)
-        client = clients.create_guest_client(context, id)
+        client = create_guest_client(context, id)
         guest_log_list = client.guest_log_list()
         return wsgi.Result({'logs': guest_log_list}, 200)
 
     def guest_log_action(self, req, body, tenant_id, id):
         """Processes a guest log."""
-        LOG.info("Processing log for tenant %s", tenant_id)
+        LOG.info(_("Processing log for tenant %s"), tenant_id)
         context = req.environ[wsgi.CONTEXT_KEY]
-
-        try:
-            backup_model.verify_swift_auth_token(context)
-        except exception.SwiftNotFound:
-            raise exception.LogsNotAvailable()
-
         instance = models.Instance.load(context, id)
         if not instance:
             raise exception.NotFound(uuid=id)
         log_name = body['name']
         enable = body.get('enable', None)
         disable = body.get('disable', None)
         publish = body.get('publish', None)
         discard = body.get('discard', None)
         if enable and disable:
             raise exception.BadRequest(_("Cannot enable and disable log."))
-        client = clients.create_guest_client(context, id)
+        client = create_guest_client(context, id)
         guest_log = client.guest_log_action(log_name, enable, disable,
                                             publish, discard)
         return wsgi.Result({'log': guest_log}, 200)
 
     def module_list(self, req, tenant_id, id):
         """Return information about modules on an instance."""
         context = req.environ[wsgi.CONTEXT_KEY]
@@ -667,21 +508,21 @@
                 context, id, include_contents=include_contents)
         else:
             return self._module_list(
                 context, id, include_contents=include_contents)
 
     def _module_list_guest(self, context, id, include_contents):
         """Return information about modules on an instance."""
-        client = clients.create_guest_client(context, id)
+        client = create_guest_client(context, id)
         result_list = client.module_list(include_contents)
         return wsgi.Result({'modules': result_list}, 200)
 
     def _module_list(self, context, id, include_contents):
         """Return information about instance modules."""
-        client = clients.create_guest_client(context, id)
+        client = create_guest_client(context, id)
         result_list = client.module_list(include_contents)
         return wsgi.Result({'modules': result_list}, 200)
 
     def module_apply(self, req, body, tenant_id, id):
         """Apply modules to an instance."""
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.Instance.load(context, id)
@@ -689,29 +530,29 @@
             raise exception.NotFound(uuid=id)
         self.authorize_instance_action(context, 'module_apply', instance)
         module_ids = [mod['id'] for mod in body.get('modules', [])]
         modules = module_models.Modules.load_by_ids(context, module_ids)
         module_models.Modules.validate(
             modules, instance.datastore.id, instance.datastore_version.id)
         module_list = module_views.convert_modules_to_list(modules)
-        client = clients.create_guest_client(context, id)
+        client = create_guest_client(context, id)
         result_list = client.module_apply(module_list)
         models.Instance.add_instance_modules(context, id, modules)
         return wsgi.Result({'modules': result_list}, 200)
 
     def module_remove(self, req, tenant_id, id, module_id):
         """Remove module from an instance."""
         context = req.environ[wsgi.CONTEXT_KEY]
         instance = models.Instance.load(context, id)
         if not instance:
             raise exception.NotFound(uuid=id)
         self.authorize_instance_action(context, 'module_remove', instance)
         module = module_models.Module.load(context, module_id)
         module_info = module_views.DetailedModuleView(module).data()
-        client = clients.create_guest_client(context, id)
+        client = create_guest_client(context, id)
         client.module_remove(module_info)
         instance_modules = module_models.InstanceModules.load_all(
             context, instance_id=id, module_id=module_id)
         for instance_module in instance_modules:
             module_models.InstanceModule.delete(context, instance_module)
             LOG.debug("Deleted IM record %(instance_module_id)s "
                       "(instance %(id)s, module %(module_id)s).",
```

### Comparing `trove-21.0.0.0rc2/trove/instance/service_status.py` & `trove-8.0.1/trove/common/instance.py`

 * *Files 6% similar despite different names*

```diff
@@ -31,16 +31,14 @@
     @property
     def action_is_allowed(self):
         allowed_statuses = [
             ServiceStatuses.RUNNING._code,
             ServiceStatuses.SHUTDOWN._code,
             ServiceStatuses.CRASHED._code,
             ServiceStatuses.BLOCKED._code,
-            ServiceStatuses.HEALTHY._code,
-            ServiceStatuses.RESTART_REQUIRED._code,
         ]
         return self._code in allowed_statuses
 
     @property
     def api_status(self):
         return self._api_status
 
@@ -100,14 +98,10 @@
     NEW = ServiceStatus(0x17, 'new', 'NEW')
     DELETED = ServiceStatus(0x05, 'deleted', 'DELETED')
     FAILED_TIMEOUT_GUESTAGENT = ServiceStatus(0x18, 'guestagent error',
                                               'ERROR')
     INSTANCE_READY = ServiceStatus(0x19, 'instance ready', 'BUILD')
     RESTART_REQUIRED = ServiceStatus(0x20, 'restart required',
                                      'RESTART_REQUIRED')
-    HEALTHY = ServiceStatus(0x21, 'healthy', 'HEALTHY')
-    UPGRADING = ServiceStatus(0x22, 'upgrading', 'UPGRADING')
-    RESTARTING = ServiceStatus(0x22, 'restarting', 'RESTARTING')
-
 
 # Dissuade further additions at run-time.
 ServiceStatus.__init__ = None
```

### Comparing `trove-21.0.0.0rc2/trove/instance/tasks.py` & `trove-8.0.1/trove/instance/tasks.py`

 * *Files 2% similar despite different names*

```diff
@@ -78,24 +78,27 @@
     PROMOTING = InstanceTask(0x08, 'PROMOTING',
                              'Promoting the instance to replica source.')
     EJECTING = InstanceTask(0x09, 'EJECTING',
                             'Ejecting the replica source.')
     LOGGING = InstanceTask(0x0a, 'LOGGING', 'Transferring guest logs.')
     DETACHING = InstanceTask(0x0b, 'DETACHING',
                              'Detaching the instance from replica source.')
-    UPDATING = InstanceTask(0x0c, 'UPDATING', 'Updating the instance.')
 
     BUILDING_ERROR_DNS = InstanceTask(0x50, 'BUILDING', 'Build error: DNS.',
                                       is_error=True)
     BUILDING_ERROR_SERVER = InstanceTask(0x51, 'BUILDING',
                                          'Build error: Server.',
                                          is_error=True)
     BUILDING_ERROR_VOLUME = InstanceTask(0x52, 'BUILDING',
                                          'Build error: Volume.',
                                          is_error=True)
+    BUILDING_ERROR_TIMEOUT_GA = InstanceTask(0x54, 'ERROR',
+                                             'Build error: '
+                                             'guestagent timeout.',
+                                             is_error=True)
     BUILDING_ERROR_SEC_GROUP = InstanceTask(0x53, 'BUILDING',
                                             'Build error: Secgroup '
                                             'or rule.',
                                             is_error=True)
     BUILDING_ERROR_REPLICA = InstanceTask(0x54, 'BUILDING',
                                           'Build error: Replica.',
                                           is_error=True)
@@ -107,25 +110,14 @@
                                         is_error=True)
     GROWING_ERROR = InstanceTask(0x57, 'GROWING',
                                        'Growing Cluster Error.',
                                        is_error=True)
     SHRINKING_ERROR = InstanceTask(0x58, 'SHRINKING',
                                          'Shrinking Cluster Error.',
                                          is_error=True)
-    UPGRADING = InstanceTask(0x59, 'UPGRADING', 'Upgrading the instance.')
-    UPGRADING_ERROR = InstanceTask(0x5a, 'UPGRADING',
+    UPGRADING_ERROR = InstanceTask(0x59, 'UPGRADING',
                                          'Upgrading Cluster Error.',
                                          is_error=True)
-    BUILDING_ERROR_TIMEOUT_GA = InstanceTask(0x5b, 'ERROR',
-                                             'Build error: '
-                                             'guestagent timeout.',
-                                             is_error=True)
-    BUILDING_ERROR_PORT = InstanceTask(0x5c, 'BUILDING',
-                                       'Build error: Port.',
-                                       is_error=True)
-    UPDATING_ERROR_ACCESS = InstanceTask(0x5d, 'UPDATING',
-                                         'Update error: Access.',
-                                         is_error=True)
-
+    UPGRADING = InstanceTask(0x59, 'UPGRADING', 'Upgrading the instance.')
 
 # Dissuade further additions at run-time.
 InstanceTask.__init__ = None
```

### Comparing `trove-21.0.0.0rc2/trove/instance/views.py` & `trove-8.0.1/trove/instance/views.py`

 * *Files 20% similar despite different names*

```diff
@@ -31,57 +31,35 @@
         self.context = req.environ[wsgi.CONTEXT_KEY]
 
     def data(self):
         instance_dict = {
             "id": self.instance.id,
             "name": self.instance.name,
             "status": self.instance.status,
-            "operating_status": self.instance.operating_status,
             "links": self._build_links(),
             "flavor": self._build_flavor_info(),
-            "datastore": {"type": None, "version": None},
-            "region": self.instance.region_name,
-            "access": {}
-        }
-        if self.instance.datastore_version:
-            instance_dict['datastore'] = {
-                "type": self.instance.datastore.name,
-                "version": self.instance.datastore_version.name
-            }
+            "datastore": {"type": self.instance.datastore.name,
+                          "version": self.instance.datastore_version.name},
+            "region": self.instance.region_name
+        }
         if self.context.is_admin:
             instance_dict['tenant_id'] = self.instance.tenant_id
         if self.instance.volume_support:
             instance_dict['volume'] = {'size': self.instance.volume_size}
 
         if self.instance.hostname:
             instance_dict['hostname'] = self.instance.hostname
         else:
-            addresses = self.instance.get_visible_ip_addresses()
-            if addresses:
-                # NOTE(lxkong): 'ip' is deprecated in stable/ussuri and should
-                # be removed in W.
-                instance_dict['ip'] = [addr['address'] for addr in addresses]
-                instance_dict['addresses'] = addresses
+            ip = self.instance.get_visible_ip_addresses()
+            if ip:
+                instance_dict['ip'] = ip
 
         if self.instance.slave_of_id is not None:
             instance_dict['replica_of'] = self._build_master_info()
 
-        if self.instance.slaves:
-            instance_dict['replicas'] = self._build_slaves_info()
-
-        if self.instance.access:
-            instance_dict['access'] = self.instance.access
-        elif 'addresses' in instance_dict:
-            for addr in instance_dict['addresses']:
-                if addr.get('type') == 'public':
-                    instance_dict['access']['is_public'] = True
-                    break
-            else:
-                instance_dict['access']['is_public'] = False
-
         LOG.debug(instance_dict)
         return {"instance": instance_dict}
 
     def _build_links(self):
         return create_links("instances", self.req, self.instance.id)
 
     def _build_flavor_info(self):
@@ -97,49 +75,36 @@
     def _build_master_info(self):
         return {
             "id": self.instance.slave_of_id,
             "links": create_links("instances", self.req,
                                   self.instance.slave_of_id)
         }
 
-    def _build_slaves_info(self):
-        data = []
-        for slave in self.instance.slaves:
-            data.append({
-                "id": slave.id,
-                "links": create_links("instances", self.req, slave.id)
-            })
-
-        return data
-
 
 class InstanceDetailView(InstanceView):
     """Works with a full-blown instance."""
 
     def __init__(self, instance, req):
         super(InstanceDetailView, self).__init__(instance,
                                                  req=req)
 
     def data(self):
         result = super(InstanceDetailView, self).data()
         result['instance']['created'] = self.instance.created
         result['instance']['updated'] = self.instance.updated
-        result['instance']['service_status_updated'] = (self.instance.
-                                                        service_status_updated)
 
-        result['instance']['datastore']['version'] = None
-        if self.instance.datastore_version:
-            result['instance']['datastore']['version'] = \
-                self.instance.datastore_version.name
-            result['instance']['datastore']['version_number'] = \
-                self.instance.datastore_version.version
+        result['instance']['datastore']['version'] = (self.instance.
+                                                      datastore_version.name)
 
         if self.instance.fault:
             result['instance']['fault'] = self._build_fault_info()
 
+        if self.instance.slaves:
+            result['instance']['replicas'] = self._build_slaves_info()
+
         if self.instance.configuration is not None:
             result['instance']['configuration'] = (self.
                                                    _build_configuration_info())
 
         if self.instance.locality:
             result['instance']['locality'] = self.instance.locality
 
@@ -172,43 +137,48 @@
     def _build_fault_info(self):
         return {
             "message": self.instance.fault.message,
             "created": self.instance.fault.updated,
             "details": self.instance.fault.details,
         }
 
+    def _build_slaves_info(self):
+        data = []
+        for slave in self.instance.slaves:
+            data.append({
+                "id": slave.id,
+                "links": create_links("instances", self.req, slave.id)
+            })
+        return data
+
     def _build_configuration_info(self):
         return {
             "id": self.instance.configuration.id,
             "name": self.instance.configuration.name,
             "links": create_links("configurations", self.req,
                                   self.instance.configuration.id)
         }
 
 
 class InstancesView(object):
     """Shows a list of SimpleInstance objects."""
 
-    def __init__(self, instances, item_view=InstanceView, req=None):
+    def __init__(self, instances, req=None):
         self.instances = instances
-        self.item_view = item_view
         self.req = req
 
     def data(self):
         data = []
-
-        # Return instances in the order of 'created'
         # These are model instances
-        for instance in sorted(self.instances, key=lambda ins: ins.created,
-                               reverse=True):
+        for instance in self.instances:
             data.append(self.data_for_instance(instance))
         return {'instances': data}
 
     def data_for_instance(self, instance):
-        view = self.item_view(instance, req=self.req)
+        view = InstanceView(instance, req=self.req)
         return view.data()['instance']
 
 
 class DefaultConfigurationView(object):
     def __init__(self, config):
         self.config = config
 
@@ -239,15 +209,15 @@
 
 class GuestLogsView(object):
 
     def __init__(self, guest_logs):
         self.guest_logs = guest_logs
 
     def data(self):
-        return [GuestLogView(guestlog).data() for guestlog in self.guest_logs]
+        return [GuestLogView(l).data() for l in self.guest_logs]
 
 
 def convert_instance_count_to_list(instance_count):
     instance_list = []
     for row in instance_count:
         (_, name, id, md5, count, current, min_date, max_date) = row
         instance_list.append(
```

### Comparing `trove-21.0.0.0rc2/trove/limits/service.py` & `trove-8.0.1/trove/limits/service.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/limits/views.py` & `trove-8.0.1/trove/limits/views.py`

 * *Files 4% similar despite different names*

```diff
@@ -49,10 +49,10 @@
         data = []
         abs_view = dict()
         abs_view["verb"] = "ABSOLUTE"
         for resource_name, abs_limit in self.abs_limits.items():
             abs_view["max_" + resource_name] = abs_limit
 
         data.append(abs_view)
-        for limit in self.rate_limits:
-            data.append(LimitView(limit).data()["limit"])
+        for l in self.rate_limits:
+            data.append(LimitView(l).data()["limit"])
         return {"limits": data}
```

### Comparing `trove-21.0.0.0rc2/trove/module/models.py` & `trove-8.0.1/trove/module/models.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 """Model classes that form the core of Module functionality."""
 
 import hashlib
+import six
 from sqlalchemy.sql.expression import or_
 
 from oslo_log import log as logging
 
 from trove.common import cfg
 from trove.common import crypto_utils
 from trove.common import exception
@@ -59,46 +60,40 @@
             db_info = DBModule.find_all(**query_opts)
             if db_info.count() == 0:
                 LOG.debug("No modules found for admin user")
         else:
             # build a query manually, since we need current tenant
             # plus the 'all' tenant ones
             query_opts['visible'] = True
-            with DBModule.query() as query:
-                db_info = query.filter_by(**query_opts)
-                db_info = db_info.filter(
-                    or_(DBModule.tenant_id == context.project_id,
-                        DBModule.tenant_id.is_(None))
-                )
-                if db_info.count() == 0:
-                    LOG.debug("No modules found for tenant %s",
-                              context.project_id)
-                modules = db_info.all()
+            db_info = DBModule.query().filter_by(**query_opts)
+            db_info = db_info.filter(or_(DBModule.tenant_id == context.tenant,
+                                         DBModule.tenant_id.is_(None)))
+            if db_info.count() == 0:
+                LOG.debug("No modules found for tenant %s", context.tenant)
+        modules = db_info.all()
         return modules
 
     @staticmethod
     def load_auto_apply(context, datastore_id, datastore_version_id):
         """Return all the auto-apply modules for the given criteria."""
         if context is None:
             raise TypeError(_("Argument context not defined."))
         elif id is None:
             raise TypeError(_("Argument is not defined."))
 
         query_opts = {'deleted': False,
                       'auto_apply': True}
-        with DBModule.query() as query:
-            db_info = query.filter_by(**query_opts)
-            db_info = Modules.add_tenant_filter(db_info, context.project_id)
-            db_info = Modules.add_datastore_filter(db_info, datastore_id)
-            db_info = Modules.add_ds_version_filter(db_info,
-                                                    datastore_version_id)
-            if db_info.count() == 0:
-                LOG.debug("No auto-apply modules found for tenant %s",
-                          context.project_id)
-            modules = db_info.all()
+        db_info = DBModule.query().filter_by(**query_opts)
+        db_info = Modules.add_tenant_filter(db_info, context.tenant)
+        db_info = Modules.add_datastore_filter(db_info, datastore_id)
+        db_info = Modules.add_ds_version_filter(db_info, datastore_version_id)
+        if db_info.count() == 0:
+            LOG.debug("No auto-apply modules found for tenant %s",
+                      context.tenant)
+        modules = db_info.all()
         return modules
 
     @staticmethod
     def add_tenant_filter(query, tenant_id):
         return query.filter(or_(DBModule.tenant_id == tenant_id,
                                 DBModule.tenant_id.is_(None)))
 
@@ -122,21 +117,19 @@
             raise TypeError(_("Argument context not defined."))
         elif id is None:
             raise TypeError(_("Argument is not defined."))
 
         modules = []
         if module_ids:
             query_opts = {'deleted': False}
-            with DBModule.query() as query:
-                db_info = query.filter_by(**query_opts)
-                if not context.is_admin:
-                    db_info = Modules.add_tenant_filter(db_info,
-                                                        context.project_id)
-                db_info = db_info.filter(DBModule.id.in_(module_ids))
-                modules = db_info.all()
+            db_info = DBModule.query().filter_by(**query_opts)
+            if not context.is_admin:
+                db_info = Modules.add_tenant_filter(db_info, context.tenant)
+            db_info = db_info.filter(DBModule.id.in_(module_ids))
+            modules = db_info.all()
         return modules
 
     @staticmethod
     def validate(modules, datastore_id, datastore_version_id):
         for module in modules:
             if (module.datastore_id and
                     module.datastore_id != datastore_id):
@@ -164,15 +157,15 @@
 
     @staticmethod
     def create(context, name, module_type, contents,
                description, tenant_id, datastore,
                datastore_version, auto_apply, visible, live_update,
                priority_apply, apply_order, full_access):
         if module_type.lower() not in Modules.VALID_MODULE_TYPES:
-            LOG.error("Valid module types: %s", Modules.VALID_MODULE_TYPES)
+            LOG.error(_("Valid module types: %s"), Modules.VALID_MODULE_TYPES)
             raise exception.ModuleTypeNotFound(module_type=module_type)
         Module.validate_action(
             context, 'create', tenant_id, auto_apply, visible, priority_apply,
             full_access)
         datastore_id, datastore_version_id = (
             datastore_models.get_datastore_or_version(
                 datastore, datastore_version))
@@ -247,15 +240,15 @@
 
     # We encrypt the contents (which should be encoded already, since it
     # might be in binary format) and then encode them again so they can
     # be stored in a text field in the Trove database.
     @staticmethod
     def process_contents(contents):
         md5 = contents
-        if isinstance(md5, str):
+        if isinstance(md5, six.text_type):
             md5 = md5.encode('utf-8')
         md5 = hashlib.md5(md5).hexdigest()
         encrypted_contents = crypto_utils.encrypt_data(
             contents, Modules.ENCRYPT_KEY)
         return md5, crypto_utils.encode_data(encrypted_contents)
 
     # Do the reverse to 'deprocess' the contents
@@ -288,15 +281,15 @@
     def load(context, module_id):
         module = None
         try:
             if context.is_admin:
                 module = DBModule.find_by(id=module_id, deleted=False)
             else:
                 module = DBModule.find_by(
-                    id=module_id, tenant_id=context.project_id, visible=True,
+                    id=module_id, tenant_id=context.tenant, visible=True,
                     deleted=False)
         except exception.ModelNotFoundError:
             # See if we have the module in the 'all' tenant section
             if not context.is_admin:
                 try:
                     module = DBModule.find_by(
                         id=module_id, tenant_id=None, visible=True,
@@ -449,24 +442,22 @@
     def update(context, instance_module):
         instance_module.updated = timeutils.utcnow()
         DBInstanceModule.save(instance_module)
 
 
 class DBInstanceModule(models.DatabaseModelBase):
     _data_fields = [
-        'instance_id', 'module_id', 'md5', 'created',
+        'id', 'instance_id', 'module_id', 'md5', 'created',
         'updated', 'deleted', 'deleted_at']
-    _table_name = 'instance_modules'
 
 
 class DBModule(models.DatabaseModelBase):
     _data_fields = [
-        'name', 'type', 'contents', 'description',
+        'id', 'name', 'type', 'contents', 'description',
         'tenant_id', 'datastore_id', 'datastore_version_id',
         'auto_apply', 'visible', 'live_update',
-        'md5', 'created', 'updated', 'deleted', 'deleted_at',
-        'priority_apply', 'apply_order', 'is_admin']
-    _table_name = 'modules'
+        'priority_apply', 'apply_order', 'is_admin',
+        'md5', 'created', 'updated', 'deleted', 'deleted_at']
 
 
 def persisted_models():
     return {'modules': DBModule, 'instance_modules': DBInstanceModule}
```

### Comparing `trove-21.0.0.0rc2/trove/module/service.py` & `trove-8.0.1/trove/module/service.py`

 * *Files 2% similar despite different names*

```diff
@@ -60,29 +60,29 @@
                 type=datastore)
             datastore = ds.id
         modules = models.Modules.load(context, datastore=datastore)
         view = views.ModulesView(modules)
         return wsgi.Result(view.data(), 200)
 
     def show(self, req, tenant_id, id):
-        LOG.info("Showing module %s.", id)
+        LOG.info(_("Showing module %s."), id)
 
         context = req.environ[wsgi.CONTEXT_KEY]
         module = models.Module.load(context, id)
         self.authorize_module_action(context, 'show', module)
         module.instance_count = len(models.InstanceModules.load(
             context, module_id=module.id, md5=module.md5))
 
         return wsgi.Result(
             views.DetailedModuleView(module).data(), 200)
 
     def create(self, req, body, tenant_id):
 
         name = body['module']['name']
-        LOG.info("Creating module '%s'", name)
+        LOG.info(_("Creating module '%s'"), name)
 
         context = req.environ[wsgi.CONTEXT_KEY]
         policy.authorize_on_tenant(context, 'module:create')
         module_type = body['module']['module_type']
         contents = body['module']['contents']
 
         description = body['module'].get('description')
@@ -102,24 +102,24 @@
             description, module_tenant_id, datastore, ds_version,
             auto_apply, visible, live_update, priority_apply,
             apply_order, full_access)
         view_data = views.DetailedModuleView(module)
         return wsgi.Result(view_data.data(), 200)
 
     def delete(self, req, tenant_id, id):
-        LOG.info("Deleting module %s.", id)
+        LOG.info(_("Deleting module %s."), id)
 
         context = req.environ[wsgi.CONTEXT_KEY]
         module = models.Module.load(context, id)
         self.authorize_module_action(context, 'delete', module)
         models.Module.delete(context, module)
         return wsgi.Result(None, 200)
 
     def update(self, req, body, tenant_id, id):
-        LOG.info("Updating module %s.", id)
+        LOG.info(_("Updating module %s."), id)
 
         context = req.environ[wsgi.CONTEXT_KEY]
         module = models.Module.load(context, id)
         self.authorize_module_action(context, 'update', module)
         original_module = copy.deepcopy(module)
         if 'name' in body['module']:
             module.name = body['module']['name']
@@ -169,15 +169,15 @@
             full_access = body['module']['full_access']
 
         models.Module.update(context, module, original_module, full_access)
         view_data = views.DetailedModuleView(module)
         return wsgi.Result(view_data.data(), 200)
 
     def instances(self, req, tenant_id, id):
-        LOG.info("Getting instances for module %s.", id)
+        LOG.info(_("Getting instances for module %s."), id)
 
         context = req.environ[wsgi.CONTEXT_KEY]
 
         module = models.Module.load(context, id)
         self.authorize_module_action(context, 'instances', module)
 
         count_only = req.GET.get('count_only', '').lower() == 'true'
@@ -202,15 +202,15 @@
                 marker = None
             view = instance_views.InstancesView(instances, req=req)
             result_list = pagination.SimplePaginatedDataView(
                 req.url, 'instances', view, marker).data()
         return wsgi.Result(result_list, 200)
 
     def reapply(self, req, body, tenant_id, id):
-        LOG.info("Reapplying module %s to all instances.", id)
+        LOG.info(_("Reapplying module %s to all instances."), id)
 
         context = req.environ[wsgi.CONTEXT_KEY]
         md5 = None
         if 'md5' in body['reapply']:
             md5 = body['reapply']['md5']
         include_clustered = None
         if 'include_clustered' in body['reapply']:
```

### Comparing `trove-21.0.0.0rc2/trove/module/views.py` & `trove-8.0.1/trove/module/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/network/base.py` & `trove-8.0.1/trove/network/base.py`

 * *Files 13% similar despite different names*

```diff
@@ -11,24 +11,41 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 import abc
 
+import six
 
-class NetworkDriver(object, metaclass=abc.ABCMeta):
+
+@six.add_metaclass(abc.ABCMeta)
+class NetworkDriver(object):
     """Base Network Driver class to abstract the network driver used."""
 
     @abc.abstractmethod
     def get_sec_group_by_id(self, group_id):
         """
         Returns security group with given group_id
         """
 
     @abc.abstractmethod
+    def create_security_group(self, name, description):
+        """
+        Creates the security group with given name and description
+        """
+
+    @abc.abstractmethod
     def delete_security_group(self, sec_group_id):
         """Deletes the security group by given ID."""
 
     @abc.abstractmethod
+    def add_security_group_rule(self, sec_group_id, protocol,
+                                from_port, to_port, cidr):
+        """
+        Adds the rule identified by the security group ID,
+        transport protocol, port range: from -> to, CIDR.
+        """
+
+    @abc.abstractmethod
     def delete_security_group_rule(self, sec_group_rule_id):
         """Deletes the rule by given ID."""
```

### Comparing `trove-21.0.0.0rc2/trove/network/neutron.py` & `trove-8.0.1/trove/network/nova.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,47 +8,75 @@
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-from neutronclient.common import exceptions as neutron_exceptions
+#
+
+from novaclient import exceptions as nova_exceptions
 from oslo_log import log as logging
 
-from trove.common import clients
 from trove.common import exception
+from trove.common.i18n import _
+from trove.common import remote
 from trove.network import base
 
+
 LOG = logging.getLogger(__name__)
 
 
-class NeutronDriver(base.NetworkDriver):
+class NovaNetwork(base.NetworkDriver):
 
     def __init__(self, context, region_name):
         try:
-            self.client = clients.create_neutron_client(context, region_name)
-        except neutron_exceptions.NeutronClientException as e:
+            self.client = remote.create_nova_client(
+                context, region_name)
+        except nova_exceptions.ClientException as e:
             raise exception.TroveError(str(e))
 
     def get_sec_group_by_id(self, group_id):
         try:
-            return self.client.show_security_group(security_group=group_id)
-        except neutron_exceptions.NeutronClientException as e:
-            LOG.exception('Failed to get remote security group')
+            return self.client.security_groups.get(group_id)
+        except nova_exceptions.ClientException as e:
+            LOG.exception(_('Failed to get remote security group'))
             raise exception.TroveError(str(e))
 
+    def create_security_group(self, name, description):
+        try:
+            sec_group = self.client.security_groups.create(
+                name=name, description=description)
+            return sec_group
+        except nova_exceptions.ClientException as e:
+            LOG.exception(_('Failed to create remote security group'))
+            raise exception.SecurityGroupCreationError(str(e))
+
     def delete_security_group(self, sec_group_id):
         try:
-            self.client.delete_security_group(security_group=sec_group_id)
-        except neutron_exceptions.NeutronClientException as e:
-            LOG.exception('Failed to delete remote security group')
+            self.client.security_groups.delete(sec_group_id)
+        except nova_exceptions.ClientException as e:
+            LOG.exception(_('Failed to delete remote security group'))
             raise exception.SecurityGroupDeletionError(str(e))
 
+    def add_security_group_rule(self, sec_group_id, protocol,
+                                from_port, to_port, cidr):
+        try:
+            sec_group_rule = self.client.security_group_rules.create(
+                parent_group_id=sec_group_id,
+                ip_protocol=protocol,
+                from_port=from_port,
+                to_port=to_port,
+                cidr=cidr)
+
+            return sec_group_rule
+        except nova_exceptions.ClientException as e:
+            LOG.exception(_('Failed to add rule to remote security group'))
+            raise exception.SecurityGroupRuleCreationError(str(e))
+
     def delete_security_group_rule(self, sec_group_rule_id):
         try:
-            self.client.delete_security_group_rule(
-                security_group_rule=sec_group_rule_id)
+            self.client.security_group_rules.delete(sec_group_rule_id)
 
-        except neutron_exceptions.NeutronClientException as e:
-            LOG.exception('Failed to delete rule to remote security group')
+        except nova_exceptions.ClientException as e:
+            LOG.exception(_('Failed to delete rule to remote security group'))
             raise exception.SecurityGroupRuleDeletionError(str(e))
```

### Comparing `trove-21.0.0.0rc2/trove/quota/models.py` & `trove-8.0.1/trove/quota/models.py`

 * *Files 7% similar despite different names*

```diff
@@ -25,16 +25,15 @@
     return type('Enum', (), enums)
 
 
 class Quota(dbmodels.DatabaseModelBase):
     """Defines the base model class for a quota."""
 
     _data_fields = ['created', 'updated', 'tenant_id', 'resource',
-                    'hard_limit']
-    _table_name = 'quotas'
+                    'hard_limit', 'id']
 
     def __init__(self, tenant_id, resource, hard_limit,
                  id=utils.generate_uuid(), created=timeutils.utcnow(),
                  update=timeutils.utcnow()):
         self.tenant_id = tenant_id
         self.resource = resource
         self.hard_limit = hard_limit
@@ -42,24 +41,23 @@
         self.created = created
         self.update = update
 
 
 class QuotaUsage(dbmodels.DatabaseModelBase):
     """Defines the quota usage for a tenant."""
 
-    _data_fields = ['created', 'updated', 'tenant_id',
-                    'in_use', 'reserved', 'resource']
-    _table_name = 'quota_usages'
+    _data_fields = ['created', 'updated', 'tenant_id', 'resource',
+                    'in_use', 'reserved', 'id']
 
 
 class Reservation(dbmodels.DatabaseModelBase):
     """Defines the reservation for a quota."""
 
-    _data_fields = ['created', 'updated', 'usage_id', 'delta', 'status']
-    _table_name = 'reservations'
+    _data_fields = ['created', 'updated', 'usage_id',
+                    'id', 'delta', 'status']
 
     Statuses = enum(NEW='New',
                     RESERVED='Reserved',
                     COMMITTED='Committed',
                     ROLLEDBACK='Rolled Back')
 
 
@@ -71,15 +69,14 @@
     }
 
 
 class Resource(object):
     """Describe a single resource for quota checking."""
 
     INSTANCES = 'instances'
-    RAM = 'ram'
     VOLUMES = 'volumes'
     BACKUPS = 'backups'
 
     def __init__(self, name, flag=None):
         """
         Initializes a Resource.
```

### Comparing `trove-21.0.0.0rc2/trove/quota/quota.py` & `trove-8.0.1/trove/quota/quota.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,16 +14,18 @@
 #    under the License.
 
 """Quotas for DB instances and resources."""
 
 from oslo_config import cfg
 from oslo_log import log as logging
 from oslo_utils import importutils
+import six
 
 from trove.common import exception
+from trove.common.i18n import _
 from trove.quota.models import Quota
 from trove.quota.models import QuotaUsage
 from trove.quota.models import Reservation
 from trove.quota.models import Resource
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
@@ -141,15 +143,14 @@
 
         quotas = self.get_all_quotas_by_tenant(tenant_id, deltas.keys())
         quota_usages = self.get_all_quota_usages_by_tenant(tenant_id,
                                                            deltas.keys())
 
         overs = [resource for resource in deltas
                  if (int(deltas[resource]) > 0 and
-                     quotas[resource].hard_limit >= 0 and
                      (quota_usages[resource].in_use +
                       quota_usages[resource].reserved +
                       int(deltas[resource])) > quotas[resource].hard_limit)]
 
         if overs:
             raise exception.QuotaExceeded(overs=sorted(overs))
 
@@ -225,15 +226,15 @@
     def __init__(self, quota_driver_class=None):
         """Initialize a Quota object."""
 
         self._resources = {}
 
         if not quota_driver_class:
             quota_driver_class = CONF.quota_driver
-        if isinstance(quota_driver_class, str):
+        if isinstance(quota_driver_class, six.string_types):
             quota_driver_class = importutils.import_object(quota_driver_class,
                                                            self._resources)
         self._driver = quota_driver_class
 
     def __contains__(self, resource):
         return resource in self._resources
 
@@ -319,54 +320,54 @@
         :param reservations: A list of the reservation UUIDs, as
                              returned by the reserve() method.
         """
 
         try:
             self._driver.commit(reservations)
         except Exception:
-            LOG.exception("Failed to commit reservations "
-                          "%(reservations)s", {'reservations': reservations})
+            LOG.exception(_("Failed to commit reservations "
+                          "%(reservations)s"), {'reservations': reservations})
 
     def rollback(self, reservations):
         """Roll back reservations.
 
         :param reservations: A list of the reservation UUIDs, as
                              returned by the reserve() method.
         """
 
         try:
             self._driver.rollback(reservations)
         except Exception:
-            LOG.exception("Failed to roll back reservations "
-                          "%(reservations)s", {'reservations': reservations})
+            LOG.exception(_("Failed to roll back reservations "
+                          "%(reservations)s"), {'reservations': reservations})
 
     @property
     def resources(self):
         return sorted(self._resources.keys())
 
 
 QUOTAS = QuotaEngine()
 
 ''' Define all kind of resources here '''
 
-resources = [Resource(Resource.INSTANCES, 'max_instances_per_tenant'),
-             Resource(Resource.RAM, 'max_ram_per_tenant'),
+resources = [Resource(Resource.INSTANCES,
+             'max_instances_per_tenant'),
              Resource(Resource.BACKUPS, 'max_backups_per_tenant'),
              Resource(Resource.VOLUMES, 'max_volumes_per_tenant')]
 
 QUOTAS.register_resources(resources)
 
 
-def run_with_quotas(tenant_id, deltas, f, *args, **kwargs):
+def run_with_quotas(tenant_id, deltas, f):
     """Quota wrapper."""
 
     reservations = QUOTAS.reserve(tenant_id, **deltas)
     result = None
     try:
-        result = f(*args, **kwargs)
+        result = f()
     except Exception:
         QUOTAS.rollback(reservations)
         raise
     else:
         QUOTAS.commit(reservations)
     return result
```

### Comparing `trove-21.0.0.0rc2/trove/rpc.py` & `trove-8.0.1/trove/rpc.py`

 * *Files 14% similar despite different names*

```diff
@@ -35,48 +35,40 @@
 
 import trove.common.exception
 from trove.common.rpc import secure_serializer as ssz
 from trove.common.rpc import serializer as sz
 
 CONF = cfg.CONF
 TRANSPORT = None
-NOTIFICATION_TRANSPORT = None
 NOTIFIER = None
 
 ALLOWED_EXMODS = [
     trove.common.exception.__name__,
 ]
 
 EXTRA_EXMODS = []
 
 
 def init(conf):
-    global TRANSPORT, NOTIFICATION_TRANSPORT, NOTIFIER
+    global TRANSPORT, NOTIFIER
     exmods = get_allowed_exmods()
-    TRANSPORT = messaging.get_rpc_transport(conf,
-                                            allowed_remote_exmods=exmods)
-
-    NOTIFICATION_TRANSPORT = messaging.get_notification_transport(
-        conf,
-        allowed_remote_exmods=exmods)
+    TRANSPORT = messaging.get_transport(conf,
+                                        allowed_remote_exmods=exmods)
 
     serializer = sz.TroveRequestContextSerializer(
         messaging.JsonPayloadSerializer())
-    NOTIFIER = messaging.Notifier(NOTIFICATION_TRANSPORT,
-                                  serializer=serializer)
+    NOTIFIER = messaging.Notifier(TRANSPORT, serializer=serializer)
 
 
 def cleanup():
-    global TRANSPORT, NOTIFICATION_TRANSPORT, NOTIFIER
+    global TRANSPORT, NOTIFIER
     assert TRANSPORT is not None
-    assert NOTIFICATION_TRANSPORT is not None
     assert NOTIFIER is not None
     TRANSPORT.cleanup()
-    NOTIFICATION_TRANSPORT.cleanup()
-    TRANSPORT = NOTIFICATION_TRANSPORT = NOTIFIER = None
+    TRANSPORT = NOTIFIER = None
 
 
 def set_defaults(control_exchange):
     messaging.set_transport_defaults(control_exchange)
 
 
 def add_extra_exmods(*args):
@@ -99,17 +91,18 @@
                secure_serializer=ssz.SecureSerializer):
     assert TRANSPORT is not None
     # BUG(1650518): Cleanup in the Pike release
     # uncomment this (following) line in the pike release
     # assert key is not None
     serializer = secure_serializer(
         sz.TroveRequestContextSerializer(serializer), key)
-    return messaging.get_rpc_client(
-        TRANSPORT, target, version_cap=version_cap,
-        serializer=serializer)
+    return messaging.RPCClient(TRANSPORT,
+                               target,
+                               version_cap=version_cap,
+                               serializer=serializer)
 
 
 def get_server(target, endpoints, key, serializer=None,
                secure_serializer=ssz.SecureSerializer):
     assert TRANSPORT is not None
 
     # Thread module is not monkeypatched if remote debugging is enabled.
@@ -128,15 +121,15 @@
 
     return messaging.get_rpc_server(
         TRANSPORT,
         target,
         endpoints,
         executor=executor,
         serializer=serializer,
-        access_policy=dispatcher.DefaultRPCAccessPolicy)
+        access_policy=dispatcher.LegacyRPCAccessPolicy)
 
 
 def get_notifier(service=None, host=None, publisher_id=None):
     assert NOTIFIER is not None
     if not publisher_id:
         publisher_id = "%s.%s" % (service, host or CONF.host)
     return NOTIFIER.prepare(publisher_id=publisher_id)
```

### Comparing `trove-21.0.0.0rc2/trove/taskmanager/api.py` & `trove-8.0.1/trove/taskmanager/api.py`

 * *Files 7% similar despite different names*

```diff
@@ -161,35 +161,21 @@
     def migrate(self, instance_id, host):
         LOG.debug("Making async call to migrate instance: %s", instance_id)
         version = self.API_BASE_VERSION
 
         self._cast("migrate", version=version,
                    instance_id=instance_id, host=host)
 
-    def rebuild(self, instance_id, image_id):
-        LOG.debug("Making async call to rebuild instance: %s", instance_id)
-        version = self.API_BASE_VERSION
-
-        self._cast("rebuild", version=version, instance_id=instance_id,
-                   image_id=image_id)
-
     def delete_instance(self, instance_id):
         LOG.debug("Making async call to delete instance: %s", instance_id)
         version = self.API_BASE_VERSION
 
         self._cast("delete_instance", version=version,
                    instance_id=instance_id)
 
-    def update_access(self, instance_id, access):
-        LOG.debug(f"Making async call to update instance: {instance_id}")
-        version = self.API_BASE_VERSION
-
-        self._cast("update_access", version=version,
-                   instance_id=instance_id, access=access)
-
     def create_backup(self, backup_info, instance_id):
         LOG.debug("Making async call to create a backup for instance: %s",
                   instance_id)
         version = self.API_BASE_VERSION
 
         self._cast("create_backup", version=version,
                    backup_info=backup_info,
@@ -203,16 +189,15 @@
 
     def create_instance(self, instance_id, name, flavor,
                         image_id, databases, users, datastore_manager,
                         packages, volume_size, backup_id=None,
                         availability_zone=None, root_password=None,
                         nics=None, overrides=None, slave_of_id=None,
                         cluster_config=None, volume_type=None,
-                        modules=None, locality=None, access=None,
-                        ds_version=None):
+                        modules=None, locality=None):
 
         LOG.debug("Making async call to create instance %s ", instance_id)
         version = self.API_BASE_VERSION
         self._cast("create_instance", version=version,
                    instance_id=instance_id, name=name,
                    flavor=self._transform_obj(flavor),
                    image_id=image_id,
@@ -225,16 +210,15 @@
                    availability_zone=availability_zone,
                    root_password=root_password,
                    nics=nics,
                    overrides=overrides,
                    slave_of_id=slave_of_id,
                    cluster_config=cluster_config,
                    volume_type=volume_type,
-                   modules=modules, locality=locality, access=access,
-                   ds_version=ds_version)
+                   modules=modules, locality=locality)
 
     def create_cluster(self, cluster_id):
         LOG.debug("Making async call to create cluster %s ", cluster_id)
         version = self.API_BASE_VERSION
 
         self._cast("create_cluster", version=version, cluster_id=cluster_id)
```

### Comparing `trove-21.0.0.0rc2/trove/taskmanager/manager.py` & `trove-8.0.1/trove/taskmanager/manager.py`

 * *Files 18% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 from oslo_log import log as logging
 from oslo_service import periodic_task
 from oslo_utils import importutils
 
 from trove.backup.models import Backup
 import trove.common.cfg as cfg
-from trove.common import clients
 from trove.common.context import TroveContext
 from trove.common import exception
 from trove.common.exception import ReplicationSlaveAttachError
 from trove.common.exception import TroveError
 from trove.common.i18n import _
 from trove.common.notification import DBaaSQuotas, EndNotification
+from trove.common import remote
 from trove.common import server_group as srv_grp
 from trove.common.strategies.cluster import strategy
 from trove.datastore.models import DatastoreVersion
 import trove.extensions.mgmt.instances.models as mgmtmodels
 from trove.instance.tasks import InstanceTasks
 from trove.taskmanager import models
 from trove.taskmanager.models import FreshInstanceTasks, BuiltInstanceTasks
@@ -40,17 +40,17 @@
 
 
 class Manager(periodic_task.PeriodicTasks):
 
     def __init__(self):
         super(Manager, self).__init__(CONF)
         self.admin_context = TroveContext(
-            user_id=CONF.service_credentials.username,
-            project_id=CONF.service_credentials.project_id,
-            user_domain_name=CONF.service_credentials.user_domain_name)
+            user=CONF.nova_proxy_admin_user,
+            auth_token=CONF.nova_proxy_admin_pass,
+            tenant=CONF.nova_proxy_admin_tenant_id)
         if CONF.exists_notification_transformer:
             self.exists_transformer = importutils.import_object(
                 CONF.exists_notification_transformer,
                 context=self.admin_context)
 
     def resize_volume(self, context, instance_id, new_size):
         with EndNotification(context):
@@ -95,105 +95,74 @@
         # be simply reassigned to a new master. See:
         # https://bugs.launchpad.net/trove/+bug/1553339
 
         def _promote_to_replica_source(old_master, master_candidate,
                                        replica_models):
             # First, we transition from the old master to new as quickly as
             # possible to minimize the scope of unrecoverable error
-
-            # NOTE(zhaochao): we cannot reattach the old master to the new
-            # one immediately after the new master is up, because for MariaDB
-            # the other replicas are still connecting to the old master, and
-            # during reattaching the old master as a slave, new GTID may be
-            # created and synced to the replicas. After that, when attaching
-            # the replicas to the new master, 'START SLAVE' will fail by
-            # 'fatal error 1236' if the binlog of the replica diverged from
-            # the new master. So the proper order should be:
-            # -1. make the old master read only (and detach floating ips)
-            # -2. make sure the new master is up-to-date
-            # -3. detach the new master from the old one
-            # -4. enable the new master (and attach floating ips)
-            # -5. attach the other replicas to the new master
-            # -6. attach the old master to the new one
-            #     (and attach floating ips)
-            # -7. demote the old master
-            # What we changed here is the order of the 6th step, previously
-            # this step took place right after step 4, which causes failures
-            # with MariaDB replications.
             old_master.make_read_only(True)
+            master_ips = old_master.detach_public_ips()
+            slave_ips = master_candidate.detach_public_ips()
             latest_txn_id = old_master.get_latest_txn_id()
             master_candidate.wait_for_txn(latest_txn_id)
             master_candidate.detach_replica(old_master, for_failover=True)
             master_candidate.enable_as_master()
+            old_master.attach_replica(master_candidate)
+            master_candidate.attach_public_ips(master_ips)
             master_candidate.make_read_only(False)
+            old_master.attach_public_ips(slave_ips)
 
             # At this point, should something go wrong, there
             # should be a working master with some number of working slaves,
             # and possibly some number of "orphaned" slaves
 
             exception_replicas = []
             error_messages = ""
             for replica in replica_models:
                 try:
                     if replica.id != master_candidate.id:
                         replica.detach_replica(old_master, for_failover=True)
-                        replica.attach_replica(master_candidate, restart=True)
+                        replica.attach_replica(master_candidate)
                 except exception.TroveError as ex:
-                    log_fmt = ("Unable to migrate replica %(slave)s from "
-                               "old replica source %(old_master)s to "
-                               "new source %(new_master)s on promote.")
-                    exc_fmt = _("Unable to migrate replica %(slave)s from "
-                                "old replica source %(old_master)s to "
-                                "new source %(new_master)s on promote.")
-                    msg_content = {
-                        "slave": replica.id,
-                        "old_master": old_master.id,
-                        "new_master": master_candidate.id}
-                    LOG.error(log_fmt, msg_content)
-
+                    msg = (_("Unable to migrate replica %(slave)s from "
+                             "old replica source %(old_master)s to "
+                             "new source %(new_master)s on promote.") %
+                           {"slave": replica.id,
+                            "old_master": old_master.id,
+                            "new_master": master_candidate.id})
+                    LOG.exception(msg)
                     exception_replicas.append(replica)
-                    error_messages += "%s (%s)\n" % (
-                        exc_fmt % msg_content, ex)
+                    error_messages += "%s (%s)\n" % (msg, ex)
 
-            # dealing with the old master after all the other replicas
-            # has been migrated.
-            old_master.attach_replica(master_candidate, restart=False)
             try:
                 old_master.demote_replication_master()
             except Exception as ex:
-                log_fmt = "Exception demoting old replica source %s."
-                exc_fmt = _("Exception demoting old replica source %s.")
-                LOG.error(log_fmt, old_master.id)
+                msg = (_("Exception demoting old replica source %s.") %
+                       old_master.id)
+                LOG.exception(msg)
                 exception_replicas.append(old_master)
-                error_messages += "%s (%s)\n" % (
-                    exc_fmt % old_master.id, ex)
+                error_messages += "%s (%s)\n" % (msg, ex)
 
             self._set_task_status([old_master] + replica_models,
                                   InstanceTasks.NONE)
-
             if exception_replicas:
                 self._set_task_status(exception_replicas,
                                       InstanceTasks.PROMOTION_ERROR)
                 msg = (_("promote-to-replica-source %(id)s: The following "
                          "replicas may not have been switched: %(replicas)s:"
                          "\n%(err)s") %
                        {"id": master_candidate.id,
                         "replicas": [repl.id for repl in exception_replicas],
                         "err": error_messages})
                 raise ReplicationSlaveAttachError(msg)
 
-            LOG.info('Finished to promote %s as master.', instance_id)
-
         with EndNotification(context):
-            LOG.info('Promoting %s as replication master', instance_id)
-
             master_candidate = BuiltInstanceTasks.load(context, instance_id)
             old_master = BuiltInstanceTasks.load(context,
                                                  master_candidate.slave_of_id)
-
             replicas = []
             for replica_dbinfo in old_master.slaves:
                 if replica_dbinfo.id == instance_id:
                     replica = master_candidate
                 else:
                     replica = BuiltInstanceTasks.load(context,
                                                       replica_dbinfo.id)
@@ -210,71 +179,67 @@
                 raise
 
     # pulled out to facilitate testing
     def _get_replica_txns(self, replica_models):
         return [[repl] + repl.get_last_txn() for repl in replica_models]
 
     def _most_current_replica(self, old_master, replica_models):
-        # last_txns is [instance, master UUID, last txn]
         last_txns = self._get_replica_txns(replica_models)
         master_ids = [txn[1] for txn in last_txns if txn[1]]
         if len(set(master_ids)) > 1:
             raise TroveError(_("Replicas of %s not all replicating"
                                " from same master") % old_master.id)
         return sorted(last_txns, key=lambda x: x[2], reverse=True)[0][0]
 
     def eject_replica_source(self, context, instance_id):
 
         def _eject_replica_source(old_master, replica_models):
 
             master_candidate = self._most_current_replica(old_master,
                                                           replica_models)
-            LOG.info('New master selected: %s', master_candidate.id)
 
+            master_ips = old_master.detach_public_ips()
+            slave_ips = master_candidate.detach_public_ips()
             master_candidate.detach_replica(old_master, for_failover=True)
             master_candidate.enable_as_master()
+            master_candidate.attach_public_ips(master_ips)
             master_candidate.make_read_only(False)
+            old_master.attach_public_ips(slave_ips)
 
             exception_replicas = []
             error_messages = ""
             for replica in replica_models:
                 try:
                     if replica.id != master_candidate.id:
                         replica.detach_replica(old_master, for_failover=True)
                         replica.attach_replica(master_candidate)
                 except exception.TroveError as ex:
-                    log_fmt = ("Unable to migrate replica %(slave)s from "
-                               "old replica source %(old_master)s to "
-                               "new source %(new_master)s on eject.")
-                    exc_fmt = _("Unable to migrate replica %(slave)s from "
-                                "old replica source %(old_master)s to "
-                                "new source %(new_master)s on eject.")
-                    msg_content = {
-                        "slave": replica.id,
-                        "old_master": old_master.id,
-                        "new_master": master_candidate.id}
-                    LOG.error(log_fmt, msg_content)
+                    msg = (_("Unable to migrate replica %(slave)s from "
+                             "old replica source %(old_master)s to "
+                             "new source %(new_master)s on eject.") %
+                           {"slave": replica.id,
+                            "old_master": old_master.id,
+                            "new_master": master_candidate.id})
+                    LOG.exception(msg)
                     exception_replicas.append(replica)
-                    error_messages += "%s (%s)\n" % (exc_fmt % msg_content, ex)
+                    error_messages += "%s (%s)\n" % (msg, ex)
 
             self._set_task_status([old_master] + replica_models,
                                   InstanceTasks.NONE)
             if exception_replicas:
                 self._set_task_status(exception_replicas,
                                       InstanceTasks.EJECTION_ERROR)
                 msg = (_("eject-replica-source %(id)s: The following "
                          "replicas may not have been switched: %(replicas)s:"
                          "\n%(err)s") %
                        {"id": master_candidate.id,
                         "replicas": [repl.id for repl in exception_replicas],
                         "err": error_messages})
                 raise ReplicationSlaveAttachError(msg)
 
-            LOG.info('New master enabled: %s', master_candidate.id)
-
         with EndNotification(context):
             master = BuiltInstanceTasks.load(context, instance_id)
             replicas = [BuiltInstanceTasks.load(context, dbinfo.id)
                         for dbinfo in master.slaves]
             try:
                 _eject_replica_source(master, replicas)
             except ReplicationSlaveAttachError:
@@ -286,18 +251,14 @@
 
     def migrate(self, context, instance_id, host):
         with EndNotification(context):
             instance_tasks = models.BuiltInstanceTasks.load(context,
                                                             instance_id)
             instance_tasks.migrate(host)
 
-    def rebuild(self, context, instance_id, image_id):
-        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
-        instance_tasks.rebuild(image_id)
-
     def delete_instance(self, context, instance_id):
         with EndNotification(context):
             try:
                 instance_tasks = models.BuiltInstanceTasks.load(context,
                                                                 instance_id)
                 instance_tasks.delete_async()
             except exception.UnprocessableEntity:
@@ -316,177 +277,134 @@
             instance_tasks.create_backup(backup_info)
 
     def _create_replication_slave(self, context, instance_id, name, flavor,
                                   image_id, databases, users,
                                   datastore_manager, packages, volume_size,
                                   availability_zone, root_password, nics,
                                   overrides, slave_of_id, backup_id,
-                                  volume_type, modules, access=None,
-                                  ds_version=None):
+                                  volume_type, modules):
 
         if type(instance_id) in [list]:
             ids = instance_id
             root_passwords = root_password
         else:
             ids = [instance_id]
             root_passwords = [root_password]
         replica_number = 0
         replica_backup_id = backup_id
+        replica_backup_created = False
         replicas = []
 
         master_instance_tasks = BuiltInstanceTasks.load(context, slave_of_id)
         server_group = master_instance_tasks.server_group
         scheduler_hints = srv_grp.ServerGroup.convert_to_hint(server_group)
-        LOG.debug("Using scheduler hints %s for creating instance %s",
-                  scheduler_hints, instance_id)
+        LOG.debug("Using scheduler hints for locality: %s", scheduler_hints)
 
-        # Create backup for master
-        snapshot = None
-        try:
-            instance_tasks = FreshInstanceTasks.load(context, ids[0])
-            snapshot = instance_tasks.get_replication_master_snapshot(
-                context, slave_of_id, flavor,
-                parent_backup_id=replica_backup_id)
-            LOG.info('Snapshot info for creating replica of %s: %s',
-                     slave_of_id, snapshot)
-        except Exception as err:
-            LOG.error('Failed to get master snapshot info for creating '
-                      'replica, error: %s', str(err))
-
-            if snapshot and snapshot.get('dataset', {}).get('snapshot_id'):
-                backup_id = snapshot['dataset']['snapshot_id']
-                Backup.delete(context, backup_id)
-
-            raise
-
-        # Create replicas using the master backup
-        replica_backup_id = snapshot['dataset']['snapshot_id']
         try:
             for replica_index in range(0, len(ids)):
-                replica_number += 1
-                LOG.info(f"Creating replica {replica_number} "
-                         f"({ids[replica_index]}) of {len(ids)}.")
-
-                instance_tasks = FreshInstanceTasks.load(
-                    context, ids[replica_index])
-                instance_tasks.create_instance(
-                    flavor, image_id, databases, users, datastore_manager,
-                    packages, volume_size, replica_backup_id,
-                    availability_zone, root_passwords[replica_index],
-                    nics, overrides, None, snapshot, volume_type,
-                    modules, scheduler_hints, access=access,
-                    ds_version=ds_version)
-                replicas.append(instance_tasks)
+                try:
+                    replica_number += 1
+                    LOG.debug("Creating replica %(num)d of %(count)d.",
+                              {'num': replica_number, 'count': len(ids)})
+                    instance_tasks = FreshInstanceTasks.load(
+                        context, ids[replica_index])
+                    snapshot = instance_tasks.get_replication_master_snapshot(
+                        context, slave_of_id, flavor, replica_backup_id,
+                        replica_number=replica_number)
+                    replica_backup_id = snapshot['dataset']['snapshot_id']
+                    replica_backup_created = (replica_backup_id is not None)
+                    instance_tasks.create_instance(
+                        flavor, image_id, databases, users, datastore_manager,
+                        packages, volume_size, replica_backup_id,
+                        availability_zone, root_passwords[replica_index],
+                        nics, overrides, None, snapshot, volume_type,
+                        modules, scheduler_hints)
+                    replicas.append(instance_tasks)
+                except Exception:
+                    # if it's the first replica, then we shouldn't continue
+                    LOG.exception(_(
+                        "Could not create replica %(num)d of %(count)d."),
+                        {'num': replica_number, 'count': len(ids)})
+                    if replica_number == 1:
+                        raise
 
             for replica in replicas:
                 replica.wait_for_instance(CONF.restore_usage_timeout, flavor)
-        except Exception as err:
-            LOG.error('Failed to create replica from %s, error: %s',
-                      slave_of_id, str(err))
-            raise
+
         finally:
-            Backup.delete(context, replica_backup_id)
+            if replica_backup_created:
+                Backup.delete(context, replica_backup_id)
 
     def _create_instance(self, context, instance_id, name, flavor,
                          image_id, databases, users, datastore_manager,
                          packages, volume_size, backup_id, availability_zone,
                          root_password, nics, overrides, slave_of_id,
-                         cluster_config, volume_type, modules, locality,
-                         access=None, ds_version=None):
+                         cluster_config, volume_type, modules, locality):
         if slave_of_id:
             self._create_replication_slave(context, instance_id, name,
                                            flavor, image_id, databases, users,
                                            datastore_manager, packages,
                                            volume_size,
                                            availability_zone, root_password,
                                            nics, overrides, slave_of_id,
-                                           backup_id, volume_type, modules,
-                                           access=access,
-                                           ds_version=ds_version)
+                                           backup_id, volume_type, modules)
         else:
             if type(instance_id) in [list]:
                 raise AttributeError(_(
                     "Cannot create multiple non-replica instances."))
-
-            scheduler_hints = srv_grp.ServerGroup.build_scheduler_hint(
-                context, locality, instance_id
-            )
-            LOG.debug("Using scheduler hints %s for creating instance %s",
-                      scheduler_hints, instance_id)
-
             instance_tasks = FreshInstanceTasks.load(context, instance_id)
-            instance_tasks.create_instance(
-                flavor, image_id, databases, users,
-                datastore_manager, packages,
-                volume_size, backup_id,
-                availability_zone, root_password,
-                nics, overrides, cluster_config,
-                None, volume_type, modules,
-                scheduler_hints, access=access, ds_version=ds_version
-            )
 
+            scheduler_hints = srv_grp.ServerGroup.build_scheduler_hint(
+                context, locality, instance_id)
+            instance_tasks.create_instance(flavor, image_id, databases, users,
+                                           datastore_manager, packages,
+                                           volume_size, backup_id,
+                                           availability_zone, root_password,
+                                           nics, overrides, cluster_config,
+                                           None, volume_type, modules,
+                                           scheduler_hints)
             timeout = (CONF.restore_usage_timeout if backup_id
                        else CONF.usage_timeout)
             instance_tasks.wait_for_instance(timeout, flavor)
 
     def create_instance(self, context, instance_id, name, flavor,
                         image_id, databases, users, datastore_manager,
                         packages, volume_size, backup_id, availability_zone,
                         root_password, nics, overrides, slave_of_id,
-                        cluster_config, volume_type, modules, locality,
-                        access=None, ds_version=None):
-        with EndNotification(
-            context,
-            instance_id=(
-                instance_id[0]
-                if isinstance(instance_id, list)
-                else instance_id
-            )
-        ):
+                        cluster_config, volume_type, modules, locality):
+        with EndNotification(context,
+                             instance_id=(instance_id[0]
+                                          if isinstance(instance_id, list)
+                                          else instance_id)):
             self._create_instance(context, instance_id, name, flavor,
                                   image_id, databases, users,
                                   datastore_manager, packages, volume_size,
                                   backup_id, availability_zone,
                                   root_password, nics, overrides, slave_of_id,
                                   cluster_config, volume_type, modules,
-                                  locality, access=access,
-                                  ds_version=ds_version)
+                                  locality)
 
     def upgrade(self, context, instance_id, datastore_version_id):
         instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
         datastore_version = DatastoreVersion.load_by_uuid(datastore_version_id)
         with EndNotification(context):
             instance_tasks.upgrade(datastore_version)
 
-    def update_access(self, context, instance_id, access):
-        instance_tasks = models.BuiltInstanceTasks.load(context, instance_id)
-
-        try:
-            instance_tasks.update_access(access)
-        except Exception as e:
-            LOG.error(f"Failed to update access configuration for "
-                      f"{instance_id}: {str(e)}")
-            self.update_db(task_status=InstanceTasks.UPDATING_ERROR_ACCESS)
-
     def create_cluster(self, context, cluster_id):
         with EndNotification(context, cluster_id=cluster_id):
             cluster_tasks = models.load_cluster_tasks(context, cluster_id)
             cluster_tasks.create_cluster(context, cluster_id)
 
     def grow_cluster(self, context, cluster_id, new_instance_ids):
-        with EndNotification(context, cluster_id=cluster_id,
-                             instance_ids=new_instance_ids):
-            cluster_tasks = models.load_cluster_tasks(context, cluster_id)
-            cluster_tasks.grow_cluster(context, cluster_id, new_instance_ids)
+        cluster_tasks = models.load_cluster_tasks(context, cluster_id)
+        cluster_tasks.grow_cluster(context, cluster_id, new_instance_ids)
 
     def shrink_cluster(self, context, cluster_id, instance_ids):
-        with EndNotification(context, cluster_id=cluster_id,
-                             instance_ids=instance_ids):
-            cluster_tasks = models.load_cluster_tasks(context, cluster_id)
-            cluster_tasks.shrink_cluster(context, cluster_id, instance_ids)
+        cluster_tasks = models.load_cluster_tasks(context, cluster_id)
+        cluster_tasks.shrink_cluster(context, cluster_id, instance_ids)
 
     def restart_cluster(self, context, cluster_id):
         cluster_tasks = models.load_cluster_tasks(context, cluster_id)
         cluster_tasks.restart_cluster(context, cluster_id)
 
     def upgrade_cluster(self, context, cluster_id, datastore_version_id):
         datastore_version = DatastoreVersion.load_by_uuid(datastore_version_id)
@@ -513,15 +431,15 @@
             """
             mgmtmodels.publish_exist_events(self.exists_transformer,
                                             self.admin_context)
 
     if CONF.quota_notification_interval:
         @periodic_task.periodic_task(spacing=CONF.quota_notification_interval)
         def publish_quota_notifications(self, context):
-            nova_client = clients.create_nova_client(self.admin_context)
+            nova_client = remote.create_nova_client(self.admin_context)
             for tenant in nova_client.tenants.list():
                 for quota in QUOTAS.get_all_quotas_by_tenant(tenant.id):
                     usage = QUOTAS.get_quota_usage(quota)
                     DBaaSQuotas(self.admin_context, quota, usage).notify()
 
     def __getattr__(self, name):
         """
```

### Comparing `trove-21.0.0.0rc2/trove/taskmanager/models.py` & `trove-8.0.1/trove/taskmanager/models.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,89 +8,104 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-import copy
-import json
+import os.path
 import time
 import traceback
 
+from cinderclient import exceptions as cinder_exceptions
 from eventlet import greenthread
 from eventlet.timeout import Timeout
+from novaclient import exceptions as nova_exceptions
 from oslo_log import log as logging
 from swiftclient.client import ClientException
 
 from trove.backup import models as bkup_models
 from trove.backup.models import Backup
 from trove.backup.models import DBBackup
 from trove.backup.state import BackupState
 from trove.cluster.models import Cluster
 from trove.cluster.models import DBCluster
 from trove.cluster import tasks
 from trove.common import cfg
-from trove.common import clients
-from trove.common.clients import create_cinder_client
-from trove.common.clients import create_dns_client
-from trove.common.clients import create_guest_client
-from trove.common import constants
+from trove.common import crypto_utils as cu
 from trove.common import exception
 from trove.common.exception import BackupCreationError
 from trove.common.exception import GuestError
 from trove.common.exception import GuestTimeout
 from trove.common.exception import InvalidModelError
+from trove.common.exception import MalformedSecurityGroupRuleError
 from trove.common.exception import PollTimeOut
 from trove.common.exception import TroveError
 from trove.common.exception import VolumeCreationFailure
 from trove.common.i18n import _
-from trove.common import neutron
-from trove.common.notification import DBaaSInstanceRestart
-from trove.common.notification import DBaaSInstanceUpgrade
-from trove.common.notification import EndNotification
-from trove.common.notification import StartNotification
-from trove.common.notification import TroveInstanceCreate
-from trove.common.notification import TroveInstanceModifyFlavor
+from trove.common import instance as rd_instance
+from trove.common.instance import ServiceStatuses
+from trove.common.notification import (
+    DBaaSInstanceRestart,
+    DBaaSInstanceUpgrade,
+    EndNotification,
+    StartNotification,
+    TroveInstanceCreate,
+    TroveInstanceModifyVolume,
+    TroveInstanceModifyFlavor,
+    TroveInstanceDelete)
+import trove.common.remote as remote
+from trove.common.remote import create_cinder_client
+from trove.common.remote import create_dns_client
+from trove.common.remote import create_guest_client
+from trove.common import server_group as srv_grp
 from trove.common.strategies.cluster import strategy
 from trove.common import template
 from trove.common import timeutils
 from trove.common import utils
 from trove.common.utils import try_recover
-from trove.configuration import models as config_models
 from trove.extensions.mysql import models as mysql_models
+from trove.extensions.security_group.models import SecurityGroup
+from trove.extensions.security_group.models import SecurityGroupRule
 from trove.instance import models as inst_models
+from trove.instance.models import BuiltInstance
 from trove.instance.models import DBInstance
 from trove.instance.models import FreshInstance
 from trove.instance.models import Instance
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import InstanceStatus
-from trove.instance import service_status as srvstatus
 from trove.instance.tasks import InstanceTasks
 from trove.module import models as module_models
 from trove.module import views as module_views
 from trove.quota.quota import run_with_quotas
 from trove import rpc
 
 LOG = logging.getLogger(__name__)
 CONF = cfg.CONF
+VOLUME_TIME_OUT = CONF.volume_time_out  # seconds.
+DNS_TIME_OUT = CONF.dns_time_out  # seconds.
+RESIZE_TIME_OUT = CONF.resize_time_out  # seconds.
+REVERT_TIME_OUT = CONF.revert_time_out  # seconds.
+USAGE_SLEEP_TIME = CONF.usage_sleep_time  # seconds.
+
+use_nova_server_volume = CONF.use_nova_server_volume
 
 
 class NotifyMixin(object):
     """Notification Mixin
 
     This adds the ability to send usage events to an Instance object.
     """
 
     def _get_service_id(self, datastore_manager, id_map):
         if datastore_manager in id_map:
             datastore_manager_id = id_map[datastore_manager]
         else:
             datastore_manager_id = cfg.UNKNOWN_SERVICE_ID
-            LOG.error("Datastore ID for Manager (%s) is not configured",
+            LOG.error(_("Datastore ID for Manager (%s) is not configured"),
                       datastore_manager)
         return datastore_manager_id
 
     def send_usage_event(self, event_type, **kwargs):
         event_type = 'trove.instance.%s' % event_type
         publisher_id = CONF.host
         # Grab the instance size from the kwargs or from the nova client
@@ -114,15 +129,15 @@
             'instance_type_id': flavor.id,
             'launched_at': created_time,
             'nova_instance_id': self.server_id,
             'region': CONF.region,
             'state_description': self.status,
             'state': self.status,
             'tenant_id': self.tenant_id,
-            'user_id': self.context.user_id,
+            'user_id': self.context.user,
         }
 
         if CONF.get(self.datastore_version.manager).volume_support:
             payload.update({
                 'volume_size': self.volume_size,
                 'nova_volume_id': self.volume_id
             })
@@ -189,85 +204,57 @@
             for db_instance in db_instances:
                 db_instance.set_task_status(
                     status or InstanceTasks.BUILDING_ERROR_SERVER)
                 db_instance.save()
 
     @classmethod
     def get_ip(cls, instance):
-        return instance.get_visible_ip_addresses()[0].get('address')
+        return instance.get_visible_ip_addresses()[0]
 
     def _all_instances_ready(self, instance_ids, cluster_id,
                              shard_id=None):
         """Wait for all instances to get READY."""
         return self._all_instances_acquire_status(
-            instance_ids, cluster_id, shard_id,
-            srvstatus.ServiceStatuses.INSTANCE_READY,
-            fast_fail_statuses=[
-                srvstatus.ServiceStatuses.FAILED,
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT
-            ]
-        )
+            instance_ids, cluster_id, shard_id, ServiceStatuses.INSTANCE_READY,
+            fast_fail_statuses=[ServiceStatuses.FAILED,
+                                ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT])
 
     def _all_instances_shutdown(self, instance_ids, cluster_id,
                                 shard_id=None):
         """Wait for all instances to go SHUTDOWN."""
         return self._all_instances_acquire_status(
-            instance_ids, cluster_id, shard_id,
-            srvstatus.ServiceStatuses.SHUTDOWN,
-            fast_fail_statuses=[
-                srvstatus.ServiceStatuses.FAILED,
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT
-            ]
-        )
+            instance_ids, cluster_id, shard_id, ServiceStatuses.SHUTDOWN,
+            fast_fail_statuses=[ServiceStatuses.FAILED,
+                                ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT])
 
     def _all_instances_running(self, instance_ids, cluster_id, shard_id=None):
         """Wait for all instances to become ACTIVE."""
         return self._all_instances_acquire_status(
-            instance_ids, cluster_id, shard_id,
-            srvstatus.ServiceStatuses.RUNNING,
-            fast_fail_statuses=[
-                srvstatus.ServiceStatuses.FAILED,
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT
-            ]
-        )
-
-    def _all_instances_healthy(self, instance_ids, cluster_id, shard_id=None):
-        """Wait for all instances to become HEALTHY."""
-        return self._all_instances_acquire_status(
-            instance_ids, cluster_id, shard_id,
-            srvstatus.ServiceStatuses.HEALTHY,
-            fast_fail_statuses=[
-                srvstatus.ServiceStatuses.FAILED,
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT
-            ]
-        )
+            instance_ids, cluster_id, shard_id, ServiceStatuses.RUNNING,
+            fast_fail_statuses=[ServiceStatuses.FAILED,
+                                ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT])
 
     def _all_instances_acquire_status(
             self, instance_ids, cluster_id, shard_id, expected_status,
             fast_fail_statuses=None):
 
         def _is_fast_fail_status(status):
             return ((fast_fail_statuses is not None) and
                     ((status == fast_fail_statuses) or
                      (status in fast_fail_statuses)))
 
         def _all_have_status(ids):
             for instance_id in ids:
                 status = InstanceServiceStatus.find_by(
                     instance_id=instance_id).get_status()
-                task_status = DBInstance.find_by(
-                    id=instance_id).get_task_status()
-                if (_is_fast_fail_status(status) or
-                        (task_status == InstanceTasks.BUILDING_ERROR_SERVER)):
+                if _is_fast_fail_status(status):
                     # if one has failed, no need to continue polling
                     LOG.debug("Instance %(id)s has acquired a fast-fail "
-                              "status %(status)s and"
-                              " task_status %(task_status)s.",
-                              {'id': instance_id, 'status': status,
-                               'task_status': task_status})
+                              "status %(status)s.", {'id': instance_id,
+                                                     'status': status})
                     return True
                 if status != expected_status:
                     # if one is not in the expected state, continue polling
                     LOG.debug("Instance %(id)s was %(status)s.",
                               {'id': instance_id, 'status': status})
                     return False
 
@@ -275,38 +262,35 @@
 
         def _instance_ids_with_failures(ids):
             LOG.debug("Checking for service failures on instances: %s", ids)
             failed_instance_ids = []
             for instance_id in ids:
                 status = InstanceServiceStatus.find_by(
                     instance_id=instance_id).get_status()
-                task_status = DBInstance.find_by(
-                    id=instance_id).get_task_status()
-                if (_is_fast_fail_status(status) or
-                        (task_status == InstanceTasks.BUILDING_ERROR_SERVER)):
+                if _is_fast_fail_status(status):
                     failed_instance_ids.append(instance_id)
             return failed_instance_ids
 
         LOG.debug("Polling until all instances acquire %(expected)s "
                   "status: %(ids)s",
                   {'expected': expected_status, 'ids': instance_ids})
         try:
             utils.poll_until(lambda: instance_ids,
                              lambda ids: _all_have_status(ids),
-                             sleep_time=CONF.usage_sleep_time,
+                             sleep_time=USAGE_SLEEP_TIME,
                              time_out=CONF.usage_timeout)
         except PollTimeOut:
-            LOG.exception("Timed out while waiting for all instances "
-                          "to become %s.", expected_status)
+            LOG.exception(_("Timed out while waiting for all instances "
+                            "to become %s."), expected_status)
             self.update_statuses_on_failure(cluster_id, shard_id)
             return False
 
         failed_ids = _instance_ids_with_failures(instance_ids)
         if failed_ids:
-            LOG.error("Some instances failed: %s", failed_ids)
+            LOG.error(_("Some instances failed: %s"), failed_ids)
             self.update_statuses_on_failure(cluster_id, shard_id)
             return False
 
         LOG.debug("All instances have acquired the expected status %s.",
                   expected_status)
 
         return True
@@ -321,15 +305,15 @@
             return len(db_instances) == 0
 
         try:
             utils.poll_until(all_instances_marked_deleted,
                              sleep_time=2,
                              time_out=CONF.cluster_delete_time_out)
         except PollTimeOut:
-            LOG.error("timeout for instances to be marked as deleted.")
+            LOG.error(_("timeout for instances to be marked as deleted."))
             return
 
         LOG.debug("setting cluster %s as deleted.", cluster_id)
         cluster = DBCluster.find_by(id=cluster_id)
         cluster.deleted = True
         cluster.deleted_at = timeutils.utcnow()
         cluster.task_status = tasks.ClusterTasks.NONE
@@ -348,41 +332,39 @@
                     instance.update_db(task_status=InstanceTasks.REBOOTING)
                     instance.restart()
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         cluster_notification = context.notification
         request_info = cluster_notification.serialize(context)
         try:
-            node_db_inst = DBInstance.find_all(cluster_id=cluster_id,
-                                               deleted=False).all()
+            node_db_inst = DBInstance.find_all(cluster_id=cluster_id).all()
             for index, db_inst in enumerate(node_db_inst):
                 if index > 0:
                     LOG.debug(
                         "Waiting (%ds) for restarted nodes to rejoin the "
                         "cluster before proceeding.", delay_sec)
                     time.sleep(delay_sec)
                 instance = BuiltInstanceTasks.load(context, db_inst.id)
                 _restart_cluster_instance(instance)
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for restarting cluster.")
+            LOG.exception(_("Timeout for restarting cluster."))
             raise
         except Exception:
-            LOG.exception("Error restarting cluster.", cluster_id)
+            LOG.exception(_("Error restarting cluster."), cluster_id)
             raise
         finally:
             context.notification = cluster_notification
             timeout.cancel()
             self.reset_task()
 
         LOG.debug("End rolling restart for id: %s.", cluster_id)
 
-    def rolling_upgrade_cluster(self, context, cluster_id,
-                                datastore_version, ordering_function=None):
+    def rolling_upgrade_cluster(self, context, cluster_id, datastore_version):
         LOG.debug("Begin rolling cluster upgrade for id: %s.", cluster_id)
 
         def _upgrade_cluster_instance(instance):
             LOG.debug("Upgrading instance with id: %s.", instance.id)
             context.notification = (
                 DBaaSInstanceUpgrade(context, **request_info))
             with StartNotification(
@@ -394,739 +376,569 @@
                         task_status=InstanceTasks.UPGRADING)
                     instance.upgrade(datastore_version)
 
         timeout = Timeout(CONF.cluster_usage_timeout)
         cluster_notification = context.notification
         request_info = cluster_notification.serialize(context)
         try:
-            instances = []
-            for db_inst in DBInstance.find_all(cluster_id=cluster_id,
-                                               deleted=False).all():
+            for db_inst in DBInstance.find_all(cluster_id=cluster_id).all():
                 instance = BuiltInstanceTasks.load(
                     context, db_inst.id)
-                instances.append(instance)
-
-            if ordering_function is not None:
-                instances.sort(key=ordering_function)
-
-            for instance in instances:
                 _upgrade_cluster_instance(instance)
 
             self.reset_task()
         except Timeout as t:
             if t is not timeout:
                 raise  # not my timeout
-            LOG.exception("Timeout for upgrading cluster.")
+            LOG.exception(_("Timeout for upgrading cluster."))
             self.update_statuses_on_failure(
                 cluster_id, status=InstanceTasks.UPGRADING_ERROR)
         except Exception:
-            LOG.exception("Error upgrading cluster %s.", cluster_id)
+            LOG.exception(_("Error upgrading cluster %s."), cluster_id)
             self.update_statuses_on_failure(
                 cluster_id, status=InstanceTasks.UPGRADING_ERROR)
         finally:
             context.notification = cluster_notification
             timeout.cancel()
 
         LOG.debug("End upgrade_cluster for id: %s.", cluster_id)
 
 
 class FreshInstanceTasks(FreshInstance, NotifyMixin, ConfigurationMixin):
-    """
-    FreshInstanceTasks contains the tasks related an instance that not
-    associated with a compute server.
-    """
+
+    def _delete_resources(self, deleted_at):
+        LOG.debug("Begin _delete_resources for instance %s", self.id)
+
+        # If volume has "available" status, delete it manually.
+        try:
+            if self.volume_id:
+                volume_client = create_cinder_client(self.context)
+                volume = volume_client.volumes.get(self.volume_id)
+                if volume.status == "available":
+                    LOG.info(_("Deleting volume %(v)s for instance: %(i)s."),
+                             {'v': self.volume_id, 'i': self.id})
+                    volume.delete()
+        except Exception:
+            LOG.exception(_("Error deleting volume of instance %(id)s."),
+                          {'id': self.db_info.id})
+
+        LOG.debug("End _delete_resource for instance %s", self.id)
 
     def wait_for_instance(self, timeout, flavor):
         # Make sure the service becomes active before sending a usage
         # record to avoid over billing a customer for an instance that
         # fails to build properly.
         error_message = ''
         error_details = ''
         try:
-            LOG.info("Waiting for instance %s up and running with "
-                     "timeout %ss", self.id, timeout)
             utils.poll_until(self._service_is_active,
-                             sleep_time=CONF.usage_sleep_time,
+                             sleep_time=USAGE_SLEEP_TIME,
                              time_out=timeout)
-
-            LOG.info("Created instance %s successfully.", self.id)
-            if not self.db_info.task_status.is_error:
-                self.reset_task_status()
+            LOG.info(_("Created instance %s successfully."), self.id)
             TroveInstanceCreate(instance=self,
                                 instance_size=flavor['ram']).notify()
-        except exception.ComputeInstanceNotFound:
-            # Check if the instance has been deleted by another request.
-            instance = DBInstance.find_by(id=self.id)
-            if (instance.deleted or
-                    instance.task_status == InstanceTasks.DELETING):
-                LOG.warning(f"Instance {self.id} has been deleted during "
-                            f"waiting for creation")
-                return
-        except (TroveError, PollTimeOut) as ex:
-            LOG.error("Failed to create instance %s, error: %s.",
-                      self.id, str(ex))
+        except PollTimeOut as ex:
+            LOG.error(_("Failed to create instance %s. "
+                        "Timeout waiting for instance to become active. "
+                        "No usage create-event was sent."), self.id)
             self.update_statuses_on_time_out()
             error_message = "%s" % ex
             error_details = traceback.format_exc()
         except Exception as ex:
-            LOG.error("Failed to send usage create-event for instance %s, "
-                      "error: %s", self.id, str(ex))
+            LOG.exception(_("Failed to send usage create-event for "
+                            "instance %s."), self.id)
             error_message = "%s" % ex
             error_details = traceback.format_exc()
         finally:
             if error_message:
                 inst_models.save_instance_fault(
                     self.id, error_message, error_details,
-                    skip_delta=CONF.usage_sleep_time + 1
-                )
-
-    def _create_port(self, network_info, security_groups, is_mgmt=False,
-                     is_public=False):
-        name = 'trove-%s' % self.id
-        type = 'Management' if is_mgmt else 'User'
-        description = '%s port for trove instance %s' % (type, self.id)
-
-        try:
-            port_id = neutron.create_port(
-                self.neutron_client, name,
-                description, network_info.get('network_id'),
-                security_groups,
-                is_public=is_public,
-                subnet_id=network_info.get('subnet_id'),
-                ip=network_info.get('ip_address'),
-                is_mgmt=is_mgmt,
-                project_id=self.tenant_id
-            )
-        except Exception as e:
-            self.update_db(
-                task_status=inst_models.InstanceTasks.BUILDING_ERROR_PORT
-            )
-            error = (f"Failed to create {type} port for instance {self.id}: "
-                     f"{str(e)}")
-            raise TroveError(message=error)
-
-        return port_id
-
-    def _prepare_networks_for_instance(self, datastore_manager, nics,
-                                       access=None):
-        """Prepare the networks for the trove instance.
-
-        'nics' contains the networks that management network always comes at
-        last.
-
-        returns a list of dicts which only contains port-id.
-        """
-        LOG.info("Preparing networks for the instance %s", self.id)
-        security_group = None
-        networks = copy.deepcopy(nics)
-        access = access or {}
-
-        if CONF.trove_security_groups_support:
-            security_group = self._create_secgroup(
-                datastore_manager,
-                access.get('allowed_cidrs', [])
-            )
-            LOG.info(
-                "Security group %s created for instance %s",
-                security_group, self.id
-            )
-
-        # Create management port
-        if CONF.management_networks:
-            port_sgs = [security_group] if security_group else []
-            if len(CONF.management_security_groups) > 0:
-                port_sgs = CONF.management_security_groups
-            # The management network is always the last one
-            networks.pop(-1)
-            port_id = self._create_port(
-                {'network_id': CONF.management_networks[-1]},
-                port_sgs,
-                is_mgmt=True,
-            )
-            LOG.info("Management port %s created for instance: %s", port_id,
-                     self.id)
-            networks.append({"port-id": port_id})
-
-        if not CONF.management_networks and not networks:
-            return None
-
-        # Create port in the user defined network, associate floating IP if
-        # needed
-        if len(networks) > 1 or not CONF.management_networks:
-            network_info = networks.pop(0)
-            port_sgs = [security_group] if security_group else []
-            port_id = self._create_port(
-                network_info,
-                port_sgs,
-                is_mgmt=False,
-                is_public=access.get('is_public', False),
-            )
-            LOG.info("User port %s created for instance %s", port_id,
-                     self.id)
-            networks.insert(0, {"port-id": port_id})
-
-        LOG.info(
-            "Finished to prepare networks for the instance %s, networks: %s",
-            self.id, networks
-        )
-        return networks
-
-    def _get_user_nic_info(self, port_id):
-        nic_info = dict()
-        port = self.neutron_client.show_port(port_id)
-        fixed_ips = port['port']["fixed_ips"]
-        nic_info["mac_address"] = port['port']["mac_address"]
-        for fixed_ip in fixed_ips:
-            subnet = self.neutron_client.show_subnet(
-                fixed_ip["subnet_id"])['subnet']
-            if subnet.get("ip_version") == 4:
-                nic_info["ipv4_address"] = fixed_ip.get("ip_address")
-                nic_info["ipv4_cidr"] = subnet.get("cidr")
-                nic_info["ipv4_gateway"] = subnet.get("gateway_ip")
-                nic_info["ipv4_host_routes"] = subnet.get("host_routes")
-            # NOTE: only if the ipv6_ra_mode is dhcpv6-stateful,
-            # we need to configure the ip route for container. for slaac
-            # and dhcpv6-stateless mode, the route will be configured
-            # by the nic itself via the RA protocol.
-            elif subnet.get("ip_version") == 6:
-                nic_info["ipv6_address"] = fixed_ip.get("ip_address")
-                nic_info["ipv6_cidr"] = subnet.get("cidr")
-                nic_info["ipv6_host_routes"] = subnet.get("host_routes")
-                if subnet.get("ipv6_ra_mode") == "dhcpv6-stateful":
-                    nic_info["ipv6_gateway"] = subnet.get("gateway_ip")
-        return nic_info
+                    skip_delta=USAGE_SLEEP_TIME + 1)
 
     def create_instance(self, flavor, image_id, databases, users,
                         datastore_manager, packages, volume_size,
                         backup_id, availability_zone, root_password, nics,
                         overrides, cluster_config, snapshot, volume_type,
-                        modules, scheduler_hints, access=None,
-                        ds_version=None):
-        """Create trove instance.
-
-        It is the caller's responsibility to ensure that
-        FreshInstanceTasks.wait_for_instance is called after
-        create_instance to ensure that the proper usage event gets sent
-        """
-        LOG.info(
-            "Creating instance %s, nics: %s, access: %s",
-            self.id, nics, access
-        )
-        networks = self._prepare_networks_for_instance(
-            datastore_manager, nics, access=access
-        )
-
-        if CONF.network.network_isolation and len(nics) > 1:
-            # the user defined port is always the first one.
-            nic_info = self._get_user_nic_info(networks[0]["port-id"])
-            LOG.debug("Generate the eth1_config.json file: %s", nic_info)
-            files = self.get_injected_files(datastore_manager,
-                                            ds_version,
-                                            disable_bridge=True)
-            files[constants.ETH1_CONFIG_PATH] = json.dumps(nic_info)
-        else:
-            files = self.get_injected_files(datastore_manager, ds_version)
+                        modules, scheduler_hints):
+        # It is the caller's responsibility to ensure that
+        # FreshInstanceTasks.wait_for_instance is called after
+        # create_instance to ensure that the proper usage event gets sent
+
+        LOG.info(_("Creating instance %s."), self.id)
+        security_groups = None
+
+        if CONF.trove_security_groups_support:
+            try:
+                security_groups = self._create_secgroup(datastore_manager)
+            except Exception as e:
+                msg = (_("Error creating security group for instance: %s") %
+                       self.id)
+                err = inst_models.InstanceTasks.BUILDING_ERROR_SEC_GROUP
+                self._log_and_raise(e, msg, err)
+            else:
+                LOG.debug("Successfully created security group for "
+                          "instance: %s", self.id)
 
+        files = self.get_injected_files(datastore_manager)
         cinder_volume_type = volume_type or CONF.cinder_volume_type
-        volume_info = self._create_server_volume(
-            flavor['id'], image_id,
-            datastore_manager, volume_size,
-            availability_zone, networks,
-            files, cinder_volume_type,
-            scheduler_hints
-        )
+        if use_nova_server_volume:
+            volume_info = self._create_server_volume(
+                flavor['id'],
+                image_id,
+                security_groups,
+                datastore_manager,
+                volume_size,
+                availability_zone,
+                nics,
+                files,
+                scheduler_hints)
+        else:
+            volume_info = self._create_server_volume_individually(
+                flavor['id'],
+                image_id,
+                security_groups,
+                datastore_manager,
+                volume_size,
+                availability_zone,
+                nics,
+                files,
+                cinder_volume_type,
+                scheduler_hints)
 
         config = self._render_config(flavor)
 
         backup_info = None
         if backup_id is not None:
-            backup = bkup_models.Backup.get_by_id(self.context, backup_id)
-            backup_info = {'id': backup_id,
-                           'instance_id': backup.instance_id,
-                           'location': backup.location,
-                           'type': backup.backup_type,
-                           'checksum': backup.checksum,
-                           }
+                backup = bkup_models.Backup.get_by_id(self.context, backup_id)
+                backup_info = {'id': backup_id,
+                               'instance_id': backup.instance_id,
+                               'location': backup.location,
+                               'type': backup.backup_type,
+                               'checksum': backup.checksum,
+                               }
         self._guest_prepare(flavor['ram'], volume_info,
                             packages, databases, users, backup_info,
                             config.config_contents, root_password,
                             overrides,
-                            cluster_config, snapshot, modules,
-                            ds_version=ds_version)
+                            cluster_config, snapshot, modules)
 
         if root_password:
             self.report_root_enabled()
 
+        if not self.db_info.task_status.is_error:
+            self.reset_task_status()
+
         # when DNS is supported, we attempt to add this after the
         # instance is prepared.  Otherwise, if DNS fails, instances
         # end up in a poorer state and there's no tooling around
         # re-sending the prepare call; retrying DNS is much easier.
         try:
             self._create_dns_entry()
         except Exception as e:
-            log_fmt = "Error creating DNS entry for instance: %s"
-            exc_fmt = _("Error creating DNS entry for instance: %s")
+            msg = _("Error creating DNS entry for instance: %s") % self.id
             err = inst_models.InstanceTasks.BUILDING_ERROR_DNS
-            self._log_and_raise(e, log_fmt, exc_fmt, self.id, err)
+            self._log_and_raise(e, msg, err)
 
     def attach_replication_slave(self, snapshot, flavor):
         LOG.debug("Calling attach_replication_slave for %s.", self.id)
         try:
             replica_config = self._render_replica_config(flavor)
             self.guest.attach_replication_slave(snapshot,
                                                 replica_config.config_contents)
         except GuestError as e:
-            log_fmt = "Error attaching instance %s as replica."
-            exc_fmt = _("Error attaching instance %s as replica.")
+            msg = (_("Error attaching instance %s "
+                     "as replica.") % self.id)
             err = inst_models.InstanceTasks.BUILDING_ERROR_REPLICA
-            self._log_and_raise(e, log_fmt, exc_fmt, self.id, err)
+            self._log_and_raise(e, msg, err)
 
     def get_replication_master_snapshot(self, context, slave_of_id, flavor,
-                                        parent_backup_id=None):
+                                        backup_id=None, replica_number=1):
         # First check to see if we need to take a backup
         master = BuiltInstanceTasks.load(context, slave_of_id)
         backup_required = master.backup_required_for_replication()
         if backup_required:
             # if we aren't passed in a backup id, look it up to possibly do
             # an incremental backup, thus saving time
-            if not parent_backup_id:
+            if not backup_id:
                 backup = Backup.get_last_completed(
                     context, slave_of_id, include_incremental=True)
                 if backup:
-                    parent_backup_id = backup.id
+                    backup_id = backup.id
         else:
-            LOG.debug('Skip creating replication master backup')
-
+            LOG.debug('Skipping replication backup, as none is required.')
         snapshot_info = {
             'name': "Replication snapshot for %s" % self.id,
-            'description': "Backup image used to initialize replication slave",
+            'description': "Backup image used to initialize "
+                           "replication slave",
             'instance_id': slave_of_id,
-            'parent_id': parent_backup_id,
+            'parent_id': backup_id,
             'tenant_id': self.tenant_id,
             'state': BackupState.NEW,
             'datastore_version_id': self.datastore_version.id,
             'deleted': False,
-            'replica_number': 1,
+            'replica_number': replica_number,
         }
 
         replica_backup_id = None
         if backup_required:
-            try:
-                db_info = DBBackup.create(**snapshot_info)
-                replica_backup_id = db_info.id
-            except InvalidModelError:
-                log_fmt = ("Unable to create replication snapshot record "
-                           "for instance: %s")
-                exc_fmt = _("Unable to create replication snapshot record "
-                            "for instance: %s")
-                LOG.exception(log_fmt, self.id)
-                raise BackupCreationError(exc_fmt % self.id)
-            if parent_backup_id:
-                # Look up the parent backup  info or fail early if not
-                #  found or if the user does not have access to the parent.
-                _parent = Backup.get_by_id(context, parent_backup_id)
-                parent = {
-                    'location': _parent.location,
-                    'checksum': _parent.checksum,
-                }
-                snapshot_info.update({
-                    'parent': parent,
-                })
+            # Only do a backup if it's the first replica
+            if replica_number == 1:
+                try:
+                    db_info = DBBackup.create(**snapshot_info)
+                    replica_backup_id = db_info.id
+                except InvalidModelError:
+                    msg = (_("Unable to create replication snapshot record "
+                             "for instance: %s") % self.id)
+                    LOG.exception(msg)
+                    raise BackupCreationError(msg)
+                if backup_id:
+                    # Look up the parent backup  info or fail early if not
+                    #  found or if the user does not have access to the parent.
+                    _parent = Backup.get_by_id(context, backup_id)
+                    parent = {
+                        'location': _parent.location,
+                        'checksum': _parent.checksum,
+                    }
+                    snapshot_info.update({
+                        'parent': parent,
+                    })
+            else:
+                # we've been passed in the actual replica backup id,
+                # so just use it
+                replica_backup_id = backup_id
 
         try:
             snapshot_info.update({
                 'id': replica_backup_id,
                 'datastore': master.datastore.name,
                 'datastore_version': master.datastore_version.name,
             })
             snapshot = master.get_replication_snapshot(
                 snapshot_info, flavor=master.flavor_id)
             snapshot.update({
                 'config': self._render_replica_config(flavor).config_contents
             })
             return snapshot
         except Exception as e_create:
-            create_log_fmt = (
-                "Error creating replication snapshot from "
-                "instance %(source)s for new replica %(replica)s.")
-            create_exc_fmt = (
-                "Error creating replication snapshot from "
-                "instance %(source)s for new replica %(replica)s.")
-            create_fmt_content = {
-                'source': slave_of_id,
-                'replica': self.id
-            }
+            msg_create = (
+                _("Error creating replication snapshot from "
+                  "instance %(source)s for new replica %(replica)s.") %
+                {'source': slave_of_id, 'replica': self.id})
             err = inst_models.InstanceTasks.BUILDING_ERROR_REPLICA
-            e_create_fault = create_log_fmt % create_fmt_content
-            e_create_stack = traceback.format_exc()
-            # we persist fault details to source instance
-            inst_models.save_instance_fault(slave_of_id, e_create_fault,
-                                            e_create_stack)
-
             # if the delete of the 'bad' backup fails, it'll mask the
             # create exception, so we trap it here
             try:
-                if backup_required:
+                # Only try to delete the backup if it's the first replica
+                if replica_number == 1 and backup_required:
                     Backup.delete(context, replica_backup_id)
             except Exception as e_delete:
-                LOG.error(create_log_fmt, create_fmt_content)
+                LOG.error(msg_create)
                 # Make sure we log any unexpected errors from the create
                 if not isinstance(e_create, TroveError):
                     LOG.exception(e_create)
-                delete_log_fmt = (
-                    "An error occurred while deleting a bad "
-                    "replication snapshot from instance %(source)s.")
-                delete_exc_fmt = _(
-                    "An error occurred while deleting a bad "
-                    "replication snapshot from instance %(source)s.")
+                msg_delete = (
+                    _("An error occurred while deleting a bad "
+                      "replication snapshot from instance %(source)s.") %
+                    {'source': slave_of_id})
                 # we've already logged the create exception, so we'll raise
                 # the delete (otherwise the create will be logged twice)
-                self._log_and_raise(e_delete, delete_log_fmt, delete_exc_fmt,
-                                    {'source': slave_of_id}, err)
+                self._log_and_raise(e_delete, msg_delete, err)
 
             # the delete worked, so just log the original problem with create
-            self._log_and_raise(e_create, create_log_fmt, create_exc_fmt,
-                                create_fmt_content, err)
+            self._log_and_raise(e_create, msg_create, err)
 
     def report_root_enabled(self):
-        mysql_models.RootHistory.create(self.context, self.id)
+        mysql_models.RootHistory.create(self.context, self.id, 'root')
 
     def update_statuses_on_time_out(self):
+
         if CONF.update_status_on_fail:
             # Updating service status
             service = InstanceServiceStatus.find_by(instance_id=self.id)
-            service.set_status(
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT)
+            service.set_status(ServiceStatuses.
+                               FAILED_TIMEOUT_GUESTAGENT)
             service.save()
-            LOG.error(
-                "Service status: %s, service error description: %s",
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT.api_status,
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT.description
-            )
-
+            LOG.error(_("Service status: %(status)s\n"
+                        "Service error description: %(desc)s"),
+                      {'status': ServiceStatuses.
+                       FAILED_TIMEOUT_GUESTAGENT.api_status,
+                       'desc': ServiceStatuses.
+                       FAILED_TIMEOUT_GUESTAGENT.description})
             # Updating instance status
             db_info = DBInstance.find_by(id=self.id, deleted=False)
-            db_info.set_task_status(InstanceTasks.BUILDING_ERROR_TIMEOUT_GA)
+            db_info.set_task_status(InstanceTasks.
+                                    BUILDING_ERROR_TIMEOUT_GA)
             db_info.save()
-            LOG.error(
-                "Trove instance status: %s, Trove instance status "
-                "description: %s",
-                InstanceTasks.BUILDING_ERROR_TIMEOUT_GA.action,
-                InstanceTasks.BUILDING_ERROR_TIMEOUT_GA.db_text
-            )
+            LOG.error(_("Trove instance status: %(action)s\n"
+                        "Trove instance status description: %(text)s"),
+                      {'action': InstanceTasks.
+                       BUILDING_ERROR_TIMEOUT_GA.action,
+                       'text': InstanceTasks.
+                       BUILDING_ERROR_TIMEOUT_GA.db_text})
 
     def _service_is_active(self):
         """
         Check that the database guest is active.
 
         This function is meant to be called with poll_until to check that
         the guest is alive before sending a 'create' message. This prevents
         over billing a customer for an instance that they can never use.
 
         Returns: boolean if the service is active.
         Raises: TroveError if the service is in a failure state.
         """
         service = InstanceServiceStatus.find_by(instance_id=self.id)
         status = service.get_status()
-
-        if (status == srvstatus.ServiceStatuses.RUNNING or
-                status == srvstatus.ServiceStatuses.INSTANCE_READY or
-                status == srvstatus.ServiceStatuses.HEALTHY):
-            return True
-        elif status not in [srvstatus.ServiceStatuses.NEW,
-                            srvstatus.ServiceStatuses.BUILDING,
-                            srvstatus.ServiceStatuses.UNKNOWN,
-                            srvstatus.ServiceStatuses.DELETED]:
+        if (status == rd_instance.ServiceStatuses.RUNNING or
+           status == rd_instance.ServiceStatuses.INSTANCE_READY):
+                return True
+        elif status not in [rd_instance.ServiceStatuses.NEW,
+                            rd_instance.ServiceStatuses.BUILDING,
+                            rd_instance.ServiceStatuses.UNKNOWN,
+                            rd_instance.ServiceStatuses.DELETED]:
             raise TroveError(_("Service not active, status: %s") % status)
 
         c_id = self.db_info.compute_instance_id
-        try:
-            server = self.nova_client.servers.get(c_id)
-        except Exception as e:
-            if getattr(e, 'message', '') == 'Not found':
-                raise exception.ComputeInstanceNotFound(instance_id=self.id,
-                                                        server_id=c_id)
-            else:
-                raise TroveError(
-                    _("Failed to get server %(server)s for instance "
-                      "%(instance)s, error: %(error)s"),
-                    server=c_id, instance=self.id, error=str(e)
-                )
-
+        server = self.nova_client.servers.get(c_id)
         server_status = server.status
         if server_status in [InstanceStatus.ERROR,
                              InstanceStatus.FAILED]:
             server_fault_message = 'No fault found'
             try:
                 server_fault_message = server.fault.get('message', 'Unknown')
             except AttributeError:
                 pass
-            raise TroveError(
-                _("Server not active, status: %(status)s, fault message: "
-                  "%(srv_msg)s") %
-                {'status': server_status, 'srv_msg': server_fault_message}
-            )
+            server_message = "\nServer error: %s" % server_fault_message
+            raise TroveError(_("Server not active, status: %(status)s"
+                               "%(srv_msg)s") %
+                             {'status': server_status,
+                              'srv_msg': server_message})
         return False
 
+    def _create_server_volume(self, flavor_id, image_id, security_groups,
+                              datastore_manager, volume_size,
+                              availability_zone, nics, files,
+                              scheduler_hints):
+        LOG.debug("Begin _create_server_volume for id: %s", self.id)
+        try:
+            userdata = self._prepare_userdata(datastore_manager)
+            name = self.hostname or self.name
+            volume_desc = ("datastore volume for %s" % self.id)
+            volume_name = ("datastore-%s" % self.id)
+            volume_ref = {'size': volume_size, 'name': volume_name,
+                          'description': volume_desc}
+            config_drive = CONF.use_nova_server_config_drive
+            server = self.nova_client.servers.create(
+                name, image_id, flavor_id,
+                files=files, volume=volume_ref,
+                security_groups=security_groups,
+                availability_zone=availability_zone,
+                nics=nics, config_drive=config_drive,
+                userdata=userdata, scheduler_hints=scheduler_hints)
+            server_dict = server._info
+            LOG.debug("Created new compute instance %(server_id)s "
+                      "for id: %(id)s\nServer response: %(response)s",
+                      {'server_id': server.id, 'id': self.id,
+                       'response': server_dict})
+
+            volume_id = None
+            for volume in server_dict.get('os:volumes', []):
+                volume_id = volume.get('id')
+
+            # Record the server ID and volume ID in case something goes wrong.
+            self.update_db(compute_instance_id=server.id, volume_id=volume_id)
+        except Exception as e:
+            msg = _("Error creating server and volume for "
+                    "instance %s") % self.id
+            LOG.debug("End _create_server_volume for id: %s", self.id)
+            err = inst_models.InstanceTasks.BUILDING_ERROR_SERVER
+            self._log_and_raise(e, msg, err)
+
+        device_path = self.device_path
+        mount_point = CONF.get(datastore_manager).mount_point
+        volume_info = {'device_path': device_path, 'mount_point': mount_point}
+        LOG.debug("End _create_server_volume for id: %s", self.id)
+        return volume_info
+
     def _build_sg_rules_mapping(self, rule_ports):
         final = []
         cidr = CONF.trove_security_group_rule_cidr
         for port_or_range in set(rule_ports):
             from_, to_ = port_or_range[0], port_or_range[-1]
             final.append({'cidr': cidr,
                           'from_': str(from_),
                           'to_': str(to_)})
         return final
 
-    def _create_server_volume(self, flavor_id, image_id, datastore_manager,
-                              volume_size, availability_zone, nics, files,
-                              volume_type, scheduler_hints):
-        LOG.debug("Begin _create_server_volume for instance: %s", self.id)
+    def _create_server_volume_individually(self, flavor_id, image_id,
+                                           security_groups, datastore_manager,
+                                           volume_size, availability_zone,
+                                           nics, files, volume_type,
+                                           scheduler_hints):
+        LOG.debug("Begin _create_server_volume_individually for id: %s",
+                  self.id)
         server = None
-        volume_info = self._build_volume_info(
-            datastore_manager,
-            volume_size=volume_size,
-            volume_type=volume_type,
-            availability_zone=availability_zone)
-        block_device_mapping_v2 = volume_info['block_device']
-
-        if CONF.volume_rootdisk_support:
-            block_device_mapping_v2.insert(
-                0, self._create_root_volume(
-                    image_id,
-                    CONF.volume_rootdisk_size,
-                    volume_type,
-                    availability_zone
-                ))
-            image_id = None
-
-        try:
-            server = self._create_server(
-                flavor_id, image_id,
-                datastore_manager,
-                block_device_mapping_v2,
-                availability_zone, nics, files,
-                scheduler_hints
-            )
+        volume_info = self._build_volume_info(datastore_manager,
+                                              volume_size=volume_size,
+                                              volume_type=volume_type)
+        block_device_mapping = volume_info['block_device']
+        try:
+            server = self._create_server(flavor_id, image_id, security_groups,
+                                         datastore_manager,
+                                         block_device_mapping,
+                                         availability_zone, nics, files,
+                                         scheduler_hints)
             server_id = server.id
             # Save server ID.
             self.update_db(compute_instance_id=server_id)
         except Exception as e:
-            log_fmt = "Failed to create server for instance %s"
-            exc_fmt = _("Failed to create server for instance %s")
+            msg = _("Failed to create server for instance %s") % self.id
             err = inst_models.InstanceTasks.BUILDING_ERROR_SERVER
-            self._log_and_raise(e, log_fmt, exc_fmt, self.id, err)
-        LOG.debug("End _create_server_volume for instance: %s", self.id)
+            self._log_and_raise(e, msg, err)
+        LOG.debug("End _create_server_volume_individually for id: %s",
+                  self.id)
         return volume_info
 
     def _build_volume_info(self, datastore_manager, volume_size=None,
-                           volume_type=None, availability_zone=None):
+                           volume_type=None):
         volume_info = None
         volume_support = self.volume_support
         device_path = self.device_path
         mount_point = CONF.get(datastore_manager).mount_point
         LOG.debug("trove volume support = %s", volume_support)
         if volume_support:
             try:
                 volume_info = self._create_volume(
-                    volume_size, volume_type, datastore_manager,
-                    availability_zone)
+                    volume_size, volume_type, datastore_manager)
             except Exception as e:
-                log_fmt = "Failed to create volume for instance %s"
-                exc_fmt = _("Failed to create volume for instance %s")
+                msg = _("Failed to create volume for instance %s") % self.id
                 err = inst_models.InstanceTasks.BUILDING_ERROR_VOLUME
-                self._log_and_raise(e, log_fmt, exc_fmt, self.id, err)
+                self._log_and_raise(e, msg, err)
         else:
             LOG.debug("device_path = %(path)s\n"
                       "mount_point = %(point)s",
                       {
                           "path": device_path,
                           "point": mount_point
                       })
             volume_info = {
                 'block_device': None,
                 'device_path': device_path,
                 'mount_point': mount_point,
+                'volumes': None,
             }
         return volume_info
 
-    # We remove all translations for messages logging execpet those for
-    # exception raising. And we cannot use _(xxxx) instead of _("xxxx")
-    # because of H701 PEP8 checking. So we pass log format , exception
-    # format, and format content in and do translations only if needed.
-    def _log_and_raise(self, exc, log_fmt, exc_fmt,
-                       fmt_content, task_status):
-        LOG.error("%(message)s\n%(exc)s\n%(trace)s",
-                  {"message": log_fmt % fmt_content,
+    def _log_and_raise(self, exc, message, task_status):
+        LOG.error(_("%(message)s\n%(exc)s\n%(trace)s"),
+                  {"message": message,
                    "exc": exc,
                    "trace": traceback.format_exc()})
         self.update_db(task_status=task_status)
         exc_message = '\n%s' % exc if exc else ''
-        full_message = "%s%s" % (exc_fmt % fmt_content, exc_message)
+        full_message = "%s%s" % (message, exc_message)
         raise TroveError(message=full_message)
 
-    def _create_root_volume(self, image_id,
-                            volume_size, volume_type, availability_zone):
-        LOG.debug("Begin _create_root_volume for instance: %s", self.id)
-        volume_client = create_cinder_client(self.context, self.region_name)
-        volume_desc = ("root volume for %s" % self.id)
-        volume_kwargs = {
-            'size': volume_size,
-            'name': "trove-%s" % self.id,
-            'description': volume_desc,
-            'volume_type': volume_type,
-            'imageRef': image_id
-        }
-        if CONF.enable_volume_az:
-            volume_kwargs['availability_zone'] = availability_zone
-
-        volume_ref = volume_client.volumes.create(**volume_kwargs)
-
-        utils.poll_until(
-            lambda: volume_client.volumes.get(volume_ref.id),
-            lambda v_ref: v_ref.status in ['available', 'error'],
-            sleep_time=2,
-            time_out=CONF.volume_time_out)
-
-        v_ref = volume_client.volumes.get(volume_ref.id)
-        if v_ref.status in ['error']:
-            raise VolumeCreationFailure()
-
-        block_device_mapping_v2 = {
-            "uuid": v_ref.id,
-            "boot_index": 0,
-            "source_type": "volume",
-            "destination_type": "volume",
-            "delete_on_termination": True
-        }
-
-        LOG.debug("End _create_root_volume for instance: %s", self.id)
-        return block_device_mapping_v2
-
-    def _create_volume(self, volume_size, volume_type, datastore_manager,
-                       availability_zone):
+    def _create_volume(self, volume_size, volume_type, datastore_manager):
         LOG.debug("Begin _create_volume for id: %s", self.id)
         volume_client = create_cinder_client(self.context, self.region_name)
         volume_desc = ("datastore volume for %s" % self.id)
-        volume_kwargs = {
-            'size': volume_size,
-            'name': "trove-%s" % self.id,
-            'description': volume_desc,
-            'volume_type': volume_type}
-        if CONF.enable_volume_az:
-            volume_kwargs['availability_zone'] = availability_zone
-        volume_ref = volume_client.volumes.create(**volume_kwargs)
+        volume_ref = volume_client.volumes.create(
+            volume_size, name="datastore-%s" % self.id,
+            description=volume_desc,
+            volume_type=volume_type)
 
         # Record the volume ID in case something goes wrong.
         self.update_db(volume_id=volume_ref.id)
 
         utils.poll_until(
             lambda: volume_client.volumes.get(volume_ref.id),
             lambda v_ref: v_ref.status in ['available', 'error'],
             sleep_time=2,
-            time_out=CONF.volume_time_out)
+            time_out=VOLUME_TIME_OUT)
 
         v_ref = volume_client.volumes.get(volume_ref.id)
         if v_ref.status in ['error']:
             raise VolumeCreationFailure()
         LOG.debug("End _create_volume for id: %s", self.id)
         return self._build_volume(v_ref, datastore_manager)
 
     def _build_volume(self, v_ref, datastore_manager):
         LOG.debug("Created volume %s", v_ref)
-        # TODO(zhaochao): from Liberty, Nova libvirt driver does not honor
-        # user-supplied device name anymore, so we may need find a new
-        # method to make sure the volume is correctly mounted inside the
-        # guest, please refer to the 'intermezzo-problem-with-device-names'
-        # section of Nova user referrence at:
-        # https://docs.openstack.org/nova/latest/user/block-device-mapping.html
-        bdm = CONF.block_device_mapping
-
-        # use Nova block_device_mapping_v2, referrence:
-        # https://docs.openstack.org/api-ref/compute/#create-server
+        # The mapping is in the format:
+        # <id>:[<type>]:[<size(GB)>]:[<delete_on_terminate>]
         # setting the delete_on_terminate instance to true=1
-        block_device_v2 = [{
-            "uuid": v_ref.id,
-            "source_type": "volume",
-            "destination_type": "volume",
-            "device_name": bdm,
-            "volume_size": v_ref.size,
-            "delete_on_termination": True
-        }]
+        mapping = "%s:%s:%s:%s" % (v_ref.id, '', v_ref.size, 1)
+        bdm = CONF.block_device_mapping
+        block_device = {bdm: mapping}
         created_volumes = [{'id': v_ref.id,
                             'size': v_ref.size}]
 
         device_path = self.device_path
         mount_point = CONF.get(datastore_manager).mount_point
 
         LOG.debug("block_device = %(device)s\n"
                   "volume = %(volume)s\n"
                   "device_path = %(path)s\n"
                   "mount_point = %(point)s",
-                  {"device": block_device_v2,
+                  {"device": block_device,
                    "volume": created_volumes,
                    "path": device_path,
                    "point": mount_point})
 
-        volume_info = {'block_device': block_device_v2,
+        volume_info = {'block_device': block_device,
                        'device_path': device_path,
-                       'mount_point': mount_point}
+                       'mount_point': mount_point,
+                       'volumes': created_volumes}
         return volume_info
 
-    def _create_server(self, flavor_id, image_id, datastore_manager,
-                       block_device_mapping_v2, availability_zone,
-                       nics, files={}, scheduler_hints=None):
-        userdata = self.prepare_userdata(datastore_manager)
-        metadata = {'trove_project_id': self.tenant_id,
-                    'trove_user_id': self.context.user_id,
-                    'trove_instance_id': self.id}
-        bdmap_v2 = block_device_mapping_v2
+    def _prepare_userdata(self, datastore_manager):
+        userdata = None
+        cloudinit = os.path.join(CONF.get('cloudinit_location'),
+                                 "%s.cloudinit" % datastore_manager)
+        if os.path.isfile(cloudinit):
+            with open(cloudinit, "r") as f:
+                userdata = f.read()
+        return userdata
+
+    def _create_server(self, flavor_id, image_id, security_groups,
+                       datastore_manager, block_device_mapping,
+                       availability_zone, nics, files={},
+                       scheduler_hints=None):
+        userdata = self._prepare_userdata(datastore_manager)
+        name = self.hostname or self.name
+        bdmap = block_device_mapping
         config_drive = CONF.use_nova_server_config_drive
-        key_name = CONF.nova_keypair
-
-        # Use config_drive instead by userdata
-        # We will inject guest config by cloud-config
-        if files:
-            if not userdata:
-                userdata = self.prepare_cloud_config(files)
-            else:
-                userdata = userdata + self.prepare_cloud_config(files)
-
-            files = {}
 
         server = self.nova_client.servers.create(
-            self.name, image_id, flavor_id, key_name=key_name, nics=nics,
-            block_device_mapping_v2=bdmap_v2,
-            files=files, userdata=userdata,
-            availability_zone=availability_zone,
-            config_drive=config_drive, scheduler_hints=scheduler_hints,
-            meta=metadata,
-        )
+            name, image_id, flavor_id, files=files, userdata=userdata,
+            security_groups=security_groups, block_device_mapping=bdmap,
+            availability_zone=availability_zone, nics=nics,
+            config_drive=config_drive, scheduler_hints=scheduler_hints)
         LOG.debug("Created new compute instance %(server_id)s "
-                  "for database instance %(id)s",
+                  "for instance %(id)s",
                   {'server_id': server.id, 'id': self.id})
         return server
 
     def _guest_prepare(self, flavor_ram, volume_info,
                        packages, databases, users, backup_info=None,
                        config_contents=None, root_password=None,
                        overrides=None, cluster_config=None, snapshot=None,
-                       modules=None, ds_version=None):
+                       modules=None):
         LOG.debug("Entering guest_prepare")
         # Now wait for the response from the create to do additional work
         self.guest.prepare(flavor_ram, packages, databases, users,
                            device_path=volume_info['device_path'],
                            mount_point=volume_info['mount_point'],
                            backup_info=backup_info,
                            config_contents=config_contents,
                            root_password=root_password,
                            overrides=overrides,
                            cluster_config=cluster_config,
-                           snapshot=snapshot, modules=modules,
-                           ds_version=ds_version)
+                           snapshot=snapshot, modules=modules)
 
     def _create_dns_entry(self):
         dns_support = CONF.trove_dns_support
         LOG.debug("trove dns support = %s", dns_support)
 
         if dns_support:
             LOG.debug("%(gt)s: Creating dns entry for instance: %(id)s",
@@ -1142,22 +954,22 @@
                 if server.addresses != {}:
                     return True
                 elif (server.addresses == {} and
                       server.status != InstanceStatus.ERROR):
                     return False
                 elif (server.addresses == {} and
                       server.status == InstanceStatus.ERROR):
-                    LOG.error("Failed to create DNS entry for instance "
-                              "%(instance)s. Server status was "
-                              "%(status)s).",
+                    LOG.error(_("Failed to create DNS entry for instance "
+                                "%(instance)s. Server status was "
+                                "%(status)s)."),
                               {'instance': self.id, 'status': server.status})
                     raise TroveError(status=server.status)
 
             utils.poll_until(get_server, ip_is_available,
-                             sleep_time=1, time_out=CONF.dns_time_out)
+                             sleep_time=1, time_out=DNS_TIME_OUT)
             server = self.nova_client.servers.get(
                 self.db_info.compute_instance_id)
             self.db_info.addresses = server.addresses
             LOG.debug("Creating dns entry...")
             ip = self.dns_ip_address
             if not ip:
                 raise TroveError(_("Failed to create DNS entry for instance "
@@ -1165,266 +977,358 @@
             dns_client.create_instance_entry(self.id, ip)
             LOG.debug("Successfully created DNS entry for instance: %s",
                       self.id)
         else:
             LOG.debug("%(gt)s: DNS not enabled for instance: %(id)s",
                       {'gt': greenthread.getcurrent(), 'id': self.id})
 
-    def _create_secgroup(self, datastore_manager, allowed_cidrs):
-        name = "%s-%s" % (CONF.trove_security_group_name_prefix, self.id)
+    def _create_secgroup(self, datastore_manager):
+        security_group = SecurityGroup.create_for_instance(
+            self.id, self.context, self.region_name)
+        tcp_ports = CONF.get(datastore_manager).tcp_ports
+        udp_ports = CONF.get(datastore_manager).udp_ports
+        icmp = CONF.get(datastore_manager).icmp
+        self._create_rules(security_group, tcp_ports, 'tcp')
+        self._create_rules(security_group, udp_ports, 'udp')
+        if icmp:
+            self._create_rules(security_group, None, 'icmp')
+        return [security_group["name"]]
+
+    def _create_rules(self, s_group, ports, protocol):
+        err = inst_models.InstanceTasks.BUILDING_ERROR_SEC_GROUP
+        err_msg = _("Failed to create security group rules for instance "
+                    "%(instance_id)s: Invalid port format - "
+                    "FromPort = %(from)s, ToPort = %(to)s")
+
+        def set_error_and_raise(port_or_range):
+            from_port, to_port = port_or_range
+            self.update_db(task_status=err)
+            msg = err_msg % {'instance_id': self.id, 'from': from_port,
+                             'to': to_port}
+            raise MalformedSecurityGroupRuleError(message=msg)
 
-        try:
-            sg_id = neutron.create_security_group(
-                self.neutron_client, name, self.id
-            )
-
-            if not allowed_cidrs:
-                allowed_cidrs = [CONF.trove_security_group_rule_cidr]
-            tcp_ports = CONF.get(datastore_manager).tcp_ports
-            udp_ports = CONF.get(datastore_manager).udp_ports
-
-            neutron.create_security_group_rule(
-                self.neutron_client, sg_id, 'tcp', tcp_ports, allowed_cidrs
-            )
-            neutron.create_security_group_rule(
-                self.neutron_client, sg_id, 'udp', udp_ports, allowed_cidrs
-            )
-        except Exception:
-            message = ("Failed to create security group for instance %s"
-                       % self.id)
-            LOG.exception(message)
-            self.update_db(
-                task_status=inst_models.InstanceTasks.BUILDING_ERROR_SEC_GROUP
-            )
-            raise TroveError(message=message)
+        cidr = CONF.trove_security_group_rule_cidr
 
-        return sg_id
+        if protocol == 'icmp':
+            SecurityGroupRule.create_sec_group_rule(
+                s_group, 'icmp', None, None,
+                cidr, self.context, self.region_name)
+        else:
+            for port_or_range in set(ports):
+                try:
+                    from_, to_ = (None, None)
+                    from_, to_ = port_or_range[0], port_or_range[-1]
+                    SecurityGroupRule.create_sec_group_rule(
+                        s_group, protocol, int(from_), int(to_),
+                        cidr, self.context, self.region_name)
+                except (ValueError, TroveError):
+                    set_error_and_raise([from_, to_])
 
 
-class BuiltInstanceTasks(Instance, NotifyMixin, ConfigurationMixin):
+class BuiltInstanceTasks(BuiltInstance, NotifyMixin, ConfigurationMixin):
     """
-    BuiltInstanceTasks contains the tasks related an instance that already
-    associated with a compute server.
+    Performs the various asynchronous instance related tasks.
     """
 
-    def is_service_healthy(self):
-        """Wait for the db service up and running.
-
-        This method is supposed to be called with poll_until against an
-        existing db instance.
-        """
-        service = InstanceServiceStatus.find_by(instance_id=self.id)
-        status = service.get_status()
+    def _delete_resources(self, deleted_at):
+        LOG.debug("Begin _delete_resources for instance %s", self.id)
+        server_id = self.db_info.compute_instance_id
+        old_server = self.nova_client.servers.get(server_id)
+        try:
+            # The server may have already been marked as 'SHUTDOWN'
+            # but check for 'ACTIVE' in case of any race condition
+            # We specifically don't want to attempt to stop db if
+            # the server is in 'ERROR' or 'FAILED" state, as it will
+            # result in a long timeout
+            if self.server_status_matches(['ACTIVE', 'SHUTDOWN'], server=self):
+                LOG.debug("Stopping datastore on instance %s before deleting "
+                          "any resources.", self.id)
+                self.guest.stop_db()
+        except Exception:
+            LOG.exception(_("Error stopping the datastore before attempting "
+                            "to delete instance id %s."), self.id)
+        try:
+            self.server.delete()
+        except Exception as ex:
+            LOG.exception(_("Error during delete compute server %s"),
+                          self.server.id)
+        try:
+            dns_support = CONF.trove_dns_support
+            LOG.debug("trove dns support = %s", dns_support)
+            if dns_support:
+                dns_api = create_dns_client(self.context)
+                dns_api.delete_instance_entry(instance_id=self.db_info.id)
+        except Exception as ex:
+            LOG.exception(_("Error during dns entry of instance %(id)s: "
+                            "%(ex)s"), {'id': self.db_info.id, 'ex': ex})
+        try:
+            srv_grp.ServerGroup.delete(self.context, self.server_group)
+        except Exception:
+            LOG.exception(_("Error during delete server group for %s"),
+                          self.id)
 
-        if service.is_uptodate():
-            if status in [srvstatus.ServiceStatuses.HEALTHY]:
+        # Poll until the server is gone.
+        def server_is_finished():
+            try:
+                server = self.nova_client.servers.get(server_id)
+                if not self.server_status_matches(['SHUTDOWN', 'ACTIVE'],
+                                                  server=server):
+                    LOG.error(_("Server %(server_id)s entered ERROR status "
+                                "when deleting instance %(instance_id)s!"),
+                              {'server_id': server.id, 'instance_id': self.id})
+                return False
+            except nova_exceptions.NotFound:
                 return True
-            elif status in [
-                srvstatus.ServiceStatuses.FAILED,
-                srvstatus.ServiceStatuses.UNKNOWN,
-                srvstatus.ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT
-            ]:
-                raise TroveError('Database service error, status: %s' % status)
 
-        return False
+        try:
+            utils.poll_until(server_is_finished, sleep_time=2,
+                             time_out=CONF.server_delete_time_out)
+        except PollTimeOut:
+            LOG.exception(_("Failed to delete instance %(instance_id)s: "
+                            "Timeout deleting compute server %(server_id)s"),
+                          {'instance_id': self.id, 'server_id': server_id})
+
+        # If volume has been resized it must be manually removed in cinder
+        try:
+            if self.volume_id:
+                volume_client = create_cinder_client(self.context,
+                                                     self.region_name)
+                volume = volume_client.volumes.get(self.volume_id)
+                if volume.status == "available":
+                    LOG.info(_("Deleting volume %(v)s for instance: %(i)s."),
+                             {'v': self.volume_id, 'i': self.id})
+                    volume.delete()
+        except Exception:
+            LOG.exception(_("Error deleting volume of instance %(id)s."),
+                          {'id': self.db_info.id})
+
+        TroveInstanceDelete(instance=self,
+                            deleted_at=timeutils.isotime(deleted_at),
+                            server=old_server).notify()
+        LOG.debug("End _delete_resources for instance %s", self.id)
+
+    def server_status_matches(self, expected_status, server=None):
+        if not server:
+            server = self.server
+        return server.status.upper() in (
+            status.upper() for status in expected_status)
 
     def resize_volume(self, new_size):
-        LOG.info("Resizing volume for instance %(instance_id)s from "
-                 "%(old_size)s GB to %(new_size)s GB.",
+        LOG.info(_("Resizing volume for instance %(instance_id)s from "
+                 "%(old_size)s GB to %(new_size)s GB."),
                  {'instance_id': self.id, 'old_size': self.volume_size,
                   'new_size': new_size})
         action = ResizeVolumeAction(self, self.volume_size, new_size)
         action.execute()
-        LOG.info("Resized volume for instance %s successfully.", self.id)
+        LOG.info(_("Resized volume for instance %s successfully."), self.id)
 
     def resize_flavor(self, old_flavor, new_flavor):
-        LOG.info("Resizing instance %(instance_id)s from flavor "
-                 "%(old_flavor)s to %(new_flavor)s.",
+        LOG.info(_("Resizing instance %(instance_id)s from flavor "
+                   "%(old_flavor)s to %(new_flavor)s."),
                  {'instance_id': self.id, 'old_flavor': old_flavor['id'],
                   'new_flavor': new_flavor['id']})
         action = ResizeAction(self, old_flavor, new_flavor)
         action.execute()
-        LOG.info("Resized instance %s successfully.", self.id)
+        LOG.info(_("Resized instance %s successfully."), self.id)
 
     def migrate(self, host):
-        LOG.info("Initiating migration to host %s.", host)
+        LOG.info(_("Initiating migration to host %s."), host)
         action = MigrateAction(self, host)
         action.execute()
 
-    def rebuild(self, image_id):
-        LOG.info(f"Rebuilding instance {self.id}, new image {image_id}")
-        action = RebuildAction(self, image_id)
-        action.execute()
-
     def create_backup(self, backup_info):
-        LOG.info("Initiating backup for instance %s, backup_info: %s", self.id,
-                 backup_info)
+        LOG.info(_("Initiating backup for instance %s."), self.id)
         self.guest.create_backup(backup_info)
 
     def backup_required_for_replication(self):
-        LOG.debug("Check if replication backup is required for instance %s.",
+        LOG.debug("Seeing if replication backup is required for instance %s.",
                   self.id)
         return self.guest.backup_required_for_replication()
 
     def get_replication_snapshot(self, snapshot_info, flavor):
+
         def _get_replication_snapshot():
-            LOG.info("Getting replication snapshot for instance %s.", self.id)
+            LOG.debug("Calling get_replication_snapshot on %s.", self.id)
             try:
                 rep_source_config = self._render_replica_source_config(flavor)
                 result = self.guest.get_replication_snapshot(
                     snapshot_info, rep_source_config.config_contents)
-
-                LOG.info("Finnished getting replication snapshot for "
-                         "instance %s", self.id)
+                LOG.debug("Got replication snapshot from guest successfully.")
                 return result
-            except Exception as err:
-                LOG.error("Failed to get replication snapshot from %s, "
-                          "error: %s", self.id, str(err))
+            except Exception:
+                LOG.exception(_("Failed to get replication snapshot from %s."),
+                              self.id)
                 raise
 
-        return run_with_quotas(self.context.project_id, {'backups': 1},
+        return run_with_quotas(self.context.tenant, {'backups': 1},
                                _get_replication_snapshot)
 
     def detach_replica(self, master, for_failover=False):
-        LOG.info("Detaching replica %s from %s", self.id, master.id)
+        LOG.debug("Calling detach_replica on %s", self.id)
         try:
             self.guest.detach_replica(for_failover)
             self.update_db(slave_of_id=None)
             self.slave_list = None
-            LOG.info('Replica %s detached', self.id)
         except (GuestError, GuestTimeout):
-            LOG.error("Failed to detach replica %s from %s", self.id,
-                      master.id)
+            LOG.exception(_("Failed to detach replica %s."), self.id)
             raise
         finally:
             if not for_failover:
                 self.reset_task_status()
 
-    def attach_replica(self, master, restart=False):
-        LOG.info("Attaching replica %s to primary %s", self.id, master.id)
+    def attach_replica(self, master):
+        LOG.debug("Calling attach_replica on %s", self.id)
         try:
             replica_info = master.guest.get_replica_context()
             flavor = self.nova_client.flavors.get(self.flavor_id)
             slave_config = self._render_replica_config(flavor).config_contents
-            self.guest.attach_replica(replica_info, slave_config,
-                                      restart=restart)
+            self.guest.attach_replica(replica_info, slave_config)
             self.update_db(slave_of_id=master.id)
             self.slave_list = None
         except (GuestError, GuestTimeout):
-            LOG.exception("Failed to attach replica %s.", self.id)
+            LOG.exception(_("Failed to attach replica %s."), self.id)
             raise
 
     def make_read_only(self, read_only):
         LOG.debug("Calling make_read_only on %s", self.id)
         self.guest.make_read_only(read_only)
 
-    def get_public_ip(self):
-        """Get public IP (IP, ID) for the trove instance."""
-        for item in self.get_visible_ip_addresses():
-            if item['type'] == 'public':
-                fips = self.neutron_client.list_floatingips(
-                    floating_ip_address=item['address'])['floatingips']
-                if fips:
-                    fip_id = fips[0]['id']
-                    return item['address'], fip_id
-
-        return None, None
+    def _get_floating_ips(self):
+        """Returns floating ips as a dict indexed by the ip."""
+        floating_ips = {}
+        neutron_client = remote.create_neutron_client(self.context)
+        network_floating_ips = neutron_client.list_floatingips()
+        for ip in network_floating_ips.get('floatingips'):
+            floating_ips.update({ip.get('floating_ip_address'): ip})
+        LOG.debug("In _get_floating_ips(), returning %s", floating_ips)
+        return floating_ips
+
+    def detach_public_ips(self):
+        LOG.debug("Begin detach_public_ips for instance %s", self.id)
+        removed_ips = []
+        server_id = self.db_info.compute_instance_id
+        nova_instance = self.nova_client.servers.get(server_id)
+        floating_ips = self._get_floating_ips()
+        for ip in self.get_visible_ip_addresses():
+            if ip in floating_ips:
+                nova_instance.remove_floating_ip(ip)
+                removed_ips.append(ip)
+        return removed_ips
+
+    def attach_public_ips(self, ips):
+        LOG.debug("Begin attach_public_ips for instance %s", self.id)
+        server_id = self.db_info.compute_instance_id
+        nova_instance = self.nova_client.servers.get(server_id)
+        for ip in ips:
+            nova_instance.add_floating_ip(ip)
 
     def enable_as_master(self):
-        LOG.info("Enable %s as master", self.id)
-
+        LOG.debug("Calling enable_as_master on %s", self.id)
         flavor = self.nova_client.flavors.get(self.flavor_id)
         replica_source_config = self._render_replica_source_config(flavor)
         self.update_db(slave_of_id=None)
         self.slave_list = None
-
         self.guest.enable_as_master(replica_source_config.config_contents)
 
     def get_last_txn(self):
-        LOG.info("Getting master UUID and last txn for replica %s", self.id)
+        LOG.debug("Calling get_last_txn on %s", self.id)
         return self.guest.get_last_txn()
 
     def get_latest_txn_id(self):
-        LOG.info("Getting latest txn id on %s", self.id)
+        LOG.debug("Calling get_latest_txn_id on %s", self.id)
         return self.guest.get_latest_txn_id()
 
     def wait_for_txn(self, txn):
-        LOG.info("Waiting for txn sync on %s, txn: %s", self.id, txn)
+        LOG.debug("Calling wait_for_txn on %s", self.id)
         if txn:
             self.guest.wait_for_txn(txn)
 
     def cleanup_source_on_replica_detach(self, replica_info):
         LOG.debug("Calling cleanup_source_on_replica_detach on %s", self.id)
         self.guest.cleanup_source_on_replica_detach(replica_info)
 
     def demote_replication_master(self):
-        LOG.info("Demoting old replication master %s", self.id)
+        LOG.debug("Calling demote_replication_master on %s", self.id)
         self.guest.demote_replication_master()
 
     def reboot(self):
         try:
-            LOG.debug("Stopping database on instance %s.", self.id)
+            # Issue a guest stop db call to shutdown the db if running
+            LOG.debug("Stopping datastore on instance %s.", self.id)
             try:
                 self.guest.stop_db()
             except (exception.GuestError, exception.GuestTimeout) as e:
                 # Acceptable to be here if db was already in crashed state
                 # Also we check guest state before issuing reboot
-                LOG.warning(str(e))
+                LOG.debug(str(e))
 
-            LOG.info("Rebooting instance %s.", self.id)
+            self._refresh_datastore_status()
+            if not (self.datastore_status_matches(
+                    rd_instance.ServiceStatuses.SHUTDOWN) or
+                    self.datastore_status_matches(
+                    rd_instance.ServiceStatuses.CRASHED)):
+                # We will bail if db did not get stopped or is blocked
+                LOG.error(_("Cannot reboot instance. DB status is %s."),
+                          self.datastore_status.status)
+                return
+            LOG.debug("The guest service status is %s.",
+                      self.datastore_status.status)
+
+            LOG.info(_("Rebooting instance %s."), self.id)
             self.server.reboot()
+            # Poll nova until instance is active
+            reboot_time_out = CONF.reboot_time_out
 
             def update_server_info():
                 self.refresh_compute_server_info()
                 return self.server_status_matches(['ACTIVE'])
 
-            utils.poll_until(update_server_info, sleep_time=3,
-                             time_out=CONF.reboot_time_out, initial_delay=5)
-
-            LOG.info("Starting database on instance %s.", self.id)
-            self.guest.restart()
-
-            # Wait for database service up and running
-            utils.poll_until(self.is_service_healthy,
-                             time_out=CONF.report_interval * 2)
-
-            LOG.info("Rebooted instance %s successfully.", self.id)
+            utils.poll_until(
+                update_server_info,
+                sleep_time=2,
+                time_out=reboot_time_out)
+
+            # Set the status to PAUSED. The guest agent will reset the status
+            # when the reboot completes and MySQL is running.
+            self.set_datastore_status_to_paused()
+            LOG.info(_("Rebooted instance %s successfully."), self.id)
         except Exception as e:
-            LOG.error("Failed to reboot instance %(id)s: %(e)s",
+            LOG.error(_("Failed to reboot instance %(id)s: %(e)s"),
                       {'id': self.id, 'e': str(e)})
         finally:
+            LOG.debug("Rebooting FINALLY %s", self.id)
             self.reset_task_status()
 
     def restart(self):
-        LOG.info("Initiating datastore restart on instance %s.", self.id)
+        LOG.info(_("Initiating datastore restart on instance %s."), self.id)
         try:
             self.guest.restart()
         except GuestError:
-            LOG.error("Failed to initiate datastore restart on instance "
-                      "%s.", self.id)
+            LOG.error(_("Failed to initiate datastore restart on instance "
+                        "%s."), self.id)
         finally:
             self.reset_task_status()
 
     def guest_log_list(self):
-        LOG.info("Retrieving guest log list for instance %s.", self.id)
+        LOG.info(_("Retrieving guest log list for instance %s."), self.id)
         try:
             return self.guest.guest_log_list()
         except GuestError:
-            LOG.error("Failed to retrieve guest log list for instance "
-                      "%s.", self.id)
+            LOG.error(_("Failed to retrieve guest log list for instance "
+                        "%s."), self.id)
         finally:
             self.reset_task_status()
 
     def guest_log_action(self, log_name, enable, disable, publish, discard):
-        LOG.info("Processing guest log for instance %s.", self.id)
+        LOG.info(_("Processing guest log for instance %s."), self.id)
         try:
             return self.guest.guest_log_action(log_name, enable, disable,
                                                publish, discard)
         except GuestError:
-            LOG.error("Failed to process guest log for instance %s.",
+            LOG.error(_("Failed to process guest log for instance %s."),
                       self.id)
         finally:
             self.reset_task_status()
 
     def refresh_compute_server_info(self):
         """Refreshes the compute server field."""
         server = self.nova_client.servers.get(self.server.id)
@@ -1440,127 +1344,107 @@
 
     def set_datastore_status_to_paused(self):
         """
         Updates the InstanceServiceStatus for this BuiltInstance to PAUSED.
         This does not change the reference for this BuiltInstanceTask
         """
         datastore_status = InstanceServiceStatus.find_by(instance_id=self.id)
-        datastore_status.status = srvstatus.ServiceStatuses.PAUSED
+        datastore_status.status = rd_instance.ServiceStatuses.PAUSED
         datastore_status.save()
 
     def upgrade(self, datastore_version):
-        LOG.info("Upgrading instance %s to new datastore version %s",
-                 self.id, datastore_version)
+        LOG.debug("Upgrading instance %s to new datastore version %s",
+                  self, datastore_version)
 
-        self.set_service_status(srvstatus.ServiceStatuses.UPGRADING)
+        def server_finished_rebuilding():
+            self.refresh_compute_server_info()
+            return not self.server_status_matches(['REBUILD'])
 
         try:
             upgrade_info = self.guest.pre_upgrade()
-            upgrade_info = upgrade_info if upgrade_info else {}
-            upgrade_info.update({'datastore_version': datastore_version.name})
-            self.guest.upgrade(upgrade_info)
-
-            # Wait for db instance healthy
-            LOG.info('Waiting for instance %s to be healthy after upgrading',
-                     self.id)
-            utils.poll_until(self.is_service_healthy, time_out=600,
-                             sleep_time=5)
+
+            if self.volume_id:
+                volume = self.volume_client.volumes.get(self.volume_id)
+                volume_device = self._fix_device_path(
+                    volume.attachments[0]['device'])
+
+            # BUG(1650518): Cleanup in the Pike release some instances
+            # that we will be upgrading will be pre secureserialier
+            # and will have no instance_key entries. If this is one of
+            # those instances, make a key. That will make it appear in
+            # the injected files that are generated next. From this
+            # point, and until the guest comes up, attempting to send
+            # messages to it will fail because the RPC framework will
+            # encrypt messages to a guest which potentially doesn't
+            # have the code to handle it.
+            if CONF.enable_secure_rpc_messaging and (
+                    self.db_info.encrypted_key is None):
+                encrypted_key = cu.encode_data(cu.encrypt_data(
+                    cu.generate_random_key(),
+                    CONF.inst_rpc_key_encr_key))
+                self.update_db(encrypted_key=encrypted_key)
+                LOG.debug("Generated unique RPC encryption key for "
+                          "instance = %(id)s, key = %(key)s",
+                          {'id': self.id, 'key': encrypted_key})
+
+            injected_files = self.get_injected_files(
+                datastore_version.manager)
+            LOG.debug("Rebuilding instance %(instance)s with image %(image)s.",
+                      {'instance': self, 'image': datastore_version.image_id})
+            self.server.rebuild(datastore_version.image_id,
+                                files=injected_files)
+            utils.poll_until(
+                server_finished_rebuilding,
+                sleep_time=2, time_out=600)
+            if not self.server_status_matches(['ACTIVE']):
+                raise TroveError(_("Instance %(instance)s failed to "
+                                   "upgrade to %(datastore_version)s"),
+                                 {'instance': self,
+                                  'datastore_version': datastore_version})
+
+            if volume:
+                upgrade_info['device'] = volume_device
+
+            self.guest.post_upgrade(upgrade_info)
 
             self.reset_task_status()
-            LOG.info("Finished upgrading instance %s to new datastore "
-                     "version %s", self.id, datastore_version)
+
         except Exception as e:
-            LOG.error('Failed to upgrade instance %s, error: %s', self.id, e)
-            self.update_db(
-                task_status=inst_models.InstanceTasks.BUILDING_ERROR_SERVER)
+            LOG.exception(e)
+            err = inst_models.InstanceTasks.BUILDING_ERROR_SERVER
+            self.update_db(task_status=err)
+            raise e
 
+    # Some cinder drivers appear to return "vdb" instead of "/dev/vdb".
+    # We need to account for that.
     def _fix_device_path(self, device):
-        """Get correct device path.
-
-        Some cinder drivers appear to return "vdb" instead of "/dev/vdb".
-        """
         if device.startswith("/dev"):
             return device
         else:
             return "/dev/%s" % device
 
-    def update_access(self, access):
-        LOG.info(f"Updating access for instance {self.id}, access {access}")
-
-        new_is_public = access.get('is_public', False)
-        new_allowed_cidrs = access.get('allowed_cidrs', [])
-        is_public = (self.access.get('is_public', False) if self.access
-                     else None)
-        allowed_cidrs = (self.access.get('allowed_cidrs', []) if self.access
-                         else None)
-
-        ports = self.neutron_client.list_ports(
-            name='trove-%s' % self.id)['ports']
-
-        if is_public != new_is_public:
-            for port in ports:
-                if port['network_id'] not in CONF.management_networks:
-                    LOG.debug(f"Updating port {port['id']}, is_public: "
-                              f"{new_is_public}")
-                    neutron.ensure_port_access(self.neutron_client, port['id'],
-                                               new_is_public, self.tenant_id)
-
-        if CONF.trove_security_groups_support:
-            if allowed_cidrs != new_allowed_cidrs:
-                name = f"{CONF.trove_security_group_name_prefix}-{self.id}"
-                sgs = self.neutron_client.list_security_groups(
-                    name=name)['security_groups']
-
-                LOG.debug(f"Updating security group rules for instance "
-                          f"{self.id}")
-                for sg in sgs:
-                    neutron.clear_ingress_security_group_rules(
-                        self.neutron_client,
-                        sg['id'])
-
-                    if new_allowed_cidrs:
-                        tcp_ports = CONF.get(self.datastore.name).tcp_ports
-                        udp_ports = CONF.get(self.datastore.name).udp_ports
-
-                        neutron.create_security_group_rule(
-                            self.neutron_client, sg['id'], 'tcp', tcp_ports,
-                            new_allowed_cidrs)
-                        neutron.create_security_group_rule(
-                            self.neutron_client, sg['id'], 'udp', udp_ports,
-                            new_allowed_cidrs)
-        else:
-            LOG.warning('Security group not supported.')
-
-        LOG.info(f"Finished to update access for instance {self.id}")
-        self.update_db(
-            task_status=InstanceTasks.NONE,
-            access={'is_public': new_is_public,
-                    'allowed_cidrs': new_allowed_cidrs}
-        )
-
 
 class BackupTasks(object):
-
     @classmethod
     def _parse_manifest(cls, manifest):
         # manifest is in the format 'container/prefix'
         # where prefix can be 'path' or 'lots/of/paths'
         try:
             container_index = manifest.index('/')
             prefix_index = container_index + 1
         except ValueError:
             return None, None
         container = manifest[:container_index]
         prefix = manifest[prefix_index:]
         return container, prefix
 
     @classmethod
-    def delete_files_from_swift(cls, context, container, filename):
-        container = container or CONF.backup_swift_container
-        client = clients.create_swift_client(context)
+    def delete_files_from_swift(cls, context, filename):
+        container = CONF.backup_swift_container
+        client = remote.create_swift_client(context)
         obj = client.head_object(container, filename)
         if 'x-static-large-object' in obj:
             # Static large object
             LOG.debug("Deleting large object file: %(cont)s/%(filename)s",
                       {'cont': container, 'filename': filename})
             client.delete_object(container, filename,
                                  query_string='multipart-manifest=delete')
@@ -1569,57 +1453,45 @@
             LOG.debug("Deleting object file: %(cont)s/%(filename)s",
                       {'cont': container, 'filename': filename})
             client.delete_object(container, filename)
 
     @classmethod
     def delete_backup(cls, context, backup_id):
         """Delete backup from swift."""
-
-        def _delete(backup):
-            backup.deleted = True
-            backup.deleted_at = timeutils.utcnow()
-            # Set datastore_version_id to None to remove dependency.
-            backup.datastore_version_id = None
-            backup.save()
-
-        LOG.info("Deleting backup %s.", backup_id)
+        LOG.info(_("Deleting backup %s."), backup_id)
         backup = bkup_models.Backup.get_by_id(context, backup_id)
         try:
             filename = backup.filename
-            # Do not remove the object if the backup was restored from remote
-            # location.
-            if filename and backup.state != bkup_models.BackupState.RESTORED:
-                BackupTasks.delete_files_from_swift(context,
-                                                    backup.container_name,
-                                                    filename)
+            if filename:
+                BackupTasks.delete_files_from_swift(context, filename)
         except ValueError:
-            _delete(backup)
+            backup.delete()
         except ClientException as e:
             if e.http_status == 404:
                 # Backup already deleted in swift
-                _delete(backup)
+                backup.delete()
             else:
-                LOG.exception("Error occurred when deleting from swift. "
-                              "Details: %s", e)
+                LOG.exception(_("Error occurred when deleting from swift. "
+                                "Details: %s"), e)
                 backup.state = bkup_models.BackupState.DELETE_FAILED
                 backup.save()
                 raise TroveError(_("Failed to delete swift object for backup "
                                    "%s.") % backup_id)
         else:
-            _delete(backup)
-        LOG.info("Deleted backup %s successfully.", backup_id)
+            backup.delete()
+        LOG.info(_("Deleted backup %s successfully."), backup_id)
 
 
 class ModuleTasks(object):
 
     @classmethod
     def reapply_module(cls, context, module_id, md5, include_clustered,
                        batch_size, batch_delay, force):
         """Reapply module."""
-        LOG.info("Reapplying module %s.", module_id)
+        LOG.info(_("Reapplying module %s."), module_id)
 
         batch_size = batch_size or CONF.module_reapply_max_batch_size
         batch_delay = batch_delay or CONF.module_reapply_min_batch_delay
         # Don't let non-admin bypass the safeguards
         if not context.is_admin:
             batch_size = min(batch_size, CONF.module_reapply_max_batch_size)
             batch_delay = max(batch_delay, CONF.module_reapply_min_batch_delay)
@@ -1649,15 +1521,15 @@
                                 instance.datastore_version.id)
                             client = create_guest_client(context, instance_id)
                             client.module_apply(module_list)
                             Instance.add_instance_modules(
                                 context, instance_id, modules)
                             reapply_count += 1
                         except exception.ModuleInvalid as ex:
-                            LOG.info("Skipping: %s", ex)
+                            LOG.info(_("Skipping: %s"), ex)
                             skipped_count += 1
 
                         # Sleep if we've fired off too many in a row.
                         if (batch_size and
                                 not reapply_count % batch_size and
                                 (reapply_count + skipped_count) < total_count):
                             LOG.debug("Applied module to %(cnt)d of %(total)d "
@@ -1670,16 +1542,16 @@
                         LOG.debug("Instance '%s' not found or doesn't match "
                                   "criteria, skipping reapply.", instance_id)
                         skipped_count += 1
                 else:
                     LOG.debug("Instance '%s' does not match "
                               "criteria, skipping reapply.", instance_id)
                     skipped_count += 1
-        LOG.info("Reapplied module to %(num)d instances "
-                 "(skipped %(skip)d).",
+        LOG.info(_("Reapplied module to %(num)d instances "
+                   "(skipped %(skip)d)."),
                  {'num': reapply_count, 'skip': skipped_count})
 
 
 class ResizeVolumeAction(object):
     """Performs volume resize action."""
 
     def __init__(self, instance, old_size, new_size):
@@ -1692,80 +1564,79 @@
             self.instance.datastore_version.manager).mount_point
         return mount_point
 
     def get_device_path(self):
         return self.instance.device_path
 
     def _fail(self, orig_func):
-        LOG.error("%(func)s encountered an error when "
-                  "attempting to resize the volume for "
-                  "instance %(id)s. Setting service "
-                  "status to failed.", {'func': orig_func.__name__,
-                                        'id': self.instance.id})
+        LOG.exception(_("%(func)s encountered an error when "
+                        "attempting to resize the volume for "
+                        "instance %(id)s. Setting service "
+                        "status to failed."), {'func': orig_func.__name__,
+                                               'id': self.instance.id})
         service = InstanceServiceStatus.find_by(instance_id=self.instance.id)
-        service.set_status(srvstatus.ServiceStatuses.FAILED)
+        service.set_status(ServiceStatuses.FAILED)
         service.save()
 
     def _recover_restart(self, orig_func):
-        LOG.exception("%(func)s encountered an error when attempting to "
-                      "resize the volume for instance %(id)s. Trying to "
-                      "recover by restarting the "
-                      "guest.", {'func': orig_func.__name__,
-                                 'id': self.instance.id})
+        LOG.exception(_("%(func)s encountered an error when attempting to "
+                        "resize the volume for instance %(id)s. Trying to "
+                        "recover by restarting the "
+                        "guest."), {'func': orig_func.__name__,
+                                    'id': self.instance.id})
         self.instance.restart()
 
     def _recover_mount_restart(self, orig_func):
-        LOG.exception("%(func)s encountered an error when attempting to "
-                      "resize the volume for instance %(id)s. Trying to "
-                      "recover by mounting the volume and then restarting "
-                      "the guest.", {'func': orig_func.__name__,
-                                     'id': self.instance.id})
+        LOG.exception(_("%(func)s encountered an error when attempting to "
+                        "resize the volume for instance %(id)s. Trying to "
+                        "recover by mounting the volume and then restarting "
+                        "the guest."), {'func': orig_func.__name__,
+                                        'id': self.instance.id})
         self._mount_volume()
         self.instance.restart()
 
     def _recover_full(self, orig_func):
-        LOG.error("%(func)s encountered an error when attempting to "
-                  "resize the volume for instance %(id)s. Trying to "
-                  "recover by attaching and"
-                  " mounting the volume and then restarting the "
-                  "guest.", {'func': orig_func.__name__,
-                             'id': self.instance.id})
+        LOG.exception(_("%(func)s encountered an error when attempting to "
+                        "resize the volume for instance %(id)s. Trying to "
+                        "recover by attaching and"
+                        " mounting the volume and then restarting the "
+                        "guest."), {'func': orig_func.__name__,
+                                    'id': self.instance.id})
         self._attach_volume()
         self._mount_volume()
         self.instance.restart()
 
     def _stop_db(self):
         LOG.debug("Instance %s calling stop_db.", self.instance.id)
         self.instance.guest.stop_db()
 
     @try_recover
     def _unmount_volume(self):
         LOG.debug("Unmounting the volume on instance %(id)s", {
-            'id': self.instance.id})
+                  'id': self.instance.id})
         mount_point = self.get_mount_point()
         device_path = self.get_device_path()
         self.instance.guest.unmount_volume(device_path=device_path,
                                            mount_point=mount_point)
         LOG.debug("Successfully unmounted the volume %(vol_id)s for "
                   "instance %(id)s", {'vol_id': self.instance.volume_id,
                                       'id': self.instance.id})
 
     @try_recover
     def _detach_volume(self):
         LOG.debug("Detach volume %(vol_id)s from instance %(id)s", {
-            'vol_id': self.instance.volume_id,
-            'id': self.instance.id})
+                  'vol_id': self.instance.volume_id,
+                  'id': self.instance.id})
         self.instance.nova_client.volumes.delete_server_volume(
             self.instance.server.id, self.instance.volume_id)
 
         def volume_available():
             volume = self.instance.volume_client.volumes.get(
                 self.instance.volume_id)
             return volume.status == 'available'
-
         utils.poll_until(volume_available,
                          sleep_time=2,
                          time_out=CONF.volume_time_out)
 
         LOG.debug("Successfully detached volume %(vol_id)s from instance "
                   "%(id)s", {'vol_id': self.instance.volume_id,
                              'id': self.instance.id})
@@ -1779,328 +1650,306 @@
         self.instance.nova_client.volumes.create_server_volume(
             self.instance.server.id, self.instance.volume_id, device_path)
 
         def volume_in_use():
             volume = self.instance.volume_client.volumes.get(
                 self.instance.volume_id)
             return volume.status == 'in-use'
-
         utils.poll_until(volume_in_use,
                          sleep_time=2,
                          time_out=CONF.volume_time_out)
 
         LOG.debug("Successfully attached volume %(vol_id)s to instance "
                   "%(id)s", {'vol_id': self.instance.volume_id,
                              'id': self.instance.id})
 
     @try_recover
-    def _resize_fs(self, online=False):
-        LOG.info(f"Resizing the filesystem for instance {self.instance.id}, "
-                 f"online: {online}")
+    def _resize_fs(self):
+        LOG.debug("Resizing the filesystem for instance %(id)s", {
+                  'id': self.instance.id})
         mount_point = self.get_mount_point()
         device_path = self.get_device_path()
         self.instance.guest.resize_fs(device_path=device_path,
-                                      mount_point=mount_point,
-                                      online=online)
-        LOG.debug(f"Successfully resized volume {self.instance.volume_id} "
-                  f"filesystem for instance {self.instance.id}")
+                                      mount_point=mount_point)
+        LOG.debug("Successfully resized volume %(vol_id)s filesystem for "
+                  "instance %(id)s", {'vol_id': self.instance.volume_id,
+                                      'id': self.instance.id})
 
     @try_recover
     def _mount_volume(self):
         LOG.debug("Mount the volume on instance %(id)s", {
-            'id': self.instance.id})
+                  'id': self.instance.id})
         mount_point = self.get_mount_point()
         device_path = self.get_device_path()
         self.instance.guest.mount_volume(device_path=device_path,
                                          mount_point=mount_point)
         LOG.debug("Successfully mounted the volume %(vol_id)s on instance "
                   "%(id)s", {'vol_id': self.instance.volume_id,
                              'id': self.instance.id})
 
     @try_recover
     def _extend(self):
-        LOG.info(f"Calling Cinder to extend volume {self.instance.volume_id} "
-                 f"for instance {self.instance.id} to size {self.new_size}")
+        LOG.debug("Extending volume %(vol_id)s for instance %(id)s to "
+                  "size %(size)s", {'vol_id': self.instance.volume_id,
+                                    'id': self.instance.id,
+                                    'size': self.new_size})
         self.instance.volume_client.volumes.extend(self.instance.volume_id,
                                                    self.new_size)
         LOG.debug("Successfully extended the volume %(vol_id)s for instance "
                   "%(id)s", {'vol_id': self.instance.volume_id,
                              'id': self.instance.id})
 
     def _verify_extend(self):
         try:
             volume = self.instance.volume_client.volumes.get(
                 self.instance.volume_id)
             if not volume:
-                msg = f'Failed to get volume {self.instance.volume_id}'
-                raise exception.TroveError(msg)
+                msg = (_('Failed to get volume %(vol_id)s') % {
+                       'vol_id': self.instance.volume_id})
+                raise cinder_exceptions.ClientException(msg)
 
             def volume_is_new_size():
                 volume = self.instance.volume_client.volumes.get(
                     self.instance.volume_id)
-                LOG.debug(f'Waiting for volume available, '
-                          f'id: {volume.id}, status: {volume.status}, '
-                          f'size: {volume.size}')
-                return (volume.size == self.new_size and
-                        volume.status in ['available', 'in-use'])
-
+                return volume.size == self.new_size
             utils.poll_until(volume_is_new_size,
-                             initial_delay=5,
-                             sleep_time=5,
+                             sleep_time=2,
                              time_out=CONF.volume_time_out)
 
             self.instance.update_db(volume_size=self.new_size)
         except PollTimeOut:
-            LOG.error("Timeout trying to extend the volume %(vol_id)s "
-                      "for instance %(id)s",
-                      {'vol_id': self.instance.volume_id,
-                       'id': self.instance.id})
+            LOG.exception(_("Timeout trying to extend the volume %(vol_id)s "
+                          "for instance %(id)s"), {
+                          'vol_id': self.instance.volume_id,
+                          'id': self.instance.id})
             volume = self.instance.volume_client.volumes.get(
                 self.instance.volume_id)
             if volume.status == 'extending':
                 self._fail(self._verify_extend)
             elif volume.size != self.new_size:
                 self.instance.update_db(volume_size=volume.size)
-                if not CONF.online_volume_resize:
-                    self._recover_full(self._verify_extend)
-            raise
-        except Exception as e:
-            LOG.error("Error encountered trying to verify extend for "
-                      "the volume %(vol_id)s for instance %(id)s, "
-                      "error: %(error)s",
-                      {'vol_id': self.instance.volume_id,
-                       'id': self.instance.id,
-                       'error': str(e)})
-            if not CONF.online_volume_resize:
                 self._recover_full(self._verify_extend)
             raise
+        except Exception:
+            LOG.exception(_("Error encountered trying to verify extend for "
+                          "the volume %(vol_id)s for instance %(id)s"), {
+                          'vol_id': self.instance.volume_id,
+                          'id': self.instance.id})
+            self._recover_full(self._verify_extend)
+            raise
 
     def _resize_active_volume(self):
-        if CONF.online_volume_resize:
-            try:
-                self._extend()
-            except Exception as e:
-                LOG.error(f'Failed to extend volume, error: {str(e)}')
-
-            self._verify_extend()
-            self._resize_fs(recover_func=self._fail, online=True)
-            return
-
+        LOG.debug("Begin _resize_active_volume for id: %(id)s", {
+                  'id': self.instance.id})
         self._stop_db()
         self._unmount_volume(recover_func=self._recover_restart)
         self._detach_volume(recover_func=self._recover_mount_restart)
         self._extend(recover_func=self._recover_full)
         self._verify_extend()
         # if anything fails after this point, recovery is futile
         self._attach_volume(recover_func=self._fail)
-        self._resize_fs(recover_func=self._fail, online=False)
+        self._resize_fs(recover_func=self._fail)
         self._mount_volume(recover_func=self._fail)
         self.instance.restart()
+        LOG.debug("End _resize_active_volume for id: %(id)s", {
+                  'id': self.instance.id})
 
     def execute(self):
         LOG.debug("%(gt)s: Resizing instance %(id)s volume for server "
                   "%(server_id)s from %(old_volume_size)s to "
                   "%(new_size)r GB", {'gt': greenthread.getcurrent(),
                                       'id': self.instance.id,
                                       'server_id': self.instance.server.id,
                                       'old_volume_size': self.old_size,
                                       'new_size': self.new_size})
 
-        if self.instance.server.status in [InstanceStatus.ACTIVE,
-                                           InstanceStatus.HEALTHY]:
-            try:
-                self._resize_active_volume()
-            finally:
-                self.instance.reset_task_status()
-
+        if self.instance.server.status == InstanceStatus.ACTIVE:
+            self._resize_active_volume()
+            self.instance.reset_task_status()
+            # send usage event for size reported by cinder
+            volume = self.instance.volume_client.volumes.get(
+                self.instance.volume_id)
+            launched_time = timeutils.isotime(self.instance.updated)
+            modified_time = timeutils.isotime(self.instance.updated)
+            TroveInstanceModifyVolume(instance=self.instance,
+                                      old_volume_size=self.old_size,
+                                      launched_at=launched_time,
+                                      modify_at=modified_time,
+                                      volume_size=volume.size,
+                                      ).notify()
         else:
             self.instance.reset_task_status()
-            msg = (
-                "Failed to resize instance %(id)s volume for server "
-                "%(server_id)s. The instance must be in state %(state)s "
-                "not %(inst_state)s." %
-                {
-                    'id': self.instance.id,
-                    'server_id': self.instance.server.id,
-                    'state': [InstanceStatus.ACTIVE, InstanceStatus.HEALTHY],
-                    'inst_state': self.instance.server.status
-                }
-            )
+            msg = _("Failed to resize instance %(id)s volume for server "
+                    "%(server_id)s. The instance must be in state %(state)s "
+                    "not %(inst_state)s.") % {
+                        'id': self.instance.id,
+                        'server_id': self.instance.server.id,
+                        'state': InstanceStatus.ACTIVE,
+                        'inst_state': self.instance.server.status}
             raise TroveError(msg)
 
 
 class ResizeActionBase(object):
     """Base class for executing a resize action."""
 
     def __init__(self, instance):
         """
         Creates a new resize action for a given instance
         :param instance: reference to existing instance that will be resized
         :type instance: trove.taskmanager.models.BuiltInstanceTasks
         """
         self.instance = instance
-        self.wait_status = ['VERIFY_RESIZE']
-        self.ignore_stop_error = False
 
     def _assert_guest_is_ok(self):
         # The guest will never set the status to PAUSED.
         self.instance.set_datastore_status_to_paused()
         # Now we wait until it sets it to anything at all,
         # so we know it's alive.
         utils.poll_until(
             self._guest_is_awake,
-            sleep_time=3,
-            time_out=CONF.resize_time_out)
+            sleep_time=2,
+            time_out=RESIZE_TIME_OUT)
 
     def _assert_nova_status_is_ok(self):
         # Make sure Nova thinks things went well.
         if not self.instance.server_status_matches(["VERIFY_RESIZE"]):
             msg = "Migration failed! status=%(act_status)s and " \
                   "not %(exp_status)s" % {
                       "act_status": self.instance.server.status,
                       "exp_status": 'VERIFY_RESIZE'}
             raise TroveError(msg)
 
-    def _assert_nova_action_was_successful(self):
-        pass
-
     def _assert_datastore_is_ok(self):
-        LOG.info(f"Re-config database for instance {self.instance.id} after "
-                 f"resize")
+        # Tell the guest to turn on datastore, and ensure the status becomes
+        # RUNNING.
         self._start_datastore()
+        utils.poll_until(
+            self._datastore_is_online,
+            sleep_time=2,
+            time_out=RESIZE_TIME_OUT)
+
+    def _assert_datastore_is_offline(self):
+        # Tell the guest to turn off MySQL, and ensure the status becomes
+        # SHUTDOWN.
+        self.instance.guest.stop_db(do_not_start_on_reboot=True)
+        utils.poll_until(
+            self._datastore_is_offline,
+            sleep_time=2,
+            time_out=RESIZE_TIME_OUT)
 
     def _assert_processes_are_ok(self):
         """Checks the procs; if anything is wrong, reverts the operation."""
         # Tell the guest to turn back on, and make sure it can start.
-        LOG.info(f"Waiting for database status changed after resizing "
-                 f"{self.instance.id}")
         self._assert_guest_is_ok()
+        LOG.debug("Nova guest is ok.")
         self._assert_datastore_is_ok()
+        LOG.debug("Datastore is ok.")
 
     def _confirm_nova_action(self):
         LOG.debug("Instance %s calling Compute confirm resize...",
                   self.instance.id)
-        self.instance.refresh_compute_server_info()
-        if self.instance.server_status_matches(["VERIFY_RESIZE"]):
-            self.instance.server.confirm_resize()
+        self.instance.server.confirm_resize()
 
     def _datastore_is_online(self):
         self.instance._refresh_datastore_status()
         return self.instance.is_datastore_running
 
     def _datastore_is_offline(self):
         self.instance._refresh_datastore_status()
         return (self.instance.datastore_status_matches(
-            srvstatus.ServiceStatuses.SHUTDOWN))
+                rd_instance.ServiceStatuses.SHUTDOWN))
 
     def _revert_nova_action(self):
         LOG.debug("Instance %s calling Compute revert resize...",
                   self.instance.id)
         self.instance.server.revert_resize()
 
-    def _record_action_success(self):
-        pass
-
     def execute(self):
         """Initiates the action."""
         try:
-            LOG.info(f"Stopping database service for {self.instance.id}")
-            self.instance.guest.stop_db(do_not_start_on_reboot=True)
-        except Exception as e:
-            if self.ignore_stop_error:
-                LOG.warning(f"Failed to stop db {self.instance.id}, error: "
-                            f"{str(e)}")
-            else:
-                raise
-
-        try:
+            LOG.debug("Instance %s calling stop_db...", self.instance.id)
+            self._assert_datastore_is_offline()
             self._perform_nova_action()
         finally:
             if self.instance.db_info.task_status != (
                     inst_models.InstanceTasks.NONE):
                 self.instance.reset_task_status()
 
     def _guest_is_awake(self):
         self.instance._refresh_datastore_status()
         return not self.instance.datastore_status_matches(
-            srvstatus.ServiceStatuses.PAUSED)
-
-    def _guest_is_healthy(self):
-        self.instance._refresh_datastore_status()
-        return self.instance.datastore_status_matches(
-            srvstatus.ServiceStatuses.HEALTHY)
-
-    def wait_for_healthy(self):
-        utils.poll_until(
-            self._guest_is_healthy,
-            sleep_time=3,
-            time_out=CONF.resize_time_out)
+            rd_instance.ServiceStatuses.PAUSED)
 
     def _perform_nova_action(self):
         """Calls Nova to resize or migrate an instance, and confirms."""
+        LOG.debug("Begin resize method _perform_nova_action instance: %s",
+                  self.instance.id)
         need_to_revert = False
         try:
+            LOG.debug("Initiating nova action")
             self._initiate_nova_action()
+            LOG.debug("Waiting for nova action")
             self._wait_for_nova_action()
+            LOG.debug("Asserting nova status is ok")
             self._assert_nova_status_is_ok()
             need_to_revert = True
+            LOG.debug("* * * REVERT BARRIER PASSED * * *")
+            LOG.debug("Asserting nova action success")
             self._assert_nova_action_was_successful()
+            LOG.debug("Asserting processes are OK")
             self._assert_processes_are_ok()
+            LOG.debug("Confirming nova action")
             self._confirm_nova_action()
         except Exception:
-            LOG.exception(f"Failed to resize instance {self.instance.id}")
+            LOG.exception(_("Exception during nova action."))
             if need_to_revert:
-                LOG.error("Reverting action for instance %s",
+                LOG.error(_("Reverting action for instance %s"),
                           self.instance.id)
                 self._revert_nova_action()
                 self._wait_for_revert_nova_action()
 
             if self.instance.server_status_matches(['ACTIVE']):
-                LOG.error(f"Restarting instance {self.instance.id}")
+                LOG.error(_("Restarting datastore."))
                 self.instance.guest.restart()
             else:
-                LOG.error(f"Cannot restart instance {self.instance.id} "
-                          f"because Nova server status is not ACTIVE")
+                LOG.error(_("Cannot restart datastore because "
+                            "Nova server status is not ACTIVE"))
 
+            LOG.error(_("Error resizing instance %s."), self.instance.id)
             raise
 
+        LOG.debug("Recording success")
         self._record_action_success()
         LOG.debug("End resize method _perform_nova_action instance: %s",
                   self.instance.id)
 
     def _wait_for_nova_action(self):
-        LOG.debug(f"Waiting for Nova server status changed to "
-                  f"{self.wait_status} for {self.instance.id}")
-
+        # Wait for the flavor to change.
         def update_server_info():
             self.instance.refresh_compute_server_info()
-            if self.instance.server.status.upper() == 'ERROR':
-                raise TroveError("Nova server is in ERROR status")
-            return self.instance.server_status_matches(self.wait_status)
+            return not self.instance.server_status_matches(['RESIZE'])
 
         utils.poll_until(
             update_server_info,
-            sleep_time=5,
-            time_out=CONF.resize_time_out,
-            initial_delay=10)
+            sleep_time=2,
+            time_out=RESIZE_TIME_OUT)
 
     def _wait_for_revert_nova_action(self):
         # Wait for the server to return to ACTIVE after revert.
         def update_server_info():
             self.instance.refresh_compute_server_info()
             return self.instance.server_status_matches(['ACTIVE'])
 
         utils.poll_until(
             update_server_info,
             sleep_time=2,
-            time_out=CONF.revert_time_out)
+            time_out=REVERT_TIME_OUT)
 
 
 class ResizeAction(ResizeActionBase):
-
     def __init__(self, instance, old_flavor, new_flavor):
         """
         :type instance: trove.taskmanager.models.BuiltInstanceTasks
         :type old_flavor: dict
         :type new_flavor: dict
         """
         super(ResizeAction, self).__init__(instance)
@@ -2112,26 +1961,25 @@
         # Do check to make sure the status and flavor id are correct.
         if str(self.instance.server.flavor['id']) != str(self.new_flavor_id):
             msg = "Assertion failed! flavor_id=%s and not %s" \
                   % (self.instance.server.flavor['id'], self.new_flavor_id)
             raise TroveError(msg)
 
     def _initiate_nova_action(self):
-        LOG.info(f"Resizing Nova server for instance {self.instance.id}")
         self.instance.server.resize(self.new_flavor_id)
 
     def _revert_nova_action(self):
         LOG.debug("Instance %s calling Compute revert resize... "
                   "Repairing config.", self.instance.id)
         try:
             config = self.instance._render_config(self.old_flavor)
             config = {'config_contents': config.config_contents}
             self.instance.guest.reset_configuration(config)
         except GuestTimeout:
-            LOG.exception("Error sending reset_configuration call.")
+            LOG.exception(_("Error sending reset_configuration call."))
         LOG.debug("Reverting resize.")
         super(ResizeAction, self)._revert_nova_action()
 
     def _record_action_success(self):
         LOG.debug("Updating instance %(id)s to flavor_id %(flavor_id)s.",
                   {'id': self.instance.id, 'flavor_id': self.new_flavor_id})
         self.instance.update_db(flavor_id=self.new_flavor_id,
@@ -2142,21 +1990,18 @@
                                   instance_size=self.new_flavor['ram'],
                                   launched_at=update_time,
                                   modify_at=update_time,
                                   server=self.instance.server).notify()
 
     def _start_datastore(self):
         config = self.instance._render_config(self.new_flavor)
-        self.instance.guest.start_db_with_conf_changes(
-            config.config_contents,
-            self.instance.datastore_version.name)
+        self.instance.guest.start_db_with_conf_changes(config.config_contents)
 
 
 class MigrateAction(ResizeActionBase):
-
     def __init__(self, instance, host=None):
         super(MigrateAction, self).__init__(instance)
         self.instance = instance
         self.host = host
 
     def _assert_nova_action_was_successful(self):
         LOG.debug("Currently no assertions for a Migrate Action")
@@ -2175,103 +2020,13 @@
                   {'hostname': self.instance.hostname,
                    'id': self.instance.id})
 
     def _start_datastore(self):
         self.instance.guest.restart()
 
 
-class RebuildAction(ResizeActionBase):
-
-    def __init__(self, instance, image_id):
-        """The action to perform rebuild.
-
-        :param instance: A BuiltInstanceTasks instance.
-        :param image_id: Image ID.
-        """
-        super(RebuildAction, self).__init__(instance)
-        self.image_id = image_id
-        self.ignore_stop_error = True
-        self.wait_status = ['ACTIVE']
-
-    def _initiate_nova_action(self):
-        files = self.instance.get_injected_files(
-            self.instance.datastore.name,
-            self.instance.datastore_version.name)
-
-        LOG.info(f"Rebuilding Nova server for instance {self.instance.id}")
-        # Before Nova version 2.57, userdata is not supported when doing
-        # rebuild, have to use injected files instead.
-        self.instance.server.rebuild(
-            self.image_id,
-            files=files,
-        )
-
-    def _assert_nova_status_is_ok(self):
-        pass
-
-    def _assert_nova_action_was_successful(self):
-        if self.instance.server.image['id'] != self.image_id:
-            msg = (f"Assertion failed! The service image ID is "
-                   f"{self.instance.server.image['id']} not {self.image_id}")
-            raise TroveError(msg)
-
-    def _assert_processes_are_ok(self):
-        """Send rebuild async request to the guest and wait."""
-        context = self.instance.context
-        flavor = self.instance.nova_client.flavors.get(self.instance.flavor_id)
-        config = self.instance._render_config(flavor)
-        config_contents = config.config_contents
-
-        overrides = {}
-        if self.instance.configuration:
-            user_config = config_models.Configuration(
-                context, self.instance.configuration.id)
-            overrides = user_config.get_configuration_overrides()
-
-        LOG.info(f"Sending rebuild request to the instance {self.instance.id}")
-        self.instance.guest.rebuild(
-            self.instance.datastore_version.version,
-            config_contents=config_contents, config_overrides=overrides)
-
-        LOG.info(f"Waiting for instance {self.instance.id} healthy after "
-                 f"rebuild")
-        self._assert_guest_is_ok()
-        self.wait_for_healthy()
-
-        # Restore replication config for primary
-        if self.instance.slaves:
-            LOG.info(f"Enabling {self.instance.id} as primay after rebuild")
-            self.instance.enable_as_master()
-
-            # Restart all the replicas to reconnect with the primary
-            for replica in self.instance.slaves:
-                LOG.info(f"Restarting replica {replica.id} after rebuild "
-                         f"primary {self.instance.id}")
-                replica_tasks = BuiltInstanceTasks.load(context, replica.id)
-                replica_tasks.restart()
-
-        # Restore replication config for replica
-        elif self.instance.slave_of_id:
-            LOG.info(f"Re-attaching replica {self.instance.id} after rebuild")
-            primary = BuiltInstanceTasks.load(
-                context, self.instance.slave_of_id)
-            self.instance.detach_replica(primary, for_failover=True)
-            self.instance.attach_replica(primary, restart=True)
-
-        LOG.info(f"Finished to rebuild {self.instance.id}")
-
-    def _revert_nova_action(self):
-        pass
-
-    def _wait_for_revert_nova_action(self):
-        pass
-
-    def _confirm_nova_action(self):
-        pass
-
-
 def load_cluster_tasks(context, cluster_id):
     manager = Cluster.manager_from_cluster_id(context, cluster_id)
     strat = strategy.load_taskmanager_strategy(manager)
     task_manager_cluster_tasks_class = strat.task_manager_cluster_tasks_class
     return ClusterTasks.load(context, cluster_id,
                              task_manager_cluster_tasks_class)
```

### Comparing `trove-21.0.0.0rc2/trove/taskmanager/service.py` & `trove-8.0.1/trove/version.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,14 +9,10 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+import pbr.version
 
-class TaskService(object):
-    """Task Manager interface."""
-
-
-def app_factory(global_conf, **local_conf):
-    return TaskService()
+version_info = pbr.version.VersionInfo('trove')
```

### Comparing `trove-21.0.0.0rc2/trove/templates/cassandra/config.template` & `trove-8.0.1/trove/templates/cassandra/config.template`

 * *Files 0% similar despite different names*

```diff
@@ -552,15 +552,15 @@
 tombstone_warn_threshold: 1000
 tombstone_failure_threshold: 100000
 
 # Granularity of the collation index of rows within a partition.
 # Increase if your rows are large, or if you have a very large
 # number of rows per partition.  The competing goals are these:
 #   1) a smaller granularity means more index entries are generated
-#      and looking up rows within the partition by collation column
+#      and looking up rows withing the partition by collation column
 #      is faster
 #   2) but, Cassandra will keep the collation index in memory for hot
 #      rows (as part of the key cache), so a larger granularity means
 #      you can cache more hot rows
 column_index_size_in_kb: 64
```

### Comparing `trove-21.0.0.0rc2/trove/templates/cassandra/validation-rules.json` & `trove-8.0.1/trove/templates/cassandra/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/db2/validation-rules.json` & `trove-8.0.1/trove/templates/db2/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/mariadb/cluster.config.template` & `trove-8.0.1/trove/templates/mariadb/cluster.config.template`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/mariadb/config.template` & `trove-8.0.1/trove/templates/percona/config.template`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 [client]
 port = 3306
 socket = /var/run/mysqld/mysqld.sock
 
 [mysqld_safe]
+pid-file = /var/run/mysqld/mysqld.pid
+socket = /var/run/mysqld/mysqld.sock
 nice = 0
 
 [mysqld]
+user = mysql
 port = 3306
 basedir = /usr
 datadir = /var/lib/mysql/data
 tmpdir = /var/tmp
-pid_file = /var/run/mysqld/mysqld.pid
+pid-file = /var/run/mysqld/mysqld.pid
 socket = /var/run/mysqld/mysqld.sock
 skip-external-locking = 1
 key_buffer_size = {{ (50 * flavor['ram']/512)|int }}M
 max_allowed_packet = {{ (1024 * flavor['ram']/512)|int }}K
 thread_stack = 192K
 thread_cache_size = {{ (4 * flavor['ram']/512)|int }}
 myisam-recover-options = BACKUP,FORCE
```

### Comparing `trove-21.0.0.0rc2/trove/templates/mariadb/validation-rules.json` & `trove-8.0.1/trove/templates/mariadb/validation-rules.json`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.975%*

 * *Differences: {"'configuration-parameters'": '{delete: [12, 10]}'}*

```diff
@@ -67,33 +67,21 @@
             "max": 2,
             "min": 0,
             "name": "innodb_flush_log_at_trx_commit",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "innodb_flush_method",
-            "restart_required": true,
-            "type": "string"
-        },
-        {
             "max": 4294967295,
             "min": 262144,
             "name": "innodb_log_buffer_size",
             "restart_required": true,
             "type": "integer"
         },
         {
-            "max": 274877906944,
-            "min": 4194304,
-            "name": "innodb_log_file_size",
-            "restart_required": true,
-            "type": "integer"
-        },
-        {
             "max": 4294967295,
             "min": 10,
             "name": "innodb_open_files",
             "restart_required": true,
             "type": "integer"
         },
         {
```

### Comparing `trove-21.0.0.0rc2/trove/templates/mongodb/validation-rules.json` & `trove-8.0.1/trove/templates/mongodb/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/mysql/config.template` & `trove-8.0.1/trove/templates/pxc/config.template`

 * *Files 7% similar despite different names*

```diff
@@ -4,28 +4,30 @@
 
 [mysqld_safe]
 pid-file = /var/run/mysqld/mysqld.pid
 socket = /var/run/mysqld/mysqld.sock
 nice = 0
 
 [mysqld]
+user = mysql
 port = 3306
 basedir = /usr
 datadir = /var/lib/mysql/data
-secure-file-priv = NULL
 tmpdir = /var/tmp
 pid-file = /var/run/mysqld/mysqld.pid
 socket = /var/run/mysqld/mysqld.sock
-default_authentication_plugin = mysql_native_password
 skip-external-locking = 1
 key_buffer_size = {{ (50 * flavor['ram']/512)|int }}M
 max_allowed_packet = {{ (1024 * flavor['ram']/512)|int }}K
 thread_stack = 192K
 thread_cache_size = {{ (4 * flavor['ram']/512)|int }}
 myisam-recover-options = BACKUP,FORCE
+query_cache_type = 1
+query_cache_limit = 1M
+query_cache_size = {{ (8 * flavor['ram']/512)|int }}M
 innodb_data_file_path = ibdata1:10M:autoextend
 innodb_buffer_pool_size = {{ (150 * flavor['ram']/512)|int }}M
 innodb_file_per_table = 1
 innodb_log_files_in_group = 2
 innodb_log_file_size=50M
 innodb_log_buffer_size=25M
 connect_timeout = 15
@@ -40,15 +42,14 @@
 table_definition_cache = {{ (256 * flavor['ram']/512)|int }}
 open_files_limit = {{ (512 * flavor['ram']/512)|int }}
 max_user_connections = {{ (100 * flavor['ram']/512)|int }}
 max_connections = {{ (100 * flavor['ram']/512)|int }}
 default_storage_engine = innodb
 local-infile = 0
 server_id = {{server_id}}
-performance_schema = ON
 
 [mysqldump]
 quick = 1
 quote-names = 1
 max_allowed_packet = 16M
 
 [isamchk]
```

### Comparing `trove-21.0.0.0rc2/trove/templates/percona/config.template` & `trove-8.0.1/trove/templates/mysql/config.template`

 * *Files 8% similar despite different names*

```diff
@@ -42,14 +42,15 @@
 table_definition_cache = {{ (256 * flavor['ram']/512)|int }}
 open_files_limit = {{ (512 * flavor['ram']/512)|int }}
 max_user_connections = {{ (100 * flavor['ram']/512)|int }}
 max_connections = {{ (100 * flavor['ram']/512)|int }}
 default_storage_engine = innodb
 local-infile = 0
 server_id = {{server_id}}
+performance_schema = ON
 
 [mysqldump]
 quick = 1
 quote-names = 1
 max_allowed_packet = 16M
 
 [isamchk]
```

### Comparing `trove-21.0.0.0rc2/trove/templates/percona/validation-rules.json` & `trove-8.0.1/trove/templates/percona/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/postgresql/config.template` & `trove-8.0.1/trove/templates/postgresql/config.template`

 * *Files 13% similar despite different names*

```diff
@@ -1,7 +1,10 @@
+# Pre-compute values used by the template expressions.
+# Note: The variables have to be in lists due to how scoping works in JINJA templates.
+#
 # The recommended amount for 'shared_buffers' on a dedicated database server is 25% of RAM.
 # Servers with less than 3GB of RAM require a more conservative value to save memory for other processes.
 {% set shared_buffers_mb = [(0.25 if flavor['ram'] >= 3072 else 0.10) * flavor['ram']] %}
 #
 # -----------------------------
 # PostgreSQL configuration file
 # -----------------------------
@@ -16,108 +19,113 @@
 #
 # The commented-out settings shown in this file represent the default values.
 # Re-commenting a setting is NOT sufficient to revert it to the default value;
 # you need to reload the server.
 #
 # This file is read on server startup and when the server receives a SIGHUP
 # signal.  If you edit the file on a running system, you have to SIGHUP the
-# server for the changes to take effect, run "pg_ctl reload", or execute
-# "SELECT pg_reload_conf()".  Some parameters, which are marked below,
-# require a server shutdown and restart to take effect.
+# server for the changes to take effect, or use "pg_ctl reload".  Some
+# parameters, which are marked below, require a server shutdown and restart to
+# take effect.
 #
 # Any parameter can also be given as a command-line option to the server, e.g.,
 # "postgres -c log_connections=on".  Some parameters can be changed at run time
 # with the "SET" SQL command.
 #
 # Memory units:  kB = kilobytes        Time units:  ms  = milliseconds
 #                MB = megabytes                     s   = seconds
 #                GB = gigabytes                     min = minutes
 #                TB = terabytes                     h   = hours
 #                                                   d   = days
-
+#
+# The properties marked as controlled by Trove are managed by the Trove
+# guest-agent. Any changes to them will be overwritten.
 
 #------------------------------------------------------------------------------
 # FILE LOCATIONS
 #------------------------------------------------------------------------------
 
 # The default values of these variables are driven from the -D command-line
 # option or PGDATA environment variable, represented here as ConfigDir.
 
 #data_directory = 'ConfigDir'		# use data in another directory
 					# (change requires restart)
+					# (controlled by Trove)
 #hba_file = 'ConfigDir/pg_hba.conf'	# host-based authentication file
 					# (change requires restart)
+					# (controlled by Trove)
 #ident_file = 'ConfigDir/pg_ident.conf'	# ident configuration file
 					# (change requires restart)
+					# (controlled by Trove)
 
 # If external_pid_file is not explicitly set, no extra PID file is written.
 #external_pid_file = ''			# write an extra PID file
 					# (change requires restart)
+					# (controlled by Trove)
 
 
 #------------------------------------------------------------------------------
 # CONNECTIONS AND AUTHENTICATION
 #------------------------------------------------------------------------------
 
 # - Connection Settings -
 
-listen_addresses = '*'
+#listen_addresses = 'localhost'		# what IP address(es) to listen on;
 					# comma-separated list of addresses;
 					# defaults to 'localhost'; use '*' for all
 					# (change requires restart)
+					# (controlled by Trove)
 #port = 5432				# (change requires restart)
-max_connections = 100			# (change requires restart)
+					# (controlled by Trove)
+#max_connections = 100			# (change requires restart)
+# Note:  Increasing max_connections costs ~400 bytes of shared memory per
+# connection slot, plus lock space (see max_locks_per_transaction).
 #superuser_reserved_connections = 3	# (change requires restart)
-#unix_socket_directories = '/var/run/postgresql'	# comma-separated list of directories
+#unix_socket_directories = '/tmp'	# comma-separated list of directories
 					# (change requires restart)
+					# (controlled by Trove)
 #unix_socket_group = ''			# (change requires restart)
+					# (controlled by Trove)
 #unix_socket_permissions = 0777		# begin with 0 to use octal notation
 					# (change requires restart)
+					# (controlled by Trove)
 #bonjour = off				# advertise server via Bonjour
 					# (change requires restart)
 #bonjour_name = ''			# defaults to the computer name
 					# (change requires restart)
 
-# - TCP settings -
-# see "man 7 tcp" for details
-
-#tcp_keepalives_idle = 0		# TCP_KEEPIDLE, in seconds;
-					# 0 selects the system default
-#tcp_keepalives_interval = 0		# TCP_KEEPINTVL, in seconds;
-					# 0 selects the system default
-#tcp_keepalives_count = 0		# TCP_KEEPCNT;
-					# 0 selects the system default
-#tcp_user_timeout = 0			# TCP_USER_TIMEOUT, in milliseconds;
-					# 0 selects the system default
-
-# - Authentication -
+# - Security and Authentication -
 
 #authentication_timeout = 1min		# 1s-600s
-#password_encryption = md5		# md5 or scram-sha-256
+#ssl = off				# (change requires restart)
+#ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphers
+					# (change requires restart)
+#ssl_prefer_server_ciphers = on		# (change requires restart)
+#ssl_ecdh_curve = 'prime256v1'		# (change requires restart)
+#ssl_renegotiation_limit = 0		# amount of data between renegotiations
+#ssl_cert_file = 'server.crt'		# (change requires restart)
+#ssl_key_file = 'server.key'		# (change requires restart)
+#ssl_ca_file = ''			# (change requires restart)
+#ssl_crl_file = ''			# (change requires restart)
+#password_encryption = on
 #db_user_namespace = off
 
 # GSSAPI using Kerberos
 #krb_server_keyfile = ''
 #krb_caseins_users = off
 
-# - SSL -
+# - TCP Keepalives -
+# see "man 7 tcp" for details
 
-#ssl = off
-#ssl_ca_file = ''
-#ssl_cert_file = 'server.crt'
-#ssl_crl_file = ''
-#ssl_key_file = 'server.key'
-#ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphers
-#ssl_prefer_server_ciphers = on
-#ssl_ecdh_curve = 'prime256v1'
-#ssl_min_protocol_version = 'TLSv1'
-#ssl_max_protocol_version = ''
-#ssl_dh_params_file = ''
-#ssl_passphrase_command = ''
-#ssl_passphrase_command_supports_reload = off
+#tcp_keepalives_idle = 0		# TCP_KEEPIDLE, in seconds;
+					# 0 selects the system default
+#tcp_keepalives_interval = 0		# TCP_KEEPINTVL, in seconds;
+					# 0 selects the system default
+#tcp_keepalives_count = 0		# TCP_KEEPCNT;
+					# 0 selects the system default
 
 
 #------------------------------------------------------------------------------
 # RESOURCE USAGE (except WAL)
 #------------------------------------------------------------------------------
 
 # - Memory -
@@ -125,236 +133,163 @@
 shared_buffers = {{ shared_buffers_mb[0]|int }}MB			# min 128kB
 					# (change requires restart)
 #huge_pages = try			# on, off, or try
 					# (change requires restart)
 #temp_buffers = 8MB			# min 800kB
 #max_prepared_transactions = 0		# zero disables the feature
 					# (change requires restart)
-# Caution: it is not advisable to set max_prepared_transactions nonzero unless
-# you actively intend to use prepared transactions.
+# Note:  Increasing max_prepared_transactions costs ~600 bytes of shared memory
+# per transaction slot, plus lock space (see max_locks_per_transaction).
+# It is not advisable to set max_prepared_transactions nonzero unless you
+# actively intend to use prepared transactions.
 #work_mem = 4MB				# min 64kB
 #maintenance_work_mem = 64MB		# min 1MB
 #autovacuum_work_mem = -1		# min 1MB, or -1 to use maintenance_work_mem
 max_stack_depth = 7MB			# min 100kB
 								# The ideal value is the actual limit enforced
 								# by the OS (8MB on 64-bit flavors) less a safety
 								# margin of 1MB or so.
-#shared_memory_type = mmap		# the default is the first option
-					# supported by the operating system:
-					#   mmap
-					#   sysv
-					#   windows
-					# (change requires restart)
-dynamic_shared_memory_type = posix	# the default is the first option
+#dynamic_shared_memory_type = posix	# the default is the first option
 					# supported by the operating system:
 					#   posix
 					#   sysv
 					#   windows
 					#   mmap
-					# (change requires restart)
+					# use none to disable dynamic shared memory
 
 # - Disk -
 
-#temp_file_limit = -1			# limits per-process temp file space
+#temp_file_limit = -1			# limits per-session temp file space
 					# in kB, or -1 for no limit
 
-# - Kernel Resources -
+# - Kernel Resource Usage -
 
 #max_files_per_process = 1000		# min 25
 					# (change requires restart)
+#shared_preload_libraries = ''		# (change requires restart)
 
 # - Cost-Based Vacuum Delay -
 
-#vacuum_cost_delay = 0			# 0-100 milliseconds (0 disables)
+#vacuum_cost_delay = 0			# 0-100 milliseconds
 #vacuum_cost_page_hit = 1		# 0-10000 credits
 #vacuum_cost_page_miss = 10		# 0-10000 credits
 #vacuum_cost_page_dirty = 20		# 0-10000 credits
 #vacuum_cost_limit = 200		# 1-10000 credits
 
 # - Background Writer -
 
 #bgwriter_delay = 200ms			# 10-10000ms between rounds
-#bgwriter_lru_maxpages = 100		# max buffers written/round, 0 disables
-#bgwriter_lru_multiplier = 2.0		# 0-10.0 multiplier on buffers scanned/round
-#bgwriter_flush_after = 512kB		# measured in pages, 0 disables
+#bgwriter_lru_maxpages = 100		# 0-1000 max buffers written/round
+#bgwriter_lru_multiplier = 2.0		# 0-10.0 multipler on buffers scanned/round
 
 # - Asynchronous Behavior -
 
 #effective_io_concurrency = 1		# 1-1000; 0 disables prefetching
-#max_worker_processes = 8		# (change requires restart)
-#max_parallel_maintenance_workers = 2	# taken from max_parallel_workers
-#max_parallel_workers_per_gather = 2	# taken from max_parallel_workers
-#parallel_leader_participation = on
-#max_parallel_workers = 8		# maximum number of max_worker_processes that
-					# can be used in parallel operations
-#old_snapshot_threshold = -1		# 1min-60d; -1 disables; 0 is immediate
-					# (change requires restart)
-#backend_flush_after = 0		# measured in pages, 0 disables
+#max_worker_processes = 8
 
 
 #------------------------------------------------------------------------------
-# WRITE-AHEAD LOG
+# WRITE AHEAD LOG
 #------------------------------------------------------------------------------
 
 # - Settings -
 
-wal_level = replica			# minimal, replica, or logical
+wal_level = minimal			# minimal, archive, hot_standby, or logical
 					# (change requires restart)
-					# (Trove default)
-#fsync = on				# flush data to disk for crash safety
-					# (turning this off can cause
-					# unrecoverable data corruption)
+					# (controlled by Trove)
+#fsync = on				# turns forced synchronization on or off
 #synchronous_commit = on		# synchronization level;
-					# off, local, remote_write, remote_apply, or on
+					# off, local, remote_write, or on
 #wal_sync_method = fsync		# the default is the first option
 					# supported by the operating system:
 					#   open_datasync
 					#   fdatasync (default on Linux)
 					#   fsync
 					#   fsync_writethrough
 					#   open_sync
 #full_page_writes = on			# recover from partial page writes
-#wal_compression = off			# enable compression of full-page writes
-wal_log_hints = on			# also do full page writes of non-critical updates
+#wal_log_hints = off			# also do full page writes of non-critical updates
 					# (change requires restart)
-					# (Trove default)
-#wal_init_zero = on			# zero-fill new WAL files
-#wal_recycle = on			# recycle WAL files
 #wal_buffers = -1			# min 32kB, -1 sets based on shared_buffers
 					# (change requires restart)
 #wal_writer_delay = 200ms		# 1-10000 milliseconds
-#wal_writer_flush_after = 1MB		# measured in pages, 0 disables
 
 #commit_delay = 0			# range 0-100000, in microseconds
 #commit_siblings = 5			# range 1-1000
 
 # - Checkpoints -
 
-#checkpoint_timeout = 5min		# range 30s-1d
-max_wal_size = 1GB
-min_wal_size = 80MB
+checkpoint_segments = {{ (shared_buffers_mb[0] / 16 + 1)|int }}		# in logfile segments, min 1, 16MB each
+								# Each segment is normally 16MB long.
+								# The number of segments should be enough to
+								# span the 'shared_buffers' size.
+								# We set the default to (shared_buffers / 16 + 1).
+#checkpoint_timeout = 5min		# range 30s-1h
 #checkpoint_completion_target = 0.5	# checkpoint target duration, 0.0 - 1.0
-#checkpoint_flush_after = 256kB		# measured in pages, 0 disables
 #checkpoint_warning = 30s		# 0 disables
 
 # - Archiving -
 
-archive_mode = on		# enables archiving; off, on, or always
+archive_mode = off		# allows archiving to be done
 				# (change requires restart)
-        # (Trove default)
-archive_command = 'test ! -f /var/lib/postgresql/data/wal_archive/%f && cp %p /var/lib/postgresql/data/wal_archive/%f'		# command to use to archive a logfile segment
+				# (controlled by Trove)
+#archive_command = ''		# command to use to archive a logfile segment
 				# placeholders: %p = path of file to archive
 				#               %f = file name only
 				# e.g. 'test ! -f /mnt/server/archivedir/%f && cp %p /mnt/server/archivedir/%f'
-				# (Trove default)
+				# (controlled by Trove)
 #archive_timeout = 0		# force a logfile segment switch after this
 				# number of seconds; 0 disables
-
-# - Archive Recovery -
-
-# These are only used in recovery mode.
-
-restore_command = 'cp /var/lib/postgresql/data/wal_archive/%f "%p"'		# command to use to restore an archived logfile segment
-				# placeholders: %p = path of file to restore
-				#               %f = file name only
-				# e.g. 'cp /mnt/server/archivedir/%f %p'
-				# (change requires restart)
-				# (Trove default)
-#archive_cleanup_command = ''	# command to execute at every restartpoint
-#recovery_end_command = ''	# command to execute at completion of recovery
-
-# - Recovery Target -
-
-# Set these only when performing a targeted recovery.
-
-#recovery_target = ''		# 'immediate' to end recovery as soon as a
-                                # consistent state is reached
-				# (change requires restart)
-#recovery_target_name = ''	# the named restore point to which recovery will proceed
-				# (change requires restart)
-#recovery_target_time = ''	# the time stamp up to which recovery will proceed
-				# (change requires restart)
-#recovery_target_xid = ''	# the transaction ID up to which recovery will proceed
-				# (change requires restart)
-#recovery_target_lsn = ''	# the WAL LSN up to which recovery will proceed
-				# (change requires restart)
-#recovery_target_inclusive = on # Specifies whether to stop:
-				# just after the specified recovery target (on)
-				# just before the recovery target (off)
-				# (change requires restart)
-#recovery_target_timeline = 'latest'	# 'current', 'latest', or timeline ID
-				# (change requires restart)
-#recovery_target_action = 'pause'	# 'pause', 'promote', 'shutdown'
-				# (change requires restart)
+				# (controlled by Trove)
 
 
 #------------------------------------------------------------------------------
 # REPLICATION
 #------------------------------------------------------------------------------
 
-# - Sending Servers -
+# - Sending Server(s) -
 
 # Set these on the master and on any standby that will send replication data.
 
-#max_wal_senders = 10		# max number of walsender processes
+#max_wal_senders = 0		# max number of walsender processes
 				# (change requires restart)
-wal_keep_segments = 5		# in logfile segments; 0 disables
-        # (Trove default)
+#wal_keep_segments = 0		# in logfile segments, 16MB each; 0 disables
 #wal_sender_timeout = 60s	# in milliseconds; 0 disables
 
-#max_replication_slots = 10	# max number of replication slots
-				# (change requires restart)
-#track_commit_timestamp = off	# collect timestamp of transaction commit
+#max_replication_slots = 0	# max number of replication slots
 				# (change requires restart)
 
 # - Master Server -
 
 # These settings are ignored on a standby server.
 
 #synchronous_standby_names = ''	# standby servers that provide sync rep
-				# method to choose sync standbys, number of sync standbys,
-				# and comma-separated list of application_name
+				# comma-separated list of application_name
 				# from standby(s); '*' = all
 #vacuum_defer_cleanup_age = 0	# number of xacts by which cleanup is delayed
 
 # - Standby Servers -
 
 # These settings are ignored on a master server.
 
-#primary_conninfo = ''			# connection string to sending server
-					# (change requires restart)
-#primary_slot_name = ''			# replication slot on sending server
-					# (change requires restart)
-#promote_trigger_file = ''		# file name whose presence ends recovery
-#hot_standby = on			# "off" disallows queries during recovery
+#hot_standby = off			# "on" allows queries during recovery
 					# (change requires restart)
 #max_standby_archive_delay = 30s	# max delay before canceling queries
 					# when reading WAL from archive;
 					# -1 allows indefinite delay
 #max_standby_streaming_delay = 30s	# max delay before canceling queries
 					# when reading streaming WAL;
 					# -1 allows indefinite delay
 #wal_receiver_status_interval = 10s	# send replies at least this often
 					# 0 disables
 #hot_standby_feedback = off		# send info from standby to prevent
 					# query conflicts
 #wal_receiver_timeout = 60s		# time that receiver waits for
 					# communication from master
 					# in milliseconds; 0 disables
-#wal_retrieve_retry_interval = 5s	# time to wait before retrying to
-					# retrieve WAL after a failed attempt
-#recovery_min_apply_delay = 0		# minimum delay for applying changes during recovery
-
-# - Subscribers -
-
-# These settings are ignored on a publisher.
-
-#max_logical_replication_workers = 4	# taken from max_worker_processes
-					# (change requires restart)
-#max_sync_workers_per_subscription = 2	# taken from max_logical_replication_workers
 
 
 #------------------------------------------------------------------------------
 # QUERY TUNING
 #------------------------------------------------------------------------------
 
 # - Planner Method Configuration -
@@ -363,45 +298,27 @@
 #enable_hashagg = on
 #enable_hashjoin = on
 #enable_indexscan = on
 #enable_indexonlyscan = on
 #enable_material = on
 #enable_mergejoin = on
 #enable_nestloop = on
-#enable_parallel_append = on
 #enable_seqscan = on
 #enable_sort = on
 #enable_tidscan = on
-#enable_partitionwise_join = off
-#enable_partitionwise_aggregate = off
-#enable_parallel_hash = on
-#enable_partition_pruning = on
 
 # - Planner Cost Constants -
 
 #seq_page_cost = 1.0			# measured on an arbitrary scale
 #random_page_cost = 4.0			# same scale as above
 #cpu_tuple_cost = 0.01			# same scale as above
 #cpu_index_tuple_cost = 0.005		# same scale as above
 #cpu_operator_cost = 0.0025		# same scale as above
-#parallel_tuple_cost = 0.1		# same scale as above
-#parallel_setup_cost = 1000.0	# same scale as above
-
-#jit_above_cost = 100000		# perform JIT compilation if available
-					# and query more expensive than this;
-					# -1 disables
-#jit_inline_above_cost = 500000		# inline small functions if query is
-					# more expensive than this; -1 disables
-#jit_optimize_above_cost = 500000	# use expensive JIT optimizations if
-					# query is more expensive than this;
-					# -1 disables
-
-#min_parallel_table_scan_size = 8MB
-#min_parallel_index_scan_size = 512kB
-effective_cache_size = {{ max(flavor['ram'] - 512, 512)|int }}MB
+effective_cache_size = {{ max(flavor['ram'] - 512, 512)|int }}MB	# Set to the amount of available RAM 
+					# less the minimum required for other processes or 512MB. 
 
 # - Genetic Query Optimizer -
 
 #geqo = on
 #geqo_threshold = 12
 #geqo_effort = 5			# range 1-10
 #geqo_pool_size = 0			# selects default based on effort
@@ -413,44 +330,44 @@
 
 #default_statistics_target = 100	# range 1-10000
 #constraint_exclusion = partition	# on, off, or partition
 #cursor_tuple_fraction = 0.1		# range 0.0-1.0
 #from_collapse_limit = 8
 #join_collapse_limit = 8		# 1 disables collapsing of explicit
 					# JOIN clauses
-#force_parallel_mode = off
-#jit = on				# allow JIT compilation
-#plan_cache_mode = auto			# auto, force_generic_plan or
-					# force_custom_plan
 
 
 #------------------------------------------------------------------------------
-# REPORTING AND LOGGING
+# ERROR REPORTING AND LOGGING
 #------------------------------------------------------------------------------
 
 # - Where to Log -
 
 #log_destination = 'stderr'		# Valid values are combinations of
 					# stderr, csvlog, syslog, and eventlog,
 					# depending on platform.  csvlog
 					# requires logging_collector to be on.
+					# (controlled by Trove)
 
 # This is used when logging to stderr:
 #logging_collector = off		# Enable capturing of stderr and csvlog
 					# into log files. Required to be on for
 					# csvlogs.
 					# (change requires restart)
+					# (controlled by Trove)
 
 # These are only used if logging_collector is on:
-#log_directory = 'log'			# directory where log files are written,
+#log_directory = 'pg_log'		# directory where log files are written,
 					# can be absolute or relative to PGDATA
+					# (controlled by Trove)
 #log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'	# log file name pattern,
 					# can include strftime() escapes
 #log_file_mode = 0600			# creation mode for log files,
 					# begin with 0 to use octal notation
+					# (controlled by Trove)
 #log_truncate_on_rotation = off		# If on, an existing log file with the
 					# same name as the new log file will be
 					# truncated rather than appended to.
 					# But such truncation only occurs on
 					# time-driven rotation, not on restarts
 					# or size-driven rotation.  Default is
 					# off, meaning append to existing files
@@ -460,23 +377,31 @@
 #log_rotation_size = 10MB		# Automatic rotation of logfiles will
 					# happen after that much log output.
 					# 0 disables.
 
 # These are relevant when logging to syslog:
 #syslog_facility = 'LOCAL0'
 #syslog_ident = 'postgres'
-#syslog_sequence_numbers = on
-#syslog_split_messages = on
 
 # This is only relevant when logging to eventlog (win32):
-# (change requires restart)
 #event_source = 'PostgreSQL'
 
 # - When to Log -
 
+#client_min_messages = notice		# values in order of decreasing detail:
+					#   debug5
+					#   debug4
+					#   debug3
+					#   debug2
+					#   debug1
+					#   log
+					#   notice
+					#   warning
+					#   error
+
 #log_min_messages = warning		# values in order of decreasing detail:
 					#   debug5
 					#   debug4
 					#   debug3
 					#   debug2
 					#   debug1
 					#   info
@@ -502,92 +427,80 @@
 					#   panic (effectively off)
 
 #log_min_duration_statement = -1	# -1 is disabled, 0 logs all statements
 					# and their durations, > 0 logs only
 					# statements running at least this number
 					# of milliseconds
 
-#log_transaction_sample_rate = 0.0	# Fraction of transactions whose statements
-					# are logged regardless of their duration. 1.0 logs all
-					# statements from all transactions, 0.0 never logs.
 
 # - What to Log -
 
 #debug_print_parse = off
 #debug_print_rewritten = off
 #debug_print_plan = off
 #debug_pretty_print = on
 #log_checkpoints = off
 #log_connections = off
 #log_disconnections = off
 #log_duration = off
 #log_error_verbosity = default		# terse, default, or verbose messages
 #log_hostname = off
-#log_line_prefix = '%m [%p] '		# special values:
+#log_line_prefix = ''			# special values:
 					#   %a = application name
 					#   %u = user name
 					#   %d = database name
 					#   %r = remote host and port
 					#   %h = remote host
 					#   %p = process ID
 					#   %t = timestamp without milliseconds
 					#   %m = timestamp with milliseconds
-					#   %n = timestamp with milliseconds (as a Unix epoch)
 					#   %i = command tag
 					#   %e = SQL state
 					#   %c = session ID
 					#   %l = session line number
 					#   %s = session start timestamp
 					#   %v = virtual transaction ID
 					#   %x = transaction ID (0 if none)
 					#   %q = stop here in non-session
 					#        processes
 					#   %% = '%'
 					# e.g. '<%u%%%d> '
 #log_lock_waits = off			# log lock waits >= deadlock_timeout
 #log_statement = 'none'			# none, ddl, mod, all
-#log_replication_commands = off
 #log_temp_files = -1			# log temporary files equal or larger
 					# than the specified size in kilobytes;
 					# -1 disables, 0 logs all temp files
-log_timezone = 'Etc/UTC'
-
-#------------------------------------------------------------------------------
-# PROCESS TITLE
-#------------------------------------------------------------------------------
-
-#cluster_name = ''			# added to process titles if nonempty
-					# (change requires restart)
-update_process_title = off	# (controlled by Trove)
+#log_timezone = 'GMT'
 
 
 #------------------------------------------------------------------------------
-# STATISTICS
+# RUNTIME STATISTICS
 #------------------------------------------------------------------------------
 
-# - Query and Index Statistics Collector -
+# - Query/Index Statistics Collector -
 
 #track_activities = on
 #track_counts = on
 #track_io_timing = off
 #track_functions = none			# none, pl, all
 #track_activity_query_size = 1024	# (change requires restart)
+update_process_title = off	# (controlled by Trove)
 #stats_temp_directory = 'pg_stat_tmp'
 
 
-# - Monitoring -
+# - Statistics Monitoring -
 
 #log_parser_stats = off
 #log_planner_stats = off
 #log_executor_stats = off
 #log_statement_stats = off
 
 
 #------------------------------------------------------------------------------
-# AUTOVACUUM
+# AUTOVACUUM PARAMETERS
 #------------------------------------------------------------------------------
 
 #autovacuum = on			# Enable autovacuum subprocess?  'on'
 					# requires track_counts to also be on.
 #log_autovacuum_min_duration = -1	# -1 disables, 0 logs all actions and
 					# their durations, > 0 logs only
 					# actions running at least this number
@@ -602,163 +515,134 @@
 #autovacuum_vacuum_scale_factor = 0.2	# fraction of table size before vacuum
 #autovacuum_analyze_scale_factor = 0.1	# fraction of table size before analyze
 #autovacuum_freeze_max_age = 200000000	# maximum XID age before forced vacuum
 					# (change requires restart)
 #autovacuum_multixact_freeze_max_age = 400000000	# maximum multixact age
 					# before forced vacuum
 					# (change requires restart)
-#autovacuum_vacuum_cost_delay = 2ms	# default vacuum cost delay for
+#autovacuum_vacuum_cost_delay = 20ms	# default vacuum cost delay for
 					# autovacuum, in milliseconds;
 					# -1 means use vacuum_cost_delay
 #autovacuum_vacuum_cost_limit = -1	# default vacuum cost limit for
 					# autovacuum, -1 means use
 					# vacuum_cost_limit
 
 
 #------------------------------------------------------------------------------
 # CLIENT CONNECTION DEFAULTS
 #------------------------------------------------------------------------------
 
 # - Statement Behavior -
 
-#client_min_messages = notice		# values in order of decreasing detail:
-					#   debug5
-					#   debug4
-					#   debug3
-					#   debug2
-					#   debug1
-					#   log
-					#   notice
-					#   warning
-					#   error
-#search_path = '"$user", public'	# schema names
-#row_security = on
+#search_path = '"$user",public'		# schema names
 #default_tablespace = ''		# a tablespace name, '' uses the default
 #temp_tablespaces = ''			# a list of tablespace names, '' uses
 					# only default tablespace
-#default_table_access_method = 'heap'
 #check_function_bodies = on
 #default_transaction_isolation = 'read committed'
 #default_transaction_read_only = off
 #default_transaction_deferrable = off
 #session_replication_role = 'origin'
 #statement_timeout = 0			# in milliseconds, 0 is disabled
 #lock_timeout = 0			# in milliseconds, 0 is disabled
-#idle_in_transaction_session_timeout = 0	# in milliseconds, 0 is disabled
 #vacuum_freeze_min_age = 50000000
 #vacuum_freeze_table_age = 150000000
 #vacuum_multixact_freeze_min_age = 5000000
 #vacuum_multixact_freeze_table_age = 150000000
-#vacuum_cleanup_index_scale_factor = 0.1	# fraction of total number of tuples
-						# before index cleanup, 0 always performs
-						# index cleanup
 #bytea_output = 'hex'			# hex, escape
 #xmlbinary = 'base64'
 #xmloption = 'content'
-#gin_fuzzy_search_limit = 0
-#gin_pending_list_limit = 4MB
 
 # - Locale and Formatting -
 
-datestyle = 'iso, mdy'
+#datestyle = 'iso, mdy'
 #intervalstyle = 'postgres'
-timezone = 'Etc/UTC'
+#timezone = 'GMT'
 #timezone_abbreviations = 'Default'     # Select the set of available time zone
 					# abbreviations.  Currently, there are
 					#   Default
 					#   Australia (historical usage)
 					#   India
 					# You can create your own file in
 					# share/timezonesets/.
-#extra_float_digits = 1			# min -15, max 3; any value >0 actually
-					# selects precise output mode
+#extra_float_digits = 0			# min -15, max 3
 #client_encoding = sql_ascii		# actually, defaults to database
 					# encoding
 
 # These settings are initialized by initdb, but they can be changed.
-lc_messages = 'en_US.utf8'			# locale for system error message
+#lc_messages = 'C'			# locale for system error message
 					# strings
-lc_monetary = 'en_US.utf8'			# locale for monetary formatting
-lc_numeric = 'en_US.utf8'			# locale for number formatting
-lc_time = 'en_US.utf8'				# locale for time formatting
+#lc_monetary = 'C'			# locale for monetary formatting
+#lc_numeric = 'C'			# locale for number formatting
+#lc_time = 'C'				# locale for time formatting
 
 # default configuration for text search
-default_text_search_config = 'pg_catalog.english'
-
-# - Shared Library Preloading -
-
-#shared_preload_libraries = ''	# (change requires restart)
-#local_preload_libraries = ''
-#session_preload_libraries = ''
-#jit_provider = 'llvmjit'		# JIT library to use
+#default_text_search_config = 'pg_catalog.simple'
 
 # - Other Defaults -
 
 #dynamic_library_path = '$libdir'
+#local_preload_libraries = ''
+#session_preload_libraries = ''
 
 
 #------------------------------------------------------------------------------
 # LOCK MANAGEMENT
 #------------------------------------------------------------------------------
 
 #deadlock_timeout = 1s
 #max_locks_per_transaction = 64		# min 10
 					# (change requires restart)
+# Note:  Each lock table slot uses ~270 bytes of shared memory, and there are
+# max_locks_per_transaction * (max_connections + max_prepared_transactions)
+# lock table slots.
 #max_pred_locks_per_transaction = 64	# min 10
 					# (change requires restart)
-#max_pred_locks_per_relation = -2	# negative values mean
-					# (max_pred_locks_per_transaction
-					#  / -max_pred_locks_per_relation) - 1
-#max_pred_locks_per_page = 2            # min 0
 
 
 #------------------------------------------------------------------------------
-# VERSION AND PLATFORM COMPATIBILITY
+# VERSION/PLATFORM COMPATIBILITY
 #------------------------------------------------------------------------------
 
 # - Previous PostgreSQL Versions -
 
 #array_nulls = on
 #backslash_quote = safe_encoding	# on, off, or safe_encoding
+#default_with_oids = off
 #escape_string_warning = on
 #lo_compat_privileges = off
-#operator_precedence_warning = off
 #quote_all_identifiers = off
+#sql_inheritance = on
 #standard_conforming_strings = on
 #synchronize_seqscans = on
 
 # - Other Platforms and Clients -
 
 #transform_null_equals = off
 
 
 #------------------------------------------------------------------------------
 # ERROR HANDLING
 #------------------------------------------------------------------------------
 
 #exit_on_error = off			# terminate session on any error?
 #restart_after_crash = on		# reinitialize after backend crash?
-#data_sync_retry = off			# retry or panic on failure to fsync
-					# data?
-					# (change requires restart)
 
 
 #------------------------------------------------------------------------------
 # CONFIG FILE INCLUDES
 #------------------------------------------------------------------------------
 
 # These options allow settings to be loaded from files other than the
-# default postgresql.conf.  Note that these are directives, not variable
-# assignments, so they can usefully be given more than once.
+# default postgresql.conf.
 
-include_dir = '/etc/postgresql/conf.d'			# include files ending in '.conf' from
-					# a directory, e.g., 'conf.d'
-					# (Enabled by Trove)
-#include_if_exists = '...'		# include file only if it exists
-#include = '...'			# include file
+#include_dir = 'conf.d'			# include files ending in '.conf' from
+					# directory 'conf.d'
+#include_if_exists = 'exists.conf'	# include file only if it exists
+#include = 'special.conf'		# include file
 
 
 #------------------------------------------------------------------------------
 # CUSTOMIZED OPTIONS
 #------------------------------------------------------------------------------
 
 # Add settings for extensions here
```

### Comparing `trove-21.0.0.0rc2/trove/templates/postgresql/validation-rules.json` & `trove-8.0.1/trove/templates/postgresql/validation-rules.json`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8248414855072465%*

 * *Differences: {"'configuration-parameters'": "{2: {'name': 'bonjour', 'restart_required': True}, 5: {'name': "*

 * *                               "'password_encryption'}, 6: {'name': 'db_user_namespace', 'type': "*

 * *                               "'boolean'}, 7: {'name': 'tcp_keepalives_idle'}, 8: {'name': "*

 * *                               "'tcp_keepalives_interval', 'restart_required': False}, 9: {'name': "*

 * *                               "'tcp_keepalives_count', 'restart_required': False}, 10: {'name': "*

 * *                       […]*

```diff
@@ -1,291 +1,302 @@
 {
     "configuration-parameters": [
         {
-            "name": "array_nulls",
-            "restart_required": false,
+            "min": 0,
+            "name": "max_connections",
+            "restart_required": true,
+            "type": "integer"
+        },
+        {
+            "min": 1,
+            "name": "superuser_reserved_connections",
+            "restart_required": true,
+            "type": "integer"
+        },
+        {
+            "name": "bonjour",
+            "restart_required": true,
             "type": "boolean"
         },
         {
+            "name": "bonjour_name",
+            "restart_required": true,
+            "type": "string"
+        },
+        {
             "name": "authentication_timeout",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "autovacuum",
+            "name": "password_encryption",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "autovacuum_analyze_scale_factor",
+            "name": "db_user_namespace",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
             "min": 0,
-            "name": "autovacuum_analyze_threshold",
+            "name": "tcp_keepalives_idle",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "autovacuum_freeze_max_age",
-            "restart_required": true,
+            "name": "tcp_keepalives_interval",
+            "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "autovacuum_max_workers",
-            "restart_required": true,
+            "name": "tcp_keepalives_count",
+            "restart_required": false,
             "type": "integer"
         },
         {
-            "min": 0,
-            "name": "autovacuum_multixact_freeze_max_age",
+            "name": "shared_buffers",
             "restart_required": true,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "name": "autovacuum_naptime",
-            "restart_required": false,
+            "name": "huge_pages",
+            "restart_required": true,
             "type": "string"
         },
         {
-            "name": "autovacuum_vacuum_cost_delay",
+            "name": "temp_buffers",
             "restart_required": false,
             "type": "string"
         },
         {
-            "min": -1,
-            "name": "autovacuum_vacuum_cost_limit",
-            "restart_required": false,
+            "min": 0,
+            "name": "max_prepared_transactions",
+            "restart_required": true,
             "type": "integer"
         },
         {
-            "name": "autovacuum_vacuum_scale_factor",
+            "name": "work_mem",
             "restart_required": false,
             "type": "string"
         },
         {
-            "min": 0,
-            "name": "autovacuum_vacuum_threshold",
+            "name": "maintenance_work_mem",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
             "min": -1,
             "name": "autovacuum_work_mem",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "backend_flush_after",
-            "restart_required": false,
-            "type": "string"
-        },
-        {
-            "name": "backslash_quote",
+            "name": "max_stack_depth",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "bgwriter_delay",
+            "name": "dynamic_shared_memory_type",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "bgwriter_flush_after",
+            "min": -1,
+            "name": "temp_file_limit",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
             "min": 0,
-            "name": "bgwriter_lru_maxpages",
-            "restart_required": false,
+            "name": "max_files_per_process",
+            "restart_required": true,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "bgwriter_lru_multiplier",
+            "name": "vacuum_cost_delay",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "bonjour",
-            "restart_required": true,
-            "type": "boolean"
-        },
-        {
-            "name": "bonjour_name",
-            "restart_required": true,
-            "type": "string"
-        },
-        {
-            "name": "bytea_output",
-            "restart_required": false,
-            "type": "string"
-        },
-        {
-            "name": "check_function_bodies",
+            "min": 0,
+            "name": "vacuum_cost_page_hit",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
-            "name": "checkpoint_completion_target",
+            "min": 0,
+            "name": "vacuum_cost_page_miss",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "checkpoint_flush_after",
+            "min": 0,
+            "name": "vacuum_cost_page_dirty",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "checkpoint_timeout",
+            "min": 0,
+            "name": "vacuum_cost_limit",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "checkpoint_warning",
+            "name": "bgwriter_delay",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "client_encoding",
+            "min": 0,
+            "name": "bgwriter_lru_maxpages",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "client_min_messages",
+            "min": 0,
+            "name": "bgwriter_lru_multiplier",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
             "min": 0,
-            "name": "commit_delay",
+            "name": "effective_io_concurrency",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "commit_siblings",
+            "name": "max_worker_processes",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "constraint_exclusion",
+            "name": "fsync",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "cpu_index_tuple_cost",
+            "name": "synchronous_commit",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "cpu_operator_cost",
+            "name": "wal_sync_method",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "cpu_tuple_cost",
+            "name": "full_page_writes",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "cursor_tuple_fraction",
-            "restart_required": false,
-            "type": "string"
+            "name": "wal_log_hints",
+            "restart_required": true,
+            "type": "boolean"
         },
         {
-            "name": "datestyle",
+            "min": -1,
+            "name": "wal_buffers",
+            "restart_required": true,
+            "type": "integer"
+        },
+        {
+            "name": "wal_writer_delay",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "db_user_namespace",
+            "min": 0,
+            "name": "commit_delay",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
-            "name": "deadlock_timeout",
+            "min": 0,
+            "name": "commit_siblings",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "debug_pretty_print",
+            "min": 0,
+            "name": "checkpoint_segments",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
-            "name": "debug_print_parse",
+            "name": "checkpoint_timeout",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "debug_print_plan",
+            "name": "checkpoint_completion_target",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "debug_print_rewritten",
+            "name": "checkpoint_warning",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
             "min": 0,
-            "name": "default_statistics_target",
+            "name": "wal_keep_segments",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "default_tablespace",
+            "name": "wal_sender_timeout",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "default_text_search_config",
+            "name": "synchronous_standby_names",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "default_transaction_deferrable",
+            "min": 0,
+            "name": "vacuum_defer_cleanup_age",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
-            "name": "default_transaction_isolation",
-            "restart_required": false,
-            "type": "string"
+            "name": "hot_standby",
+            "restart_required": true,
+            "type": "boolean"
         },
         {
-            "name": "default_transaction_read_only",
+            "name": "max_standby_archive_delay",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "default_with_oids",
+            "name": "max_standby_streaming_delay",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "dynamic_shared_memory_type",
+            "name": "wal_receiver_status_interval",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "effective_cache_size",
+            "name": "hot_standby_feedback",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "effective_io_concurrency",
+            "name": "wal_receiver_timeout",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
             "name": "enable_bitmapscan",
             "restart_required": false,
             "type": "boolean"
         },
         {
@@ -295,20 +306,20 @@
         },
         {
             "name": "enable_hashjoin",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "enable_indexonlyscan",
+            "name": "enable_indexscan",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "enable_indexscan",
+            "name": "enable_indexonlyscan",
             "restart_required": false,
             "type": "boolean"
         },
         {
             "name": "enable_material",
             "restart_required": false,
             "type": "boolean"
@@ -335,164 +346,163 @@
         },
         {
             "name": "enable_tidscan",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "escape_string_warning",
-            "restart_required": false,
-            "type": "boolean"
-        },
-        {
-            "name": "exit_on_error",
+            "min": 0,
+            "name": "seq_page_cost",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
             "min": 0,
-            "name": "extra_float_digits",
+            "name": "random_page_cost",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "force_parallel_mode",
+            "name": "cpu_tuple_cost",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "min": 0,
-            "name": "from_collapse_limit",
+            "name": "cpu_index_tuple_cost",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "name": "fsync",
+            "name": "cpu_operator_cost",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "full_page_writes",
+            "name": "effective_cache_size",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
             "name": "geqo",
             "restart_required": false,
             "type": "boolean"
         },
         {
             "min": 0,
-            "name": "geqo_effort",
+            "name": "geqo_threshold",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "geqo_generations",
+            "name": "geqo_effort",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
             "name": "geqo_pool_size",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "geqo_seed",
+            "name": "geqo_generations",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
             "name": "geqo_selection_bias",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "geqo_threshold",
+            "name": "geqo_seed",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "gin_pending_list_limit",
+            "min": 0,
+            "name": "default_statistics_target",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "hot_standby",
-            "restart_required": true,
-            "type": "boolean"
+            "name": "constraint_exclusion",
+            "restart_required": false,
+            "type": "string"
         },
         {
-            "name": "hot_standby_feedback",
+            "name": "cursor_tuple_fraction",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "huge_pages",
-            "restart_required": true,
-            "type": "string"
+            "min": 0,
+            "name": "from_collapse_limit",
+            "restart_required": false,
+            "type": "integer"
         },
         {
             "min": 0,
-            "name": "idle_in_transaction_session_timeout",
+            "name": "join_collapse_limit",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "intervalstyle",
+            "name": "log_truncate_on_rotation",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "join_collapse_limit",
+            "name": "log_rotation_age",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "name": "lc_messages",
+            "name": "log_rotation_size",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "lc_monetary",
+            "name": "client_min_messages",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "lc_numeric",
+            "name": "log_min_messages",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "lc_time",
+            "name": "log_min_error_statement",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "lo_compat_privileges",
+            "name": "debug_print_parse",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "lock_timeout",
+            "name": "debug_print_rewritten",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "min": -1,
-            "name": "log_autovacuum_min_duration",
+            "name": "debug_print_plan",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
+        },
+        {
+            "name": "debug_pretty_print",
+            "restart_required": false,
+            "type": "boolean"
         },
         {
             "name": "log_checkpoints",
             "restart_required": false,
             "type": "boolean"
         },
         {
@@ -512,19 +522,14 @@
         },
         {
             "name": "log_error_verbosity",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "log_executor_stats",
-            "restart_required": false,
-            "type": "boolean"
-        },
-        {
             "name": "log_hostname",
             "restart_required": false,
             "type": "boolean"
         },
         {
             "name": "log_line_prefix",
             "restart_required": false,
@@ -532,424 +537,366 @@
         },
         {
             "name": "log_lock_waits",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "log_min_duration_statement",
+            "name": "log_statement",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "log_min_error_statement",
+            "min": -1,
+            "name": "log_temp_files",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "log_min_messages",
+            "name": "log_timezone",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "log_parser_stats",
+            "name": "track_activities",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "log_planner_stats",
+            "name": "track_counts",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "log_rotation_age",
+            "name": "track_io_timing",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "log_rotation_size",
+            "name": "track_functions",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "log_statement",
-            "restart_required": false,
-            "type": "string"
+            "min": 0,
+            "name": "track_activity_query_size",
+            "restart_required": true,
+            "type": "integer"
         },
         {
-            "name": "log_statement_stats",
+            "name": "log_parser_stats",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "min": -1,
-            "name": "log_temp_files",
+            "name": "log_planner_stats",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "name": "log_timezone",
+            "name": "log_executor_stats",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "log_truncate_on_rotation",
+            "name": "log_statement_stats",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "maintenance_work_mem",
+            "name": "autovacuum",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "max_connections",
-            "restart_required": true,
+            "min": -1,
+            "name": "log_autovacuum_min_duration",
+            "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "max_files_per_process",
+            "name": "autovacuum_max_workers",
             "restart_required": true,
             "type": "integer"
         },
         {
-            "min": 0,
-            "name": "max_locks_per_transaction",
-            "restart_required": true,
-            "type": "integer"
+            "name": "autovacuum_naptime",
+            "restart_required": false,
+            "type": "string"
         },
         {
             "min": 0,
-            "name": "max_pred_locks_per_transaction",
-            "restart_required": true,
+            "name": "autovacuum_vacuum_threshold",
+            "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "max_prepared_transactions",
-            "restart_required": true,
-            "type": "integer"
-        },
-        {
-            "name": "max_stack_depth",
+            "name": "autovacuum_analyze_threshold",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
-            "name": "max_standby_archive_delay",
+            "name": "autovacuum_vacuum_scale_factor",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "max_standby_streaming_delay",
+            "name": "autovacuum_analyze_scale_factor",
             "restart_required": false,
             "type": "string"
         },
         {
             "min": 0,
-            "name": "max_wal_size",
-            "restart_required": false,
-            "type": "integer"
-        },
-        {
-            "min": 0,
-            "name": "max_worker_processes",
-            "restart_required": false,
+            "name": "autovacuum_freeze_max_age",
+            "restart_required": true,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "min_wal_size",
-            "restart_required": false,
+            "name": "autovacuum_multixact_freeze_max_age",
+            "restart_required": true,
             "type": "integer"
         },
         {
-            "name": "parallel_setup_cost",
-            "restart_required": false,
-            "type": "float"
-        },
-        {
-            "name": "parallel_tuple_cost",
-            "restart_required": false,
-            "type": "float"
-        },
-        {
-            "name": "password_encryption",
-            "restart_required": false,
-            "type": "boolean"
-        },
-        {
-            "name": "quote_all_identifiers",
+            "name": "autovacuum_vacuum_cost_delay",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "min": 0,
-            "name": "random_page_cost",
+            "min": -1,
+            "name": "autovacuum_vacuum_cost_limit",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "min": 0,
-            "name": "replacement_sort_tuples",
+            "name": "search_path",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "name": "restart_after_crash",
+            "name": "default_tablespace",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "search_path",
+            "name": "temp_tablespaces",
             "restart_required": false,
             "type": "string"
         },
         {
-            "min": 0,
-            "name": "seq_page_cost",
+            "name": "check_function_bodies",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "name": "session_replication_role",
+            "name": "default_transaction_isolation",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "shared_buffers",
-            "restart_required": true,
-            "type": "string"
+            "name": "default_transaction_read_only",
+            "restart_required": false,
+            "type": "boolean"
         },
         {
-            "name": "sql_inheritance",
+            "name": "default_transaction_deferrable",
             "restart_required": false,
             "type": "boolean"
         },
         {
-            "name": "standard_conforming_strings",
+            "name": "session_replication_role",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
             "min": 0,
             "name": "statement_timeout",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "min": 1,
-            "name": "superuser_reserved_connections",
-            "restart_required": true,
-            "type": "integer"
-        },
-        {
-            "name": "synchronize_seqscans",
+            "min": 0,
+            "name": "lock_timeout",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
-            "name": "synchronous_commit",
+            "min": 0,
+            "name": "vacuum_freeze_min_age",
             "restart_required": false,
-            "type": "boolean"
+            "type": "integer"
         },
         {
-            "name": "synchronous_standby_names",
+            "min": 0,
+            "name": "vacuum_freeze_table_age",
             "restart_required": false,
-            "type": "string"
+            "type": "integer"
         },
         {
             "min": 0,
-            "name": "tcp_keepalives_count",
+            "name": "vacuum_multixact_freeze_min_age",
             "restart_required": false,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "tcp_keepalives_idle",
+            "name": "vacuum_multixact_freeze_table_age",
             "restart_required": false,
             "type": "integer"
         },
         {
-            "min": 0,
-            "name": "tcp_keepalives_interval",
+            "name": "bytea_output",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "name": "temp_buffers",
+            "name": "xmlbinary",
             "restart_required": false,
             "type": "string"
         },
         {
-            "min": -1,
-            "name": "temp_file_limit",
+            "name": "xmloption",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "name": "temp_tablespaces",
+            "name": "datestyle",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "timezone",
+            "name": "intervalstyle",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "timezone_abbreviations",
+            "name": "timezone",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "track_activities",
+            "name": "timezone_abbreviations",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
             "min": 0,
-            "name": "track_activity_query_size",
-            "restart_required": true,
+            "name": "extra_float_digits",
+            "restart_required": false,
             "type": "integer"
         },
         {
-            "name": "track_counts",
+            "name": "client_encoding",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "track_functions",
+            "name": "lc_messages",
             "restart_required": false,
             "type": "string"
         },
         {
-            "name": "track_io_timing",
+            "name": "lc_monetary",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "name": "transform_null_equals",
+            "name": "lc_numeric",
             "restart_required": false,
-            "type": "boolean"
+            "type": "string"
         },
         {
-            "min": 0,
-            "name": "vacuum_cost_delay",
+            "name": "lc_time",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "min": 0,
-            "name": "vacuum_cost_limit",
+            "name": "default_text_search_config",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "min": 0,
-            "name": "vacuum_cost_page_dirty",
+            "name": "deadlock_timeout",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
             "min": 0,
-            "name": "vacuum_cost_page_hit",
-            "restart_required": false,
+            "name": "max_locks_per_transaction",
+            "restart_required": true,
             "type": "integer"
         },
         {
             "min": 0,
-            "name": "vacuum_cost_page_miss",
-            "restart_required": false,
+            "name": "max_pred_locks_per_transaction",
+            "restart_required": true,
             "type": "integer"
         },
         {
-            "min": 0,
-            "name": "vacuum_defer_cleanup_age",
+            "name": "array_nulls",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "vacuum_freeze_min_age",
+            "name": "backslash_quote",
             "restart_required": false,
-            "type": "integer"
+            "type": "string"
         },
         {
-            "min": 0,
-            "name": "vacuum_freeze_table_age",
+            "name": "default_with_oids",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "vacuum_multixact_freeze_min_age",
+            "name": "escape_string_warning",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "min": 0,
-            "name": "vacuum_multixact_freeze_table_age",
+            "name": "lo_compat_privileges",
             "restart_required": false,
-            "type": "integer"
-        },
-        {
-            "min": -1,
-            "name": "wal_buffers",
-            "restart_required": true,
-            "type": "integer"
-        },
-        {
-            "min": 0,
-            "name": "wal_keep_segments",
-            "restart_required": true,
-            "type": "integer"
-        },
-        {
-            "name": "wal_log_hints",
-            "restart_required": true,
             "type": "boolean"
         },
         {
-            "name": "wal_receiver_status_interval",
+            "name": "quote_all_identifiers",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "wal_receiver_timeout",
+            "name": "sql_inheritance",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "wal_sender_timeout",
+            "name": "standard_conforming_strings",
             "restart_required": false,
-            "type": "integer"
+            "type": "boolean"
         },
         {
-            "name": "wal_sync_method",
+            "name": "synchronize_seqscans",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "wal_writer_delay",
+            "name": "transform_null_equals",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "work_mem",
+            "name": "exit_on_error",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "xmlbinary",
+            "name": "restart_after_crash",
             "restart_required": false,
-            "type": "string"
+            "type": "boolean"
         },
         {
-            "name": "xmloption",
+            "name": "log_min_duration_statement",
             "restart_required": false,
             "type": "string"
         }
     ]
 }
```

### Comparing `trove-21.0.0.0rc2/trove/templates/pxc/cluster.config.template` & `trove-8.0.1/trove/templates/pxc/cluster.config.template`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/pxc/config.template` & `trove-8.0.1/trove/templates/mariadb/config.template`

 * *Files 11% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 [client]
 port = 3306
 socket = /var/run/mysqld/mysqld.sock
 
 [mysqld_safe]
-pid-file = /var/run/mysqld/mysqld.pid
-socket = /var/run/mysqld/mysqld.sock
 nice = 0
 
 [mysqld]
+ignore_builtin_innodb
+plugin_load=innodb=ha_innodb.so
 user = mysql
 port = 3306
 basedir = /usr
 datadir = /var/lib/mysql/data
 tmpdir = /var/tmp
-pid-file = /var/run/mysqld/mysqld.pid
+pid_file = /var/run/mysqld/mysqld.pid
 socket = /var/run/mysqld/mysqld.sock
 skip-external-locking = 1
 key_buffer_size = {{ (50 * flavor['ram']/512)|int }}M
 max_allowed_packet = {{ (1024 * flavor['ram']/512)|int }}K
 thread_stack = 192K
 thread_cache_size = {{ (4 * flavor['ram']/512)|int }}
 myisam-recover-options = BACKUP,FORCE
```

### Comparing `trove-21.0.0.0rc2/trove/templates/pxc/validation-rules.json` & `trove-8.0.1/trove/templates/pxc/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/redis/config.template` & `trove-8.0.1/trove/templates/redis/config.template`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/redis/validation-rules.json` & `trove-8.0.1/trove/templates/redis/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/templates/vertica/validation-rules.json` & `trove-8.0.1/trove/templates/vertica/validation-rules.json`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/api/backups.py` & `trove-8.0.1/trove/tests/api/backups.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,36 +11,35 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 from proboscis.asserts import assert_equal
 from proboscis.asserts import assert_not_equal
 from proboscis.asserts import assert_raises
-from proboscis.asserts import assert_true
 from proboscis.asserts import fail
 from proboscis.decorators import time_out
 from proboscis import SkipTest
 from proboscis import test
 from troveclient.compat import exceptions
 
 from trove.common import cfg
 from trove.common import exception
 from trove.common.utils import generate_uuid
 from trove.common.utils import poll_until
 from trove import tests
 from trove.tests.api.instances import instance_info
 from trove.tests.api.instances import TIMEOUT_INSTANCE_CREATE
 from trove.tests.api.instances import TIMEOUT_INSTANCE_DELETE
-from trove.tests.api.instances import TIMEOUT_INSTANCE_RESTORE
 from trove.tests.api.instances import WaitForGuestInstallationToFinish
 from trove.tests.config import CONFIG
 from trove.tests.util import create_dbaas_client
 from trove.tests.util.users import Requirements
 
 
+GROUP = "dbaas.api.backups"
 BACKUP_NAME = 'backup_test'
 BACKUP_DESC = 'test description'
 
 TIMEOUT_BACKUP_CREATE = 60 * 30
 TIMEOUT_BACKUP_DELETE = 120
 
 backup_info = None
@@ -48,17 +47,16 @@
 incremental_db = generate_uuid()
 incremental_restore_instance_id = None
 total_num_dbs = 0
 backup_count_prior_to_create = 0
 backup_count_for_instance_prior_to_create = 0
 
 
-@test(depends_on_groups=[tests.DBAAS_API_INSTANCE_ACTIONS],
-      groups=[tests.DBAAS_API_BACKUPS],
-      enabled=CONFIG.swift_enabled)
+@test(depends_on_classes=[WaitForGuestInstallationToFinish],
+      groups=[GROUP, tests.INSTANCES])
 class CreateBackups(object):
 
     @test
     def test_backup_create_instance(self):
         """Test create backup for a given instance."""
         # Necessary to test that the count increases.
         global backup_count_prior_to_create
@@ -77,15 +75,15 @@
         backup_info = result
         assert_equal(BACKUP_NAME, result.name)
         assert_equal(BACKUP_DESC, result.description)
         assert_equal(instance_info.id, result.instance_id)
         assert_equal('NEW', result.status)
         instance = instance_info.dbaas.instances.get(instance_info.id)
 
-        assert_true(instance.status in ['ACTIVE', 'BACKUP', 'HEALTHY'])
+        assert_equal('BACKUP', instance.status)
         assert_equal(instance_info.dbaas_datastore,
                      result.datastore['type'])
         assert_equal(instance_info.dbaas_datastore_version,
                      result.datastore['version'])
         assert_equal(datastore_version.id, result.datastore['version_id'])
 
 
@@ -125,44 +123,44 @@
 
         poll_until(backup_is_gone, time_out=TIMEOUT_BACKUP_DELETE)
 
     def verify_instance_is_active(self, instance_id):
         # This version just checks the REST API status.
         def result_is_active():
             instance = instance_info.dbaas.instances.get(instance_id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
                 # If its not ACTIVE, anything but BUILD must be
                 # an error.
                 assert_equal("BUILD", instance.status)
                 if instance_info.volume is not None:
                     assert_equal(instance.volume.get('used', None), None)
                 return False
 
         poll_until(result_is_active, sleep_time=5,
                    time_out=TIMEOUT_INSTANCE_CREATE)
 
 
-@test(depends_on_classes=[CreateBackups],
-      groups=[tests.DBAAS_API_BACKUPS],
-      enabled=CONFIG.swift_enabled)
+@test(runs_after=[CreateBackups],
+      groups=[GROUP, tests.INSTANCES])
 class WaitForBackupCreateToFinish(BackupRestoreMixin):
-    """Wait until the backup creation is finished."""
+    """
+        Wait until the backup create is finished.
+    """
 
     @test
     @time_out(TIMEOUT_BACKUP_CREATE)
     def test_backup_created(self):
-        """Wait for the backup to be finished."""
+        # This version just checks the REST API status.
         self.verify_backup(backup_info.id)
 
 
 @test(depends_on=[WaitForBackupCreateToFinish],
-      groups=[tests.DBAAS_API_BACKUPS],
-      enabled=CONFIG.swift_enabled)
+      groups=[GROUP, tests.INSTANCES])
 class ListBackups(object):
 
     @test
     def test_backup_list(self):
         """Test list backups."""
         result = instance_info.dbaas.backups.list()
         assert_equal(backup_count_prior_to_create + 1, len(result))
@@ -186,15 +184,15 @@
         assert_equal(instance_info.id, backup.instance_id)
         assert_equal('COMPLETED', backup.status)
 
     @test
     def test_backup_list_filter_different_datastore(self):
         """Test list backups and filter by datastore."""
         result = instance_info.dbaas.backups.list(
-            datastore=CONFIG.dbaas_datastore_name_no_versions)
+            datastore='Test_Datastore_1')
         # There should not be any backups for this datastore
         assert_equal(0, len(result))
 
     @test
     def test_backup_list_filter_datastore_not_found(self):
         """Test list backups and filter by datastore."""
         assert_raises(exceptions.NotFound, instance_info.dbaas.backups.list,
@@ -242,16 +240,15 @@
         other_client = create_dbaas_client(other_user)
         assert_raises(exceptions.NotFound, other_client.backups.get,
                       backup_info.id)
 
 
 @test(runs_after=[ListBackups],
       depends_on=[WaitForBackupCreateToFinish],
-      groups=[tests.DBAAS_API_BACKUPS],
-      enabled=CONFIG.swift_enabled)
+      groups=[GROUP, tests.INSTANCES])
 class IncrementalBackups(BackupRestoreMixin):
 
     @test
     def test_create_db(self):
         global total_num_dbs
         total_num_dbs = len(instance_info.dbaas.databases.list(
             instance_info.id))
@@ -270,18 +267,17 @@
         assert_equal(202, instance_info.dbaas.last_http_code)
 
         # Wait for the backup to finish
         self.verify_backup(incremental_info.id)
         assert_equal(backup_info.id, incremental_info.parent_id)
 
 
-@test(groups=[tests.DBAAS_API_BACKUPS],
-      depends_on_classes=[IncrementalBackups],
-      enabled=CONFIG.swift_enabled)
+@test(groups=[GROUP, tests.INSTANCES])
 class RestoreUsingBackup(object):
+
     @classmethod
     def _restore(cls, backup_ref):
         restorePoint = {"backupRef": backup_ref}
         result = instance_info.dbaas.instances.create(
             instance_info.name + "_restore",
             instance_info.dbaas_flavor_href,
             instance_info.volume,
@@ -291,79 +287,80 @@
             restorePoint=restorePoint)
         assert_equal(200, instance_info.dbaas.last_http_code)
         assert_equal("BUILD", result.status)
         return result.id
 
     @test(depends_on=[IncrementalBackups])
     def test_restore_incremental(self):
-        """Restore from incremental backup."""
         global incremental_restore_instance_id
         incremental_restore_instance_id = self._restore(incremental_info.id)
 
 
-@test(depends_on_classes=[RestoreUsingBackup],
-      groups=[tests.DBAAS_API_BACKUPS],
-      enabled=CONFIG.swift_enabled)
+@test(depends_on_classes=[WaitForGuestInstallationToFinish],
+      runs_after_groups=['dbaas.api.configurations.define'],
+      groups=[GROUP, tests.INSTANCES])
 class WaitForRestoreToFinish(object):
+
     @classmethod
     def _poll(cls, instance_id_to_poll):
         """Shared "instance restored" test logic."""
-
         # This version just checks the REST API status.
         def result_is_active():
             instance = instance_info.dbaas.instances.get(instance_id_to_poll)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
                 # If its not ACTIVE, anything but BUILD must be
                 # an error.
                 assert_equal("BUILD", instance.status)
                 if instance_info.volume is not None:
                     assert_equal(instance.volume.get('used', None), None)
                 return False
 
-        poll_until(result_is_active, time_out=TIMEOUT_INSTANCE_RESTORE,
+        poll_until(result_is_active, time_out=TIMEOUT_INSTANCE_CREATE,
                    sleep_time=10)
 
-    @test
+    """
+        Wait until the instance is finished restoring from incremental backup.
+    """
+    @test(depends_on=[RestoreUsingBackup.test_restore_incremental])
     def test_instance_restored_incremental(self):
         try:
             self._poll(incremental_restore_instance_id)
         except exception.PollTimeOut:
             fail('Timed out')
 
 
-@test(enabled=(not CONFIG.fake_mode and CONFIG.swift_enabled),
-      depends_on_classes=[WaitForRestoreToFinish],
-      groups=[tests.DBAAS_API_BACKUPS])
+@test(enabled=(not CONFIG.fake_mode),
+      groups=[GROUP, tests.INSTANCES])
 class VerifyRestore(object):
 
     @classmethod
     def _poll(cls, instance_id, db):
         def db_is_found():
             databases = instance_info.dbaas.databases.list(instance_id)
             if db in [d.name for d in databases]:
                 return True
             else:
                 return False
 
         poll_until(db_is_found, time_out=60 * 10, sleep_time=10)
 
-    @test
+    @test(depends_on=[WaitForRestoreToFinish.
+          test_instance_restored_incremental])
     def test_database_restored_incremental(self):
         try:
             self._poll(incremental_restore_instance_id, incremental_db)
             assert_equal(total_num_dbs, len(instance_info.dbaas.databases.list(
                 incremental_restore_instance_id)))
         except exception.PollTimeOut:
             fail('Timed out')
 
 
-@test(groups=[tests.DBAAS_API_BACKUPS], enabled=CONFIG.swift_enabled,
-      depends_on_classes=[VerifyRestore])
+@test(groups=[GROUP, tests.INSTANCES])
 class DeleteRestoreInstance(object):
 
     @classmethod
     def _delete(cls, instance_id):
         """Test delete restored instance."""
         instance_info.dbaas.instances.delete(instance_id)
         assert_equal(202, instance_info.dbaas.last_http_code)
@@ -375,26 +372,26 @@
             except exceptions.NotFound:
                 return True
 
         poll_until(instance_is_gone, time_out=TIMEOUT_INSTANCE_DELETE)
         assert_raises(exceptions.NotFound, instance_info.dbaas.instances.get,
                       instance_id)
 
-    @test
+    @test(runs_after=[VerifyRestore.test_database_restored_incremental])
     def test_delete_restored_instance_incremental(self):
         try:
             self._delete(incremental_restore_instance_id)
         except exception.PollTimeOut:
             fail('Timed out')
 
 
-@test(depends_on_classes=[DeleteRestoreInstance],
-      groups=[tests.DBAAS_API_BACKUPS],
-      enabled=CONFIG.swift_enabled)
+@test(runs_after=[DeleteRestoreInstance],
+      groups=[GROUP, tests.INSTANCES])
 class DeleteBackups(object):
+
     @test
     def test_backup_delete_not_found(self):
         """Test delete unknown backup."""
         assert_raises(exceptions.NotFound, instance_info.dbaas.backups.delete,
                       'nonexistent_backup')
 
     @test
@@ -431,16 +428,15 @@
         if incremental_info is None:
             raise SkipTest("Incremental Backup not created")
         assert_raises(exceptions.NotFound, instance_info.dbaas.backups.get,
                       incremental_info.id)
 
 
 @test(depends_on=[WaitForGuestInstallationToFinish],
-      runs_after=[DeleteBackups],
-      enabled=CONFIG.swift_enabled)
+      runs_after=[DeleteBackups])
 class FakeTestHugeBackupOnSmallInstance(BackupRestoreMixin):
 
     report = CONFIG.get_report()
 
     def tweak_fake_guest(self, size):
         from trove.tests.fakes import guestagent
         guestagent.BACKUP_SIZE = size
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/configurations.py` & `trove-8.0.1/trove/tests/api/configurations.py`

 * *Files 9% similar despite different names*

```diff
@@ -26,31 +26,36 @@
 from proboscis.asserts import assert_raises
 from proboscis.asserts import assert_true
 from proboscis.asserts import fail
 from proboscis import before_class
 from proboscis.decorators import time_out
 from proboscis import SkipTest
 from proboscis import test
+import six
 from troveclient.compat import exceptions
 
 from trove.common.utils import poll_until
-from trove import tests
+from trove.tests.api.backups import RestoreUsingBackup
 from trove.tests.api.instances import assert_unprocessable
 from trove.tests.api.instances import instance_info
 from trove.tests.api.instances import InstanceTestInfo
 from trove.tests.api.instances import TIMEOUT_INSTANCE_CREATE
 from trove.tests.api.instances import TIMEOUT_INSTANCE_DELETE
+from trove.tests.api.instances import WaitForGuestInstallationToFinish
 from trove.tests.config import CONFIG
 from trove.tests.util.check import AttrCheck
 from trove.tests.util.check import CollectionCheck
 from trove.tests.util.check import TypeCheck
 from trove.tests.util import create_dbaas_client
 from trove.tests.util.mysql import create_mysql_connection
 from trove.tests.util.users import Requirements
 
+
+GROUP = "dbaas.api.configurations"
+GROUP_CONFIG_DEFINE = "dbaas.api.configurations.define"
 CONFIG_NAME = "test_configuration"
 CONFIG_DESC = "configuration description"
 
 configuration_default = None
 configuration_info = None
 configuration_href = None
 configuration_instance = InstanceTestInfo()
@@ -68,20 +73,19 @@
     except ValueError:
         return False
     return True
 
 
 # helper methods to validate configuration is applied to instance
 def _execute_query(host, user_name, password, query):
-    print("Starting to query database, host: %s, user: %s, password: %s, "
-          "query: %s" % (host, user_name, password, query))
-
+    print(host, user_name, password, query)
     with create_mysql_connection(host, user_name, password) as db:
         result = db.execute(query)
         return result
+    assert_true(False, "something went wrong in the sql connection")
 
 
 def _get_address(instance_id):
     result = instance_info.dbaas_admin.mgmt.instances.show(instance_id)
     try:
         return next(str(ip) for ip in result.ip
                     if netaddr.valid_ipv4(ip))
@@ -130,15 +134,15 @@
         instance_info.dbaas.configuration_parameters.get_parameter(
             instance_info.dbaas_datastore,
             instance_info.dbaas_datastore_version,
             name)
         resp, body = instance_info.dbaas.client.last_response
         print(resp)
         print(body)
-        return json.loads(body.decode())['type']
+        return json.loads(body)['type']
 
     # check the config values are correct
     for key, value in actual_values:
         key_type = _get_parameter_type(key)
         # mysql returns 'ON' and 'OFF' for True and False respectively
         if value == 'ON':
             converted_key_value = (str(key), 1)
@@ -170,28 +174,29 @@
         defined in the Test Config as dbaas_datastore.
         """
         default_datastore = CONFIG.get('dbaas_datastore', None)
         datastore_test_configs = CONFIG.get(default_datastore, {})
         return datastore_test_configs.get("configurations", {})
 
 
-@test(depends_on_groups=[tests.DBAAS_API_BACKUPS],
-      groups=[tests.DBAAS_API_CONFIGURATIONS])
+@test(depends_on_classes=[WaitForGuestInstallationToFinish],
+      runs_after=[RestoreUsingBackup],
+      groups=[GROUP, GROUP_CONFIG_DEFINE])
 class CreateConfigurations(ConfigurationsTestBase):
 
     @test
     def test_expected_configurations_parameters(self):
         """Test get expected configurations parameters."""
         allowed_attrs = ["configuration-parameters"]
         instance_info.dbaas.configuration_parameters.parameters(
             instance_info.dbaas_datastore,
             instance_info.dbaas_datastore_version)
         resp, body = instance_info.dbaas.client.last_response
         attrcheck = AttrCheck()
-        config_parameters_dict = json.loads(body.decode())
+        config_parameters_dict = json.loads(body)
         attrcheck.contains_allowed_attrs(
             config_parameters_dict, allowed_attrs,
             msg="Configurations parameters")
         # sanity check that a few options are in the list
         config_params_list = config_parameters_dict['configuration-parameters']
         config_param_keys = []
         for param in config_params_list:
@@ -217,28 +222,28 @@
             instance_info.dbaas_datastore_version,
             param_name)
         resp, body = instance_info.dbaas.client.last_response
         print("params: %s" % param)
         print("resp: %s" % resp)
         print("body: %s" % body)
         attrcheck = AttrCheck()
-        config_parameter_dict = json.loads(body.decode())
+        config_parameter_dict = json.loads(body)
         print("config_parameter_dict: %s" % config_parameter_dict)
         attrcheck.contains_allowed_attrs(
             config_parameter_dict,
             allowed_config_params,
             msg="Get Configuration parameter")
         assert_equal(param_name, config_parameter_dict['name'])
         with TypeCheck('ConfigurationParameter', param) as parameter:
-            parameter.has_field('name', str)
+            parameter.has_field('name', six.string_types)
             parameter.has_field('restart_required', bool)
-            parameter.has_field('max', int)
-            parameter.has_field('min', int)
-            parameter.has_field('type', str)
-            parameter.has_field('datastore_version_id', str)
+            parameter.has_field('max', six.integer_types)
+            parameter.has_field('min', six.integer_types)
+            parameter.has_field('type', six.string_types)
+            parameter.has_field('datastore_version_id', six.text_type)
 
     @test
     def test_configurations_create_invalid_values(self):
         """Test create configurations with invalid values."""
         values = '{"this_is_invalid": 123}'
         try:
             instance_info.dbaas.configurations.create(
@@ -265,118 +270,143 @@
                              CONFIG_NAME, values, CONFIG_DESC)
         values = json.dumps(expected_configs.get('out_of_bounds_under'))
         assert_unprocessable(instance_info.dbaas.configurations.create,
                              CONFIG_NAME, values, CONFIG_DESC)
 
     @test
     def test_valid_configurations_create(self):
-        """create a configuration with valid parameters from config."""
+        # create a configuration with valid parameters
         expected_configs = self.expected_default_datastore_configs()
         values = json.dumps(expected_configs.get('valid_values'))
         expected_values = json.loads(values)
         result = instance_info.dbaas.configurations.create(
             CONFIG_NAME,
             values,
             CONFIG_DESC,
             datastore=instance_info.dbaas_datastore,
             datastore_version=instance_info.dbaas_datastore_version)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 200)
         with TypeCheck('Configuration', result) as configuration:
-            configuration.has_field('name', str)
-            configuration.has_field('description', str)
+            configuration.has_field('name', six.string_types)
+            configuration.has_field('description', six.string_types)
             configuration.has_field('values', dict)
-            configuration.has_field('datastore_name', str)
-            configuration.has_field('datastore_version_id', str)
-            configuration.has_field('datastore_version_name', str)
+            configuration.has_field('datastore_name', six.string_types)
+            configuration.has_field('datastore_version_id', six.text_type)
+            configuration.has_field('datastore_version_name', six.string_types)
         global configuration_info
         configuration_info = result
         assert_equal(configuration_info.name, CONFIG_NAME)
         assert_equal(configuration_info.description, CONFIG_DESC)
         assert_equal(configuration_info.values, expected_values)
 
     @test(runs_after=[test_valid_configurations_create])
     def test_appending_to_existing_configuration(self):
-        """test_appending_to_existing_configuration"""
         # test being able to update and insert new parameter name and values
         # to an existing configuration
         expected_configs = self.expected_default_datastore_configs()
         values = json.dumps(expected_configs.get('appending_values'))
         # ensure updated timestamp is different than created
         if not CONFIG.fake_mode:
             sleep(1)
         instance_info.dbaas.configurations.edit(configuration_info.id,
                                                 values)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 200)
 
 
-@test(depends_on_classes=[CreateConfigurations],
-      groups=[tests.DBAAS_API_CONFIGURATIONS])
+@test(runs_after=[CreateConfigurations],
+      groups=[GROUP, GROUP_CONFIG_DEFINE])
 class AfterConfigurationsCreation(ConfigurationsTestBase):
 
     @test
     def test_assign_configuration_to_invalid_instance(self):
-        """test assigning to an instance that does not exist"""
+        # test assigning to an instance that does not exist
         invalid_id = "invalid-inst-id"
         try:
             instance_info.dbaas.instances.modify(invalid_id,
                                                  configuration_info.id)
         except exceptions.NotFound:
             resp, body = instance_info.dbaas.client.last_response
             assert_equal(resp.status, 404)
 
     @test
     def test_assign_configuration_to_valid_instance(self):
-        """test assigning a configuration to an instance"""
+        # test assigning a configuration to an instance
         print("instance_info.id: %s" % instance_info.id)
         print("configuration_info: %s" % configuration_info)
         print("configuration_info.id: %s" % configuration_info.id)
         config_id = configuration_info.id
         instance_info.dbaas.instances.modify(instance_info.id,
                                              configuration=config_id)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
 
+    @test
+    def test_assign_name_to_instance_using_patch(self):
+        # test assigning a name to an instance
+        new_name = 'new_name_1'
+        report = CONFIG.get_report()
+        report.log("instance_info.id: %s" % instance_info.id)
+        report.log("instance name:%s" % instance_info.name)
+        report.log("instance new name:%s" % new_name)
+        instance_info.dbaas.instances.edit(instance_info.id, name=new_name)
+        assert_equal(202, instance_info.dbaas.last_http_code)
+        check = instance_info.dbaas.instances.get(instance_info.id)
+        assert_equal(200, instance_info.dbaas.last_http_code)
+        assert_equal(check.name, new_name)
+        # Restore instance name
+        instance_info.dbaas.instances.edit(instance_info.id,
+                                           name=instance_info.name)
+        assert_equal(202, instance_info.dbaas.last_http_code)
+
+    @test
+    def test_assign_configuration_to_invalid_instance_using_patch(self):
+        # test assign config group to an invalid instance
+        invalid_id = "invalid-inst-id"
+        assert_raises(exceptions.NotFound,
+                      instance_info.dbaas.instances.edit,
+                      invalid_id, configuration=configuration_info.id)
+
     @test(depends_on=[test_assign_configuration_to_valid_instance])
     def test_assign_configuration_to_instance_with_config(self):
-        """test assigning a configuration to an instance conflicts"""
+        # test assigning a configuration to an instance that
+        # already has an assigned configuration
         config_id = configuration_info.id
         assert_raises(exceptions.BadRequest,
                       instance_info.dbaas.instances.modify, instance_info.id,
                       configuration=config_id)
 
     @test(depends_on=[test_assign_configuration_to_valid_instance])
     @time_out(30)
     def test_get_configuration_details_from_instance_validation(self):
-        """validate the configuration after attaching"""
+        # validate that the configuration was applied correctly to the instance
         print("instance_info.id: %s" % instance_info.id)
         inst = instance_info.dbaas.instances.get(instance_info.id)
         configuration_id = inst.configuration['id']
         print("configuration_info: %s" % configuration_id)
-        assert_not_equal(None, configuration_id)
+        assert_not_equal(None, inst.configuration['id'])
         _test_configuration_is_applied_to_instance(instance_info,
                                                    configuration_id)
 
-    @test(depends_on=[test_get_configuration_details_from_instance_validation])
     def test_configurations_get(self):
-        """test that the instance shows up on the assigned configuration"""
+        # test that the instance shows up on the assigned configuration
         result = instance_info.dbaas.configurations.get(configuration_info.id)
         assert_equal(configuration_info.id, result.id)
         assert_equal(configuration_info.name, result.name)
         assert_equal(configuration_info.description, result.description)
 
         # check the result field types
         with TypeCheck("configuration", result) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
-            check.has_field("description", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
+            check.has_field("description", six.string_types)
             check.has_field("values", dict)
-            check.has_field("created", str)
-            check.has_field("updated", str)
+            check.has_field("created", six.string_types)
+            check.has_field("updated", six.string_types)
             check.has_field("instance_count", int)
 
         print(result.values)
 
         # check for valid timestamps
         assert_true(_is_valid_timestamp(result.created))
         assert_true(_is_valid_timestamp(result.updated))
@@ -398,15 +428,15 @@
                 param = dbaas.configuration_parameters.get_parameter(
                     instance_info.dbaas_datastore,
                     instance_info.dbaas_datastore_version,
                     item_key)
                 if param.type == 'integer':
                     check.has_element(item_key, int)
                 if param.type == 'string':
-                    check.has_element(item_key, str)
+                    check.has_element(item_key, six.string_types)
                 if param.type == 'boolean':
                     check.has_element(item_key, bool)
 
         # Test to make sure that another user is not able to GET this config
         reqs = Requirements(is_admin=False)
         test_auth_user = instance_info.user.auth_user
         other_user = CONFIG.users.find_user(reqs, black_list=[test_auth_user])
@@ -419,30 +449,30 @@
         print(other_user)
         print(other_user.__dict__)
         other_client = create_dbaas_client(other_user)
         assert_raises(exceptions.NotFound, other_client.configurations.get,
                       configuration_info.id)
 
 
-@test(depends_on_classes=[AfterConfigurationsCreation],
-      groups=[tests.DBAAS_API_CONFIGURATIONS])
+@test(runs_after=[AfterConfigurationsCreation],
+      groups=[GROUP, GROUP_CONFIG_DEFINE])
 class ListConfigurations(ConfigurationsTestBase):
 
     @test
     def test_configurations_list(self):
         # test listing configurations show up
         result = instance_info.dbaas.configurations.list()
         for conf in result:
             with TypeCheck("Configuration", conf) as check:
-                check.has_field('id', str)
-                check.has_field('name', str)
-                check.has_field('description', str)
-                check.has_field('datastore_version_id', str)
-                check.has_field('datastore_version_name', str)
-                check.has_field('datastore_name', str)
+                check.has_field('id', six.string_types)
+                check.has_field('name', six.string_types)
+                check.has_field('description', six.string_types)
+                check.has_field('datastore_version_id', six.string_types)
+                check.has_field('datastore_version_name', six.string_types)
+                check.has_field('datastore_name', six.string_types)
 
         exists = [config for config in result if
                   config.id == configuration_info.id]
         assert_equal(1, len(exists))
         configuration = exists[0]
         assert_equal(configuration.id, configuration_info.id)
         assert_equal(configuration.name, configuration_info.name)
@@ -466,65 +496,66 @@
         result = instance_info.dbaas.instances.configuration(instance_info.id)
         global configuration_default
         configuration_default = result
         assert_not_equal(None, result.configuration)
 
     @test
     def test_changing_configuration_with_nondynamic_parameter(self):
-        """test_changing_configuration_with_nondynamic_parameter"""
+        # test that changing a non-dynamic parameter is applied to instance
+        # and show that the instance requires a restart
         expected_configs = self.expected_default_datastore_configs()
         values = json.dumps(expected_configs.get('nondynamic_parameter'))
         instance_info.dbaas.configurations.update(configuration_info.id,
                                                   values)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
 
         instance_info.dbaas.configurations.get(configuration_info.id)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 200)
 
     @test(depends_on=[test_changing_configuration_with_nondynamic_parameter])
     @time_out(20)
     def test_waiting_for_instance_in_restart_required(self):
-        """test_waiting_for_instance_in_restart_required"""
         def result_is_not_active():
             instance = instance_info.dbaas.instances.get(
                 instance_info.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return False
             else:
                 return True
         poll_until(result_is_not_active)
 
         instance = instance_info.dbaas.instances.get(instance_info.id)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 200)
+        print(instance.status)
         assert_equal('RESTART_REQUIRED', instance.status)
 
     @test(depends_on=[test_waiting_for_instance_in_restart_required])
     def test_restart_service_should_return_active(self):
-        """test_restart_service_should_return_active"""
+        # test that after restarting the instance it becomes active
         instance_info.dbaas.instances.restart(instance_info.id)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
 
         def result_is_active():
             instance = instance_info.dbaas.instances.get(
                 instance_info.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
-                assert_true(instance.status in ['REBOOT', 'SHUTDOWN'])
+                assert_equal("REBOOT", instance.status)
                 return False
         poll_until(result_is_active)
 
     @test(depends_on=[test_restart_service_should_return_active])
     @time_out(30)
     def test_get_configuration_details_from_instance_validation(self):
-        """test_get_configuration_details_from_instance_validation"""
+        # validate that the configuraiton was applied correctly to the instance
         inst = instance_info.dbaas.instances.get(instance_info.id)
         configuration_id = inst.configuration['id']
         assert_not_equal(None, inst.configuration['id'])
         _test_configuration_is_applied_to_instance(instance_info,
                                                    configuration_id)
 
     @test(depends_on=[test_configurations_list])
@@ -536,21 +567,21 @@
         assert_equal(1, len(list_config))
         details_config = instance_info.dbaas.configurations.get(
             configuration_info.id)
         assert_equal(list_config[0].created, details_config.created)
         assert_equal(list_config[0].updated, details_config.updated)
 
 
-@test(depends_on_classes=[ListConfigurations],
-      groups=[tests.DBAAS_API_CONFIGURATIONS])
+@test(runs_after=[ListConfigurations],
+      groups=[GROUP, GROUP_CONFIG_DEFINE])
 class StartInstanceWithConfiguration(ConfigurationsTestBase):
 
     @test
     def test_start_instance_with_configuration(self):
-        """test that a new instance will apply the configuration on create"""
+        # test that a new instance will apply the configuration on create
         global configuration_instance
         databases = []
         databases.append({"name": "firstdbconfig", "character_set": "latin2",
                           "collate": "latin2_general_ci"})
         databases.append({"name": "db2"})
         configuration_instance.databases = databases
         users = []
@@ -579,77 +610,76 @@
             configuration=configuration_href)
         assert_equal(200, instance_info.dbaas.last_http_code)
         assert_equal("BUILD", result.status)
         configuration_instance.id = result.id
 
 
 @test(depends_on_classes=[StartInstanceWithConfiguration],
-      groups=[tests.DBAAS_API_CONFIGURATIONS])
+      runs_after_groups=['dbaas.api.backups'],
+      groups=[GROUP])
 class WaitForConfigurationInstanceToFinish(ConfigurationsTestBase):
 
     @test
     @time_out(TIMEOUT_INSTANCE_CREATE)
     def test_instance_with_configuration_active(self):
-        """wait for the instance created with configuration"""
+        # wait for the instance to become active
 
         def result_is_active():
             instance = instance_info.dbaas.instances.get(
                 configuration_instance.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
                 assert_equal("BUILD", instance.status)
                 return False
 
         poll_until(result_is_active)
 
     @test(depends_on=[test_instance_with_configuration_active])
     @time_out(30)
     def test_get_configuration_details_from_instance_validation(self):
-        """Test configuration is applied correctly to the instance."""
+        # validate that the configuration was applied correctly to the instance
         inst = instance_info.dbaas.instances.get(configuration_instance.id)
         configuration_id = inst.configuration['id']
-        assert_not_equal(None, configuration_id)
+        assert_not_equal(None, inst.configuration['id'])
         _test_configuration_is_applied_to_instance(configuration_instance,
                                                    configuration_id)
 
 
-@test(depends_on=[WaitForConfigurationInstanceToFinish],
-      groups=[tests.DBAAS_API_CONFIGURATIONS])
+@test(runs_after=[WaitForConfigurationInstanceToFinish], groups=[GROUP])
 class DeleteConfigurations(ConfigurationsTestBase):
 
     @before_class
     def setUp(self):
         # need to store the parameter details that will be deleted
         config_param_name = sql_variables[1]
         instance_info.dbaas.configuration_parameters.get_parameter(
             instance_info.dbaas_datastore,
             instance_info.dbaas_datastore_version,
             config_param_name)
         resp, body = instance_info.dbaas.client.last_response
         print(resp)
         print(body)
-        self.config_parameter_dict = json.loads(body.decode())
+        self.config_parameter_dict = json.loads(body)
 
     @after_class(always_run=True)
     def tearDown(self):
         # need to "undelete" the parameter that was deleted from the mgmt call
-        if instance_info.dbaas:
-            ds = instance_info.dbaas_datastore
-            ds_v = instance_info.dbaas_datastore_version
-            version = instance_info.dbaas.datastore_versions.get(
-                ds, ds_v)
-            client = instance_info.dbaas_admin.mgmt_configs
-            print(self.config_parameter_dict)
-            client.create(version.id,
-                          self.config_parameter_dict['name'],
-                          self.config_parameter_dict['restart_required'],
-                          self.config_parameter_dict['type'],
-                          self.config_parameter_dict['max'],
-                          self.config_parameter_dict['min'])
+        ds = instance_info.dbaas_datastore
+        ds_v = instance_info.dbaas_datastore_version
+        version = instance_info.dbaas.datastore_versions.get(
+            ds, ds_v)
+        client = instance_info.dbaas_admin.mgmt_configs
+        print(self.config_parameter_dict)
+        client.create(version.id,
+                      self.config_parameter_dict['name'],
+                      self.config_parameter_dict['restart_required'],
+                      self.config_parameter_dict['type'],
+                      self.config_parameter_dict['max'],
+                      self.config_parameter_dict['min'])
 
     @test
     def test_delete_invalid_configuration_not_found(self):
         # test deleting a configuration that does not exist throws exception
         invalid_configuration_id = "invalid-config-id"
         assert_raises(exceptions.NotFound,
                       instance_info.dbaas.configurations.delete,
@@ -681,174 +711,195 @@
         assert_raises(exceptions.BadRequest,
                       instance_info.dbaas.configurations.delete,
                       configuration_info.id)
 
     @test(depends_on=[test_unable_delete_instance_configurations])
     @time_out(30)
     def test_unassign_configuration_from_instances(self):
-        """test to unassign configuration from instance"""
-        instance_info.dbaas.instances.update(configuration_instance.id,
-                                             remove_configuration=True)
+        # test to unassign configuration from instance
+        instance_info.dbaas.instances.modify(configuration_instance.id,
+                                             configuration="")
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
-
-        instance_info.dbaas.instances.update(instance_info.id,
-                                             remove_configuration=True)
+        instance_info.dbaas.instances.get(configuration_instance.id)
+        # test that config group is not removed
+        instance_info.dbaas.instances.modify(instance_info.id,
+                                             configuration=None)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
         instance_info.dbaas.instances.get(instance_info.id)
 
         def result_has_no_configuration():
             instance = instance_info.dbaas.instances.get(inst_info.id)
             if hasattr(instance, 'configuration'):
                 return False
             else:
                 return True
-
         inst_info = instance_info
         poll_until(result_has_no_configuration)
         inst_info = configuration_instance
         poll_until(result_has_no_configuration)
-
         instance = instance_info.dbaas.instances.get(instance_info.id)
         assert_equal('RESTART_REQUIRED', instance.status)
 
     @test(depends_on=[test_unassign_configuration_from_instances])
     def test_assign_in_wrong_state(self):
         # test assigning a config to an instance in RESTART state
         assert_raises(exceptions.BadRequest,
                       instance_info.dbaas.instances.modify,
                       configuration_instance.id,
                       configuration=configuration_info.id)
 
     @test(depends_on=[test_assign_in_wrong_state])
     def test_no_instances_on_configuration(self):
-        """test_no_instances_on_configuration"""
+        # test there is no configuration on the instance after unassigning
         result = instance_info.dbaas.configurations.get(configuration_info.id)
         assert_equal(configuration_info.id, result.id)
         assert_equal(configuration_info.name, result.name)
         assert_equal(configuration_info.description, result.description)
         assert_equal(result.instance_count, 0)
         print(configuration_instance.id)
         print(instance_info.id)
 
     @test(depends_on=[test_unassign_configuration_from_instances])
     @time_out(120)
+    def test_restart_service_after_unassign_return_active(self):
+        def result_is_not_active():
+            instance = instance_info.dbaas.instances.get(
+                instance_info.id)
+            if instance.status == "ACTIVE":
+                return False
+            else:
+                return True
+        poll_until(result_is_not_active)
+
+        config = instance_info.dbaas.configurations.list()
+        print(config)
+        instance = instance_info.dbaas.instances.get(instance_info.id)
+        print(instance.__dict__)
+        resp, body = instance_info.dbaas.client.last_response
+        assert_equal(resp.status, 200)
+        print(instance.status)
+        assert_equal('RESTART_REQUIRED', instance.status)
+
+    @test(depends_on=[test_restart_service_after_unassign_return_active])
+    @time_out(120)
     def test_restart_service_should_return_active(self):
-        """test that after restarting the instance it becomes active"""
+        # test that after restarting the instance it becomes active
         instance_info.dbaas.instances.restart(instance_info.id)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
 
         def result_is_active():
             instance = instance_info.dbaas.instances.get(
                 instance_info.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
                 assert_equal("REBOOT", instance.status)
                 return False
         poll_until(result_is_active)
 
     @test(depends_on=[test_restart_service_should_return_active])
     def test_assign_config_and_name_to_instance_using_patch(self):
-        """test_assign_config_and_name_to_instance_using_patch"""
+        # test assigning a configuration and name to an instance
         new_name = 'new_name'
         report = CONFIG.get_report()
         report.log("instance_info.id: %s" % instance_info.id)
         report.log("configuration_info: %s" % configuration_info)
         report.log("configuration_info.id: %s" % configuration_info.id)
         report.log("instance name:%s" % instance_info.name)
         report.log("instance new name:%s" % new_name)
         saved_name = instance_info.name
         config_id = configuration_info.id
-        instance_info.dbaas.instances.update(instance_info.id,
-                                             configuration=config_id,
-                                             name=new_name)
+        instance_info.dbaas.instances.edit(instance_info.id,
+                                           configuration=config_id,
+                                           name=new_name)
         assert_equal(202, instance_info.dbaas.last_http_code)
         check = instance_info.dbaas.instances.get(instance_info.id)
         assert_equal(200, instance_info.dbaas.last_http_code)
         assert_equal(check.name, new_name)
 
         # restore instance name
-        instance_info.dbaas.instances.update(instance_info.id,
-                                             name=saved_name)
+        instance_info.dbaas.instances.edit(instance_info.id,
+                                           name=saved_name)
         assert_equal(202, instance_info.dbaas.last_http_code)
 
         instance = instance_info.dbaas.instances.get(instance_info.id)
         assert_equal('RESTART_REQUIRED', instance.status)
         # restart to be sure configuration is applied
         instance_info.dbaas.instances.restart(instance_info.id)
         assert_equal(202, instance_info.dbaas.last_http_code)
         sleep(2)
 
         def result_is_active():
             instance = instance_info.dbaas.instances.get(
                 instance_info.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
                 assert_equal("REBOOT", instance.status)
                 return False
         poll_until(result_is_active)
         # test assigning a configuration to an instance that
         # already has an assigned configuration with patch
         config_id = configuration_info.id
         assert_raises(exceptions.BadRequest,
-                      instance_info.dbaas.instances.update,
+                      instance_info.dbaas.instances.edit,
                       instance_info.id, configuration=config_id)
 
     @test(runs_after=[test_assign_config_and_name_to_instance_using_patch])
     def test_unassign_configuration_after_patch(self):
-        """Remove the configuration from the instance"""
-        instance_info.dbaas.instances.update(instance_info.id,
-                                             remove_configuration=True)
+        # remove the configuration from the instance
+        instance_info.dbaas.instances.edit(instance_info.id,
+                                           remove_configuration=True)
         assert_equal(202, instance_info.dbaas.last_http_code)
         instance = instance_info.dbaas.instances.get(instance_info.id)
         assert_equal('RESTART_REQUIRED', instance.status)
         # restart to be sure configuration has been unassigned
         instance_info.dbaas.instances.restart(instance_info.id)
         assert_equal(202, instance_info.dbaas.last_http_code)
         sleep(2)
 
         def result_is_active():
             instance = instance_info.dbaas.instances.get(
                 instance_info.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
             else:
                 assert_equal("REBOOT", instance.status)
                 return False
-
         poll_until(result_is_active)
         result = instance_info.dbaas.configurations.get(configuration_info.id)
         assert_equal(result.instance_count, 0)
 
     @test
     def test_unassign_configuration_from_invalid_instance_using_patch(self):
         # test unassign config group from an invalid instance
         invalid_id = "invalid-inst-id"
         try:
-            instance_info.dbaas.instances.update(invalid_id,
-                                                 remove_configuration=True)
+            instance_info.dbaas.instances.edit(invalid_id,
+                                               remove_configuration=True)
         except exceptions.NotFound:
             resp, body = instance_info.dbaas.client.last_response
             assert_equal(resp.status, 404)
 
     @test(runs_after=[test_unassign_configuration_after_patch])
     def test_delete_unassigned_configuration(self):
-        """test_delete_unassigned_configuration"""
+        # test that we can delete the configuration after no instances are
+        # assigned to it any longer
         instance_info.dbaas.configurations.delete(configuration_info.id)
         resp, body = instance_info.dbaas.client.last_response
         assert_equal(resp.status, 202)
 
     @test(depends_on=[test_delete_unassigned_configuration])
     @time_out(TIMEOUT_INSTANCE_DELETE)
     def test_delete_configuration_instance(self):
-        """test_delete_configuration_instance"""
+        # test that we can delete the instance even though there is a
+        # configuration applied to the instance
         instance_info.dbaas.instances.delete(configuration_instance.id)
         assert_equal(202, instance_info.dbaas.last_http_code)
 
         def instance_is_gone():
             try:
                 instance_info.dbaas.instances.get(configuration_instance.id)
                 return False
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/databases.py` & `trove-8.0.1/trove/tests/api/databases.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,29 +15,57 @@
 import time
 
 from proboscis.asserts import assert_equal
 from proboscis.asserts import assert_false
 from proboscis.asserts import assert_raises
 from proboscis.asserts import assert_true
 from proboscis import before_class
+from proboscis.decorators import time_out
 from proboscis import test
 from troveclient.compat import exceptions
 
 from trove import tests
+from trove.tests.api.instances import GROUP_START
 from trove.tests.api.instances import instance_info
 from trove.tests import util
 from trove.tests.util import test_config
 
+GROUP = "dbaas.api.databases"
 FAKE = test_config.values['fake_mode']
 
 
-@test(depends_on_groups=[tests.DBAAS_API_USERS_ACCESS],
-      groups=[tests.DBAAS_API_DATABASES])
+@test(depends_on_groups=[GROUP_START],
+      groups=[tests.INSTANCES, "dbaas.guest.mysql"],
+      enabled=not test_config.values['fake_mode'])
+class TestMysqlAccess(object):
+    """
+        Make sure that MySQL server was secured.
+    """
+
+    @time_out(60 * 2)
+    @test
+    def test_mysql_admin(self):
+        """Ensure we aren't allowed access with os_admin and wrong password."""
+        util.mysql_connection().assert_fails(
+            instance_info.get_address(), "os_admin", "asdfd-asdf234")
+
+    @test
+    def test_mysql_root(self):
+        """Ensure we aren't allowed access with root and wrong password."""
+        util.mysql_connection().assert_fails(
+            instance_info.get_address(), "root", "dsfgnear")
+
+
+@test(depends_on_groups=[GROUP_START],
+      depends_on_classes=[TestMysqlAccess],
+      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES])
 class TestDatabases(object):
-    """Test the creation and deletion of additional MySQL databases"""
+    """
+    Test the creation and deletion of additional MySQL databases
+    """
 
     dbname = "third #?@some_-"
     dbname_urlencoded = "third%20%23%3F%40some_-"
 
     dbname2 = "seconddb"
     created_dbs = [dbname, dbname2]
     system_dbs = ['information_schema', 'mysql', 'lost+found']
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/datastores.py` & `trove-8.0.1/trove/tests/api/datastores.py`

 * *Files 7% similar despite different names*

```diff
@@ -15,112 +15,100 @@
 
 
 from nose.tools import assert_equal
 from proboscis.asserts import assert_raises
 from proboscis.asserts import assert_true
 from proboscis import before_class
 from proboscis import test
+import six
 from troveclient.compat import exceptions
 
 from trove import tests
 from trove.tests.util.check import TypeCheck
 from trove.tests.util import create_dbaas_client
 from trove.tests.util import test_config
 from trove.tests.util.users import Requirements
 
+
+GROUP = "dbaas.api.datastores"
 NAME = "nonexistent"
 
 
-@test(groups=[tests.DBAAS_API_DATASTORES],
-      depends_on_groups=[tests.DBAAS_API_VERSIONS])
+@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
+      depends_on_groups=["services.initialize"])
 class Datastores(object):
+
     @before_class
     def setUp(self):
         rd_user = test_config.users.find_user(
             Requirements(is_admin=False, services=["trove"]))
         rd_admin = test_config.users.find_user(
             Requirements(is_admin=True, services=["trove"]))
         self.rd_client = create_dbaas_client(rd_user)
         self.rd_admin = create_dbaas_client(rd_admin)
 
     @test
     def test_datastore_list_attrs(self):
         datastores = self.rd_client.datastores.list()
         for datastore in datastores:
             with TypeCheck('Datastore', datastore) as check:
-                check.has_field("id", str)
-                check.has_field("name", str)
+                check.has_field("id", six.string_types)
+                check.has_field("name", six.string_types)
                 check.has_field("links", list)
                 check.has_field("versions", list)
 
     @test
     def test_datastore_get(self):
         # Test get by name
         datastore_by_name = self.rd_client.datastores.get(
             test_config.dbaas_datastore)
         with TypeCheck('Datastore', datastore_by_name) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
             check.has_field("links", list)
         assert_equal(datastore_by_name.name, test_config.dbaas_datastore)
 
         # test get by id
         datastore_by_id = self.rd_client.datastores.get(
             datastore_by_name.id)
         with TypeCheck('Datastore', datastore_by_id) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
             check.has_field("links", list)
             check.has_field("versions", list)
         assert_equal(datastore_by_id.id, datastore_by_name.id)
 
     @test
     def test_datastore_not_found(self):
         try:
             assert_raises(exceptions.NotFound,
                           self.rd_client.datastores.get, NAME)
         except exceptions.BadRequest as e:
             assert_equal(e.message,
                          "Datastore '%s' cannot be found." % NAME)
 
     @test
-    def test_create_inactive_datastore_by_admin(self):
-        datastores = self.rd_admin.datastores.list()
-        for ds in datastores:
-            if ds.name == test_config.dbaas_datastore_name_no_versions:
-                for version in ds.versions:
-                    if version['name'] == 'inactive_version':
-                        return
-
-        # Create datastore version for testing
-        # 'Test_Datastore_1' is also used in other test cases.
-        # Will be deleted in test_delete_datastore_version
-        self.rd_admin.mgmt_datastore_versions.create(
-            "inactive_version", test_config.dbaas_datastore_name_no_versions,
-            "test_manager", None, image_tags=['trove'],
-            active='false', default='false'
-        )
-
-    @test(depends_on=[test_create_inactive_datastore_by_admin])
     def test_datastore_with_no_active_versions_is_hidden(self):
         datastores = self.rd_client.datastores.list()
         name_list = [datastore.name for datastore in datastores]
+        name_no_versions = test_config.dbaas_datastore_name_no_versions
+        assert_true(name_no_versions not in name_list)
 
-        assert_true(
-            test_config.dbaas_datastore_name_no_versions not in name_list)
-
-    @test(depends_on=[test_create_inactive_datastore_by_admin])
+    @test
     def test_datastore_with_no_active_versions_is_visible_for_admin(self):
         datastores = self.rd_admin.datastores.list()
         name_list = [datastore.name for datastore in datastores]
-        assert_true(test_config.dbaas_datastore_name_no_versions in name_list)
+        name_no_versions = test_config.dbaas_datastore_name_no_versions
+        assert_true(name_no_versions in name_list)
 
 
-@test(groups=[tests.DBAAS_API_DATASTORES])
+@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES],
+      depends_on_groups=["services.initialize"])
 class DatastoreVersions(object):
+
     @before_class
     def setUp(self):
         rd_user = test_config.users.find_user(
             Requirements(is_admin=False, services=["trove"]))
         self.rd_client = create_dbaas_client(rd_user)
         self.datastore_active = self.rd_client.datastores.get(
             test_config.dbaas_datastore)
@@ -129,64 +117,68 @@
 
     @test
     def test_datastore_version_list_attrs(self):
         versions = self.rd_client.datastore_versions.list(
             self.datastore_active.name)
         for version in versions:
             with TypeCheck('DatastoreVersion', version) as check:
-                check.has_field("id", str)
-                check.has_field("name", str)
+                check.has_field("id", six.string_types)
+                check.has_field("name", six.string_types)
                 check.has_field("links", list)
 
     @test
     def test_datastore_version_get_attrs(self):
         version = self.rd_client.datastore_versions.get(
             self.datastore_active.name, self.datastore_version_active.name)
         with TypeCheck('DatastoreVersion', version) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
-            check.has_field("datastore", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
+            check.has_field("datastore", six.string_types)
             check.has_field("links", list)
         assert_equal(version.name, self.datastore_version_active.name)
 
     @test
     def test_datastore_version_get_by_uuid_attrs(self):
         version = self.rd_client.datastore_versions.get_by_uuid(
             self.datastore_version_active.id)
         with TypeCheck('DatastoreVersion', version) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
-            check.has_field("datastore", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
+            check.has_field("datastore", six.string_types)
             check.has_field("links", list)
         assert_equal(version.name, self.datastore_version_active.name)
 
     @test
     def test_datastore_version_not_found(self):
-        assert_raises(exceptions.BadRequest,
-                      self.rd_client.datastore_versions.get,
-                      self.datastore_active.name, NAME)
+        try:
+            assert_raises(exceptions.NotFound,
+                          self.rd_client.datastore_versions.get,
+                          self.datastore_active.name, NAME)
+        except exceptions.BadRequest as e:
+            assert_equal(e.message,
+                         "Datastore version '%s' cannot be found." % NAME)
 
     @test
     def test_datastore_version_list_by_uuid(self):
         versions = self.rd_client.datastore_versions.list(
             self.datastore_active.id)
         for version in versions:
             with TypeCheck('DatastoreVersion', version) as check:
-                check.has_field("id", str)
-                check.has_field("name", str)
+                check.has_field("id", six.string_types)
+                check.has_field("name", six.string_types)
                 check.has_field("links", list)
 
     @test
     def test_datastore_version_get_by_uuid(self):
         version = self.rd_client.datastore_versions.get(
             self.datastore_active.id, self.datastore_version_active.id)
         with TypeCheck('DatastoreVersion', version) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
-            check.has_field("datastore", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
+            check.has_field("datastore", six.string_types)
             check.has_field("links", list)
         assert_equal(version.name, self.datastore_version_active.name)
 
     @test
     def test_datastore_version_invalid_uuid(self):
         try:
             self.rd_client.datastore_versions.get_by_uuid(
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/instances.py` & `trove-8.0.1/integration/tests/examples/examples/example_generation.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1146 +1,1042 @@
-# Copyright 2011 OpenStack Foundation
-# All Rights Reserved.
-#
-#    Licensed under the Apache License, Version 2.0 (the "License"); you may
-#    not use this file except in compliance with the License. You may obtain
-#    a copy of the License at
-#
-#         http://www.apache.org/licenses/LICENSE-2.0
-#
-#    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-#    License for the specific language governing permissions and limitations
-#    under the License.
-
-import netaddr
+import httplib2
+import json
 import os
+import re
+import sys
 import time
-import unittest
-import uuid
+from urlparse import urlparse
+import xml.dom.minidom
 
-from proboscis import asserts
-from proboscis.asserts import assert_equal
-from proboscis.asserts import assert_not_equal
-from proboscis.asserts import assert_raises
-from proboscis.asserts import assert_true
-from proboscis.asserts import fail
 from proboscis import before_class
-from proboscis.decorators import time_out
-from proboscis import SkipTest
 from proboscis import test
-from troveclient.compat import exceptions
+from proboscis import TestProgram
+from proboscis.asserts import *
+from proboscis.asserts import Check
 
-from trove.common import cfg
-from trove.common.utils import poll_until
-from trove.datastore import models as datastore_models
-from trove import tests
-from trove.tests.config import CONFIG
-from trove.tests import util
-from trove.tests.util.check import AttrCheck
-from trove.tests.util import create_dbaas_client
-from trove.tests.util import test_config
-from trove.tests.util.usage import create_usage_verifier
-from trove.tests.util.users import Requirements
-
-CONF = cfg.CONF
-
-FAKE = test_config.values['fake_mode']
-
-TIMEOUT_INSTANCE_CREATE = 60 * 32
-TIMEOUT_INSTANCE_RESTORE = 60 * 60
-TIMEOUT_INSTANCE_DELETE = 120
+from troveclient.compat import Dbaas
+from troveclient.compat import TroveHTTPClient
 
 
-class InstanceTestInfo(object):
-    """Stores new instance information used by dependent tests."""
+from client import ConfigFile
+from client import SnippetWriter
+from client import JsonClient
+from client import XmlClient
 
-    def __init__(self):
-        self.dbaas = None  # The rich client instance used by these tests.
-        self.dbaas_admin = None  # The rich client with admin access.
-        self.dbaas_flavor = None  # The flavor object of the instance.
-        self.dbaas_flavor_href = None  # The flavor of the instance.
-        self.dbaas_datastore = None  # The datastore id
-        self.dbaas_datastore_version = None  # The datastore version id
-        self.id = None  # The ID of the instance in the database.
-        self.local_id = None
-
-        # The IP address of the database instance for the user.
-        self.address = None
-        # The management network IP address.
-        self.mgmt_address = None
-
-        self.nics = None  # The dict of type/id for nics used on the instance.
-        shared_network = CONFIG.get('shared_network', None)
-        if shared_network:
-            self.nics = [{'net-id': shared_network}]
-        self.initial_result = None  # The initial result from the create call.
-        self.result = None  # The instance info returned by the API
-        self.nova_client = None  # The instance of novaclient.
-        self.volume_client = None  # The instance of the volume client.
-        self.name = None  # Test name, generated each test run.
-        self.pid = None  # The process ID of the instance.
-        self.user = None  # The user instance who owns the instance.
-        self.admin_user = None  # The admin user for the management interfaces.
-        self.volume = None  # The volume the instance will have.
-        self.volume_id = None  # Id for the attached vo186lume
-        self.storage = None  # The storage device info for the volumes.
-        self.databases = None  # The databases created on the instance.
-        self.host_info = None  # Host Info before creating instances
-        self.user_context = None  # A regular user context
-        self.users = None  # The users created on the instance.
-        self.consumer = create_usage_verifier()
-        self.flavors = None  # The cache of Nova flavors.
-
-    def find_default_flavor(self):
-        if EPHEMERAL_SUPPORT:
-            flavor_name = CONFIG.values.get('instance_eph_flavor_name',
-                                            'eph.rd-tiny')
-        else:
-            flavor_name = CONFIG.values.get('instance_flavor_name', 'm1.tiny')
 
-        flavor = None
-        flavor_href = None
+print_req = True
 
-        for item in self.flavors:
-            if item.name == flavor_name:
-                flavor = item
-                flavor_href = item.id
-                break
-
-        asserts.assert_is_not_none(flavor)
-        asserts.assert_is_not_none(flavor_href)
-
-        return flavor, flavor_href
-
-    def get_address(self, mgmt=False):
-        if mgmt:
-            if self.mgmt_address:
-                return self.mgmt_address
-
-            mgmt_netname = test_config.get("trove_mgmt_network", "trove-mgmt")
-            result = self.dbaas_admin.mgmt.instances.show(self.id)
-            mgmt_interfaces = result.server['addresses'].get(mgmt_netname, [])
-            mgmt_addresses = [str(inf["addr"]) for inf in mgmt_interfaces
-                              if inf["version"] == 4]
-            if len(mgmt_addresses) == 0:
-                fail("No IPV4 ip found for management network.")
-            self.mgmt_address = mgmt_addresses[0]
-            return self.mgmt_address
-        else:
-            if self.address:
-                return self.address
 
-            result = self.dbaas.instances.get(self.id)
-            addresses = [str(ip) for ip in result.ip if netaddr.valid_ipv4(ip)]
-            if len(addresses) == 0:
-                fail("No IPV4 ip found for database network.")
-            self.address = addresses[0]
-            return self.address
-
-    def get_local_id(self):
-        mgmt_instance = self.dbaas_admin.management.show(self.id)
-        return mgmt_instance.server["local_id"]
-
-    def get_volume_filesystem_size(self):
-        mgmt_instance = self.dbaas_admin.management.show(self.id)
-        return mgmt_instance.volume["total"]
-
-
-# The two variables are used below by tests which depend on an instance
-# existing.
-instance_info = InstanceTestInfo()
-dbaas = None  # Rich client used throughout this test.
-dbaas_admin = None  # Same as above, with admin privs.
-ROOT_ON_CREATE = CONFIG.get('root_on_create', False)
-VOLUME_SUPPORT = CONFIG.get('trove_volume_support', False)
-EPHEMERAL_SUPPORT = not VOLUME_SUPPORT and CONFIG.get('device_path',
-                                                      '/dev/vdb') is not None
-ROOT_PARTITION = not VOLUME_SUPPORT and CONFIG.get('device_path',
-                                                   None) is None
-
-
-# This is like a cheat code which allows the tests to skip creating a new
-# instance and use an old one.
-def existing_instance():
-    return os.environ.get("TESTS_USE_INSTANCE_ID", None)
-
-
-def create_new_instance():
-    return existing_instance() is None
-
-
-def assert_unprocessable(func, *args):
-    try:
-        func(*args)
-        # If the exception didn't get raised, but the instance is still in
-        # the BUILDING state, that's a bug.
-        result = dbaas.instances.get(instance_info.id)
-        if result.status == "BUILD":
-            fail("When an instance is being built, this function should "
-                 "always raise UnprocessableEntity.")
-    except exceptions.UnprocessableEntity:
-        assert_equal(422, dbaas.last_http_code)
-        pass  # Good
-
-
-class CheckInstance(AttrCheck):
-    """Class to check various attributes of Instance details."""
-
-    def __init__(self, instance):
-        super(CheckInstance, self).__init__()
-        self.instance = instance
-
-    def flavor(self):
-        if 'flavor' not in self.instance:
-            self.fail("'flavor' not found in instance.")
-        else:
-            allowed_attrs = ['id', 'links']
-            self.contains_allowed_attrs(
-                self.instance['flavor'], allowed_attrs,
-                msg="Flavor")
-
-    def datastore(self):
-        if 'datastore' not in self.instance:
-            self.fail("'datastore' not found in instance.")
-        else:
-            allowed_attrs = ['type', 'version', 'version_number']
-            self.contains_allowed_attrs(
-                self.instance['datastore'], allowed_attrs,
-                msg="datastore")
-
-    def volume_key_exists(self):
-        if 'volume' not in self.instance:
-            self.fail("'volume' not found in instance.")
-            return False
-        return True
-
-    def volume(self):
-        if not VOLUME_SUPPORT:
-            return
-        if self.volume_key_exists():
-            allowed_attrs = ['size']
-            if not create_new_instance():
-                allowed_attrs.append('used')
-            self.contains_allowed_attrs(
-                self.instance['volume'], allowed_attrs,
-                msg="Volumes")
-
-    def used_volume(self):
-        if not VOLUME_SUPPORT:
-            return
-        if self.volume_key_exists():
-            allowed_attrs = ['size', 'used']
-            print(self.instance)
-            self.contains_allowed_attrs(
-                self.instance['volume'], allowed_attrs,
-                msg="Volumes")
-
-    def volume_mgmt(self):
-        if not VOLUME_SUPPORT:
-            return
-        if self.volume_key_exists():
-            allowed_attrs = ['description', 'id', 'name', 'size']
-            self.contains_allowed_attrs(
-                self.instance['volume'], allowed_attrs,
-                msg="Volumes")
-
-    def addresses(self):
-        allowed_attrs = ['addr', 'version']
-        print(self.instance)
-        networks = ['usernet']
-        for network in networks:
-            for address in self.instance['addresses'][network]:
-                self.contains_allowed_attrs(
-                    address, allowed_attrs,
-                    msg="Address")
-
-    def guest_status(self):
-        allowed_attrs = ['created_at', 'deleted', 'deleted_at', 'instance_id',
-                         'state', 'state_description', 'updated_at']
-        self.contains_allowed_attrs(
-            self.instance['guest_status'], allowed_attrs,
-            msg="Guest status")
-
-    def mgmt_volume(self):
-        if not VOLUME_SUPPORT:
-            return
-        allowed_attrs = ['description', 'id', 'name', 'size']
-        self.contains_allowed_attrs(
-            self.instance['volume'], allowed_attrs,
-            msg="Volume")
-
-    def replica_of(self):
-        if 'replica_of' not in self.instance:
-            self.fail("'replica_of' not found in instance.")
-        else:
-            allowed_attrs = ['id', 'links']
-            self.contains_allowed_attrs(
-                self.instance['replica_of'], allowed_attrs,
-                msg="Replica-of links not found")
-            self.links(self.instance['replica_of']['links'])
-
-    def slaves(self):
-        if 'replicas' not in self.instance:
-            self.fail("'replicas' not found in instance.")
+class ExampleClient(object):
+
+    def __init__(self, config_file):
+        if not os.path.exists(config_file):
+            raise RuntimeError("Could not find Example CONF at %s." %
+                               config_file)
+        file_contents = open(config_file, "r").read()
+        try:
+            config = json.loads(file_contents)
+        except Exception as exception:
+            msg = 'Error loading config file "%s".' % config_file
+            raise RuntimeError(msg, exception)
+
+        self.directory = config.get("directory", None)
+        if not self.directory.endswith('/'):
+            self.directory += '/'
+        print "directory = %s" % self.directory
+        self.api_url = config.get("api_url", None)
+        print "api_url = %s" % self.api_url
+        #auth
+        auth_url = config.get("auth_url", None)
+        print "auth_url = %s" % auth_url
+        username = config.get("username", None)
+        print "username = %s" % username
+        password = config.get("password", None)
+        print "password = %s" % password
+        self.tenant = config.get("tenant", None)
+        self.replace_host = config.get("replace_host", None)
+        print "tenant = %s" % self.tenant
+        self.replace_dns_hostname = config.get("replace_dns_hostname", None)
+        if auth_url:
+            auth_id, tenant_id = self.get_auth_token_id_tenant_id(auth_url,
+                                                                  username,
+                                                                  password)
+        else:
+            auth_id = self.tenant
+            tenant_id = self.tenant
+
+        print "id = %s" % auth_id
+        self.headers = {
+            'X-Auth-Token': str(auth_id)
+        }
+        print "tenantID = %s" % tenant_id
+        self.tenantID = tenant_id
+        self.dbaas_url = "%s/v1.0/%s" % (self.api_url, self.tenantID)
+
+    def write_request_file(self, name, content_type, url, method,
+                           req_headers, request_body):
+        def write_request():
+            return self.output_request(url, req_headers, request_body,
+                                       content_type, method)
+        if print_req:
+            print("\t%s req url:%s" % (content_type, url))
+            print("\t%s req method:%s" % (content_type, method))
+            print("\t%s req headers:%s" % (content_type, req_headers))
+            print("\t%s req body:%s" % (content_type, request_body))
+        self.write_file(name, content_type, url, method, write_request)
+
+    def write_response_file(self, name, content_type, url, method,
+                           resp, resp_content):
+        def write_response():
+            return self.output_response(resp, resp_content, content_type)
+        self.write_file(name, content_type, url, method, write_response)
+        if print_req:
+            print("\t%s resp:%s" % (content_type, resp))
+            print("\t%s resp content:%s" % (content_type, resp_content))
+
+    def write_file(self, name, content_type, url, method, func):
+        filename = "%sdb-%s-request.%s" % (self.directory, name, content_type)
+        with open(filename, "w") as file:
+            output = func()
+            output = output.replace(self.tenantID, '1234')
+            if self.replace_host:
+                output = output.replace(self.api_url, self.replace_host)
+                pre_host_port = urlparse(self.api_url).netloc
+                post_host = urlparse(self.replace_host).netloc
+                output = output.replace(pre_host_port, post_host)
+
+            file.write(output)
+
+    def version_http_call(self, name, method, json, xml,
+                           output=True, print_resp=False):
+        json['url'] = "%s/%s" % (self.api_url, json['url'])
+        xml['url'] = "%s/%s" % (self.api_url, xml['url'])
+        return self.make_request(name, method, json, xml, output, print_resp)
+
+    def http_call(self, name, method, url, json, xml,
+                  output=True, print_resp=False):
+        json['url'] = "%s/%s" % (self.dbaas_url, json['url'])
+        xml['url'] = "%s/%s" % (self.dbaas_url, xml['url'])
+        return self.make_request(name, method, json, xml, output, print_resp)
+
+    # print_req and print_resp for debugging purposes
+    def make_request(self, name, method, json, xml,
+                     output=True, print_resp=False):
+        name = name.replace('_', '-')
+        print "http call for %s" % name
+        http = httplib2.Http(disable_ssl_certificate_validation=True)
+        req_headers = {'User-Agent': "python-example-client",
+                       'Content-Type': "application/json",
+                       'Accept': "application/json"
+                      }
+        req_headers.update(self.headers)
+
+
+        content_type = 'json'
+        request_body = json.get('body', None)
+        url = json.get('url')
+        if output:
+            self.write_request_file(name, 'json', url, method, req_headers,
+                                    request_body)
+
+        resp, resp_content = http.request(url, method, body=request_body,
+                                          headers=req_headers)
+        json_resp = resp, resp_content
+        if output:
+            filename = "%sdb-%s-response.%s" % (self.directory, name,
+                                                content_type)
+            self.write_response_file(name, 'json', url, method, resp,
+                                     resp_content)
+
+
+        content_type = 'xml'
+        req_headers['Accept'] = 'application/xml'
+        req_headers['Content-Type'] = 'application/xml'
+        request_body = xml.get('body', None)
+        url = xml.get('url')
+        if output:
+            filename = "%sdb-%s-request.%s" % (self.directory, name,
+                                               content_type)
+            output = self.write_request_file(name, 'xml', url, method,
+                                             req_headers, request_body)
+        resp, resp_content = http.request(url, method, body=request_body,
+                                          headers=req_headers)
+        xml_resp = resp, resp_content
+        if output:
+            filename = "%sdb-%s-response.%s" % (self.directory, name,
+                                                content_type)
+            self.write_response_file(name, 'xml', url, method, resp,
+                                     resp_content)
+
+
+        return json_resp, xml_resp
+
+    def _indent_xml(self, my_string):
+        my_string = my_string.encode("utf-8")
+        # convert to plain string without indents and spaces
+        my_re = re.compile('>\s+([^\s])', re.DOTALL)
+        my_string = myre.sub('>\g<1>', my_string)
+        my_string = xml.dom.minidom.parseString(my_string).toprettyxml()
+        # remove line breaks
+        my_re = re.compile('>\n\s+([^<>\s].*?)\n\s+</', re.DOTALL)
+        my_string = my_re.sub('>\g<1></', my_string)
+        return my_string
+
+    def output_request(self, url, output_headers, body, content_type, method,
+                       static_auth_token=True):
+        output_list = []
+        parsed = urlparse(url)
+        if parsed.query:
+            method_url = parsed.path + '?' + parsed.query
+        else:
+            method_url = parsed.path
+        output_list.append("%s %s HTTP/1.1" % (method, method_url))
+        output_list.append("User-Agent: %s" % output_headers['User-Agent'])
+        output_list.append("Host: %s" % parsed.netloc)
+        # static_auth_token option for documentation purposes
+        if static_auth_token:
+            output_token = '87c6033c-9ff6-405f-943e-2deb73f278b7'
+        else:
+            output_token = output_headers['X-Auth-Token']
+        output_list.append("X-Auth-Token: %s" % output_token)
+        output_list.append("Accept: %s" % output_headers['Accept'])
+        output_list.append("Content-Type: %s" % output_headers['Content-Type'])
+        output_list.append("")
+        pretty_body = self.format_body(body, content_type)
+        output_list.append("%s" % pretty_body)
+        output_list.append("")
+        return '\n'.join(output_list)
+
+    def output_response(self, resp, body, content_type):
+        output_list = []
+        version = "1.1" if resp.version == 11 else "1.0"
+        lines = [
+            ["HTTP/%s %s %s" % (version, resp.status, resp.reason)],
+            ["Content-Type: %s" % resp['content-type']],
+            ["Content-Length: %s" % resp['content-length']],
+            ["Date: %s" % resp['date']]]
+        new_lines = [x[0] for x in lines]
+        joined_lines = '\n'.join(new_lines)
+        output_list.append(joined_lines)
+        if body:
+            output_list.append("")
+            pretty_body = self.format_body(body, content_type)
+            output_list.append("%s" % pretty_body)
+        output_list.append("")
+        return '\n'.join(output_list)
+
+    def format_body(self, body, content_type):
+        if content_type == 'json':
+            try:
+                if self.replace_dns_hostname:
+                    before = r'\"hostname\": \"[a-zA-Z0-9-_\.]*\"'
+                    after = '\"hostname\": \"%s\"' % self.replace_dns_hostname
+                    body = re.sub(before, after, body)
+                return json.dumps(json.loads(body), sort_keys=True, indent=4)
+            except Exception:
+                return body if body else ''
         else:
-            allowed_attrs = ['id', 'links']
-            for slave in self.instance['replicas']:
-                self.contains_allowed_attrs(
-                    slave, allowed_attrs,
-                    msg="Replica links not found")
-                self.links(slave['links'])
+            # expected type of body is xml
+            try:
+                if self.replace_dns_hostname:
+                    hostname = 'hostname=\"%s\"' % self.replace_dns_hostname,
+                    body = re.sub(r'hostname=\"[a-zA-Z0-9-_\.]*\"',
+                                  hostname, body)
+                return self._indent_xml(body)
+            except Exception as ex:
+                return body if body else ''
+
+    def get_auth_token_id_tenant_id(self, url, username, password):
+        body = ('{"auth":{"tenantName": "%s", "passwordCredentials": '
+                '{"username": "%s", "password": "%s"}}}')
+        body = body % (self.tenant, username, password)
+        http = httplib2.Http(disable_ssl_certificate_validation=True)
+        req_headers = {'User-Agent': "python-example-client",
+                       'Content-Type': "application/json",
+                       'Accept': "application/json",
+                      }
+        resp, body = http.request(url, 'POST', body=body, headers=req_headers)
+        auth = json.loads(body)
+        auth_id = auth['access']['token']['id']
+        tenant_id = auth['access']['token']['tenant']['id']
+        return auth_id, tenant_id
+
+
+@test
+def load_config_file():
+    global conf
+    print("RUNNING ARGS :  " + str(sys.argv))
+    conf = None
+    for arg in sys.argv[1:]:
+        conf_file_path = os.path.expanduser(arg)
+        conf = ConfigFile(conf_file_path)
+        return
+    if not conf:
+        fail("Missing conf file.")
+
+def create_client(cls=TroveHTTPClient):
+    client = Dbaas(conf.username, conf.password, tenant=conf.tenant,
+                   auth_url="blah/", auth_strategy='fake',
+                   insecure=True, service_type='trove',
+                   service_url=conf.dbaas_url, client_cls=cls)
+    return client
+
+class ClientPair(object):
+    """
+    Combines a Json and XML version of the Dbaas client.
+    """
 
+    def __init__(self):
+        snippet_writer = SnippetWriter(conf)
+        def make_client(cls):
+            client = create_client(cls)
+            client.client.name = "auth"
+            client.client.snippet_writer = snippet_writer
+            client.authenticate()
+            return client
+        self.json = make_client(JsonClient)
+        self.xml = make_client(XmlClient)
+        self.clients = [self.json, self.xml]
+
+    def do(self, name, url, method, status, reason, func, func_args=None):
+        """
+        Performs the given function twice, first for the JSON client, then for
+        the XML one, and writes both to their respective files.
+        'name' is the name of the file, while 'url,' 'method,' 'status,'
+        and 'reason' are expected values that are asserted against.
+        If func_args is present, it is a list of lists, each one of which
+        is passed as the *args to the two invocations of "func".
+        """
+        func_args = func_args or [[], []]
+        snippet_writer = SnippetWriter(conf)
+        results = []
+        for index, client in enumerate(self.clients):
+            client.client.snippet_writer = snippet_writer
+            client.client.name = name
+            args = func_args[index]
+            result = func(client, *args)
+            with Check() as check:
+                if isinstance(url, (list, tuple)):
+                    check.equal(client.client.old_info['url'], url[index])
+                else:
+                    check.equal(client.client.old_info['url'], url)
+                check.equal(client.client.old_info['method'], method)
+                check.equal(client.client.old_info['response_headers'].status,
+                            status)
+                check.equal(client.client.old_info['response_headers'].reason,
+                            reason)
+            results.append(result)
+            # To prevent this from writing a snippet somewhere else...
+            client.client.name = "junk"
 
-@test(groups=[tests.DBAAS_API_INSTANCES],
-      depends_on_groups=[tests.DBAAS_API_MGMT_DATASTORES])
-class TestInstanceSetup(object):
-    """Prepare the instance creation parameters."""
+        return results
+
+
+JSON_INDEX = 0
+XML_INDEX = 1
+
+@test(depends_on=[load_config_file])
+class Versions(object):
 
     @before_class
-    def setUp(self):
-        """Sets up the client."""
+    def setup(self):
+        self.clients = ClientPair()
 
-        reqs = Requirements(is_admin=True)
-        instance_info.admin_user = CONFIG.users.find_user(reqs)
-        instance_info.dbaas_admin = create_dbaas_client(
-            instance_info.admin_user)
-        global dbaas_admin
-        dbaas_admin = instance_info.dbaas_admin
-
-        # Make sure we create the client as the correct user if we're using
-        # a pre-built instance.
-        if existing_instance():
-            mgmt_inst = dbaas_admin.mgmt.instances.show(existing_instance())
-            t_id = mgmt_inst.tenant_id
-            instance_info.user = CONFIG.users.find_user_by_tenant_id(t_id)
-        else:
-            reqs = Requirements(is_admin=False)
-            instance_info.user = CONFIG.users.find_user(reqs)
+    @test
+    def get_versions(self):
+        self.clients.do("versions",
+           "", "GET", 200, "OK",
+            lambda client : client.versions.index(conf.api_url))
 
-        instance_info.dbaas = create_dbaas_client(instance_info.user)
 
-        instance_info.nova_client = util.create_nova_client(instance_info.user)
-        instance_info.flavors = instance_info.nova_client.flavors.list()
+    @test
+    def get_version(self):
+        self.clients.do("versions",
+            "/v1.0", "GET", 200, "OK",
+            lambda client : client.versions.index(conf.api_url + "/v1.0/"))
+
+
+@test(depends_on=[load_config_file])
+class Flavors(object):
 
-        global dbaas
-        dbaas = instance_info.dbaas
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_find_flavor(self):
-        flavor, flavor_href = instance_info.find_default_flavor()
-        instance_info.dbaas_flavor = flavor
-        instance_info.dbaas_flavor_href = flavor_href
+    def get_flavors(self):
+        self.clients.do("flavors",
+            "/flavors", "GET", 200, "OK",
+            lambda client : client.flavors.list())
 
     @test
-    def create_instance_name(self):
-        id = existing_instance()
-        if id is None:
-            instance_info.name = "TEST_" + str(uuid.uuid4())
-        else:
-            instance_info.name = dbaas.instances.get(id).name
+    def get_flavor_by_id(self):
+        self.clients.do("flavors_by_id",
+            "/flavors/1", "GET", 200, "OK",
+            lambda client : client.flavors.get(1))
 
 
-@test(groups=[tests.DBAAS_API_INSTANCES],
-      depends_on_classes=[TestInstanceSetup])
-class TestCreateInstanceQuota(unittest.TestCase):
-    def tearDown(self):
-        quota_dict = {'instances': CONFIG.trove_max_instances_per_tenant,
-                      'volumes': CONFIG.trove_max_volumes_per_tenant}
-        dbaas_admin.quota.update(instance_info.user.tenant_id,
-                                 quota_dict)
-
-    def test_instance_size_too_big(self):
-        if ('trove_max_accepted_volume_size' in CONFIG.values and
-                VOLUME_SUPPORT):
-            too_big = CONFIG.trove_max_accepted_volume_size
-
-            assert_raises(exceptions.OverLimit,
-                          dbaas.instances.create,
-                          "volume_size_too_large",
-                          instance_info.dbaas_flavor_href,
-                          {'size': too_big + 1},
-                          nics=instance_info.nics)
-
-    def test_update_quota_invalid_resource_should_fail(self):
-        quota_dict = {'invalid_resource': 100}
-        assert_raises(exceptions.NotFound, dbaas_admin.quota.update,
-                      instance_info.user.tenant_id, quota_dict)
-
-    def test_update_quota_volume_should_fail_volume_not_supported(self):
-        if VOLUME_SUPPORT:
-            raise SkipTest("Volume support needs to be disabled")
-        quota_dict = {'volumes': 100}
-        assert_raises(exceptions.NotFound, dbaas_admin.quota.update,
-                      instance_info.user.tenant_id, quota_dict)
-
-    def test_create_too_many_instances(self):
-        instance_quota = 0
-        quota_dict = {'instances': instance_quota}
-        new_quotas = dbaas_admin.quota.update(instance_info.user.tenant_id,
-                                              quota_dict)
-
-        set_quota = dbaas_admin.quota.show(instance_info.user.tenant_id)
-        verify_quota = {q.resource: q.limit for q in set_quota}
-
-        assert_equal(new_quotas['instances'], quota_dict['instances'])
-        assert_equal(0, verify_quota['instances'])
-
-        volume = None
-        if VOLUME_SUPPORT:
-            assert_equal(CONFIG.trove_max_volumes_per_tenant,
-                         verify_quota['volumes'])
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-
-        assert_raises(exceptions.OverLimit,
-                      dbaas.instances.create,
-                      "too_many_instances",
-                      instance_info.dbaas_flavor_href,
-                      volume,
-                      nics=instance_info.nics)
-
-        assert_equal(413, dbaas.last_http_code)
-
-    def test_create_instances_total_volume_exceeded(self):
-        if not VOLUME_SUPPORT:
-            raise SkipTest("Volume support not enabled")
-        volume_quota = 3
-        quota_dict = {'volumes': volume_quota}
-        new_quotas = dbaas_admin.quota.update(instance_info.user.tenant_id,
-                                              quota_dict)
-        assert_equal(volume_quota, new_quotas['volumes'])
-
-        assert_raises(exceptions.OverLimit,
-                      dbaas.instances.create,
-                      "too_large_volume",
-                      instance_info.dbaas_flavor_href,
-                      {'size': volume_quota + 1},
-                      nics=instance_info.nics)
-
-        assert_equal(413, dbaas.last_http_code)
-
-
-@test(groups=[tests.DBAAS_API_INSTANCES],
-      depends_on_classes=[TestCreateInstanceQuota])
-class CreateInstanceFail(object):
-    """Negative instance creation tests."""
-    def instance_in_error(self, instance_id):
-        def check_if_error():
-            instance = dbaas.instances.get(instance_id)
-            if instance.status == "ERROR":
-                return True
-            else:
-                # The status should still be BUILD
-                assert_equal("BUILD", instance.status)
-                return False
-        return check_if_error
-
-    def delete_async(self, instance_id):
-        dbaas.instances.delete(instance_id)
-        while True:
-            try:
-                dbaas.instances.get(instance_id)
-            except exceptions.NotFound:
-                return True
-            time.sleep(1)
+@test(depends_on=[load_config_file])
+def clean_slate():
+    client = create_client()
+    client.client.name = "list"
+    instances = client.instances.list()
+    assert_equal(0, len(instances), "Instance count must be zero.")
+
+
+@test(depends_on=[clean_slate])
+class CreateInstance(object):
+
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_create_with_bad_availability_zone(self):
-        instance_name = "instance-failure-with-bad-az"
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        databases = []
-        result = dbaas.instances.create(instance_name,
-                                        instance_info.dbaas_flavor_href,
-                                        volume, databases,
-                                        availability_zone="BAD_ZONE",
-                                        nics=instance_info.nics)
-
-        poll_until(self.instance_in_error(result.id), sleep_time=5,
-                   time_out=30)
-        instance = dbaas.instances.get(result.id)
-        assert_equal("ERROR", instance.status)
-
-        self.delete_async(result.id)
-
-    @test
-    def test_create_with_invalid_net_id(self):
-        instance_name = "instance-failure-with-invalid-net"
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        databases = []
-        bad_nic = [{"net-id": "1234"}]
-
-        assert_raises(
-            exceptions.BadRequest,
-            dbaas.instances.create,
-            instance_name, instance_info.dbaas_flavor_href,
-            volume, databases, nics=bad_nic
-        )
-        assert_equal(400, dbaas.last_http_code)
-
-    @test
-    def test_create_with_multiple_net_id(self):
-        instance_name = "instance_failure_with_multiple_net_id"
-        volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        databases = []
-        multi_nics = [
-            {"net-id": str(uuid.uuid4())},
-            {"net-id": str(uuid.uuid4())}
-        ]
-
-        assert_raises(
-            exceptions.BadRequest,
-            dbaas.instances.create,
-            instance_name, instance_info.dbaas_flavor_href,
-            volume, databases, nics=multi_nics
-        )
-        assert_equal(400, dbaas.last_http_code)
-
-    @test
-    def test_create_with_port_id(self):
-        instance_name = "instance-failure-with-port-id"
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        databases = []
-        bad_nic = [{"port-id": "1234"}]
-
-        assert_raises(
-            exceptions.BadRequest,
-            dbaas.instances.create,
-            instance_name, instance_info.dbaas_flavor_href,
-            volume, databases, nics=bad_nic
-        )
-        assert_equal(400, dbaas.last_http_code)
-
-    @test
-    def test_create_failure_with_empty_flavor(self):
-        instance_name = "instance-failure-with-empty-flavor"
-        databases = []
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        assert_raises(exceptions.BadRequest, dbaas.instances.create,
-                      instance_name, '',
-                      volume, databases,
-                      nics=instance_info.nics)
-        assert_equal(400, dbaas.last_http_code)
-
-    @test(enabled=VOLUME_SUPPORT)
-    def test_create_failure_with_empty_volume(self):
-        instance_name = "instance-failure-with-no-volume-size"
-        databases = []
-        volume = {}
-        assert_raises(exceptions.BadRequest, dbaas.instances.create,
-                      instance_name, instance_info.dbaas_flavor_href,
-                      volume, databases,
-                      nics=instance_info.nics)
-        assert_equal(400, dbaas.last_http_code)
-
-    @test(enabled=VOLUME_SUPPORT)
-    def test_create_failure_with_no_volume_size(self):
-        instance_name = "instance-failure-with-no-volume-size"
-        databases = []
-        volume = {'size': None}
-        assert_raises(exceptions.BadRequest, dbaas.instances.create,
-                      instance_name, instance_info.dbaas_flavor_href,
-                      volume, databases,
-                      nics=instance_info.nics)
-        assert_equal(400, dbaas.last_http_code)
-
-    @test(enabled=not VOLUME_SUPPORT)
-    def test_create_failure_with_volume_size_and_volume_disabled(self):
-        instance_name = "instance-failure-volume-size_and_volume_disabled"
-        databases = []
-        volume = {'size': 2}
-        assert_raises(exceptions.HTTPNotImplemented, dbaas.instances.create,
-                      instance_name, instance_info.dbaas_flavor_href,
-                      volume, databases,
-                      nics=instance_info.nics)
-        assert_equal(501, dbaas.last_http_code)
-
-    @test(enabled=EPHEMERAL_SUPPORT)
-    def test_create_failure_with_no_ephemeral_flavor(self):
-        instance_name = "instance-failure-with-no-ephemeral-flavor"
-        databases = []
-        flavor_name = CONFIG.values.get('instance_flavor_name', 'm1.tiny')
-
-        flavor_id = None
-        for item in instance_info.flavors:
-            if item.name == flavor_name:
-                flavor_id = item.id
-
-        asserts.assert_is_not_none(flavor_id)
-
-        assert_raises(exceptions.BadRequest, dbaas.instances.create,
-                      instance_name, flavor_id, None, databases,
-                      nics=instance_info.nics)
-        assert_equal(400, dbaas.last_http_code)
-
-    @test
-    def test_create_failure_with_no_name(self):
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        instance_name = ""
-        databases = []
-        assert_raises(exceptions.BadRequest, dbaas.instances.create,
-                      instance_name, instance_info.dbaas_flavor_href,
-                      volume, databases,
-                      nics=instance_info.nics)
-        assert_equal(400, dbaas.last_http_code)
+    def post_create_instance(self):
+        def create_instance(client, name):
+            instance = client.instances.create(name, 1, volume={'size':2},
+                databases=[{
+                        "name": "sampledb",
+                        "character_set": "utf8",
+                        "collate": "utf8_general_ci"
+                    },{
+                        "name": "nextround"
+                    }
+                ],
+                users =[{
+                        "databases":[{ "name":"sampledb"}],
+                        "name":"demouser",
+                        "password": "demopassword"
+                    }
+                ])
+            assert_equal(instance.status, "BUILD")
+            return instance
+        self.instances = self.clients.do("create_instance",
+            "/instances", "POST", 200, "OK",
+            create_instance,
+            (["json_rack_instance"], ["xml_rack_instance"]))
+        #self.instance_j = create_instance(self.clients.json,
+        #                                  "json_rack_instance")
+        #self.instance_x = create_instance(self.clients.xml,
+        #                                  "xml_rack_instance")
+
+    @test(depends_on=[post_create_instance])
+    def wait_for_instances(self):
+        for instance in self.instances:
+            while instance.status != "ACTIVE":
+                assert_equal(instance.status, "BUILD")
+                instance.get()
+                time.sleep(0.1)
+        global json_instance
+        json_instance = self.instances[0]
+        global xml_instance
+        xml_instance = self.instances[1]
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class Databases(object):
+
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_create_failure_with_spaces_for_name(self):
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        instance_name = "      "
-        databases = []
-        assert_raises(exceptions.BadRequest, dbaas.instances.create,
-                      instance_name, instance_info.dbaas_flavor_href,
-                      volume, databases,
-                      nics=instance_info.nics)
-        assert_equal(400, dbaas.last_http_code)
-
-    @test
-    def test_mgmt_get_instance_on_create(self):
-        if CONFIG.test_mgmt:
-            result = dbaas_admin.management.show(instance_info.id)
-            allowed_attrs = ['account_id', 'addresses', 'created',
-                             'databases', 'flavor', 'guest_status', 'host',
-                             'hostname', 'id', 'name', 'datastore',
-                             'server_state_description', 'status', 'updated',
-                             'users', 'volume', 'root_enabled_at',
-                             'root_enabled_by', 'fault',
-                             'service_status_updated', 'operating_status']
-            with CheckInstance(result._info) as check:
-                check.contains_allowed_attrs(
-                    result._info, allowed_attrs,
-                    msg="Mgmt get instance")
-                check.flavor()
-                check.datastore()
-                check.guest_status()
-
-    @test
-    def test_create_failure_with_datastore_default_not_defined(self):
-        if not FAKE:
-            raise SkipTest("This test only for fake mode.")
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        instance_name = "datastore_default_notfound"
-        databases = []
-        users = []
-        origin_default_datastore = (datastore_models.CONF.
-                                    default_datastore)
-        datastore_models.CONF.default_datastore = ""
-        try:
-            assert_raises(exceptions.NotFound,
-                          dbaas.instances.create, instance_name,
-                          instance_info.dbaas_flavor_href,
-                          volume, databases, users,
-                          nics=instance_info.nics)
-        except exceptions.BadRequest as e:
-            assert_equal(e.message,
-                         "Please specify datastore. No default datastore "
-                         "is defined.")
-        datastore_models.CONF.default_datastore = \
-            origin_default_datastore
-
-    @test
-    def test_create_failure_with_datastore_default_version_notfound(self):
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        instance_name = "datastore_default_version_notfound"
-        databases = []
-        users = []
-        datastore = CONFIG.dbaas_datastore_name_no_versions
+    def post_create_databases(self):
+        self.clients.do("create_databases",
+            ("/instances/%s/databases" % json_instance.id,
+             "/instances/%s/databases" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id : client.databases.create(id, databases=[
+                {
+                    "name": "testingdb",
+                    "character_set": "utf8",
+                    "collate": "utf8_general_ci"
+                },
+                {
+                    "name": "anotherdb"
+                },
+                    {
+                    "name": "oneMoreDB"
+                }
+            ]), ([json_instance.id], [xml_instance.id]))
+
+    @test(depends_on=[post_create_databases])
+    def get_list_databases(self):
+        results = self.clients.do("list_databases",
+            ("/instances/%s/databases" % json_instance.id,
+             "/instances/%s/databases" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id : client.databases.list(id),
+            ([json_instance.id], [xml_instance.id]))
+
+    @test(depends_on=[post_create_databases])
+    def get_list_databases_limit_two(self):
+        results = self.clients.do("list_databases_pagination",
+            ("/instances/%s/databases?limit=1" % json_instance.id,
+             "/instances/%s/databases?limit=2" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id, limit : client.databases.list(id, limit=limit),
+            ([json_instance.id, 1], [xml_instance.id, 2]))
+        assert_equal(1, len(results[JSON_INDEX]))
+        assert_equal(2, len(results[XML_INDEX]))
+        assert_equal("anotherdb", results[JSON_INDEX].next)
+        assert_equal("nextround", results[XML_INDEX].next)
+
+    @test(depends_on=[post_create_databases],
+          runs_after=[get_list_databases, get_list_databases_limit_two])
+    def delete_databases(self):
+        results = self.clients.do("delete_databases",
+            ("/instances/%s/databases/testingdb" % json_instance.id,
+             "/instances/%s/databases/oneMoreDB" % xml_instance.id),
+            "DELETE", 202, "Accepted",
+            lambda client, id, name : client.databases.delete(id, name),
+            ([json_instance.id, 'testingdb'], [xml_instance.id, 'oneMoreDB']))
+
 
-        try:
-            assert_raises(exceptions.NotFound,
-                          dbaas.instances.create, instance_name,
-                          instance_info.dbaas_flavor_href,
-                          volume, databases, users,
-                          datastore=datastore,
-                          nics=instance_info.nics)
-        except exceptions.BadRequest as e:
-            assert_equal(e.message,
-                         "Default version for datastore '%s' not found." %
-                         datastore)
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class Users(object):
+
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_create_failure_with_datastore_notfound(self):
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        instance_name = "datastore_notfound"
-        databases = []
-        users = []
-        datastore = "nonexistent"
-        try:
-            assert_raises(exceptions.NotFound,
-                          dbaas.instances.create, instance_name,
-                          instance_info.dbaas_flavor_href,
-                          volume, databases, users,
-                          datastore=datastore,
-                          nics=instance_info.nics)
-        except exceptions.BadRequest as e:
-            assert_equal(e.message,
-                         "Datastore '%s' cannot be found." %
-                         datastore)
+    def post_create_users(self):
+        results = self.clients.do("create_users",
+            ("/instances/%s/users" % json_instance.id,
+             "/instances/%s/users" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id : client.users.create(id, [
+                {
+                    "name": "dbuser1",
+                    "password": "password",
+                    "database": "databaseA"
+                    },
+                {
+                    "name": "dbuser2",
+                    "password": "password",
+                    "databases": [
+                        {
+                            "name": "databaseB"
+                            },
+                        {
+                            "name": "databaseC"
+                            }
+                        ]
+                    },
+                {
+                    "name": "dbuser3",
+                    "password": "password",
+                    "database": "databaseD"
+                    }
+                ]),
+            ([json_instance.id], [xml_instance.id]))
+
+    @test(depends_on=[post_create_users])
+    def get_list_users(self):
+        results = self.clients.do("list_users",
+            ("/instances/%s/users" % json_instance.id,
+             "/instances/%s/users" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id : client.users.list(id),
+            ([json_instance.id], [xml_instance.id]))
+
+    @test(depends_on=[post_create_users])
+    def get_list_users_limit_two(self):
+        results = self.clients.do("list_users_pagination",
+            ("/instances/%s/users?limit=2" % json_instance.id,
+             "/instances/%s/users?limit=2" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id : client.users.list(id, limit=2),
+            ([json_instance.id], [xml_instance.id]))
+
+    @test(depends_on=[post_create_users],
+          runs_after=[get_list_users, get_list_users_limit_two])
+    def delete_users(self):
+        user_name = "testuser"
+        results = self.clients.do("delete_users",
+            ("/instances/%s/users/%s" % (json_instance.id, user_name),
+             "/instances/%s/users/%s" % (xml_instance.id, user_name)),
+            "DELETE", 202, "Accepted",
+            lambda client, id : client.users.delete(id, user=user_name),
+            ([json_instance.id], [xml_instance.id]))
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class Root(object):
+
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_create_failure_with_datastore_version_notfound(self):
-        if VOLUME_SUPPORT:
-            volume = {'size': CONFIG.get('trove_volume_size', 1)}
-        else:
-            volume = None
-        instance_name = "datastore_version_notfound"
-        databases = []
-        users = []
-        datastore = CONFIG.dbaas_datastore
-        datastore_version = "nonexistent"
-        assert_raises(exceptions.BadRequest,
-                      dbaas.instances.create, instance_name,
-                      instance_info.dbaas_flavor_href,
-                      volume, databases, users,
-                      datastore=datastore,
-                      datastore_version=datastore_version,
-                      nics=instance_info.nics)
-
-
-@test(
-    groups=[tests.DBAAS_API_INSTANCES],
-    depends_on_classes=[CreateInstanceFail],
-)
-class CreateInstance(object):
+    def post_enable_root_access(self):
+        results = self.clients.do("enable_root_user",
+            ("/instances/%s/root" % json_instance.id,
+             "/instances/%s/root" % xml_instance.id),
+            "POST", 200, "OK",
+            lambda client, id : client.root.create(id),
+            ([json_instance.id], [xml_instance.id]))
+
+    @test(depends_on=[post_enable_root_access])
+    def get_check_root_access(self):
+        results = self.clients.do("check_root_user",
+            ("/instances/%s/root" % json_instance.id,
+             "/instances/%s/root" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id : client.root.is_root_enabled(id),
+            ([json_instance.id], [xml_instance.id]))
+        assert_equal(results[JSON_INDEX], True)
+        assert_equal(results[XML_INDEX], True)
 
-    """Test to create a Database Instance
 
-    If the call returns without raising an exception this test passes.
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class InstanceList(object):
 
-    """
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_create(self):
-        databases = []
-        databases.append({"name": "firstdb", "character_set": "latin2",
-                          "collate": "latin2_general_ci"})
-        databases.append({"name": "db2"})
-        instance_info.databases = databases
-        users = []
-        users.append({"name": "lite", "password": "litepass",
-                      "databases": [{"name": "firstdb"}]})
-        instance_info.users = users
-        instance_info.dbaas_datastore = CONFIG.dbaas_datastore
-        instance_info.dbaas_datastore_version = CONFIG.dbaas_datastore_version
-        if VOLUME_SUPPORT:
-            instance_info.volume = {'size': CONFIG.get('trove_volume_size', 2)}
-        else:
-            instance_info.volume = None
+    def get_list_instance_index(self):
+        results = self.clients.do("instances_index",
+            "/instances", "GET", 200, "OK",
+            lambda client : client.instances.list())
+        for result in results:
+            assert_equal(2, len(result))
+
+    @test
+    def get_instance_details(self):
+        results = self.clients.do("instance_status_detail",
+            ("/instances/%s" % json_instance.id,
+             "/instances/%s" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id : client.instances.get(id),
+            ([json_instance.id], [xml_instance.id]))
+        assert_equal(results[JSON_INDEX].id, json_instance.id)
+        assert_equal(results[XML_INDEX].id, xml_instance.id)
+
+    @test
+    def get_list_instance_index_limit_two(self):
+        third_instance = self.clients.json.instances.create(
+            "The Third Instance", 1, volume={'size':2})
+        while third_instance.status != "ACTIVE":
+            third_instance.get()
+            time.sleep(0.1)
+
+        results = self.clients.do("instances_index_pagination",
+            "/instances?limit=2", "GET", 200, "OK",
+            lambda client : client.instances.list(limit=2))
+        for result in results:
+            assert_equal(2, len(result))
 
-        if create_new_instance():
-            instance_info.initial_result = dbaas.instances.create(
-                instance_info.name,
-                instance_info.dbaas_flavor_href,
-                instance_info.volume,
-                databases,
-                users,
-                nics=instance_info.nics,
-                availability_zone="nova",
-                datastore=instance_info.dbaas_datastore,
-                datastore_version=instance_info.dbaas_datastore_version)
-            assert_equal(200, dbaas.last_http_code)
-        else:
-            id = existing_instance()
-            instance_info.initial_result = dbaas.instances.get(id)
+        self.clients.json.instances.delete(third_instance.id)
 
-        result = instance_info.initial_result
-        instance_info.id = result.id
-        instance_info.dbaas_datastore_version = result.datastore['version']
-
-        report = CONFIG.get_report()
-        report.log("Instance UUID = %s" % instance_info.id)
-        if create_new_instance():
-            assert_equal("BUILD", instance_info.initial_result.status)
 
-        else:
-            report.log("Test was invoked with TESTS_USE_INSTANCE_ID=%s, so no "
-                       "instance was actually created." % id)
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class Actions(object):
 
-        # Check these attrs only are returned in create response
-        allowed_attrs = ['created', 'flavor', 'addresses', 'id', 'links',
-                         'name', 'status', 'updated', 'datastore', 'fault',
-                         'region', 'service_status_updated', 'access',
-                         'operating_status']
-        if ROOT_ON_CREATE:
-            allowed_attrs.append('password')
-        if VOLUME_SUPPORT:
-            allowed_attrs.append('volume')
-        if CONFIG.trove_dns_support:
-            allowed_attrs.append('hostname')
-
-        with CheckInstance(result._info) as check:
-            if create_new_instance():
-                check.contains_allowed_attrs(
-                    result._info, allowed_attrs,
-                    msg="Create response")
-            # Don't CheckInstance if the instance already exists.
-            check.flavor()
-            check.datastore()
-            check.links(result._info['links'])
-            if VOLUME_SUPPORT:
-                check.volume()
-
-
-@test(
-    groups=[tests.DBAAS_API_INSTANCES],
-    depends_on_classes=[CreateInstance],
-    enabled=create_new_instance()
-)
-class AfterInstanceCreation(unittest.TestCase):
-
-    # instance calls
-    def test_instance_delete_right_after_create(self):
-        assert_unprocessable(dbaas.instances.delete, instance_info.id)
-
-    # root calls
-    def test_root_create_root_user_after_create(self):
-        assert_unprocessable(dbaas.root.create, instance_info.id)
-
-    def test_root_is_root_enabled_after_create(self):
-        assert_unprocessable(dbaas.root.is_root_enabled, instance_info.id)
-
-    # database calls
-    def test_database_index_after_create(self):
-        assert_unprocessable(dbaas.databases.list, instance_info.id)
-
-    def test_database_delete_after_create(self):
-        assert_unprocessable(dbaas.databases.delete, instance_info.id,
-                             "testdb")
-
-    def test_database_create_after_create(self):
-        assert_unprocessable(dbaas.databases.create, instance_info.id,
-                             instance_info.databases)
-
-    # user calls
-    def test_users_index_after_create(self):
-        assert_unprocessable(dbaas.users.list, instance_info.id)
-
-    def test_users_delete_after_create(self):
-        assert_unprocessable(dbaas.users.delete, instance_info.id,
-                             "testuser")
-
-    def test_users_create_after_create(self):
-        users = list()
-        users.append({"name": "testuser", "password": "password",
-                      "databases": [{"name": "testdb"}]})
-        assert_unprocessable(dbaas.users.create, instance_info.id, users)
-
-    def test_resize_instance_after_create(self):
-        assert_unprocessable(dbaas.instances.resize_instance,
-                             instance_info.id, 8)
-
-    def test_resize_volume_after_create(self):
-        assert_unprocessable(dbaas.instances.resize_volume,
-                             instance_info.id, 2)
-
-
-@test(
-    depends_on_classes=[AfterInstanceCreation],
-    groups=[tests.DBAAS_API_INSTANCES],
-    enabled=create_new_instance()
-)
-class WaitForGuestInstallationToFinish(object):
-    @test
-    @time_out(TIMEOUT_INSTANCE_CREATE)
-    def test_instance_created(self):
-        """Wait for normal instance to be created."""
-        def result_is_active():
-            instance = dbaas.instances.get(instance_info.id)
-            if instance.status in CONFIG.running_status:
-                return True
-            else:
-                # If its not ACTIVE, anything but BUILD must be
-                # an error.
-                assert_equal("BUILD", instance.status)
-                if instance_info.volume is not None:
-                    assert_equal(instance.volume.get('used', None), None)
-                return False
-
-        poll_until(result_is_active, sleep_time=5)
-        dbaas.instances.get(instance_info.id)
-
-        report = CONFIG.get_report()
-        report.log("Created an instance, ID = %s." % instance_info.id)
-        report.log("TIP:")
-        report.log("Rerun the tests with TESTS_USE_INSTANCE_ID=%s "
-                   "to skip ahead to this point." % instance_info.id)
-        report.log("Add TESTS_DO_NOT_DELETE_INSTANCE=True to avoid deleting "
-                   "the instance at the end of the tests.")
-
-
-@test(depends_on_classes=[WaitForGuestInstallationToFinish],
-      groups=[tests.DBAAS_API_INSTANCES])
-class TestDBandUserAfterInstanceCreated(object):
-    @test
-    def test_databases(self):
-        """Get databases after instance creation."""
-        databases = dbaas.databases.list(instance_info.id)
-        dbs = [database.name for database in databases]
-        for db in instance_info.databases:
-            assert_true(db["name"] in dbs)
-
-    @test
-    def test_users(self):
-        """Get users after instance creation."""
-        users = dbaas.users.list(instance_info.id)
-        usernames = [user.name for user in users]
-        for user in instance_info.users:
-            assert_true(user["name"] in usernames)
-
-
-@test(depends_on_classes=[WaitForGuestInstallationToFinish],
-      groups=[tests.DBAAS_API_INSTANCES])
-class TestGetInstances(object):
     @before_class
-    def setUp(self):
-        reqs = Requirements(is_admin=False)
-        self.other_user = CONFIG.users.find_user(
-            reqs,
-            black_list=[instance_info.user.auth_user])
-        self.other_client = create_dbaas_client(self.other_user)
-
-    @test
-    def test_index_list(self):
-        allowed_attrs = ['id', 'links', 'name', 'status', 'flavor',
-                         'datastore', 'ip', 'hostname', 'replica_of',
-                         'region', 'addresses', 'access', 'operating_status']
-        if VOLUME_SUPPORT:
-            allowed_attrs.append('volume')
-        instances = dbaas.instances.list()
-        assert_equal(200, dbaas.last_http_code)
-        for instance in instances:
-            instance_dict = instance._info
-            with CheckInstance(instance_dict) as check:
-                print("testing instance_dict=%s" % instance_dict)
-                check.contains_allowed_attrs(
-                    instance_dict, allowed_attrs,
-                    msg="Instance Index")
-                check.links(instance_dict['links'])
-                check.flavor()
-                check.datastore()
-                check.volume()
-
-    @test
-    def test_detailed_list(self):
-        allowed_attrs = ['created', 'databases', 'flavor', 'hostname', 'id',
-                         'links', 'name', 'status', 'updated', 'ip',
-                         'datastore', 'fault', 'region',
-                         'service_status_updated', 'addresses', 'access',
-                         'operating_status']
-        if VOLUME_SUPPORT:
-            allowed_attrs.append('volume')
-        instances = dbaas.instances.list(detailed=True)
-        assert_equal(200, dbaas.last_http_code)
-        for instance in instances:
-            instance_dict = instance._info
-            with CheckInstance(instance_dict) as check:
-                check.contains_allowed_attrs(instance_dict, allowed_attrs,
-                                             msg="Instance Detailed Index")
-                check.flavor()
-                check.datastore()
-                check.volume()
-                check.used_volume()
-
-    @test
-    def test_get_instance(self):
-        allowed_attrs = ['created', 'databases', 'flavor', 'hostname', 'id',
-                         'links', 'name', 'status', 'updated', 'ip',
-                         'datastore', 'fault', 'region',
-                         'service_status_updated', 'addresses', 'access',
-                         'operating_status']
-        if VOLUME_SUPPORT:
-            allowed_attrs.append('volume')
-        else:
-            allowed_attrs.append('local_storage')
-        instance = dbaas.instances.get(instance_info.id)
-        assert_equal(200, dbaas.last_http_code)
-        instance_dict = instance._info
-        print("instance_dict=%s" % instance_dict)
-        with CheckInstance(instance_dict) as check:
-            check.contains_allowed_attrs(
-                instance_dict, allowed_attrs,
-                msg="Get Instance")
-            check.flavor()
-            check.datastore()
-            check.links(instance_dict['links'])
-            check.used_volume()
-
-    @test
-    def test_get_instance_status(self):
-        result = dbaas.instances.get(instance_info.id)
-        assert_equal(200, dbaas.last_http_code)
-        asserts.assert_true(result.status in CONFIG.running_status)
-
-    @test
-    def test_get_legacy_status(self):
-        result = dbaas.instances.get(instance_info.id)
-        assert_equal(200, dbaas.last_http_code)
-        assert_true(result is not None)
-
-    @test
-    def test_get_legacy_status_notfound(self):
-        assert_raises(exceptions.NotFound, dbaas.instances.get, -2)
-
-    @test(enabled=VOLUME_SUPPORT)
-    def test_volume_found(self):
-        instance = dbaas.instances.get(instance_info.id)
-        if create_new_instance():
-            assert_equal(instance_info.volume['size'], instance.volume['size'])
-
-    @test(enabled=EPHEMERAL_SUPPORT)
-    def test_ephemeral_mount(self):
-        instance = dbaas.instances.get(instance_info.id)
-        assert_true(isinstance(instance.local_storage['used'], float))
-
-    @test(enabled=ROOT_PARTITION)
-    def test_root_partition(self):
-        instance = dbaas.instances.get(instance_info.id)
-        assert_true(isinstance(instance.local_storage['used'], float))
-
-    @test
-    def test_instance_not_shown_to_other_user(self):
-        daffy_ids = [instance.id for instance in
-                     self.other_client.instances.list()]
-        assert_equal(200, self.other_client.last_http_code)
-        admin_ids = [instance.id for instance in dbaas.instances.list()]
-        assert_equal(200, dbaas.last_http_code)
-
-        assert_not_equal(sorted(admin_ids), sorted(daffy_ids))
-        assert_raises(exceptions.NotFound,
-                      self.other_client.instances.get, instance_info.id)
-
-        for id in admin_ids:
-            assert_equal(daffy_ids.count(id), 0)
-
-    @test
-    def test_instance_not_deleted_by_other_user(self):
-        assert_raises(exceptions.NotFound,
-                      self.other_client.instances.get, instance_info.id)
-        assert_raises(exceptions.NotFound,
-                      self.other_client.instances.delete, instance_info.id)
-
-    @test(enabled=CONFIG.test_mgmt)
-    def test_mgmt_get_instance_after_started(self):
-        result = dbaas_admin.management.show(instance_info.id)
-        allowed_attrs = ['account_id', 'addresses', 'created', 'databases',
-                         'flavor', 'guest_status', 'host', 'hostname', 'id',
-                         'name', 'root_enabled_at', 'root_enabled_by',
-                         'server_state_description', 'status', 'datastore',
-                         'updated', 'users', 'volume', 'fault', 'region',
-                         'access', 'operating_status']
-        with CheckInstance(result._info) as check:
-            check.contains_allowed_attrs(
-                result._info, allowed_attrs,
-                msg="Mgmt get instance")
-            check.flavor()
-            check.datastore()
-            check.guest_status()
-            check.addresses()
-            check.volume_mgmt()
-
-
-@test(depends_on_classes=[TestGetInstances],
-      groups=[tests.DBAAS_API_INSTANCES],
-      enabled=CONFIG.test_mgmt)
-class TestInstanceMgmtInfo(object):
+    def setup(self):
+        self.clients = ClientPair()
+
+    def _wait_for_active(self, *acceptable_states):
+        for instance in (json_instance, xml_instance):
+            instance.get()
+            print('instance.status=%s' % instance.status)
+            while instance.status != "ACTIVE":
+                assert_true(instance.status in acceptable_states,
+                    "Instance status == %s; expected it to be one of these: %s"
+                    % (instance.status, acceptable_states))
+                instance.get()
+                time.sleep(0.1)
+
+    @test
+    def instance_restart(self):
+        results = self.clients.do("instance_restart",
+            ("/instances/%s/action" % json_instance.id,
+             "/instances/%s/action" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id : client.instances.restart(id),
+            ([json_instance.id], [xml_instance.id]))
+        self._wait_for_active("RESTART")
+
+    @test
+    def instance_resize_volume(self):
+        results = self.clients.do("instance_resize_volume",
+            ("/instances/%s/action" % json_instance.id,
+             "/instances/%s/action" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id : client.instances.resize_volume(id, 4),
+            ([json_instance.id], [xml_instance.id]))
+        self._wait_for_active("RESIZE")
+        assert_equal(json_instance.volume['size'], 4)
+        assert_equal(xml_instance.volume['size'], '4')
+
+    @test
+    def instance_resize_flavor(self):
+        results = self.clients.do("instance_resize_flavor",
+            ("/instances/%s/action" % json_instance.id,
+             "/instances/%s/action" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id : client.instances.resize_flavor(id, 3),
+            ([json_instance.id], [xml_instance.id]))
+        self._wait_for_active("RESIZE")
+        assert_equal(json_instance.flavor['id'], '3')
+        assert_equal(xml_instance.flavor['id'], '3')
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances', "MgmtHosts"])
+class MgmtHosts(object):
+
     @before_class
-    def set_up(self):
-        self.mgmt_details = dbaas_admin.management.show(instance_info.id)
+    def setup(self):
+        self.clients = ClientPair()
 
     @test
-    def test_mgmt_ips_associated(self):
-        """Every instances has exactly one address"""
-        mgmt_index = dbaas_admin.management.index()
-        for instance in mgmt_index:
-            assert_equal(1, len(instance.ips))
-
-    @test
-    def test_mgmt_data(self):
-        """Test management API returns all the values expected."""
-        info = instance_info
-        ir = info.initial_result
-        cid = ir.id
-        expected = {
-            'id': cid,
-            'name': ir.name,
-            'account_id': info.user.auth_user,
-            'databases': [
-                {
-                    'name': 'db2',
-                    'character_set': 'utf8mb3',
-                    'collate': 'utf8mb3_general_ci',
-                },
-                {
-                    'name': 'firstdb',
-                    'character_set': 'latin2',
-                    'collate': 'latin2_general_ci',
-                }
-            ],
-        }
+    def mgmt_list_hosts(self):
+        results = self.clients.do("mgmt_list_hosts",
+            "/mgmt/hosts", "GET", 200, "OK",
+            lambda client : client.mgmt.hosts.index())
+        with Check() as check:
+            for hosts in results:
+                check.equal(1, len(hosts))
+                check.equal("fake_host", hosts[0].name)
+            check.equal(2, results[0][0].instanceCount)
+            # In XML land this is a string. :'(
+            check.equal("2", results[1][0].instanceCount)
+
+    @test
+    def mgmt_get_host_detail(self):
+        results = self.clients.do("mgmt_get_host_detail",
+            "/mgmt/hosts/fake_host", "GET", 200, "OK",
+            lambda client : client.mgmt.hosts.get("fake_host"))
+        with Check() as check:
+            for host in results:
+                check.equal(results[0].name, "fake_host")
+                check.equal(results[1].name, "fake_host")
+                # XML entries won't come back as these types. :(
+                check.true(isinstance(results[0].percentUsed, int)),
+                check.true(isinstance(results[0].totalRAM, int)),
+                check.true(isinstance(results[0].usedRAM, int)),
+        with Check() as check:
+            for host in results:
+                check.equal(2, len(host.instances))
+                for instance in host.instances:
+                    check.equal(instance['status'], 'ACTIVE')
+                    check.true(instance['name'] == 'json_rack_instance' or
+                               instance['name'] == 'xml_rack_instance')
+                    #TODO: Check with GUID regex.
+                    check.true(isinstance(instance['id'], basestring))
+                    check.true(isinstance(instance['server_id'], basestring))
+                    check.true(isinstance(instance['tenant_id'], basestring))
+
+    @test
+    def mgmt_host_update_all(self):
+        results = self.clients.do("mgmt_host_update",
+            "/mgmt/hosts/fake_host/instances/action",
+            "POST", 202, "Accepted",
+            lambda client : client.mgmt.hosts.update_all("fake_host"))
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class MgmtStorage(object):
+
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_get_storage(self):
+        results = self.clients.do("mgmt_get_storage",
+            "/mgmt/storage", "GET", 200, "OK",
+            lambda client : client.mgmt.storage.index())
+        for index, devices in enumerate(results):
+            with Check() as check:
+                check.equal(1, len(devices))
+                device = devices[0]
+                check.equal(int(device.capacity['available']), 90)
+                check.equal(int(device.capacity['total']), 100)
+                check.equal(device.name, "fake_storage")
+                check.equal(int(device.provision['available']), 40)
+                check.equal(int(device.provision['percent']), 10)
+                check.equal(int(device.provision['total']), 50)
+                check.equal(device.type, "test_type")
+                check.equal(int(device.used), 10)
+                if index == JSON_INDEX:
+                    check.true(isinstance(device.capacity['available'], int))
+                    check.true(isinstance(device.capacity['total'], int))
+                    check.true(isinstance(device.provision['available'], int))
+                    check.true(isinstance(device.provision['percent'], int))
+                    check.true(isinstance(device.provision['total'], int))
+                    check.true(isinstance(device.used, int))
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class MgmtAccount(object):
+
+    @before_class
+    def setup(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_get_account_details(self):
+        results = self.clients.do("mgmt_get_account_details",
+            "/mgmt/accounts/admin", "GET", 200, "OK",
+            lambda client : client.mgmt.accounts.show("admin"))
+        with Check() as check:
+            for account_info in results:
+                check.equal(2, len(account_info.instances))
+                check.equal('admin', account_info.id)
+
+    @test
+    def mgmt_get_account_list(self):
+        results = self.clients.do("mgmt_list_accounts",
+            "/mgmt/accounts", "GET", 200, "OK",
+            lambda client : client.mgmt.accounts.index())
+        for index, result in enumerate(results):
+            for account in result.accounts:
+                assert_equal('admin', account['id'])
+                if index == JSON_INDEX:
+                    assert_equal(2, account['num_instances'])
+                else:
+                    assert_equal("2", account['num_instances'])
+
+
+def for_both(func):
+    def both(self):
+        for result in self.results:
+            func(self, result)
+    return both
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class MgmtInstance(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+        self.results = self.clients.do("mgmt_get_instance_details",
+            ("/mgmt/instances/%s" % json_instance.id,
+             "/mgmt/instances/%s" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id : client.mgmt.instances.show(id),
+            ([json_instance.id], [xml_instance.id]))
+
+    @test
+    @for_both
+    def created(self, result):
+        #TODO: use regex
+        assert_true(isinstance(result.created, basestring))
+
+    @test
+    def deleted(self):
+        assert_equal(self.results[JSON_INDEX].deleted, False)
+        assert_equal(self.results[XML_INDEX].deleted, "False")
+
+    @test
+    @for_both
+    def flavor(self, result):
+        assert_true(result.flavor['id'] == "1" or result.flavor['id'] == "3")
+        assert_equal(len(result.flavor['links']), 2)
+        #TODO: validate the flavors format.
+
+    @test
+    @for_both
+    def guest_status(self, result):
+        assert_equal(result.guest_status['state_description'], 'running')
+
+    @test
+    @for_both
+    def host(self, result):
+        assert_equal(result.host, 'fake_host')
+
+    @test
+    def id(self):
+        assert_equal(self.results[JSON_INDEX].id, json_instance.id)
+        assert_equal(self.results[XML_INDEX].id, xml_instance.id)
+
+    @test
+    @for_both
+    def links(self, result):
+        assert_true(isinstance(result.links, list))
+        for link in result.links:
+            assert_true(isinstance(link, dict))
+            assert_true(isinstance(link['href'], basestring))
+            assert_true(isinstance(link['rel'], basestring))
+
+    @test
+    def local_id(self):
+        #TODO: regex
+        assert_true(isinstance(self.results[JSON_INDEX].local_id, int))
+        assert_true(isinstance(self.results[XML_INDEX].local_id, basestring))
+
+    @test
+    @for_both
+    def name(self, result):
+        #TODO: regex
+        assert_true(isinstance(result.name, basestring))
+
+    @test
+    @for_both
+    def server_id(self, result):
+        #TODO: regex
+        assert_true(isinstance(result.server_id, basestring))
+
+    @test
+    @for_both
+    def status(self, result):
+        #TODO: regex
+        assert_equal("ACTIVE", result.status)
+
+    @test
+    @for_both
+    def task_description(self, result):
+        assert_equal(result.task_description, "No tasks for the instance.")
+
+    @test
+    @for_both
+    def tenant_id(self, result):
+        assert_equal(result.tenant_id, "admin")
+
+    @test
+    @for_both
+    def updated(self, result):
+        #TODO: regex
+        assert_true(isinstance(result.updated, basestring))
+
+    @test
+    @for_both
+    def volume(self, result):
+        #TODO: regex
+        assert_true(isinstance(result.volume, dict))
+        assert_true('id' in result.volume)
+        assert_true('size' in result.volume)
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class MgmtInstanceIndex(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_instance_index(self, deleted=False):
+        url = "/mgmt/instances?deleted=false"
+        results = self.clients.do("mgmt_instance_index",
+            "/mgmt/instances?deleted=false", "GET", 200, "OK",
+            lambda client : client.mgmt.instances.index(deleted=False))
+        #TODO: Valdiate everything... *sigh*
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class MgmtInstanceDiagnostics(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_get_instance_diagnostics(self):
+        results = self.clients.do("mgmt_instance_diagnostics",
+            ("/mgmt/instances/%s/diagnostics" % json_instance.id,
+             "/mgmt/instances/%s/diagnostics" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id: client.diagnostics.get(id),
+            ([json_instance.id], [xml_instance.id]))
+        #TODO: validate the actual stuff that comes back (booorring!).
+
+
+@test(depends_on=[CreateInstance])
+class MgmtInstanceRoot(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_get_root_details(self):
+        results = self.clients.do("mgmt_get_root_details",
+            ("/mgmt/instances/%s/root" % json_instance.id,
+             "/mgmt/instances/%s/root" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id: client.mgmt.instances.root_enabled_history(id),
+            ([json_instance.id], [xml_instance.id]))
+        #TODO: validate the actual stuff that comes back (booorring!).
+
+
+@test(depends_on=[CreateInstance])
+class MgmtInstanceHWInfo(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_get_hw_info(self):
+        results = self.clients.do("mgmt_get_hw_info",
+            ("/mgmt/instances/%s/hwinfo" % json_instance.id,
+             "/mgmt/instances/%s/hwinfo" % xml_instance.id),
+            "GET", 200, "OK",
+            lambda client, id: client.hw_info.get(id),
+            ([json_instance.id], [xml_instance.id]))
+
+
+@test(depends_on=[CreateInstance], groups=['uses_instances'])
+class MgmtInstanceReboot(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_instance_reboot(self):
+        results = self.clients.do("instance_reboot",
+            ("/mgmt/instances/%s/action" % json_instance.id,
+             "/mgmt/instances/%s/action" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id: client.mgmt.instances.reboot(id),
+            ([json_instance.id], [xml_instance.id]))
+
+
+@test(depends_on=[CreateInstance],
+      groups=['uses_instances'], enabled=False)
+class MgmtInstanceGuestUpdate(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def mgmt_instance_guest_update(self):
+        results = self.clients.do("guest_update",
+            ("/mgmt/instances/%s/action" % json_instance.id,
+             "/mgmt/instances/%s/action" % xml_instance.id),
+            "POST", 202, "Accepted",
+            lambda client, id: client.mgmt.instances.update(id),
+            ([json_instance.id], [xml_instance.id]))
+
+
+@test(depends_on=[CreateInstance], runs_after_groups=['uses_instances'])
+class ZzzDeleteInstance(object):
+
+    @before_class
+    def mgmt_get_instance_details(self):
+        self.clients = ClientPair()
+
+    @test
+    def zzz_delete_instance(self):
+        results = self.clients.do("delete_instance",
+            ("/instances/%s" % json_instance.id,
+             "/instances/%s" % xml_instance.id),
+            "DELETE", 202, "Accepted",
+            lambda client, id: client.instances.delete(id),
+            ([json_instance.id], [xml_instance.id]))
+        for result in json_instance, xml_instance:
+            result.get()
+            assert_equal(result.status, "SHUTDOWN")
+
 
-        expected_entry = info.expected_dns_entry()
-        if expected_entry:
-            expected['hostname'] = expected_entry.name
-
-        assert_true(self.mgmt_details is not None)
-        for (k, v) in expected.items():
-            msg = "Attr %r is missing." % k
-            assert_true(hasattr(self.mgmt_details, k), msg)
-            msg = ("Attr %r expected to be %r but was %r." %
-                   (k, v, getattr(self.mgmt_details, k)))
-            assert_equal(getattr(self.mgmt_details, k), v, msg)
-        print(self.mgmt_details.users)
-        for user in self.mgmt_details.users:
-            assert_true('name' in user, "'name' not in users element.")
-
-
-@test(depends_on_classes=[TestInstanceMgmtInfo],
-      groups=[tests.DBAAS_API_INSTANCES])
-class TestUpdateInstance(object):
-    """Test updating instance."""
-    @test
-    def test_update_name(self):
-        new_name = 'new-name'
-        result = dbaas.instances.update(instance_info.id, name=new_name)
-        assert_equal(202, dbaas.last_http_code)
-        result = dbaas.instances.get(instance_info.id)
-        assert_equal(200, dbaas.last_http_code)
-        assert_equal(new_name, result.name)
-        # Restore instance name because other tests depend on it
-        dbaas.instances.update(instance_info.id, name=instance_info.name)
-        assert_equal(202, dbaas.last_http_code)
-
-    @test
-    def test_update_name_to_invalid_instance(self):
-        # test assigning to an instance that does not exist
-        invalid_id = "invalid-inst-id"
-        assert_raises(exceptions.NotFound,
-                      instance_info.dbaas.instances.update,
-                      invalid_id, name='name')
-        assert_equal(404, instance_info.dbaas.last_http_code)
+if __name__ == "__main__":
+    TestProgram().run_and_exit()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/instances_actions.py` & `trove-8.0.1/trove/tests/api/instances_actions.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,44 +9,56 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-import os
 import time
 
 from proboscis import after_class
 from proboscis import asserts
 from proboscis import before_class
 from proboscis.decorators import time_out
 from proboscis import SkipTest
 from proboscis import test
+from sqlalchemy import exc as sqlalchemy_exc
+from sqlalchemy.sql.expression import text
 from troveclient.compat.exceptions import BadRequest
 from troveclient.compat.exceptions import HTTPNotImplemented
 
-from trove.common import cfg
 from trove.common.utils import poll_until
 from trove import tests
 from trove.tests.api.instances import assert_unprocessable
 from trove.tests.api.instances import EPHEMERAL_SUPPORT
+from trove.tests.api.instances import GROUP as INSTANCE_GROUP
+from trove.tests.api.instances import GROUP_START
 from trove.tests.api.instances import instance_info
 from trove.tests.api.instances import VOLUME_SUPPORT
 from trove.tests.config import CONFIG
 import trove.tests.util as testsutil
+from trove.tests.util.check import Checker
 from trove.tests.util.check import TypeCheck
 from trove.tests.util import LocalSqlClient
 from trove.tests.util.server_connection import create_server_connection
 
+GROUP = "dbaas.api.instances.actions"
+GROUP_REBOOT = "dbaas.api.instances.actions.reboot"
+GROUP_RESTART = "dbaas.api.instances.actions.restart"
+GROUP_RESIZE = "dbaas.api.instances.actions.resize.instance"
+GROUP_STOP_MYSQL = "dbaas.api.instances.actions.stop"
 MYSQL_USERNAME = "test_user"
 MYSQL_PASSWORD = "abcde"
+# stored in test conf
+SERVICE_ID = '123'
 FAKE_MODE = CONFIG.fake_mode
 # If true, then we will actually log into the database.
 USE_IP = not FAKE_MODE
+# If true, then we will actually search for the process
+USE_LOCAL_OVZ = CONFIG.use_local_ovz
 
 
 class MySqlConnection(object):
 
     def __init__(self, host):
         self.host = host
 
@@ -55,48 +67,31 @@
         print("Connecting to MySQL, mysql --host %s -u %s -p%s"
               % (self.host, MYSQL_USERNAME, MYSQL_PASSWORD))
         sql_engine = LocalSqlClient.init_engine(MYSQL_USERNAME, MYSQL_PASSWORD,
                                                 self.host)
         self.client = LocalSqlClient(sql_engine, use_flush=False)
 
     def is_connected(self):
-        cmd = "SELECT 1;"
         try:
             with self.client:
-                self.client.execute(cmd)
+                self.client.execute(text("""SELECT "Hello.";"""))
             return True
-        except Exception as e:
-            print(
-                "Failed to execute command: %s, error: %s" % (cmd, str(e))
-            )
+        except (sqlalchemy_exc.OperationalError,
+                sqlalchemy_exc.DisconnectionError,
+                sqlalchemy_exc.TimeoutError):
             return False
+        except Exception as ex:
+            print("EX WAS:")
+            print(type(ex))
+            print(ex)
+            raise ex
 
-    def execute(self, cmd):
-        try:
-            with self.client:
-                self.client.execute(cmd)
-            return True
-        except Exception as e:
-            print(
-                "Failed to execute command: %s, error: %s" % (cmd, str(e))
-            )
-            return False
-
-
-# Use default value from trove.common.cfg, and it could be overridden by
-# a environment variable when the tests run.
-def get_resize_timeout():
-    value_from_env = os.environ.get("TROVE_RESIZE_TIME_OUT", None)
-    if value_from_env:
-        return int(value_from_env)
-
-    return cfg.CONF.resize_time_out
 
-
-TIME_OUT_TIME = get_resize_timeout()
+TIME_OUT_TIME = 15 * 60
+USER_WAS_DELETED = False
 
 
 class ActionTestBase(object):
     """Has some helpful functions for testing actions.
 
     The test user must be created for some of these functions to work.
 
@@ -114,18 +109,14 @@
         return self.dbaas.instances.get(self.instance_id)
 
     @property
     def instance_address(self):
         return instance_info.get_address()
 
     @property
-    def instance_mgmt_address(self):
-        return instance_info.get_address(mgmt=True)
-
-    @property
     def instance_id(self):
         return instance_info.id
 
     def create_user(self):
         """Create a MySQL user we can use for this test."""
 
         users = [{"name": MYSQL_USERNAME, "password": MYSQL_PASSWORD,
@@ -137,48 +128,36 @@
             return any([user.name == MYSQL_USERNAME for user in users])
 
         poll_until(has_user, time_out=30)
         if not FAKE_MODE:
             time.sleep(5)
 
     def ensure_mysql_is_running(self):
-        if USE_IP:
-            self.connection.connect()
-            asserts.assert_true(self.connection.is_connected(),
-                                "Unable to connect to MySQL.")
-
-            self.proc_id = self.find_mysql_proc_on_instance()
-            asserts.assert_is_not_none(self.proc_id,
-                                       "MySQL process can not be found.")
-
-        asserts.assert_is_not_none(self.instance)
-        asserts.assert_true(self.instance.status in CONFIG.running_status)
+        """Make sure MySQL is accessible before restarting."""
+        with Checker() as check:
+            if USE_IP:
+                self.connection.connect()
+                check.true(self.connection.is_connected(),
+                           "Able to connect to MySQL.")
+                self.proc_id = self.find_mysql_proc_on_instance()
+                check.true(self.proc_id is not None,
+                           "MySQL process can not be found.")
+            instance = self.instance
+            check.false(instance is None)
+            check.equal(instance.status, "ACTIVE")
 
     def find_mysql_proc_on_instance(self):
-        server = create_server_connection(
-            self.instance_id,
-            ip_address=self.instance_mgmt_address
-        )
-        container_exist_cmd = 'sudo docker ps -q'
-        pid_cmd = "sudo docker inspect database -f '{{.State.Pid}}'"
-
+        server = create_server_connection(self.instance_id)
+        cmd = "ps acux | grep mysqld " \
+              "| grep -v mysqld_safe | awk '{print $2}'"
+        stdout, _ = server.execute(cmd)
         try:
-            server.execute(container_exist_cmd)
-        except Exception as err:
-            asserts.fail("Failed to execute command: %s, error: %s" %
-                         (container_exist_cmd, str(err)))
-
-        try:
-            stdout = server.execute(pid_cmd)
             return int(stdout)
         except ValueError:
             return None
-        except Exception as err:
-            asserts.fail("Failed to execute command: %s, error: %s" %
-                         (pid_cmd, str(err)))
 
     def log_current_users(self):
         users = self.dbaas.users.list(self.instance_id)
         CONFIG.get_report().log("Current user count = %d" % len(users))
         for user in users:
             CONFIG.get_report().log("\t" + str(user))
 
@@ -192,15 +171,15 @@
                 instance_info.initial_result.created),
             'launched_at': testsutil.iso_time(self.instance.updated),
             'modify_at': testsutil.iso_time(self.instance.updated)
         }
         return expected
 
 
-@test(depends_on_groups=[tests.DBAAS_API_INSTANCES])
+@test(depends_on_groups=[GROUP_START])
 def create_user():
     """Create a test user so that subsequent tests can log in."""
     helper = ActionTestBase()
     helper.set_up()
     if USE_IP:
         try:
             helper.create_user()
@@ -213,112 +192,133 @@
 
 class RebootTestBase(ActionTestBase):
     """Tests restarting MySQL."""
 
     def call_reboot(self):
         raise NotImplementedError()
 
-    def wait_for_successful_restart(self):
-        """Wait until status becomes running.
-
-        Reboot is an async operation, make sure the instance is rebooting
-        before active.
-        """
-        def _is_rebooting():
-            instance = self.instance
-            if instance.status == "REBOOT":
-                return True
-            return False
-
-        poll_until(_is_rebooting, time_out=TIME_OUT_TIME)
+    def wait_for_broken_connection(self):
+        """Wait until our connection breaks."""
+        if not USE_IP:
+            return
+        if not hasattr(self, "connection"):
+            return
+        poll_until(self.connection.is_connected,
+                   lambda connected: not connected,
+                   time_out=TIME_OUT_TIME)
 
+    def wait_for_successful_restart(self):
+        """Wait until status becomes running."""
         def is_finished_rebooting():
             instance = self.instance
-            asserts.assert_not_equal(instance.status, "ERROR")
-            if instance.status in CONFIG.running_status:
-                return True
-            return False
+            if instance.status == "REBOOT":
+                return False
+            asserts.assert_equal("ACTIVE", instance.status)
+            return True
 
         poll_until(is_finished_rebooting, time_out=TIME_OUT_TIME)
 
     def assert_mysql_proc_is_different(self):
         if not USE_IP:
             return
         new_proc_id = self.find_mysql_proc_on_instance()
         asserts.assert_not_equal(new_proc_id, self.proc_id,
                                  "MySQL process ID should be different!")
 
     def successful_restart(self):
         """Restart MySQL via the REST API successfully."""
+        self.fix_mysql()
         self.call_reboot()
+        self.wait_for_broken_connection()
         self.wait_for_successful_restart()
         self.assert_mysql_proc_is_different()
 
+    def mess_up_mysql(self):
+        """Ruin MySQL's ability to restart."""
+        server = create_server_connection(self.instance_id)
+        cmd = "sudo cp /dev/null /var/lib/mysql/data/ib_logfile%d"
+        instance_info.dbaas_admin.management.stop(self.instance_id)
+        for index in range(2):
+            server.execute(cmd % index)
+
+    def fix_mysql(self):
+        """Fix MySQL's ability to restart."""
+        if not FAKE_MODE:
+            server = create_server_connection(self.instance_id)
+            cmd = "sudo rm /var/lib/mysql/data/ib_logfile%d"
+            # We want to stop mysql so that upstart does not keep trying to
+            # respawn it and block the guest agent from accessing the logs.
+            instance_info.dbaas_admin.management.stop(self.instance_id)
+            for index in range(2):
+                server.execute(cmd % index)
+
     def wait_for_failure_status(self):
         """Wait until status becomes running."""
         def is_finished_rebooting():
             instance = self.instance
-            if instance.status in ['REBOOT', 'ACTIVE', 'HEALTHY']:
+            if instance.status == "REBOOT" or instance.status == "ACTIVE":
                 return False
             # The reason we check for BLOCKED as well as SHUTDOWN is because
             # Upstart might try to bring mysql back up after the borked
             # connection and the guest status can be either
             asserts.assert_true(instance.status in ("SHUTDOWN", "BLOCKED"))
             return True
 
         poll_until(is_finished_rebooting, time_out=TIME_OUT_TIME)
 
-    def wait_for_status(self, status, timeout=60, sleep_time=5):
-        def is_status():
-            instance = self.instance
-            if instance.status in status:
-                return True
-            return False
-
-        poll_until(is_status, time_out=timeout, sleep_time=sleep_time)
-
-    def wait_for_operating_status(self, status, timeout=60, sleep_time=5):
-        def is_status():
-            instance = self.instance
-            if instance.operating_status in status:
-                return True
-            return False
+    def unsuccessful_restart(self):
+        """Restart MySQL via the REST when it should fail, assert it does."""
+        assert not FAKE_MODE
+        self.mess_up_mysql()
+        self.call_reboot()
+        self.wait_for_broken_connection()
+        self.wait_for_failure_status()
 
-        poll_until(is_status, time_out=timeout, sleep_time=sleep_time)
+    def restart_normally(self):
+        """Fix iblogs and reboot normally."""
+        self.fix_mysql()
+        self.test_successful_restart()
 
 
-@test(groups=[tests.DBAAS_API_INSTANCE_ACTIONS],
-      depends_on_groups=[tests.DBAAS_API_DATABASES],
-      depends_on=[create_user])
+@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP_RESTART],
+      depends_on_groups=[GROUP_START], depends_on=[create_user])
 class RestartTests(RebootTestBase):
-    """Test restarting MySQL."""
+    """Tests restarting MySQL."""
 
     def call_reboot(self):
         self.instance.restart()
         asserts.assert_equal(202, self.dbaas.last_http_code)
 
     @before_class
     def test_set_up(self):
         self.set_up()
 
     @test
     def test_ensure_mysql_is_running(self):
         """Make sure MySQL is accessible before restarting."""
         self.ensure_mysql_is_running()
 
-    @test(depends_on=[test_ensure_mysql_is_running])
+    @test(depends_on=[test_ensure_mysql_is_running], enabled=not FAKE_MODE)
+    def test_unsuccessful_restart(self):
+        """Restart MySQL via the REST when it should fail, assert it does."""
+        if FAKE_MODE:
+            raise SkipTest("Cannot run this in fake mode.")
+        self.unsuccessful_restart()
+
+    @test(depends_on=[test_set_up],
+          runs_after=[test_ensure_mysql_is_running, test_unsuccessful_restart])
     def test_successful_restart(self):
         """Restart MySQL via the REST API successfully."""
         self.successful_restart()
 
 
-@test(groups=[tests.DBAAS_API_INSTANCE_ACTIONS],
-      depends_on_classes=[RestartTests])
+@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP_STOP_MYSQL],
+      depends_on_groups=[GROUP_START], depends_on=[create_user])
 class StopTests(RebootTestBase):
-    """Test stopping MySQL."""
+    """Tests which involve stopping MySQL."""
 
     def call_reboot(self):
         self.instance.restart()
 
     @before_class
     def test_set_up(self):
         self.set_up()
@@ -326,178 +326,191 @@
     @test
     def test_ensure_mysql_is_running(self):
         """Make sure MySQL is accessible before restarting."""
         self.ensure_mysql_is_running()
 
     @test(depends_on=[test_ensure_mysql_is_running])
     def test_stop_mysql(self):
-        """Stops MySQL by admin."""
+        """Stops MySQL."""
         instance_info.dbaas_admin.management.stop(self.instance_id)
-        self.wait_for_operating_status(['SHUTDOWN'], timeout=90, sleep_time=10)
+        self.wait_for_broken_connection()
+        self.wait_for_failure_status()
 
     @test(depends_on=[test_stop_mysql])
-    def test_volume_info_while_mysql_is_down(self):
+    def test_instance_get_shows_volume_info_while_mysql_is_down(self):
         """
         Confirms the get call behaves appropriately while an instance is
         down.
         """
         if not VOLUME_SUPPORT:
             raise SkipTest("Not testing volumes.")
         instance = self.dbaas.instances.get(self.instance_id)
         with TypeCheck("instance", instance) as check:
             check.has_field("volume", dict)
             check.true('size' in instance.volume)
             check.true('used' in instance.volume)
             check.true(isinstance(instance.volume.get('size', None), int))
             check.true(isinstance(instance.volume.get('used', None), float))
 
-    @test(depends_on=[test_volume_info_while_mysql_is_down])
-    def test_successful_restart_from_shutdown(self):
+    @test(depends_on=[test_set_up],
+          runs_after=[test_instance_get_shows_volume_info_while_mysql_is_down])
+    def test_successful_restart_when_in_shutdown_state(self):
         """Restart MySQL via the REST API successfully when MySQL is down."""
         self.successful_restart()
 
 
-@test(groups=[tests.DBAAS_API_INSTANCE_ACTIONS],
-      depends_on_classes=[StopTests])
+@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP_REBOOT],
+      depends_on_groups=[GROUP_START], depends_on=[RestartTests, create_user])
 class RebootTests(RebootTestBase):
-    """Test restarting instance."""
+    """Tests restarting instance."""
 
     def call_reboot(self):
         instance_info.dbaas_admin.management.reboot(self.instance_id)
 
     @before_class
     def test_set_up(self):
         self.set_up()
         asserts.assert_true(hasattr(self, 'dbaas'))
         asserts.assert_true(self.dbaas is not None)
 
     @test
     def test_ensure_mysql_is_running(self):
-        """Make sure MySQL is accessible before rebooting."""
+        """Make sure MySQL is accessible before restarting."""
         self.ensure_mysql_is_running()
 
-    @after_class(depends_on=[test_ensure_mysql_is_running])
-    def test_successful_reboot(self):
-        """MySQL process is different after rebooting."""
+    @test(depends_on=[test_ensure_mysql_is_running])
+    def test_unsuccessful_restart(self):
+        """Restart MySQL via the REST when it should fail, assert it does."""
+        if FAKE_MODE:
+            raise SkipTest("Cannot run this in fake mode.")
+        self.unsuccessful_restart()
+
+    @after_class(depends_on=[test_set_up])
+    def test_successful_restart(self):
+        """Restart MySQL via the REST API successfully."""
         if FAKE_MODE:
             raise SkipTest("Cannot run this in fake mode.")
         self.successful_restart()
 
 
-@test(groups=[tests.DBAAS_API_INSTANCE_ACTIONS],
-      depends_on_classes=[RebootTests])
+@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP,
+              GROUP_RESIZE],
+      depends_on_groups=[GROUP_START], depends_on=[create_user],
+      runs_after=[RebootTests])
 class ResizeInstanceTest(ActionTestBase):
-    """Test resizing instance."""
+
+    """
+    Integration Test cases for resize instance
+    """
     @property
     def flavor_id(self):
         return instance_info.dbaas_flavor_href
 
+    def get_flavor_href(self, flavor_id=2):
+        res = instance_info.dbaas.find_flavor_and_self_href(flavor_id)
+        _, dbaas_flavor_href = res
+        return dbaas_flavor_href
+
     def wait_for_resize(self):
         def is_finished_resizing():
             instance = self.instance
             if instance.status == "RESIZE":
                 return False
-            asserts.assert_true(instance.status in CONFIG.running_status)
+            asserts.assert_equal("ACTIVE", instance.status)
             return True
-
         poll_until(is_finished_resizing, time_out=TIME_OUT_TIME)
 
     @before_class
     def setup(self):
         self.set_up()
         if USE_IP:
             self.connection.connect()
             asserts.assert_true(self.connection.is_connected(),
                                 "Should be able to connect before resize.")
+        self.user_was_deleted = False
 
     @test
     def test_instance_resize_same_size_should_fail(self):
         asserts.assert_raises(BadRequest, self.dbaas.instances.resize_instance,
                               self.instance_id, self.flavor_id)
 
     @test(enabled=VOLUME_SUPPORT)
     def test_instance_resize_to_ephemeral_in_volume_support_should_fail(self):
         flavor_name = CONFIG.values.get('instance_bigger_eph_flavor_name',
                                         'eph.rd-smaller')
-        flavor_id = None
-        for item in instance_info.flavors:
-            if item.name == flavor_name:
-                flavor_id = item.id
-
-        asserts.assert_is_not_none(flavor_id)
+        flavors = self.dbaas.find_flavors_by_name(flavor_name)
 
         def is_active():
-            return self.instance.status in CONFIG.running_status
-
+            return self.instance.status == 'ACTIVE'
         poll_until(is_active, time_out=TIME_OUT_TIME)
-        asserts.assert_true(self.instance.status in CONFIG.running_status)
+        asserts.assert_equal(self.instance.status, 'ACTIVE')
 
+        self.get_flavor_href(
+            flavor_id=self.expected_old_flavor_id)
         asserts.assert_raises(HTTPNotImplemented,
                               self.dbaas.instances.resize_instance,
-                              self.instance_id, flavor_id)
+                              self.instance_id, flavors[0].id)
 
     @test(enabled=EPHEMERAL_SUPPORT)
     def test_instance_resize_to_non_ephemeral_flavor_should_fail(self):
         flavor_name = CONFIG.values.get('instance_bigger_flavor_name',
                                         'm1-small')
-        flavor_id = None
-        for item in instance_info.flavors:
-            if item.name == flavor_name:
-                flavor_id = item.id
-
-        asserts.assert_is_not_none(flavor_id)
+        flavors = self.dbaas.find_flavors_by_name(flavor_name)
         asserts.assert_raises(BadRequest, self.dbaas.instances.resize_instance,
-                              self.instance_id, flavor_id)
+                              self.instance_id, flavors[0].id)
 
     def obtain_flavor_ids(self):
         old_id = self.instance.flavor['id']
         self.expected_old_flavor_id = old_id
+        res = instance_info.dbaas.find_flavor_and_self_href(old_id)
+        self.expected_dbaas_flavor, _ = res
         if EPHEMERAL_SUPPORT:
             flavor_name = CONFIG.values.get('instance_bigger_eph_flavor_name',
                                             'eph.rd-smaller')
         else:
             flavor_name = CONFIG.values.get('instance_bigger_flavor_name',
                                             'm1.small')
-
-        new_flavor = None
-        for item in instance_info.flavors:
-            if item.name == flavor_name:
-                new_flavor = item
-                break
-
-        asserts.assert_is_not_none(new_flavor)
-
+        flavors = self.dbaas.find_flavors_by_name(flavor_name)
+        asserts.assert_equal(len(flavors), 1,
+                             "Number of flavors with name '%s' "
+                             "found was '%d'." % (flavor_name,
+                                                  len(flavors)))
+        flavor = flavors[0]
         self.old_dbaas_flavor = instance_info.dbaas_flavor
-        instance_info.dbaas_flavor = new_flavor
-        self.expected_new_flavor_id = new_flavor.id
+        instance_info.dbaas_flavor = flavor
+        asserts.assert_true(flavor is not None,
+                            "Flavor '%s' not found!" % flavor_name)
+        flavor_href = self.dbaas.find_flavor_self_href(flavor)
+        asserts.assert_true(flavor_href is not None,
+                            "Flavor href '%s' not found!" % flavor_name)
+        self.expected_new_flavor_id = flavor.id
 
     @test(depends_on=[test_instance_resize_same_size_should_fail])
     def test_status_changed_to_resize(self):
-        """test_status_changed_to_resize"""
         self.log_current_users()
         self.obtain_flavor_ids()
         self.dbaas.instances.resize_instance(
             self.instance_id,
-            self.expected_new_flavor_id)
+            self.get_flavor_href(flavor_id=self.expected_new_flavor_id))
         asserts.assert_equal(202, self.dbaas.last_http_code)
 
         # (WARNING) IF THE RESIZE IS WAY TOO FAST THIS WILL FAIL
         assert_unprocessable(
             self.dbaas.instances.resize_instance,
             self.instance_id,
-            self.expected_new_flavor_id)
+            self.get_flavor_href(flavor_id=self.expected_new_flavor_id))
 
     @test(depends_on=[test_status_changed_to_resize])
     @time_out(TIME_OUT_TIME)
     def test_instance_returns_to_active_after_resize(self):
-        """test_instance_returns_to_active_after_resize"""
         self.wait_for_resize()
 
     @test(depends_on=[test_instance_returns_to_active_after_resize,
-                      test_status_changed_to_resize])
+                      test_status_changed_to_resize],
+          groups=["dbaas.usage"])
     def test_resize_instance_usage_event_sent(self):
         expected = self._build_expected_msg()
         expected['old_instance_size'] = self.old_dbaas_flavor.ram
         instance_info.consumer.check_message(instance_info.id,
                                              'trove.instance.modify_flavor',
                                              **expected)
 
@@ -505,36 +518,83 @@
           runs_after=[test_resize_instance_usage_event_sent])
     def resize_should_not_delete_users(self):
         """Resize should not delete users."""
         # Resize has an incredibly weird bug where users are deleted after
         # a resize. The code below is an attempt to catch this while proceeding
         # with the rest of the test (note the use of runs_after).
         if USE_IP:
-            users = self.dbaas.users.list(self.instance_id)
-            usernames = [user.name for user in users]
-            if MYSQL_USERNAME not in usernames:
+            self.connection.connect()
+            if not self.connection.is_connected():
+                # Ok, this is def. a failure, but before we toss up an error
+                # lets recreate to see how far we can get.
+                CONFIG.get_report().log(
+                    "Having to recreate the test_user! Resizing killed it!")
+                self.log_current_users()
                 self.create_user()
-                asserts.fail("Resize made the test user disappear.")
+                asserts.fail(
+                    "Somehow, the resize made the test user disappear.")
 
     @test(depends_on=[test_instance_returns_to_active_after_resize],
           runs_after=[resize_should_not_delete_users])
     def test_make_sure_mysql_is_running_after_resize(self):
         self.ensure_mysql_is_running()
 
-    @test(depends_on=[test_make_sure_mysql_is_running_after_resize])
+    @test(depends_on=[test_instance_returns_to_active_after_resize],
+          runs_after=[test_make_sure_mysql_is_running_after_resize])
     def test_instance_has_new_flavor_after_resize(self):
-        actual = self.instance.flavor['id']
-        asserts.assert_equal(actual, self.expected_new_flavor_id)
+        actual = self.get_flavor_href(self.instance.flavor['id'])
+        expected = self.get_flavor_href(flavor_id=self.expected_new_flavor_id)
+        asserts.assert_equal(actual, expected)
 
+    @test(depends_on=[test_instance_has_new_flavor_after_resize])
+    @time_out(TIME_OUT_TIME)
+    def test_resize_down(self):
+        expected_dbaas_flavor = self.expected_dbaas_flavor
+
+        def is_active():
+            return self.instance.status == 'ACTIVE'
+        poll_until(is_active, time_out=TIME_OUT_TIME)
+        asserts.assert_equal(self.instance.status, 'ACTIVE')
 
-@test(depends_on_classes=[ResizeInstanceTest],
-      groups=[tests.DBAAS_API_INSTANCE_ACTIONS],
+        old_flavor_href = self.get_flavor_href(
+            flavor_id=self.expected_old_flavor_id)
+
+        self.dbaas.instances.resize_instance(self.instance_id, old_flavor_href)
+        asserts.assert_equal(202, self.dbaas.last_http_code)
+        self.old_dbaas_flavor = instance_info.dbaas_flavor
+        instance_info.dbaas_flavor = expected_dbaas_flavor
+        self.wait_for_resize()
+        asserts.assert_equal(str(self.instance.flavor['id']),
+                             str(self.expected_old_flavor_id))
+
+    @test(depends_on=[test_resize_down],
+          groups=["dbaas.usage"])
+    def test_resize_instance_down_usage_event_sent(self):
+        expected = self._build_expected_msg()
+        expected['old_instance_size'] = self.old_dbaas_flavor.ram
+        instance_info.consumer.check_message(instance_info.id,
+                                             'trove.instance.modify_flavor',
+                                             **expected)
+
+
+@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP,
+              GROUP + ".resize.instance"],
+      depends_on_groups=[GROUP_START], depends_on=[create_user],
+      runs_after=[RebootTests, ResizeInstanceTest])
+def resize_should_not_delete_users():
+    if USER_WAS_DELETED:
+        asserts.fail("Somehow, the resize made the test user disappear.")
+
+
+@test(runs_after=[ResizeInstanceTest], depends_on=[create_user],
+      groups=[GROUP, tests.INSTANCES, INSTANCE_GROUP, GROUP_RESIZE],
       enabled=VOLUME_SUPPORT)
-class ResizeInstanceVolumeTest(ActionTestBase):
+class ResizeInstanceVolume(ActionTestBase):
     """Resize the volume of the instance."""
+
     @before_class
     def setUp(self):
         self.set_up()
         self.old_volume_size = int(instance_info.volume['size'])
         self.new_volume_size = self.old_volume_size + 1
         self.old_volume_fs_size = instance_info.get_volume_filesystem_size()
 
@@ -544,67 +604,116 @@
         for name in self.expected_dbs:
             databases.append({"name": name})
         instance_info.dbaas.databases.create(instance_info.id, databases)
 
     @test
     @time_out(60)
     def test_volume_resize(self):
-        """test_volume_resize"""
         instance_info.dbaas.instances.resize_volume(instance_info.id,
                                                     self.new_volume_size)
 
     @test(depends_on=[test_volume_resize])
+    @time_out(300)
     def test_volume_resize_success(self):
-        """test_volume_resize_success"""
 
         def check_resize_status():
             instance = instance_info.dbaas.instances.get(instance_info.id)
-            if instance.status in CONFIG.running_status:
+            if instance.status == "ACTIVE":
                 return True
-            elif instance.status in ["RESIZE", "SHUTDOWN"]:
+            elif instance.status == "RESIZE":
                 return False
             else:
                 asserts.fail("Status should not be %s" % instance.status)
 
-        poll_until(check_resize_status, sleep_time=5, time_out=300,
-                   initial_delay=5)
+        poll_until(check_resize_status, sleep_time=2, time_out=300)
         instance = instance_info.dbaas.instances.get(instance_info.id)
         asserts.assert_equal(instance.volume['size'], self.new_volume_size)
 
     @test(depends_on=[test_volume_resize_success])
     def test_volume_filesystem_resize_success(self):
-        """test_volume_filesystem_resize_success"""
         # The get_volume_filesystem_size is a mgmt call through the guestagent
         # and the volume resize occurs through the fake nova-volume.
         # Currently the guestagent fakes don't have access to the nova fakes so
         # it doesn't know that a volume resize happened and to what size so
         # we can't fake the filesystem size.
         if FAKE_MODE:
             raise SkipTest("Cannot run this in fake mode.")
         new_volume_fs_size = instance_info.get_volume_filesystem_size()
         asserts.assert_true(self.old_volume_fs_size < new_volume_fs_size)
         # The total filesystem size is not going to be exactly the same size of
         # cinder volume but it should round to it. (e.g. round(1.9) == 2)
         asserts.assert_equal(round(new_volume_fs_size), self.new_volume_size)
 
-    @test(depends_on=[test_volume_resize_success])
+    @test(depends_on=[test_volume_resize_success], groups=["dbaas.usage"])
     def test_resize_volume_usage_event_sent(self):
-        """test_resize_volume_usage_event_sent"""
         expected = self._build_expected_msg()
         expected['volume_size'] = self.new_volume_size
         expected['old_volume_size'] = self.old_volume_size
         instance_info.consumer.check_message(instance_info.id,
                                              'trove.instance.modify_volume',
                                              **expected)
 
-    @test(depends_on=[test_volume_resize_success])
+    @test
+    @time_out(300)
     def test_volume_resize_success_databases(self):
-        """test_volume_resize_success_databases"""
         databases = instance_info.dbaas.databases.list(instance_info.id)
         db_list = []
         for database in databases:
             db_list.append(database.name)
         for name in self.expected_dbs:
             if name not in db_list:
                 asserts.fail(
                     "Database %s was not found after the volume resize. "
                     "Returned list: %s" % (name, databases))
+
+
+# This tests the ability of the guest to upgrade itself.
+# It is necessarily tricky because we need to be able to upload a new copy of
+# the guest into an apt-repo in the middle of the test.
+# "guest-update-test" is where the knowledge of how to do this is set in the
+# test conf. If it is not specified this test never runs.
+UPDATE_GUEST_CONF = CONFIG.values.get("guest-update-test", None)
+
+
+@test(groups=[tests.INSTANCES, INSTANCE_GROUP, GROUP, GROUP + ".update_guest"],
+      depends_on=[create_user],
+      depends_on_groups=[GROUP_START])
+class UpdateGuest(object):
+
+    def get_version(self):
+        info = instance_info.dbaas_admin.diagnostics.get(instance_info.id)
+        return info.version
+
+    @before_class(enabled=UPDATE_GUEST_CONF is not None)
+    def check_version_is_old(self):
+        """Make sure we have the old version before proceeding."""
+        self.old_version = self.get_version()
+        self.next_version = UPDATE_GUEST_CONF["next-version"]
+        asserts.assert_not_equal(self.old_version, self.next_version)
+
+    @test(enabled=UPDATE_GUEST_CONF is not None)
+    def upload_update_to_repo(self):
+        cmds = UPDATE_GUEST_CONF["install-repo-cmd"]
+        testsutil.execute(*cmds, run_as_root=True, root_helper="sudo")
+
+    @test(enabled=UPDATE_GUEST_CONF is not None,
+          depends_on=[upload_update_to_repo])
+    def update_and_wait_to_finish(self):
+        instance_info.dbaas_admin.management.update(instance_info.id)
+
+        def finished():
+            current_version = self.get_version()
+            if current_version == self.next_version:
+                return True
+            # The only valid thing for it to be aside from next_version is
+            # old version.
+            asserts.assert_equal(current_version, self.old_version)
+        poll_until(finished, sleep_time=1, time_out=3 * 60)
+
+    @test(enabled=UPDATE_GUEST_CONF is not None,
+          depends_on=[upload_update_to_repo])
+    @time_out(30)
+    def update_again(self):
+        """Test the wait time of a pointless update."""
+        instance_info.dbaas_admin.management.update(instance_info.id)
+        # Make sure this isn't taking too long.
+        instance_info.dbaas_admin.diagnostics.get(instance_info.id)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/limits.py` & `trove-8.0.1/trove/tests/api/limits.py`

 * *Files 5% similar despite different names*

```diff
@@ -35,15 +35,14 @@
 
 GROUP = "dbaas.api.limits"
 DEFAULT_RATE = CONF.http_get_rate
 
 DEFAULT_MAX_VOLUMES = CONF.max_volumes_per_tenant
 DEFAULT_MAX_INSTANCES = CONF.max_instances_per_tenant
 DEFAULT_MAX_BACKUPS = CONF.max_backups_per_tenant
-DEFAULT_MAX_RAM = CONF.max_ram_per_tenant
 
 
 def ensure_limits_are_not_faked(func):
     def _cd(*args, **kwargs):
         fake_limits.ENABLED = True
         try:
             return func(*args, **kwargs)
@@ -86,41 +85,40 @@
         return self._users.find_user_by_name(name)
 
     def __is_available(self, next_available):
         dt_next = timeutils.parse_isotime(next_available)
         dt_now = datetime.now()
         return dt_next.time() < dt_now.time()
 
-    # def _get_limits_as_dict(self, limits):
-    #     d = {}
-    #     for limit in limits:
-    #         d[l.verb] = limit
-    #     return d
-
-    # @test
-    # @ensure_limits_are_not_faked
-    # def test_limits_index(self):
-    #     """Test_limits_index."""
-
-    #     limits = self.rd_client.limits.list()
-    #     d = self._get_limits_as_dict(limits)
-
-    #     # remove the abs_limits from the rate limits
-    #     abs_limits = d.pop("ABSOLUTE", None)
-    #     assert_equal(abs_limits.verb, "ABSOLUTE")
-    #     assert_equal(int(abs_limits.max_instances), DEFAULT_MAX_INSTANCES)
-    #     assert_equal(int(abs_limits.max_backups), DEFAULT_MAX_BACKUPS)
-    #     assert_equal(int(abs_limits.max_volumes), DEFAULT_MAX_VOLUMES)
-    #     assert_equal(int(abs_limits.max_ram), DEFAULT_MAX_RAM)
-
-    #     for k in d:
-    #         assert_equal(d[k].verb, k)
-    #         assert_equal(d[k].unit, "MINUTE")
-    #         assert_true(int(d[k].remaining) <= DEFAULT_RATE)
-    #         assert_true(d[k].nextAvailable is not None)
+    def _get_limits_as_dict(self, limits):
+        d = {}
+        for l in limits:
+            d[l.verb] = l
+        return d
+
+    @test
+    @ensure_limits_are_not_faked
+    def test_limits_index(self):
+        """Test_limits_index."""
+
+        limits = self.rd_client.limits.list()
+        d = self._get_limits_as_dict(limits)
+
+        # remove the abs_limits from the rate limits
+        abs_limits = d.pop("ABSOLUTE", None)
+        assert_equal(abs_limits.verb, "ABSOLUTE")
+        assert_equal(int(abs_limits.max_instances), DEFAULT_MAX_INSTANCES)
+        assert_equal(int(abs_limits.max_backups), DEFAULT_MAX_BACKUPS)
+        assert_equal(int(abs_limits.max_volumes), DEFAULT_MAX_VOLUMES)
+
+        for k in d:
+            assert_equal(d[k].verb, k)
+            assert_equal(d[k].unit, "MINUTE")
+            assert_true(int(d[k].remaining) <= DEFAULT_RATE)
+            assert_true(d[k].nextAvailable is not None)
 
     @test
     @ensure_limits_are_not_faked
     def test_limits_get_remaining(self):
         """Test_limits_get_remaining."""
 
         limits = ()
@@ -130,15 +128,14 @@
         d = self._get_limits_as_dict(limits)
         abs_limits = d["ABSOLUTE"]
         get = d["GET"]
 
         assert_equal(int(abs_limits.max_instances), DEFAULT_MAX_INSTANCES)
         assert_equal(int(abs_limits.max_backups), DEFAULT_MAX_BACKUPS)
         assert_equal(int(abs_limits.max_volumes), DEFAULT_MAX_VOLUMES)
-        assert_equal(int(abs_limits.max_ram), DEFAULT_MAX_RAM)
         assert_equal(get.verb, "GET")
         assert_equal(get.unit, "MINUTE")
         assert_true(int(get.remaining) <= DEFAULT_RATE - 5)
         assert_true(get.nextAvailable is not None)
 
     @test
     @ensure_limits_are_not_faked
@@ -162,15 +159,13 @@
                 assert_equal(get.unit, "MINUTE")
                 assert_equal(int(abs_limits.max_instances),
                              DEFAULT_MAX_INSTANCES)
                 assert_equal(int(abs_limits.max_backups),
                              DEFAULT_MAX_BACKUPS)
                 assert_equal(int(abs_limits.max_volumes),
                              DEFAULT_MAX_VOLUMES)
-                assert_equal(int(abs_limits.max_ram,),
-                             DEFAULT_MAX_RAM)
 
             except exceptions.OverLimit:
                 encountered = True
 
         assert_true(encountered)
         assert_true(int(get.remaining) <= 50)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/mgmt/configurations.py` & `trove-8.0.1/trove/tests/api/mgmt/configurations.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,16 @@
 from trove.tests.util import create_dbaas_client
 from trove.tests.util import test_config
 from trove.tests.util.users import Requirements
 
 GROUP = "dbaas.api.mgmt.configurations"
 
 
-@test(groups=[GROUP, tests.DBAAS_API, tests.PRE_INSTANCES])
+@test(groups=[GROUP, tests.DBAAS_API, tests.PRE_INSTANCES],
+      depends_on_groups=["services.initialize"])
 class ConfigGroupsSetupBeforeInstanceCreation(object):
 
     @before_class
     def setUp(self):
         self.user = test_config.users.find_user(Requirements(is_admin=True))
         self.admin_client = create_dbaas_client(self.user)
         self.datastore_version_id = self.admin_client.datastore_versions.get(
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/mgmt/datastore_versions.py` & `trove-8.0.1/trove/tests/api/mgmt/datastore_versions.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,62 +18,62 @@
 from proboscis.asserts import assert_raises
 from proboscis.asserts import assert_true
 from proboscis import before_class
 from proboscis.check import Check
 from proboscis import test
 from troveclient.compat import exceptions
 
-from trove import tests
 from trove.tests.config import CONFIG
 from trove.tests.util import create_client
 from trove.tests.util import create_dbaas_client
 from trove.tests.util import create_glance_client
 from trove.tests.util import test_config
 from trove.tests.util.users import Requirements
 
+GROUP = "dbaas.api.mgmt.ds_versions"
 
-@test(groups=[tests.DBAAS_API_MGMT_DATASTORES],
-      depends_on_groups=[tests.DBAAS_API_DATASTORES])
+
+@test(groups=[GROUP])
+def mgmt_datastore_version_list_requires_admin_account():
+    """Verify that an admin context is required to call this function."""
+    client = create_client(is_admin=False)
+    assert_raises(exceptions.Unauthorized, client.mgmt_datastore_versions.list)
+
+
+@test(groups=[GROUP])
 class MgmtDataStoreVersion(object):
     """Tests the mgmt datastore version methods."""
 
     @before_class
     def setUp(self):
         """Create client for tests."""
         reqs = Requirements(is_admin=True)
         self.user = CONFIG.users.find_user(reqs)
         self.client = create_dbaas_client(self.user)
         self.images = []
-
-        glance_user = test_config.users.find_user(
-            Requirements(is_admin=True, services=["glance"]))
-        self.glance_client = create_glance_client(glance_user)
-        images = self.glance_client.images.list()
-        for image in images:
-            self.images.append(image.id)
+        if test_config.glance_client is not None:
+            glance_user = test_config.users.find_user(
+                Requirements(services=["glance"]))
+            self.glance_client = create_glance_client(glance_user)
+            images = self.glance_client.images.list()
+            for image in images:
+                self.images.append(image.id)
 
     def _find_ds_version_by_name(self, ds_version_name):
         ds_versions = self.client.mgmt_datastore_versions.list()
         for ds_version in ds_versions:
             if ds_version_name == ds_version.name:
                 return ds_version
 
     @test
     def test_mgmt_ds_version_list_original_count(self):
         """Tests the mgmt datastore version list method."""
         self.ds_versions = self.client.mgmt_datastore_versions.list()
-        # datastore-versions should exist for a functional Trove deployment.
-        assert_true(len(self.ds_versions) > 0)
-
-    @test
-    def mgmt_datastore_version_list_requires_admin_account(self):
-        """Test admin is required to list datastore versions."""
-        client = create_client(is_admin=False)
-        assert_raises(exceptions.Unauthorized,
-                      client.mgmt_datastore_versions.list)
+        # By default we create two datastore-versions for mysql
+        assert_equal(2, len(self.ds_versions))
 
     @test(depends_on=[test_mgmt_ds_version_list_original_count])
     def test_mgmt_ds_version_list_fields_present(self):
         """Verify that all expected fields are returned by list method."""
 
         expected_fields = [
             'id',
@@ -150,14 +150,11 @@
 
     @test(depends_on=[test_mgmt_ds_version_patch])
     def test_mgmt_ds_version_delete(self):
         """Tests the mgmt datastore version delete method."""
         self.client.mgmt_datastore_versions.delete(self.created_version.id)
         assert_equal(202, self.client.last_http_code)
 
-        # Delete the created datastore as well.
-        self.client.datastores.delete(self.created_version.datastore_id)
-
         # Lets match the total count of ds_version,
         # it should get back to original
         ds_versions = self.client.mgmt_datastore_versions.list()
         assert_equal(len(self.ds_versions), len(ds_versions))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/mgmt/instances_actions.py` & `trove-8.0.1/trove/tests/api/mgmt/instances_actions.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,45 +8,45 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest import mock
-
+from mox3 import mox
 from novaclient.v2.servers import Server
 from proboscis import after_class
 from proboscis.asserts import assert_equal
 from proboscis.asserts import assert_raises
 from proboscis import before_class
 from proboscis import SkipTest
 from proboscis import test
 
 from trove.backup import models as backup_models
 from trove.backup import state
 from trove.common.context import TroveContext
 from trove.common import exception
+import trove.common.instance as tr_instance
 from trove.extensions.mgmt.instances.models import MgmtInstance
 from trove.extensions.mgmt.instances.service import MgmtInstanceController
 from trove.instance import models as imodels
 from trove.instance.models import DBInstance
-from trove.instance import service_status as srvstatus
 from trove.instance.tasks import InstanceTasks
 from trove.tests.config import CONFIG
 from trove.tests.util import create_dbaas_client
 from trove.tests.util import test_config
 from trove.tests.util.users import Requirements
 
 GROUP = "dbaas.api.mgmt.action.reset-task-status"
 
 
 class MgmtInstanceBase(object):
 
     def setUp(self):
+        self.mock = mox.Mox()
         self._create_instance()
         self.controller = MgmtInstanceController()
 
     def tearDown(self):
         self.db_info.delete()
 
     def _create_instance(self):
@@ -56,21 +56,21 @@
             id="inst-id-1",
             name="instance",
             flavor_id=1,
             datastore_version_id=test_config.dbaas_datastore_version_id,
             tenant_id=self.tenant_id,
             volume_size=None,
             task_status=InstanceTasks.NONE)
-        self.server = mock.MagicMock(spec=Server)
+        self.server = self.mock.CreateMock(Server)
         self.instance = imodels.Instance(
             self.context,
             self.db_info,
             self.server,
             datastore_status=imodels.InstanceServiceStatus(
-                srvstatus.ServiceStatuses.RUNNING))
+                tr_instance.ServiceStatuses.RUNNING))
 
     def _make_request(self, path='/', context=None, **kwargs):
         from webob import Request
         path = '/'
         print("path: %s" % path)
         return Request.blank(path=path, environ={'trove.context': context},
                              **kwargs)
@@ -99,23 +99,26 @@
         req = super(RestartTaskStatusTests, self)._make_request(path, context,
                                                                 **kwargs)
         req.method = 'POST'
         body = {'reset-task-status': {}}
         return req, body
 
     def reset_task_status(self):
-        with mock.patch.object(MgmtInstance, 'load') as mock_load:
-            mock_load.return_value = self.instance
-            req, body = self._make_request(context=self.context)
-            self.controller = MgmtInstanceController()
-            resp = self.controller.action(req, body, self.tenant_id,
-                                          self.db_info.id)
+        self.mock.StubOutWithMock(MgmtInstance, 'load')
+        MgmtInstance.load(context=self.context,
+                          id=self.db_info.id).AndReturn(self.instance)
+        self.mock.ReplayAll()
+
+        req, body = self._make_request(context=self.context)
+        self.controller = MgmtInstanceController()
+        resp = self.controller.action(req, body, self.tenant_id,
+                                      self.db_info.id)
 
-            mock_load.assert_called_once_with(context=self.context,
-                                              id=self.db_info.id)
+        self.mock.UnsetStubs()
+        self.mock.VerifyAll()
         return resp
 
     @test
     def mgmt_restart_task_requires_admin_account(self):
         context = TroveContext(is_admin=False)
         req, body = self._make_request(context=context)
         self.controller = MgmtInstanceController()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/mgmt/quotas.py` & `trove-8.0.1/trove/tests/api/mgmt/quotas.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/api/root.py` & `trove-8.0.1/trove/tests/api/root.py`

 * *Files 11% similar despite different names*

```diff
@@ -8,97 +8,109 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+
 from nose.plugins.skip import SkipTest
-import proboscis
 from proboscis.asserts import assert_equal
 from proboscis.asserts import assert_false
 from proboscis.asserts import assert_not_equal
 from proboscis.asserts import assert_raises
 from proboscis.asserts import assert_true
+from proboscis import before_class
 from proboscis import test
 from troveclient.compat import exceptions
 
 from trove import tests
-from trove.tests.api import instances
+from trove.tests.api.databases import TestMysqlAccess
+from trove.tests.api.instances import instance_info
+from trove.tests.api.users import TestUsers
+from trove.tests import util
 from trove.tests.util import test_config
 
 
-@test(groups=[tests.DBAAS_API_USERS_ROOT],
-      depends_on_groups=[tests.DBAAS_API_INSTANCES])
+GROUP = "dbaas.api.root"
+
+
+@test(depends_on_classes=[TestMysqlAccess],
+      runs_after=[TestUsers],
+      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES])
 class TestRoot(object):
+    """
+    Test the root operations
+    """
+
     root_enabled_timestamp = 'Never'
+    system_users = ['root', 'debian_sys_maint']
 
-    @proboscis.before_class
+    @before_class
     def setUp(self):
-        # Reuse the instance created previously.
-        self.id = instances.instance_info.id
-        self.dbaas = instances.instance_info.dbaas
-        self.dbaas_admin = instances.instance_info.dbaas_admin
+        self.dbaas = util.create_dbaas_client(instance_info.user)
+        self.dbaas_admin = util.create_dbaas_client(instance_info.admin_user)
 
     def _verify_root_timestamp(self, id):
         reh = self.dbaas_admin.management.root_enabled_history(id)
         timestamp = reh.enabled
         assert_equal(self.root_enabled_timestamp, timestamp)
         assert_equal(id, reh.id)
 
     def _root(self):
-        self.dbaas.root.create(self.id)
+        global root_password
+        self.dbaas.root.create(instance_info.id)
         assert_equal(200, self.dbaas.last_http_code)
         reh = self.dbaas_admin.management.root_enabled_history
-        self.root_enabled_timestamp = reh(self.id).enabled
+        self.root_enabled_timestamp = reh(instance_info.id).enabled
 
     @test
     def test_root_initially_disabled(self):
         """Test that root is disabled."""
-        enabled = self.dbaas.root.is_root_enabled(self.id)
+        enabled = self.dbaas.root.is_root_enabled(instance_info.id)
         assert_equal(200, self.dbaas.last_http_code)
 
         is_enabled = enabled
         if hasattr(enabled, 'rootEnabled'):
             is_enabled = enabled.rootEnabled
         assert_false(is_enabled, "Root SHOULD NOT be enabled.")
 
     @test
     def test_create_user_os_admin_failure(self):
         users = [{"name": "os_admin", "password": "12345"}]
         assert_raises(exceptions.BadRequest, self.dbaas.users.create,
-                      self.id, users)
+                      instance_info.id, users)
 
     @test
     def test_delete_user_os_admin_failure(self):
         assert_raises(exceptions.BadRequest, self.dbaas.users.delete,
-                      self.id, "os_admin")
+                      instance_info.id, "os_admin")
 
     @test(depends_on=[test_root_initially_disabled],
           enabled=not test_config.values['root_removed_from_instance_api'])
     def test_root_initially_disabled_details(self):
         """Use instance details to test that root is disabled."""
-        instance = self.dbaas.instances.get(self.id)
+        instance = self.dbaas.instances.get(instance_info.id)
         assert_true(hasattr(instance, 'rootEnabled'),
                     "Instance has no rootEnabled property.")
         assert_false(instance.rootEnabled, "Root SHOULD NOT be enabled.")
         assert_equal(self.root_enabled_timestamp, 'Never')
 
     @test(depends_on=[test_root_initially_disabled_details])
     def test_root_disabled_in_mgmt_api(self):
         """Verifies in the management api that the timestamp exists."""
-        self._verify_root_timestamp(self.id)
+        self._verify_root_timestamp(instance_info.id)
 
     @test(depends_on=[test_root_initially_disabled_details])
     def test_root_disable_when_root_not_enabled(self):
         reh = self.dbaas_admin.management.root_enabled_history
-        self.root_enabled_timestamp = reh(self.id).enabled
+        self.root_enabled_timestamp = reh(instance_info.id).enabled
         assert_raises(exceptions.NotFound, self.dbaas.root.delete,
-                      self.id)
-        self._verify_root_timestamp(self.id)
+                      instance_info.id)
+        self._verify_root_timestamp(instance_info.id)
 
     @test(depends_on=[test_root_disable_when_root_not_enabled])
     def test_enable_root(self):
         self._root()
 
     @test(depends_on=[test_enable_root])
     def test_enabled_timestamp(self):
@@ -106,71 +118,71 @@
 
     @test(depends_on=[test_enable_root])
     def test_root_not_in_users_list(self):
         """
         Tests that despite having enabled root, user root doesn't appear
         in the users list for the instance.
         """
-        users = self.dbaas.users.list(self.id)
+        users = self.dbaas.users.list(instance_info.id)
         usernames = [user.name for user in users]
         assert_true('root' not in usernames)
 
     @test(depends_on=[test_enable_root])
     def test_root_now_enabled(self):
         """Test that root is now enabled."""
-        enabled = self.dbaas.root.is_root_enabled(self.id)
+        enabled = self.dbaas.root.is_root_enabled(instance_info.id)
         assert_equal(200, self.dbaas.last_http_code)
         assert_true(enabled, "Root SHOULD be enabled.")
 
     @test(depends_on=[test_root_now_enabled],
           enabled=not test_config.values['root_removed_from_instance_api'])
     def test_root_now_enabled_details(self):
         """Use instance details to test that root is now enabled."""
-        instance = self.dbaas.instances.get(self.id)
+        instance = self.dbaas.instances.get(instance_info.id)
         assert_true(hasattr(instance, 'rootEnabled'),
                     "Instance has no rootEnabled property.")
         assert_true(instance.rootEnabled, "Root SHOULD be enabled.")
         assert_not_equal(self.root_enabled_timestamp, 'Never')
-        self._verify_root_timestamp(self.id)
+        self._verify_root_timestamp(instance_info.id)
 
     @test(depends_on=[test_root_now_enabled_details])
     def test_reset_root(self):
         if test_config.values['root_timestamp_disabled']:
             raise SkipTest("Enabled timestamp not enabled yet")
         old_ts = self.root_enabled_timestamp
         self._root()
         assert_not_equal(self.root_enabled_timestamp, 'Never')
         assert_equal(self.root_enabled_timestamp, old_ts)
 
     @test(depends_on=[test_reset_root])
     def test_root_still_enabled(self):
         """Test that after root was reset it's still enabled."""
-        enabled = self.dbaas.root.is_root_enabled(self.id)
+        enabled = self.dbaas.root.is_root_enabled(instance_info.id)
         assert_equal(200, self.dbaas.last_http_code)
         assert_true(enabled, "Root SHOULD still be enabled.")
 
     @test(depends_on=[test_root_still_enabled],
           enabled=not test_config.values['root_removed_from_instance_api'])
     def test_root_still_enabled_details(self):
         """Use instance details to test that after root was reset,
             it's still enabled.
         """
-        instance = self.dbaas.instances.get(self.id)
+        instance = self.dbaas.instances.get(instance_info.id)
         assert_true(hasattr(instance, 'rootEnabled'),
                     "Instance has no rootEnabled property.")
         assert_true(instance.rootEnabled, "Root SHOULD still be enabled.")
         assert_not_equal(self.root_enabled_timestamp, 'Never')
-        self._verify_root_timestamp(self.id)
+        self._verify_root_timestamp(instance_info.id)
 
     @test(depends_on=[test_enable_root])
     def test_root_cannot_be_deleted(self):
         """Even if root was enabled, the user root cannot be deleted."""
         assert_raises(exceptions.BadRequest, self.dbaas.users.delete,
-                      self.id, "root")
+                      instance_info.id, "root")
 
     @test(depends_on=[test_root_still_enabled_details])
     def test_root_disable(self):
         reh = self.dbaas_admin.management.root_enabled_history
-        self.root_enabled_timestamp = reh(self.id).enabled
-        self.dbaas.root.delete(self.id)
-        assert_equal(204, self.dbaas.last_http_code)
-        self._verify_root_timestamp(self.id)
+        self.root_enabled_timestamp = reh(instance_info.id).enabled
+        self.dbaas.root.delete(instance_info.id)
+        assert_equal(200, self.dbaas.last_http_code)
+        self._verify_root_timestamp(instance_info.id)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/user_access.py` & `trove-8.0.1/trove/tests/api/user_access.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,17 +18,22 @@
 from proboscis import asserts
 from proboscis import before_class
 from proboscis import test
 from troveclient.compat import exceptions
 
 from trove import tests
 from trove.tests.api.instances import instance_info
+from trove.tests.api.users import TestUsers
 from trove.tests import util
 from trove.tests.util import test_config
 
+GROUP = "dbaas.api.useraccess"
+GROUP_POSITIVE = GROUP + ".positive"
+GROUP_NEGATIVE = GROUP + ".negative"
+
 FAKE = test_config.values['fake_mode']
 
 
 class UserAccessBase(object):
     """
     Base class for Positive and Negative TestUserAccess classes
     """
@@ -114,18 +119,22 @@
                                         )
                 except exceptions.NotFound:
                     # This is all right here, since we're resetting.
                     pass
         self._test_access(self.users, [])
 
 
-@test(depends_on_groups=[tests.DBAAS_API_USERS],
-      groups=[tests.DBAAS_API_USERS_ACCESS])
+@test(depends_on_classes=[TestUsers],
+      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES],
+      runs_after=[TestUsers])
 class TestUserAccessPasswordChange(UserAccessBase):
-    """Test that change_password works."""
+    """
+    Test that change_password works.
+    """
+
     @before_class
     def setUp(self):
         super(TestUserAccessPasswordChange, self).set_up()
 
     def _check_mysql_connection(self, username, password, success=True):
         # This can only test connections for users with the host %.
         # Much more difficult to simulate connection attempts from other hosts.
@@ -182,50 +191,56 @@
     def test_change_password(self):
         # Doesn't actually change anything, just tests that the call doesn't
         # have any problems. As an aside, also checks that a user can
         # change its password to the same thing again.
         user = self._pick_a_user()
         password = user["password"]
         self.dbaas.users.change_passwords(instance_info.id, [user])
-
-        asserts.assert_equal(202, self.dbaas.last_http_code)
         self._check_mysql_connection(user["name"], password)
 
     @test(depends_on=[test_change_password])
     def test_change_password_back(self):
-        """Test change and restore user password."""
         user = self._pick_a_user()
         old_password = user["password"]
         new_password = "NEWPASSWORD"
 
         user["password"] = new_password
         self.dbaas.users.change_passwords(instance_info.id, [user])
-
-        asserts.assert_equal(202, self.dbaas.last_http_code)
         self._check_mysql_connection(user["name"], new_password)
 
         user["password"] = old_password
         self.dbaas.users.change_passwords(instance_info.id, [user])
-
-        asserts.assert_equal(202, self.dbaas.last_http_code)
         self._check_mysql_connection(user["name"], old_password)
 
+    @test(depends_on=[test_change_password_back])
+    def test_change_password_twice(self):
+        # Changing the password twice isn't a problem.
+        user = self._pick_a_user()
+        password = "NEWPASSWORD"
+        user["password"] = password
+        self.dbaas.users.change_passwords(instance_info.id, [user])
+        self.dbaas.users.change_passwords(instance_info.id, [user])
+        self._check_mysql_connection(user["name"], password)
+
     @after_class(always_run=True)
     def tearDown(self):
         for database in self.databases:
             self.dbaas.databases.delete(instance_info.id, database)
             asserts.assert_equal(202, self.dbaas.last_http_code)
         for username in self.users:
             self.dbaas.users.delete(instance_info.id, username)
 
 
-@test(depends_on_classes=[TestUserAccessPasswordChange],
-      groups=[tests.DBAAS_API_USERS_ACCESS])
+@test(depends_on_classes=[TestUsers],
+      groups=[tests.DBAAS_API, GROUP, GROUP_POSITIVE, tests.INSTANCES],
+      runs_after=[TestUsers])
 class TestUserAccessPositive(UserAccessBase):
-    """Test the creation and deletion of user grants."""
+    """
+    Test the creation and deletion of user grants.
+    """
 
     @before_class
     def setUp(self):
         super(TestUserAccessPositive, self).set_up()
         # None of the ghosts are real databases or users.
         self.ghostdbs = ["test_user_access_ghost_db"]
         self.ghostusers = ["test_ghostuser"]
@@ -349,17 +364,21 @@
             self.dbaas.databases.delete(instance_info.id, database)
             asserts.assert_equal(202, self.dbaas.last_http_code)
         for username in self.users:
             self.dbaas.users.delete(instance_info.id, username)
 
 
 @test(depends_on_classes=[TestUserAccessPositive],
-      groups=[tests.DBAAS_API_USERS_ACCESS])
+      groups=[tests.DBAAS_API, GROUP, GROUP_NEGATIVE, tests.INSTANCES],
+      depends_on=[TestUserAccessPositive])
 class TestUserAccessNegative(UserAccessBase):
-    """Negative tests for the creation and deletion of user grants."""
+    """
+    Negative tests for the creation and deletion of user grants.
+    """
+
     @before_class
     def setUp(self):
         super(TestUserAccessNegative, self).set_up()
         self.users = ["qe_user?neg3F", "qe_user#neg23"]
         self.databases = [("qe_user_neg_db%02i" % i) for i in range(2)]
         self.ghostdbs = []
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/users.py` & `trove-8.0.1/trove/tests/api/users.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,54 +9,42 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import time
-from urllib import parse as urllib_parse
+
+from six.moves.urllib import parse as urllib_parse
 
 from proboscis import after_class
 from proboscis.asserts import assert_equal
 from proboscis.asserts import assert_false
 from proboscis.asserts import assert_raises
 from proboscis.asserts import assert_true
 from proboscis.asserts import fail
 from proboscis import before_class
 from proboscis import test
 from troveclient.compat import exceptions
 
 from trove import tests
+from trove.tests.api.databases import TestDatabases
+from trove.tests.api.databases import TestMysqlAccess
 from trove.tests.api.instances import instance_info
 from trove.tests import util
 from trove.tests.util import test_config
 
-FAKE = test_config.values['fake_mode']
-
-
-@test(depends_on_groups=[tests.DBAAS_API_USERS_ROOT],
-      groups=[tests.DBAAS_API_USERS],
-      enabled=not test_config.values['fake_mode'])
-class TestMysqlAccessNegative(object):
-    """Make sure that MySQL server was secured."""
-    @test
-    def test_mysql_admin(self):
-        """Ensure we aren't allowed access with os_admin and wrong password."""
-        util.mysql_connection().assert_fails(
-            instance_info.get_address(), "os_admin", "asdfd-asdf234")
 
-    @test
-    def test_mysql_root(self):
-        """Ensure we aren't allowed access with root and wrong password."""
-        util.mysql_connection().assert_fails(
-            instance_info.get_address(), "root", "dsfgnear")
+GROUP = "dbaas.api.users"
+FAKE = test_config.values['fake_mode']
 
 
-@test(depends_on_classes=[TestMysqlAccessNegative],
-      groups=[tests.DBAAS_API_USERS])
+@test(depends_on_classes=[TestMysqlAccess],
+      groups=[tests.DBAAS_API, GROUP, tests.INSTANCES],
+      runs_after=[TestDatabases])
 class TestUsers(object):
     """
     Test the creation and deletion of users
     """
 
     username = "tes!@#tuser"
     password = "testpa$^%ssword"
@@ -195,25 +183,25 @@
         assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                       instance_info.id, [user])
 
         self.dbaas.users.delete(instance_info.id, username)
 
     @test(depends_on=[test_create_users_list, test_delete_users])
     def test_hostnames_make_users_unique(self):
-        """test_hostnames_make_users_unique."""
+        # These tests rely on test_delete_users as they create users only
+        # they use.
         username = "testuser_unique"
         hostnames = ["192.168.0.1", "192.168.0.2"]
         users = [{"name": username, "password": "password", "databases": [],
                   "host": hostname}
                  for hostname in hostnames]
 
         # Nothing wrong with creating two users with the same name, so long
         # as their hosts are different.
         self.dbaas.users.create(instance_info.id, users)
-
         for hostname in hostnames:
             self.dbaas.users.delete(instance_info.id, username,
                                     hostname=hostname)
 
     @test()
     def test_updateduser_newname_host_unique(self):
         # The updated_username@hostname should not exist already
@@ -375,14 +363,33 @@
         users = []
         users.append({"name": "user,", "password": self.password,
                       "database": self.db1})
         assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                       instance_info.id, users)
         assert_equal(400, self.dbaas.last_http_code)
 
+    @test(enabled=False)
+    # TODO(hub_cap): Make this test work once python-routes is updated,
+    # if ever.
+    def test_delete_user_with_period_in_name(self):
+        """Attempt to create/destroy a user with a period in its name."""
+        users = []
+        username_with_period = "user.name"
+        users.append({"name": username_with_period, "password": self.password,
+                      "databases": [{"name": self.db1}]})
+        self.dbaas.users.create(instance_info.id, users)
+        assert_equal(202, self.dbaas.last_http_code)
+        if not FAKE:
+            time.sleep(5)
+
+        self.check_database_for_user(username_with_period, self.password,
+                                     [self.db1])
+        self.dbaas.users.delete(instance_info.id, username_with_period)
+        assert_equal(202, self.dbaas.last_http_code)
+
     @test
     def test_invalid_password(self):
         users = [{"name": "anouser", "password": "sdf,;",
                   "database": self.db1}]
         assert_raises(exceptions.BadRequest, self.dbaas.users.create,
                       instance_info.id, users)
         assert_equal(400, self.dbaas.last_http_code)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/api/versions.py` & `trove-8.0.1/trove/tests/api/versions.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,28 +19,30 @@
 from troveclient.compat.exceptions import ClientException
 
 from trove import tests
 from trove.tests.util import create_dbaas_client
 from trove.tests.util import test_config
 from trove.tests.util.users import Requirements
 
+GROUP = "dbaas.api.versions"
 
-@test(groups=[tests.DBAAS_API_VERSIONS])
+
+@test(groups=[tests.DBAAS_API, GROUP, tests.PRE_INSTANCES, 'DBAAS_VERSIONS'],
+      depends_on_groups=["services.initialize"])
 class Versions(object):
     """Test listing all versions and verify the current version."""
 
     @before_class
     def setUp(self):
         """Sets up the client."""
         user = test_config.users.find_user(Requirements(is_admin=False))
         self.client = create_dbaas_client(user)
 
     @test
     def test_list_versions_index(self):
-        """test_list_versions_index"""
         versions = self.client.versions.index(test_config.version_url)
         assert_equal(1, len(versions))
         assert_equal("CURRENT", versions[0].status,
                      message="Version status: %s" % versions[0].status)
         expected_version = test_config.values['trove_version']
         assert_equal(expected_version, versions[0].id,
                      message="Version ID: %s" % versions[0].id)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/config.py` & `trove-8.0.1/trove/tests/config.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 The tests are capable of running in other contexts, such as in a VM or against
 a real deployment. Using this configuration ensures we can run them in other
 environments if we choose to.
 
 """
 
-from collections.abc import Mapping
+from collections import Mapping
 from datetime import datetime
 import json
 import os
 
 
 # TODO(tim.simpson): I feel like this class already exists somewhere in core
 #                    Python.
@@ -74,14 +74,15 @@
             'swift_url': "http://localhost:8080/v1/AUTH_",
             'dbaas_datastore': "mysql",
             'dbaas_datastore_id': "a00000a0-00a0-0a00-00a0-000a000000aa",
             'dbaas_datastore_name_no_versions': "Test_Datastore_1",
             'dbaas_datastore_version': "5.5",
             'dbaas_datastore_version_id': "b00000b0-00b0-0b00-00b0-"
                                           "000b000000bb",
+            'dbaas_inactive_datastore_version': "mysql_inactive_version",
             'instance_create_time': 16 * 60,
             'mysql_connection_method': {"type": "direct"},
             'typical_nova_image_name': None,
             'white_box': os.environ.get("WHITE_BOX", "False") == "True",
             'test_mgmt': False,
             'use_local_ovz': False,
             "known_bugs": {},
@@ -94,14 +95,15 @@
             "usage_endpoint": USAGE_ENDPOINT,
             "root_on_create": False,
             "mysql": {
                 "configurations": {
                     "valid_values": {
                         "connect_timeout": 120,
                         "local_infile": 0,
+                        "collation_server": "latin1_swedish_ci"
                     },
                     "appending_values": {
                         "join_buffer_size": 1048576,
                         "connect_timeout": 15
                     },
                     "nondynamic_parameter": {
                         "join_buffer_size": 1048576,
@@ -117,17 +119,14 @@
                         "key_buffer_size",
                         "connect_timeout"
                     ]
                 },
                 "volume_support": True,
             },
             "redis": {"volume_support": False},
-            "swift_enabled": True,
-            "trove_mgmt_network": "trove-mgmt",
-            "running_status": ["ACTIVE", "HEALTHY"],
         }
         self._frozen_values = FrozenDict(self._values)
         self._users = None
 
     def get(self, name, default_value):
         return self.values.get(name, default_value)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/db/migrations.py` & `trove-8.0.1/trove/tests/db/migrations.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/__init__.py` & `trove-8.0.1/trove/tests/fakes/__init__.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/common.py` & `trove-8.0.1/trove/tests/fakes/common.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/conf.py` & `trove-8.0.1/trove/tests/fakes/conf.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/dns.py` & `trove-8.0.1/trove/tests/fakes/dns.py`

 * *Files 6% similar despite different names*

```diff
@@ -74,12 +74,13 @@
         entry = ENTRIES[hostname]
         # See if the ip address assigned to the record is what we expect.
         # This isn't perfect, but for Fake Mode its good enough. If we
         # really want to know exactly what it should be then we should restore
         # the ability to return the IP from the API as well as a hostname,
         # since that lines up to the DnsEntry's content field.
         ip_addresses = mgmt_instance.server['addresses']
-        for address in ip_addresses:
-            if entry.content == address['address']:
-                return
+        for network_name, ip_list in ip_addresses.items():
+            for ip in ip_list:
+                if entry.content == ip['addr']:
+                    return
         fail("Couldn't find IP address %s among these values: %s"
              % (entry.content, ip_addresses))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/guestagent.py` & `trove-8.0.1/trove/tests/fakes/guestagent.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 import re
 import time
 
 import eventlet
 from oslo_log import log as logging
 
 from trove.common import exception as rd_exception
-from trove.instance import service_status as srvstatus
+from trove.common import instance as rd_instance
 from trove.tests.util import unquote_user_host
 
 DB = {}
 LOG = logging.getLogger(__name__)
 BACKUP_SIZE = 0.14
 
 
@@ -62,15 +62,15 @@
         }
 
     def update_guest(self):
         LOG.debug("Updating guest %s", self.id)
         self.version += 1
 
     def _check_username(self, username):
-        unsupported_chars = re.compile(r"""^\s|\s$|'|"|;|`|,|/|\\""")
+        unsupported_chars = re.compile("^\s|\s$|'|\"|;|`|,|/|\\\\")
         if (not username or
                 unsupported_chars.search(username) or
                 ("%r" % username).find("\\") != -1):
             raise ValueError("'%s' is not a valid user name." % username)
         if len(username) > 16:
             raise ValueError("User name '%s' is too long. Max length = 16" %
                              username)
@@ -88,17 +88,17 @@
                     % (username, hostname))
             self.users[(username, hostname)]['password'] = password
 
     def update_attributes(self, username, hostname, user_attrs):
         LOG.debug("Updating attributes")
         self._check_username(username)
         if (username, hostname) not in self.users:
-            raise rd_exception.UserNotFound(
-                "User %s@%s cannot be found on the instance."
-                % (username, hostname))
+                raise rd_exception.UserNotFound(
+                    "User %s@%s cannot be found on the instance."
+                    % (username, hostname))
         new_name = user_attrs.get('name')
         new_host = user_attrs.get('host')
         new_password = user_attrs.get('password')
         old_name = username
         old_host = hostname
         name = new_name or old_name
         host = new_host or old_host
@@ -232,48 +232,48 @@
         self.create_user(users)
         self.create_database(databases)
         self.overrides = overrides or {}
 
         def update_db():
             status = InstanceServiceStatus.find_by(instance_id=self.id)
             if instance_name.endswith('GUEST_ERROR'):
-                status.status = srvstatus.ServiceStatuses.FAILED
+                status.status = rd_instance.ServiceStatuses.FAILED
             else:
-                status.status = srvstatus.ServiceStatuses.HEALTHY
+                status.status = rd_instance.ServiceStatuses.RUNNING
             status.save()
             AgentHeartBeat.create(instance_id=self.id)
         eventlet.spawn_after(3.5, update_db)
 
-    def _set_task_status(self, new_status='HEALTHY'):
+    def _set_task_status(self, new_status='RUNNING'):
         from trove.instance.models import InstanceServiceStatus
         print("Setting status to %s" % new_status)
-        states = {'HEALTHY': srvstatus.ServiceStatuses.HEALTHY,
-                  'SHUTDOWN': srvstatus.ServiceStatuses.SHUTDOWN,
+        states = {'RUNNING': rd_instance.ServiceStatuses.RUNNING,
+                  'SHUTDOWN': rd_instance.ServiceStatuses.SHUTDOWN,
                   }
         status = InstanceServiceStatus.find_by(instance_id=self.id)
         status.status = states[new_status]
         status.save()
 
     def restart(self):
         # All this does is restart, and shut off the status updates while it
         # does so. So there's actually nothing to do to fake this out except
         # take a nap.
         print("Sleeping for a second.")
         time.sleep(1)
-        self._set_task_status('HEALTHY')
+        self._set_task_status('RUNNING')
 
     def reset_configuration(self, config):
         # There's nothing to do here, since there is no config to update.
         pass
 
-    def start_db_with_conf_changes(self, config_contents, ds_version):
+    def start_db_with_conf_changes(self, config_contents):
         time.sleep(2)
-        self._set_task_status('HEALTHY')
+        self._set_task_status('RUNNING')
 
-    def stop_db(self):
+    def stop_db(self, do_not_start_on_reboot=False):
         self._set_task_status('SHUTDOWN')
 
     def get_volume_info(self):
         """Return used and total volume filesystem information in GB."""
         return {'used': 0.16, 'total': 4.0}
 
     def grant_access(self, username, hostname, databases):
@@ -326,15 +326,15 @@
 
     def mount_volume(self, device_path=None, mount_point=None):
         pass
 
     def unmount_volume(self, device_path=None, mount_point=None):
         pass
 
-    def resize_fs(self, device_path=None, mount_point=None, online=False):
+    def resize_fs(self, device_path=None, mount_point=None):
         pass
 
     def update_overrides(self, overrides, remove=False):
         self.overrides = overrides
 
     def apply_overrides(self, overrides):
         self.overrides = overrides
@@ -376,9 +376,9 @@
 
 def get_or_create(id):
     if id not in DB:
         DB[id] = FakeGuest(id)
     return DB[id]
 
 
-def fake_create_guest_client(context, id, manager=None):
+def fake_create_guest_client(context, id):
     return get_or_create(id)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/keystone.py` & `trove-8.0.1/trove/tests/fakes/keystone.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/limits.py` & `trove-8.0.1/integration/tests/integration/tests/volumes/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,26 +1,25 @@
-# Copyright 2014 Rackspace Hosting
+# Copyright (c) 2011 OpenStack, LLC.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-#
-
-from trove.common import limits
-
-
-ENABLED = False
-
-
-class FakeRateLimitingMiddleware(limits.RateLimitingMiddleware):
 
-    def enabled(self):
-        return ENABLED
+"""
+:mod:`volumes` -- Tests for volumes.
+===================================
+"""
+
+""""Tests for Volumes."""
+
+# Is a set of tests written directly against the VolumeManager and VolumeClient
+# classes which doesn't require standing up Nova daemons or anything.
+VOLUMES_DRIVER = "trove.volumes.driver"
```

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/nova.py` & `trove-8.0.1/trove/tests/fakes/nova.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,26 +9,27 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-import collections
-import uuid
-
-import eventlet
-
 from novaclient import exceptions as nova_exceptions
 from oslo_log import log as logging
 
 from trove.common.exception import PollTimeOut
-from trove.instance import service_status as srvstatus
+from trove.common.i18n import _
+from trove.common import instance as rd_instance
 from trove.tests.fakes.common import authorize
 
+import collections
+import eventlet
+import uuid
+
+
 LOG = logging.getLogger(__name__)
 FAKE_HOSTS = ["fake_host_1", "fake_host_2"]
 
 
 class FakeFlavor(object):
 
     def __init__(self, id, disk, name, ram, ephemeral=0, vcpus=10):
@@ -97,15 +98,15 @@
 
 
 class FakeServer(object):
 
     next_local_id = 0
 
     def __init__(self, parent, owner, id, name, image_id, flavor_ref,
-                 volumes, key_name):
+                 block_device_mapping, volumes):
         self.owner = owner  # This is a context.
         self.id = id
         self.parent = parent
         self.name = name
         self.image_id = image_id
         self.flavor_ref = flavor_ref
         self.old_flavor_ref = None
@@ -121,42 +122,38 @@
             volume.set_attachment(id)
             volume.schedule_status("in-use", 1)
         self.host = FAKE_HOSTS[0]
         self.old_host = None
         setattr(self, 'OS-EXT-AZ:availability_zone', 'nova')
 
         self._info = {'os:volumes': info_vols}
-        self.key_name = key_name
 
     @property
     def addresses(self):
-        return [{
-            "address": "123.123.123.123",
-            'type': 'private',
-            'network': 'net-id'}]
+        return {"private": [{"addr": "123.123.123.123"}]}
 
     def confirm_resize(self):
         if self.status != "VERIFY_RESIZE":
             raise RuntimeError("Not in resize confirm mode.")
-        self._current_status = "HEALTHY"
+        self._current_status = "ACTIVE"
 
     def revert_resize(self):
         if self.status != "VERIFY_RESIZE":
             raise RuntimeError("Not in resize confirm mode.")
         self.host = self.old_host
         self.old_host = None
         self.flavor_ref = self.old_flavor_ref
         self.old_flavor_ref = None
-        self._current_status = "HEALTHY"
+        self._current_status = "ACTIVE"
 
     def reboot(self):
         LOG.debug("Rebooting server %s", self.id)
 
         def set_to_active():
-            self._current_status = "HEALTHY"
+            self._current_status = "ACTIVE"
             self.parent.schedule_simulate_running_server(self.id, 1.5)
 
         self._current_status = "REBOOT"
         eventlet.spawn_after(1, set_to_active)
 
     def delete(self):
         self.schedule_status = []
@@ -203,15 +200,15 @@
                 self.host = [host for host in FAKE_HOSTS
                              if host != self.host][0]
             else:
                 self.host = force_host
 
         def set_flavor():
             if self.name.endswith("_RESIZE_ERROR"):
-                self._current_status = "HEALTHY"
+                self._current_status = "ACTIVE"
                 return
             if new_flavor_id is None:
                 # Migrations are flavorless flavor resizes.
                 # A resize MIGHT change the host, but a migrate
                 # deliberately does.
                 LOG.debug("Migrating fake instance.")
                 eventlet.spawn_after(0.75, change_host)
@@ -263,77 +260,112 @@
         self.db = FAKE_SERVERS_DB
         self.flavors = flavors
 
     def can_see(self, id):
         """Can this FakeServers, with its context, see some resource?"""
         server = self.db[id]
         return (self.context.is_admin or
-                server.owner.tenant == self.context.project_id)
+                server.owner.tenant == self.context.tenant)
 
     def create(self, name, image_id, flavor_ref, files=None, userdata=None,
-               block_device_mapping_v2=None, security_groups=None,
+               block_device_mapping=None, volume=None, security_groups=None,
                availability_zone=None, nics=None, config_drive=False,
-               scheduler_hints=None, key_name=None):
+               scheduler_hints=None):
         id = "FAKE_%s" % uuid.uuid4()
-        volumes = self._get_volumes_from_bdm_v2(block_device_mapping_v2)
+        if volume:
+            volume = self.volumes.create(volume['size'], volume['name'],
+                                         volume['description'])
+            while volume.status == "BUILD":
+                eventlet.sleep(0.1)
+            if volume.status != "available":
+                LOG.info(_("volume status = %s"), volume.status)
+                raise nova_exceptions.ClientException("Volume was bad!")
+            mapping = "%s::%s:%s" % (volume.id, volume.size, 1)
+            block_device_mapping = {'vdb': mapping}
+            volumes = [volume]
+            LOG.debug("Fake Volume Create %(volumeid)s with "
+                      "status %(volumestatus)s",
+                      {'volumeid': volume.id, 'volumestatus': volume.status})
+        else:
+            volumes = self._get_volumes_from_bdm(block_device_mapping)
+            for volume in volumes:
+                volume.schedule_status('in-use', 1)
         server = FakeServer(self, self.context, id, name, image_id, flavor_ref,
-                            volumes, key_name)
+                            block_device_mapping, volumes)
         self.db[id] = server
         if name.endswith('SERVER_ERROR'):
             raise nova_exceptions.ClientException("Fake server create error.")
 
         if availability_zone == 'BAD_ZONE':
             raise nova_exceptions.ClientException("The requested availability "
                                                   "zone is not available.")
 
-        server.schedule_status("HEALTHY", 1)
+        if nics:
+            if 'port-id' in nics[0] and nics[0]['port-id'] == "UNKNOWN":
+                raise nova_exceptions.ClientException("The requested "
+                                                      "port-id is not "
+                                                      "available.")
+
+        server.schedule_status("ACTIVE", 1)
         LOG.info("FAKE_SERVERS_DB : %s", str(FAKE_SERVERS_DB))
         return server
 
-    def _get_volumes_from_bdm_v2(self, block_device_mapping_v2):
+    def _get_volumes_from_bdm(self, block_device_mapping):
         volumes = []
-        if block_device_mapping_v2 is not None:
-            # block_device_mapping_v2 is an array of dicts. Every dict is
-            # a volume attachment, which contains "uuid", "source_type",
-            # "destination_type", "device_name", "volume_size" and
-            # "delete_on_termination".
-            for bdm in block_device_mapping_v2:
-                volume = self.volumes.get(bdm['uuid'])
+        if block_device_mapping is not None:
+            # block_device_mapping is a dictionary, where the key is the
+            # device name on the compute instance and the mapping info is a
+            # set of fields in a string, separated by colons.
+            # For each device, find the volume, and record the mapping info
+            # to another fake object and attach it to the volume
+            # so that the fake API can later retrieve this.
+            for device in block_device_mapping:
+                mapping = block_device_mapping[device]
+                (id, _type, size, delete_on_terminate) = mapping.split(":")
+                volume = self.volumes.get(id)
+                volume.mapping = FakeBlockDeviceMappingInfo(
+                    id, device, _type, size, delete_on_terminate)
                 volumes.append(volume)
         return volumes
 
     def get(self, id):
         if id not in self.db:
-            LOG.error("Couldn't find server id %(id)s, collection=%(db)s",
+            LOG.error(_("Couldn't find server id %(id)s, collection=%(db)s"),
                       {'id': id, 'db': self.db})
             raise nova_exceptions.NotFound(404, "Not found")
         else:
             if self.can_see(id):
                 return self.db[id]
             else:
                 raise nova_exceptions.NotFound(404, "Bad permissions")
 
+    def get_server_volumes(self, server_id):
+        """Fake method we've added to grab servers from the volume."""
+        return [volume.mapping
+                for volume in self.get(server_id).volumes
+                if volume.mapping is not None]
+
     def list(self):
         return [v for (k, v) in self.db.items() if self.can_see(v.id)]
 
     def schedule_delete(self, id, time_from_now):
         def delete_server():
-            LOG.info("Simulated event ended, deleting server %s.", id)
+            LOG.info(_("Simulated event ended, deleting server %s."), id)
             del self.db[id]
         eventlet.spawn_after(time_from_now, delete_server)
 
     def schedule_simulate_running_server(self, id, time_from_now):
         from trove.instance.models import DBInstance
         from trove.instance.models import InstanceServiceStatus
 
         def set_server_running():
             instance = DBInstance.find_by(compute_instance_id=id)
             LOG.debug("Setting server %s to running", instance.id)
             status = InstanceServiceStatus.find_by(instance_id=instance.id)
-            status.status = srvstatus.ServiceStatuses.RUNNING
+            status.status = rd_instance.ServiceStatuses.RUNNING
             status.save()
         eventlet.spawn_after(time_from_now, set_server_running)
 
 
 class FakeRdServer(object):
 
     def __init__(self, server):
@@ -355,40 +387,62 @@
         return FakeRdServer(self.servers.get(id))
 
     def list(self):
         # Attach the extra Rd Server stuff to the normal server.
         return [FakeRdServer(server) for server in self.servers.list()]
 
 
+class FakeServerVolumes(object):
+
+    def __init__(self, context):
+        self.context = context
+
+    def get_server_volumes(self, server_id):
+        class ServerVolumes(object):
+            def __init__(self, block_device_mapping):
+                LOG.debug("block_device_mapping = %s", block_device_mapping)
+                device = block_device_mapping['vdb']
+                (self.volumeId,
+                    self.type,
+                    self.size,
+                    self.delete_on_terminate) = device.split(":")
+        fake_servers = FakeServers(self.context, FLAVORS)
+        server = fake_servers.get(server_id)
+        return [ServerVolumes(server.block_device_mapping)]
+
+
 class FakeVolume(object):
 
     def __init__(self, parent, owner, id, size, name,
-                 description, volume_type, availability_zone):
+                 description, volume_type):
         self.attachments = []
         self.parent = parent
         self.owner = owner  # This is a context.
         self.id = id
         self.size = size
         self.name = name
         self.description = description
         self._current_status = "BUILD"
         # For some reason we grab this thing from device then call it mount
         # point.
         self.device = "vdb"
         self.volume_type = volume_type
-        self.availability_zone = availability_zone
 
     def __repr__(self):
         msg = ("FakeVolume(id=%s, size=%s, name=%s, "
                "description=%s, _current_status=%s)")
         params = (self.id, self.size, self.name,
                   self.description, self._current_status)
         return (msg % params)
 
     @property
+    def availability_zone(self):
+        return "fake-availability-zone"
+
+    @property
     def created_at(self):
         return "2001-01-01-12:30:30"
 
     def get(self, key):
         return getattr(self, key)
 
     def schedule_status(self, new_status, time_from_now):
@@ -429,32 +483,31 @@
         self.context = context
         self.db = FAKE_VOLUMES_DB
 
     def can_see(self, id):
         """Can this FakeVolumes, with its context, see some resource?"""
         server = self.db[id]
         return (self.context.is_admin or
-                server.owner.tenant == self.context.project_id)
+                server.owner.tenant == self.context.tenant)
 
     def get(self, id):
         if id not in self.db:
-            LOG.error("Couldn't find volume id %(id)s, collection=%(db)s",
+            LOG.error(_("Couldn't find volume id %(id)s, collection=%(db)s"),
                       {'id': id, 'db': self.db})
             raise nova_exceptions.NotFound(404, "Not found")
         else:
             if self.can_see(id):
                 return self.db[id]
             else:
                 raise nova_exceptions.NotFound(404, "Bad permissions")
 
-    def create(self, size, name=None, description=None, volume_type=None,
-               availability_zone=None):
+    def create(self, size, name=None, description=None, volume_type=None):
         id = "FAKE_VOL_%s" % uuid.uuid4()
         volume = FakeVolume(self, self.context, id, size, name,
-                            description, volume_type, availability_zone)
+                            description, volume_type)
         self.db[id] = volume
         if size == 9:
             volume.schedule_status("error", 2)
         elif size == 13:
             raise Exception("No volume for you!")
         else:
             volume.schedule_status("available", 2)
@@ -576,15 +629,15 @@
                 continue
             self.instances.append({
                 'uuid': server.id,
                 'name': server.name,
                 'status': server.status
             })
             if (str(server.flavor_ref).startswith('http:') or
-                    str(server.flavor_ref).startswith('https:')):
+               str(server.flavor_ref).startswith('https:')):
                 flavor = FLAVORS.get_by_href(server.flavor_ref)
             else:
                 flavor = FLAVORS.get(server.flavor_ref)
             ram = flavor.ram
             self.usedRAM += ram
         decimal = float(self.usedRAM) / float(self.totalRAM)
         self.percentUsed = int(decimal * 100)
@@ -739,15 +792,15 @@
     def delete(self, id):
         if id in self.securityGroupRules:
             del self.securityGroupRules[id]
 
 
 class FakeServerGroup(object):
 
-    def __init__(self, name=None, policies=None, description=None):
+    def __init__(self, name=None, policies=None, context=None):
         self.name = name
         self.description = description
         self.id = "FAKE_SRVGRP_%s" % uuid.uuid4()
         self.policies = policies or {}
 
     def get_id(self):
         return self.id
@@ -790,14 +843,17 @@
         self.rdhosts = FakeHosts(self.servers)
         self.rdstorage = FakeRdStorages()
         self.rdservers = FakeRdServers(self.servers)
         self.security_groups = FakeSecurityGroups(context)
         self.security_group_rules = FakeSecurityGroupRules(context)
         self.server_groups = FakeServerGroups(context)
 
+    def get_server_volumes(self, server_id):
+        return self.servers.get_server_volumes(server_id)
+
     def rescan_server_volume(self, server, volume_id):
         LOG.info("FAKE rescanning volume.")
 
 
 CLIENT_DATA = {}
```

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/swift.py` & `trove-8.0.1/trove/tests/fakes/swift.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,37 +10,38 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from hashlib import md5
-import http
-from unittest.mock import MagicMock, patch
+from mock import MagicMock, patch
 import json
 import os
 import socket
 import swiftclient
 import swiftclient.client as swift_client
 import uuid
 
 from oslo_log import log as logging
+import six
+from six.moves import http_client
 from swiftclient import client as swift
 
 
 LOG = logging.getLogger(__name__)
 
 
 class FakeSwiftClient(object):
     """Logs calls instead of executing."""
     def __init__(self, *args, **kwargs):
         pass
 
     @classmethod
-    def Connection(cls, *args, **kargs):
+    def Connection(self, *args, **kargs):
         LOG.debug("fake FakeSwiftClient Connection")
         return FakeSwiftConnection()
 
 
 class FakeSwiftConnection(object):
     """Logging calls instead of executing."""
     MANIFEST_QUERY_STRING_PUT = 'multipart-manifest=put'
@@ -51,22 +52,22 @@
     def __init__(self, *args, **kwargs):
         self.manifest_prefix = None
         self.manifest_name = None
         self.container_objects = {}
 
     def get_auth(self):
         return (
-            "http://127.0.0.1:8080/v1/AUTH_c7b038976df24d96bf1980f5da17bd89",
-            'MIINrwYJKoZIhvcNAQcCoIINoDCCDZwCAQExCTAHBgUrDgMCGjCCDIgGCSqGSIb3'
-            'DQEHAaCCDHkEggx1eyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAi'
-            'MjAxMy0wMy0xOFQxODoxMzoyMC41OTMyNzYiLCAiZXhwaXJlcyI6ICIyMDEzLTAz'
-            'LTE5VDE4OjEzOjIwWiIsICJpZCI6ICJwbGFjZWhvbGRlciIsICJ0ZW5hbnQiOiB7'
-            'ImVuYWJsZWQiOiB0cnVlLCAiZGVzY3JpcHRpb24iOiBudWxsLCAibmFtZSI6ICJy'
-            'ZWRkd2FyZiIsICJpZCI6ICJjN2IwMzg5NzZkZjI0ZDk2YmYxOTgwZjVkYTE3YmQ4'
-            'OSJ9fSwgInNlcnZpY2VDYXRhbG9nIjogW3siZW5kcG9pbnRzIjogW3siYWRtaW5')
+            u"http://127.0.0.1:8080/v1/AUTH_c7b038976df24d96bf1980f5da17bd89",
+            u'MIINrwYJKoZIhvcNAQcCoIINoDCCDZwCAQExCTAHBgUrDgMCGjCCDIgGCSqGSIb3'
+            u'DQEHAaCCDHkEggx1eyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAi'
+            u'MjAxMy0wMy0xOFQxODoxMzoyMC41OTMyNzYiLCAiZXhwaXJlcyI6ICIyMDEzLTAz'
+            u'LTE5VDE4OjEzOjIwWiIsICJpZCI6ICJwbGFjZWhvbGRlciIsICJ0ZW5hbnQiOiB7'
+            u'ImVuYWJsZWQiOiB0cnVlLCAiZGVzY3JpcHRpb24iOiBudWxsLCAibmFtZSI6ICJy'
+            u'ZWRkd2FyZiIsICJpZCI6ICJjN2IwMzg5NzZkZjI0ZDk2YmYxOTgwZjVkYTE3YmQ4'
+            u'OSJ9fSwgInNlcnZpY2VDYXRhbG9nIjogW3siZW5kcG9pbnRzIjogW3siYWRtaW5')
 
     def get_account(self):
         return ({'content-length': '2', 'accept-ranges': 'bytes',
                  'x-timestamp': '1363049003.92304',
                  'x-trans-id': 'tx9e5da02c49ed496395008309c8032a53',
                  'date': 'Tue, 10 Mar 2013 00:43:23 GMT',
                  'x-account-bytes-used': '0',
@@ -74,20 +75,18 @@
                  'content-type': 'application/json; charset=utf-8',
                  'x-account-object-count': '0'}, [])
 
     def head_container(self, container):
         LOG.debug("fake head_container(%s)", container)
         if container == 'missing_container':
             raise swift.ClientException('fake exception',
-                                        http_status=http.HTTPStatus.NOT_FOUND)
+                                        http_status=http_client.NOT_FOUND)
         elif container == 'unauthorized_container':
-            raise swift.ClientException(
-                'fake exception',
-                http_status=http.HTTPStatus.UNAUTHORIZED
-            )
+            raise swift.ClientException('fake exception',
+                                        http_status=http_client.UNAUTHORIZED)
         elif container == 'socket_error_on_head':
             raise socket.error(111, 'ECONNREFUSED')
         pass
 
     def put_container(self, container):
         LOG.debug("fake put_container(%s)", container)
         pass
@@ -105,15 +104,18 @@
                   {'container': container, 'name': name})
         checksum = md5()
         if self.manifest_name == name:
             for object_name in sorted(self.container_objects):
                 object_checksum = md5(self.container_objects[object_name])
                 # The manifest file etag for a HEAD or GET is the checksum of
                 # the concatenated checksums.
-                checksum.update(object_checksum.hexdigest().encode())
+                if six.PY3:
+                    checksum.update(object_checksum.hexdigest().encode())
+                else:
+                    checksum.update(object_checksum.hexdigest())
             # this is included to test bad swift segment etags
             if name.startswith("bad_manifest_etag_"):
                 return {'etag': '"this_is_an_intentional_bad_manifest_etag"'}
         else:
             if name in self.container_objects:
                 checksum.update(self.container_objects[name])
             else:
@@ -169,15 +171,15 @@
         query_string = kwargs.get('query_string', '')
         object_checksum = md5()
         if query_string == self.MANIFEST_QUERY_STRING_PUT:
             # the manifest prefix format is <container>/<prefix> where
             # container is where the object segments are in and prefix is the
             # common prefix for all segments.
             self.manifest_name = name
-            if isinstance(contents, str):
+            if isinstance(contents, six.text_type):
                 object_checksum.update(contents.encode('utf-8'))
             else:
                 object_checksum.update(contents)
         elif self.COPY_OBJECT_HEADER_KEY in headers:
             # this is a copy object operation
             source_path = headers.get(self.COPY_OBJECT_HEADER_KEY)
             source_name = source_path.split('/')[1]
@@ -333,22 +335,22 @@
                      'date': 'Tue, 10 Mar 2013 00:43:23 GMT',
                      'x-account-bytes-used': '0',
                      'x-account-container-count': '0',
                      'content-type': 'application/json; charset=utf-8',
                      'x-account-object-count': '0'}, self._containers_list)
 
         get_auth_return_value = (
-            "http://127.0.0.1:8080/v1/AUTH_c7b038976df24d96bf1980f5da17bd89",
-            'MIINrwYJKoZIhvcNAQcCoIINoDCCDZwCAQExCTAHBgUrDgMCGjCCDIgGCSqGSIb3'
-            'DQEHAaCCDHkEggx1eyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAi'
-            'MjAxMy0wMy0xOFQxODoxMzoyMC41OTMyNzYiLCAiZXhwaXJlcyI6ICIyMDEzLTAz'
-            'LTE5VDE4OjEzOjIwWiIsICJpZCI6ICJwbGFjZWhvbGRlciIsICJ0ZW5hbnQiOiB7'
-            'ImVuYWJsZWQiOiB0cnVlLCAiZGVzY3JpcHRpb24iOiBudWxsLCAibmFtZSI6ICJy'
-            'ZWRkd2FyZiIsICJpZCI6ICJjN2IwMzg5NzZkZjI0ZDk2YmYxOTgwZjVkYTE3YmQ4'
-            'OSJ9fSwgInNlcnZpY2VDYXRhbG9nIjogW3siZW5kcG9pbnRzIjogW3siYWRtaW5')
+            u"http://127.0.0.1:8080/v1/AUTH_c7b038976df24d96bf1980f5da17bd89",
+            u'MIINrwYJKoZIhvcNAQcCoIINoDCCDZwCAQExCTAHBgUrDgMCGjCCDIgGCSqGSIb3'
+            u'DQEHAaCCDHkEggx1eyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAi'
+            u'MjAxMy0wMy0xOFQxODoxMzoyMC41OTMyNzYiLCAiZXhwaXJlcyI6ICIyMDEzLTAz'
+            u'LTE5VDE4OjEzOjIwWiIsICJpZCI6ICJwbGFjZWhvbGRlciIsICJ0ZW5hbnQiOiB7'
+            u'ImVuYWJsZWQiOiB0cnVlLCAiZGVzY3JpcHRpb24iOiBudWxsLCAibmFtZSI6ICJy'
+            u'ZWRkd2FyZiIsICJpZCI6ICJjN2IwMzg5NzZkZjI0ZDk2YmYxOTgwZjVkYTE3YmQ4'
+            u'OSJ9fSwgInNlcnZpY2VDYXRhbG9nIjogW3siZW5kcG9pbnRzIjogW3siYWRtaW5')
 
         get_auth_patcher = patch.object(
             swift_client.Connection, 'get_auth',
             MagicMock(return_value=get_auth_return_value))
         self._start_patcher(get_auth_patcher)
 
         get_account_patcher = patch.object(
```

### Comparing `trove-21.0.0.0rc2/trove/tests/fakes/taskmanager.py` & `trove-8.0.1/trove/tests/fakes/taskmanager.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/int_tests.py` & `trove-8.0.1/trove/tests/int_tests.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,34 +10,55 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import proboscis
-
-from trove import tests
+from trove.tests.api import backups
+from trove.tests.api import configurations
+from trove.tests.api import databases
+from trove.tests.api import datastores
+from trove.tests.api import flavors
+from trove.tests.api import instances
+from trove.tests.api import instances_actions
+from trove.tests.api.mgmt import accounts
+from trove.tests.api.mgmt import admin_required
+from trove.tests.api.mgmt import datastore_versions
+from trove.tests.api.mgmt import hosts
+from trove.tests.api.mgmt import instances as mgmt_instances
+from trove.tests.api.mgmt import storage
+from trove.tests.api import replication
+from trove.tests.api import root
+from trove.tests.api import user_access
+from trove.tests.api import users
+from trove.tests.api import versions
 from trove.tests.scenario import groups
 from trove.tests.scenario.groups import backup_group
 from trove.tests.scenario.groups import cluster_group
 from trove.tests.scenario.groups import configuration_group
 from trove.tests.scenario.groups import database_actions_group
 from trove.tests.scenario.groups import guest_log_group
 from trove.tests.scenario.groups import instance_actions_group
 from trove.tests.scenario.groups import instance_create_group
 from trove.tests.scenario.groups import instance_delete_group
 from trove.tests.scenario.groups import instance_error_create_group
 from trove.tests.scenario.groups import instance_force_delete_group
 from trove.tests.scenario.groups import instance_upgrade_group
 from trove.tests.scenario.groups import module_group
+from trove.tests.scenario.groups import negative_cluster_actions_group
 from trove.tests.scenario.groups import replication_group
 from trove.tests.scenario.groups import root_actions_group
 from trove.tests.scenario.groups import user_actions_group
 
 
+GROUP_SERVICES_INITIALIZE = "services.initialize"
+GROUP_SETUP = 'dbaas.setup'
+
+
 def build_group(*groups):
     def merge(collection, *items):
         for item in items:
             if isinstance(item, list):
                 merge(collection, *item)
             else:
                 if item not in collection:
@@ -56,31 +77,89 @@
             register([name + '_' + suffix for name in group_names], *grp_set)
         return
 
     # Do the actual registration here
     proboscis.register(groups=build_group(group_names),
                        depends_on_groups=build_group(*test_groups))
     # Now register the same groups with '-' instead of '_'
-    proboscis.register(
-        groups=build_group([name.replace('_', '-') for name in group_names]),
-        depends_on_groups=build_group(*test_groups))
+    proboscis.register(groups=build_group(
+                       [name.replace('_', '-') for name in group_names]),
+                       depends_on_groups=build_group(*test_groups))
+
+black_box_groups = [
+    flavors.GROUP,
+    users.GROUP,
+    user_access.GROUP,
+    databases.GROUP,
+    root.GROUP,
+    GROUP_SERVICES_INITIALIZE,
+    instances.GROUP_START,
+    instances.GROUP_QUOTAS,
+    instances.GROUP_SECURITY_GROUPS,
+    backups.GROUP,
+    replication.GROUP,
+    configurations.GROUP,
+    datastores.GROUP,
+    instances_actions.GROUP_RESIZE,
+    # TODO(SlickNik): The restart tests fail intermittently so pulling
+    # them out of the blackbox group temporarily. Refer to Trove bug:
+    # https://bugs.launchpad.net/trove/+bug/1204233
+    # instances_actions.GROUP_RESTART,
+    instances_actions.GROUP_STOP_MYSQL,
+    instances.GROUP_STOP,
+    versions.GROUP,
+    instances.GROUP_GUEST,
+    datastore_versions.GROUP,
+]
+proboscis.register(groups=["blackbox", "mysql"],
+                   depends_on_groups=black_box_groups)
 
+simple_black_box_groups = [
+    GROUP_SERVICES_INITIALIZE,
+    flavors.GROUP,
+    versions.GROUP,
+    instances.GROUP_START_SIMPLE,
+    admin_required.GROUP,
+    datastore_versions.GROUP,
+]
+proboscis.register(groups=["simple_blackbox"],
+                   depends_on_groups=simple_black_box_groups)
+
+black_box_mgmt_groups = [
+    accounts.GROUP,
+    hosts.GROUP,
+    storage.GROUP,
+    instances_actions.GROUP_REBOOT,
+    admin_required.GROUP,
+    mgmt_instances.GROUP,
+    datastore_versions.GROUP,
+]
+proboscis.register(groups=["blackbox_mgmt"],
+                   depends_on_groups=black_box_mgmt_groups)
 
+#
+# Group designations for datastore agnostic int-tests
+#
 # Base groups for all other groups
 base_groups = [
-    tests.DBAAS_API_VERSIONS,
+    GROUP_SERVICES_INITIALIZE,
+    flavors.GROUP,
+    versions.GROUP,
+    GROUP_SETUP
 ]
 
 # Cluster-based groups
 cluster_create_groups = list(base_groups)
 cluster_create_groups.extend([groups.CLUSTER_DELETE_WAIT])
 
 cluster_actions_groups = list(cluster_create_groups)
 cluster_actions_groups.extend([groups.CLUSTER_ACTIONS_SHRINK_WAIT])
 
+cluster_negative_actions_groups = list(negative_cluster_actions_group.GROUP)
+
 cluster_root_groups = list(cluster_create_groups)
 cluster_root_groups.extend([groups.CLUSTER_ACTIONS_ROOT_ENABLE])
 
 cluster_root_actions_groups = list(cluster_actions_groups)
 cluster_root_actions_groups.extend([groups.CLUSTER_ACTIONS_ROOT_ACTIONS])
 
 cluster_restart_groups = list(cluster_create_groups)
@@ -163,72 +242,52 @@
 root_actions_groups.extend([root_actions_group.GROUP])
 
 user_actions_groups = list(instance_create_groups)
 user_actions_groups.extend([user_actions_group.GROUP])
 
 # groups common to all datastores
 common_groups = list(instance_create_groups)
-# NOTE(lxkong): Remove the module related tests(module_groups) for now because
-# of no use case.
-common_groups.extend([guest_log_groups, instance_init_groups])
-
-integration_groups = [
-    tests.DBAAS_API_VERSIONS,
-    tests.DBAAS_API_DATASTORES,
-    tests.DBAAS_API_MGMT_DATASTORES,
-    tests.DBAAS_API_INSTANCES,
-    tests.DBAAS_API_USERS_ROOT,
-    tests.DBAAS_API_USERS,
-    tests.DBAAS_API_USERS_ACCESS,
-    tests.DBAAS_API_DATABASES,
-    tests.DBAAS_API_INSTANCE_ACTIONS,
-    tests.DBAAS_API_BACKUPS,
-    tests.DBAAS_API_CONFIGURATIONS,
-    tests.DBAAS_API_REPLICATION,
-    tests.DBAAS_API_INSTANCES_DELETE
-]
-# We intentionally make the functional tests running in series and dependent
-# on each other, so that one test case failure will stop the whole testing.
-proboscis.register(groups=["mysql"],
-                   depends_on_groups=integration_groups)
-
-register(
-    ["mysql_supported"],
-    single=[instance_create_group.GROUP,
-            backup_group.GROUP,
-            configuration_group.GROUP,
-            database_actions_group.GROUP,
-            guest_log_group.GROUP,
-            instance_actions_group.GROUP,
-            instance_error_create_group.GROUP,
-            instance_force_delete_group.GROUP,
-            root_actions_group.GROUP,
-            user_actions_group.GROUP,
-            instance_delete_group.GROUP],
-    multi=[replication_group.GROUP,
-           instance_delete_group.GROUP]
-)
-
-register(
-    ["mariadb_supported"],
-    single=[instance_create_group.GROUP,
-            backup_group.GROUP,
-            configuration_group.GROUP,
-            database_actions_group.GROUP,
-            guest_log_group.GROUP,
-            instance_actions_group.GROUP,
-            instance_error_create_group.GROUP,
-            instance_force_delete_group.GROUP,
-            root_actions_group.GROUP,
-            user_actions_group.GROUP,
-            instance_delete_group.GROUP],
-    multi=[replication_group.GROUP,
-           instance_delete_group.GROUP]
-)
+common_groups.extend([guest_log_groups, instance_init_groups, module_groups])
 
+# Register: Component based groups
+register(["backup"], backup_groups)
+register(["backup_incremental"], backup_incremental_groups)
+register(["backup_negative"], backup_negative_groups)
+register(["cluster"], cluster_actions_groups)
+register(["cluster_actions"], cluster_actions_groups)
+register(["cluster_create"], cluster_create_groups)
+register(["cluster_negative_actions"], cluster_negative_actions_groups)
+register(["cluster_restart"], cluster_restart_groups)
+register(["cluster_root"], cluster_root_groups)
+register(["cluster_root_actions"], cluster_root_actions_groups)
+register(["cluster_upgrade"], cluster_upgrade_groups)
+register(["cluster_config"], cluster_config_groups)
+register(["cluster_config_actions"], cluster_config_actions_groups)
+register(["common"], common_groups)
+register(["configuration"], configuration_groups)
+register(["configuration_create"], configuration_create_groups)
+register(["database"], database_actions_groups)
+register(["guest_log"], guest_log_groups)
+register(["instance"], instance_groups)
+register(["instance_actions"], instance_actions_groups)
+register(["instance_create"], instance_create_groups)
+register(["instance_error"], instance_error_create_groups)
+register(["instance_force_delete"], instance_force_delete_groups)
+register(["instance_init"], instance_init_groups)
+register(["instance_upgrade"], instance_upgrade_groups)
+register(["module"], module_groups)
+register(["module_create"], module_create_groups)
+register(["replication"], replication_groups)
+register(["replication_promote"], replication_promote_groups)
+register(["root"], root_actions_groups)
+register(["user"], user_actions_groups)
+
+# Register: Datastore based groups
+# These should contain all functionality currently supported by the datastore.
+# Keeping them in alphabetical order may reduce the number of merge conflicts.
 register(
     ["db2_supported"],
     single=[common_groups,
             configuration_groups,
             database_actions_groups,
             user_actions_groups, ],
     multi=[]
@@ -238,14 +297,15 @@
     ["cassandra_supported"],
     single=[common_groups,
             backup_groups,
             database_actions_groups,
             configuration_groups,
             user_actions_groups, ],
     multi=[cluster_actions_groups,
+           cluster_negative_actions_groups,
            cluster_root_actions_groups,
            cluster_config_actions_groups, ]
 )
 
 register(
     ["couchbase_supported"],
     single=[common_groups,
@@ -261,25 +321,53 @@
             database_actions_groups,
             root_actions_groups,
             user_actions_groups, ],
     multi=[]
 )
 
 register(
+    ["mariadb_supported"],
+    single=[common_groups,
+            backup_incremental_groups,
+            configuration_groups,
+            database_actions_groups,
+            root_actions_groups,
+            user_actions_groups, ],
+    multi=[replication_promote_groups, ]
+    # multi=[cluster_actions_groups,
+    #        cluster_negative_actions_groups,
+    #        cluster_root_actions_groups,
+    #        replication_promote_groups, ]
+)
+
+register(
     ["mongodb_supported"],
     single=[common_groups,
             backup_groups,
             configuration_groups,
             database_actions_groups,
             root_actions_groups,
             user_actions_groups, ],
     multi=[cluster_actions_groups, ]
 )
 
 register(
+    ["mysql_supported"],
+    single=[common_groups,
+            backup_incremental_groups,
+            configuration_groups,
+            database_actions_groups,
+            instance_groups,
+            instance_upgrade_groups,
+            root_actions_groups,
+            user_actions_groups, ],
+    multi=[replication_promote_groups, ]
+)
+
+register(
     ["percona_supported"],
     single=[common_groups,
             backup_incremental_groups,
             configuration_groups,
             database_actions_groups,
             instance_upgrade_groups,
             root_actions_groups,
@@ -304,35 +392,31 @@
             backup_incremental_groups,
             configuration_groups,
             database_actions_groups,
             root_actions_groups,
             user_actions_groups, ],
     multi=[]
     # multi=[cluster_actions_groups,
+    #        cluster_negative_actions_groups,
     #        cluster_root_actions_groups, ]
 )
 
-# Redis instances does not support inherit root state from backups,
-# so a customized root actions group is created, instance backuping
-# and restoring tests will not be included.
-redis_root_actions_groups = list(instance_create_groups)
-redis_root_actions_groups.extend([groups.ROOT_ACTION_ENABLE,
-                                  groups.ROOT_ACTION_DISABLE])
 register(
     ["redis_supported"],
     single=[common_groups,
             backup_groups,
-            configuration_groups,
-            redis_root_actions_groups, ],
+            configuration_groups, ],
     multi=[replication_promote_groups, ]
     # multi=[cluster_actions_groups,
+    #        cluster_negative_actions_groups,
     #        replication_promote_groups, ]
 )
 
 register(
     ["vertica_supported"],
     single=[common_groups,
             configuration_groups,
             root_actions_groups, ],
     multi=[cluster_actions_groups,
+           cluster_negative_actions_groups,
            cluster_root_actions_groups, ]
 )
```

### Comparing `trove-21.0.0.0rc2/trove/tests/root_logger.py` & `trove-8.0.1/trove/tests/root_logger.py`

 * *Files 14% similar despite different names*

```diff
@@ -64,16 +64,14 @@
 
 
 class DefaultRootLogger(object):
     """A root logger that uses the singleton handler"""
 
     def __init__(self, enable_backtrace=False):
         super(DefaultRootLogger, self).__init__()
-        handler = DefaultRootHandler.activate(
-            enable_backtrace=enable_backtrace
-        )
+        handler = DefaultRootHandler.activate(enable_backtrace=False)
 
         handler.acquire()
         if handler not in logging.getLogger('').handlers:
             logging.getLogger('').addHandler(handler)
 
         handler.release()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/__init__.py` & `trove-8.0.1/trove/tests/scenario/groups/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -165,11 +165,7 @@
 USER_ACTION_CREATE = "scenario.user_action_create_grp"
 USER_ACTION_DELETE = "scenario.user_action_delete_grp"
 USER_ACTION_INST = "scenario.user_action_inst_grp"
 USER_ACTION_INST_CREATE = "scenario.user_action_inst_create_grp"
 USER_ACTION_INST_CREATE_WAIT = "scenario.user_action_inst_create_wait_grp"
 USER_ACTION_INST_DELETE = "scenario.user_action_inst_delete_grp"
 USER_ACTION_INST_DELETE_WAIT = "scenario.user_action_inst_delete_wait_grp"
-
-
-# Instance Log Group
-INST_LOG = "scenario.inst_log_grp"
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/backup_group.py` & `trove-8.0.1/trove/tests/scenario/groups/backup_group.py`

 * *Files 7% similar despite different names*

```diff
@@ -25,16 +25,18 @@
 
 class BackupRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'backup_runners'
     _runner_cls = 'BackupRunner'
 
 
-@test(depends_on_groups=[groups.INST_CREATE],
-      groups=[GROUP, groups.BACKUP, groups.BACKUP_CREATE])
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
+      groups=[GROUP, groups.BACKUP, groups.BACKUP_CREATE],
+      runs_after_groups=[groups.MODULE_INST_DELETE,
+                         groups.CFGGRP_INST_DELETE])
 class BackupCreateGroup(TestGroup):
     """Test Backup Create functionality."""
 
     def __init__(self):
         super(BackupCreateGroup, self).__init__(
             BackupRunnerFactory.instance())
 
@@ -55,16 +57,16 @@
 
     @test(runs_after=[save_backup_counts])
     def backup_create(self):
         """Check that create backup is started successfully."""
         self.test_runner.run_backup_create()
 
 
-@test(depends_on_classes=[BackupCreateGroup],
-      groups=[GROUP, groups.BACKUP_CREATE_NEGATIVE])
+@test(depends_on_groups=[groups.BACKUP_CREATE],
+      groups=[groups.BACKUP_CREATE_NEGATIVE])
 class BackupCreateNegativeGroup(TestGroup):
     """Test Backup Create Negative functionality."""
 
     def __init__(self):
         super(BackupCreateNegativeGroup, self).__init__(
             BackupRunnerFactory.instance())
 
@@ -100,16 +102,17 @@
 
     @test(runs_after=[instance_action_right_after_backup_create])
     def backup_create_instance_not_found(self):
         """Ensure create backup fails with unknown instance id."""
         self.test_runner.run_backup_create_instance_not_found()
 
 
-@test(depends_on_classes=[BackupCreateNegativeGroup],
-      groups=[GROUP, groups.BACKUP, groups.BACKUP_CREATE_WAIT])
+@test(depends_on_groups=[groups.BACKUP_CREATE],
+      groups=[GROUP, groups.BACKUP, groups.BACKUP_CREATE_WAIT],
+      runs_after_groups=[groups.BACKUP_CREATE_NEGATIVE])
 class BackupCreateWaitGroup(TestGroup):
     """Wait for Backup Create to Complete."""
 
     def __init__(self):
         super(BackupCreateWaitGroup, self).__init__(
             BackupRunnerFactory.instance())
 
@@ -130,14 +133,19 @@
 
     @test(depends_on=[backup_create_completed])
     def backup_list_filter_datastore(self):
         """Test list backups and filter by datastore."""
         self.test_runner.run_backup_list_filter_datastore()
 
     @test(depends_on=[backup_create_completed])
+    def backup_list_filter_different_datastore(self):
+        """Test list backups and filter by different datastore."""
+        self.test_runner.run_backup_list_filter_different_datastore()
+
+    @test(depends_on=[backup_create_completed])
     def backup_list_filter_datastore_not_found(self):
         """Test list backups and filter by unknown datastore."""
         self.test_runner.run_backup_list_filter_datastore_not_found()
 
     @test(depends_on=[backup_create_completed])
     def backup_list_for_instance(self):
         """Test backup list for instance."""
@@ -150,15 +158,15 @@
 
     @test(depends_on=[backup_create_completed])
     def backup_get_unauthorized_user(self):
         """Ensure backup show fails for an unauthorized user."""
         self.test_runner.run_backup_get_unauthorized_user()
 
 
-@test(depends_on_classes=[BackupCreateWaitGroup],
+@test(depends_on_groups=[groups.BACKUP_CREATE],
       groups=[GROUP, groups.BACKUP_INC, groups.BACKUP_INC_CREATE])
 class BackupIncCreateGroup(TestGroup):
     """Test Backup Incremental Create functionality."""
 
     def __init__(self):
         super(BackupIncCreateGroup, self).__init__(
             BackupRunnerFactory.instance())
@@ -212,31 +220,51 @@
 
     @test(depends_on=[wait_for_inc_backup_2])
     def instance_goes_active_inc_2(self):
         """Check that the instance goes active after the inc 2 backup."""
         self.test_runner.run_instance_goes_active()
 
 
-@test(depends_on_classes=[BackupIncCreateGroup],
+@test(depends_on_groups=[groups.BACKUP_CREATE],
       groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_CREATE])
 class BackupInstCreateGroup(TestGroup):
     """Test Backup Instance Create functionality."""
 
     def __init__(self):
         super(BackupInstCreateGroup, self).__init__(
             BackupRunnerFactory.instance())
 
     @test
     def restore_from_backup(self):
         """Check that restoring an instance from a backup starts."""
         self.test_runner.run_restore_from_backup()
 
 
-@test(depends_on_classes=[BackupInstCreateGroup],
-      groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_CREATE_WAIT])
+@test(depends_on_groups=[groups.BACKUP_INC_CREATE],
+      groups=[GROUP, groups.BACKUP_INC_INST,
+              groups.BACKUP_INC_INST_CREATE],
+      runs_after_groups=[groups.BACKUP_INST_CREATE])
+class BackupIncInstCreateGroup(TestGroup):
+    """Test Backup Incremental Instance Create functionality."""
+
+    def __init__(self):
+        super(BackupIncInstCreateGroup, self).__init__(
+            BackupRunnerFactory.instance())
+
+    @test
+    def restore_from_inc_1_backup(self):
+        """Check that restoring an instance from inc 1 backup starts."""
+        self.test_runner.run_restore_from_inc_1_backup()
+
+
+@test(depends_on_groups=[groups.BACKUP_INST_CREATE],
+      groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_CREATE_WAIT],
+      runs_after_groups=[groups.BACKUP_INC_INST_CREATE,
+                         groups.DB_ACTION_INST_CREATE,
+                         groups.INST_ACTIONS_RESIZE])
 class BackupInstCreateWaitGroup(TestGroup):
     """Test Backup Instance Create completes."""
 
     def __init__(self):
         super(BackupInstCreateWaitGroup, self).__init__(
             BackupRunnerFactory.instance())
 
@@ -252,63 +280,18 @@
 
     @test(depends_on=[restore_from_backup_completed])
     def verify_databases_in_restored_instance(self):
         """Verify databases in restored instance."""
         self.test_runner.run_verify_databases_in_restored_instance()
 
 
-@test(depends_on_classes=[BackupInstCreateWaitGroup],
-      groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_DELETE])
-class BackupInstDeleteGroup(TestGroup):
-    """Test Backup Instance Delete functionality."""
-
-    def __init__(self):
-        super(BackupInstDeleteGroup, self).__init__(
-            BackupRunnerFactory.instance())
-
-    @test
-    def delete_restored_instance(self):
-        """Test deleting the restored instance."""
-        self.test_runner.run_delete_restored_instance()
-
-
-@test(depends_on_classes=[BackupInstDeleteGroup],
-      groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_DELETE_WAIT])
-class BackupInstDeleteWaitGroup(TestGroup):
-    """Test Backup Instance Delete completes."""
-
-    def __init__(self):
-        super(BackupInstDeleteWaitGroup, self).__init__(
-            BackupRunnerFactory.instance())
-
-    @test
-    def wait_for_restored_instance_delete(self):
-        """Wait until deleting the restored instance completes."""
-        self.test_runner.run_wait_for_restored_instance_delete()
-
-
-@test(depends_on_classes=[BackupInstDeleteWaitGroup],
+@test(depends_on_groups=[groups.BACKUP_INC_INST_CREATE],
       groups=[GROUP, groups.BACKUP_INC_INST,
-              groups.BACKUP_INC_INST_CREATE])
-class BackupIncInstCreateGroup(TestGroup):
-    """Test Backup Incremental Instance Create functionality."""
-
-    def __init__(self):
-        super(BackupIncInstCreateGroup, self).__init__(
-            BackupRunnerFactory.instance())
-
-    @test
-    def restore_from_inc_1_backup(self):
-        """Check that restoring an instance from inc 1 backup starts."""
-        self.test_runner.run_restore_from_inc_1_backup()
-
-
-@test(depends_on_classes=[BackupIncInstCreateGroup],
-      groups=[GROUP, groups.BACKUP_INC_INST,
-              groups.BACKUP_INC_INST_CREATE_WAIT])
+              groups.BACKUP_INC_INST_CREATE_WAIT],
+      runs_after_groups=[groups.BACKUP_INST_CREATE_WAIT])
 class BackupIncInstCreateWaitGroup(TestGroup):
     """Test Backup Incremental Instance Create completes."""
 
     def __init__(self):
         super(BackupIncInstCreateWaitGroup, self).__init__(
             BackupRunnerFactory.instance())
 
@@ -324,48 +307,83 @@
 
     @test(depends_on=[restore_from_inc_1_backup_completed])
     def verify_databases_in_restored_inc_1_instance(self):
         """Verify databases in restored inc 1 instance."""
         self.test_runner.run_verify_databases_in_restored_inc_1_instance()
 
 
-@test(depends_on_classes=[BackupIncInstCreateWaitGroup],
+@test(depends_on_groups=[groups.BACKUP_INST_CREATE_WAIT],
+      groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_DELETE],
+      runs_after_groups=[groups.BACKUP_INC_INST_CREATE_WAIT])
+class BackupInstDeleteGroup(TestGroup):
+    """Test Backup Instance Delete functionality."""
+
+    def __init__(self):
+        super(BackupInstDeleteGroup, self).__init__(
+            BackupRunnerFactory.instance())
+
+    @test
+    def delete_restored_instance(self):
+        """Test deleting the restored instance."""
+        self.test_runner.run_delete_restored_instance()
+
+
+@test(depends_on_groups=[groups.BACKUP_INC_INST_CREATE_WAIT],
       groups=[GROUP, groups.BACKUP_INC_INST,
-              groups.BACKUP_INC_INST_DELETE])
+              groups.BACKUP_INC_INST_DELETE],
+      runs_after_groups=[groups.BACKUP_INST_DELETE])
 class BackupIncInstDeleteGroup(TestGroup):
     """Test Backup Incremental Instance Delete functionality."""
 
     def __init__(self):
         super(BackupIncInstDeleteGroup, self).__init__(
             BackupRunnerFactory.instance())
 
     @test
     def delete_restored_inc_1_instance(self):
         """Test deleting the restored inc 1 instance."""
         self.test_runner.run_delete_restored_inc_1_instance()
 
 
-@test(depends_on_classes=[BackupIncInstDeleteGroup],
+@test(depends_on_groups=[groups.BACKUP_INST_DELETE],
+      groups=[GROUP, groups.BACKUP_INST, groups.BACKUP_INST_DELETE_WAIT],
+      runs_after_groups=[groups.INST_DELETE])
+class BackupInstDeleteWaitGroup(TestGroup):
+    """Test Backup Instance Delete completes."""
+
+    def __init__(self):
+        super(BackupInstDeleteWaitGroup, self).__init__(
+            BackupRunnerFactory.instance())
+
+    @test
+    def wait_for_restored_instance_delete(self):
+        """Wait until deleting the restored instance completes."""
+        self.test_runner.run_wait_for_restored_instance_delete()
+
+
+@test(depends_on_groups=[groups.BACKUP_INC_INST_DELETE],
       groups=[GROUP, groups.BACKUP_INC_INST,
-              groups.BACKUP_INC_INST_DELETE_WAIT])
+              groups.BACKUP_INC_INST_DELETE_WAIT],
+      runs_after_groups=[groups.INST_DELETE])
 class BackupIncInstDeleteWaitGroup(TestGroup):
     """Test Backup Incremental Instance Delete completes."""
 
     def __init__(self):
         super(BackupIncInstDeleteWaitGroup, self).__init__(
             BackupRunnerFactory.instance())
 
     @test
     def wait_for_restored_inc_1_instance_delete(self):
         """Wait until deleting the restored inc 1 instance completes."""
         self.test_runner.run_wait_for_restored_inc_1_instance_delete()
 
 
-@test(depends_on_classes=[BackupIncInstDeleteWaitGroup],
-      groups=[GROUP, groups.BACKUP_INC, groups.BACKUP_INC_DELETE])
+@test(depends_on_groups=[groups.BACKUP_INC_CREATE],
+      groups=[GROUP, groups.BACKUP_INC, groups.BACKUP_INC_DELETE],
+      runs_after_groups=[groups.BACKUP_INC_INST_CREATE_WAIT])
 class BackupIncDeleteGroup(TestGroup):
     """Test Backup Incremental Delete functionality."""
 
     def __init__(self):
         super(BackupIncDeleteGroup, self).__init__(
             BackupRunnerFactory.instance())
 
@@ -373,16 +391,19 @@
     def delete_inc_2_backup(self):
         """Test deleting the inc 2 backup."""
         # We only delete the inc 2 backup, as the inc 1 should be deleted
         # by the full backup delete that runs after.
         self.test_runner.run_delete_inc_2_backup()
 
 
-@test(depends_on_classes=[BackupIncDeleteGroup],
-      groups=[GROUP, groups.BACKUP, groups.BACKUP_DELETE])
+@test(depends_on_groups=[groups.BACKUP_CREATE],
+      groups=[GROUP, groups.BACKUP, groups.BACKUP_DELETE],
+      runs_after_groups=[groups.BACKUP_INST_CREATE_WAIT,
+                         groups.BACKUP_INC_DELETE,
+                         groups.INST_ACTIONS_RESIZE_WAIT])
 class BackupDeleteGroup(TestGroup):
     """Test Backup Delete functionality."""
 
     def __init__(self):
         super(BackupDeleteGroup, self).__init__(
             BackupRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/cluster_group.py` & `trove-8.0.1/trove/tests/scenario/groups/cluster_group.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/configuration_group.py` & `trove-8.0.1/trove/tests/scenario/groups/configuration_group.py`

 * *Files 13% similar despite different names*

```diff
@@ -26,15 +26,15 @@
 class ConfigurationRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'configuration_runners'
     _runner_cls = 'ConfigurationRunner'
 
 
 @test(groups=[GROUP, groups.CFGGRP_CREATE],
-      depends_on_groups=[groups.BACKUP_DELETE])
+      runs_after_groups=[groups.MODULE_CREATE])
 class ConfigurationCreateGroup(TestGroup):
     """Test Configuration Group functionality."""
 
     def __init__(self):
         super(ConfigurationCreateGroup, self).__init__(
             ConfigurationRunnerFactory.instance())
 
@@ -90,17 +90,19 @@
 
     @test(depends_on=[create_non_dynamic_group])
     def non_dynamic_conf_get_unauthorized_user(self):
         """Ensure show non-dynamic fails with unauthorized user."""
         self.test_runner.run_non_dynamic_conf_get_unauthorized_user()
 
 
-@test(depends_on_classes=[ConfigurationCreateGroup],
+@test(depends_on_groups=[groups.INST_CREATE_WAIT,
+                         groups.CFGGRP_CREATE],
       groups=[GROUP, groups.CFGGRP_INST,
-              groups.CFGGRP_INST_CREATE])
+              groups.CFGGRP_INST_CREATE],
+      runs_after_groups=[groups.MODULE_INST_CREATE])
 class ConfigurationInstCreateGroup(TestGroup):
     """Test Instance Configuration Group Create functionality."""
 
     def __init__(self):
         super(ConfigurationInstCreateGroup, self).__init__(
             ConfigurationRunnerFactory.instance())
 
@@ -224,17 +226,20 @@
 
     @test(runs_after=[detach_non_dynamic_group])
     def create_instance_with_conf(self):
         """Test create instance with conf group."""
         self.test_runner.run_create_instance_with_conf()
 
 
-@test(depends_on_classes=[ConfigurationInstCreateGroup],
+@test(depends_on_groups=[groups.CFGGRP_INST_CREATE],
       groups=[GROUP, groups.CFGGRP_INST,
-              groups.CFGGRP_INST_CREATE_WAIT])
+              groups.CFGGRP_INST_CREATE_WAIT],
+      runs_after_groups=[groups.INST_ACTIONS,
+                         groups.INST_UPGRADE,
+                         groups.MODULE_INST_CREATE_WAIT])
 class ConfigurationInstCreateWaitGroup(TestGroup):
     """Test that Instance Configuration Group Create Completes."""
 
     def __init__(self):
         super(ConfigurationInstCreateWaitGroup, self).__init__(
             ConfigurationRunnerFactory.instance())
 
@@ -245,47 +250,50 @@
 
     @test(depends_on=[wait_for_conf_instance])
     def verify_instance_values(self):
         """Verify configuration values on the instance."""
         self.test_runner.run_verify_instance_values()
 
 
-@test(depends_on_classes=[ConfigurationInstCreateWaitGroup],
+@test(depends_on_groups=[groups.CFGGRP_INST_CREATE_WAIT],
       groups=[GROUP, groups.CFGGRP_INST,
-              groups.CFGGRP_INST_DELETE])
+              groups.CFGGRP_INST_DELETE],
+      runs_after_groups=[groups.MODULE_INST_DELETE])
 class ConfigurationInstDeleteGroup(TestGroup):
     """Test Instance Configuration Group Delete functionality."""
 
     def __init__(self):
         super(ConfigurationInstDeleteGroup, self).__init__(
             ConfigurationRunnerFactory.instance())
 
     @test
     def delete_conf_instance(self):
         """Test delete instance with conf group."""
         self.test_runner.run_delete_conf_instance()
 
 
-@test(depends_on_classes=[ConfigurationInstDeleteGroup],
+@test(depends_on_groups=[groups.CFGGRP_INST_DELETE],
       groups=[GROUP, groups.CFGGRP_INST,
-              groups.CFGGRP_INST_DELETE_WAIT])
+              groups.CFGGRP_INST_DELETE_WAIT],
+      runs_after_groups=[groups.INST_DELETE])
 class ConfigurationInstDeleteWaitGroup(TestGroup):
     """Test that Instance Configuration Group Delete Completes."""
 
     def __init__(self):
         super(ConfigurationInstDeleteWaitGroup, self).__init__(
             ConfigurationRunnerFactory.instance())
 
     @test
     def wait_for_delete_conf_instance(self):
         """Wait for delete instance with conf group to complete."""
         self.test_runner.run_wait_for_delete_conf_instance()
 
 
-@test(depends_on_classes=[ConfigurationInstDeleteWaitGroup],
+@test(depends_on_groups=[groups.CFGGRP_CREATE],
+      runs_after_groups=[groups.CFGGRP_INST_DELETE_WAIT],
       groups=[GROUP, groups.CFGGRP_DELETE])
 class ConfigurationDeleteGroup(TestGroup):
     """Test Configuration Group Delete functionality."""
 
     def __init__(self):
         super(ConfigurationDeleteGroup, self).__init__(
             ConfigurationRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/database_actions_group.py` & `trove-8.0.1/trove/tests/scenario/groups/database_actions_group.py`

 * *Files 22% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 
 class InstanceCreateRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'instance_create_runners'
     _runner_cls = 'InstanceCreateRunner'
 
 
-@test(depends_on_groups=[groups.CFGGRP_DELETE],
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
       groups=[GROUP, groups.DB_ACTION_CREATE])
 class DatabaseActionsCreateGroup(TestGroup):
     """Test Database Actions Create functionality."""
 
     def __init__(self):
         super(DatabaseActionsCreateGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
@@ -69,15 +69,15 @@
     @test(depends_on=[create_databases],
           runs_after=[create_database_with_blank_name])
     def create_existing_database(self):
         """Ensure creating an existing database fails."""
         self.test_runner.run_existing_database_create()
 
 
-@test(depends_on_classes=[DatabaseActionsCreateGroup],
+@test(depends_on_groups=[groups.DB_ACTION_CREATE],
       groups=[GROUP, groups.DB_ACTION_DELETE])
 class DatabaseActionsDeleteGroup(TestGroup):
     """Test Database Actions Delete functionality."""
 
     def __init__(self):
         super(DatabaseActionsDeleteGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
@@ -99,16 +99,16 @@
 
     @test(runs_after=[create_system_database])
     def delete_system_database(self):
         """Ensure deleting a system database fails."""
         self.test_runner.run_system_database_delete()
 
 
-@test(depends_on_classes=[DatabaseActionsDeleteGroup],
-      groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_CREATE])
+@test(groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_CREATE],
+      runs_after_groups=[groups.INST_ACTIONS_RESIZE])
 class DatabaseActionsInstCreateGroup(TestGroup):
     """Test Database Actions Instance Create functionality."""
 
     def __init__(self):
         super(DatabaseActionsInstCreateGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
         self.instance_create_runner = InstanceCreateRunnerFactory.instance()
@@ -117,16 +117,19 @@
     def create_initialized_instance(self):
         """Create an instance with initial databases."""
         self.instance_create_runner.run_initialized_instance_create(
             with_dbs=True, with_users=False, configuration_id=None,
             name_suffix='_db')
 
 
-@test(depends_on_classes=[DatabaseActionsInstCreateGroup],
-      groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_CREATE_WAIT])
+@test(depends_on_groups=[groups.DB_ACTION_INST_CREATE],
+      groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_CREATE_WAIT],
+      runs_after_groups=[groups.BACKUP_INST_CREATE,
+                         groups.BACKUP_INC_INST_CREATE,
+                         groups.INST_ACTIONS_RESIZE])
 class DatabaseActionsInstCreateWaitGroup(TestGroup):
     """Wait for Database Actions Instance Create to complete."""
 
     def __init__(self):
         super(DatabaseActionsInstCreateWaitGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
         self.instance_create_runner = InstanceCreateRunnerFactory.instance()
@@ -143,15 +146,15 @@
 
     @test(runs_after=[add_initialized_instance_data])
     def validate_initialized_instance(self):
         """Validate the database instance data and properties."""
         self.instance_create_runner.run_validate_initialized_instance()
 
 
-@test(depends_on_classes=[DatabaseActionsInstCreateWaitGroup],
+@test(depends_on_groups=[groups.DB_ACTION_INST_CREATE_WAIT],
       groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_DELETE])
 class DatabaseActionsInstDeleteGroup(TestGroup):
     """Test Database Actions Instance Delete functionality."""
 
     def __init__(self):
         super(DatabaseActionsInstDeleteGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
@@ -159,16 +162,17 @@
 
     @test
     def delete_initialized_instance(self):
         """Delete the database instance."""
         self.instance_create_runner.run_initialized_instance_delete()
 
 
-@test(depends_on_classes=[DatabaseActionsInstDeleteGroup],
-      groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_DELETE_WAIT])
+@test(depends_on_groups=[groups.DB_ACTION_INST_DELETE],
+      groups=[GROUP, groups.DB_ACTION_INST, groups.DB_ACTION_INST_DELETE_WAIT],
+      runs_after_groups=[groups.INST_DELETE])
 class DatabaseActionsInstDeleteWaitGroup(TestGroup):
     """Wait for Database Actions Instance Delete to complete."""
 
     def __init__(self):
         super(DatabaseActionsInstDeleteWaitGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
         self.instance_create_runner = InstanceCreateRunnerFactory.instance()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/guest_log_group.py` & `trove-8.0.1/trove/tests/scenario/groups/guest_log_group.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,16 +24,18 @@
 
 class GuestLogRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'guest_log_runners'
     _runner_cls = 'GuestLogRunner'
 
 
-@test(depends_on_groups=[groups.DB_ACTION_INST_DELETE_WAIT],
-      groups=[GROUP, groups.INST_LOG])
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
+      groups=[GROUP],
+      runs_after_groups=[groups.USER_ACTION_INST_CREATE,
+                         groups.ROOT_ACTION_INST_CREATE])
 class GuestLogGroup(TestGroup):
     """Test Guest Log functionality."""
 
     def __init__(self):
         super(GuestLogGroup, self).__init__(
             GuestLogRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_actions_group.py` & `trove-8.0.1/trove/tests/scenario/groups/instance_actions_group.py`

 * *Files 7% similar despite different names*

```diff
@@ -25,16 +25,18 @@
 
 class InstanceActionsRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'instance_actions_runners'
     _runner_cls = 'InstanceActionsRunner'
 
 
-@test(depends_on_groups=[groups.INST_LOG],
-      groups=[GROUP, groups.INST_ACTIONS])
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
+      groups=[GROUP, groups.INST_ACTIONS],
+      runs_after_groups=[groups.MODULE_INST_CREATE,
+                         groups.CFGGRP_INST_CREATE])
 class InstanceActionsGroup(TestGroup):
     """Test Instance Actions functionality."""
 
     def __init__(self):
         super(InstanceActionsGroup, self).__init__(
             InstanceActionsRunnerFactory.instance())
 
@@ -72,16 +74,22 @@
     @test(depends_on=[add_test_data],
           runs_after=[verify_test_data_after_volume_resize])
     def remove_test_data(self):
         """Remove test data."""
         self.test_runner.run_remove_test_data()
 
 
-@test(depends_on_classes=[InstanceActionsGroup],
-      groups=[GROUP, groups.INST_ACTIONS_RESIZE])
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
+      groups=[GROUP, groups.INST_ACTIONS_RESIZE],
+      runs_after_groups=[groups.INST_ACTIONS,
+                         groups.INST_UPGRADE,
+                         groups.MODULE_INST_CREATE_WAIT,
+                         groups.CFGGRP_INST_CREATE_WAIT,
+                         groups.BACKUP_CREATE,
+                         groups.BACKUP_INC_CREATE])
 class InstanceActionsResizeGroup(TestGroup):
     """Test Instance Actions Resize functionality."""
 
     def __init__(self):
         super(InstanceActionsResizeGroup, self).__init__(
             InstanceActionsRunnerFactory.instance())
 
@@ -97,16 +105,19 @@
 
     @test(runs_after=[verify_test_data])
     def instance_resize_flavor(self):
         """Resize instance flavor."""
         self.test_runner.run_instance_resize_flavor()
 
 
-@test(depends_on_classes=[InstanceActionsResizeGroup],
-      groups=[GROUP, groups.INST_ACTIONS_RESIZE_WAIT])
+@test(depends_on_groups=[groups.INST_ACTIONS_RESIZE],
+      groups=[GROUP, groups.INST_ACTIONS_RESIZE_WAIT],
+      runs_after_groups=[groups.BACKUP_INST_CREATE,
+                         groups.BACKUP_INC_INST_CREATE,
+                         groups.DB_ACTION_INST_CREATE])
 class InstanceActionsResizeWaitGroup(TestGroup):
     """Test that Instance Actions Resize Completes."""
 
     def __init__(self):
         super(InstanceActionsResizeWaitGroup, self).__init__(
             InstanceActionsRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_create_group.py` & `trove-8.0.1/trove/tests/scenario/groups/instance_create_group.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,43 +11,46 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from proboscis import test
 
+from trove.tests import PRE_INSTANCES
 from trove.tests.scenario import groups
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
 GROUP = "scenario.instance_create_group"
 
 
 class InstanceCreateRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'instance_create_runners'
     _runner_cls = 'InstanceCreateRunner'
 
 
-@test(groups=[GROUP, groups.INST_CREATE])
+@test(depends_on_groups=["services.initialize"],
+      runs_after_groups=[PRE_INSTANCES],
+      groups=[GROUP, groups.INST_CREATE])
 class InstanceCreateGroup(TestGroup):
     """Test Instance Create functionality."""
 
     def __init__(self):
         super(InstanceCreateGroup, self).__init__(
             InstanceCreateRunnerFactory.instance())
 
     @test
     def create_empty_instance(self):
         """Create an empty instance."""
         self.test_runner.run_empty_instance_create()
 
 
-@test(depends_on_classes=[InstanceCreateGroup],
+@test(depends_on_groups=[groups.INST_CREATE],
       groups=[GROUP, groups.INST_INIT_CREATE])
 class InstanceInitCreateGroup(TestGroup):
     """Test Instance Init Create functionality."""
 
     def __init__(self):
         super(InstanceInitCreateGroup, self).__init__(
             InstanceCreateRunnerFactory.instance())
@@ -59,31 +62,34 @@
 
     @test(runs_after=[create_initial_configuration])
     def create_initialized_instance(self):
         """Create an instance with initial properties."""
         self.test_runner.run_initialized_instance_create()
 
 
-@test(depends_on_classes=[InstanceCreateGroup],
-      groups=[GROUP, groups.INST_CREATE])
+@test(depends_on_groups=[groups.INST_CREATE],
+      groups=[GROUP, groups.INST_CREATE_WAIT],
+      runs_after_groups=[groups.MODULE_CREATE, groups.CFGGRP_CREATE,
+                         groups.INST_ERROR_DELETE])
 class InstanceCreateWaitGroup(TestGroup):
     """Test that Instance Create Completes."""
 
     def __init__(self):
         super(InstanceCreateWaitGroup, self).__init__(
             InstanceCreateRunnerFactory.instance())
 
     @test
     def wait_for_instance(self):
         """Waiting for main instance to become active."""
         self.test_runner.run_wait_for_instance()
 
 
-@test(depends_on_classes=[InstanceCreateWaitGroup],
-      groups=[GROUP, groups.INST_INIT_CREATE_WAIT])
+@test(depends_on_groups=[groups.INST_INIT_CREATE],
+      groups=[GROUP, groups.INST_INIT_CREATE_WAIT],
+      runs_after_groups=[groups.INST_CREATE_WAIT])
 class InstanceInitCreateWaitGroup(TestGroup):
     """Test that Instance Init Create Completes."""
 
     def __init__(self):
         super(InstanceInitCreateWaitGroup, self).__init__(
             InstanceCreateRunnerFactory.instance())
 
@@ -99,30 +105,31 @@
 
     @test(runs_after=[add_initialized_instance_data])
     def validate_initialized_instance(self):
         """Validate the initialized instance data and properties."""
         self.test_runner.run_validate_initialized_instance()
 
 
-@test(depends_on_classes=[InstanceInitCreateWaitGroup],
+@test(depends_on_groups=[groups.INST_INIT_CREATE_WAIT],
       groups=[GROUP, groups.INST_INIT_DELETE])
 class InstanceInitDeleteGroup(TestGroup):
     """Test Initialized Instance Delete functionality."""
 
     def __init__(self):
         super(InstanceInitDeleteGroup, self).__init__(
             InstanceCreateRunnerFactory.instance())
 
     @test
     def delete_initialized_instance(self):
         """Delete the initialized instance."""
         self.test_runner.run_initialized_instance_delete()
 
 
-@test(depends_on_classes=[InstanceInitDeleteGroup],
+@test(depends_on_groups=[groups.INST_INIT_DELETE],
+      runs_after_groups=[groups.INST_ERROR_DELETE],
       groups=[GROUP, groups.INST_INIT_DELETE_WAIT])
 class InstanceInitDeleteWaitGroup(TestGroup):
     """Test that Initialized Instance Delete Completes."""
 
     def __init__(self):
         super(InstanceInitDeleteWaitGroup, self).__init__(
             InstanceCreateRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_delete_group.py` & `trove-8.0.1/trove/tests/scenario/groups/instance_force_delete_group.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2015 Tesora Inc.
+# Copyright 2016 Tesora Inc.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
@@ -11,51 +11,57 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from proboscis import test
 
+from trove.tests import PRE_INSTANCES
 from trove.tests.scenario import groups
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
-GROUP = "scenario.instance_delete_group"
+GROUP = "scenario.instance_force_delete_group"
 
 
-class InstanceDeleteRunnerFactory(test_runners.RunnerFactory):
+class InstanceForceDeleteRunnerFactory(test_runners.RunnerFactory):
 
-    _runner_ns = 'instance_delete_runners'
-    _runner_cls = 'InstanceDeleteRunner'
+    _runner_ns = 'instance_force_delete_runners'
+    _runner_cls = 'InstanceForceDeleteRunner'
 
 
-@test(depends_on_groups=[groups.INST_CREATE],
-      groups=[GROUP, groups.INST_DELETE],
-      runs_after_groups=[groups.USER_ACTION_INST_DELETE_WAIT,
-                         groups.REPL_INST_DELETE_WAIT])
-class InstanceDeleteGroup(TestGroup):
-    """Test Instance Delete functionality."""
+@test(depends_on_groups=["services.initialize"],
+      runs_after_groups=[PRE_INSTANCES, groups.INST_ERROR_CREATE],
+      groups=[GROUP, groups.INST_FORCE_DELETE])
+class InstanceForceDeleteGroup(TestGroup):
+    """Test Instance Force Delete functionality."""
 
     def __init__(self):
-        super(InstanceDeleteGroup, self).__init__(
-            InstanceDeleteRunnerFactory.instance())
+        super(InstanceForceDeleteGroup, self).__init__(
+            InstanceForceDeleteRunnerFactory.instance())
 
     @test
-    def instance_delete(self):
-        """Delete an existing instance."""
-        self.test_runner.run_instance_delete()
+    def create_build_instance(self):
+        """Create an instance in BUILD state."""
+        self.test_runner.run_create_build_instance()
+
+    @test(depends_on=['create_build_instance'])
+    def delete_build_instance(self):
+        """Make sure the instance in BUILD state deletes."""
+        self.test_runner.run_delete_build_instance()
 
 
-@test(depends_on_classes=[InstanceDeleteGroup],
-      groups=[GROUP, groups.INST_DELETE_WAIT])
-class InstanceDeleteWaitGroup(TestGroup):
-    """Test that Instance Delete Completes."""
+@test(depends_on_groups=[groups.INST_FORCE_DELETE],
+      runs_after_groups=[groups.MODULE_INST_CREATE],
+      groups=[GROUP, groups.INST_FORCE_DELETE_WAIT])
+class InstanceForceDeleteWaitGroup(TestGroup):
+    """Make sure the Force Delete instance goes away."""
 
     def __init__(self):
-        super(InstanceDeleteWaitGroup, self).__init__(
-            InstanceDeleteRunnerFactory.instance())
+        super(InstanceForceDeleteWaitGroup, self).__init__(
+            InstanceForceDeleteRunnerFactory.instance())
 
     @test
-    def instance_delete_wait(self):
-        """Wait for existing instance to be gone."""
-        self.test_runner.run_instance_delete_wait()
+    def wait_for_force_delete(self):
+        """Wait for the Force Delete instance to be gone."""
+        self.test_runner.run_wait_for_force_delete()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_error_create_group.py` & `trove-8.0.1/trove/tests/scenario/groups/instance_error_create_group.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,29 +11,31 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from proboscis import test
 
+from trove.tests import PRE_INSTANCES
 from trove.tests.scenario import groups
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
 GROUP = "scenario.instance_error_create_group"
 
 
 class InstanceErrorCreateRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'instance_error_create_runners'
     _runner_cls = 'InstanceErrorCreateRunner'
 
 
-@test(depends_on_groups=[groups.INST_CREATE],
+@test(depends_on_groups=["services.initialize"],
+      runs_after_groups=[PRE_INSTANCES, groups.INST_CREATE],
       groups=[GROUP, groups.INST_ERROR_CREATE])
 class InstanceErrorCreateGroup(TestGroup):
     """Test Instance Error Create functionality."""
 
     def __init__(self):
         super(InstanceErrorCreateGroup, self).__init__(
             InstanceErrorCreateRunnerFactory.instance())
@@ -45,15 +47,16 @@
 
     @test(runs_after=[create_error_instance])
     def create_error2_instance(self):
         """Create another instance in error state."""
         self.test_runner.run_create_error2_instance()
 
 
-@test(depends_on_classes=[InstanceErrorCreateGroup],
+@test(depends_on_groups=[groups.INST_ERROR_CREATE],
+      runs_after_groups=[groups.MODULE_CREATE, groups.CFGGRP_CREATE],
       groups=[GROUP, groups.INST_ERROR_CREATE_WAIT])
 class InstanceErrorCreateWaitGroup(TestGroup):
     """Test that Instance Error Create Completes."""
 
     def __init__(self):
         super(InstanceErrorCreateWaitGroup, self).__init__(
             InstanceErrorCreateRunnerFactory.instance())
@@ -71,30 +74,31 @@
     @test(depends_on=[wait_for_error_instances],
           runs_after=[validate_error_instance])
     def validate_error2_instance(self):
         """Validate the error2 instance fault message as admin."""
         self.test_runner.run_validate_error2_instance()
 
 
-@test(depends_on_classes=[InstanceErrorCreateWaitGroup],
+@test(depends_on_groups=[groups.INST_ERROR_CREATE_WAIT],
       groups=[GROUP, groups.INST_ERROR_DELETE])
 class InstanceErrorDeleteGroup(TestGroup):
     """Test Instance Error Delete functionality."""
 
     def __init__(self):
         super(InstanceErrorDeleteGroup, self).__init__(
             InstanceErrorCreateRunnerFactory.instance())
 
     @test
     def delete_error_instances(self):
         """Delete the error instances."""
         self.test_runner.run_delete_error_instances()
 
 
-@test(depends_on_classes=[InstanceErrorDeleteGroup],
+@test(depends_on_groups=[groups.INST_ERROR_DELETE],
+      runs_after_groups=[groups.MODULE_INST_CREATE],
       groups=[GROUP, groups.INST_ERROR_DELETE_WAIT])
 class InstanceErrorDeleteWaitGroup(TestGroup):
     """Test that Instance Error Delete Completes."""
 
     def __init__(self):
         super(InstanceErrorDeleteWaitGroup, self).__init__(
             InstanceErrorCreateRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_force_delete_group.py` & `trove-8.0.1/trove/tests/scenario/groups/negative_cluster_actions_group.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2016 Tesora Inc.
+# Copyright 2015 Tesora Inc.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
@@ -11,54 +11,36 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from proboscis import test
 
-from trove.tests.scenario import groups
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
-GROUP = "scenario.instance_force_delete_group"
+GROUP = "scenario.negative_cluster_actions_group"
 
 
-class InstanceForceDeleteRunnerFactory(test_runners.RunnerFactory):
+class NegativeClusterActionsRunnerFactory(test_runners.RunnerFactory):
 
-    _runner_ns = 'instance_force_delete_runners'
-    _runner_cls = 'InstanceForceDeleteRunner'
+    _runner_ns = 'negative_cluster_actions_runners'
+    _runner_cls = 'NegativeClusterActionsRunner'
 
 
-@test(depends_on_groups=[groups.INST_ERROR_DELETE_WAIT],
-      groups=[GROUP, groups.INST_FORCE_DELETE])
-class InstanceForceDeleteGroup(TestGroup):
-    """Test Instance Force Delete functionality."""
+@test(groups=[GROUP])
+class NegativeClusterActionsGroup(TestGroup):
 
     def __init__(self):
-        super(InstanceForceDeleteGroup, self).__init__(
-            InstanceForceDeleteRunnerFactory.instance())
+        super(NegativeClusterActionsGroup, self).__init__(
+            NegativeClusterActionsRunnerFactory.instance())
 
     @test
-    def create_build_instance(self):
-        """Create an instance in BUILD state."""
-        self.test_runner.run_create_build_instance()
-
-    @test(depends_on=['create_build_instance'])
-    def delete_build_instance(self):
-        """Make sure the instance in BUILD state deletes."""
-        self.test_runner.run_delete_build_instance()
-
-
-@test(depends_on_classes=[InstanceForceDeleteGroup],
-      groups=[GROUP, groups.INST_FORCE_DELETE_WAIT])
-class InstanceForceDeleteWaitGroup(TestGroup):
-    """Make sure the Force Delete instance goes away."""
-
-    def __init__(self):
-        super(InstanceForceDeleteWaitGroup, self).__init__(
-            InstanceForceDeleteRunnerFactory.instance())
+    def create_constrained_size_cluster(self):
+        """Ensure creating a cluster with wrong number of nodes fails."""
+        self.test_runner.run_create_constrained_size_cluster()
 
     @test
-    def wait_for_force_delete(self):
-        """Wait for the Force Delete instance to be gone."""
-        self.test_runner.run_wait_for_force_delete()
+    def create_heterogeneous_cluster(self):
+        """Ensure creating a cluster with unequal nodes fails."""
+        self.test_runner.run_create_heterogeneous_cluster()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/instance_upgrade_group.py` & `trove-8.0.1/trove/tests/scenario/groups/instance_upgrade_group.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-from proboscis import SkipTest
+
 from proboscis import test
 
 from trove.tests.scenario import groups
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
@@ -70,27 +70,20 @@
         self.test_runner.run_add_test_data()
 
     @test(depends_on=[add_test_data])
     def verify_test_data(self):
         """Verify test data."""
         self.test_runner.run_verify_test_data()
 
-    @test(depends_on=[verify_test_data])
-    def list_users_before_upgrade(self):
-        """List the created users before upgrade."""
-        self.user_actions_runner.run_users_list()
-
-    @test(depends_on=[list_users_before_upgrade])
+    @test(runs_after=[verify_test_data])
     def instance_upgrade(self):
         """Upgrade an existing instance."""
-        raise SkipTest("Skip the instance upgrade integration test "
-                       "temporarily because of not stable in CI")
-        # self.test_runner.run_instance_upgrade()
+        self.test_runner.run_instance_upgrade()
 
-    @test(depends_on=[list_users_before_upgrade])
+    @test(depends_on=[instance_upgrade])
     def show_user(self):
         """Show created users."""
         self.user_actions_runner.run_user_show()
 
     @test(depends_on=[create_users],
           runs_after=[show_user])
     def list_users(self):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/module_group.py` & `trove-8.0.1/trove/tests/scenario/groups/module_group.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/replication_group.py` & `trove-8.0.1/trove/tests/scenario/groups/replication_group.py`

 * *Files 4% similar despite different names*

```diff
@@ -31,16 +31,23 @@
 
 class BackupRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'backup_runners'
     _runner_cls = 'BackupRunner'
 
 
-@test(depends_on_groups=[groups.INST_CREATE],
-      groups=[GROUP, groups.REPL_INST_CREATE])
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
+      groups=[GROUP, groups.REPL_INST_CREATE],
+      runs_after_groups=[groups.MODULE_INST_DELETE,
+                         groups.CFGGRP_INST_DELETE,
+                         groups.INST_ACTIONS_RESIZE_WAIT,
+                         groups.DB_ACTION_INST_DELETE,
+                         groups.USER_ACTION_DELETE,
+                         groups.USER_ACTION_INST_DELETE,
+                         groups.ROOT_ACTION_INST_DELETE])
 class ReplicationInstCreateGroup(TestGroup):
     """Test Replication Instance Create functionality."""
 
     def __init__(self):
         super(ReplicationInstCreateGroup, self).__init__(
             ReplicationRunnerFactory.instance())
 
@@ -61,16 +68,17 @@
 
     @test(runs_after=[create_non_affinity_master])
     def create_single_replica(self):
         """Test creating a single replica."""
         self.test_runner.run_create_single_replica()
 
 
-@test(depends_on_classes=[ReplicationInstCreateGroup],
-      groups=[GROUP, groups.REPL_INST_CREATE_WAIT])
+@test(depends_on_groups=[groups.REPL_INST_CREATE],
+      groups=[GROUP, groups.REPL_INST_CREATE_WAIT],
+      runs_after_groups=[groups.INST_INIT_DELETE_WAIT])
 class ReplicationInstCreateWaitGroup(TestGroup):
     """Wait for Replication Instance Create to complete."""
 
     def __init__(self):
         super(ReplicationInstCreateWaitGroup, self).__init__(
             ReplicationRunnerFactory.instance())
 
@@ -106,15 +114,15 @@
 
     @test(depends_on=[add_data_after_replica])
     def verify_replica_data_after_single(self):
         """Verify data exists on single replica"""
         self.test_runner.run_verify_replica_data_after_single()
 
 
-@test(depends_on_classes=[ReplicationInstCreateWaitGroup],
+@test(depends_on_groups=[groups.REPL_INST_CREATE_WAIT],
       groups=[GROUP, groups.REPL_INST_MULTI_CREATE])
 class ReplicationInstMultiCreateGroup(TestGroup):
     """Test Replication Instance Multi-Create functionality."""
 
     def __init__(self):
         super(ReplicationInstMultiCreateGroup, self).__init__(
             ReplicationRunnerFactory.instance())
@@ -134,16 +142,18 @@
 
     @test(depends_on=[create_multiple_replicas])
     def check_has_incremental_backup(self):
         """Test that creating multiple replicas uses incr backup."""
         self.backup_runner.run_check_has_incremental()
 
 
-@test(depends_on_classes=[ReplicationInstMultiCreateGroup],
-      groups=[GROUP, groups.REPL_INST_DELETE_NON_AFFINITY_WAIT])
+@test(depends_on_groups=[groups.REPL_INST_CREATE_WAIT],
+      groups=[GROUP, groups.REPL_INST_DELETE_NON_AFFINITY_WAIT],
+      runs_after_groups=[groups.REPL_INST_MULTI_CREATE,
+                         groups.USER_ACTION_DELETE])
 class ReplicationInstDeleteNonAffReplWaitGroup(TestGroup):
     """Wait for Replication Instance Non-Affinity repl to be gone."""
 
     def __init__(self):
         super(ReplicationInstDeleteNonAffReplWaitGroup, self).__init__(
             ReplicationRunnerFactory.instance())
 
@@ -154,15 +164,16 @@
 
     @test(depends_on=[wait_for_delete_non_affinity_repl])
     def delete_non_affinity_master(self):
         """Test deleting non-affinity master."""
         self.test_runner.run_delete_non_affinity_master()
 
 
-@test(depends_on_classes=[ReplicationInstDeleteNonAffReplWaitGroup],
+@test(depends_on_groups=[groups.REPL_INST_DELETE_NON_AFFINITY_WAIT,
+                         groups.REPL_INST_MULTI_CREATE],
       groups=[GROUP, groups.REPL_INST_MULTI_CREATE_WAIT])
 class ReplicationInstMultiCreateWaitGroup(TestGroup):
     """Wait for Replication Instance Multi-Create to complete."""
 
     def __init__(self):
         super(ReplicationInstMultiCreateWaitGroup, self).__init__(
             ReplicationRunnerFactory.instance())
@@ -226,15 +237,15 @@
     @test(depends_on=[wait_for_multiple_replicas],
           runs_after=[eject_valid_master])
     def delete_valid_master(self):
         """Ensure deleting valid master fails."""
         self.test_runner.run_delete_valid_master()
 
 
-@test(depends_on_classes=[ReplicationInstMultiCreateWaitGroup],
+@test(depends_on_groups=[groups.REPL_INST_MULTI_CREATE_WAIT],
       groups=[GROUP, groups.REPL_INST_MULTI_PROMOTE])
 class ReplicationInstMultiPromoteGroup(TestGroup):
     """Test Replication Instance Multi-Promote functionality."""
 
     def __init__(self):
         super(ReplicationInstMultiPromoteGroup, self).__init__(
             ReplicationRunnerFactory.instance())
@@ -284,15 +295,16 @@
 
     @test(depends_on=[verify_data_to_replicate_final])
     def verify_final_data_replicated(self):
         """Verify final data was transferred to all replicas."""
         self.test_runner.run_verify_final_data_replicated()
 
 
-@test(depends_on_classes=[ReplicationInstMultiPromoteGroup],
+@test(depends_on_groups=[groups.REPL_INST_MULTI_CREATE_WAIT],
+      runs_after_groups=[groups.REPL_INST_MULTI_PROMOTE],
       groups=[GROUP, groups.REPL_INST_DELETE])
 class ReplicationInstDeleteGroup(TestGroup):
     """Test Replication Instance Delete functionality."""
 
     def __init__(self):
         super(ReplicationInstDeleteGroup, self).__init__(
             ReplicationRunnerFactory.instance())
@@ -314,15 +326,15 @@
 
     @test(runs_after=[delete_detached_replica])
     def delete_all_replicas(self):
         """Test deleting all the remaining replicas."""
         self.test_runner.run_delete_all_replicas()
 
 
-@test(depends_on_classes=[ReplicationInstDeleteGroup],
+@test(depends_on_groups=[groups.REPL_INST_DELETE],
       groups=[GROUP, groups.REPL_INST_DELETE_WAIT])
 class ReplicationInstDeleteWaitGroup(TestGroup):
     """Wait for Replication Instance Delete to complete."""
 
     def __init__(self):
         super(ReplicationInstDeleteWaitGroup, self).__init__(
             ReplicationRunnerFactory.instance())
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/root_actions_group.py` & `trove-8.0.1/trove/tests/scenario/groups/root_actions_group.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from proboscis import test
 
 from trove.tests.scenario import groups
+from trove.tests.scenario.groups import guest_log_group
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
 GROUP = "scenario.root_actions_group"
 
 
@@ -37,15 +38,15 @@
 
 class BackupRunnerFactory2(test_runners.RunnerFactory):
 
     _runner_ns = 'backup_runners'
     _runner_cls = 'BackupRunner'
 
 
-@test(depends_on_groups=[groups.INST_FORCE_DELETE_WAIT],
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
       groups=[GROUP, groups.ROOT_ACTION_ENABLE])
 class RootActionsEnableGroup(TestGroup):
     """Test Root Actions Enable functionality."""
 
     def __init__(self):
         super(RootActionsEnableGroup, self).__init__(
             RootActionsRunnerFactory.instance())
@@ -73,15 +74,14 @@
     def check_root_enabled(self):
         """Check the root is now enabled."""
         self.test_runner.run_check_root_enabled()
 
     @test(depends_on=[check_root_enabled])
     def backup_root_enabled_instance(self):
         """Backup the root-enabled instance."""
-        self.test_runner.check_inherit_root_state_supported()
         self.backup_runner.run_backup_create()
         self.backup_runner.run_backup_create_completed()
 
     @test(depends_on=[check_root_enabled],
           runs_after=[backup_root_enabled_instance])
     def delete_root(self):
         """Ensure an attempt to delete the root user fails."""
@@ -95,15 +95,15 @@
 
     @test(depends_on=[enable_root_with_password])
     def check_root_still_enabled(self):
         """Check the root is still enabled."""
         self.test_runner.run_check_root_enabled()
 
 
-@test(depends_on_classes=[RootActionsEnableGroup],
+@test(depends_on_groups=[groups.ROOT_ACTION_ENABLE],
       groups=[GROUP, groups.ROOT_ACTION_DISABLE])
 class RootActionsDisableGroup(TestGroup):
     """Test Root Actions Disable functionality."""
 
     def __init__(self):
         super(RootActionsDisableGroup, self).__init__(
             RootActionsRunnerFactory.instance())
@@ -122,21 +122,21 @@
         self.test_runner.check_root_disable_supported()
         self.test_runner.run_check_root_still_enabled_after_disable()
 
     @test(depends_on=[check_root_still_enabled_after_disable])
     def backup_root_disabled_instance(self):
         """Backup the root-disabled instance."""
         self.test_runner.check_root_disable_supported()
-        self.test_runner.check_inherit_root_state_supported()
         self.backup_runner2.run_backup_create()
         self.backup_runner2.run_backup_create_completed()
 
 
-@test(depends_on_classes=[RootActionsDisableGroup],
-      groups=[GROUP, groups.ROOT_ACTION_INST, groups.ROOT_ACTION_INST_CREATE])
+@test(depends_on_groups=[groups.ROOT_ACTION_DISABLE],
+      groups=[GROUP, groups.ROOT_ACTION_INST, groups.ROOT_ACTION_INST_CREATE],
+      runs_after_groups=[groups.INST_ACTIONS_RESIZE_WAIT])
 class RootActionsInstCreateGroup(TestGroup):
     """Test Root Actions Instance Create functionality."""
 
     def __init__(self):
         super(RootActionsInstCreateGroup, self).__init__(
             RootActionsRunnerFactory.instance())
         self.backup_runner = BackupRunnerFactory.instance()
@@ -150,17 +150,18 @@
     @test
     def restore_root_disabled_instance(self):
         """Restore the root-disabled instance."""
         self.test_runner.check_root_disable_supported()
         self.backup_runner2.run_restore_from_backup(suffix='_root_disable')
 
 
-@test(depends_on_classes=[RootActionsInstCreateGroup],
+@test(depends_on_groups=[groups.ROOT_ACTION_INST_CREATE],
       groups=[GROUP, groups.ROOT_ACTION_INST,
-              groups.ROOT_ACTION_INST_CREATE_WAIT])
+              groups.ROOT_ACTION_INST_CREATE_WAIT],
+      runs_after_groups=[guest_log_group.GROUP])
 class RootActionsInstCreateWaitGroup(TestGroup):
     """Wait for Root Actions Instance Create to complete."""
 
     def __init__(self):
         super(RootActionsInstCreateWaitGroup, self).__init__(
             RootActionsRunnerFactory.instance())
         self.backup_runner = BackupRunnerFactory.instance()
@@ -190,15 +191,15 @@
         """Check the root is also enabled on the restored instance."""
         instance_id = self.backup_runner2.restore_instance_id
         root_creds = self.test_runner.restored_root_creds2
         self.test_runner.run_check_root_enabled_after_restore2(
             instance_id, root_creds)
 
 
-@test(depends_on_classes=[RootActionsInstCreateWaitGroup],
+@test(depends_on_groups=[groups.ROOT_ACTION_INST_CREATE_WAIT],
       groups=[GROUP, groups.ROOT_ACTION_INST, groups.ROOT_ACTION_INST_DELETE])
 class RootActionsInstDeleteGroup(TestGroup):
     """Test Root Actions Instance Delete functionality."""
 
     def __init__(self):
         super(RootActionsInstDeleteGroup, self).__init__(
             RootActionsRunnerFactory.instance())
@@ -224,17 +225,18 @@
     @test
     def delete_instance_backup2(self):
         """Delete the root-disabled instance backup."""
         self.test_runner.check_root_disable_supported()
         self.backup_runner2.run_delete_backup()
 
 
-@test(depends_on_classes=[RootActionsInstDeleteGroup],
+@test(depends_on_groups=[groups.ROOT_ACTION_INST_DELETE],
       groups=[GROUP, groups.ROOT_ACTION_INST,
-              groups.ROOT_ACTION_INST_DELETE_WAIT])
+              groups.ROOT_ACTION_INST_DELETE_WAIT],
+      runs_after_groups=[groups.INST_DELETE])
 class RootActionsInstDeleteWaitGroup(TestGroup):
     """Wait for Root Actions Instance Delete to complete."""
 
     def __init__(self):
         super(RootActionsInstDeleteWaitGroup, self).__init__(
             RootActionsRunnerFactory.instance())
         self.backup_runner = BackupRunnerFactory.instance()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/test_group.py` & `trove-8.0.1/trove/tests/scenario/groups/test_group.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,17 +10,19 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import abc
+import six
 
 
-class TestGroup(object, metaclass=abc.ABCMeta):
+@six.add_metaclass(abc.ABCMeta)
+class TestGroup(object):
 
     def __init__(self, test_runner):
         self._test_runner = test_runner
 
     @property
     def test_runner(self):
         return self._test_runner
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/groups/user_actions_group.py` & `trove-8.0.1/trove/tests/scenario/groups/user_actions_group.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 from proboscis import test
 
 from trove.tests.scenario import groups
+from trove.tests.scenario.groups import guest_log_group
 from trove.tests.scenario.groups.test_group import TestGroup
 from trove.tests.scenario.runners import test_runners
 
 
 GROUP = "scenario.user_actions_group"
 
 
@@ -37,15 +38,15 @@
 
 class DatabaseActionsRunnerFactory(test_runners.RunnerFactory):
 
     _runner_ns = 'database_actions_runners'
     _runner_cls = 'DatabaseActionsRunner'
 
 
-@test(depends_on_groups=[groups.ROOT_ACTION_INST_DELETE_WAIT],
+@test(depends_on_groups=[groups.INST_CREATE_WAIT],
       groups=[GROUP, groups.USER_ACTION_CREATE])
 class UserActionsCreateGroup(TestGroup):
     """Test User Actions Create functionality."""
 
     def __init__(self):
         super(UserActionsCreateGroup, self).__init__(
             UserActionsRunnerFactory.instance())
@@ -166,15 +167,15 @@
 
     @test
     def update_system_user(self):
         """Ensure updating a system user fails."""
         self.test_runner.run_system_user_attribute_update()
 
 
-@test(depends_on_classes=[UserActionsCreateGroup],
+@test(depends_on_groups=[groups.USER_ACTION_CREATE],
       groups=[GROUP, groups.USER_ACTION_DELETE])
 class UserActionsDeleteGroup(TestGroup):
     """Test User Actions Delete functionality."""
 
     def __init__(self):
         super(UserActionsDeleteGroup, self).__init__(
             UserActionsRunnerFactory.instance())
@@ -193,15 +194,15 @@
     @test
     def delete_user_databases(self):
         """Delete the user databases."""
         self.database_actions_runner.run_database_delete()
 
 
 @test(groups=[GROUP, groups.USER_ACTION_INST, groups.USER_ACTION_INST_CREATE],
-      depends_on_classes=[UserActionsDeleteGroup])
+      runs_after_groups=[groups.INST_ACTIONS_RESIZE_WAIT])
 class UserActionsInstCreateGroup(TestGroup):
     """Test User Actions Instance Create functionality."""
 
     def __init__(self):
         super(UserActionsInstCreateGroup, self).__init__(
             UserActionsRunnerFactory.instance())
         self.instance_create_runner = InstanceCreateRunnerFactory.instance()
@@ -210,17 +211,18 @@
     def create_initialized_instance(self):
         """Create an instance with initial users."""
         self.instance_create_runner.run_initialized_instance_create(
             with_dbs=False, with_users=True, configuration_id=None,
             create_helper_user=False, name_suffix='_user')
 
 
-@test(depends_on_classes=[UserActionsInstCreateGroup],
+@test(depends_on_groups=[groups.USER_ACTION_INST_CREATE],
       groups=[GROUP, groups.USER_ACTION_INST,
-              groups.USER_ACTION_INST_CREATE_WAIT])
+              groups.USER_ACTION_INST_CREATE_WAIT],
+      runs_after_groups=[guest_log_group.GROUP])
 class UserActionsInstCreateWaitGroup(TestGroup):
     """Wait for User Actions Instance Create to complete."""
 
     def __init__(self):
         super(UserActionsInstCreateWaitGroup, self).__init__(
             UserActionsRunnerFactory.instance())
         self.instance_create_runner = InstanceCreateRunnerFactory.instance()
@@ -232,15 +234,15 @@
 
     @test(depends_on=[wait_for_instances])
     def validate_initialized_instance(self):
         """Validate the user instance data and properties."""
         self.instance_create_runner.run_validate_initialized_instance()
 
 
-@test(depends_on_classes=[UserActionsInstCreateWaitGroup],
+@test(depends_on_groups=[groups.USER_ACTION_INST_CREATE_WAIT],
       groups=[GROUP, groups.USER_ACTION_INST, groups.USER_ACTION_INST_DELETE])
 class UserActionsInstDeleteGroup(TestGroup):
     """Test User Actions Instance Delete functionality."""
 
     def __init__(self):
         super(UserActionsInstDeleteGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
@@ -248,17 +250,18 @@
 
     @test
     def delete_initialized_instance(self):
         """Delete the user instance."""
         self.instance_create_runner.run_initialized_instance_delete()
 
 
-@test(depends_on_classes=[UserActionsInstDeleteGroup],
+@test(depends_on_groups=[groups.USER_ACTION_INST_DELETE],
       groups=[GROUP, groups.USER_ACTION_INST,
-              groups.USER_ACTION_INST_DELETE_WAIT])
+              groups.USER_ACTION_INST_DELETE_WAIT],
+      runs_after_groups=[groups.INST_DELETE])
 class UserActionsInstDeleteWaitGroup(TestGroup):
     """Wait for User Actions Instance Delete to complete."""
 
     def __init__(self):
         super(UserActionsInstDeleteWaitGroup, self).__init__(
             DatabaseActionsRunnerFactory.instance())
         self.instance_create_runner = InstanceCreateRunnerFactory.instance()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/cassandra_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/cassandra_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/couchbase_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/couchbase_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/couchdb_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/couchdb_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/db2_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/db2_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/mariadb_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/mariadb_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/mongodb_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/mongodb_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/mysql_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/mysql_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 from trove.tests.scenario.helpers.sql_helper import SqlHelper
 
 
 class MysqlHelper(SqlHelper):
 
     def __init__(self, expected_override_name, report):
         super(MysqlHelper, self).__init__(expected_override_name, report,
-                                          'mysql+pymysql')
+                                          'mysql')
 
     def get_helper_credentials(self):
         return {'name': 'lite', 'password': 'litepass', 'database': 'firstdb'}
 
     def get_helper_credentials_root(self):
         return {'name': 'root', 'password': 'rootpass'}
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/percona_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/percona_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/postgresql_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/postgresql_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/pxc_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/pxc_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/redis_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/redis_helper.py`

 * *Files 15% similar despite different names*

```diff
@@ -27,62 +27,33 @@
 
         self.key_patterns = ['user_a:%s', 'user_b:%s']
         self.value_pattern = 'id:%s'
         self.label_value = 'value_set'
 
         self._ds_client_cache = dict()
 
-    def get_helper_credentials_root(self):
-        return {'name': '-', 'password': 'rootpass'}
-
     def get_client(self, host, *args, **kwargs):
         # We need to cache the Redis client in order to prevent Error 99
         # (Cannot assign requested address) when working with large data sets.
         # A new client may be created frequently due to how the redirection
         # works (see '_execute_with_redirection').
         # The old (now closed) connections however have to wait for about 60s
         # (TIME_WAIT) before the port can be released.
         # This is a feature of the operating system that helps it dealing with
         # packets that arrive after the connection is closed.
-        #
-        # NOTE(zhaochao): when connecting to Redis server with a password,
-        # current cached client may not updated to use the same password,
-        # connection_kwargs of the ConnectPool object should be checked,
-        # if the new password is different, A new client instance will be
-        # created.
-        recreate_client = True
-
-        # NOTE(zhaochao): Another problem about caching clients is,  when
-        # the 'requirepass' paramter of Redis server is changed, already
-        # connected client can still issue commands. If we want to make sure
-        # old passwords cannot be used to connect to the server, cached
-        # clients shouldn't be used, a new one should be created instead.
-        # We cannot easily tell whether the 'requirepass' paramter is changed.
-        # So we have to always recreate a client when a password is explicitly
-        # specified. The cached client is only used when no password
-        # specified(i.e. we're going to use the default password) and the
-        # cached password is same as the default one.
-        if (host in self._ds_client_cache and 'password' not in kwargs):
-            default_password = self.get_helper_credentials()['password']
-            cached_password = (self._ds_client_cache[host]
-                               .connection_pool
-                               .connection_kwargs.get('password'))
-            if cached_password == default_password:
-                recreate_client = False
-
-        if recreate_client:
+        if host not in self._ds_client_cache:
             self._ds_client_cache[host] = (
                 self.create_client(host, *args, **kwargs))
 
         return self._ds_client_cache[host]
 
     def create_client(self, host, *args, **kwargs):
         user = self.get_helper_credentials()
         password = kwargs.get('password', user['password'])
-        client = redis.Redis(password=password, host=host)
+        client = redis.StrictRedis(password=password, host=host)
         return client
 
     # Add data overrides
     # We use multiple keys to make the Redis backup take longer
     def add_actual_data(self, data_label, data_start, data_size, host,
                         *args, **kwargs):
         test_set = self._get_data_point(host, data_label, *args, **kwargs)
@@ -205,10 +176,10 @@
 
     def get_invalid_groups(self):
         return [{'hz': 600}, {'databases': -1}, {'databases': 'string_value'}]
 
     def ping(self, host, *args, **kwargs):
         try:
             client = self.get_client(host, *args, **kwargs)
-            return client.ping()
+            return client.ping() == 'PONG'
         except Exception:
             return False
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/sql_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/sql_helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,16 +23,15 @@
 class SqlHelper(TestHelper):
 
     """This mixin provides data handling helper functions for SQL datastores.
     """
 
     DATA_COLUMN_NAME = 'value'
 
-    def __init__(self, expected_override_name, report,
-                 protocol="mysql+pymysql", port=None):
+    def __init__(self, expected_override_name, report, protocol, port=None):
         super(SqlHelper, self).__init__(expected_override_name, report)
 
         self.protocol = protocol
         self.port = port
         self.credentials = self.get_helper_credentials()
         self.credentials_root = self.get_helper_credentials_root()
 
@@ -135,16 +134,15 @@
         return client.execute(data_table.select()).fetchall()
 
     def ping(self, host, *args, **kwargs):
         try:
             root_client = self.get_client(host, *args, **kwargs)
             root_client.execute("SELECT 1;")
             return True
-        except Exception as e:
-            print("Failed to execute sql command, error: %s" % str(e))
+        except Exception:
             return False
 
     def get_configuration_value(self, property_name, host, *args, **kwargs):
         client = self.get_client(host, *args, **kwargs)
         cmd = "SHOW GLOBAL VARIABLES LIKE '%s';" % property_name
         row = client.execute(cmd).fetchone()
         return row['Value']
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/test_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/test_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/helpers/vertica_helper.py` & `trove-8.0.1/trove/tests/scenario/helpers/vertica_helper.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/backup_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/backup_runners.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 from trove.tests.scenario.helpers.test_helper import DataType
 from trove.tests.scenario.runners.test_runners import TestRunner
 
 
 class BackupRunner(TestRunner):
 
     def __init__(self):
-        self.TIMEOUT_BACKUP_CREATE = 60 * 60
+        self.TIMEOUT_BACKUP_CREATE = 60 * 30
         self.TIMEOUT_BACKUP_DELETE = 120
 
         super(BackupRunner, self).__init__(timeout=self.TIMEOUT_BACKUP_CREATE)
 
         self.BACKUP_NAME = 'backup_test'
         self.BACKUP_DESC = 'test description'
 
@@ -192,15 +192,15 @@
             else:
                 self.assert_not_equal('FAILED', backup.status,
                                       'Backup status should not be')
                 return False
 
         poll_until(_result_is_active, time_out=self.TIMEOUT_BACKUP_CREATE)
 
-    def run_instance_goes_active(self, expected_states=['BACKUP', 'HEALTHY']):
+    def run_instance_goes_active(self, expected_states=['BACKUP', 'ACTIVE']):
         self._assert_instance_states(self.instance_info.id, expected_states)
 
     def run_backup_list(self):
         backup_list = self.auth_client.backups.list()
         self.assert_backup_list(
             backup_list, self.backup_count_prior_to_create + 1)
 
@@ -221,14 +221,19 @@
 
     def run_backup_list_filter_datastore(self):
         backup_list = self.auth_client.backups.list(
             datastore=self.instance_info.dbaas_datastore)
         self.assert_backup_list(
             backup_list, self.backup_count_for_ds_prior_to_create + 1)
 
+    def run_backup_list_filter_different_datastore(self):
+        backup_list = self.auth_client.backups.list(
+            datastore='Test_Datastore_1')
+        self.assert_backup_list(backup_list, 0)
+
     def run_backup_list_filter_datastore_not_found(
             self, expected_exception=exceptions.NotFound,
             expected_http_code=404):
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.backups.list,
@@ -325,25 +330,25 @@
 
     def run_restore_from_inc_1_backup(self, expected_http_code=200):
         self.restore_inc_1_instance_id = self.assert_restore_from_backup(
             self.backup_inc_1_info.id, suffix='_inc_1',
             expected_http_code=expected_http_code)
 
     def run_restore_from_backup_completed(
-            self, expected_states=['BUILD', 'HEALTHY']):
+            self, expected_states=['BUILD', 'ACTIVE']):
         self.assert_restore_from_backup_completed(
             self.restore_instance_id, expected_states)
         self.restore_host = self.get_instance_host(self.restore_instance_id)
 
     def assert_restore_from_backup_completed(
             self, instance_id, expected_states):
         self._assert_instance_states(instance_id, expected_states)
 
     def run_restore_from_inc_1_backup_completed(
-            self, expected_states=['BUILD', 'HEALTHY']):
+            self, expected_states=['BUILD', 'ACTIVE']):
         self.assert_restore_from_backup_completed(
             self.restore_inc_1_instance_id, expected_states)
         self.restore_inc_1_host = self.get_instance_host(
             self.restore_inc_1_instance_id)
 
     def run_verify_data_in_restored_instance(self):
         self.assert_verify_backup_data(self.restore_host, DataType.large)
@@ -353,15 +358,15 @@
                                             self.databases_before_backup)
 
     def run_verify_data_in_restored_inc_1_instance(self):
         self.assert_verify_backup_data(self.restore_inc_1_host, DataType.large)
         self.assert_verify_backup_data(self.restore_inc_1_host, DataType.tiny)
 
     def run_verify_databases_in_restored_inc_1_instance(self):
-        self.assert_verify_backup_databases(self.restore_inc_1_instance_id,
+        self.assert_verify_backup_databases(self.restore_instance_id,
                                             self.databases_before_backup)
 
     def assert_verify_backup_databases(self, instance_id, expected_databases):
         if expected_databases is not None:
             actual = self._get_databases(instance_id)
             self.assert_list_elements_equal(
                 expected_databases, actual,
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/cluster_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/cluster_runners.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import json
 import os
 
 from proboscis import SkipTest
+import six
 import time as timer
 
 from trove.common import exception
 from trove.common.utils import poll_until
 from trove.tests.scenario.helpers.test_helper import DataType
 from trove.tests.scenario import runners
 from trove.tests.scenario.runners.test_runners import SkipKnownBug
@@ -120,15 +121,15 @@
             self.active_config_group_id = configuration
             self._assert_cluster_values(cluster, expected_task_name)
             for instance in cluster.instances:
                 self.register_debug_inst_ids(instance['id'])
         return cluster.id
 
     def run_cluster_create_wait(self,
-                                expected_instance_states=['BUILD', 'HEALTHY']):
+                                expected_instance_states=['BUILD', 'ACTIVE']):
 
         self.assert_cluster_create_wait(
             self.cluster_id, expected_instance_states=expected_instance_states)
 
     def assert_cluster_create_wait(
             self, cluster_id, expected_instance_states):
         client = self.auth_client
@@ -193,15 +194,15 @@
         self.assert_cluster_restart_wait(self.cluster_id)
 
     def assert_cluster_restart_wait(self, cluster_id):
         client = self.auth_client
         cluster_instances = self._get_cluster_instances(
             client, cluster_id)
         self.assert_all_instance_states(
-            cluster_instances, ['REBOOT', 'HEALTHY'])
+            cluster_instances, ['REBOOT', 'ACTIVE'])
 
         self._assert_cluster_states(
             client, cluster_id, ['NONE'])
         self._assert_cluster_response(
             client, cluster_id, 'NONE')
 
     def assert_cluster_show(self, cluster_id, expected_task_name,
@@ -308,15 +309,15 @@
 
     def run_cluster_grow_wait(self):
         self.assert_cluster_grow_wait(self.cluster_id)
 
     def assert_cluster_grow_wait(self, cluster_id):
         client = self.auth_client
         cluster_instances = self._get_cluster_instances(client, cluster_id)
-        self.assert_all_instance_states(cluster_instances, ['HEALTHY'])
+        self.assert_all_instance_states(cluster_instances, ['ACTIVE'])
 
         self._assert_cluster_states(client, cluster_id, ['NONE'])
         self._assert_cluster_response(client, cluster_id, 'NONE')
 
     def run_add_grow_cluster_data(self, data_type=DataType.tiny2):
         self.assert_add_cluster_data(data_type, self.cluster_id)
 
@@ -340,29 +341,27 @@
         client.clusters.upgrade(
             cluster_id, self.instance_info.dbaas_datastore_version)
         self.assert_client_code(client, expected_http_code)
         self._assert_cluster_response(client, cluster_id, expected_task_name)
 
     def run_cluster_upgrade_wait(self):
         self.assert_cluster_upgrade_wait(
-            self.cluster_id,
-            expected_last_instance_states=['HEALTHY']
-        )
+            self.cluster_id, expected_last_instance_state='ACTIVE')
 
     def assert_cluster_upgrade_wait(self, cluster_id,
-                                    expected_last_instance_states):
+                                    expected_last_instance_state):
         client = self.auth_client
         self._assert_cluster_states(client, cluster_id, ['NONE'])
         cluster_instances = self._get_cluster_instances(client, cluster_id)
         self.assert_equal(
             self.initial_instance_count,
             len(cluster_instances),
             "Unexpected number of instances after upgrade.")
         self.assert_all_instance_states(cluster_instances,
-                                        expected_last_instance_states)
+                                        [expected_last_instance_state])
         self._assert_cluster_response(client, cluster_id, 'NONE')
 
     def run_add_upgrade_cluster_data(self, data_type=DataType.tiny3):
         self.assert_add_cluster_data(data_type, self.cluster_id)
 
     def run_verify_upgrade_cluster_data(self, data_type=DataType.tiny3):
         self.assert_verify_cluster_data(data_type, self.cluster_id)
@@ -408,15 +407,15 @@
         cluster = client.clusters.get(cluster_id)
         self.assert_equal(
             len(self.cluster_removed_instances),
             self.initial_instance_count - len(cluster.instances),
             "Unexpected number of removed nodes.")
 
         cluster_instances = self._get_cluster_instances(client, cluster_id)
-        self.assert_all_instance_states(cluster_instances, ['HEALTHY'])
+        self.assert_all_instance_states(cluster_instances, ['ACTIVE'])
         self.assert_all_gone(self.cluster_removed_instances,
                              expected_last_instance_state)
         self._assert_cluster_response(client, cluster_id, 'NONE')
 
     def run_add_shrink_cluster_data(self, data_type=DataType.tiny4):
         self.assert_add_cluster_data(data_type, self.cluster_id)
 
@@ -512,25 +511,25 @@
         cluster = client.clusters.get(cluster_id)
         self._assert_cluster_values(cluster, expected_task_name,
                                     check_locality=check_locality)
 
     def _assert_cluster_values(self, cluster, expected_task_name,
                                check_locality=True):
         with TypeCheck('Cluster', cluster) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
+            check.has_field("id", six.string_types)
+            check.has_field("name", six.string_types)
             check.has_field("datastore", dict)
             check.has_field("instances", list)
             check.has_field("links", list)
-            check.has_field("created", str)
-            check.has_field("updated", str)
+            check.has_field("created", six.text_type)
+            check.has_field("updated", six.text_type)
             if check_locality:
-                check.has_field("locality", str)
+                check.has_field("locality", six.text_type)
             if self.active_config_group_id:
-                check.has_field("configuration", str)
+                check.has_field("configuration", six.text_type)
             for instance in cluster.instances:
                 isinstance(instance, dict)
                 self.assert_is_not_none(instance['id'])
                 self.assert_is_not_none(instance['links'])
                 self.assert_is_not_none(instance['name'])
         self.assert_equal(expected_task_name, cluster.task['name'],
                           'Unexpected cluster task name')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/configuration_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/configuration_runners.py`

 * *Files 1% similar despite different names*

```diff
@@ -97,15 +97,15 @@
             self, expected_exception=exceptions.NotFound,
             expected_http_code=404):
         self.assert_instance_modify_failure(
             generate_uuid(), generate_uuid(),
             expected_exception, expected_http_code)
 
     def run_detach_group_with_none_attached(self,
-                                            expected_states=['HEALTHY'],
+                                            expected_states=['ACTIVE'],
                                             expected_http_code=202):
         self.assert_instance_modify(
             self.instance_info.id, None,
             expected_states, expected_http_code)
         # run again, just to make sure
         self.assert_instance_modify(
             self.instance_info.id, None,
@@ -135,20 +135,20 @@
             json_def,
             description,
             datastore=self.instance_info.dbaas_datastore,
             datastore_version=self.instance_info.dbaas_datastore_version)
         self.assert_client_code(client, expected_http_code)
 
         with TypeCheck('Configuration', result) as configuration:
-            configuration.has_field('name', str)
-            configuration.has_field('description', str)
+            configuration.has_field('name', basestring)
+            configuration.has_field('description', basestring)
             configuration.has_field('values', dict)
-            configuration.has_field('datastore_name', str)
-            configuration.has_field('datastore_version_id', str)
-            configuration.has_field('datastore_version_name', str)
+            configuration.has_field('datastore_name', basestring)
+            configuration.has_field('datastore_version_id', unicode)
+            configuration.has_field('datastore_version_name', basestring)
 
             self.assert_equal(name, result.name)
             self.assert_equal(description, result.description)
             self.assert_equal(values, result.values)
 
         return result.id
 
@@ -212,42 +212,42 @@
     def assert_configuration_show(self, config_id, config_name):
         result = self.auth_client.configurations.get(config_id)
         self.assert_equal(config_id, result.id, "Unexpected config id")
         self.assert_equal(config_name, result.name, "Unexpected config name")
 
         # check the result field types
         with TypeCheck("configuration", result) as check:
-            check.has_field("id", str)
-            check.has_field("name", str)
-            check.has_field("description", str)
+            check.has_field("id", basestring)
+            check.has_field("name", basestring)
+            check.has_field("description", basestring)
             check.has_field("values", dict)
-            check.has_field("created", str)
-            check.has_field("updated", str)
+            check.has_field("created", basestring)
+            check.has_field("updated", basestring)
             check.has_field("instance_count", int)
 
         # check for valid timestamps
         self.assert_true(self._is_valid_timestamp(result.created),
                          'Created timestamp %s is invalid' % result.created)
         self.assert_true(self._is_valid_timestamp(result.updated),
                          'Updated timestamp %s is invalid' % result.updated)
 
         with CollectionCheck("configuration_values", result.values) as check:
             # check each item has the correct type according to the rules
-            for (item_key, item_val) in result.values.items():
+            for (item_key, item_val) in result.values.iteritems():
                 print("item_key: %s" % item_key)
                 print("item_val: %s" % item_val)
                 param = (
                     self.auth_client.configuration_parameters.get_parameter(
                         self.instance_info.dbaas_datastore,
                         self.instance_info.dbaas_datastore_version,
                         item_key))
                 if param.type == 'integer':
                     check.has_element(item_key, int)
                 if param.type == 'string':
-                    check.has_element(item_key, str)
+                    check.has_element(item_key, basestring)
                 if param.type == 'boolean':
                     check.has_element(item_key, bool)
 
     def _is_valid_timestamp(self, time_string):
         try:
             datetime.strptime(time_string, "%Y-%m-%dT%H:%M:%S")
         except ValueError:
@@ -296,15 +296,15 @@
                           'Unexpected number of configurations found')
         if expected_count:
             conf_instance_ids = [inst.id for inst in conf_instance_list]
             self.assert_true(
                 self.instance_info.id in conf_instance_ids)
 
     def run_attach_dynamic_group(
-            self, expected_states=['HEALTHY'], expected_http_code=202):
+            self, expected_states=['ACTIVE'], expected_http_code=202):
         if self.dynamic_group_id:
             self.assert_instance_modify(
                 self.instance_info.id, self.dynamic_group_id,
                 expected_states, expected_http_code)
 
     def run_verify_dynamic_values(self):
         if self.dynamic_group_id:
@@ -357,16 +357,16 @@
         # The exception here should probably be UnprocessableEntity or
         # something else other than BadRequest as the request really is
         # valid.
         if self.dynamic_group_id:
             self.assert_group_delete_failure(
                 self.dynamic_group_id, expected_exception, expected_http_code)
 
-    def run_update_dynamic_group(self, expected_states=['HEALTHY'],
-                                 expected_http_code=202):
+    def run_update_dynamic_group(
+            self, expected_states=['ACTIVE'], expected_http_code=202):
         if self.dynamic_group_id:
             values = json.dumps(self.test_helper.get_dynamic_group())
             self.assert_update_group(
                 self.instance_info.id, self.dynamic_group_id, values,
                 expected_states, expected_http_code)
 
     def assert_update_group(
@@ -376,15 +376,15 @@
         client.configurations.update(group_id, values)
         self.assert_client_code(client, expected_http_code)
         self.assert_instance_action(instance_id, expected_states)
         if restart_inst:
             self._restart_instance(instance_id)
 
     def run_detach_dynamic_group(
-            self, expected_states=['HEALTHY'], expected_http_code=202):
+            self, expected_states=['ACTIVE'], expected_http_code=202):
         if self.dynamic_group_id:
             self.assert_instance_modify(
                 self.instance_info.id, None,
                 expected_states, expected_http_code)
 
     def run_list_non_dynamic_inst_conf_groups_before(self):
         if self.non_dynamic_group_id:
@@ -443,21 +443,15 @@
                 self.instance_info.id, None, expected_states,
                 expected_http_code, restart_inst=True)
 
     def assert_instance_modify(
             self, instance_id, group_id, expected_states, expected_http_code,
             restart_inst=False):
         client = self.auth_client
-        params = {}
-        if group_id:
-            params['configuration'] = group_id
-        else:
-            params['remove_configuration'] = True
-        client.instances.update(instance_id, **params)
-
+        client.instances.modify(instance_id, configuration=group_id)
         self.assert_client_code(client, expected_http_code)
         self.assert_instance_action(instance_id, expected_states)
 
         # Verify the group has been attached.
         instance = self.get_instance(instance_id)
         if group_id:
             group = self.auth_client.configurations.get(group_id)
@@ -503,15 +497,15 @@
                                     expected_http_code):
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.configurations.delete, group_id)
 
     def _restart_instance(
-            self, instance_id, expected_states=['REBOOT', 'HEALTHY'],
+            self, instance_id, expected_states=['REBOOT', 'ACTIVE'],
             expected_http_code=202):
         client = self.auth_client
         client.instances.restart(instance_id)
         self.assert_client_code(client, expected_http_code)
         self.assert_instance_action(instance_id, expected_states)
 
     def run_create_instance_with_conf(self):
@@ -539,15 +533,15 @@
             configuration=config_id)
         self.assert_client_code(client, 200)
         self.assert_equal("BUILD", result.status, 'Unexpected inst status')
         self.register_debug_inst_ids(result.id)
         return result.id
 
     def run_wait_for_conf_instance(
-            self, expected_states=['BUILD', 'HEALTHY']):
+            self, expected_states=['BUILD', 'ACTIVE']):
         if self.config_inst_id:
             self.assert_instance_action(self.config_inst_id, expected_states)
             self.create_test_helper_on_instance(self.config_inst_id)
             inst = self.auth_client.instances.get(self.config_inst_id)
             self.assert_equal(self.config_id_for_inst,
                               inst.configuration['id'])
         else:
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/database_actions_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/database_actions_runners.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/guest_log_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/guest_log_runners.py`

 * *Files 8% similar despite different names*

```diff
@@ -97,33 +97,30 @@
             value = None
         return value
 
     def assert_log_show(self, client, log_name,
                         expected_http_code=200,
                         expected_type=guest_log.LogType.USER.name,
                         expected_status=guest_log.LogStatus.Disabled.name,
-                        expected_published=None, expected_pending=None,
-                        is_admin=False):
+                        expected_published=None, expected_pending=None):
         self.report.log("Executing log_show for log '%s'" % log_name)
         log_details = client.instances.log_show(
             self.instance_info.id, log_name)
         self.assert_client_code(client, expected_http_code)
         self.assert_log_details(
             log_details, log_name,
             expected_type=expected_type,
             expected_status=expected_status,
             expected_published=expected_published,
-            expected_pending=expected_pending,
-            is_admin=is_admin)
+            expected_pending=expected_pending)
 
     def assert_log_details(self, log_details, expected_log_name,
                            expected_type=guest_log.LogType.USER.name,
                            expected_status=guest_log.LogStatus.Disabled.name,
-                           expected_published=None, expected_pending=None,
-                           is_admin=False):
+                           expected_published=None, expected_pending=None):
         """Check that the action generates the proper response data.
         For log_published and log_pending, setting the value to 'None'
         will skip that check (useful when using an existing instance,
         as there may be pending things in user logs right from the get-go)
         and setting it to a value other than '0' will verify that the actual
         value is '>=value' (since it's impossible to know what the actual
         value will be at any given time). '0' will still match exclusively.
@@ -161,31 +158,26 @@
                               expected_log_name)
         else:
             self.assert_true(log_details.pending >= expected_pending,
                              "Missing log pending for '%s' log: "
                              "expected %d, got %d" %
                              (expected_log_name, expected_pending,
                               log_details.pending))
-
         container = self.container
         prefix = self.prefix_pattern % {
             'instance_id': self.instance_info.id,
             'datastore': CONFIG.dbaas_datastore,
             'log': expected_log_name}
         metafile = prefix.rstrip('/') + '_metafile'
-
         if expected_published == 0:
-            self.assert_storage_gone(container, prefix, metafile,
-                                     is_admin=is_admin)
+            self.assert_storage_gone(container, prefix, metafile)
             container = 'None'
             prefix = 'None'
         else:
-            self.assert_storage_exists(container, prefix, metafile,
-                                       is_admin=is_admin)
-
+            self.assert_storage_exists(container, prefix, metafile)
         self.assert_equal(container, log_details.container,
                           "Wrong log container for '%s' log" %
                           expected_log_name)
         self.assert_equal(prefix, log_details.prefix,
                           "Wrong log prefix for '%s' log" % expected_log_name)
         self.assert_equal(metafile, log_details.metafile,
                           "Wrong log metafile for '%s' log" %
@@ -193,16 +185,16 @@
 
     def assert_log_enable(self, client, log_name,
                           expected_http_code=200,
                           expected_type=guest_log.LogType.USER.name,
                           expected_status=guest_log.LogStatus.Disabled.name,
                           expected_published=None, expected_pending=None):
         self.report.log("Executing log_enable for log '%s'" % log_name)
-        log_details = client.instances.log_action(
-            self.instance_info.id, log_name, enable=True)
+        log_details = client.instances.log_enable(
+            self.instance_info.id, log_name)
         self.assert_client_code(client, expected_http_code)
         self.assert_log_details(
             log_details, log_name,
             expected_type=expected_type,
             expected_status=expected_status,
             expected_published=expected_published,
             expected_pending=expected_pending)
@@ -210,111 +202,97 @@
     def assert_log_disable(self, client, log_name, discard=None,
                            expected_http_code=200,
                            expected_type=guest_log.LogType.USER.name,
                            expected_status=guest_log.LogStatus.Disabled.name,
                            expected_published=None, expected_pending=None):
         self.report.log("Executing log_disable for log '%s' (discard: %s)" %
                         (log_name, discard))
-        log_details = client.instances.log_action(
-            self.instance_info.id, log_name, disable=True, discard=discard)
+        log_details = client.instances.log_disable(
+            self.instance_info.id, log_name, discard=discard)
         self.assert_client_code(client, expected_http_code)
         self.assert_log_details(
             log_details, log_name,
             expected_type=expected_type,
             expected_status=expected_status,
             expected_published=expected_published,
             expected_pending=expected_pending)
 
     def assert_log_publish(self, client, log_name, disable=None, discard=None,
                            expected_http_code=200,
                            expected_type=guest_log.LogType.USER.name,
                            expected_status=guest_log.LogStatus.Disabled.name,
-                           expected_published=None, expected_pending=None,
-                           is_admin=False):
+                           expected_published=None, expected_pending=None):
         self.report.log("Executing log_publish for log '%s' (disable: %s  "
                         "discard: %s)" %
                         (log_name, disable, discard))
-        log_details = client.instances.log_action(
-            self.instance_info.id, log_name, publish=True, disable=disable,
-            discard=discard)
+        log_details = client.instances.log_publish(
+            self.instance_info.id, log_name, disable=disable, discard=discard)
         self.assert_client_code(client, expected_http_code)
         self.assert_log_details(
             log_details, log_name,
             expected_type=expected_type,
             expected_status=expected_status,
             expected_published=expected_published,
-            expected_pending=expected_pending,
-            is_admin=is_admin)
+            expected_pending=expected_pending)
 
     def assert_log_discard(self, client, log_name,
                            expected_http_code=200,
                            expected_type=guest_log.LogType.USER.name,
                            expected_status=guest_log.LogStatus.Disabled.name,
                            expected_published=None, expected_pending=None):
         self.report.log("Executing log_discard for log '%s'" % log_name)
-        log_details = client.instances.log_action(
-            self.instance_info.id, log_name, discard=True)
+        log_details = client.instances.log_discard(
+            self.instance_info.id, log_name)
         self.assert_client_code(client, expected_http_code)
         self.assert_log_details(
             log_details, log_name,
             expected_type=expected_type,
             expected_status=expected_status,
             expected_published=expected_published,
             expected_pending=expected_pending)
 
-    def assert_storage_gone(self, container, prefix, metafile, is_admin=False):
-        if is_admin:
-            swift_client = self.admin_swift_client
-        else:
-            swift_client = self.swift_client
-
+    def assert_storage_gone(self, container, prefix, metafile):
         try:
-            headers, container_files = swift_client.get_container(
+            headers, container_files = self.swift_client.get_container(
                 container, prefix=prefix)
             self.assert_equal(0, len(container_files),
                               "Found files in %s/%s: %s" %
                               (container, prefix, container_files))
         except ClientException as ex:
             if ex.http_status == 404:
                 self.report.log("Container '%s' does not exist" %
                                 container)
                 pass
             else:
                 raise
         try:
-            swift_client.get_object(container, metafile)
+            self.swift_client.get_object(container, metafile)
             self.fail("Found metafile after discard: %s" % metafile)
         except ClientException as ex:
             if ex.http_status == 404:
                 self.report.log("Metafile '%s' gone as expected" %
                                 metafile)
                 pass
             else:
                 raise
 
-    def assert_storage_exists(self, container, prefix, metafile,
-                              is_admin=False):
-        if is_admin:
-            swift_client = self.admin_swift_client
-        else:
-            swift_client = self.swift_client
-
+    def assert_storage_exists(self, container, prefix, metafile):
         try:
-            headers, container_files = swift_client.get_container(
+            headers, container_files = self.swift_client.get_container(
                 container, prefix=prefix)
             self.assert_true(len(container_files) > 0,
                              "No files found in %s/%s" %
                              (container, prefix))
         except ClientException as ex:
             if ex.http_status == 404:
                 self.fail("Container '%s' does not exist" % container)
             else:
                 raise
         try:
-            swift_client.get_object(container, metafile)
+            self.swift_client.get_object(container, metafile)
         except ClientException as ex:
             if ex.http_status == 404:
                 self.fail("Missing metafile: %s" % metafile)
             else:
                 raise
 
     def run_test_log_enable_sys(self,
@@ -326,32 +304,32 @@
             expected_exception, expected_http_code,
             log_name)
 
     def assert_log_enable_fails(self, client,
                                 expected_exception, expected_http_code,
                                 log_name):
         self.assert_raises(expected_exception, expected_http_code,
-                           client, client.instances.log_action,
-                           self.instance_info.id, log_name, enable=True)
+                           client, client.instances.log_enable,
+                           self.instance_info.id, log_name)
 
     def run_test_log_disable_sys(self,
                                  expected_exception=exceptions.BadRequest,
                                  expected_http_code=400):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_disable_fails(
             self.admin_client,
             expected_exception, expected_http_code,
             log_name)
 
     def assert_log_disable_fails(self, client,
                                  expected_exception, expected_http_code,
                                  log_name, discard=None):
         self.assert_raises(expected_exception, expected_http_code,
-                           client, client.instances.log_action,
-                           self.instance_info.id, log_name, disable=True,
+                           client, client.instances.log_disable,
+                           self.instance_info.id, log_name,
                            discard=discard)
 
     def run_test_log_show_unauth_user(self,
                                       expected_exception=exceptions.NotFound,
                                       expected_http_code=404):
         log_name = self._get_exposed_user_log_name()
         self.assert_log_show_fails(
@@ -437,16 +415,16 @@
             log_name)
 
     def assert_log_publish_fails(self, client,
                                  expected_exception, expected_http_code,
                                  log_name,
                                  disable=None, discard=None):
         self.assert_raises(expected_exception, expected_http_code,
-                           client, client.instances.log_action,
-                           self.instance_info.id, log_name, publish=True,
+                           client, client.instances.log_publish,
+                           self.instance_info.id, log_name,
                            disable=disable, discard=discard)
 
     def run_test_log_discard_unexposed_user(
             self, expected_exception=exceptions.BadRequest,
             expected_http_code=400):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_discard_fails(
@@ -454,16 +432,16 @@
             expected_exception, expected_http_code,
             log_name)
 
     def assert_log_discard_fails(self, client,
                                  expected_exception, expected_http_code,
                                  log_name):
         self.assert_raises(expected_exception, expected_http_code,
-                           client, client.instances.log_action,
-                           self.instance_info.id, log_name, discard=True)
+                           client, client.instances.log_discard,
+                           self.instance_info.id, log_name)
 
     def run_test_log_enable_user(self):
         expected_status = guest_log.LogStatus.Ready.name
         expected_pending = 1
         if self.test_helper.log_enable_requires_restart():
             expected_status = guest_log.LogStatus.Restart_Required.name
             # if using an existing instance, there may already be something
@@ -497,21 +475,21 @@
                     expected_published=0, expected_pending=expected_pending)
 
     def run_test_restart_datastore(self, expected_http_code=202):
         if self.test_helper.log_enable_requires_restart():
             instance_id = self.instance_info.id
             # we need to wait until the heartbeat flips the instance
             # back into 'ACTIVE' before we issue the restart command
-            expected_states = ['RESTART_REQUIRED', 'HEALTHY']
+            expected_states = ['RESTART_REQUIRED', 'ACTIVE']
             self.assert_instance_action(instance_id, expected_states)
             client = self.auth_client
             client.instances.restart(instance_id)
             self.assert_client_code(client, expected_http_code)
 
-    def run_test_wait_for_restart(self, expected_states=['REBOOT', 'HEALTHY']):
+    def run_test_wait_for_restart(self, expected_states=['REBOOT', 'ACTIVE']):
         if self.test_helper.log_enable_requires_restart():
             self.assert_instance_action(self.instance_info.id, expected_states)
 
     def run_test_log_publish_user(self):
         for log_name in self._get_exposed_user_log_names():
             self.assert_log_publish(
                 self.auth_client,
@@ -525,15 +503,15 @@
 
     def run_test_verify_data(self):
         self.test_helper.verify_data(DataType.micro, self.get_instance_host())
 
     def run_test_log_publish_again_user(self):
         for log_name in self._get_exposed_user_log_names():
             self.assert_log_publish(
-                self.auth_client,
+                self.admin_client,
                 log_name,
                 expected_status=[guest_log.LogStatus.Published.name,
                                  guest_log.LogStatus.Partial.name],
                 expected_published=self._get_last_log_published(log_name),
                 expected_pending=None)
 
     def run_test_log_generator_user(self):
@@ -544,27 +522,20 @@
                 lines=2, expected_lines=2)
 
     def assert_log_generator(self, client, log_name, publish=False,
                              lines=4, expected_lines=None,
                              swift_client=None):
         self.report.log("Executing log_generator for log '%s' (publish: %s)" %
                         (log_name, publish))
-
-        if publish:
-            client.instances.log_action(self.instance_info.id, log_name,
-                                        publish=True)
-
         log_gen = client.instances.log_generator(
             self.instance_info.id, log_name,
-            lines=lines, swift=swift_client)
+            publish=publish, lines=lines, swift=swift_client)
         log_contents = "".join([chunk for chunk in log_gen()])
-
         self.report.log("Returned %d lines for log '%s': %s" % (
             len(log_contents.splitlines()), log_name, log_contents))
-
         self._set_last_log_contents(log_name, log_contents)
         if expected_lines:
             self.assert_equal(expected_lines,
                               len(log_contents.splitlines()),
                               "Wrong line count for '%s' log" % log_name)
         else:
             self.assert_true(len(log_contents.splitlines()) <= lines,
@@ -622,23 +593,17 @@
         for log_name in self._get_exposed_user_log_names():
             self.assert_test_log_save(self.auth_client, log_name, publish=True)
 
     def assert_test_log_save(self, client, log_name, publish=False):
         # generate the file
         self.report.log("Executing log_save for log '%s' (publish: %s)" %
                         (log_name, publish))
-
-        if publish:
-            client.instances.log_action(self.instance_info.id,
-                                        log_name=log_name,
-                                        publish=True)
-
         with tempfile.NamedTemporaryFile() as temp_file:
             client.instances.log_save(self.instance_info.id,
-                                      log_name=log_name,
+                                      log_name=log_name, publish=publish,
                                       filename=temp_file.name)
             file_contents = operating_system.read_file(temp_file.name)
             # now grab the contents ourselves
             self.assert_log_generator(client, log_name, lines=100000)
             # and compare them
             self.assert_equal(self._get_last_log_contents(log_name),
                               file_contents)
@@ -727,40 +692,34 @@
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_show(
             self.admin_client,
             log_name,
             expected_type=guest_log.LogType.SYS.name,
             expected_status=[guest_log.LogStatus.Ready.name,
                              guest_log.LogStatus.Partial.name],
-            expected_published=0, expected_pending=1,
-            is_admin=True
-        )
+            expected_published=0, expected_pending=1)
 
     def run_test_log_publish_sys(self):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_publish(
             self.admin_client,
             log_name,
             expected_type=guest_log.LogType.SYS.name,
-            expected_status=[guest_log.LogStatus.Partial.name,
-                             guest_log.LogStatus.Published.name],
-            expected_published=1, expected_pending=None,
-            is_admin=True)
+            expected_status=guest_log.LogStatus.Partial.name,
+            expected_published=1, expected_pending=1)
 
     def run_test_log_publish_again_sys(self):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_publish(
             self.admin_client,
             log_name,
             expected_type=guest_log.LogType.SYS.name,
-            expected_status=[guest_log.LogStatus.Partial.name,
-                             guest_log.LogStatus.Published.name],
+            expected_status=guest_log.LogStatus.Partial.name,
             expected_published=self._get_last_log_published(log_name) + 1,
-            expected_pending=None,
-            is_admin=True)
+            expected_pending=1)
 
     def run_test_log_generator_sys(self):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_generator(
             self.admin_client,
             log_name,
             lines=4, expected_lines=4)
@@ -774,15 +733,15 @@
 
     def run_test_log_generator_swift_client_sys(self):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_log_generator(
             self.admin_client,
             log_name, publish=True,
             lines=4, expected_lines=4,
-            swift_client=self.admin_swift_client)
+            swift_client=self.swift_client)
 
     def run_test_log_save_sys(self):
         log_name = self._get_unexposed_sys_log_name()
         self.assert_test_log_save(
             self.admin_client,
             log_name)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_actions_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/instance_actions_runners.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
         self.test_helper.verify_data(DataType.small, host)
 
     def run_remove_test_data(self):
         host = self.get_instance_host(self.instance_info.id)
         self.test_helper.remove_data(DataType.small, host)
 
     def run_instance_restart(
-            self, expected_states=['REBOOT', 'HEALTHY'],
+            self, expected_states=['REBOOT', 'ACTIVE'],
             expected_http_code=202):
         self.assert_instance_restart(self.instance_info.id, expected_states,
                                      expected_http_code)
 
     def assert_instance_restart(self, instance_id, expected_states,
                                 expected_http_code):
         self.report.log("Testing restart on instance: %s" % instance_id)
@@ -62,15 +62,15 @@
         client = self.auth_client
         client.instances.restart(instance_id)
         self.assert_client_code(client, expected_http_code)
         self.assert_instance_action(instance_id, expected_states)
 
     def run_instance_resize_volume(
             self, resize_amount=1,
-            expected_states=['RESIZE', 'HEALTHY'],
+            expected_states=['RESIZE', 'ACTIVE'],
             expected_http_code=202):
         if self.VOLUME_SUPPORT:
             self.assert_instance_resize_volume(
                 self.instance_info.id, resize_amount, expected_states,
                 expected_http_code)
         else:
             raise SkipTest("Volume support is disabled.")
@@ -102,14 +102,14 @@
         self.report.log("Testing resize to '%s' on instance: %s" %
                         (resize_flavor_id, instance_id))
         client = self.auth_client
         client.instances.resize_instance(instance_id, resize_flavor_id)
         self.assert_client_code(client, expected_http_code)
 
     def run_wait_for_instance_resize_flavor(
-            self, expected_states=['RESIZE', 'HEALTHY']):
+            self, expected_states=['RESIZE', 'ACTIVE']):
         self.report.log("Waiting for resize to '%s' on instance: %s" %
                         (self.resize_flavor_id, self.instance_info.id))
         self._assert_instance_states(self.instance_info.id, expected_states)
         instance = self.get_instance(self.instance_info.id)
         self.assert_equal(self.resize_flavor_id, instance.flavor['id'],
                           'Unexpected resize flavor_id')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_create_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/instance_create_runners.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,16 +30,16 @@
         self.init_inst_dbs = None
         self.init_inst_users = None
         self.init_inst_host = None
         self.init_inst_data = None
         self.init_inst_config_group_id = None
         self.config_group_id = None
 
-    def run_empty_instance_create(self, expected_states=['BUILD', 'HEALTHY'],
-                                  expected_http_code=200):
+    def run_empty_instance_create(
+            self, expected_states=['BUILD', 'ACTIVE'], expected_http_code=200):
         name = self.instance_info.name
         flavor = self.get_instance_flavor()
         volume_size = self.instance_info.volume_size
 
         instance_info = self.assert_instance_create(
             name, flavor, volume_size, [], [], None, None,
             CONFIG.dbaas_datastore, CONFIG.dbaas_datastore_version,
@@ -64,15 +64,15 @@
         if group_id:
             self.config_group_id = group_id
         else:
             raise SkipTest("No groups defined.")
 
     def run_initialized_instance_create(
             self, with_dbs=True, with_users=True, configuration_id=None,
-            expected_states=['BUILD', 'HEALTHY'], expected_http_code=200,
+            expected_states=['BUILD', 'ACTIVE'], expected_http_code=200,
             create_helper_user=True, name_suffix='_init'):
         if self.is_using_existing_instance:
             # The user requested to run the tests using an existing instance.
             # We therefore skip any scenarios that involve creating new
             # test instances.
             raise SkipTest("Using an existing instance.")
 
@@ -195,38 +195,40 @@
                 check.volume()
                 self.assert_equal(trove_volume_size,
                                   instance._info['volume']['size'],
                                   "Unexpected Trove volume size")
 
             self.assert_equal(instance_info.name, instance._info['name'],
                               "Unexpected instance name")
-            self.assert_equal(str(flavor.id),
-                              str(instance._info['flavor']['id']),
+            self.assert_equal(flavor.id,
+                              int(instance._info['flavor']['id']),
                               "Unexpected instance flavor")
             self.assert_equal(instance_info.dbaas_datastore,
                               instance._info['datastore']['type'],
                               "Unexpected instance datastore version")
             self.assert_equal(instance_info.dbaas_datastore_version,
                               instance._info['datastore']['version'],
                               "Unexpected instance datastore version")
             self.assert_configuration_group(instance_info.id, configuration_id)
             if locality:
                 self.assert_equal(locality, instance._info['locality'],
                                   "Unexpected locality")
 
         return instance_info
 
-    def run_wait_for_instance(self, expected_states=['BUILD', 'HEALTHY']):
+    def run_wait_for_instance(
+            self, expected_states=['BUILD', 'ACTIVE']):
         instances = [self.instance_info.id]
         self.assert_all_instance_states(instances, expected_states)
         self.instance_info.srv_grp_id = self.assert_server_group_exists(
             self.instance_info.id)
         self.wait_for_test_helpers(self.instance_info)
 
-    def run_wait_for_init_instance(self, expected_states=['BUILD', 'HEALTHY']):
+    def run_wait_for_init_instance(
+            self, expected_states=['BUILD', 'ACTIVE']):
         if self.init_inst_info:
             instances = [self.init_inst_info.id]
             self.assert_all_instance_states(instances, expected_states)
             self.wait_for_test_helpers(self.init_inst_info)
 
     def wait_for_test_helpers(self, inst_info):
         self.report.log("Waiting for helper users and databases to be "
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_delete_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/instance_delete_runners.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_error_create_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/instance_error_create_runners.py`

 * *Files 2% similar despite different names*

```diff
@@ -116,14 +116,7 @@
             delete_ids.append(self.error_inst_id)
         if self.error2_inst_id:
             delete_ids.append(self.error2_inst_id)
         if delete_ids:
             self.assert_all_gone(delete_ids, expected_states[-1])
         else:
             raise SkipTest("Cleanup is not required.")
-
-        # All the neutron ports should be removed.
-        if self.error_inst_id:
-            ports = self.neutron_client.list_ports(
-                name='trove-%s' % self.error_inst_id
-            )
-            self.assert_equal(0, len(ports.get("ports", [])))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_force_delete_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/instance_force_delete_runners.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/instance_upgrade_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/instance_upgrade_runners.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-from oslo_service import loopingcall
 
 from trove.tests.scenario.helpers.test_helper import DataType
 from trove.tests.scenario.runners.test_runners import TestRunner
 
 
 class InstanceUpgradeRunner(TestRunner):
 
@@ -31,38 +30,18 @@
         host = self.get_instance_host(self.instance_info.id)
         self.test_helper.verify_data(DataType.small, host)
 
     def run_remove_test_data(self):
         host = self.get_instance_host(self.instance_info.id)
         self.test_helper.remove_data(DataType.small, host)
 
-    def run_instance_upgrade(self, expected_states=['UPGRADE', 'HEALTHY'],
-                             expected_http_code=202):
+    def run_instance_upgrade(
+            self, expected_states=['UPGRADE', 'ACTIVE'],
+            expected_http_code=202):
         instance_id = self.instance_info.id
         self.report.log("Testing upgrade on instance: %s" % instance_id)
 
         target_version = self.instance_info.dbaas_datastore_version
         client = self.auth_client
         client.instances.upgrade(instance_id, target_version)
         self.assert_client_code(client, expected_http_code)
         self.assert_instance_action(instance_id, expected_states)
-
-        def _wait_for_user_list():
-            try:
-                all_users = self.get_user_names(client, instance_id)
-                self.report.log("Users in the db instance %s: %s" %
-                                (instance_id, all_users))
-            except Exception as e:
-                self.report.log(
-                    "Failed to list users in db instance %s(will continue), "
-                    "error: %s" % (instance_id, str(e))
-                )
-            else:
-                raise loopingcall.LoopingCallDone()
-
-        timer = loopingcall.FixedIntervalWithTimeoutLoopingCall(
-            _wait_for_user_list)
-        try:
-            timer.start(interval=3, timeout=120).wait()
-        except loopingcall.LoopingCallTimeOut:
-            self.fail("Timed out: Cannot list users in the db instance %s"
-                      % instance_id)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/module_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/module_runners.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-import os
+import Crypto.Random
 from proboscis import SkipTest
 import re
 import tempfile
 import time
 
 from troveclient.compat import exceptions
 
@@ -38,52 +38,36 @@
         self.MODULE_CONTENTS_PATTERN = 'Message=%s\n'
         self.MODULE_MESSAGE_PATTERN = 'Hello World from: %s'
         self.MODULE_NAME = 'test_module_1'
         self.MODULE_DESC = 'test description'
         self.MODULE_NEG_CONTENTS = 'contents for negative tests'
         self.MODULE_BINARY_SUFFIX = '_bin_auto'
         self.MODULE_BINARY_SUFFIX2 = self.MODULE_BINARY_SUFFIX + '_2'
-        self.MODULE_BINARY_CONTENTS = os.urandom(20)
-        self.MODULE_BINARY_CONTENTS2 = b'\x00\xFF\xea\x9c\x11\xfeok\xb1\x8ax'
+        self.MODULE_BINARY_CONTENTS = Crypto.Random.new().read(20)
+        self.MODULE_BINARY_CONTENTS2 = '\x00\xFF\xea\x9c\x11\xfeok\xb1\x8ax'
 
         self.module_name_order = [
-            # 0
             {'suffix': self.MODULE_BINARY_SUFFIX,
              'priority': True, 'order': 1},
-            # 1
             {'suffix': self.MODULE_BINARY_SUFFIX2,
              'priority': True, 'order': 2},
-            # 2
             {'suffix': '_hidden_all_tenant_auto_priority',
              'priority': True, 'order': 3},
-            # 3
             {'suffix': '_hidden', 'priority': True, 'order': 4},
-            # 4
             {'suffix': '_auto', 'priority': True, 'order': 5},
-            # 5
             {'suffix': '_live', 'priority': True, 'order': 6},
-            # 6
             {'suffix': '_priority', 'priority': True, 'order': 7},
-            # 7
             {'suffix': '_ds', 'priority': False, 'order': 1},
-            # 8
             {'suffix': '_ds_ver', 'priority': False, 'order': 2},
-            # 9
             {'suffix': '_all_tenant_ds_ver', 'priority': False, 'order': 3},
-            # 10
             {'suffix': '', 'priority': False, 'order': 4},
-            # 11
             {'suffix': '_ds_diff', 'priority': False, 'order': 5},
-            # 12
             {'suffix': '_diff_tenant', 'priority': False, 'order': 6},
-            # 13
             {'suffix': '_full_access', 'priority': False, 'order': 7},
-            # 14
             {'suffix': '_for_update', 'priority': False, 'order': 8},
-            # 15
             {'suffix': '_updated', 'priority': False, 'order': 8},
         ]
 
         self.apply_count = 0
         self.mod_inst_id = None
         self.mod_inst_apply_count = 0
         self.temp_module = None
@@ -91,14 +75,15 @@
         self.reapply_max_upd_date = None
         self._module_type = None
 
         self.test_modules = []
         self.module_count_prior_to_create = 0
         self.module_ds_count_prior_to_create = 0
         self.module_ds_all_count_prior_to_create = 0
+        self.module_all_tenant_count_prior_to_create = 0
         self.module_auto_apply_count_prior_to_create = 0
         self.module_admin_count_prior_to_create = 0
         self.module_other_count_prior_to_create = 0
 
         self.module_create_count = 0
         self.module_ds_create_count = 0
         self.module_ds_all_create_count = 0
@@ -116,20 +101,18 @@
     def _get_test_module(self, index):
         if not self.test_modules or len(self.test_modules) < (index + 1):
             raise SkipTest("Requested module not created")
         return self.test_modules[index]
 
     @property
     def main_test_module(self):
-        # The module named "test_module_1"
         return self._get_test_module(0)
 
     @property
     def update_test_module(self):
-        # The module named "test_module_1_updated"
         return self._get_test_module(1)
 
     @property
     def live_update_test_module(self):
         return self._find_live_update_module()
 
     def build_module_args(self, name_order=None):
@@ -217,15 +200,14 @@
         def _match(mod):
             return mod.id == module_id
         return self._find_module(_match, "Could not find module with id %s" %
                                  module_id)
 
     # Tests start here
     def run_module_delete_existing(self):
-        """Delete all the testing modules if exist."""
         modules = self.admin_client.modules.list()
         for module in modules:
             if module.name.startswith(self.MODULE_NAME):
                 self.admin_client.modules.delete(module.id)
 
     def run_module_create_bad_type(
             self, expected_exception=exceptions.NotFound,
@@ -235,70 +217,64 @@
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, 'invalid-type', self.MODULE_NEG_CONTENTS)
 
     def run_module_create_non_admin_auto(
             self, expected_exception=exceptions.Forbidden,
             expected_http_code=403):
-        """Non-admin cannot create modules by specifying auto_apply."""
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, self.module_type, self.MODULE_NEG_CONTENTS,
             auto_apply=True)
 
     def run_module_create_non_admin_all_tenant(
             self, expected_exception=exceptions.Forbidden,
             expected_http_code=403):
-        """Non-admin cannot create modules by specifying all_tenants."""
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, self.module_type, self.MODULE_NEG_CONTENTS,
             all_tenants=True)
 
     def run_module_create_non_admin_hidden(
             self, expected_exception=exceptions.Forbidden,
             expected_http_code=403):
-        """Non-admin cannot create modules by specifying visible."""
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, self.module_type, self.MODULE_NEG_CONTENTS,
             visible=False)
 
     def run_module_create_non_admin_priority(
             self, expected_exception=exceptions.Forbidden,
             expected_http_code=403):
-        """Non-admin cannot create modules by specifying priority_apply."""
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, self.module_type, self.MODULE_NEG_CONTENTS,
             priority_apply=True)
 
     def run_module_create_non_admin_no_full_access(
             self, expected_exception=exceptions.Forbidden,
             expected_http_code=403):
-        """Non-admin cannot create modules by specifying full_access."""
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, self.module_type, self.MODULE_NEG_CONTENTS,
             full_access=False)
 
     def run_module_create_full_access_with_admin_opt(
             self, expected_exception=exceptions.BadRequest,
             expected_http_code=400):
-        """full_access cannot be used together with auto_apply."""
         client = self.admin_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.create,
             self.MODULE_NAME, self.module_type, self.MODULE_NEG_CONTENTS,
             full_access=True, auto_apply=True)
 
@@ -339,23 +315,23 @@
             self.auth_client.modules.list())
         self.module_ds_count_prior_to_create = len(
             self.auth_client.modules.list(
                 datastore=self.instance_info.dbaas_datastore))
         self.module_ds_all_count_prior_to_create = len(
             self.auth_client.modules.list(
                 datastore=models.Modules.MATCH_ALL_NAME))
+        self.module_all_tenant_count_prior_to_create = len(
+            self.unauth_client.modules.list())
         self.module_auto_apply_count_prior_to_create = len(
             [module for module in self.admin_client.modules.list()
              if module.auto_apply])
         self.module_admin_count_prior_to_create = len(
             self.admin_client.modules.list())
         self.module_other_count_prior_to_create = len(
             self.unauth_client.modules.list())
-
-        # Create module "test_module_1" for datastore "all"
         self.assert_module_create(self.auth_client, 10)
 
     def assert_module_create(self, client, name_order,
                              name=None, module_type=None,
                              contents=None, description=None,
                              all_tenants=False,
                              datastore=None, datastore_version=None,
@@ -380,39 +356,31 @@
             all_tenants=all_tenants,
             datastore=datastore, datastore_version=datastore_version,
             auto_apply=auto_apply,
             live_update=live_update, visible=visible,
             priority_apply=priority_apply,
             apply_order=apply_order,
             full_access=full_access)
-
         username = client.real_client.client.username
-        if username == self.instance_info.user.auth_user:
+        if (('alt' in username and 'admin' not in username) or
+                ('admin' in username and visible)):
             self.module_create_count += 1
             if datastore:
                 if datastore == self.instance_info.dbaas_datastore:
                     self.module_ds_create_count += 1
             else:
                 self.module_ds_all_create_count += 1
-        elif (username != self.instance_info.admin_user.auth_user and
-              username != self.instance_info.user.auth_user):
-            self.module_other_create_count += 1
-        else:
+        elif not visible:
             self.module_admin_create_count += 1
-
+        else:
+            self.module_other_create_count += 1
         if all_tenants and visible:
             self.module_all_tenant_create_count += 1
-            if datastore:
-                if datastore == self.instance_info.dbaas_datastore:
-                    self.module_ds_create_count += 1
-            else:
-                self.module_ds_all_create_count += 1
         if auto_apply and visible:
             self.module_auto_apply_create_count += 1
-
         self.test_modules.append(result)
 
         tenant_id = None
         tenant = models.Modules.MATCH_ALL_NAME
         if not all_tenants:
             tenant, tenant_id = self.get_client_tenant(client)
             # If we find a way to grab the tenant name in the module
@@ -427,19 +395,15 @@
             expected_description=description,
             expected_tenant=tenant,
             expected_tenant_id=tenant_id,
             expected_datastore=datastore,
             expected_datastore_version=datastore_version,
             expected_auto_apply=auto_apply,
             expected_contents=contents,
-            expected_is_admin=(
-                username == self.instance_info.admin_user.auth_user and
-                not full_access
-            )
-        )
+            expected_is_admin=('admin' in username and not full_access))
 
     def validate_module(self, module, validate_all=False,
                         expected_name=None,
                         expected_module_type=None,
                         expected_description=None,
                         expected_tenant=None,
                         expected_tenant_id=None,
@@ -516,15 +480,14 @@
                 self.assert_equal(expected_live_update, module.live_update,
                                   'Unexpected live_update')
             if expected_visible is not None:
                 self.assert_equal(expected_visible, module.visible,
                                   'Unexpected visible')
 
     def run_module_create_for_update(self):
-        # Create module "test_module_1_updated"
         self.assert_module_create(self.auth_client, 14)
 
     def run_module_create_dupe(
             self, expected_exception=exceptions.BadRequest,
             expected_http_code=400):
         client = self.auth_client
         self.assert_raises(
@@ -579,20 +542,15 @@
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.get, self.main_test_module.id)
 
     def run_module_list(self):
         self.assert_module_list(
             self.auth_client,
-            (
-                self.module_count_prior_to_create +
-                self.module_create_count +
-                self.module_all_tenant_create_count
-            )
-        )
+            self.module_count_prior_to_create + self.module_create_count)
 
     def assert_module_list(self, client, expected_count, datastore=None):
         if datastore:
             module_list = client.modules.list(datastore=datastore)
         else:
             module_list = client.modules.list()
         self.assert_equal(expected_count, len(module_list),
@@ -613,15 +571,15 @@
                     expected_priority_apply=test_module.priority_apply,
                     expected_apply_order=test_module.apply_order,
                     expected_is_admin=test_module.is_admin)
 
     def run_module_list_unauth_user(self):
         self.assert_module_list(
             self.unauth_client,
-            (self.module_other_count_prior_to_create +
+            (self.module_all_tenant_count_prior_to_create +
              self.module_all_tenant_create_count +
              self.module_other_create_count))
 
     def run_module_create_admin_all(self):
         self.assert_module_create(
             self.admin_client, 2,
             all_tenants=True,
@@ -702,20 +660,15 @@
             self.admin_client,
             self.main_test_module.id,
             full_access=True)
 
     def run_module_list_again(self):
         self.assert_module_list(
             self.auth_client,
-            (
-                self.module_count_prior_to_create +
-                self.module_create_count +
-                self.module_all_tenant_create_count
-            )
-        )
+            self.module_count_prior_to_create + self.module_create_count)
 
     def run_module_list_ds(self):
         self.assert_module_list(
             self.auth_client,
             self.module_ds_count_prior_to_create + self.module_ds_create_count,
             datastore=self.instance_info.dbaas_datastore)
 
@@ -740,15 +693,15 @@
             self.admin_client,
             (self.module_admin_count_prior_to_create +
              self.module_create_count +
              self.module_admin_create_count +
              self.module_other_create_count))
 
     def run_module_update(self):
-        self.assert_module_update_description(
+        self.assert_module_update(
             self.auth_client,
             self.main_test_module.id,
             description=self.MODULE_DESC + " modified")
 
     def assert_module_update(self, client, module_id, **kwargs):
         result = client.modules.update(module_id, **kwargs)
         found = False
@@ -774,16 +727,16 @@
             self.auth_client,
             self.main_test_module.id,
             contents=self.get_module_contents(self.main_test_module.name))
         self.assert_equal(old_md5, self.main_test_module.md5,
                           "MD5 changed with same contents")
 
     def run_module_update_auto_toggle(self,
-                                      expected_exception=exceptions.NotFound,
-                                      expected_http_code=404):
+                                      expected_exception=exceptions.Forbidden,
+                                      expected_http_code=403):
         module = self._find_auto_apply_module()
         toggle_off_args = {'auto_apply': False}
         toggle_on_args = {'auto_apply': True}
         self.assert_module_toggle(module, toggle_off_args, toggle_on_args,
                                   expected_exception=expected_exception,
                                   expected_http_code=expected_http_code)
 
@@ -804,36 +757,36 @@
             client, module.id, description='Updated by admin')
         # Now set it back
         self.assert_module_update(
             client, module.id, description=module.description,
             **toggle_on_args)
 
     def run_module_update_all_tenant_toggle(
-            self, expected_exception=exceptions.NotFound,
-            expected_http_code=404):
+            self, expected_exception=exceptions.Forbidden,
+            expected_http_code=403):
         module = self._find_all_tenant_module()
         toggle_off_args = {'all_tenants': False}
         toggle_on_args = {'all_tenants': True}
         self.assert_module_toggle(module, toggle_off_args, toggle_on_args,
                                   expected_exception=expected_exception,
                                   expected_http_code=expected_http_code)
 
     def run_module_update_invisible_toggle(
-            self, expected_exception=exceptions.NotFound,
-            expected_http_code=404):
+            self, expected_exception=exceptions.Forbidden,
+            expected_http_code=403):
         module = self._find_invisible_module()
         toggle_off_args = {'visible': True}
         toggle_on_args = {'visible': False}
         self.assert_module_toggle(module, toggle_off_args, toggle_on_args,
                                   expected_exception=expected_exception,
                                   expected_http_code=expected_http_code)
 
     def run_module_update_priority_toggle(
-            self, expected_exception=exceptions.NotFound,
-            expected_http_code=404):
+            self, expected_exception=exceptions.Forbidden,
+            expected_http_code=403):
         module = self._find_priority_apply_module()
         toggle_off_args = {'priority_apply': False}
         toggle_on_args = {'priority_apply': True}
         self.assert_module_toggle(module, toggle_off_args, toggle_on_args,
                                   expected_exception=expected_exception,
                                   expected_http_code=expected_http_code)
 
@@ -852,25 +805,25 @@
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.update,
             self.main_test_module.id, visible=False)
 
     def run_module_update_non_admin_auto_off(
-            self, expected_exception=exceptions.NotFound,
-            expected_http_code=404):
+            self, expected_exception=exceptions.Forbidden,
+            expected_http_code=403):
         module = self._find_auto_apply_module()
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.update, module.id, auto_apply=False)
 
     def run_module_update_non_admin_auto_any(
-            self, expected_exception=exceptions.NotFound,
-            expected_http_code=404):
+            self, expected_exception=exceptions.Forbidden,
+            expected_http_code=403):
         module = self._find_auto_apply_module()
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.update, module.id, description='Upd')
 
     def run_module_update_non_admin_all_tenant(
@@ -978,14 +931,19 @@
                     row.module_md5)
 
     def run_module_query_empty(self):
         self.assert_module_query(
             self.auth_client, self.instance_info.id,
             self.module_auto_apply_count_prior_to_create)
 
+    def run_module_query_after_remove(self):
+        self.assert_module_query(
+            self.auth_client, self.instance_info.id,
+            self.module_auto_apply_count_prior_to_create + 2)
+
     def assert_module_query(self, client, instance_id, expected_count,
                             expected_http_code=200, expected_results=None):
         modquery_list = client.instances.module_query(instance_id)
         self.assert_client_code(client, expected_http_code)
         count = len(modquery_list)
         self.assert_equal(expected_count, count,
                           "Wrong number of modules from query")
@@ -1100,30 +1058,24 @@
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.instances.module_apply,
             self.instance_info.id, [module.id])
 
     def run_module_list_instance_after_apply(self):
         self.assert_module_list_instance(
-            self.auth_client,
-            self.instance_info.id,
-            self.apply_count + self.module_auto_apply_count_prior_to_create
-        )
+            self.auth_client, self.instance_info.id, self.apply_count)
 
     def run_module_apply_another(self):
         self.assert_module_apply(self.auth_client, self.instance_info.id,
                                  self.update_test_module)
         self.apply_count += 1
 
     def run_module_list_instance_after_apply_another(self):
         self.assert_module_list_instance(
-            self.auth_client,
-            self.instance_info.id,
-            self.apply_count + self.module_auto_apply_count_prior_to_create
-        )
+            self.auth_client, self.instance_info.id, self.apply_count)
 
     def run_module_update_after_remove(self):
         name, description, contents, priority, order = (
             self.build_module_args(15))
         self.assert_module_update(
             self.auth_client,
             self.update_test_module.id,
@@ -1138,16 +1090,15 @@
 
     def run_module_instance_count_after_apply(self):
         self.assert_module_instance_count(
             self.auth_client, self.main_test_module.id, 1,
             {self.main_test_module.md5: 1})
 
     def run_module_query_after_apply(self):
-        expected_count = (self.module_auto_apply_count_prior_to_create +
-                          self.apply_count)
+        expected_count = self.module_auto_apply_count_prior_to_create + 2
         expected_results = self.create_default_query_expected_results(
             [self.main_test_module])
         self.assert_module_query(self.auth_client, self.instance_info.id,
                                  expected_count=expected_count,
                                  expected_results=expected_results)
 
     def create_default_query_expected_results(self, modules, is_admin=False):
@@ -1182,16 +1133,15 @@
 
     def run_module_instance_count_after_apply_another(self):
         self.assert_module_instance_count(
             self.auth_client, self.main_test_module.id, 1,
             {self.main_test_module.md5: 1})
 
     def run_module_query_after_apply_another(self):
-        expected_count = (self.module_auto_apply_count_prior_to_create +
-                          self.apply_count)
+        expected_count = self.module_auto_apply_count_prior_to_create + 3
         expected_results = self.create_default_query_expected_results(
             [self.main_test_module, self.update_test_module])
         self.assert_module_query(self.auth_client, self.instance_info.id,
                                  expected_count=expected_count,
                                  expected_results=expected_results)
 
     def run_module_update_not_live(
@@ -1207,18 +1157,15 @@
         module = self.live_update_test_module
         self.assert_module_apply(self.auth_client, self.instance_info.id,
                                  module, expected_is_admin=module.is_admin)
         self.apply_count += 1
 
     def run_module_list_instance_after_apply_live(self):
         self.assert_module_list_instance(
-            self.auth_client,
-            self.instance_info.id,
-            self.apply_count + self.module_auto_apply_count_prior_to_create
-        )
+            self.auth_client, self.instance_info.id, self.apply_count)
 
     def run_module_update_live_update(self):
         module = self.live_update_test_module
         new_contents = self.get_module_contents(name=module.name + '_upd')
         self.assert_module_update(
             self.admin_client,
             module.id,
@@ -1280,23 +1227,21 @@
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.modules.delete, self.main_test_module.id)
 
     def run_module_remove(self):
         self.assert_module_remove(self.auth_client, self.instance_info.id,
                                   self.update_test_module.id)
-        self.apply_count -= 1
 
     def assert_module_remove(self, client, instance_id, module_id,
                              expected_http_code=200):
         client.instances.module_remove(instance_id, module_id)
         self.assert_client_code(client, expected_http_code)
 
-    def run_wait_for_inst_with_mods(self,
-                                    expected_states=['BUILD', 'HEALTHY']):
+    def run_wait_for_inst_with_mods(self, expected_states=['BUILD', 'ACTIVE']):
         self.assert_instance_action(self.mod_inst_id, expected_states)
 
     def run_module_query_after_inst_create(self):
         auto_modules = self._find_all_auto_apply_modules(visible=True)
         expected_count = self.mod_inst_apply_count + len(auto_modules)
         expected_results = self.create_default_query_expected_results(
             [self.main_test_module] + auto_modules)
@@ -1334,20 +1279,20 @@
                     expected_filename = guestagent_utils.build_file_path(
                         temp_dir, contents_name, 'dat')
                     self.assert_equal(expected_filename, filename,
                                       'Unexpected retrieve filename')
                     if 'contents' in expected and expected['contents']:
                         with open(filename, 'rb') as fh:
                             contents = fh.read()
-
-                        expected = expected['contents']
-                        if isinstance(expected, str):
-                            expected = expected.encode()
-
-                        self.assert_equal(expected, contents,
+                        # convert contents into bytearray to work with py27
+                        # and py34
+                        contents = bytes([ord(item) for item in contents])
+                        expected_contents = bytes(
+                            [ord(item) for item in expected['contents']])
+                        self.assert_equal(expected_contents, contents,
                                           "Unexpected contents for %s" %
                                           module_name)
         finally:
             operating_system.remove(temp_dir)
 
     def run_module_query_after_inst_create_admin(self):
         auto_modules = self._find_all_auto_apply_modules()
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/negative_cluster_actions_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/negative_cluster_actions_runners.py`

 * *Files 0% similar despite different names*

```diff
@@ -68,15 +68,15 @@
                            self.instance_info.dbaas_datastore_version,
                            instances=instances_def)
 
 
 class MongodbNegativeClusterActionsRunner(NegativeClusterActionsRunner):
 
     def run_create_constrained_size_cluster(self):
-        super(MongodbNegativeClusterActionsRunner,
+        super(NegativeClusterActionsRunner,
               self).run_create_constrained_size_cluster(min_nodes=3,
                                                         max_nodes=3)
 
 
 class CassandraNegativeClusterActionsRunner(NegativeClusterActionsRunner):
 
     def run_create_constrained_size_cluster(self):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/replication_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/replication_runners.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 class ReplicationRunner(TestRunner):
 
     def __init__(self):
         super(ReplicationRunner, self).__init__()
 
         self.master_id = self.instance_info.id
         self.replica_1_id = 0
+        self.replica_2_id = 0
         self.master_host = self.get_instance_host(self.master_id)
         self.replica_1_host = None
         self.master_backup_count = None
         self.used_data_sets = set()
         self.non_affinity_master_id = None
         self.non_affinity_srv_grp_id = None
         self.non_affinity_repl_id = None
@@ -73,35 +74,32 @@
         self.assert_client_code(client, expected_http_code)
         self.register_debug_inst_ids(self.non_affinity_master_id)
 
     def run_create_single_replica(self, expected_http_code=200):
         self.master_backup_count = len(
             self.auth_client.instances.backups(self.master_id))
         self.replica_1_id = self.assert_replica_create(
-            self.master_id, 'replica1', 1, expected_http_code)[0]
+            self.master_id, 'replica1', 1, expected_http_code)
 
     def assert_replica_create(
             self, master_id, replica_name, replica_count, expected_http_code):
-        # When creating multiple replicas, only one replica info will be
-        # returned, so we should compare the replica set members before and
-        # after the creation to get the correct new replica ids.
-        original_replicas = self._get_replica_set(master_id)
         client = self.auth_client
-        client.instances.create(
+        replica = client.instances.create(
             self.instance_info.name + '_' + replica_name,
-            replica_of=master_id,
+            self.instance_info.dbaas_flavor_href,
+            self.instance_info.volume, replica_of=master_id,
+            datastore=self.instance_info.dbaas_datastore,
+            datastore_version=self.instance_info.dbaas_datastore_version,
             nics=self.instance_info.nics,
             replica_count=replica_count)
         self.assert_client_code(client, expected_http_code)
-        new_replicas = self._get_replica_set(master_id) - original_replicas
-        self.register_debug_inst_ids(new_replicas)
-        return list(new_replicas)
+        self.register_debug_inst_ids(replica.id)
+        return replica.id
 
-    def run_wait_for_single_replica(self,
-                                    expected_states=['BUILD', 'HEALTHY']):
+    def run_wait_for_single_replica(self, expected_states=['BUILD', 'ACTIVE']):
         self.assert_instance_action(self.replica_1_id, expected_states)
         self._assert_is_master(self.master_id, [self.replica_1_id])
         self._assert_is_replica(self.replica_1_id, self.master_id)
         self._assert_locality(self.master_id)
         self.replica_1_host = self.get_instance_host(self.replica_1_id)
 
     def _assert_is_master(self, instance_id, replica_ids):
@@ -111,17 +109,15 @@
         CheckInstance(instance._info).slaves()
         self.assert_true(
             set(replica_ids).issubset(self._get_replica_set(instance_id)))
         self._validate_master(instance_id)
 
     def _get_replica_set(self, master_id):
         instance = self.get_instance(master_id)
-        # Return an empty set before the first replia is created
-        return set([replica['id']
-                    for replica in instance._info.get('replicas', [])])
+        return set([replica['id'] for replica in instance._info['replicas']])
 
     def _assert_is_replica(self, instance_id, master_id):
         client = self.admin_client
         instance = self.get_instance(instance_id, client=client)
         self.assert_client_code(client, 200)
         CheckInstance(instance._info).replica_of()
         self.assert_equal(master_id, instance._info['replica_of']['id'],
@@ -137,49 +133,53 @@
         for replica_id in replica_ids:
             replica = self.get_instance(replica_id)
             self.assert_equal(self.locality, replica.locality,
                               "Unexpected locality for instance '%s'" %
                               replica_id)
 
     def run_wait_for_non_affinity_master(self,
-                                         expected_states=['BUILD', 'HEALTHY']):
+                                         expected_states=['BUILD', 'ACTIVE']):
         self._assert_instance_states(self.non_affinity_master_id,
                                      expected_states)
         self.non_affinity_srv_grp_id = self.assert_server_group_exists(
             self.non_affinity_master_id)
 
     def run_create_non_affinity_replica(self, expected_http_code=200):
         client = self.auth_client
         self.non_affinity_repl_id = client.instances.create(
             self.instance_info.name + '_non-affinity-repl',
+            self.instance_info.dbaas_flavor_href,
+            self.instance_info.volume,
+            datastore=self.instance_info.dbaas_datastore,
+            datastore_version=self.instance_info.dbaas_datastore_version,
             nics=self.instance_info.nics,
             replica_of=self.non_affinity_master_id,
             replica_count=1).id
         self.assert_client_code(client, expected_http_code)
         self.register_debug_inst_ids(self.non_affinity_repl_id)
 
     def run_create_multiple_replicas(self, expected_http_code=200):
-        self.assert_replica_create(self.master_id,
-                                   'replica2', 2, expected_http_code)
+        self.replica_2_id = self.assert_replica_create(
+            self.master_id, 'replica2', 2, expected_http_code)
 
     def run_wait_for_multiple_replicas(
-            self, expected_states=['BUILD', 'HEALTHY']):
+            self, expected_states=['BUILD', 'ACTIVE']):
         replica_ids = self._get_replica_set(self.master_id)
         self.report.log("Waiting for replicas: %s" % replica_ids)
         self.assert_instance_action(replica_ids, expected_states)
         self._assert_is_master(self.master_id, replica_ids)
         for replica_id in replica_ids:
             self._assert_is_replica(replica_id, self.master_id)
         self._assert_locality(self.master_id)
 
     def run_wait_for_non_affinity_replica_fail(
             self, expected_states=['BUILD', 'ERROR']):
         self._assert_instance_states(self.non_affinity_repl_id,
                                      expected_states,
-                                     fast_fail_status=['HEALTHY'])
+                                     fast_fail_status=['ACTIVE'])
 
     def run_delete_non_affinity_repl(self, expected_http_code=202):
         self.assert_delete_instances(
             self.non_affinity_repl_id, expected_http_code=expected_http_code)
 
     def assert_delete_instances(self, instance_ids, expected_http_code):
         instance_ids = (instance_ids if utils.is_collection(instance_ids)
@@ -257,15 +257,15 @@
         client = self.auth_client
         self.assert_raises(
             expected_exception, expected_http_code,
             client, client.instances.delete,
             self.instance_info.id)
 
     def run_promote_to_replica_source(self,
-                                      expected_states=['PROMOTE', 'HEALTHY'],
+                                      expected_states=['PROMOTE', 'ACTIVE'],
                                       expected_http_code=202):
         self.assert_promote_to_replica_source(
             self.replica_1_id, self.instance_info.id, expected_states,
             expected_http_code)
 
     def assert_promote_to_replica_source(
             self, new_master_id, old_master_id,
@@ -302,15 +302,15 @@
         self.assert_verify_replication_data(DataType.tiny2,
                                             self.replica_1_host)
 
     def run_verify_replica_data_new2(self):
         self.assert_verify_replica_data(self.replica_1_id, DataType.tiny2)
 
     def run_promote_original_source(self,
-                                    expected_states=['PROMOTE', 'HEALTHY'],
+                                    expected_states=['PROMOTE', 'ACTIVE'],
                                     expected_http_code=202):
         self.assert_promote_to_replica_source(
             self.instance_info.id, self.replica_1_id, expected_states,
             expected_http_code)
 
     def run_add_final_data_to_replicate(self):
         self.assert_add_replication_data(DataType.tiny3, self.master_host)
@@ -329,15 +329,15 @@
         'helper' class should implement the 'remove_actual_data' method.
         """
         for data_set in self.used_data_sets:
             self.report.log("Removing replicated data set: %s" % data_set)
             self.test_helper.remove_data(data_set, host)
 
     def run_detach_replica_from_source(self,
-                                       expected_states=['DETACH', 'HEALTHY'],
+                                       expected_states=['DETACH', 'ACTIVE'],
                                        expected_http_code=202):
         self.assert_detach_replica_from_source(
             self.instance_info.id, self.replica_1_id,
             expected_states, expected_http_code)
 
     def assert_detach_replica_from_source(
             self, master_id, replica_id, expected_states,
@@ -350,15 +350,15 @@
 
         self._assert_is_master(master_id, other_replica_ids)
         self._assert_is_not_replica(replica_id)
 
     def assert_detach_replica(
             self, replica_id, expected_states, expected_http_code):
         client = self.auth_client
-        client.instances.update(replica_id, detach_replica_source=True)
+        client.instances.edit(replica_id, detach_replica_source=True)
         self.assert_client_code(client, expected_http_code)
         self.assert_instance_action(replica_id, expected_states)
 
     def _assert_is_not_replica(self, instance_id):
         client = self.admin_client
         instance = self.get_instance(instance_id, client=client)
         self.assert_client_code(client, 200)
@@ -457,8 +457,10 @@
 
 
 class PerconaReplicationRunner(MysqlReplicationRunner):
     pass
 
 
 class MariadbReplicationRunner(MysqlReplicationRunner):
-    pass
+
+    def _get_expected_binlog_format(self):
+        return 'STATEMENT'
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/root_actions_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/root_actions_runners.py`

 * *Files 6% similar despite different names*

```diff
@@ -83,30 +83,25 @@
         self.assert_can_connect(instance_id, root_creds)
 
         return root_creds
 
     def assert_can_connect(self, instance_id, test_connect_creds):
         self._assert_connect(instance_id, True, test_connect_creds)
 
-    def _assert_connect(self, instance_id, expected_response,
-                        test_connect_creds):
+    def _assert_connect(
+            self, instance_id, expected_response, test_connect_creds):
         host = self.get_instance_host(instance_id=instance_id)
-        self.report.log(
-            "Pinging instance %s with credentials: %s, database: %s" %
-            (instance_id, test_connect_creds,
-             self.test_helper.credentials.get("database"))
-        )
+        self.report.log("Pinging instance %s with credentials: %s"
+                        % (instance_id, test_connect_creds))
 
         ping_response = self.test_helper.ping(
             host,
             username=test_connect_creds[0],
-            password=test_connect_creds[1],
-            database=self.test_helper.credentials.get("database")
+            password=test_connect_creds[1]
         )
-
         self.assert_equal(expected_response, ping_response)
 
     def run_check_root_enabled(self, expected_http_code=200):
         self.assert_root_enabled(self.instance_info.id, expected_http_code)
 
     def assert_root_enabled(self, instance_id, expected_http_code):
         self._assert_root_state(instance_id, True, expected_http_code,
@@ -121,15 +116,15 @@
                 self.instance_info.id,
                 password, root_credentials['name'],
                 expected_http_code)
         else:
             raise SkipTest("No valid root password defined in %s."
                            % self.test_helper.get_class_name())
 
-    def run_disable_root(self, expected_http_code=204):
+    def run_disable_root(self, expected_http_code=200):
         self.restored_root_creds2 = list(self.current_root_creds)
         self.assert_root_disable(self.instance_info.id, expected_http_code)
 
     def assert_root_disable(self, instance_id, expected_http_code):
         client = self.auth_client
         client.root.delete(instance_id)
         self.assert_client_code(client, expected_http_code)
@@ -178,18 +173,14 @@
         else:
             raise SkipTest("No restored instance.")
 
     def check_root_disable_supported(self):
         """Throw SkipTest if root-disable is not supported."""
         pass
 
-    def check_inherit_root_state_supported(self):
-        """Throw SkipTest if inherting root state is not supported."""
-        pass
-
 
 class PerconaRootActionsRunner(RootActionsRunner):
 
     def check_root_disable_supported(self):
         raise SkipTest("Operation is currently not supported.")
 
 
@@ -236,14 +227,7 @@
         raise SkipTest("Operation is currently not supported.")
 
     def run_enable_root_with_password(self):
         raise SkipTest("Operation is currently not supported.")
 
     def run_delete_root(self):
         raise SkipKnownBug(runners.BUG_WRONG_API_VALIDATION)
-
-
-class RedisRootActionsRunner(RootActionsRunner):
-
-    def check_inherit_root_state_supported(self):
-        raise SkipTest("Redis instances does not inherit root state "
-                       "from backups.")
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/test_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/test_runners.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 
 import datetime
 import inspect
 import json
 import netaddr
 import os
 import proboscis
-import sys
+import six
 import time as timer
 import types
 
 from oslo_config.cfg import NoSuchOptError
 from proboscis import asserts
 import swiftclient
 from troveclient.compat import exceptions
@@ -31,17 +31,17 @@
 from trove.common import cfg
 from trove.common import exception
 from trove.common.strategies.strategy import Strategy
 from trove.common import timeutils
 from trove.common import utils
 from trove.common.utils import poll_until, build_polling_task
 from trove.tests.config import CONFIG
-from trove.tests import util as test_util
 from trove.tests.util.check import AttrCheck
 from trove.tests.util import create_dbaas_client
+from trove.tests.util import create_nova_client
 from trove.tests.util.users import Requirements
 
 CONF = cfg.CONF
 
 TEST_RUNNERS_NS = 'trove.tests.scenario.runners'
 TEST_HELPERS_NS = 'trove.tests.scenario.helpers'
 TEST_HELPER_MODULE_NAME = 'test_helper'
@@ -140,15 +140,15 @@
         if not load_type or load_type in impl.lower():
             try:
                 clazz = Strategy.get_strategy(impl, namespace)
             except ImportError as ie:
                 # Only fail silently if it's something we expect,
                 # such as a missing override class.  Anything else
                 # shouldn't be suppressed.
-                l_msg = str(ie).lower()
+                l_msg = ie.message.lower()
                 if (load_type and load_type not in l_msg) or (
                         'no module named' not in l_msg and
                         'cannot be found' not in l_msg):
                     raise
         return clazz
 
     @classmethod
@@ -177,16 +177,14 @@
         self.volume = None  # The volume the instance will have.
         self.nics = None  # The dict of type/id for nics used on the intance.
         self.user = None  # The user instance who owns the instance.
         self.users = None  # The users created on the instance.
         self.databases = None  # The databases created on the instance.
         self.helper_user = None  # Test helper user if exists.
         self.helper_database = None  # Test helper database if exists.
-        self.admin_user = None
-        self.flavors = None
 
 
 class LogOnFail(type):
 
     """Class to log info on failure.
     This will decorate all methods that start with 'run_' with a log wrapper
     that will do a show and attempt to pull back the guest log on all
@@ -246,53 +244,49 @@
             inst_ids = mcs.get_inst_ids()
             client = mcs.get_client()
             report = mcs.get_report()
             try:
                 return fn(*args, **kwargs)
             except proboscis.SkipTest:
                 raise
-            except Exception:
-                (extype, exvalue, extb) = sys.exc_info()
+            except Exception as test_ex:
                 msg_prefix = "*** LogOnFail: "
                 if inst_ids:
                     report.log(msg_prefix + "Exception detected, "
                                "dumping info for IDs: %s." % inst_ids)
                 else:
                     report.log(msg_prefix + "Exception detected, "
                                "but no instance IDs are registered to log.")
 
-                if CONFIG.instance_log_on_failure:
-                    for inst_id in inst_ids:
-                        try:
-                            client.instances.get(inst_id)
-                        except Exception as ex:
-                            report.log("%s Error in instance show for %s:\n%s"
-                                       % (msg_prefix, inst_id, ex))
-                        try:
-                            client.instances.log_action(inst_id, 'guest',
-                                                        publish=True)
-                            log_gen = client.instances.log_generator(
-                                inst_id, 'guest',
-                                lines=0, swift=None)
-                            log_contents = "".join(
-                                [chunk for chunk in log_gen()])
-                            report.log("%s Guest log for %s:\n%s" %
-                                       (msg_prefix, inst_id, log_contents))
-                        except Exception as ex:
-                            report.log("%s Error in guest log retrieval for "
-                                       "%s:\n%s" % (msg_prefix, inst_id, ex))
+                for inst_id in inst_ids:
+                    try:
+                        client.instances.get(inst_id)
+                    except Exception as ex:
+                        report.log(msg_prefix + "Error in instance show "
+                                   "for %s:\n%s" % (inst_id, ex))
+                    try:
+                        log_gen = client.instances.log_generator(
+                            inst_id, 'guest',
+                            publish=True, lines=0, swift=None)
+                        log_contents = "".join([chunk for chunk in log_gen()])
+                        report.log(msg_prefix + "Guest log for %s:\n%s" %
+                                   (inst_id, log_contents))
+                    except Exception as ex:
+                        report.log(msg_prefix + "Error in guest log "
+                                   "retrieval for %s:\n%s" % (inst_id, ex))
 
                 # Only report on the first error that occurs
                 mcs.reset_inst_ids()
-                raise exvalue.with_traceback(extb)
+                raise test_ex
 
         return wrapper
 
 
-class TestRunner(object, metaclass=LogOnFail):
+@six.add_metaclass(LogOnFail)
+class TestRunner(object):
 
     """
     Base class for all 'Runner' classes.
 
     The Runner classes are those that actually do the work.  The 'Group'
     classes are set up with decorators that control how the tests flow,
     and are used to organized the tests - however they are typically set up
@@ -321,27 +315,24 @@
 
     GUEST_CAST_WAIT_TIMEOUT_SEC = 60
 
     # Here's where the info for the 'main' test instance goes
     instance_info = InstanceTestInfo()
     report = CONFIG.get_report()
 
-    def __init__(self, sleep_time=10, timeout=900):
+    def __init__(self, sleep_time=10, timeout=1200):
         self.def_sleep_time = sleep_time
         self.def_timeout = timeout
 
         self.instance_info.name = "TEST_" + datetime.datetime.strftime(
             timeutils.utcnow(), '%Y_%m_%d__%H_%M_%S')
         self.instance_info.dbaas_datastore = CONFIG.dbaas_datastore
         self.instance_info.dbaas_datastore_version = (
             CONFIG.dbaas_datastore_version)
-        self.instance_info.user = CONFIG.users.find_user_by_name("alt_demo")
-        self.instance_info.admin_user = CONFIG.users.find_user(
-            Requirements(is_admin=True)
-        )
+        self.instance_info.user = CONFIG.users.find_user_by_name('alt_demo')
         if self.VOLUME_SUPPORT:
             self.instance_info.volume_size = CONFIG.get('trove_volume_size', 1)
             self.instance_info.volume = {
                 'size': self.instance_info.volume_size}
         else:
             self.instance_info.volume_size = None
             self.instance_info.volume = None
@@ -351,28 +342,25 @@
             self.instance_info.nics = [{'net-id': shared_network}]
 
         self._auth_client = None
         self._unauth_client = None
         self._admin_client = None
         self._swift_client = None
         self._nova_client = None
-        self._neutron_client = None
         self._test_helper = None
         self._servers = {}
 
         # Attempt to register the main instance.  If it doesn't
         # exist, this will still set the 'report' and 'client' objects
         # correctly in LogOnFail
         inst_ids = []
         if hasattr(self.instance_info, 'id') and self.instance_info.id:
             inst_ids = [self.instance_info.id]
         self.register_debug_inst_ids(inst_ids)
 
-        self.instance_info.flavors = self.nova_client.flavors.list()
-
     @classmethod
     def fail(cls, message):
         asserts.fail(message)
 
     @classmethod
     def assert_is_sublist(cls, sub_list, full_list, message=None):
         if not message:
@@ -407,24 +395,15 @@
         asserts.assert_is_not_none(value, message=message)
 
     @classmethod
     def assert_list_elements_equal(cls, expected, actual, message=None):
         """Assert that two lists contain same elements
         (with same multiplicities) ignoring the element order.
         """
-        # Sorts the elements of a given list, including dictionaries.
-        # For dictionaries sorts based on dictionary key.
-        # example:
-        # [1, 3, 2]             -> [1, 2, 3]
-        # ["b", "a", "c"]       -> ["a", "b", "c"]
-        # [{'b':'y'},{'a':'x'}] -> [{'a':'x'},{'b':'y'}]
-        sort = lambda object: sorted(object, key=lambda e: sorted(e.keys())
-                                     if isinstance(e, dict) else e)
-
-        return cls.assert_equal(sort(expected), sort(actual), message)
+        return cls.assert_equal(sorted(expected), sorted(actual), message)
 
     @classmethod
     def assert_equal(cls, expected, actual, message=None):
         if not message:
             message = 'Unexpected value'
         try:
             message += ": '%s' (expected '%s')." % (actual, expected)
@@ -475,55 +454,38 @@
 
     @property
     def admin_client(self):
         return self._create_admin_client()
 
     def _create_admin_client(self):
         """Create a client from an admin user."""
-        requirements = Requirements(is_admin=True, services=["trove"])
+        requirements = Requirements(is_admin=True, services=["swift"])
         admin_user = CONFIG.users.find_user(requirements)
         return create_dbaas_client(admin_user)
 
     @property
     def swift_client(self):
-        return self._create_swift_client(admin=False)
-
-    @property
-    def admin_swift_client(self):
-        return self._create_swift_client(admin=True)
+        return self._create_swift_client()
 
-    def _create_swift_client(self, admin=True):
-        requirements = Requirements(is_admin=admin, services=["swift"])
+    def _create_swift_client(self):
+        """Create a swift client from the admin user details."""
+        requirements = Requirements(is_admin=True, services=["swift"])
         user = CONFIG.users.find_user(requirements)
         os_options = {'region_name': CONFIG.trove_client_region_name}
         return swiftclient.client.Connection(
-            authurl=CONFIG.auth_url,
+            authurl=CONFIG.nova_client['auth_url'],
             user=user.auth_user,
             key=user.auth_key,
             tenant_name=user.tenant,
-            auth_version='3.0',
+            auth_version='2.0',
             os_options=os_options)
 
     @property
     def nova_client(self):
-        if self._nova_client is None:
-            self._nova_client = test_util.create_nova_client(
-                self.instance_info.admin_user
-            )
-
-        return self._nova_client
-
-    @property
-    def neutron_client(self):
-        if self._neutron_client is None:
-            self._neutron_client = test_util.create_neutron_client(
-                self.instance_info.admin_user
-            )
-
-        return self._neutron_client
+        return create_nova_client(self.instance_info.user)
 
     def register_debug_inst_ids(self, inst_ids):
         """Method to 'register' an instance ID (or list of instance IDs)
         for debug purposes on failure.  Note that values are only appended
         here, not overridden.  The LogOnFail class will handle 'missing' IDs.
         """
         LogOnFail.add_inst_ids(inst_ids)
@@ -538,15 +500,15 @@
         return tenant_name, tenant_id
 
     def assert_raises(self, expected_exception, expected_http_code,
                       client, client_cmd, *cmd_args, **cmd_kwargs):
         if client:
             # Make sure that the client_cmd comes from the same client that
             # was passed in, otherwise asserting the client code may fail.
-            cmd_clz = client_cmd.__self__
+            cmd_clz = client_cmd.im_self
             cmd_clz_name = cmd_clz.__class__.__name__
             client_attrs = [attr[0] for attr in inspect.getmembers(
                             client.real_client)
                             if '__' not in attr[0]]
             match = [getattr(client, a) for a in client_attrs
                      if getattr(client, a).__class__.__name__ == cmd_clz_name]
             self.assert_true(any(match),
@@ -600,27 +562,14 @@
                 else [instance_ids], expected_states)
 
     def assert_client_code(self, client, expected_http_code):
         if client and expected_http_code is not None:
             self.assert_equal(expected_http_code, client.last_http_code,
                               "Unexpected client status code")
 
-    def assert_instance_operating_status(self, instance_id, expected_status):
-        self.report.log(f"Waiting for expected_status ({expected_status}) "
-                        f"for instances: {instance_id}")
-
-        def wait_for_operating_status():
-            instance = self.get_instance(instance_id, self.admin_client)
-            if instance.operating_status == expected_status:
-                return True
-            return False
-
-        poll_until(wait_for_operating_status, sleep_time=self.def_sleep_time,
-                   time_out=self.def_timeout)
-
     def assert_all_instance_states(self, instance_ids, expected_states,
                                    fast_fail_status=None,
                                    require_all_states=False):
         self.report.log("Waiting for states (%s) for instances: %s" %
                         (expected_states, instance_ids))
 
         def _make_fn(inst_id):
@@ -656,23 +605,14 @@
         instance had already acquired before and moves to the next expected
         state.
         """
 
         self.report.log("Waiting for states (%s) for instance: %s" %
                         (expected_states, instance_id))
 
-        # Replace HEALTHY with ACTIVE. This is needed after operating_status
-        # is introduced in Trove.
-        wait_operating_status = False
-        if 'HEALTHY' in expected_states:
-            wait_operating_status = True
-            expected_states.remove('HEALTHY')
-            if 'ACTIVE' not in expected_states:
-                expected_states.append('ACTIVE')
-
         if fast_fail_status is None:
             fast_fail_status = ['ERROR', 'FAILED']
         found = False
         for status in expected_states:
             found_current = self._has_status(
                 instance_id, status, fast_fail_status=fast_fail_status)
             if require_all_states or found or found_current:
@@ -695,17 +635,14 @@
                         % (instance_id, status, self._time_since(start_time)))
                     return False
             else:
                 self.report.log(
                     "Instance state was not '%s', moving to the next expected "
                     "state." % status)
 
-        if found and wait_operating_status:
-            self.assert_instance_operating_status(instance_id, 'HEALTHY')
-
         return found
 
     def _time_since(self, start_time):
         return '%.1fs' % (timer.time() - start_time)
 
     def assert_all_gone(self, instance_ids, expected_last_status):
         self._wait_all_deleted(instance_ids
@@ -757,15 +694,15 @@
             self.report.log(
                 "Instance '%s' still existed after %s."
                 % (instance_id, self._time_since(start_time)))
 
         return False
 
     def _poll_while(self, instance_id, expected_status,
-                    sleep_time=1, time_out=0):
+                    sleep_time=1, time_out=None):
         poll_until(lambda: not self._has_status(instance_id, expected_status),
                    sleep_time=sleep_time, time_out=time_out)
 
     def _has_status(self, instance_id, status, fast_fail_status=None):
         fast_fail_status = fast_fail_status or []
         instance = self.get_instance(instance_id, self.admin_client)
         self.report.log("Polling instance '%s' for state '%s', was '%s'."
@@ -816,43 +753,40 @@
             if sg.id == srv_grp_id:
                 server_group = sg
                 break
         if server_group:
             self.fail("Found left-over server group: %s" % server_group)
 
     def get_instance(self, instance_id, client=None):
-        client = client or self.admin_client
+        client = client or self.auth_client
         return client.instances.get(instance_id)
 
     def extract_ipv4s(self, ips):
         ipv4s = [str(ip) for ip in ips if netaddr.valid_ipv4(ip)]
         if not ipv4s:
             self.fail("No IPV4 ip found")
         return ipv4s
 
     def get_instance_host(self, instance_id=None):
         instance_id = instance_id or self.instance_info.id
         instance = self.get_instance(instance_id)
-        if 'ip' not in instance._info:
-            self.fail('Instance %s with status %s does not have an IP'
-                      ' address.' % (instance_id, instance._info['status']))
         host = self.extract_ipv4s(instance._info['ip'])[0]
         self.report.log("Found host %s for instance %s." % (host, instance_id))
         return host
 
     def build_flavor(self, flavor_id=2, volume_size=1):
         return {"flavorRef": flavor_id, "volume": {"size": volume_size}}
 
     def get_flavor(self, flavor_name):
-        flavor = None
-        for item in self.instance_info.flavors:
-            if item.name == flavor_name:
-                flavor = item
-
-        asserts.assert_is_not_none(flavor)
+        flavors = self.auth_client.find_flavors_by_name(flavor_name)
+        self.assert_equal(
+            1, len(flavors),
+            "Unexpected number of flavors with name '%s' found." % flavor_name)
+        flavor = flavors[0]
+        self.assert_is_not_none(flavor, "Flavor '%s' not found." % flavor_name)
 
         return flavor
 
     def get_instance_flavor(self, fault_num=None):
         name_format = 'instance%s%s_flavor_name'
         default = 'm1.tiny'
         fault_str = ''
@@ -865,15 +799,15 @@
 
         name = name_format % (fault_str, eph_str)
         flavor_name = CONFIG.values.get(name, default)
 
         return self.get_flavor(flavor_name)
 
     def get_flavor_href(self, flavor):
-        return flavor.id
+        return self.auth_client.find_flavor_self_href(flavor)
 
     def copy_dict(self, d, ignored_keys=None):
         return {k: v for k, v in d.items()
                 if not ignored_keys or k not in ignored_keys}
 
     def create_test_helper_on_instance(self, instance_id):
         """Here we add a helper user/database, if any, to a given instance
@@ -1008,20 +942,21 @@
         if 'flavor' not in self.instance:
             self.fail("'flavor' not found in instance.")
         else:
             allowed_attrs = ['id', 'links']
             self.contains_allowed_attrs(
                 self.instance['flavor'], allowed_attrs,
                 msg="Flavor")
+            self.links(self.instance['flavor']['links'])
 
     def datastore(self):
         if 'datastore' not in self.instance:
             self.fail("'datastore' not found in instance.")
         else:
-            allowed_attrs = ['type', 'version', 'version_number']
+            allowed_attrs = ['type', 'version']
             self.contains_allowed_attrs(
                 self.instance['datastore'], allowed_attrs,
                 msg="datastore")
 
     def volume_key_exists(self):
         if 'volume' not in self.instance:
             self.fail("'volume' not found in instance.")
```

### Comparing `trove-21.0.0.0rc2/trove/tests/scenario/runners/user_actions_runners.py` & `trove-8.0.1/trove/tests/scenario/runners/user_actions_runners.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from urllib import parse as urllib_parse
+from six.moves.urllib import parse as urllib_parse
 
 from proboscis import SkipTest
 
 from trove.common import exception
 from trove.common.utils import poll_until
 from trove.tests.scenario import runners
 from trove.tests.scenario.runners.test_runners import SkipKnownBug
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/api/common/test_extensions.py` & `trove-8.0.1/trove/tests/unittests/api/common/test_extensions.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,37 +9,41 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 
-import configparser
+import mock
 import os
-from unittest import mock
-
-import importlib.metadata as importlib_metadata
+import pkg_resources
+from six.moves import configparser as config_parser
 
 import trove
 from trove.common import extensions
+from trove.extensions.routes.account import Account
 from trove.extensions.routes.mgmt import Mgmt
 from trove.extensions.routes.mysql import Mysql
+from trove.extensions.routes.security_group import Security_group
 from trove.tests.unittests import trove_testtools
 
 DEFAULT_EXTENSION_MAP = {
+    'Account': [Account, extensions.ExtensionDescriptor],
     'Mgmt': [Mgmt, extensions.ExtensionDescriptor],
-    'MYSQL': [Mysql, extensions.ExtensionDescriptor]
+    'MYSQL': [Mysql, extensions.ExtensionDescriptor],
+    'SecurityGroup': [Security_group, extensions.ExtensionDescriptor]
 }
 
-INVALID_EXTENSION_MAP = {
-    'mgmt': 'trove.extensions.routes.mgmt:Mgmt',
-    'mysql': 'trove.extensions.routes.mysql:Mysql',
-    'invalid': 'trove.tests.unittests.api.common.'
-               'test_extensions:InvalidExtension'
-}
+EP_TEXT = '''
+account = trove.extensions.routes.account:Account
+mgmt = trove.extensions.routes.mgmt:Mgmt
+mysql = trove.extensions.routes.mysql:Mysql
+security_group = trove.extensions.routes.security_group:Security_group
+invalid = trove.tests.unittests.api.common.test_extensions:InvalidExtension
+'''
 
 
 class InvalidExtension(object):
     def get_name(self):
         return "Invalid"
 
     def get_description(self):
@@ -66,46 +70,35 @@
         super(TestExtensionLoading, self).tearDown()
 
     def _assert_default_extensions(self, ext_list):
         for alias, ext in ext_list.items():
             for clazz in DEFAULT_EXTENSION_MAP[alias]:
                 self.assertIsInstance(ext, clazz, "Improper extension class")
 
-    @mock.patch("stevedore.enabled.EnabledExtensionManager.list_entry_points")
-    def test_default_extensions(self, mock_extensions):
+    @mock.patch("pkg_resources.iter_entry_points")
+    def test_default_extensions(self, mock_iter_eps):
         trove_base = os.path.abspath(os.path.join(
             os.path.dirname(trove.__file__), ".."))
         setup_path = "%s/setup.cfg" % trove_base
         # check if we are running as unit test without module installed
         if os.path.isfile(setup_path):
-            parser = configparser.ConfigParser()
+            parser = config_parser.ConfigParser()
             parser.read(setup_path)
             entry_points = parser.get(
                 'entry_points', extensions.ExtensionManager.EXT_NAMESPACE)
-            test_extensions = list()
-            for entry in entry_points.split('\n')[1:]:
-                name = entry.split("=")[0].strip()
-                value = entry.split("=")[1].strip()
-                test_extensions.append(importlib_metadata.EntryPoint(
-                    name=name,
-                    value=value,
-                    group=extensions.ExtensionManager.EXT_NAMESPACE))
-        mock_extensions.return_value = test_extensions
+            eps = pkg_resources.EntryPoint.parse_group('plugins', entry_points)
+            mock_iter_eps.return_value = eps.values()
         extension_mgr = extensions.ExtensionManager()
         self.assertEqual(sorted(DEFAULT_EXTENSION_MAP.keys()),
                          sorted(extension_mgr.extensions.keys()),
                          "Invalid extension names")
         self._assert_default_extensions(extension_mgr.extensions)
 
-    @mock.patch("stevedore.enabled.EnabledExtensionManager.list_entry_points")
-    def test_invalid_extension(self, mock_extensions):
-        test_extensions = list()
-        for k, v in INVALID_EXTENSION_MAP.items():
-            test_extensions.append(importlib_metadata.EntryPoint(
-                name=k,
-                value=v,
-                group=extensions.ExtensionManager.EXT_NAMESPACE))
-        mock_extensions.return_value = test_extensions
+    @mock.patch("pkg_resources.iter_entry_points")
+    def test_invalid_extension(self, mock_iter_eps):
+        eps = pkg_resources.EntryPoint.parse_group('mock', EP_TEXT)
+        mock_iter_eps.return_value = eps.values()
         extension_mgr = extensions.ExtensionManager()
-        self.assertEqual(2, len(extension_mgr.extensions),
+        self.assertEqual(len(DEFAULT_EXTENSION_MAP.keys()),
+                         len(extension_mgr.extensions),
                          "Loaded invalid extensions")
         self._assert_default_extensions(extension_mgr.extensions)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/api/common/test_limits.py` & `trove-8.0.1/trove/tests/unittests/api/common/test_limits.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,19 +13,19 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 """
 Tests dealing with HTTP rate-limiting.
 """
 
-from http import client as http_client
-import io
-from unittest.mock import Mock, MagicMock, patch
 
+from mock import Mock, MagicMock, patch
 from oslo_serialization import jsonutils
+import six
+from six.moves import http_client
 import webob
 
 from trove.common import limits
 from trove.common.limits import Limit
 from trove.limits.service import LimitsController
 from trove.limits import views
 from trove.quota.models import Quota
@@ -44,15 +44,14 @@
     """Base test suite which provides relevant stubs and time abstraction."""
 
     def setUp(self):
         super(BaseLimitTestSuite, self).setUp()
         self.context = trove_testtools.TroveTestContext(self)
         self.absolute_limits = {"max_instances": 55,
                                 "max_volumes": 100,
-                                "max_ram": 200,
                                 "max_backups": 40}
 
 
 class LimitsControllerTest(BaseLimitTestSuite):
 
     def setUp(self):
         super(LimitsControllerTest, self).setUp()
@@ -111,18 +110,14 @@
             }
         ]
 
         abs_limits = {"instances": Quota(tenant_id=tenant_id,
                                          resource="instances",
                                          hard_limit=100),
 
-                      "ram": Quota(tenant_id=tenant_id,
-                                   resource="ram",
-                                   hard_limit=200),
-
                       "backups": Quota(tenant_id=tenant_id,
                                        resource="backups",
                                        hard_limit=40),
 
                       "volumes": Quota(tenant_id=tenant_id,
                                        resource="volumes",
                                        hard_limit=55)}
@@ -136,15 +131,14 @@
             view = limit_controller.index(req, tenant_id)
 
             expected = {
                 'limits': [
                     {
                         'max_instances': 100,
                         'max_backups': 40,
-                        'max_ram': 200,
                         'verb': 'ABSOLUTE',
                         'max_volumes': 55
                     },
                     {
                         'regex': '.*',
                         'nextAvailable': '2011-07-21T18:17:06Z',
                         'uri': '*',
@@ -309,46 +303,44 @@
         # Test that parse_limits() handles bad units correctly.
         self.assertRaises(ValueError, limits.Limiter.parse_limits,
                           '(GET, *, .*, 20, lightyears)')
 
     def test_multiple_rules(self):
         # Test that parse_limits() handles multiple rules correctly.
         try:
-            result = limits.Limiter.parse_limits(
-                '(get, *, .*, 20, minute);'
-                '(PUT, /foo*, /foo.*, 10, hour);'
-                '(POST, /bar*, /bar.*, 5, second);'
-                '(Say, /derp*, /derp.*, 1, day)'
-            )
+            l = limits.Limiter.parse_limits('(get, *, .*, 20, minute);'
+                                            '(PUT, /foo*, /foo.*, 10, hour);'
+                                            '(POST, /bar*, /bar.*, 5, second);'
+                                            '(Say, /derp*, /derp.*, 1, day)')
         except ValueError as e:
             assert False, str(e)
 
         # Make sure the number of returned limits are correct
-        self.assertEqual(4, len(result))
+        self.assertEqual(4, len(l))
 
         # Check all the verbs...
         expected = ['GET', 'PUT', 'POST', 'SAY']
-        self.assertEqual(expected, [t.verb for t in result])
+        self.assertEqual(expected, [t.verb for t in l])
 
         # ...the URIs...
         expected = ['*', '/foo*', '/bar*', '/derp*']
-        self.assertEqual(expected, [t.uri for t in result])
+        self.assertEqual(expected, [t.uri for t in l])
 
         # ...the regexes...
         expected = ['.*', '/foo.*', '/bar.*', '/derp.*']
-        self.assertEqual(expected, [t.regex for t in result])
+        self.assertEqual(expected, [t.regex for t in l])
 
         # ...the values...
         expected = [20, 10, 5, 1]
-        self.assertEqual(expected, [t.value for t in result])
+        self.assertEqual(expected, [t.value for t in l])
 
         # ...and the units...
         expected = [limits.PER_MINUTE, limits.PER_HOUR,
                     limits.PER_SECOND, limits.PER_DAY]
-        self.assertEqual(expected, [t.unit for t in result])
+        self.assertEqual(expected, [t.unit for t in l])
 
 
 class LimiterTest(BaseLimitTestSuite):
     """
     Tests for the in-memory `limits.Limiter` class.
     """
 
@@ -566,15 +558,15 @@
 class FakeHttplibSocket(object):
     """
     Fake `http_client.HTTPResponse` replacement.
     """
 
     def __init__(self, response_string):
         """Initialize new `FakeHttplibSocket`."""
-        self._buffer = io.BytesIO(response_string)
+        self._buffer = six.BytesIO(response_string)
 
     def makefile(self, _mode, *args):
         """Returns the socket's internal buffer."""
         return self._buffer
 
 
 class FakeHttplibConnection(object):
@@ -602,15 +594,16 @@
         req.method = method
         req.headers = headers
         req.host = self.host
         req.body = body
 
         resp = str(req.get_response(self.app))
         resp = "HTTP/1.0 %s" % resp
-        resp = resp.encode("utf-8")
+        if six.PY3:
+            resp = resp.encode("utf-8")
         sock = FakeHttplibSocket(resp)
         self.http_response = http_client.HTTPResponse(sock)
         self.http_response.begin()
 
     def getresponse(self):
         """Return our generated response from the request."""
         return self.http_response
@@ -800,23 +793,22 @@
                 "value": 10,
                 "verb": "GET",
                 "remaining": 2,
                 "unit": "MINUTE",
                 "resetTime": 1311272226
             }
         ]
-        abs_view = {"instances": 55, "volumes": 100, "backups": 40, 'ram': 200}
+        abs_view = {"instances": 55, "volumes": 100, "backups": 40}
 
         view_data = views.LimitViews(abs_view, rate_limits)
         self.assertIsNotNone(view_data)
 
         data = view_data.data()
         expected = {'limits': [{'max_instances': 55,
                                 'max_backups': 40,
-                                'max_ram': 200,
                                 'verb': 'ABSOLUTE',
                                 'max_volumes': 100},
                                {'regex': '.*',
                                 'nextAvailable': '2011-07-21T18:17:06Z',
                                 'uri': '*',
                                 'value': 10,
                                 'verb': 'POST',
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/api/test_versions.py` & `trove-8.0.1/trove/tests/unittests/api/test_versions.py`

 * *Files 26% similar despite different names*

```diff
@@ -9,18 +9,16 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import Mock
-import webob
+from mock import Mock
 
-from trove.common import cfg
 from trove.tests.unittests import trove_testtools
 from trove.versions import BaseVersion
 from trove.versions import Version
 from trove.versions import VersionDataView
 from trove.versions import VERSIONS
 from trove.versions import VersionsAPI
 from trove.versions import VersionsController
@@ -39,18 +37,14 @@
 
     def setUp(self):
         super(VersionsControllerTest, self).setUp()
         self.controller = VersionsController()
         self.assertIsNotNone(self.controller,
                              "VersionsController instance was None")
 
-    def tearDown(self):
-        super(VersionsControllerTest, self).tearDown()
-        cfg.CONF.clear_override('public_endpoint')
-
     def test_index_json(self):
         request = Mock()
         result = self.controller.index(request)
         self.assertIsNotNone(result,
                              'Result was None')
         result._data = Mock()
         result._data.data_for_json = \
@@ -68,31 +62,14 @@
         self.assertEqual('v1.0', json_data['id'],
                          'Version id is incorrect')
         self.assertEqual('CURRENT', json_data['status'],
                          'Version status is incorrect')
         self.assertEqual('2012-08-01T00:00:00Z', json_data['updated'],
                          'Version updated value is incorrect')
 
-    def test_index_json_with_public_endpoint(self):
-        cfg.CONF.set_override('public_endpoint', "https://example.com:8779")
-        req = webob.Request.blank('/')
-        resp = self.controller.index(req)
-        result = resp.data('application/json')['versions']
-        expected = [
-            {
-                'status': 'CURRENT',
-                'updated': '2012-08-01T00:00:00Z',
-                'id': 'v1.0',
-                'links': [{
-                    'href': 'https://example.com:8779/v1.0/',
-                    'rel': 'self'}]
-            }
-        ]
-        self.assertEqual(expected, result)
-
     def test_show_json(self):
         request = Mock()
         request.url_version = '1.0'
         result = self.controller.show(request)
         self.assertIsNotNone(result,
                              'Result was None')
         json_data = result.data("application/json")
@@ -103,30 +80,14 @@
 
         self.assertEqual('CURRENT', version['status'],
                          "Version status was not 'CURRENT'")
         self.assertEqual('2012-08-01T00:00:00Z', version['updated'],
                          "Version updated was not '2012-08-01T00:00:00Z'")
         self.assertEqual('v1.0', version['id'], "Version id was not 'v1.0'")
 
-    def test_show_json_with_public_endpoint(self):
-        cfg.CONF.set_override('public_endpoint', "https://example.com:8779")
-        req = webob.Request.blank('/')
-        req.url_version = '1.0'
-        resp = self.controller.show(req)
-        result = resp.data('application/json')['version']
-        expected = {
-            'status': 'CURRENT',
-            'updated': '2012-08-01T00:00:00Z',
-            'id': 'v1.0',
-            'links': [{
-                'href': 'https://example.com:8779/',
-                'rel': 'self'}]
-        }
-        self.assertEqual(expected, result)
-
 
 class BaseVersionTestCase(trove_testtools.TestCase):
 
     def setUp(self):
         super(BaseVersionTestCase, self).setUp()
         self.base_version = BaseVersion(id, status, base_url, updated)
         self.assertIsNotNone(self.base_version,
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/backup/test_backup_models.py` & `trove-8.0.1/trove/tests/unittests/backup/test_backup_models.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,40 +9,39 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import datetime
-from unittest.mock import DEFAULT
-from unittest.mock import MagicMock
-from unittest.mock import patch
-
+from mock import DEFAULT
+from mock import MagicMock
+from mock import patch
 from swiftclient.client import ClientException
 
 from trove.backup import models
 from trove.backup import state
 from trove.common import context
 from trove.common import exception
+from trove.common import remote
 from trove.common import timeutils
 from trove.common import utils
 from trove.db.models import DatabaseModelBase
 from trove.instance import models as instance_models
 from trove.taskmanager import api
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 
 def _prep_conf(current_time):
     current_time = str(current_time)
-    _context = context.TroveContext(project_id='TENANT-' + current_time)
+    _context = context.TroveContext(tenant='TENANT-' + current_time)
     instance_id = 'INSTANCE-' + current_time
     return _context, instance_id
 
-
 BACKUP_NAME = 'WORKS'
 BACKUP_NAME_2 = 'IT-WORKS'
 BACKUP_NAME_3 = 'SECOND-LAST-ONE'
 BACKUP_NAME_4 = 'LAST-ONE-FULL'
 BACKUP_NAME_5 = 'LAST-ONE-INCREMENTAL'
 BACKUP_NAME_6 = 'LAST-ONE-DELETED'
 BACKUP_STATE = state.BackupState.NEW
@@ -59,15 +58,15 @@
         self.context, self.instance_id = _prep_conf(timeutils.utcnow())
         self.created = False
 
     def tearDown(self):
         super(BackupCreateTest, self).tearDown()
         if self.created:
             models.DBBackup.find_by(
-                tenant_id=self.context.project_id).delete()
+                tenant_id=self.context.tenant).delete()
 
     @patch.object(api.API, 'get_client', MagicMock(return_value=MagicMock()))
     def test_create(self):
         instance = MagicMock()
         with patch.object(instance_models.BuiltInstance, 'load',
                           return_value=instance):
             instance.validate_can_perform_action = MagicMock(
@@ -173,30 +172,30 @@
                 side_effect=exception.UnprocessableEntity)
             self.assertRaises(exception.UnprocessableEntity,
                               models.Backup.create,
                               self.context, self.instance_id,
                               BACKUP_NAME, BACKUP_DESC)
 
     def test_create_backup_swift_token_invalid(self):
-        instance = MagicMock(cluster_id=None)
+        instance = MagicMock()
         with patch.object(instance_models.BuiltInstance, 'load',
                           return_value=instance):
             instance.validate_can_perform_action = MagicMock(
                 return_value=None)
             with patch.object(models.Backup, 'validate_can_perform_action',
                               return_value=None):
                 with patch.object(models.Backup, 'verify_swift_auth_token',
                                   side_effect=exception.SwiftAuthError):
                     self.assertRaises(exception.SwiftAuthError,
                                       models.Backup.create,
                                       self.context, self.instance_id,
                                       BACKUP_NAME, BACKUP_DESC)
 
     def test_create_backup_datastore_operation_not_supported(self):
-        instance = MagicMock(cluster_id=None)
+        instance = MagicMock()
         with patch.object(instance_models.BuiltInstance, 'load',
                           return_value=instance):
             with patch.object(
                 models.Backup, 'validate_can_perform_action',
                 side_effect=exception.DatastoreOperationNotSupported
             ):
                 self.assertRaises(exception.DatastoreOperationNotSupported,
@@ -271,66 +270,66 @@
 
 
 class BackupORMTest(trove_testtools.TestCase):
     def setUp(self):
         super(BackupORMTest, self).setUp()
         util.init_db()
         self.context, self.instance_id = _prep_conf(timeutils.utcnow())
-        self.backup = models.DBBackup.create(tenant_id=self.context.project_id,
+        self.backup = models.DBBackup.create(tenant_id=self.context.tenant,
                                              name=BACKUP_NAME,
                                              state=BACKUP_STATE,
                                              instance_id=self.instance_id,
                                              deleted=False,
                                              size=2.0,
                                              location=BACKUP_LOCATION)
         self.deleted = False
 
     def tearDown(self):
         super(BackupORMTest, self).tearDown()
         if not self.deleted:
-            models.DBBackup.find_by(tenant_id=self.context.project_id).delete()
+            models.DBBackup.find_by(tenant_id=self.context.tenant).delete()
 
     def test_list(self):
         backups, marker = models.Backup.list(self.context)
         self.assertIsNone(marker)
         self.assertEqual(1, len(backups))
 
     def test_list_for_instance(self):
-        models.DBBackup.create(tenant_id=self.context.project_id,
+        models.DBBackup.create(tenant_id=self.context.tenant,
                                name=BACKUP_NAME_2,
                                state=BACKUP_STATE,
                                instance_id=self.instance_id,
                                size=2.0,
                                deleted=False)
         backups, marker = models.Backup.list_for_instance(self.context,
                                                           self.instance_id)
         self.assertIsNone(marker)
         self.assertEqual(2, len(backups))
 
     def test_get_last_completed(self):
-        models.DBBackup.create(tenant_id=self.context.project_id,
+        models.DBBackup.create(tenant_id=self.context.tenant,
                                name=BACKUP_NAME_3,
                                state=BACKUP_STATE_COMPLETED,
                                instance_id=self.instance_id,
                                size=2.0,
                                deleted=False)
-        models.DBBackup.create(tenant_id=self.context.project_id,
+        models.DBBackup.create(tenant_id=self.context.tenant,
                                name=BACKUP_NAME_4,
                                state=BACKUP_STATE_COMPLETED,
                                instance_id=self.instance_id,
                                size=2.0,
                                deleted=False)
-        models.DBBackup.create(tenant_id=self.context.project_id,
+        models.DBBackup.create(tenant_id=self.context.tenant,
                                name=BACKUP_NAME_5,
                                state=BACKUP_STATE_COMPLETED,
                                instance_id=self.instance_id,
                                parent_id='parent_uuid',
                                size=2.0,
                                deleted=False)
-        models.DBBackup.create(tenant_id=self.context.project_id,
+        models.DBBackup.create(tenant_id=self.context.tenant,
                                name=BACKUP_NAME_6,
                                state=BACKUP_STATE_COMPLETED,
                                instance_id=self.instance_id,
                                size=2.0,
                                deleted=True)
 
         backup = models.Backup.get_last_completed(
@@ -398,82 +397,80 @@
 
         def _set_bad_filename():
             self.backup.location = 'bad'
             self.backup.filename
 
         self.assertRaises(ValueError, _set_bad_filename)
 
-    @patch('trove.common.clients.create_swift_client')
-    def test_check_swift_object_exist_integrity_error(self, mock_swift_client):
-        mock_swift_client.return_value.head_object.return_value = {'etag': ''}
-
-        self.assertRaises(exception.RestoreBackupIntegrityError,
-                          self.backup.check_swift_object_exist,
-                          self.context, True)
-
-    @patch('trove.common.clients.create_swift_client')
-    def test_check_swift_object_exist_client_exception(self,
-                                                       mock_swift_client):
-        mock_swift_client.side_effect = ClientException(
-            self.context.project_id
-        )
-        self.assertRaises(exception.SwiftAuthError,
-                          self.backup.check_swift_object_exist,
-                          self.context)
-
-    @patch('trove.common.clients.create_swift_client')
-    def test_check_swift_object_exist_client_exception_404(self,
-                                                           mock_swift_client):
-        e = ClientException(self.context.project_id)
-        e.http_status = 404
-        mock_swift_client.side_effect = e
-
-        self.assertFalse(self.backup.check_swift_object_exist(self.context))
+    def test_check_swift_object_exist_integrity_error(self):
+        mock_client = MagicMock()
+        mock_client.head_object.return_value = {'etag': ''}
+        with patch.object(remote, 'get_endpoint', return_value=None),\
+            patch.object(remote, 'Connection',
+                         return_value=mock_client):
+            self.assertRaises(exception.RestoreBackupIntegrityError,
+                              self.backup.check_swift_object_exist,
+                              self.context, True)
+
+    def test_check_swift_object_exist_client_exception(self):
+        with patch.object(remote, 'get_endpoint', return_value=None),\
+            patch.object(remote, 'Connection',
+                         side_effect=ClientException(self.context.tenant)):
+            self.assertRaises(exception.SwiftAuthError,
+                              self.backup.check_swift_object_exist,
+                              self.context)
 
-    @patch('trove.common.clients.create_swift_client')
-    def test_swift_auth_token_client_exception(self, mock_swift_client):
-        mock_swift_client.side_effect = ClientException(
-            self.context.project_id
-        )
-
-        self.assertRaises(exception.SwiftAuthError,
-                          models.Backup.verify_swift_auth_token,
-                          self.context)
-
-    @patch('trove.common.clients.create_swift_client')
-    def test_swift_auth_token_no_service_endpoint(self, mock_swift_client):
-        mock_swift_client.side_effect = exception.NoServiceEndpoint
-
-        self.assertRaises(exception.SwiftNotFound,
-                          models.Backup.verify_swift_auth_token,
-                          self.context)
+    def test_check_swift_object_exist_client_exception_404(self):
+        e = ClientException(self.context.tenant)
+        e.http_status = 404
+        with patch.object(remote, 'get_endpoint', return_value=None),\
+            patch.object(remote, 'Connection',
+                         side_effect=e):
+            self.assertFalse(
+                self.backup.check_swift_object_exist(self.context))
+
+    def test_swift_auth_token_client_exception(self):
+        with patch.object(remote, 'get_endpoint', return_value=None),\
+            patch.object(remote, 'Connection',
+                         side_effect=ClientException(self.context.tenant)):
+            self.assertRaises(exception.SwiftAuthError,
+                              models.Backup.verify_swift_auth_token,
+                              self.context)
+
+    def test_swift_auth_token_no_service_endpoint(self):
+        with patch.object(remote, 'get_endpoint', return_value=None),\
+            patch.object(remote, 'Connection',
+                         side_effect=exception.NoServiceEndpoint):
+            self.assertRaises(exception.SwiftNotFound,
+                              models.Backup.verify_swift_auth_token,
+                              self.context)
 
 
 class PaginationTests(trove_testtools.TestCase):
 
     def setUp(self):
         super(PaginationTests, self).setUp()
         util.init_db()
         self.context, self.instance_id = _prep_conf(timeutils.utcnow())
         # Create a bunch of backups
         bkup_info = {
-            'tenant_id': self.context.project_id,
+            'tenant_id': self.context.tenant,
             'state': BACKUP_STATE,
             'instance_id': self.instance_id,
             'size': 2.0,
             'deleted': False
         }
         for backup in range(50):
             bkup_info.update({'name': 'Backup-%s' % backup})
             models.DBBackup.create(**bkup_info)
 
     def tearDown(self):
         super(PaginationTests, self).tearDown()
-        with models.DBBackup.query() as query:
-            query.filter_by(instance_id=self.instance_id).delete()
+        query = models.DBBackup.query()
+        query.filter_by(instance_id=self.instance_id).delete()
 
     def test_pagination_list(self):
         # page one
         backups, marker = models.Backup.list(self.context)
         self.assertEqual(20, marker)
         self.assertEqual(20, len(backups))
         # page two
@@ -510,15 +507,15 @@
 
     def setUp(self):
         super(OrderingTests, self).setUp()
         util.init_db()
         now = timeutils.utcnow()
         self.context, self.instance_id = _prep_conf(now)
         info = {
-            'tenant_id': self.context.project_id,
+            'tenant_id': self.context.tenant,
             'state': BACKUP_STATE,
             'instance_id': self.instance_id,
             'size': 2.0,
             'deleted': False
         }
         four = now - datetime.timedelta(days=4)
         one = now - datetime.timedelta(days=1)
@@ -537,69 +534,24 @@
                             id=utils.generate_uuid(), **info))
         models.DBBackup().db_api.save(
             models.DBBackup(name='two', updated=two,
                             id=utils.generate_uuid(), **info))
 
     def tearDown(self):
         super(OrderingTests, self).tearDown()
-        with models.DBBackup.query() as query:
-            query.filter_by(instance_id=self.instance_id).delete()
+        query = models.DBBackup.query()
+        query.filter_by(instance_id=self.instance_id).delete()
 
     def test_list(self):
         backups, marker = models.Backup.list(self.context)
         self.assertIsNone(marker)
         actual = [b.name for b in backups]
-        expected = ['one', 'two', 'three', 'four']
+        expected = [u'one', u'two', u'three', u'four']
         self.assertEqual(expected, actual)
 
     def test_list_for_instance(self):
         backups, marker = models.Backup.list_for_instance(self.context,
                                                           self.instance_id)
         self.assertIsNone(marker)
         actual = [b.name for b in backups]
-        expected = ['one', 'two', 'three', 'four']
+        expected = [u'one', u'two', u'three', u'four']
         self.assertEqual(expected, actual)
-
-
-class TestBackupStrategy(trove_testtools.TestCase):
-    def setUp(self):
-        super(TestBackupStrategy, self).setUp()
-        util.init_db()
-        self.context, self.instance_id = _prep_conf(timeutils.utcnow())
-
-    def test_create(self):
-        db_backstg = models.BackupStrategy.create(self.context,
-                                                  self.instance_id,
-                                                  'test-container')
-        self.addCleanup(models.BackupStrategy.delete, self.context,
-                        self.context.project_id, self.instance_id)
-
-        self.assertEqual('test-container', db_backstg.swift_container)
-
-    def test_list(self):
-        models.BackupStrategy.create(self.context, self.instance_id,
-                                     'test_list')
-        self.addCleanup(models.BackupStrategy.delete, self.context,
-                        self.context.project_id, self.instance_id)
-
-        db_backstgs = models.BackupStrategy.list(self.context,
-                                                 self.context.project_id,
-                                                 self.instance_id).all()
-
-        self.assertEqual(1, len(db_backstgs))
-        self.assertEqual('test_list', db_backstgs[0].swift_container)
-
-    def test_delete(self):
-        models.BackupStrategy.create(self.context, self.instance_id,
-                                     'test_delete')
-        db_backstgs = models.BackupStrategy.list(self.context,
-                                                 self.context.project_id,
-                                                 self.instance_id).all()
-        self.assertEqual(1, len(db_backstgs))
-
-        models.BackupStrategy.delete(self.context, self.context.project_id,
-                                     self.instance_id)
-
-        db_backstgs = models.BackupStrategy.list(self.context,
-                                                 self.context.project_id,
-                                                 self.instance_id).all()
-        self.assertEqual(0, len(db_backstgs))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cassandra_cluster.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cassandra_cluster.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,18 +9,18 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import ANY
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import ANY
+from mock import MagicMock
+from mock import Mock
+from mock import patch
 
 from trove.cluster import models
 from trove.common.strategies.cluster.experimental.cassandra.api \
     import CassandraCluster
 from trove.common.strategies.cluster.experimental.cassandra.taskmanager \
     import CassandraClusterTasks
 from trove.instance import models as inst_models
@@ -35,22 +35,17 @@
         self.context = trove_testtools.TroveTestContext(self)
 
     def tearDown(self):
         super(ClusterTest, self).tearDown()
 
     @patch.object(inst_models.Instance, 'create')
     @patch.object(quota.QUOTAS, 'check_quotas')
-    @patch.object(models, 'assert_homogeneous_cluster')
-    @patch.object(models, 'validate_instance_nics')
     @patch.object(models, 'validate_instance_flavors')
     @patch.object(models, 'get_required_volume_size', return_value=3)
-    def test_create_cluster_instances(self, get_vol_size, _,
-                                      mock_validate_nics,
-                                      mock_homogeneous_cluster,
-                                      check_quotas,
+    def test_create_cluster_instances(self, get_vol_size, _, check_quotas,
                                       inst_create):
         test_instances = [MagicMock(), MagicMock()]
         num_instances = len(test_instances)
         datastore = Mock(manager='cassandra')
         datastore_version = Mock(manager='cassandra')
 
         with patch.object(CassandraClusterTasks, 'find_cluster_node_ids',
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,24 +9,24 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import Mock
-from unittest.mock import patch
 import uuid
 
+from mock import Mock
+from mock import patch
 from trove.cluster.models import Cluster
 from trove.cluster.models import ClusterTasks
 from trove.cluster.models import DBCluster
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
+from trove.common import remote
 from trove.common.strategies.cluster.experimental.mongodb import (
     api as mongodb_api)
 from trove.common import utils
 from trove.datastore import models as datastore_models
 from trove.instance import models as inst_models
 from trove.instance.models import DBInstance
 from trove.instance.tasks import InstanceTasks
@@ -62,61 +62,61 @@
                                                   self.datastore,
                                                   self.datastore_version)
         self.cluster._server_group_loaded = True
         self.instances = [{'volume_size': 1, 'flavor_id': '1234'},
                           {'volume_size': 1, 'flavor_id': '1234'},
                           {'volume_size': 1, 'flavor_id': '1234'}]
         self.volume_support = CONF.get(self.dv.manager).volume_support
-        self.remote_nova = clients.create_nova_client
+        self.remote_nova = remote.create_nova_client
 
     def tearDown(self):
         super(ClusterTest, self).tearDown()
         CONF.get(self.dv.manager).volume_support = self.volume_support
-        clients.create_nova_client = self.remote_nova
+        remote.create_nova_client = self.remote_nova
 
     def test_create_empty_instances(self):
         self.assertRaises(exception.ClusterNumInstancesNotSupported,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           [],
-                          {}, None, None)
+                          None, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_unequal_flavors(self, mock_client):
         instances = self.instances
         instances[0]['flavor_id'] = '4567'
         self.assertRaises(exception.ClusterFlavorsNotEqual,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_unequal_volumes(self,
                                     mock_client):
         instances = self.instances
         instances[0]['volume_size'] = 2
         flavors = Mock()
         mock_client.return_value.flavors = flavors
         self.assertRaises(exception.ClusterVolumeSizesNotEqual,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_storage_not_specified(self,
                                           mock_client):
         class FakeFlavor(object):
             def __init__(self, flavor_id):
                 self.flavor_id = flavor_id
 
             @property
@@ -135,15 +135,15 @@
         self.assertRaises(exception.LocalStorageNotSpecified,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch('trove.cluster.models.LOG')
     def test_delete_bad_task_status(self, mock_logging):
         self.cluster.db_info.task_status = ClusterTasks.BUILDING_INITIAL
         self.assertRaises(exception.UnprocessableEntity,
                           self.cluster.delete)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_controller.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster_controller.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,17 +11,17 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 import jsonschema
 
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import MagicMock
+from mock import Mock
+from mock import patch
 from testtools.matchers import Is, Equals
 from trove.cluster import models
 from trove.cluster.models import Cluster, DBCluster
 from trove.cluster.service import ClusterController
 from trove.cluster.tasks import ClusterTasks
 from trove.cluster import views
 import trove.common.cfg as cfg
@@ -61,87 +61,35 @@
                 "instances": instances,
                 "locality": self.locality,
             }
         }
         self.add_shard = {
             "add_shard": {}
         }
-        self.grow = {
-            "grow": [
-                {"flavorRef": "7"},
-            ]
-        }
-        self.shrink = {
-            "shrink": [
-                {"id": "e89aa5fd-6b0a-436d-a75c-1545d34d5331"},
-            ]
-        }
-        self.upgrade = {
-            "upgrade": {
-                "datastore_version": "2.4.10"
-            }
-        }
 
     def test_get_schema_create(self):
         schema = self.controller.get_schema('create', self.cluster)
         self.assertIsNotNone(schema)
         self.assertIn('cluster', schema['properties'])
+        self.assertTrue('cluster')
 
     def test_get_schema_action_add_shard(self):
-        schema = self.controller.get_schema('action', self.add_shard)
+        schema = self.controller.get_schema('add_shard', self.add_shard)
         self.assertIsNotNone(schema)
         self.assertIn('add_shard', schema['properties'])
 
-    def test_get_schema_action_grow(self):
-        schema = self.controller.get_schema('action', self.grow)
-        self.assertIsNotNone(schema)
-        self.assertIn('grow', schema['properties'])
-
-    def test_get_schema_action_shrink(self):
-        schema = self.controller.get_schema('action', self.shrink)
-        self.assertIsNotNone(schema)
-        self.assertIn('shrink', schema['properties'])
-
-    def test_get_schema_action_upgrade(self):
-        schema = self.controller.get_schema('action', self.upgrade)
-        self.assertIsNotNone(schema)
-        self.assertIn('upgrade', schema['properties'])
-
-    def test_get_schema_action_invalid(self):
-        schema = self.controller.get_schema('action', {'wow': {}})
-        self.assertIsNotNone(schema)
-        self.assertThat(len(schema.keys()), Is(0))
-
     def test_validate_create(self):
         body = self.cluster
         schema = self.controller.get_schema('create', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
     def test_validate_add_shard(self):
         body = self.add_shard
-        schema = self.controller.get_schema('action', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertTrue(validator.is_valid(body))
-
-    def test_validate_grow(self):
-        body = self.grow
-        schema = self.controller.get_schema('action', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertTrue(validator.is_valid(body))
-
-    def test_validate_shrink(self):
-        body = self.shrink
-        schema = self.controller.get_schema('action', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertTrue(validator.is_valid(body))
-
-    def test_validate_upgrade(self):
-        body = self.upgrade
-        schema = self.controller.get_schema('action', body)
+        schema = self.controller.get_schema('add_shard', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
     def test_validate_create_blankname(self):
         body = self.cluster
         body['cluster']['name'] = "     "
         schema = self.controller.get_schema('create', body)
@@ -176,16 +124,19 @@
         error_messages = [error.message for error in errors]
         error_paths = [error.path.pop() for error in errors]
         self.assertThat(len(errors), Is(1))
         self.assertIn("'$%^&' does not match '^.*[0-9a-zA-Z]+.*$'",
                       error_messages)
         self.assertIn("locality", error_paths)
 
+    @patch.object(Cluster, 'create')
     @patch.object(datastore_models, 'get_datastore_version')
-    def test_create_clusters_disabled(self, mock_get_datastore_version):
+    def test_create_clusters_disabled(self,
+                                      mock_get_datastore_version,
+                                      mock_cluster_create):
         body = self.cluster
         tenant_id = Mock()
         context = trove_testtools.TroveTestContext(self)
 
         req = Mock()
         req.environ = MagicMock()
         req.environ.get = Mock(return_value=context)
@@ -240,15 +191,15 @@
         mock_cluster.datastore_version.manager = 'mongodb'
         mock_cluster_create.return_value = mock_cluster
 
         self.controller.create(req, body, tenant_id)
         mock_cluster_create.assert_called_with(context, 'products',
                                                datastore, datastore_version,
                                                instances, {},
-                                               self.locality, None, None)
+                                               self.locality, None)
 
     @patch.object(Cluster, 'load')
     def test_show_cluster(self,
                           mock_cluster_load):
         tenant_id = Mock()
         id = Mock()
         context = trove_testtools.TroveTestContext(self)
@@ -413,18 +364,18 @@
         req.environ.get = Mock(return_value=context)
 
         cluster = Mock()
         cluster.instances_without_server = [Mock()]
         cluster.datastore_version.manager = 'test_dsv'
         mock_cluster_load.return_value = cluster
 
-        self.assertRaisesRegex(exception.TroveError,
-                               'should have exactly one action specified',
-                               self.controller.action, req,
-                               body, tenant_id, cluster_id)
+        self.assertRaisesRegexp(exception.TroveError,
+                                'should have exactly one action specified',
+                                self.controller.action, req,
+                                body, tenant_id, cluster_id)
 
     @patch.object(models.Cluster, 'load')
     def test_controller_action_no_strategy(self,
                                            mock_cluster_load):
 
         body = {'do_stuff2': {}}
         tenant_id = Mock()
@@ -437,18 +388,18 @@
 
         db_info = DBCluster(ClusterTasks.NONE, id=cluster_id,
                             tenant_id=tenant_id)
         cluster = Cluster(context, db_info, datastore='test_ds',
                           datastore_version='test_dsv')
         mock_cluster_load.return_value = cluster
 
-        self.assertRaisesRegex(exception.TroveError,
-                               'Action do_stuff2 not supported',
-                               self.controller.action, req,
-                               body, tenant_id, cluster_id)
+        self.assertRaisesRegexp(exception.TroveError,
+                                'Action do_stuff2 not supported',
+                                self.controller.action, req,
+                                body, tenant_id, cluster_id)
 
     @patch.object(strategy, 'load_api_strategy')
     @patch.object(models.Cluster, 'load')
     def test_controller_action_found(self,
                                      mock_cluster_load,
                                      mock_cluster_api_strategy):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_models.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster_models.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import Mock, patch
+from mock import Mock, patch
 
 from trove.cluster import models
 from trove.common.strategies.cluster.experimental.mongodb.api import (
     MongoDbCluster)
 from trove.datastore import models as datastore_models
 from trove.instance import models as instance_models
 from trove.tests.unittests import trove_testtools
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_pxc_controller.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster_pxc_controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jsonschema
 
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import MagicMock
+from mock import Mock
+from mock import patch
 from testtools.matchers import Is, Equals
 from trove.cluster import models
 from trove.cluster.models import Cluster
 from trove.cluster.service import ClusterController
 from trove.cluster import views
 import trove.common.cfg as cfg
 from trove.common import exception
@@ -56,15 +56,15 @@
             }
         }
 
     def test_get_schema_create(self):
         schema = self.controller.get_schema('create', self.cluster)
         self.assertIsNotNone(schema)
         self.assertIn('cluster', schema['properties'])
-        self.assertIsNotNone(schema['properties']['cluster'])
+        self.assertTrue('cluster')
 
     def test_validate_create(self):
         body = self.cluster
         schema = self.controller.get_schema('create', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
@@ -155,15 +155,15 @@
         mock_cluster.instances_without_server = []
         mock_cluster.datastore_version.manager = 'pxc'
         mock_cluster_create.return_value = mock_cluster
 
         self.controller.create(req, body, tenant_id)
         mock_cluster_create.assert_called_with(context, 'products',
                                                datastore, datastore_version,
-                                               instances, {}, None, None, None)
+                                               instances, {}, None, None)
 
     @patch.object(Cluster, 'load')
     def test_show_cluster(self,
                           mock_cluster_load):
         tenant_id = Mock()
         id = Mock()
         context = trove_testtools.TroveTestContext(self)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_redis_controller.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster_redis_controller.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jsonschema
 
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import MagicMock
+from mock import Mock
+from mock import patch
 from testtools.matchers import Is, Equals
 from trove.cluster import models
 from trove.cluster.models import Cluster
 from trove.cluster.service import ClusterController
 from trove.cluster import views
 import trove.common.cfg as cfg
 from trove.common import exception
@@ -70,15 +70,15 @@
             }
         }
 
     def test_get_schema_create(self):
         schema = self.controller.get_schema('create', self.cluster)
         self.assertIsNotNone(schema)
         self.assertIn('cluster', schema['properties'])
-        self.assertIsNotNone(schema['properties']['cluster'])
+        self.assertTrue('cluster')
 
     def test_validate_create(self):
         body = self.cluster
         schema = self.controller.get_schema('create', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
@@ -120,20 +120,20 @@
         req.environ = MagicMock()
         req.environ.get = Mock(return_value=context)
 
         datastore_version = Mock()
         datastore_version.manager = 'mysql'
         mock_get_datastore_version.return_value = (Mock(), datastore_version)
 
-        self.assertRaisesRegex(exception.ClusterDatastoreNotSupported,
-                               "Clusters not supported for",
-                               self.controller.create,
-                               req,
-                               body,
-                               tenant_id)
+        self.assertRaisesRegexp(exception.ClusterDatastoreNotSupported,
+                                "Clusters not supported for",
+                                self.controller.create,
+                                req,
+                                body,
+                                tenant_id)
 
     @patch.object(Cluster, 'create')
     @patch.object(utils, 'get_id_from_href')
     @patch.object(datastore_models, 'get_datastore_version')
     def test_create_clusters(self,
                              mock_get_datastore_version,
                              mock_id_from_href,
@@ -192,15 +192,15 @@
         mock_cluster.instances_without_server = []
         mock_cluster.datastore_version.manager = 'redis'
         mock_cluster_create.return_value = mock_cluster
 
         self.controller.create(req, body, tenant_id)
         mock_cluster_create.assert_called_with(context, 'products',
                                                datastore, datastore_version,
-                                               instances, {}, None, None, None)
+                                               instances, {}, None, None)
 
     @patch.object(Cluster, 'load')
     def test_show_cluster(self,
                           mock_cluster_load):
         tenant_id = Mock()
         id = Mock()
         context = trove_testtools.TroveTestContext(self)
@@ -303,20 +303,20 @@
         req.environ = MagicMock()
         req.environ.get = Mock(return_value=context)
 
         datastore_version = Mock()
         datastore_version.manager = 'redis'
         mock_get_datastore_version.return_value = (Mock(), datastore_version)
 
-        self.assertRaisesRegex(exception.TroveError,
-                               "Clusters not supported for",
-                               self.controller.create,
-                               req,
-                               body,
-                               tenant_id)
+        self.assertRaisesRegexp(exception.TroveError,
+                                "Clusters not supported for",
+                                self.controller.create,
+                                req,
+                                body,
+                                tenant_id)
 
     @patch.object(views.ClusterView, 'data', return_value={})
     @patch.object(datastore_models, 'get_datastore_version')
     @patch.object(models.Cluster, 'create')
     def test_create_clusters_enabled(self,
                                      mock_cluster_create,
                                      mock_get_datastore_version,
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_vertica_controller.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster_vertica_controller.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jsonschema
 
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import MagicMock
+from mock import Mock
+from mock import patch
 from testtools.matchers import Is, Equals
 from trove.cluster import models
 from trove.cluster.models import Cluster
 from trove.cluster.service import ClusterController
 from trove.cluster import views
 import trove.common.cfg as cfg
 from trove.common import exception
@@ -56,15 +56,15 @@
             }
         }
 
     def test_get_schema_create(self):
         schema = self.controller.get_schema('create', self.cluster)
         self.assertIsNotNone(schema)
         self.assertIn('cluster', schema['properties'])
-        self.assertIsNotNone(schema['properties']['cluster'])
+        self.assertTrue('cluster')
 
     def test_validate_create(self):
         body = self.cluster
         schema = self.controller.get_schema('create', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
@@ -155,15 +155,15 @@
         mock_cluster.instances_without_server = []
         mock_cluster.datastore_version.manager = 'vertica'
         mock_cluster_create.return_value = mock_cluster
 
         self.controller.create(req, body, tenant_id)
         mock_cluster_create.assert_called_with(context, 'products',
                                                datastore, datastore_version,
-                                               instances, {}, None, None, None)
+                                               instances, {}, None, None)
 
     @patch.object(Cluster, 'load')
     def test_show_cluster(self,
                           mock_cluster_load):
         tenant_id = Mock()
         id = Mock()
         context = trove_testtools.TroveTestContext(self)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_cluster_views.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_cluster_views.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,17 +10,17 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import MagicMock
+from mock import Mock
+from mock import patch
 
 from trove.cluster.views import ClusterInstanceDetailView
 from trove.cluster.views import ClusterView
 from trove.cluster.views import load_view
 from trove.common.strategies.cluster.experimental.mongodb.api import (
     MongoDbClusterView)
 from trove.tests.unittests import trove_testtools
@@ -78,24 +78,21 @@
     def test__build_instances(self, *args):
         cluster = Mock()
         cluster.instances = []
         cluster.instances.append(Mock())
         cluster.instances.append(Mock())
         cluster.instances.append(Mock())
         cluster.instances[0].type = 'configsvr'
-        cluster.instances[0].get_visible_ip_addresses.return_value = [
-            {'type': 'private', 'address': '1.2.3.4', 'network': 'net-id'}]
+        cluster.instances[0].get_visible_ip_addresses = lambda: ['1.2.3.4']
         cluster.instances[0].datastore_version.manager = 'mongodb'
         cluster.instances[1].type = 'query_router'
-        cluster.instances[1].get_visible_ip_addresses.return_value = [
-            {'type': 'private', 'address': '1.2.3.4', 'network': 'net-id'}]
+        cluster.instances[1].get_visible_ip_addresses = lambda: ['1.2.3.4']
         cluster.instances[1].datastore_version.manager = 'mongodb'
         cluster.instances[2].type = 'member'
-        cluster.instances[2].get_visible_ip_addresses.return_value = [
-            {'type': 'private', 'address': '1.2.3.4', 'network': 'net-id'}]
+        cluster.instances[2].get_visible_ip_addresses = lambda: ['1.2.3.4']
         cluster.instances[2].datastore_version.manager = 'mongodb'
 
         def test_case(ip_to_be_published_for,
                       instance_dict_to_be_published_for,
                       number_of_ip_published,
                       number_of_instance_dict_published):
             view = ClusterView(cluster, MagicMock())
@@ -123,46 +120,45 @@
         self.instance.datastore_version = Mock()
         self.instance.datastore_version.name = 'mysql_test_version'
         self.instance.hostname = 'test.trove.com'
         self.ip = "1.2.3.4"
         self.instance.addresses = {"private": [{"addr": self.ip}]}
         self.instance.volume_used = '3'
         self.instance.root_password = 'iloveyou'
-        self.instance.get_visible_ip_addresses.return_value = [
-            {'type': 'private', 'address': '1.2.3.4', 'network': 'net-id'}]
+        self.instance.get_visible_ip_addresses = lambda: ["1.2.3.4"]
         self.instance.slave_of_id = None
         self.instance.slaves = None
         self.context = trove_testtools.TroveTestContext(self)
         self.req = Mock()
         self.req.environ = Mock()
         self.req.environ.__getitem__ = Mock(return_value=self.context)
 
     def tearDown(self):
         super(ClusterInstanceDetailViewTest, self).tearDown()
 
     @patch.object(ClusterInstanceDetailView, '_build_links')
+    @patch.object(ClusterInstanceDetailView, '_build_flavor_links')
     @patch.object(ClusterInstanceDetailView, '_build_configuration_info')
     def test_data(self, *args):
         view = ClusterInstanceDetailView(self.instance, self.req)
         result = view.data()
         self.assertEqual(self.instance.created, result['instance']['created'])
         self.assertEqual(self.instance.updated, result['instance']['updated'])
         self.assertEqual(self.instance.datastore_version.name,
                          result['instance']['datastore']['version'])
         self.assertEqual(self.instance.hostname,
                          result['instance']['hostname'])
         self.assertNotIn('ip', result['instance'])
 
     @patch.object(ClusterInstanceDetailView, '_build_links')
+    @patch.object(ClusterInstanceDetailView, '_build_flavor_links')
     @patch.object(ClusterInstanceDetailView, '_build_configuration_info')
     def test_data_ip(self, *args):
         self.instance.hostname = None
         view = ClusterInstanceDetailView(self.instance, self.req)
         result = view.data()
         self.assertEqual(self.instance.created, result['instance']['created'])
         self.assertEqual(self.instance.updated, result['instance']['updated'])
         self.assertEqual(self.instance.datastore_version.name,
                          result['instance']['datastore']['version'])
         self.assertNotIn('hostname', result['instance'])
         self.assertEqual([self.ip], result['instance']['ip'])
-        self.assertEqual(self.ip,
-                         result['instance']['addresses'][0]['address'])
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_galera_cluster.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_galera_cluster.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,24 +9,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import uuid
 
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from novaclient import exceptions as nova_exceptions
 
 from trove.cluster.models import Cluster
 from trove.cluster.models import ClusterTasks
 from trove.cluster.models import DBCluster
-from trove.common import clients
 from trove.common import exception
+from trove.common import remote
 from trove.common.strategies.cluster.experimental.galera_common import (
     api as galera_api)
 from trove.common.strategies.cluster.experimental.galera_common import (
     taskmanager as galera_task)
 from trove.instance import models as inst_models
 from trove.quota.quota import QUOTAS
 from trove.taskmanager import api as task_api
@@ -81,27 +81,27 @@
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           [], {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_flavor_not_specified(self, mock_client):
         instances = self.instances
         instances[0]['flavor_id'] = None
         self.assertRaises(exception.ClusterFlavorsNotEqual,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances, {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_invalid_flavor_specified(self,
                                              mock_client):
         instances = [{'flavor_id': '1234'},
                      {'flavor_id': '1234'},
                      {'flavor_id': '1234'}]
 
         (mock_client.return_value.flavors.get) = Mock(
@@ -112,30 +112,30 @@
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances, {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_volume_no_specified(self,
                                         mock_client):
         instances = self.instances
         instances[0]['volume_size'] = None
         flavors = Mock()
         mock_client.return_value.flavors = flavors
         self.assertRaises(exception.ClusterVolumeSizeRequired,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances, {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(galera_api, 'CONF')
     def test_create_storage_specified_with_no_volume_support(self,
                                                              mock_conf,
                                                              mock_client):
         mock_conf.get = Mock(
             return_value=FakeOptGroup(volume_support=False))
         instances = self.instances
@@ -146,15 +146,15 @@
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances, {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(galera_api, 'CONF')
     def test_create_storage_not_specified_and_no_ephemeral_flavor(self,
                                                                   mock_conf,
                                                                   mock_client):
         class FakeFlavor(object):
             def __init__(self, flavor_id):
                 self.flavor_id = flavor_id
@@ -177,15 +177,15 @@
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances, {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_volume_not_equal(self, mock_client):
         instances = self.instances
         instances[0]['volume_size'] = 2
         flavors = Mock()
         mock_client.return_value.flavors = flavors
         self.assertRaises(exception.ClusterVolumeSizesNotEqual,
                           Cluster.create,
@@ -196,38 +196,36 @@
                           instances, {}, None, None)
 
     @patch.object(inst_models.DBInstance, 'find_all')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(DBCluster, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
-    @patch.object(clients, 'create_neutron_client')
-    def test_create(self, mock_neutron_client, mock_nova_client,
-                    mock_check_quotas, mock_task_api, mock_db_create,
-                    mock_ins_create, mock_find_all):
+    @patch.object(remote, 'create_nova_client')
+    def test_create(self, mock_client, mock_check_quotas, mock_task_api,
+                    mock_db_create, mock_ins_create, mock_find_all):
         instances = self.instances
         flavors = Mock()
         networks = Mock()
-        mock_nova_client.return_value.flavors = flavors
-        mock_neutron_client.return_value.find_resource = networks
+        mock_client.return_value.flavors = flavors
+        mock_client.return_value.networks = networks
         self.cluster.create(Mock(),
                             self.cluster_name,
                             self.datastore,
                             self.datastore_version,
                             instances, {}, None, None)
         mock_task_api.return_value.create_cluster.assert_called_with(
             mock_db_create.return_value.id)
         self.assertEqual(3, mock_ins_create.call_count)
 
     @patch.object(inst_models.Instance, 'create')
     @patch.object(DBCluster, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_over_limit(self, mock_client, mock_check_quotas,
                                mock_task_api, mock_db_create, mock_ins_create):
         instances = [{'volume_size': 1, 'flavor_id': '1234'},
                      {'volume_size': 1, 'flavor_id': '1234'},
                      {'volume_size': 1, 'flavor_id': '1234'},
                      {'volume_size': 1, 'flavor_id': '1234'}]
         flavors = Mock()
@@ -243,15 +241,15 @@
 
     @patch.object(inst_models.DBInstance, 'find_all')
     @patch.object(galera_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(DBCluster, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_with_ephemeral_flavor(self, mock_client, mock_check_quotas,
                                           mock_task_api, mock_db_create,
                                           mock_ins_create, mock_conf,
                                           mock_find_all):
         class FakeFlavor(object):
             def __init__(self, flavor_id):
                 self.flavor_id = flavor_id
@@ -308,40 +306,34 @@
         mock_update_db.assert_called_with(task_status=ClusterTasks.DELETING)
 
     @patch.object(DBCluster, 'update')
     @patch.object(galera_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
-    @patch.object(clients, 'create_neutron_client')
-    def test_grow(self, mock_neutron_client, mock_nova_client,
-                  mock_check_quotas, mock_task_api,
+    @patch.object(remote, 'create_nova_client')
+    def test_grow(self, mock_client, mock_check_quotas, mock_task_api,
                   mock_inst_create, mock_conf, mock_update):
-        mock_nova_client.return_value.flavors = Mock()
-        mock_neutron_client.return_value.find_resource = Mock()
+        mock_client.return_value.flavors = Mock()
         self.cluster.grow(self.instances)
         mock_update.assert_called_with(
             task_status=ClusterTasks.GROWING_CLUSTER)
         mock_task_api.return_value.grow_cluster.assert_called_with(
             self.db_info.id,
             [mock_inst_create.return_value.id] * 3)
         self.assertEqual(3, mock_inst_create.call_count)
 
     @patch.object(DBCluster, 'update')
     @patch.object(galera_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
-    @patch.object(clients, 'create_neutron_client')
-    def test_grow_exception(self, mock_neutron_client, mock_nova_client,
-                            mock_check_quotas, mock_inst_create,
-                            mock_conf, mock_update):
-        mock_nova_client.return_value.flavors = Mock()
-        mock_neutron_client.return_value.find_resource = Mock()
+    @patch.object(remote, 'create_nova_client')
+    def test_grow_exception(self, mock_client, mock_check_quotas,
+                            mock_inst_create, mock_conf, mock_update):
+        mock_client.return_value.flavors = Mock()
         with patch.object(task_api, 'load') as mock_load:
             mock_load.return_value.grow_cluster = Mock(
                 side_effect=exception.BadRequest)
             self.assertRaises(exception.BadRequest, self.cluster.grow,
                               self.instances)
 
     @patch.object(inst_models.DBInstance, 'find_all')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_models.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_models.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,48 +9,46 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import ANY
-from unittest.mock import call
-from unittest.mock import DEFAULT
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
-from unittest.mock import PropertyMock
-
-from neutronclient.common import exceptions as neutron_exceptions
+from mock import ANY
+from mock import call
+from mock import DEFAULT
+from mock import MagicMock
+from mock import Mock
+from mock import patch
+from mock import PropertyMock
 
 from trove.cluster import models
-from trove.common import clients
 from trove.common import exception
+from trove.common import remote
 from trove.tests.unittests import trove_testtools
 
 
 class TestModels(trove_testtools.TestCase):
 
-    @patch.object(clients, 'create_nova_client', return_value=MagicMock())
-    def test_validate_instance_flavors(self, create_nova_cli_mock):
+    @patch.object(remote, 'create_nova_client', return_value=MagicMock())
+    def test_validate_instance_flavors(self, create_nove_cli_mock):
         patch.object(
-            create_nova_cli_mock.return_value, 'flavors',
+            create_nove_cli_mock.return_value, 'flavors',
             new_callable=PropertyMock(return_value=Mock()))
-        mock_flv = create_nova_cli_mock.return_value.flavors.get.return_value
+        mock_flv = create_nove_cli_mock.return_value.flavors.get.return_value
         mock_flv.ephemeral = 0
 
         test_instances = [{'flavor_id': 1, 'volume_size': 10},
                           {'flavor_id': 1, 'volume_size': 1.5,
                            'region_name': 'home'},
                           {'flavor_id': 2, 'volume_size': 3,
                            'region_name': 'work'}]
         models.validate_instance_flavors(Mock(), test_instances,
                                          True, True)
-        create_nova_cli_mock.assert_has_calls([call(ANY, None),
+        create_nove_cli_mock.assert_has_calls([call(ANY, None),
                                                call(ANY, 'home'),
                                                call(ANY, 'work')])
 
         self.assertRaises(exception.LocalStorageNotSpecified,
                           models.validate_instance_flavors,
                           Mock(), test_instances, False, True)
 
@@ -168,40 +166,7 @@
         models.assert_homogeneous_cluster(
             test_instances, required_flavor=required_flavor,
             required_volume_size=required_volume_size)
         assert_same_instance_flavors.assert_called_once_with(
             test_instances, required_flavor=required_flavor)
         assert_same_instance_volumes.assert_called_once_with(
             test_instances, required_size=required_volume_size)
-
-    @patch.object(clients, 'create_neutron_client', return_value=MagicMock())
-    def test_validate_instance_nics(self, create_neutron_cli_mock):
-
-        test_instances = [
-            {'volume_size': 1, 'flavor_id': '1234',
-             'nics': [{"network_id": "surprise"}]},
-            {'volume_size': 1, 'flavor_id': '1234',
-             'nics': [{"network_id": "foo-bar"}]},
-            {'volume_size': 1, 'flavor_id': '1234',
-             'nics': [{"network_id": "foo-bar"}]}]
-
-        self.assertRaises(exception.ClusterNetworksNotEqual,
-                          models.validate_instance_nics,
-                          Mock(),
-                          test_instances)
-
-        test_instances = [
-            {'volume_size': 1, 'flavor_id': '1234',
-             'nics': [{"network_id": "foo-bar"}]},
-            {'volume_size': 1, 'flavor_id': '1234',
-             'nics': [{"network_id": "foo-bar"}]},
-            {'volume_size': 1, 'flavor_id': '1234',
-             'nics': [{"network_id": "foo-bar"}]}]
-
-        create_neutron_cli_mock.return_value.find_resource = Mock(
-            side_effect=neutron_exceptions.NotFound(
-                "Nic id not found %s" % id))
-
-        self.assertRaises(exception.NetworkNotFound,
-                          models.validate_instance_nics,
-                          Mock(),
-                          test_instances)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_redis_cluster.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_redis_cluster.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,26 +7,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
-from unittest.mock import Mock
-from unittest.mock import patch
 import uuid
 
+from mock import Mock
+from mock import patch
 from novaclient import exceptions as nova_exceptions
-
 from trove.cluster.models import Cluster
 from trove.cluster.models import ClusterTasks
 from trove.cluster.models import DBCluster
-from trove.common import clients
 from trove.common import exception
+from trove.common import remote
 from trove.common.strategies.cluster.experimental.redis import api as redis_api
 from trove.instance import models as inst_models
 from trove.instance.models import DBInstance
 from trove.instance.models import InstanceTasks
 from trove.quota.quota import QUOTAS
 from trove.taskmanager import api as task_api
 from trove.tests.unittests import trove_testtools
@@ -75,15 +73,15 @@
         self.instances_w_volumes = [{'volume_size': 1,
                                      'flavor_id': '1234'}] * 3
         self.instances_no_volumes = [{'flavor_id': '1234'}] * 3
 
     def tearDown(self):
         super(ClusterTest, self).tearDown()
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_invalid_flavor_specified(self,
                                              mock_client):
         (mock_client.return_value.flavors.get) = Mock(
             side_effect=nova_exceptions.NotFound(
                 404, "Flavor id not found %s" % id))
 
         self.assertRaises(exception.FlavorNotFound,
@@ -91,29 +89,29 @@
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           self.instances_w_volumes,
                           {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(redis_api, 'CONF')
     def test_create_volume_no_specified(self, mock_conf, mock_client):
         mock_conf.get = Mock(
             return_value=FakeOptGroup(volume_support=True))
         self.assertRaises(exception.ClusterVolumeSizeRequired,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           self.instances_no_volumes,
                           {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(redis_api, 'CONF')
     def test_create_storage_specified_with_no_volume_support(self,
                                                              mock_conf,
                                                              mock_client):
         mock_conf.get = Mock(
             return_value=FakeOptGroup(volume_support=False))
         mock_client.return_value.flavors = Mock()
@@ -122,15 +120,15 @@
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           self.instances_w_volumes,
                           {}, None, None)
 
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(redis_api, 'CONF')
     def test_create_storage_not_specified_and_no_ephemeral_flavor(self,
                                                                   mock_conf,
                                                                   mock_client):
         class FakeFlavor(object):
             def __init__(self, flavor_id):
                 self.flavor_id = flavor_id
@@ -155,15 +153,15 @@
                           self.instances_no_volumes,
                           {}, None, None)
 
     @patch.object(redis_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create(self, mock_client, mock_check_quotas, mock_task_api,
                     mock_ins_create, mock_conf):
         mock_conf.get = Mock(
             return_value=FakeOptGroup(volume_support=True))
         mock_client.return_value.flavors = Mock()
         self.cluster.create(Mock(),
                             self.cluster_name,
@@ -174,15 +172,15 @@
             self.dbcreate_mock.return_value.id)
         self.assertEqual(3, mock_ins_create.call_count)
 
     @patch.object(redis_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_with_ephemeral_flavor(self, mock_client, mock_check_quotas,
                                           mock_task_api, mock_ins_create,
                                           mock_conf):
         class FakeFlavor(object):
             def __init__(self, flavor_id):
                 self.flavor_id = flavor_id
 
@@ -207,15 +205,15 @@
         self.assertEqual(3, mock_ins_create.call_count)
 
     @patch.object(DBCluster, 'update')
     @patch.object(redis_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_grow(self, mock_client, mock_check_quotas, mock_task_api,
                   mock_ins_create, mock_conf, mock_update):
         mock_conf.get = Mock(
             return_value=FakeOptGroup(volume_support=True))
         mock_client.return_value.flavors = Mock()
         self.cluster.grow(self.instances_w_volumes)
         mock_task_api.return_value.grow_cluster.assert_called_with(
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/cluster/test_vertica_cluster.py` & `trove-8.0.1/trove/tests/unittests/cluster/test_vertica_cluster.py`

 * *Files 5% similar despite different names*

```diff
@@ -7,25 +7,24 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from unittest.mock import Mock
-from unittest.mock import patch
 import uuid
 
+from mock import Mock
+from mock import patch
 from novaclient import exceptions as nova_exceptions
-
 from trove.cluster.models import Cluster
 from trove.cluster.models import ClusterTasks
 from trove.cluster.models import DBCluster
-from trove.common import clients
 from trove.common import exception
+from trove.common import remote
 from trove.common.strategies.cluster.experimental.vertica import (
     api as vertica_api)
 from trove.instance import models as inst_models
 from trove.quota.quota import QUOTAS
 from trove.taskmanager import api as task_api
 from trove.tests.unittests import trove_testtools
 
@@ -77,34 +76,34 @@
     def test_create_empty_instances(self, *args):
         self.assertRaises(exception.ClusterNumInstancesNotSupported,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
-                          [], {}, None, None)
+                          [], None, None, None)
 
     @patch.object(DBCluster, 'create')
     @patch.object(inst_models.DBInstance, 'find_all')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_flavor_not_specified(self, *args):
         instances = self.instances
         instances[0]['flavor_id'] = None
         self.assertRaises(exception.ClusterFlavorsNotEqual,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch.object(DBCluster, 'create')
     @patch.object(inst_models.DBInstance, 'find_all')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_invalid_flavor_specified(self, mock_client,
                                              mock_find_all, mock_create):
         instances = [{'flavor_id': '1234'},
                      {'flavor_id': '1234'},
                      {'flavor_id': '1234'}]
 
         (mock_client.return_value.flavors.get) = Mock(
@@ -114,37 +113,37 @@
         self.assertRaises(exception.FlavorNotFound,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch.object(DBCluster, 'create')
     @patch.object(inst_models.DBInstance, 'find_all')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_volume_no_specified(self, mock_client, mock_find_all,
                                         mock_create):
         instances = self.instances
         instances[0]['volume_size'] = None
         flavors = Mock()
         mock_client.return_value.flavors = flavors
         self.assertRaises(exception.ClusterVolumeSizeRequired,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch.object(DBCluster, 'create')
     @patch.object(inst_models.DBInstance, 'find_all')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(vertica_api, 'CONF')
     def test_create_storage_specified_with_no_volume_support(self,
                                                              mock_conf,
                                                              mock_client,
                                                              mock_find_all,
                                                              mock_create):
         mock_conf.get = Mock(
@@ -156,19 +155,19 @@
         self.assertRaises(exception.VolumeNotSupported,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch.object(DBCluster, 'create')
     @patch.object(inst_models.DBInstance, 'find_all')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     @patch.object(vertica_api, 'CONF')
     def test_create_storage_not_specified_and_no_ephemeral_flavor(self,
                                                                   mock_conf,
                                                                   mock_client,
                                                                   m_find_all,
                                                                   mock_create):
         class FakeFlavor(object):
@@ -192,62 +191,62 @@
         self.assertRaises(exception.LocalStorageNotSpecified,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch.object(DBCluster, 'create')
     @patch.object(inst_models.DBInstance, 'find_all')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_volume_not_equal(self, mock_client, mock_find_all,
                                      mock_create):
         instances = self.instances
         instances[0]['volume_size'] = 2
         flavors = Mock()
         mock_client.return_value.flavors = flavors
         self.assertRaises(exception.ClusterVolumeSizesNotEqual,
                           Cluster.create,
                           Mock(),
                           self.cluster_name,
                           self.datastore,
                           self.datastore_version,
                           instances,
-                          {}, None, None)
+                          None, None, None)
 
     @patch.object(inst_models.DBInstance, 'find_all')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(DBCluster, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create(self, mock_client, mock_check_quotas, mock_task_api,
                     mock_db_create, mock_ins_create, mock_find_all):
         instances = self.instances
         flavors = Mock()
         mock_client.return_value.flavors = flavors
         self.cluster.create(Mock(),
                             self.cluster_name,
                             self.datastore,
                             self.datastore_version,
                             instances,
-                            {}, None, None)
+                            None, None, None)
         mock_task_api.return_value.create_cluster.assert_called_with(
             mock_db_create.return_value.id)
         self.assertEqual(3, mock_ins_create.call_count)
 
     @patch.object(inst_models.DBInstance, 'find_all')
     @patch.object(vertica_api, 'CONF')
     @patch.object(inst_models.Instance, 'create')
     @patch.object(DBCluster, 'create')
     @patch.object(task_api, 'load')
     @patch.object(QUOTAS, 'check_quotas')
-    @patch.object(clients, 'create_nova_client')
+    @patch.object(remote, 'create_nova_client')
     def test_create_with_ephemeral_flavor(self, mock_client, mock_check_quotas,
                                           mock_task_api, mock_db_create,
                                           mock_ins_create, mock_conf,
                                           mock_find_all):
         class FakeFlavor(object):
             def __init__(self, flavor_id):
                 self.flavor_id = flavor_id
@@ -267,15 +266,15 @@
         (mock_client.return_value.
          flavors.get.return_value) = FakeFlavor('1234')
         self.cluster.create(Mock(),
                             self.cluster_name,
                             self.datastore,
                             self.datastore_version,
                             instances,
-                            {}, None, None)
+                            None, None, None)
         mock_task_api.return_value.create_cluster.assert_called_with(
             mock_db_create.return_value.id)
         self.assertEqual(3, mock_ins_create.call_count)
 
     @patch('trove.cluster.models.LOG')
     def test_delete_bad_task_status(self, mock_logging):
         self.cluster.db_info.task_status = ClusterTasks.BUILDING_INITIAL
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_auth.py` & `trove-8.0.1/trove/tests/unittests/common/test_wsgi.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,35 +1,42 @@
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+# All Rights Reserved.
+#
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-
-import webob
-
-from trove.common import auth
+#
+from testtools.matchers import Equals, Is, Not
+from trove.common import wsgi
 from trove.tests.unittests import trove_testtools
+import webob
 
 
-class TestAuth(trove_testtools.TestCase):
-    def test_unicode_characters_in_headers(self):
-        middleware = auth.AuthorizationMiddleware(
-            "test_trove",
-            [auth.TenantBasedAuth()])
-        tenant_id = 'test_tenant_id'
-        url = '/%s/instances' % tenant_id
-        req = webob.Request.blank(url)
-
-        # test string with chinese characters
-        test_str = '\u6d4b\u8bd5'
+class TestWsgi(trove_testtools.TestCase):
+    def test_process_request(self):
+        middleware = wsgi.ContextMiddleware("test_trove")
+        req = webob.BaseRequest({})
+        token = 'MI23fdf2defg123'
+        user_id = 'test_user_id'
         req.headers = {
-            'X-Tenant-ID': tenant_id,
-            'X-Auth-Project-Id': test_str
+            'X-User': 'do not use - deprecated',
+            'X-User-ID': user_id,
+            'X-Auth-Token': token,
+            'X-Service-Catalog': '[]'
         }
+        req.environ = {}
         # invocation
         middleware.process_request(req)
+        # assertions
+        ctx = req.environ[wsgi.CONTEXT_KEY]
+        self.assertThat(ctx, Not(Is(None)))
+        self.assertThat(ctx.user, Equals(user_id))
+        self.assertThat(ctx.auth_token, Equals(token))
+        self.assertEqual(0, len(ctx.service_catalog))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_conductor_serializer.py` & `trove-8.0.1/trove/tests/unittests/common/test_conductor_serializer.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest import mock
+import mock
 
 from trove.common import cfg
 from trove.common.rpc import conductor_guest_serializer as gsz
 from trove.common.rpc import conductor_host_serializer as hsz
 
 from trove.tests.unittests import trove_testtools
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_context.py` & `trove-8.0.1/trove/tests/unittests/common/test_context.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,70 +9,79 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from unittest.mock import Mock
+import mock
+from mock import Mock
 
 from testtools.matchers import Equals, Is
 from trove.common import context
 from trove.common.notification import DBaaSInstanceCreate
 from trove.tests.unittests import trove_testtools
 
 
 class TestTroveContext(trove_testtools.TestCase):
     def test_create_with_extended_args(self):
         expected_service_catalog = {'key': 'value'}
-        ctx = context.TroveContext(user_id="test_user_id",
+        ctx = context.TroveContext(user="test_user_id",
                                    request_id="test_req_id",
                                    limit="500",
                                    marker="x",
                                    service_catalog=expected_service_catalog)
         self.assertThat(ctx.limit, Equals("500"))
         self.assertThat(ctx.marker, Equals("x"))
         self.assertThat(ctx.service_catalog, Equals(expected_service_catalog))
 
     def test_create(self):
-        ctx = context.TroveContext(user_id='test_user_id',
+        ctx = context.TroveContext(user='test_user_id',
                                    request_id='test_req_id')
-        self.assertThat(ctx.user_id, Equals('test_user_id'))
+        self.assertThat(ctx.user, Equals('test_user_id'))
         self.assertThat(ctx.request_id, Equals('test_req_id'))
         self.assertThat(ctx.limit, Is(None))
         self.assertThat(ctx.marker, Is(None))
         self.assertThat(ctx.service_catalog, Is(None))
 
     def test_to_dict(self):
-        ctx = context.TroveContext(user_id='test_user_id',
+        ctx = context.TroveContext(user='test_user_id',
                                    request_id='test_req_id')
         ctx_dict = ctx.to_dict()
         self.assertThat(ctx_dict.get('user'), Equals('test_user_id'))
         self.assertThat(ctx_dict.get('request_id'), Equals('test_req_id'))
 
     def test_to_dict_with_notification(self):
-        ctx = context.TroveContext(user_id='test_user_id',
-                                   project_id='the_tenant',
+        ctx = context.TroveContext(user='test_user_id',
+                                   tenant='the_tenant',
                                    request_id='test_req_id')
         ctx.notification = DBaaSInstanceCreate(ctx,
                                                request=Mock())
         ctx_dict = ctx.to_dict()
         self.assertThat(ctx_dict.get('user'), Equals('test_user_id'))
         self.assertThat(ctx_dict.get('request_id'), Equals('test_req_id'))
         self.assertIn('trove_notification', ctx_dict)
         n_dict = ctx_dict['trove_notification']
         self.assertThat(n_dict.get('notification_classname'),
                         Equals('trove.common.notification.'
                                'DBaaSInstanceCreate'))
 
     def test_create_with_bogus(self):
-        ctx = context.TroveContext.from_dict(
-            {'user': 'test_user_id',
-             'request_id': 'test_req_id',
-             'project_id': 'abc',
-             'blah_blah': 'blah blah'})
-        self.assertThat(ctx.user_id, Equals('test_user_id'))
+        with mock.patch('trove.common.context.LOG') as mock_log:
+            ctx = context.TroveContext.from_dict(
+                {'user': 'test_user_id',
+                 'request_id': 'test_req_id',
+                 'tenant': 'abc',
+                 'blah_blah': 'blah blah'})
+            mock_log.warning.assert_called()
+            mock_log.warning.assert_called_with('Argument being removed '
+                                                'before instantiating '
+                                                'TroveContext object - '
+                                                '%(key)s = %(value)s',
+                                                {'value': 'blah blah',
+                                                 'key': 'blah_blah'})
+        self.assertThat(ctx.user, Equals('test_user_id'))
         self.assertThat(ctx.request_id, Equals('test_req_id'))
-        self.assertThat(ctx.project_id, Equals('abc'))
+        self.assertThat(ctx.tenant, Equals('abc'))
         self.assertThat(ctx.limit, Is(None))
         self.assertThat(ctx.marker, Is(None))
         self.assertThat(ctx.service_catalog, Is(None))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_crypto_utils.py` & `trove-8.0.1/trove/tests/unittests/common/test_crypto_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,82 +10,84 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-import os
-from unittest import mock
-
+from Crypto import Random
+import mock
+import six
 
 from trove.common import crypto_utils
 from trove.tests.unittests import trove_testtools
 
 
 class TestEncryptUtils(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestEncryptUtils, self).setUp()
 
     def tearDown(self):
         super(TestEncryptUtils, self).tearDown()
 
     def test_encode_decode_string(self):
-        random_data = bytearray(os.urandom(12))
+        random_data = bytearray(Random.new().read(12))
         data = [b'abc', b'numbers01234', b'\x00\xFF\x00\xFF\xFF\x00',
-                random_data, 'Unicode:\u20ac']
+                random_data, u'Unicode:\u20ac']
 
         for datum in data:
             encoded_data = crypto_utils.encode_data(datum)
             decoded_data = crypto_utils.decode_data(encoded_data)
-            if isinstance(datum, str):
+            if isinstance(datum, six.text_type):
                 decoded_data = decoded_data.decode('utf-8')
             self. assertEqual(datum, decoded_data,
                               "Encode/decode failed")
 
     def test_pad_unpad(self):
         for size in range(1, 100):
             data_str = b'a' * size
             padded_str = crypto_utils.pad_for_encryption(
-                data_str, crypto_utils.IV_BYTE_COUNT)
-            self.assertEqual(0, len(padded_str) % crypto_utils.IV_BYTE_COUNT,
+                data_str, crypto_utils.IV_BIT_COUNT)
+            self.assertEqual(0, len(padded_str) % crypto_utils.IV_BIT_COUNT,
                              "Padding not successful")
             unpadded_str = crypto_utils.unpad_after_decryption(padded_str)
             self.assertEqual(data_str, unpadded_str,
                              "String mangled after pad/unpad")
 
     def test_encryp_decrypt(self):
         key = 'my_secure_key'
         for size in range(1, 100):
-            orig_data = os.urandom(size)
+            orig_data = Random.new().read(size)
             orig_encoded = crypto_utils.encode_data(orig_data)
             encrypted = crypto_utils.encrypt_data(orig_encoded, key)
             encoded = crypto_utils.encode_data(encrypted)
             decoded = crypto_utils.decode_data(encoded)
             decrypted = crypto_utils.decrypt_data(decoded, key)
             final_decoded = crypto_utils.decode_data(decrypted)
 
             self.assertEqual(orig_data, final_decoded,
                              "Decrypted data did not match original")
 
     def test_encrypt(self):
         # test encrypt() with an hardcoded IV
         key = 'my_secure_key'
-        salt = b'x' * crypto_utils.IV_BYTE_COUNT
+        salt = b'x' * crypto_utils.IV_BIT_COUNT
+
+        with mock.patch('Crypto.Random.new') as mock_random:
+            mock_random.return_value.read.return_value = salt
 
-        with mock.patch('os.urandom', return_value=salt):
             for orig_data, expected in (
                 # byte string
                 (b'Hello World!',
                  'eHh4eHh4eHh4eHh4eHh4eF5RK6VdDrAWl4Th1mNG2eps+VB2BouFRiY2Wa'
                     'P/RRPT'),
 
                 # Unicoded string (encoded to UTF-8)
-                ('Unicode:\u20ac',
+                (u'Unicode:\u20ac',
                  'eHh4eHh4eHh4eHh4eHh4eAMsI5YsrtMNAPJfVF0j9NegXML7OsJ0LuAy66'
                     'LKv5F4'),
             ):
                 orig_encoded = crypto_utils.encode_data(orig_data)
                 encrypted = crypto_utils.encrypt_data(orig_encoded, key)
                 encoded = crypto_utils.encode_data(encrypted)
                 self.assertEqual(expected, encoded)
@@ -95,15 +97,15 @@
 
         for encoded, expected in (
             # byte string: b'Hello World!'
             ('ZUhoNGVIaDRlSGg0ZUhoNL9PmM70hVcQ7j/kYF7Pw+BT7VSfsht0VsCIxy'
                 'KNN0NH',
              b'Hello World!'),
 
-            # Unicoded string: 'Unicode:\u20ac'
+            # Unicoded string: u'Unicode:\u20ac'
             ('ZUhoNGVIaDRlSGg0ZUhoNIHZLIuIcQCRwWY7PR2y7JcqoDf4ViqXIfh0uE'
                 'Rbg9BA',
              b'Unicode:\xe2\x82\xac'),
         ):
             decoded = crypto_utils.decode_data(encoded)
             decrypted = crypto_utils.decrypt_data(decoded, key)
             final_decoded = crypto_utils.decode_data(decrypted)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_dbmodels.py` & `trove-8.0.1/trove/tests/unittests/common/test_dbmodels.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest import mock
+import mock
 
 from trove.common.db import models
 from trove.tests.unittests import trove_testtools
 
 
 class DatastoreSchemaTest(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_exception.py` & `trove-8.0.1/trove/tests/unittests/common/test_exception.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_notification.py` & `trove-8.0.1/trove/tests/unittests/common/test_notification.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,25 +9,23 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock, patch
 
 from oslo_utils import timeutils
 
 from trove.common import cfg
 from trove.common.context import TroveContext
 from trove.common import exception
 from trove.common import notification
-from trove.common.notification import EndNotification
-from trove.common.notification import StartNotification
+from trove.common.notification import EndNotification, StartNotification
 from trove.conductor import api as conductor_api
 from trove import rpc
 from trove.tests.unittests import trove_testtools
 
 
 class TestEndNotification(trove_testtools.TestCase):
 
@@ -225,14 +223,38 @@
         serialized = orig_notify.serialize(None)
         new_notify = notification.TroveInstanceDelete().deserialize(None,
                                                                     serialized)
         new_notify.notify()
         self.assertTrue(notifier().info.called)
 
 
+class TestTroveInstanceModifyVolume(trove_testtools.TestCase):
+
+    def setUp(self):
+        super(TestTroveInstanceModifyVolume, self).setUp()
+        self.instance = Mock(db_info=Mock(created=timeutils.utcnow()))
+
+    @patch.object(cfg.CONF, 'get', Mock())
+    @patch.object(rpc, 'get_notifier')
+    def test_notification(self, notifier):
+        notification.TroveInstanceModifyVolume(instance=self.instance).notify()
+        self.assertTrue(notifier().info.called)
+
+    @patch.object(cfg.CONF, 'get', Mock())
+    @patch.object(rpc, 'get_notifier')
+    def test_notification_after_serialization(self, notifier):
+        orig_notify = notification.TroveInstanceModifyVolume(
+            instance=self.instance)
+        serialized = orig_notify.serialize(None)
+        new_notify = notification.TroveInstanceModifyVolume().deserialize(
+            None, serialized)
+        new_notify.notify()
+        self.assertTrue(notifier().info.called)
+
+
 class TestTroveInstanceModifyFlavor(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestTroveInstanceModifyFlavor, self).setUp()
         self.instance = Mock(db_info=Mock(created=timeutils.utcnow()))
 
     @patch.object(cfg.CONF, 'get', Mock())
@@ -279,38 +301,38 @@
 class TestDBaaSNotification(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestDBaaSNotification, self).setUp()
         self.test_n = DBaaSTestNotification(Mock(), request=Mock())
 
     def test_missing_required_start_traits(self):
-        self.assertRaisesRegex(exception.TroveError,
-                               self.test_n.required_start_traits()[0],
-                               self.test_n.notify_start)
+        self.assertRaisesRegexp(exception.TroveError,
+                                self.test_n.required_start_traits()[0],
+                                self.test_n.notify_start)
 
     def test_invalid_start_traits(self):
-        self.assertRaisesRegex(exception.TroveError,
-                               "The following required keys",
-                               self.test_n.notify_start, foo='bar')
+        self.assertRaisesRegexp(exception.TroveError,
+                                "The following required keys",
+                                self.test_n.notify_start, foo='bar')
 
     def test_missing_required_end_traits(self):
-        self.assertRaisesRegex(exception.TroveError,
-                               self.test_n.required_end_traits()[0],
-                               self.test_n.notify_end)
+        self.assertRaisesRegexp(exception.TroveError,
+                                self.test_n.required_end_traits()[0],
+                                self.test_n.notify_end)
 
     def test_invalid_end_traits(self):
-        self.assertRaisesRegex(exception.TroveError,
-                               "The following required keys",
-                               self.test_n.notify_end, foo='bar')
+        self.assertRaisesRegexp(exception.TroveError,
+                                "The following required keys",
+                                self.test_n.notify_end, foo='bar')
 
     def test_missing_required_error_traits(self):
-        self.assertRaisesRegex(exception.TroveError,
-                               self.test_n.required_error_traits()[0],
-                               self.test_n._notify, 'error',
-                               self.test_n.required_error_traits(), [])
+        self.assertRaisesRegexp(exception.TroveError,
+                                self.test_n.required_error_traits()[0],
+                                self.test_n._notify, 'error',
+                                self.test_n.required_error_traits(), [])
 
     @patch.object(rpc, 'get_notifier')
     def test_start_event(self, notifier):
         self.test_n.notify_start(name='foo', flavor_id=7, datastore='db')
         self.assertTrue(notifier().info.called)
         a, _ = notifier().info.call_args
         self.assertEqual('dbaas.instance_test.start', a[1])
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_pagination.py` & `trove-8.0.1/trove/tests/unittests/common/test_pagination.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest.mock import Mock
+from mock import Mock
 
 from trove.common import pagination
 from trove.tests.unittests import trove_testtools
 
 
 class TestPaginatedDataView(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_policy.py` & `trove-8.0.1/trove/tests/unittests/common/test_policy.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,17 +9,17 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import MagicMock
-from unittest.mock import NonCallableMock
-from unittest.mock import patch
+from mock import MagicMock
+from mock import NonCallableMock
+from mock import patch
 
 from trove.common import exception as trove_exceptions
 from trove.common import policy as trove_policy
 from trove.tests.unittests import trove_testtools
 
 
 class TestPolicy(trove_testtools.TestCase):
@@ -34,20 +34,17 @@
         self.mock_get_enforcer = get_enforcer_patch.start()
 
     def test_authorize_on_tenant(self):
         test_rule = NonCallableMock()
         trove_policy.authorize_on_tenant(self.context, test_rule)
         self.mock_get_enforcer.assert_called_once_with()
         self.mock_enforcer.authorize.assert_called_once_with(
-            test_rule,
-            {'tenant': self.context.project_id},
-            self.context.to_dict(),
+            test_rule, {'tenant': self.context.tenant}, self.context.to_dict(),
             do_raise=True, exc=trove_exceptions.PolicyNotAuthorized,
-            action=test_rule
-        )
+            action=test_rule)
 
     def test_authorize_on_target(self):
         test_rule = NonCallableMock()
         test_target = NonCallableMock()
         trove_policy.authorize_on_target(self.context, test_rule, test_target)
         self.mock_get_enforcer.assert_called_once_with()
         self.mock_enforcer.authorize.assert_called_once_with(
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_secure_serializer.py` & `trove-8.0.1/trove/tests/unittests/common/test_secure_serializer.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_serializer.py` & `trove-8.0.1/trove/tests/unittests/common/test_serializer.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest import mock
+import mock
 
 from trove.common.rpc import serializer
 from trove.tests.unittests import trove_testtools
 
 
 class TestSerializer(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_server_group.py` & `trove-8.0.1/trove/tests/unittests/common/test_server_group.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import copy
-from unittest.mock import Mock, patch
+from mock import Mock, patch
 
 from trove.common import server_group as srv_grp
 from trove.tests.unittests import trove_testtools
 
 
 class TestServerGroup(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_stream_codecs.py` & `trove-8.0.1/trove/tests/unittests/common/test_stream_codecs.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,30 +10,30 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-import os
+from Crypto import Random
 
 from trove.common import stream_codecs
 from trove.tests.unittests import trove_testtools
 
 
 class TestStreamCodecs(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestStreamCodecs, self).setUp()
 
     def tearDown(self):
         super(TestStreamCodecs, self).tearDown()
 
     def test_serialize_deserialize_base64codec(self):
-        random_data = bytearray(os.urandom(12))
+        random_data = bytearray(Random.new().read(12))
         data = [b'abc',
                 b'numbers01234',
                 b'non-ascii:\xe9\xff',
                 random_data]
 
         codec = stream_codecs.Base64Codec()
         for datum in data:
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_template.py` & `trove-8.0.1/trove/tests/unittests/common/test_template.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import re
 
-from unittest.mock import Mock
+from mock import Mock
 
 from trove.common import template
 from trove.datastore.models import DatastoreVersion
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 
@@ -36,42 +36,42 @@
         for line in contents.split('\n'):
             m = re.search('^%s.*' % teststr, line)
             if m:
                 found_group = m.group(0)
         return found_group
 
     def validate_template(self, contents, teststr, test_flavor, server_id):
-        # expected innodb_buffer_pool_size = {{ (150 * flavor_multiplier }}M
+        # expected query_cache_size = {{ 8 * flavor_multiplier }}M
         flavor_multiplier = test_flavor['ram'] // 512
         found_group = self._find_in_template(contents, teststr)
         if not found_group:
-            raise Exception("Could not find text in template")
+            raise "Could not find text in template"
         # Check that the last group has been rendered
         memsize = found_group.split(" ")[2]
-        self.assertEqual("%sM" % (150 * flavor_multiplier), memsize)
+        self.assertEqual("%sM" % (8 * flavor_multiplier), memsize)
         self.assertIsNotNone(server_id)
         self.assertGreater(len(server_id), 1)
 
     def test_rendering(self):
         rendered = self.template.render(flavor=self.flavor_dict,
                                         server_id=self.server_id)
         self.validate_template(rendered,
-                               "innodb_buffer_pool_size",
+                               "query_cache_size",
                                self.flavor_dict,
                                self.server_id)
 
     def test_single_instance_config_rendering(self):
         datastore = Mock(spec=DatastoreVersion)
         datastore.datastore_name = 'MySql'
-        datastore.name = 'mysql-5.7'
+        datastore.name = 'mysql-5.6'
         datastore.manager = 'mysql'
         config = template.SingleInstanceConfigTemplate(datastore,
                                                        self.flavor_dict,
                                                        self.server_id)
-        self.validate_template(config.render(), "innodb_buffer_pool_size",
+        self.validate_template(config.render(), "query_cache_size",
                                self.flavor_dict, self.server_id)
 
     def test_renderer_discovers_special_config(self):
         """Finds our special config file for the version 'mysql-test'."""
         datastore = Mock(spec=DatastoreVersion)
         datastore.datastore_name = 'mysql'
         datastore.name = 'mysql-test'
@@ -81,23 +81,23 @@
                                                        self.server_id)
         self.validate_template(config.render(), "hyper",
                                {'ram': 0}, self.server_id)
 
     def test_replica_source_config_rendering(self):
         datastore = Mock(spec=DatastoreVersion)
         datastore.datastore_name = 'MySql'
-        datastore.name = 'mysql-5.7'
+        datastore.name = 'mysql-5.6'
         datastore.manager = 'mysql'
         config = template.ReplicaSourceConfigTemplate(datastore,
                                                       self.flavor_dict,
                                                       self.server_id)
         self.assertTrue(self._find_in_template(config.render(), "log_bin"))
 
     def test_replica_config_rendering(self):
         datastore = Mock(spec=DatastoreVersion)
         datastore.datastore_name = 'MySql'
-        datastore.name = 'mysql-5.7'
+        datastore.name = 'mysql-5.6'
         datastore.manager = 'mysql'
         config = template.ReplicaConfigTemplate(datastore,
                                                 self.flavor_dict,
                                                 self.server_id)
         self.assertTrue(self._find_in_template(config.render(), "relay_log"))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_timeutils.py` & `trove-8.0.1/trove/tests/unittests/common/test_timeutils.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/common/test_utils.py` & `trove-8.0.1/trove/tests/unittests/common/test_utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -10,23 +10,22 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from testtools import ExpectedException
 from trove.common import exception
 from trove.common import utils
 from trove.tests.unittests import trove_testtools
 from trove.tests.util import utils as test_utils
-import webob
 
 
 class TestUtils(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestUtils, self).setUp()
         self.orig_utils_execute = utils.execute
@@ -170,35 +169,7 @@
         assert_retry(te.test_foo_1, TestEx3, 1, TestEx3)
         assert_retry(te.test_foo_1, TestEx1, 5, TestEx1)
         assert_retry(te.test_foo_1, [TestEx1, TestEx3], 2, TestEx3)
         assert_retry(te.test_foo_2, [TestEx1, TestEx2, None], 3, None)
         assert_retry(te.test_foo_2, TestEx3, 1, TestEx3)
         assert_retry(te.test_foo_2, TestEx2, 3, TestEx2)
         assert_retry(te.test_foo_2, [TestEx1, TestEx3, TestEx2], 2, TestEx3)
-
-    def test_req_to_text(self):
-        req = webob.Request.blank('/')
-        expected = 'GET / HTTP/1.0\r\nHost: localhost:80'
-        self.assertEqual(expected, utils.req_to_text(req))
-
-        # add a header containing unicode characters
-        req.headers.update({
-            'X-Auth-Project-Id': '\u6d4b\u8bd5'})
-        expected = ('GET / HTTP/1.0\r\nHost: localhost:80\r\n'
-                    'X-Auth-Project-Id: \u6d4b\u8bd5')
-        self.assertEqual(expected, utils.req_to_text(req))
-
-    def test_validate_command(self):
-        string1 = "hello_world"
-        string2 = "hello world"
-        string3 = "hello@world_123"
-        string4 = "example.com/databse/mysql:5.7"
-        string5 = 'test --db-user="$(touch /rce_successful.txt)"'
-        self.assertIsNone(utils.validate_command(string1))
-        self.assertRaises(exception.InvalidValue,
-                          utils.validate_command,
-                          string2)
-        self.assertIsNone(utils.validate_command(string3))
-        self.assertIsNone(utils.validate_command(string4))
-        self.assertRaises(exception.InvalidValue,
-                          utils.validate_command,
-                          string5)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/conductor/test_conf.py` & `trove-8.0.1/trove/tests/unittests/conductor/test_conf.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,17 +8,16 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import MagicMock
-from unittest.mock import patch
-
+from mock import MagicMock
+from mock import patch
 from oslo_service import service as os_service
 
 from trove.cmd import common as common_cmd
 from trove.cmd import conductor as conductor_cmd
 import trove.common.cfg as cfg
 import trove.tests.fakes.conf as fake_conf
 from trove.tests.unittests import trove_testtools
@@ -45,15 +44,15 @@
     def setUp(self):
         super(ConductorConfTests, self).setUp()
 
     def tearDown(self):
         super(ConductorConfTests, self).tearDown()
 
     def _test_manager(self, conf, rt_mgr_name):
-        def mock_launch(conf, server, workers, restart_method):
+        def mock_launch(conf, server, workers):
             qualified_mgr = "%s.%s" % (server.manager_impl.__module__,
                                        server.manager_impl.__class__.__name__)
             self.assertEqual(rt_mgr_name, qualified_mgr, "Invalid manager")
             return MagicMock()
 
         os_service.launch = mock_launch
         with patch.object(common_cmd, 'initialize',
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/conductor/test_methods.py` & `trove-8.0.1/trove/tests/unittests/conductor/test_methods.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,28 +8,28 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import patch
-
+from mock import patch
 from oslo_utils import timeutils
 
 from trove.backup import models as bkup_models
 from trove.backup import state
 from trove.common import exception as t_exception
+from trove.common.instance import ServiceStatuses
 from trove.common import utils
 from trove.conductor import manager as conductor_manager
 from trove.instance import models as t_models
-from trove.instance.service_status import ServiceStatuses
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
+
 # See LP bug #1255178
 OLD_DBB_SAVE = bkup_models.DBBackup.save
 
 
 class ConductorMethodTests(trove_testtools.TestCase):
     def setUp(self):
         # See LP bug #1255178
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/configuration/test_configuration_controller.py` & `trove-8.0.1/trove/tests/unittests/configuration/test_configuration_controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 import jsonschema
-from unittest.mock import MagicMock
+from mock import MagicMock
 
 from trove.common import configurations
 from trove.common.exception import UnprocessableEntity
 from trove.configuration.service import ConfigurationsController
 from trove.extensions.mgmt.configuration import service
 from trove.tests.unittests import trove_testtools
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/datastore/base.py` & `trove-8.0.1/trove/tests/unittests/datastore/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,82 +7,80 @@
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+import uuid
 
 from trove.datastore import models as datastore_models
 from trove.datastore.models import Capability
 from trove.datastore.models import Datastore
 from trove.datastore.models import DatastoreVersion
 from trove.datastore.models import DatastoreVersionMetadata
 from trove.datastore.models import DBCapabilityOverrides
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 
 class TestDatastoreBase(trove_testtools.TestCase):
-    @classmethod
-    def setUpClass(cls):
-        util.init_db()
 
-        cls.ds_name = cls.random_name(name='test-datastore')
-        cls.ds_version_name = cls.random_name(name='test-version')
-        cls.capability_name = cls.random_name(name='root_on_create',
-                                              prefix='TestDatastoreBase')
-        cls.capability_desc = "Enables root on create"
-        cls.capability_enabled = True
-        cls.flavor_id = 1
-        cls.volume_type = 'some-valid-volume-type'
+    def setUp(self):
+        # Basic setup and mock/fake structures for testing only
+        super(TestDatastoreBase, self).setUp()
+        util.init_db()
+        self.rand_id = str(uuid.uuid4())
+        self.ds_name = "my-test-datastore" + self.rand_id
+        self.ds_version = "my-test-version" + self.rand_id
+        self.capability_name = "root_on_create" + self.rand_id
+        self.capability_desc = "Enables root on create"
+        self.capability_enabled = True
+        self.datastore_version_id = str(uuid.uuid4())
+        self.flavor_id = 1
+        self.volume_type = 'some-valid-volume-type'
 
-        datastore_models.update_datastore(cls.ds_name, False)
-        cls.datastore = Datastore.load(cls.ds_name)
+        datastore_models.update_datastore(self.ds_name, False)
+        self.datastore = Datastore.load(self.ds_name)
 
         datastore_models.update_datastore_version(
-            cls.ds_name, cls.ds_version_name, "mysql", "", "", "", True)
-        cls.datastore_version = DatastoreVersion.load(cls.datastore,
-                                                      cls.ds_version_name)
-        cls.test_id = cls.datastore_version.id
-
+            self.ds_name, self.ds_version, "mysql", "", "", True)
         DatastoreVersionMetadata.add_datastore_version_flavor_association(
-            cls.datastore_version.id, [cls.flavor_id])
+            self.ds_name, self.ds_version, [self.flavor_id])
         DatastoreVersionMetadata.add_datastore_version_volume_type_association(
-            cls.datastore_version.id, [cls.volume_type])
-
-        cls.cap1 = Capability.create(cls.capability_name,
-                                     cls.capability_desc, True)
-        cls.cap2 = Capability.create(
-            cls.random_name(name='require_volume', prefix='TestDatastoreBase'),
-            "Require external volume", True)
-        cls.cap3 = Capability.create(
-            cls.random_name(name='test_capability',
-                            prefix='TestDatastoreBase'),
-            "Test capability", False)
+            self.ds_name, self.ds_version, [self.volume_type])
 
-        super(TestDatastoreBase, cls).setUpClass()
+        self.datastore_version = DatastoreVersion.load(self.datastore,
+                                                       self.ds_version)
+        self.test_id = self.datastore_version.id
+
+        self.cap1 = Capability.create(self.capability_name,
+                                      self.capability_desc, True)
+        self.cap2 = Capability.create("require_volume" + self.rand_id,
+                                      "Require external volume", True)
+        self.cap3 = Capability.create("test_capability" + self.rand_id,
+                                      "Test capability", False)
 
-    @classmethod
-    def tearDownClass(cls):
+    def tearDown(self):
+        super(TestDatastoreBase, self).tearDown()
         capabilities_overridden = DBCapabilityOverrides.find_all(
-            datastore_version_id=cls.test_id).all()
+            datastore_version_id=self.datastore_version.id).all()
+
         for ce in capabilities_overridden:
             ce.delete()
 
-        cls.cap1.delete()
-        cls.cap2.delete()
-        cls.cap3.delete()
-
+        self.cap1.delete()
+        self.cap2.delete()
+        self.cap3.delete()
+        datastore = datastore_models.Datastore.load(self.ds_name)
+        ds_version = datastore_models.DatastoreVersion.load(datastore,
+                                                            self.ds_version)
         datastore_models.DBDatastoreVersionMetadata.find_by(
-            datastore_version_id=cls.test_id).delete()
-        cls.datastore_version.delete()
-        cls.datastore.delete()
-
-        super(TestDatastoreBase, cls).tearDownClass()
+            datastore_version_id=ds_version.id).delete()
+        Datastore.load(self.ds_name).delete()
 
     def capability_name_filter(self, capabilities):
         new_capabilities = []
         for capability in capabilities:
-            if 'TestDatastoreBase' in capability.name:
+            if self.rand_id in capability.name:
                 new_capabilities.append(capability)
         return new_capabilities
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_capability.py` & `trove-8.0.1/trove/tests/unittests/datastore/test_capability.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,27 +15,36 @@
 from trove.common.exception import CapabilityNotFound
 from trove.datastore.models import Capability
 from trove.datastore.models import CapabilityOverride
 from trove.tests.unittests.datastore.base import TestDatastoreBase
 
 
 class TestCapabilities(TestDatastoreBase):
+    def setUp(self):
+        super(TestCapabilities, self).setUp()
+
+    def tearDown(self):
+        super(TestCapabilities, self).tearDown()
+
     def test_capability(self):
         cap = Capability.load(self.capability_name)
         self.assertEqual(self.capability_name, cap.name)
         self.assertEqual(self.capability_desc, cap.description)
         self.assertEqual(self.capability_enabled, cap.enabled)
 
     def test_ds_capability_create_disabled(self):
         self.ds_cap = CapabilityOverride.create(
             self.cap1, self.datastore_version.id, enabled=False)
         self.assertFalse(self.ds_cap.enabled)
 
         self.ds_cap.delete()
 
+    def test_capability_enabled(self):
+        self.assertTrue(Capability.load(self.capability_name).enabled)
+
     def test_capability_disabled(self):
         capability = Capability.load(self.capability_name)
         capability.disable()
         self.assertFalse(capability.enabled)
 
         self.assertFalse(Capability.load(self.capability_name).enabled)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_datastore.py` & `trove-8.0.1/trove/tests/unittests/datastore/test_datastore.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,16 +9,16 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from trove.common import exception
 from trove.datastore import models as datastore_models
 from trove.datastore.models import Datastore
 from trove.tests.unittests.datastore.base import TestDatastoreBase
 
 
@@ -32,21 +32,21 @@
     def test_load_datastore(self):
         datastore = Datastore.load(self.ds_name)
         self.assertEqual(self.ds_name, datastore.name)
 
     @patch.object(datastore_models, 'CONF')
     def test_create_failure_with_datastore_default(self, mock_conf):
         mock_conf.default_datastore = 'bad_ds'
-        self.assertRaisesRegex(exception.DatastoreDefaultDatastoreNotFound,
-                               "Default datastore 'bad_ds' cannot be found",
-                               datastore_models.get_datastore_version)
-        self.assertRaisesRegex(exception.DatastoreNotFound,
-                               "Datastore 'my_ds' cannot be found",
-                               datastore_models.get_datastore_version,
-                               'my_ds')
+        self.assertRaisesRegexp(exception.DatastoreDefaultDatastoreNotFound,
+                                "Default datastore 'bad_ds' cannot be found",
+                                datastore_models.get_datastore_version)
+        self.assertRaisesRegexp(exception.DatastoreNotFound,
+                                "Datastore 'my_ds' cannot be found",
+                                datastore_models.get_datastore_version,
+                                'my_ds')
 
     def test_get_datastore_or_version(self):
         # datastore, datastore_version, valid, exception
         data = [
             [None, None, True],
             ['ds', None, True],
             ['ds', 'ds_ver', True],
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_datastore_version_metadata.py` & `trove-8.0.1/trove/tests/unittests/datastore/test_datastore_version_metadata.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,18 +8,18 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest import mock
+import mock
 
-from trove.common import clients
 from trove.common import exception
+from trove.common import remote
 from trove.datastore import models as datastore_models
 from trove.tests.unittests.datastore.base import TestDatastoreBase
 
 
 class TestDatastoreVersionMetadata(TestDatastoreBase):
     def setUp(self):
         super(TestDatastoreVersionMetadata, self).setUp()
@@ -31,119 +31,130 @@
         ]
 
     def tearDown(self):
         super(TestDatastoreVersionMetadata, self).tearDown()
 
     def test_map_flavors_to_datastore(self):
         datastore = datastore_models.Datastore.load(self.ds_name)
-        ds_version = datastore_models.DatastoreVersion.load(
-            datastore, self.ds_version_name)
+        ds_version = datastore_models.DatastoreVersion.load(datastore,
+                                                            self.ds_version)
         mapping = datastore_models.DBDatastoreVersionMetadata.find_by(
             datastore_version_id=ds_version.id,
             value=self.flavor_id, deleted=False, key='flavor')
         self.assertEqual(str(self.flavor_id), mapping.value)
         self.assertEqual(ds_version.id, mapping.datastore_version_id)
         self.assertEqual('flavor', str(mapping.key))
 
     def test_map_volume_types_to_datastores(self):
         datastore = datastore_models.Datastore.load(self.ds_name)
-        ds_version = datastore_models.DatastoreVersion.load(
-            datastore, self.ds_version_name)
+        ds_version = datastore_models.DatastoreVersion.load(datastore,
+                                                            self.ds_version)
         mapping = datastore_models.DBDatastoreVersionMetadata.find_by(
             datastore_version_id=ds_version.id,
             value=self.volume_type, deleted=False, key='volume_type')
         self.assertEqual(str(self.volume_type), mapping.value)
         self.assertEqual(ds_version.id, mapping.datastore_version_id)
         self.assertEqual('volume_type', str(mapping.key))
 
     def test_add_existing_flavor_associations(self):
         dsmetadata = datastore_models.DatastoreVersionMetadata
-        self.assertRaises(
+        self.assertRaisesRegexp(
             exception.DatastoreFlavorAssociationAlreadyExists,
+            "Flavor %s is already associated with datastore %s version %s"
+            % (self.flavor_id, self.ds_name, self.ds_version),
             dsmetadata.add_datastore_version_flavor_association,
-            self.test_id, [self.flavor_id])
+            self.ds_name, self.ds_version, [self.flavor_id])
 
     def test_add_existing_volume_type_associations(self):
         dsmetadata = datastore_models.DatastoreVersionMetadata
         self.assertRaises(
             exception.DatastoreVolumeTypeAssociationAlreadyExists,
             dsmetadata.add_datastore_version_volume_type_association,
-            self.test_id, [self.volume_type])
+            self.ds_name, self.ds_version, [self.volume_type])
 
     def test_delete_nonexistent_flavor_mapping(self):
         dsmeta = datastore_models.DatastoreVersionMetadata
-        self.assertRaises(
+        self.assertRaisesRegexp(
             exception.DatastoreFlavorAssociationNotFound,
+            "Flavor 2 is not supported for datastore %s version %s"
+            % (self.ds_name, self.ds_version),
             dsmeta.delete_datastore_version_flavor_association,
-            self.test_id, flavor_id=2)
+            self.ds_name, self.ds_version, flavor_id=2)
 
     def test_delete_nonexistent_volume_type_mapping(self):
         dsmeta = datastore_models.DatastoreVersionMetadata
         self.assertRaises(
             exception.DatastoreVolumeTypeAssociationNotFound,
             dsmeta.delete_datastore_version_volume_type_association,
-            self.test_id,
+            self.ds_name, self.ds_version,
             volume_type_name='some random thing')
 
     def test_delete_flavor_mapping(self):
         flavor_id = 2
-        dsmetadata = datastore_models.DatastoreVersionMetadata
-        dsmetadata.add_datastore_version_flavor_association(
-            self.test_id, [flavor_id])
-        dsmetadata.delete_datastore_version_flavor_association(
-            self.test_id, flavor_id)
+        dsmetadata = datastore_models. DatastoreVersionMetadata
+        dsmetadata.add_datastore_version_flavor_association(self.ds_name,
+                                                            self.ds_version,
+                                                            [flavor_id])
+        dsmetadata.delete_datastore_version_flavor_association(self.ds_name,
+                                                               self.ds_version,
+                                                               flavor_id)
         datastore = datastore_models.Datastore.load(self.ds_name)
-        ds_version = datastore_models.DatastoreVersion.load(
-            datastore,
-            self.ds_version_name)
+        ds_version = datastore_models.DatastoreVersion.load(datastore,
+                                                            self.ds_version)
         mapping = datastore_models.DBDatastoreVersionMetadata.find_by(
             datastore_version_id=ds_version.id, value=flavor_id, key='flavor')
         self.assertTrue(mapping.deleted)
         # check update
         dsmetadata.add_datastore_version_flavor_association(
-            self.test_id, [flavor_id])
+            self.ds_name, self.ds_version, [flavor_id])
         mapping = datastore_models.DBDatastoreVersionMetadata.find_by(
             datastore_version_id=ds_version.id, value=flavor_id, key='flavor')
         self.assertFalse(mapping.deleted)
         # clear the mapping
-        datastore_models.DatastoreVersionMetadata. \
-            delete_datastore_version_flavor_association(self.test_id,
+        datastore_models.DatastoreVersionMetadata.\
+            delete_datastore_version_flavor_association(self.ds_name,
+                                                        self.ds_version,
                                                         flavor_id)
 
     def test_delete_volume_type_mapping(self):
         volume_type = 'this is bogus'
-        dsmetadata = datastore_models.DatastoreVersionMetadata
+        dsmetadata = datastore_models. DatastoreVersionMetadata
         dsmetadata.add_datastore_version_volume_type_association(
-            self.test_id, [volume_type])
+            self.ds_name,
+            self.ds_version,
+            [volume_type])
         dsmetadata.delete_datastore_version_volume_type_association(
-            self.test_id, volume_type)
+            self.ds_name,
+            self.ds_version,
+            volume_type)
         datastore = datastore_models.Datastore.load(self.ds_name)
-        ds_version = datastore_models.DatastoreVersion.load(
-            datastore,
-            self.ds_version_name)
+        ds_version = datastore_models.DatastoreVersion.load(datastore,
+                                                            self.ds_version)
         mapping = datastore_models.DBDatastoreVersionMetadata.find_by(
             datastore_version_id=ds_version.id, value=volume_type,
             key='volume_type')
         self.assertTrue(mapping.deleted)
         # check update
         dsmetadata.add_datastore_version_volume_type_association(
-            self.test_id, [volume_type])
+            self.ds_name, self.ds_version, [volume_type])
         mapping = datastore_models.DBDatastoreVersionMetadata.find_by(
             datastore_version_id=ds_version.id, value=volume_type,
             key='volume_type')
         self.assertFalse(mapping.deleted)
         # clear the mapping
         dsmetadata.delete_datastore_version_volume_type_association(
-            self.test_id, volume_type)
+            self.ds_name,
+            self.ds_version,
+            volume_type)
 
     @mock.patch.object(datastore_models.DatastoreVersionMetadata,
-                       'datastore_version_find')
+                       '_datastore_version_find')
     @mock.patch.object(datastore_models.DatastoreVersionMetadata,
                        'list_datastore_version_volume_type_associations')
-    @mock.patch.object(clients, 'create_cinder_client')
+    @mock.patch.object(remote, 'create_cinder_client')
     def _mocked_allowed_datastore_version_volume_types(self,
                                                        trove_volume_types,
                                                        mock_cinder_client,
                                                        mock_list, *args):
         """Call this with a list of strings specifying volume types."""
         cinder_vts = []
         for vt in self.volume_types:
@@ -160,15 +171,15 @@
         for trove_vt in trove_volume_types:
             trove_type = mock.Mock()
             trove_type.value = trove_vt
             mock_trove_list_result.__iter__.return_value.append(trove_type)
         mock_list.return_value = mock_trove_list_result
 
         return self.dsmetadata.allowed_datastore_version_volume_types(
-            None, self.random_uuid())
+            None, 'ds', 'dsv')
 
     def _assert_equal_types(self, test_dict, output_obj):
         self.assertEqual(test_dict.get('id'), output_obj.id)
         self.assertEqual(test_dict.get('name'), output_obj.name)
 
     def test_allowed_volume_types_from_ids(self):
         id1 = self.volume_types[0].get('id')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/datastore/test_datastore_versions.py` & `trove-8.0.1/trove/tests/unittests/datastore/test_datastore_versions.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,30 +16,30 @@
 from trove.tests.unittests.datastore.base import TestDatastoreBase
 
 
 class TestDatastoreVersions(TestDatastoreBase):
 
     def test_load_datastore_version(self):
         datastore_version = DatastoreVersion.load(self.datastore,
-                                                  self.ds_version_name)
-        self.assertEqual(self.ds_version_name, datastore_version.name)
+                                                  self.ds_version)
+        self.assertEqual(self.ds_version, datastore_version.name)
 
     def test_datastore_version_capabilities(self):
         self.datastore_version.capabilities.add(self.cap1, enabled=False)
         test_filtered_capabilities = self.capability_name_filter(
             self.datastore_version.capabilities)
         self.assertEqual(3, len(test_filtered_capabilities),
                          'Capabilities the test thinks it has are: %s, '
                          'Filtered capabilities: %s' %
                          (self.datastore_version.capabilities,
                           test_filtered_capabilities))
 
         # Test a fresh reloading of the datastore
         self.datastore_version = DatastoreVersion.load(self.datastore,
-                                                       self.ds_version_name)
+                                                       self.ds_version)
         test_filtered_capabilities = self.capability_name_filter(
             self.datastore_version.capabilities)
         self.assertEqual(3, len(test_filtered_capabilities),
                          'Capabilities the test thinks it has are: %s, '
                          'Filtered capabilities: %s' %
                          (self.datastore_version.capabilities,
                           test_filtered_capabilities))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/db/test_migration_utils.py` & `trove-8.0.1/trove/tests/unittests/db/test_migration_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,18 +9,17 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from unittest.mock import call
-from unittest.mock import Mock
-from unittest.mock import patch
-
+from mock import call
+from mock import Mock
+from mock import patch
 from sqlalchemy.engine import reflection
 from sqlalchemy.schema import Column
 
 from trove.db.sqlalchemy.migrate_repo.schema import String
 from trove.db.sqlalchemy import utils as db_utils
 from trove.tests.unittests import trove_testtools
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/extensions/common/test_service.py` & `trove-8.0.1/trove/tests/unittests/common/test_common_extensions.py`

 * *Files 22% similar despite different names*

```diff
@@ -10,17 +10,16 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest.mock import Mock
-from unittest.mock import patch
-
+from mock import Mock
+from mock import patch
 from oslo_config.cfg import NoSuchOptError
 
 from trove.common import exception
 from trove.common import utils
 from trove.extensions.common import models
 from trove.extensions.common.service import ClusterRootController
 from trove.extensions.common.service import DefaultRootController
@@ -58,82 +57,39 @@
             self.controller.root_index,
             req, tenant_id, uuid, is_cluster)
 
     @patch.object(models.Root, "create")
     def test_root_create(self, root_create):
         user = Mock()
         context = Mock()
-        context.user_id = Mock()
-        context.user_id.__getitem__ = Mock(return_value=user)
+        context.user = Mock()
+        context.user.__getitem__ = Mock(return_value=user)
         req = Mock()
         req.environ = Mock()
         req.environ.__getitem__ = Mock(return_value=context)
         tenant_id = Mock()
         uuid = utils.generate_uuid()
         is_cluster = False
         password = Mock()
         body = {'password': password}
         self.controller.root_create(req, body, tenant_id, uuid, is_cluster)
-        root_create.assert_called_with(context, uuid, password)
+        root_create.assert_called_with(context, uuid, context.user, password)
 
     def test_root_create_with_cluster(self):
         req = Mock()
         tenant_id = Mock()
         uuid = utils.generate_uuid()
         is_cluster = True
         password = Mock()
         body = {'password': password}
         self.assertRaises(
             exception.ClusterOperationNotSupported,
             self.controller.root_create,
             req, body, tenant_id, uuid, is_cluster)
 
-    @patch.object(models.Root, "delete")
-    @patch.object(models.Root, "load")
-    def test_root_delete(self, root_load, root_delete):
-        context = Mock()
-        req = Mock()
-        req.environ = Mock()
-        req.environ.__getitem__ = Mock(return_value=context)
-        tenant_id = Mock()
-        instance_id = utils.generate_uuid()
-        is_cluster = False
-        root_load.return_value = True
-        self.controller.root_delete(req, tenant_id, instance_id, is_cluster)
-        root_load.assert_called_with(context, instance_id)
-        root_delete.assert_called_with(context, instance_id)
-
-    @patch.object(models.Root, "delete")
-    @patch.object(models.Root, "load")
-    def test_root_delete_without_root_enabled(self, root_load, root_delete):
-        context = Mock()
-        req = Mock()
-        req.environ = Mock()
-        req.environ.__getitem__ = Mock(return_value=context)
-        tenant_id = Mock()
-        instance_id = utils.generate_uuid()
-        is_cluster = False
-        root_load.return_value = False
-        self.assertRaises(
-            exception.RootHistoryNotFound,
-            self.controller.root_delete,
-            req, tenant_id, instance_id, is_cluster)
-        root_load.assert_called_with(context, instance_id)
-        root_delete.assert_not_called()
-
-    def test_root_delete_with_cluster(self):
-        req = Mock()
-        tenant_id = Mock()
-        instance_id = utils.generate_uuid()
-        is_cluster = True
-        self.assertRaises(
-            exception.ClusterOperationNotSupported,
-            self.controller.root_delete,
-            req, tenant_id, instance_id, is_cluster)
-
 
 class TestRootController(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestRootController, self).setUp()
         self.context = trove_testtools.TroveTestContext(self)
         self.controller = RootController()
@@ -151,15 +107,15 @@
         is_cluster = False
         service_get_datastore.return_value = (ds_manager, is_cluster)
         root_controller = Mock()
         ret = Mock()
         root_controller.root_index = Mock(return_value=ret)
         service_load_root_controller.return_value = root_controller
 
-        self.assertEqual(ret, self.controller.index(req, tenant_id, uuid))
+        self.assertTrue(ret, self.controller.index(req, tenant_id, uuid))
         service_get_datastore.assert_called_with(tenant_id, uuid)
         service_load_root_controller.assert_called_with(ds_manager)
         root_controller.root_index.assert_called_with(
             req, tenant_id, uuid, is_cluster)
 
     @patch.object(instance_models.Instance, "load")
     @patch.object(RootController, "load_root_controller")
@@ -175,15 +131,15 @@
         is_cluster = False
         service_get_datastore.return_value = (ds_manager, is_cluster)
         root_controller = Mock()
         ret = Mock()
         root_controller.root_create = Mock(return_value=ret)
         service_load_root_controller.return_value = root_controller
 
-        self.assertEqual(
+        self.assertTrue(
             ret, self.controller.create(req, tenant_id, uuid, body=body))
         service_get_datastore.assert_called_with(tenant_id, uuid)
         service_load_root_controller.assert_called_with(ds_manager)
         root_controller.root_create.assert_called_with(
             req, body, tenant_id, uuid, is_cluster)
 
     @patch.object(instance_models.Instance, "load")
@@ -206,61 +162,14 @@
         self.assertRaises(
             NoSuchOptError,
             self.controller.create,
             req, tenant_id, uuid, body=body)
         service_get_datastore.assert_called_with(tenant_id, uuid)
         service_load_root_controller.assert_called_with(ds_manager)
 
-    @patch.object(instance_models.Instance, "load")
-    @patch.object(RootController, "load_root_controller")
-    @patch.object(RootController, "_get_datastore")
-    def test_delete(self, service_get_datastore, service_load_root_controller,
-                    service_load_instance):
-        req = Mock()
-        req.environ = {'trove.context': self.context}
-        tenant_id = Mock()
-        uuid = utils.generate_uuid()
-        ds_manager = Mock()
-        is_cluster = False
-        service_get_datastore.return_value = (ds_manager, is_cluster)
-        root_controller = Mock()
-        ret = Mock()
-        root_controller.root_delete = Mock(return_value=ret)
-        service_load_root_controller.return_value = root_controller
-
-        self.assertEqual(
-            ret, self.controller.delete(req, tenant_id, uuid))
-        service_get_datastore.assert_called_with(tenant_id, uuid)
-        service_load_root_controller.assert_called_with(ds_manager)
-        root_controller.root_delete.assert_called_with(
-            req, tenant_id, uuid, is_cluster)
-
-    @patch.object(instance_models.Instance, "load")
-    @patch.object(RootController, "load_root_controller")
-    @patch.object(RootController, "_get_datastore")
-    def test_delete_with_no_root_controller(self,
-                                            service_get_datastore,
-                                            service_load_root_controller,
-                                            service_load_instance):
-        req = Mock()
-        req.environ = {'trove.context': self.context}
-        tenant_id = Mock()
-        uuid = utils.generate_uuid()
-        ds_manager = Mock()
-        is_cluster = False
-        service_get_datastore.return_value = (ds_manager, is_cluster)
-        service_load_root_controller.return_value = None
-
-        self.assertRaises(
-            NoSuchOptError,
-            self.controller.delete,
-            req, tenant_id, uuid)
-        service_get_datastore.assert_called_with(tenant_id, uuid)
-        service_load_root_controller.assert_called_with(ds_manager)
-
 
 class TestClusterRootController(trove_testtools.TestCase):
 
     def setUp(self):
         super(TestClusterRootController, self).setUp()
         self.context = trove_testtools.TroveTestContext(self)
         self.controller = ClusterRootController()
@@ -367,43 +276,43 @@
         db_inst_1 = Mock()
         db_inst_1.id.return_value = utils.generate_uuid()
         db_inst_2 = Mock()
         db_inst_2.id.return_value = utils.generate_uuid()
         cluster_instances = [db_inst_1, db_inst_2]
         mock_find_all.return_value.all.return_value = cluster_instances
         ret = self.controller._get_cluster_instance_id(tenant_id, cluster_id)
-        self.assertEqual(db_inst_1.id, ret[0])
-        self.assertEqual([db_inst_1.id, db_inst_2.id], ret[1])
+        self.assertTrue(db_inst_1.id, ret[0])
+        self.assertTrue(cluster_instances, ret[1])
 
     @patch.object(models.ClusterRoot, "create")
     def test_instance_root_create(self, mock_cluster_root_create):
         user = Mock()
-        self.context.user_id = Mock()
-        self.context.user_id.__getitem__ = Mock(return_value=user)
+        self.context.user = Mock()
+        self.context.user.__getitem__ = Mock(return_value=user)
         req = Mock()
         req.environ = {'trove.context': self.context}
         password = Mock()
         body = {'password': password}
         instance_id = utils.generate_uuid()
         cluster_instances = Mock()
         self.controller.instance_root_create(
             req, body, instance_id, cluster_instances)
         mock_cluster_root_create.assert_called_with(
-            self.context, instance_id, password,
+            self.context, instance_id, self.context.user, password,
             cluster_instances)
 
     @patch.object(models.ClusterRoot, "create")
     def test_instance_root_create_no_body(self, mock_cluster_root_create):
         user = Mock()
-        self.context.user_id = Mock()
-        self.context.user_id.__getitem__ = Mock(return_value=user)
+        self.context.user = Mock()
+        self.context.user.__getitem__ = Mock(return_value=user)
         req = Mock()
         req.environ = {'trove.context': self.context}
         password = None
         body = None
         instance_id = utils.generate_uuid()
         cluster_instances = Mock()
         self.controller.instance_root_create(
             req, body, instance_id, cluster_instances)
         mock_cluster_root_create.assert_called_with(
-            self.context, instance_id, password,
+            self.context, instance_id, self.context.user, password,
             cluster_instances)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/extensions/mgmt/instances/test_models.py` & `trove-8.0.1/trove/tests/unittests/mgmt/test_models.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,39 +9,32 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from testtools.matchers import Equals
-from testtools.matchers import Is
-from testtools.matchers import Not
-from unittest.mock import ANY
-from unittest.mock import MagicMock
-from unittest.mock import patch
 import uuid
 
+from mock import MagicMock, patch, ANY
 from novaclient.client import Client
-from novaclient.v2.flavors import Flavor
-from novaclient.v2.flavors import FlavorManager
-from novaclient.v2.servers import Server
-from novaclient.v2.servers import ServerManager
+from novaclient.v2.flavors import FlavorManager, Flavor
+from novaclient.v2.servers import Server, ServerManager
 from oslo_config import cfg
-
+from testtools.matchers import Equals, Is, Not
 
 from trove.backup.models import Backup
-from trove.common import clients
 from trove.common import exception
+from trove.common import instance as rd_instance
+from trove.common import remote
 from trove.datastore import models as datastore_models
 import trove.extensions.mgmt.instances.models as mgmtmodels
 from trove.guestagent.api import API
 from trove.instance.models import DBInstance
 from trove.instance.models import InstanceServiceStatus
-from trove.instance import service_status as srvstatus
 from trove.instance.tasks import InstanceTasks
 from trove import rpc
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 CONF = cfg.CONF
 
@@ -79,15 +72,15 @@
         self.context.auth_token = 'some_secret_password'
         self.client = MagicMock(spec=Client)
         self.server_mgr = MagicMock(spec=ServerManager)
         self.client.servers = self.server_mgr
         self.flavor_mgr = MagicMock(spec=FlavorManager)
         self.client.flavors = self.flavor_mgr
         self.admin_client_patch = patch.object(
-            clients, 'create_admin_nova_client', return_value=self.client)
+            remote, 'create_admin_nova_client', return_value=self.client)
         self.addCleanup(self.admin_client_patch.stop)
         self.admin_client_patch.start()
         CONF.set_override('host', '127.0.0.1')
         CONF.set_override('exists_notification_interval', 1)
         CONF.set_override('notification_service_id', {'mysql': '123'})
 
         super(MockMgmtInstanceTest, self).setUp()
@@ -101,20 +94,20 @@
                               name='test_name',
                               id=str(uuid.uuid4()),
                               flavor_id='flavor_1',
                               datastore_version_id=self.version.id,
                               compute_instance_id='compute_id_1',
                               server_id='server_id_1',
                               tenant_id='tenant_id_1',
-                              server_status=srvstatus.ServiceStatuses.
+                              server_status=rd_instance.ServiceStatuses.
                               BUILDING.api_status,
                               deleted=False)
         instance.save()
         service_status = InstanceServiceStatus(
-            srvstatus.ServiceStatuses.RUNNING,
+            rd_instance.ServiceStatuses.RUNNING,
             id=str(uuid.uuid4()),
             instance_id=instance.id,
         )
         service_status.save()
         instance.set_task_status(task_status)
         instance.server_status = status
         instance.save()
@@ -125,15 +118,15 @@
 
     @classmethod
     def setUpClass(cls):
         super(TestNotificationTransformer, cls).setUpClass()
 
     @patch('trove.instance.models.LOG')
     def test_transformer(self, mock_logging):
-        status = srvstatus.ServiceStatuses.BUILDING.api_status
+        status = rd_instance.ServiceStatuses.BUILDING.api_status
         instance, service_status = self.build_db_instance(
             status, InstanceTasks.BUILDING)
         payloads = mgmtmodels.NotificationTransformer(
             context=self.context)()
         self.assertIsNotNone(payloads)
         payload = payloads[0]
         self.assertThat(payload['audit_period_beginning'],
@@ -187,15 +180,15 @@
         with patch.object(self.flavor_mgr, 'get', side_effect=[flavor, None]):
             self.assertThat(transformer._lookup_flavor('1'),
                             Equals(flavor.name))
             self.assertThat(transformer._lookup_flavor('2'),
                             Equals('unknown'))
 
     def test_transformer(self):
-        status = srvstatus.ServiceStatuses.BUILDING.api_status
+        status = rd_instance.ServiceStatuses.BUILDING.api_status
         instance, service_status = self.build_db_instance(
             status, InstanceTasks.BUILDING)
 
         flavor = MagicMock(spec=Flavor)
         flavor.name = 'db.small'
 
         server = MagicMock(spec=Server)
@@ -226,15 +219,15 @@
                                 Equals('flavor_1'))
                 self.assertThat(payload['user_id'], Equals('test_user_id'))
                 self.assertThat(payload['service_id'], Equals('123'))
         self.addCleanup(self.do_cleanup, instance, service_status)
 
     @patch('trove.extensions.mgmt.instances.models.LOG')
     def test_transformer_invalid_datastore_manager(self, mock_logging):
-        status = srvstatus.ServiceStatuses.BUILDING.api_status
+        status = rd_instance.ServiceStatuses.BUILDING.api_status
         instance, service_status = self.build_db_instance(
             status, InstanceTasks.BUILDING)
         version = datastore_models.DBDatastoreVersion.get_by(
             id=instance.datastore_version_id)
         version.update(manager='something invalid')
         server = MagicMock(spec=Server)
         server.user_id = 'test_user_id'
@@ -271,17 +264,17 @@
                                 Equals('test_user_id'))
                 self.assertThat(payload['service_id'],
                                 Equals('unknown-service-id-error'))
         version.update(manager='mysql')
         self.addCleanup(self.do_cleanup, instance, service_status)
 
     def test_transformer_shutdown_instance(self):
-        status = srvstatus.ServiceStatuses.SHUTDOWN.api_status
+        status = rd_instance.ServiceStatuses.SHUTDOWN.api_status
         instance, service_status = self.build_db_instance(status)
-        service_status.set_status(srvstatus.ServiceStatuses.SHUTDOWN)
+        service_status.set_status(rd_instance.ServiceStatuses.SHUTDOWN)
         server = MagicMock(spec=Server)
         server.user_id = 'test_user_id'
 
         mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                       instance,
                                                       server,
                                                       service_status)
@@ -299,17 +292,17 @@
                     self.assertIsNotNone(payloads)
                     self.assertNotIn(status.lower(),
                                      [db['status']
                                       for db in payloads])
         self.addCleanup(self.do_cleanup, instance, service_status)
 
     def test_transformer_no_nova_instance(self):
-        status = srvstatus.ServiceStatuses.SHUTDOWN.api_status
+        status = rd_instance.ServiceStatuses.SHUTDOWN.api_status
         instance, service_status = self.build_db_instance(status)
-        service_status.set_status(srvstatus.ServiceStatuses.SHUTDOWN)
+        service_status.set_status(rd_instance.ServiceStatuses.SHUTDOWN)
         mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                       instance,
                                                       None,
                                                       service_status)
         flavor = MagicMock(spec=Flavor)
         flavor.name = 'db.small'
         transformer = mgmtmodels.NovaNotificationTransformer(
@@ -324,15 +317,15 @@
                     self.assertIsNotNone(payloads)
                     self.assertNotIn(status.lower(),
                                      [db['status']
                                       for db in payloads])
         self.addCleanup(self.do_cleanup, instance, service_status)
 
     def test_transformer_flavor_cache(self):
-        status = srvstatus.ServiceStatuses.BUILDING.api_status
+        status = rd_instance.ServiceStatuses.BUILDING.api_status
         instance, service_status = self.build_db_instance(
             status, InstanceTasks.BUILDING)
 
         server = MagicMock(spec=Server)
         server.user_id = 'test_user_id'
         mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                       instance,
@@ -369,15 +362,15 @@
 class TestMgmtInstanceTasks(MockMgmtInstanceTest):
 
     @classmethod
     def setUpClass(cls):
         super(TestMgmtInstanceTasks, cls).setUpClass()
 
     def test_public_exists_events(self):
-        status = srvstatus.ServiceStatuses.BUILDING.api_status
+        status = rd_instance.ServiceStatuses.BUILDING.api_status
         instance, service_status = self.build_db_instance(
             status, task_status=InstanceTasks.BUILDING)
         server = MagicMock(spec=Server)
         server.user_id = 'test_user_id'
         mgmt_instance = mgmtmodels.SimpleMgmtInstance(self.context,
                                                       instance,
                                                       server,
@@ -446,15 +439,15 @@
                                               deleted=True)
                 self.assertEqual(deleted_instance.id, instance.id)
 
 
 class TestMgmtInstancePing(MockMgmtInstanceTest):
 
     def test_rpc_ping(self):
-        status = srvstatus.ServiceStatuses.RUNNING.api_status
+        status = rd_instance.ServiceStatuses.RUNNING.api_status
         instance, service_status = self.build_db_instance(
             status, task_status=InstanceTasks.NONE)
         mgmt_instance = mgmtmodels.MgmtInstance(instance,
                                                 instance,
                                                 None,
                                                 service_status)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/flavor/test_flavor_views.py` & `trove-8.0.1/trove/tests/unittests/flavor/test_flavor_views.py`

 * *Files 13% similar despite different names*

```diff
@@ -10,32 +10,29 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest.mock import Mock, patch
+from mock import Mock, patch
 from trove.flavor.views import FlavorView
 from trove.tests.unittests import trove_testtools
 
 
 class FlavorViewsTest(trove_testtools.TestCase):
 
     def setUp(self):
         super(FlavorViewsTest, self).setUp()
         self.flavor = Mock()
         self.flavor.id = 10
         self.flavor.str_id = '10'
         self.flavor.name = 'test_flavor'
         self.flavor.ram = 512
         self.links = 'my_links'
-        self.flavor.vcpus = '10'
-        self.flavor.disk = '0'
-        self.flavor.ephemeral = '0'
 
     def tearDown(self):
         super(FlavorViewsTest, self).tearDown()
 
     def test_data(self):
         data = [
             {'flavor_id': 10,
@@ -63,16 +60,9 @@
                                  msg + 'invalid id')
                 self.assertEqual(expected_str_id, result['flavor']['str_id'],
                                  msg + 'invalid str_id')
                 self.assertEqual(self.flavor.name, result['flavor']['name'],
                                  msg + 'invalid name')
                 self.assertEqual(self.flavor.ram, result['flavor']['ram'],
                                  msg + 'invalid ram')
-                self.assertEqual(self.flavor.vcpus, result['flavor']['vcpus'],
-                                 msg + 'invalid vcpus')
-                self.assertEqual(self.flavor.disk, result['flavor']['disk'],
-                                 msg + 'invalid disk')
-                self.assertEqual(self.flavor.ephemeral,
-                                 result['flavor']['ephemeral'],
-                                 msg + 'invalid ephemeral')
                 self.assertEqual(self.links, result['flavor']['links'],
                                  msg + 'invalid links')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_controller.py` & `trove-8.0.1/trove/tests/unittests/instance/test_instance_controller.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,27 +9,20 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-import copy
-from unittest import mock
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-import uuid
-
 import jsonschema
-from testtools.matchers import Equals
-from testtools.matchers import Is
+from mock import Mock
+from testtools.matchers import Is, Equals
 from testtools.testcase import skip
 
 from trove.common import apischema
-from trove.common import exception
 from trove.instance.service import InstanceController
 from trove.tests.unittests import trove_testtools
 
 
 class TestInstanceController(trove_testtools.TestCase):
     def setUp(self):
         super(TestInstanceController, self).setUp()
@@ -168,28 +161,14 @@
         error_messages = [error.message for error in errors]
         error_paths = [error.path.pop() for error in errors]
         self.assertEqual(1, len(errors))
         self.assertIn("'$%^' does not match '^.*[0-9a-zA-Z]+.*$'",
                       error_messages)
         self.assertIn("locality", error_paths)
 
-    def test_validate_create_valid_nics(self):
-        body = copy.copy(self.instance)
-        body['instance']['nics'] = [
-            {
-                'network_id': str(uuid.uuid4()),
-                'subnet_id': str(uuid.uuid4()),
-                'ip_address': '192.168.1.11'
-            }
-        ]
-
-        schema = self.controller.get_schema('create', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertTrue(validator.is_valid(body))
-
     def test_validate_restart(self):
         body = {"restart": {}}
         schema = self.controller.get_schema('action', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
     def test_validate_invalid_action(self):
@@ -207,60 +186,34 @@
 
     def test_validate_resize_volume_string(self):
         body = {"resize": {"volume": {"size": "4"}}}
         schema = self.controller.get_schema('action', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
 
-    def test_validate_resize_volume_string_start_with_zero(self):
-        body = {"resize": {"volume": {"size": "0040"}}}
-        schema = self.controller.get_schema('action', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertTrue(validator.is_valid(body))
-
     def test_validate_resize_volume_string_invalid_number(self):
         body = {"resize": {"volume": {"size": '-44.0'}}}
         schema = self.controller.get_schema('action', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertFalse(validator.is_valid(body))
         errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
         self.assertThat(errors[0].context[1].message,
-                        Equals("'-44.0' does not match '^0*[1-9]+[0-9]*$'"))
+                        Equals("'-44.0' does not match '^[0-9]+$'"))
         self.assertThat(errors[0].path.pop(), Equals('size'))
 
     def test_validate_resize_volume_invalid_characters(self):
         body = {"resize": {"volume": {"size": 'x'}}}
         schema = self.controller.get_schema('action', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertFalse(validator.is_valid(body))
         errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
         self.assertThat(errors[0].context[0].message,
                         Equals("'x' is not of type 'integer'"))
         self.assertThat(errors[0].context[1].message,
-                        Equals("'x' does not match '^0*[1-9]+[0-9]*$'"))
-        self.assertThat(errors[0].path.pop(), Equals('size'))
-
-    def test_validate_resize_volume_zero_number(self):
-        body = {"resize": {"volume": {"size": 0}}}
-        schema = self.controller.get_schema('action', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertFalse(validator.is_valid(body))
-        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
-        self.assertThat(errors[0].context[0].message,
-                        Equals("0 is less than the minimum of 1"))
-        self.assertThat(errors[0].path.pop(), Equals('size'))
-
-    def test_validate_resize_volume_string_zero_number(self):
-        body = {"resize": {"volume": {"size": '0'}}}
-        schema = self.controller.get_schema('action', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertFalse(validator.is_valid(body))
-        errors = sorted(validator.iter_errors(body), key=lambda e: e.path)
-        self.assertThat(errors[0].context[1].message,
-                        Equals("'0' does not match '^0*[1-9]+[0-9]*$'"))
+                        Equals("'x' does not match '^[0-9]+$'"))
         self.assertThat(errors[0].path.pop(), Equals('size'))
 
     def test_validate_resize_instance(self):
         body = {"resize": {"flavorRef": "https://endpoint/v1.0/123/flavors/2"}}
         schema = self.controller.get_schema('action', body)
         validator = jsonschema.Draft4Validator(schema)
         self.assertTrue(validator.is_valid(body))
@@ -303,29 +256,38 @@
 
     def _setup_modify_instance_mocks(self):
         instance = Mock()
         instance.detach_replica = Mock()
         instance.attach_configuration = Mock()
         instance.detach_configuration = Mock()
         instance.update_db = Mock()
-        instance.update_access = Mock()
         return instance
 
     def test_modify_instance_with_empty_args(self):
         instance = self._setup_modify_instance_mocks()
         args = {}
 
         self.controller._modify_instance(self.context, self.req,
                                          instance, **args)
 
         self.assertEqual(0, instance.detach_replica.call_count)
         self.assertEqual(0, instance.detach_configuration.call_count)
         self.assertEqual(0, instance.attach_configuration.call_count)
         self.assertEqual(0, instance.update_db.call_count)
 
+    def test_modify_instance_with_nonempty_args_calls_update_db(self):
+        instance = self._setup_modify_instance_mocks()
+        args = {}
+        args['any'] = 'anything'
+
+        self.controller._modify_instance(self.context, self.req,
+                                         instance, **args)
+
+        instance.update_db.assert_called_once_with(**args)
+
     def test_modify_instance_with_False_detach_replica_arg(self):
         instance = self._setup_modify_instance_mocks()
         args = {}
         args['detach_replica'] = False
 
         self.controller._modify_instance(self.context, self.req,
                                          instance, **args)
@@ -358,65 +320,19 @@
         args['configuration_id'] = None
 
         self.controller._modify_instance(self.context, self.req,
                                          instance, **args)
 
         self.assertEqual(1, instance.detach_configuration.call_count)
 
-    def test_update_api_invalid_field(self):
-        body = {
-            'instance': {
-                'invalid': 'invalid'
-            }
-        }
-        schema = self.controller.get_schema('update', body)
-        validator = jsonschema.Draft4Validator(schema)
-        self.assertFalse(validator.is_valid(body))
-
-    @mock.patch('trove.instance.models.Instance.load')
-    def test_update_name(self, load_mock):
-        body = {
-            'instance': {
-                'name': 'new_name'
-            }
-        }
-        ins_mock = MagicMock()
-        load_mock.return_value = ins_mock
-
-        self.controller.update(MagicMock(), 'fake_id', body, 'fake_tenant_id')
-
-        ins_mock.update_db.assert_called_once_with(name='new_name')
-
-    def test_update_multiple_operations(self):
-        body = {
-            'instance': {
-                'name': 'new_name',
-                'replica_of': None,
-                'configuration': 'fake_config_id'
-            }
-        }
-
-        self.assertRaises(
-            exception.BadRequest,
-            self.controller.update,
-            MagicMock(), 'fake_id', body, 'fake_tenant_id'
-        )
-
-    @mock.patch('trove.instance.models.Instance.load')
-    def test_update_name_and_access(self, load_mock):
-        body = {
-            'instance': {
-                'name': 'new_name',
-                'access': {
-                    'is_public': True,
-                    'allowed_cidrs': []
-                }
-            }
-        }
-        ins_mock = MagicMock()
-        load_mock.return_value = ins_mock
+    def test_modify_instance_with_all_args(self):
+        instance = self._setup_modify_instance_mocks()
+        args = {}
+        args['detach_replica'] = True
+        args['configuration_id'] = 'some_id'
 
-        self.controller.update(MagicMock(), 'fake_id', body, 'fake_tenant_id')
+        self.controller._modify_instance(self.context, self.req,
+                                         instance, **args)
 
-        ins_mock.update_db.assert_called_once_with(name='new_name')
-        ins_mock.update_access.assert_called_once_with(
-            body['instance']['access'])
+        self.assertEqual(1, instance.detach_replica.call_count)
+        self.assertEqual(1, instance.attach_configuration.call_count)
+        instance.update_db.assert_called_once_with(**args)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_models.py` & `trove-8.0.1/trove/tests/unittests/instance/test_instance_models.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,31 +9,29 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 import uuid
 
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock, patch
 
 from trove.backup import models as backup_models
 from trove.common import cfg
-from trove.common import clients
 from trove.common import exception
-from trove.common import neutron
+from trove.common.instance import ServiceStatuses
 from trove.datastore import models as datastore_models
 from trove.instance import models
 from trove.instance.models import DBInstance
 from trove.instance.models import DBInstanceFault
+from trove.instance.models import filter_ips
 from trove.instance.models import Instance
 from trove.instance.models import instance_encryption_key_cache
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import SimpleInstance
-from trove.instance.service_status import ServiceStatuses
 from trove.instance.tasks import InstanceTasks
 from trove.taskmanager import api as task_api
 from trove.tests.fakes import nova
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 CONF = cfg.CONF
@@ -47,57 +45,72 @@
         db_info = DBInstance(
             InstanceTasks.BUILDING, name="TestInstance")
         self.instance = SimpleInstance(
             None, db_info, InstanceServiceStatus(
                 ServiceStatuses.BUILDING), ds_version=Mock(), ds=Mock(),
             locality='affinity')
         self.instance.context = self.context
-        db_info.addresses = [{
-            'type': 'private',
-            'address': '123.123.123.123',
-            'network': 'net-id-private'}, {
-            'type': 'private',
-            'address': '10.123.123.123',
-            'network': 'net-id-private'}, {
-            'type': 'public',
-            'address': '15.123.123.123',
-            'network': 'net-id-public'}]
-
+        db_info.addresses = {"private": [{"addr": "123.123.123.123"}],
+                             "internal": [{"addr": "10.123.123.123"}],
+                             "public": [{"addr": "15.123.123.123"}]}
+        self.orig_conf = CONF.network_label_regex
         self.orig_ip_regex = CONF.ip_regex
         self.orig_black_list_regex = CONF.black_list_regex
 
     def tearDown(self):
         super(SimpleInstanceTest, self).tearDown()
+        CONF.network_label_regex = self.orig_conf
         CONF.ip_start = None
-        CONF.management_networks = []
-        CONF.ip_regex = self.orig_ip_regex
-        CONF.black_list_regex = self.orig_black_list_regex
-
-        neutron.reset_management_networks()
 
     def test_get_root_on_create(self):
         root_on_create_val = Instance.get_root_on_create(
             'redis')
         self.assertFalse(root_on_create_val)
 
     def test_filter_ips_white_list(self):
+        CONF.network_label_regex = '.*'
         CONF.ip_regex = '^(15.|123.)'
         CONF.black_list_regex = '^10.123.123.*'
         ip = self.instance.get_visible_ip_addresses()
+        ip = filter_ips(
+            ip, CONF.ip_regex, CONF.black_list_regex)
         self.assertEqual(2, len(ip))
-        self.assertIn('123.123.123.123', ip[0].get('address'))
-        self.assertIn('15.123.123.123', ip[1].get('address'))
+        self.assertIn('123.123.123.123', ip)
+        self.assertIn('15.123.123.123', ip)
 
     def test_filter_ips_black_list(self):
+        CONF.network_label_regex = '.*'
         CONF.ip_regex = '.*'
         CONF.black_list_regex = '^10.123.123.*'
         ip = self.instance.get_visible_ip_addresses()
+        ip = filter_ips(
+            ip, CONF.ip_regex, CONF.black_list_regex)
         self.assertEqual(2, len(ip))
         self.assertNotIn('10.123.123.123', ip)
 
+    def test_one_network_label(self):
+        CONF.network_label_regex = 'public'
+        ip = self.instance.get_visible_ip_addresses()
+        self.assertEqual(['15.123.123.123'], ip)
+
+    def test_two_network_labels(self):
+        CONF.network_label_regex = '^(private|public)$'
+        ip = self.instance.get_visible_ip_addresses()
+        self.assertEqual(2, len(ip))
+        self.assertIn('123.123.123.123', ip)
+        self.assertIn('15.123.123.123', ip)
+
+    def test_all_network_labels(self):
+        CONF.network_label_regex = '.*'
+        ip = self.instance.get_visible_ip_addresses()
+        self.assertEqual(3, len(ip))
+        self.assertIn('10.123.123.123', ip)
+        self.assertIn('123.123.123.123', ip)
+        self.assertIn('15.123.123.123', ip)
+
     def test_locality(self):
         self.assertEqual('affinity', self.instance.locality)
 
     def test_fault(self):
         fault_message = 'Error'
         fault_details = 'details'
         fault_date = 'now'
@@ -166,52 +179,44 @@
             tenant_id=self.tenant_id,
             state=self.backup_state,
             instance_id=self.instance_id,
             parent_id=self.parent_id,
             datastore_version_id=self.datastore_version.id,
             deleted=False
         )
+        self.backup.size = 1.1
+        self.backup.save()
         self.backup_id = self.backup.id
-
-        self.orig_client = clients.create_nova_client
-        clients.create_nova_client = nova.fake_create_nova_client
-
+        self.orig_client = models.create_nova_client
+        models.create_nova_client = nova.fake_create_nova_client
         self.orig_api = task_api.API(self.context).create_instance
         task_api.API(self.context).create_instance = Mock()
         self.run_with_quotas = models.run_with_quotas
         models.run_with_quotas = Mock()
         self.check = backup_models.DBBackup.check_swift_object_exist
         backup_models.DBBackup.check_swift_object_exist = Mock(
             return_value=True)
         self.locality = 'affinity'
-
-        self.swift_verify_patch = patch.object(models.Backup,
-                                               'verify_swift_auth_token')
-        self.addCleanup(self.swift_verify_patch.stop)
-        self.swift_verify_patch.start()
-
         super(CreateInstanceTest, self).setUp()
 
     @patch.object(task_api.API, 'get_client', Mock(return_value=Mock()))
     def tearDown(self):
         self.db_info.delete()
         self.backup.delete()
         self.datastore.delete()
         self.datastore_version.delete()
-        clients.create_nova_client = self.orig_client
+        models.create_nova_client = self.orig_client
         task_api.API(self.context).create_instance = self.orig_api
         models.run_with_quotas = self.run_with_quotas
         backup_models.DBBackup.check_swift_object_exist = self.check
         self.backup.delete()
         self.db_info.delete()
         super(CreateInstanceTest, self).tearDown()
 
     def test_exception_on_invalid_backup_size(self):
-        self.backup.size = 1.1
-        self.backup.save()
         self.assertEqual(self.backup.id, self.backup_id)
         exc = self.assertRaises(
             exception.BackupTooLarge, models.Instance.create,
             self.context, self.name, self.flavor_id,
             self.image_id, self.databases, self.users,
             self.datastore, self.datastore_version,
             self.volume_size, self.backup_id,
@@ -271,30 +276,29 @@
             name='name' + str(uuid.uuid4()),
             image_id='new_image',
             packages=str(uuid.uuid4()),
             datastore_id=self.datastore.id,
             manager='test',
             active=1)
 
-        self.safe_nova_client = clients.create_nova_client
-        clients.create_nova_client = nova.fake_create_nova_client
+        self.safe_nova_client = models.create_nova_client
+        models.create_nova_client = nova.fake_create_nova_client
         super(TestInstanceUpgrade, self).setUp()
 
     def tearDown(self):
         self.datastore.delete()
         self.datastore_version1.delete()
         self.datastore_version2.delete()
-        clients.create_nova_client = self.safe_nova_client
+        models.create_nova_client = self.safe_nova_client
         super(TestInstanceUpgrade, self).tearDown()
 
-    @patch('trove.common.clients.create_neutron_client')
     @patch.object(task_api.API, 'get_client', Mock(return_value=Mock()))
     @patch.object(task_api.API, 'upgrade')
     @patch('trove.tests.fakes.nova.LOG')
-    def test_upgrade(self, mock_logging, task_upgrade, mock_neutron_client):
+    def test_upgrade(self, mock_logging, task_upgrade):
         instance_model = DBInstance(
             InstanceTasks.NONE,
             id=str(uuid.uuid4()),
             name="TestUpgradeInstance",
             datastore_version_id=self.datastore_version1.id)
         instance_model.set_task_status(InstanceTasks.NONE)
         instance_model.save()
@@ -335,59 +339,76 @@
             name='name' + str(uuid.uuid4()),
             image_id=str(uuid.uuid4()),
             packages=str(uuid.uuid4()),
             datastore_id=self.datastore.id,
             manager='mysql',
             active=1)
 
-        self.databases = []
-
-        self.users = []
-
         self.master = DBInstance(
             InstanceTasks.NONE,
             id=str(uuid.uuid4()),
             name="TestMasterInstance",
-            datastore_version_id=self.datastore_version.id,
-            flavor_id=str(uuid.uuid4()),
-            volume_size=2)
+            datastore_version_id=self.datastore_version.id)
         self.master.set_task_status(InstanceTasks.NONE)
         self.master.save()
         self.master_status = InstanceServiceStatus(
             ServiceStatuses.RUNNING,
             id=str(uuid.uuid4()),
             instance_id=self.master.id)
         self.master_status.save()
 
-        self.safe_nova_client = clients.create_nova_client
-        clients.create_nova_client = nova.fake_create_nova_client
-
-        self.swift_verify_patch = patch.object(models.Backup,
-                                               'verify_swift_auth_token')
-        self.addCleanup(self.swift_verify_patch.stop)
-        self.swift_verify_patch.start()
-
+        self.safe_nova_client = models.create_nova_client
+        models.create_nova_client = nova.fake_create_nova_client
         super(TestReplication, self).setUp()
 
     def tearDown(self):
         self.master.delete()
         self.master_status.delete()
         self.datastore.delete()
         self.datastore_version.delete()
-        clients.create_nova_client = self.safe_nova_client
+        models.create_nova_client = self.safe_nova_client
         super(TestReplication, self).tearDown()
 
     @patch('trove.instance.models.LOG')
+    def test_replica_of_not_active_master(self, mock_logging):
+        self.master.set_task_status(InstanceTasks.BUILDING)
+        self.master.save()
+        self.master_status.set_status(ServiceStatuses.BUILDING)
+        self.master_status.save()
+        self.assertRaises(exception.UnprocessableEntity,
+                          Instance.create,
+                          None, 'name', 1, "UUID", [], [], self.datastore,
+                          self.datastore_version, 1,
+                          None, slave_of_id=self.master.id)
+
+    @patch('trove.instance.models.LOG')
     def test_replica_with_invalid_slave_of_id(self, mock_logging):
         self.assertRaises(exception.NotFound,
                           Instance.create,
                           None, 'name', 1, "UUID", [], [], self.datastore,
-                          self.datastore_version, 2,
+                          self.datastore_version, 1,
                           None, slave_of_id=str(uuid.uuid4()))
 
+    def test_create_replica_from_replica(self):
+        self.replica_datastore_version = Mock(
+            spec=datastore_models.DBDatastoreVersion)
+        self.replica_datastore_version.id = "UUID"
+        self.replica_datastore_version.manager = 'mysql'
+        self.replica_info = DBInstance(
+            InstanceTasks.NONE,
+            id="UUID",
+            name="TestInstance",
+            datastore_version_id=self.replica_datastore_version.id,
+            slave_of_id=self.master.id)
+        self.replica_info.save()
+        self.assertRaises(exception.Forbidden, Instance.create,
+                          None, 'name', 2, "UUID", [], [], self.datastore,
+                          self.datastore_version, 1,
+                          None, slave_of_id=self.replica_info.id)
+
 
 def trivial_key_function(id):
     return id * id
 
 
 class TestInstanceKeyCaching(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_status.py` & `trove-8.0.1/trove/tests/unittests/instance/test_instance_status.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,25 +9,22 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from unittest import mock
-import uuid
-
+from trove.common.instance import ServiceStatuses
 from trove.datastore import models
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import InstanceStatus
 from trove.instance.models import SimpleInstance
-from trove.instance.service_status import ServiceStatuses
-from trove.instance.tasks import InstanceTasks
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
+import uuid
 
 
 class FakeInstanceTask(object):
 
     def __init__(self):
         self.is_error = False
         self.action = None
@@ -40,113 +37,101 @@
         self.deleted = False
         self.datastore_version_id = str(uuid.uuid4())
         self.server_status = "ACTIVE"
         self.task_status = FakeInstanceTask()
 
 
 class BaseInstanceStatusTestCase(trove_testtools.TestCase):
-    @classmethod
-    def setUpClass(cls):
+
+    def setUp(self):
         util.init_db()
-        cls.db_info = FakeDBInstance()
-        cls.datastore = models.DBDatastore.create(
+        self.db_info = FakeDBInstance()
+        self.status = InstanceServiceStatus(
+            ServiceStatuses.RUNNING)
+        self.datastore = models.DBDatastore.create(
             id=str(uuid.uuid4()),
             name='mysql' + str(uuid.uuid4()),
-            default_version_id=cls.db_info.datastore_version_id
+            default_version_id=self.db_info.datastore_version_id
         )
-        cls.version = models.DBDatastoreVersion.create(
-            id=cls.db_info.datastore_version_id,
-            datastore_id=cls.datastore.id,
-            name='5.7' + str(uuid.uuid4()),
+        self.version = models.DBDatastoreVersion.create(
+            id=self.db_info.datastore_version_id,
+            datastore_id=self.datastore.id,
+            name='5.5' + str(uuid.uuid4()),
             manager='mysql',
             image_id=str(uuid.uuid4()),
             active=1,
-            packages="mysql-server-5.7"
+            packages="mysql-server-5.5"
         )
-        super(BaseInstanceStatusTestCase, cls).setUpClass()
+        super(BaseInstanceStatusTestCase, self).setUp()
 
-    @classmethod
-    def tearDownClass(cls):
-        util.cleanup_db()
-        super(BaseInstanceStatusTestCase, cls).tearDownClass()
+    def tearDown(self):
+        self.datastore.delete()
+        self.version.delete()
+        super(BaseInstanceStatusTestCase, self).tearDown()
 
 
 class InstanceStatusTest(BaseInstanceStatusTestCase):
-    def setUp(self):
-        self.db_info.task_status = FakeInstanceTask()
-        self.db_info.server_status = "ACTIVE"
-        self.ds_status = InstanceServiceStatus(ServiceStatuses.HEALTHY)
-        super(InstanceStatusTest, self).setUp()
 
     def test_task_status_error_reports_error(self):
         self.db_info.task_status.is_error = True
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.ERROR, instance.status)
 
     def test_task_status_action_building_reports_build(self):
         self.db_info.task_status.action = "BUILDING"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.BUILD, instance.status)
 
     def test_task_status_action_rebooting_reports_reboot(self):
         self.db_info.task_status.action = "REBOOTING"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.REBOOT, instance.status)
 
     def test_task_status_action_resizing_reports_resize(self):
         self.db_info.task_status.action = "RESIZING"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.RESIZE, instance.status)
 
-    def test_task_deleting_server_active(self):
+    def test_task_status_action_deleting_reports_shutdown(self):
         self.db_info.task_status.action = "DELETING"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.SHUTDOWN, instance.status)
 
     def test_nova_server_build_reports_build(self):
         self.db_info.server_status = "BUILD"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.BUILD, instance.status)
 
     def test_nova_server_error_reports_error(self):
         self.db_info.server_status = "ERROR"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.ERROR, instance.status)
 
     def test_nova_server_reboot_reports_reboot(self):
         self.db_info.server_status = "REBOOT"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.REBOOT, instance.status)
 
     def test_nova_server_resize_reports_resize(self):
         self.db_info.server_status = "RESIZE"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.RESIZE, instance.status)
 
     def test_nova_server_verify_resize_reports_resize(self):
         self.db_info.server_status = "VERIFY_RESIZE"
-        instance = SimpleInstance('dummy context', self.db_info,
-                                  self.ds_status)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
         self.assertEqual(InstanceStatus.RESIZE, instance.status)
 
-    def test_operating_status_healthy(self):
-        self.db_info.task_status = InstanceTasks.NONE
-        instance = SimpleInstance(mock.MagicMock(), self.db_info,
-                                  self.ds_status)
-        self.assertEqual(repr(ServiceStatuses.HEALTHY),
-                         instance.operating_status)
-
-    def test_operating_status_task_not_none(self):
-        self.db_info.task_status = InstanceTasks.RESIZING
-        instance = SimpleInstance(mock.MagicMock(), self.db_info,
-                                  self.ds_status)
-        self.assertEqual("",
-                         instance.operating_status)
+    def test_service_status_paused_reports_reboot(self):
+        self.status.set_status(ServiceStatuses.PAUSED)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
+        self.assertEqual(InstanceStatus.REBOOT, instance.status)
+
+    def test_service_status_new_reports_build(self):
+        self.status.set_status(ServiceStatuses.NEW)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
+        self.assertEqual(InstanceStatus.BUILD, instance.status)
+
+    def test_service_status_running_reports_active(self):
+        self.status.set_status(ServiceStatuses.RUNNING)
+        instance = SimpleInstance('dummy context', self.db_info, self.status)
+        self.assertEqual(InstanceStatus.ACTIVE, instance.status)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/instance/test_instance_views.py` & `trove-8.0.1/trove/tests/unittests/instance/test_instance_views.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,67 +9,61 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
+from mock import Mock
 from trove.common import cfg
 from trove.instance.views import InstanceDetailView
 from trove.instance.views import InstanceView
 from trove.tests.unittests import trove_testtools
-from unittest.mock import MagicMock
-from unittest.mock import Mock
 
 CONF = cfg.CONF
 
 
 class InstanceViewsTest(trove_testtools.TestCase):
 
     def setUp(self):
         super(InstanceViewsTest, self).setUp()
-        self.addresses = [{
-            'type': 'private',
-            'address': '123.123.123.123',
-            'network': 'net-id-private'}, {
-            'type': 'private',
-            'address': '10.123.123.123',
-            'network': 'net-id-private'}, {
-            'type': 'public',
-            'address': '15.123.123.123',
-            'network': 'net-id-public'}]
+        self.addresses = {"private": [{"addr": "123.123.123.123"}],
+                          "internal": [{"addr": "10.123.123.123"}],
+                          "public": [{"addr": "15.123.123.123"}]}
+        self.orig_label_regex = CONF.network_label_regex
         self.orig_ip_regex = CONF.ip_regex
 
     def tearDown(self):
         super(InstanceViewsTest, self).tearDown()
+        CONF.network_label_regex = self.orig_label_regex
         CONF.ip_regex = self.orig_ip_regex
 
 
 class InstanceDetailViewTest(trove_testtools.TestCase):
 
     def setUp(self):
         super(InstanceDetailViewTest, self).setUp()
         self.build_links_method = InstanceView._build_links
+        self.build_flavor_links_method = InstanceView._build_flavor_links
         self.build_config_method = InstanceDetailView._build_configuration_info
         InstanceView._build_links = Mock()
+        InstanceView._build_flavor_links = Mock()
         InstanceDetailView._build_configuration_info = Mock()
         self.instance = Mock()
         self.instance.created = 'Yesterday'
         self.instance.updated = 'Now'
         self.instance.datastore_version = Mock()
         self.instance.datastore_version.name = 'mysql_test_version'
         self.instance.datastore_version.manager = 'mysql'
         self.instance.hostname = 'test.trove.com'
         self.ip = "1.2.3.4"
-        self.instance.addresses = [
-            {"address": self.ip, 'type': 'private', 'network': 'net-id'}]
+        self.instance.addresses = {"private": [{"addr": self.ip}]}
         self.instance.volume_used = '3'
         self.instance.root_password = 'iloveyou'
-        self.instance.get_visible_ip_addresses.return_value = [
-            {'type': 'private', 'address': '1.2.3.4', 'network': 'net-id'}]
+        self.instance.get_visible_ip_addresses = lambda: ["1.2.3.4"]
         self.instance.slave_of_id = None
         self.instance.slaves = []
         self.instance.locality = 'affinity'
         self.instance.server_id = 'server_abc'
         self.instance.volume_id = 'volume_abc'
         self.fault_message = 'Error'
         self.fault_details = 'details'
@@ -82,14 +76,15 @@
         self.req = Mock()
         self.req.environ = Mock()
         self.req.environ.__getitem__ = Mock(return_value=self.context)
 
     def tearDown(self):
         super(InstanceDetailViewTest, self).tearDown()
         InstanceView._build_links = self.build_links_method
+        InstanceView._build_flavor_links = self.build_flavor_links_method
         InstanceDetailView._build_configuration_info = self.build_config_method
 
     def test_data_hostname(self):
         view = InstanceDetailView(self.instance, self.req)
         result = view.data()
         self.assertEqual(self.instance.created, result['instance']['created'])
         self.assertEqual(self.instance.updated, result['instance']['updated'])
@@ -105,16 +100,14 @@
         result = view.data()
         self.assertEqual(self.instance.created, result['instance']['created'])
         self.assertEqual(self.instance.updated, result['instance']['updated'])
         self.assertEqual(self.instance.datastore_version.name,
                          result['instance']['datastore']['version'])
         self.assertNotIn('hostname', result['instance'])
         self.assertEqual([self.ip], result['instance']['ip'])
-        self.assertEqual(self.ip,
-                         result['instance']['addresses'][0]['address'])
 
     def test_locality(self):
         self.instance.hostname = None
         view = InstanceDetailView(self.instance, self.req)
         result = view.data()
         self.assertEqual(self.instance.locality,
                          result['instance']['locality'])
@@ -138,31 +131,7 @@
 
     def test_non_admin_view(self):
         self.context.is_admin = False
         view = InstanceDetailView(self.instance, self.req)
         result = view.data()
         self.assertNotIn('server_id', result['instance'])
         self.assertNotIn('volume_id', result['instance'])
-
-    def test_access(self):
-        instance = MagicMock()
-        instance.hostname = None
-        instance.get_visible_ip_addresses.return_value = [
-            {'address': '10.111.0.27', 'type': 'private', 'network': 'net-id'}
-        ]
-        instance.access = None
-        instance.slaves = []
-
-        view = InstanceDetailView(instance, self.req)
-        data = view.data()['instance']
-
-        self.assertFalse(data['access']['is_public'])
-
-        instance.get_visible_ip_addresses.return_value = [
-            {'address': '10.111.0.27', 'type': 'private', 'network': 'net-id'},
-            {'address': '172.30.5.107', 'type': 'public', 'network': 'net-id'}
-        ]
-
-        view = InstanceDetailView(instance, self.req)
-        data = view.data()['instance']
-
-        self.assertTrue(data['access']['is_public'])
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/instance/test_service.py` & `trove-8.0.1/trove/tests/api/mgmt/instances.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,255 +1,274 @@
-# Copyright 2020 Catalyst Cloud
+#    Copyright 2011 OpenStack Foundation
 #
-#    Licensed under the Apache License, Version 2.0 (the "License");
-#    you may not use this file except in compliance with the License.
-#    You may obtain a copy of the License at
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
 #
-#        http://www.apache.org/licenses/LICENSE-2.0
+#         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS,
-#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#    See the License for the specific language governing permissions and
-#    limitations under the License.
-from datetime import timedelta
-from unittest import mock
-
-from trove.common import cfg
-from trove.common import clients
-from trove.common import exception
-from trove.common import timeutils
-from trove.datastore import models as ds_models
-from trove.instance import models as ins_models
-from trove.instance import service
-from trove.instance import service_status as srvstatus
-from trove.tests.unittests import trove_testtools
-from trove.tests.unittests.util import util
-
-CONF = cfg.CONF
-
-
-class TestInstanceController(trove_testtools.TestCase):
-    @classmethod
-    def setUpClass(cls):
-        util.init_db()
-
-        cls.ds_name = cls.random_name('datastore',
-                                      prefix='TestInstanceController')
-        ds_models.update_datastore(name=cls.ds_name, default_version=None)
-        cls.ds = ds_models.Datastore.load(cls.ds_name)
-
-        ds_models.update_datastore_version(
-            cls.ds_name, 'test_image_id', 'mysql', cls.random_uuid(), [], '',
-            1)
-        ds_models.update_datastore_version(
-            cls.ds_name, 'test_image_tags', 'mysql', '', ['trove', 'mysql'],
-            '', 1, version='test_image_tags version')
-        ds_models.update_datastore_version(
-            cls.ds_name, 'test_version', 'mysql', '', ['trove'], '', 1,
-            version='version 1')
-        ds_models.update_datastore_version(
-            cls.ds_name, 'test_version', 'mysql', '', ['trove'], '', 1,
-            version='version 2')
-
-        cls.ds_version_imageid = ds_models.DatastoreVersion.load(
-            cls.ds, 'test_image_id')
-        cls.ds_version_imagetags = ds_models.DatastoreVersion.load(
-            cls.ds, 'test_image_tags')
-
-        cls.controller = service.InstanceController()
-
-        super(TestInstanceController, cls).setUpClass()
-
-    @classmethod
-    def tearDownClass(cls):
-        util.cleanup_db()
-
-        super(TestInstanceController, cls).tearDownClass()
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+from proboscis.asserts import assert_equal
+from proboscis.asserts import assert_raises
+from proboscis import before_class
+from proboscis.check import Check
+from proboscis import SkipTest
+from proboscis import test
+import six
+from troveclient.compat import exceptions
+
+from trove.common.utils import poll_until
+from trove.tests.api.instances import CreateInstance
+from trove.tests.api.instances import GROUP_START
+from trove.tests.api.instances import GROUP_TEST
+from trove.tests.api.instances import instance_info
+from trove.tests.config import CONFIG
+from trove.tests.util.check import CollectionCheck
+from trove.tests.util.check import TypeCheck
+from trove.tests.util import create_client
+from trove.tests.util import create_dbaas_client
+from trove.tests.util.users import Requirements
+
+
+GROUP = "dbaas.api.mgmt.instances"
+
+
+@test(groups=[GROUP])
+def mgmt_index_requires_admin_account():
+    """Verify that an admin context is required to call this function."""
+    client = create_client(is_admin=False)
+    assert_raises(exceptions.Unauthorized, client.management.index)
+
+
+# These functions check some dictionaries in the returned response.
+def flavor_check(flavor):
+    with CollectionCheck("flavor", flavor) as check:
+        check.has_element("id", six.string_types)
+        check.has_element("links", list)
+
+
+def datastore_check(datastore):
+    with CollectionCheck("datastore", datastore) as check:
+        check.has_element("type", six.string_types)
+        check.has_element("version", six.string_types)
+
+
+def guest_status_check(guest_status):
+    with CollectionCheck("guest_status", guest_status) as check:
+        check.has_element("state_description", six.string_types)
+
+
+def volume_check(volume):
+    with CollectionCheck("volume", volume) as check:
+        check.has_element("id", six.string_types)
+        check.has_element("size", int)
+        check.has_element("used", float)
+        check.has_element("total", float)
+
+
+@test(depends_on_groups=[GROUP_START], groups=[GROUP, GROUP_TEST])
+def mgmt_instance_get():
+    """Tests the mgmt instances index method."""
+    reqs = Requirements(is_admin=True)
+    user = CONFIG.users.find_user(reqs)
+    client = create_dbaas_client(user)
+    mgmt = client.management
+    # Grab the info.id created by the main instance test which is stored in
+    # a global.
+    id = instance_info.id
+    api_instance = mgmt.show(id)
+    datastore = getattr(api_instance, 'datastore')
+    datastore_type = datastore.get('type')
+
+    # Print out all fields for extra info if the test fails.
+    for name in dir(api_instance):
+        print(str(name) + "=" + str(getattr(api_instance, name)))
+    with TypeCheck("instance", api_instance) as instance:
+        instance.has_field('created', six.string_types)
+        instance.has_field('deleted', bool)
+        # If the instance hasn't been deleted, this should be false... but
+        # lets avoid creating more ordering work.
+        instance.has_field('deleted_at', (six.string_types, None))
+        instance.has_field('flavor', dict, flavor_check)
+        instance.has_field('datastore', dict, datastore_check)
+        instance.has_field('guest_status', dict, guest_status_check)
+        instance.has_field('id', six.string_types)
+        instance.has_field('links', list)
+        instance.has_field('name', six.string_types)
+        # instance.has_field('server_status', six.string_types)
+        instance.has_field('status', six.string_types)
+        instance.has_field('tenant_id', six.string_types)
+        instance.has_field('updated', six.string_types)
+        # Can be None if no volume is given on this instance.
+        volume_support = CONFIG.get(datastore_type, 'mysql')['volume_support']
+        if volume_support:
+            instance.has_field('volume', dict, volume_check)
+        else:
+            instance.has_field('volume', None)
+        # TODO(tim-simpson): Validate additional fields, assert
+        # no extra fields exist.
+    if api_instance.server is not None:
+        print("the real content of server: %s" % dir(api_instance.server))
+        print("the type of server: %s" % type(api_instance.server))
+        print("the real content of api_instance: %s" % dir(api_instance))
+        print("the type of api_instance: %s" % type(api_instance))
+        print(hasattr(api_instance, "server"))
+
+        with CollectionCheck("server", api_instance.server) as server:
+            server.has_element("addresses", dict)
+            server.has_element("deleted", bool)
+            server.has_element("deleted_at", (six.string_types, None))
+            server.has_element("host", six.string_types)
+            server.has_element("id", six.string_types)
+            server.has_element("local_id", int)
+            server.has_element("name", six.string_types)
+            server.has_element("status", six.string_types)
+            server.has_element("tenant_id", six.string_types)
+
+    if (volume_support and
+            CONFIG.trove_main_instance_has_volume):
+        with CollectionCheck("volume", api_instance.volume) as volume:
+            volume.has_element("attachments", list)
+            volume.has_element("availability_zone", six.string_types)
+            volume.has_element("created_at", (six.string_types, None))
+            volume.has_element("id", six.string_types)
+            volume.has_element("size", int)
+            volume.has_element("status", six.string_types)
+
+
+@test(groups=["fake." + GROUP])
+class WhenMgmtInstanceGetIsCalledButServerIsNotReady(object):
+
+    @before_class
+    def set_up(self):
+        """Create client for mgmt instance test (2)."""
+        if not CONFIG.fake_mode:
+            raise SkipTest("This test only works in fake mode.")
+        self.client = create_client(is_admin=True)
+        self.mgmt = self.client.management
+        # Fake nova will fail a server ending with 'test_SERVER_ERROR'."
+        # Fake volume will fail if the size is 13.
+        # TODO(tim.simpson): This would be a lot nicer looking if we used a
+        #                    traditional mock framework.
+        datastore = {'type': 'mysql', 'version': '5.5'}
+        body = {'datastore': datastore}
+        vol_support = CONFIG.get(datastore['type'], 'mysql')['volume_support']
+        if vol_support:
+            body.update({'size': 13})
+        response = self.client.instances.create(
+            'test_SERVER_ERROR',
+            instance_info.dbaas_flavor_href,
+            body,
+            [])
+        poll_until(lambda: self.client.instances.get(response.id),
+                   lambda instance: instance.status == 'ERROR',
+                   time_out=10)
+        self.id = response.id
+
+    @test
+    def mgmt_instance_get(self):
+        """Tests the mgmt get call works when the Nova server isn't ready."""
+        api_instance = self.mgmt.show(self.id)
+        # Print out all fields for extra info if the test fails.
+        for name in dir(api_instance):
+            print(str(name) + "=" + str(getattr(api_instance, name)))
+        # Print out all fields for extra info if the test fails.
+        for name in dir(api_instance):
+            print(str(name) + "=" + str(getattr(api_instance, name)))
+        with TypeCheck("instance", api_instance) as instance:
+            instance.has_field('created', six.string_types)
+            instance.has_field('deleted', bool)
+            # If the instance hasn't been deleted, this should be false... but
+            # lets avoid creating more ordering work.
+            instance.has_field('deleted_at', (six.string_types, None))
+            instance.has_field('flavor', dict, flavor_check)
+            instance.has_field('datastore', dict, datastore_check)
+            instance.has_field('guest_status', dict, guest_status_check)
+            instance.has_field('id', six.string_types)
+            instance.has_field('links', list)
+            instance.has_field('name', six.string_types)
+            # instance.has_field('server_status', six.string_types)
+            instance.has_field('status', six.string_types)
+            instance.has_field('tenant_id', six.string_types)
+            instance.has_field('updated', six.string_types)
+            # Can be None if no volume is given on this instance.
+            instance.has_field('server', None)
+            instance.has_field('volume', None)
+            # TODO(tim-simpson): Validate additional fields,
+            # assert no extra fields exist.
+
+
+@test(depends_on_classes=[CreateInstance], groups=[GROUP])
+class MgmtInstancesIndex(object):
+    """Tests the mgmt instances index method."""
 
+    @before_class
     def setUp(self):
-        trove_testtools.patch_notifier(self)
-        super(TestInstanceController, self).setUp()
-
-    @mock.patch.object(clients, 'create_glance_client')
-    @mock.patch('trove.instance.models.Instance.create')
-    def test_create_by_ds_version_image_tags(self, mock_model_create,
-                                             mock_create_client):
-        image_id = self.random_uuid()
-        mock_glance_client = mock.MagicMock()
-        mock_glance_client.images.list.return_value = [{'id': image_id}]
-        mock_create_client.return_value = mock_glance_client
-
-        name = self.random_name(name='instance',
-                                prefix='TestInstanceController')
-        flavor = self.random_uuid()
-        body = {
-            'instance': {
-                'name': name,
-                'flavorRef': flavor,
-                'datastore': {
-                    'type': self.ds_name,
-                    'version': self.ds_version_imagetags.name,
-                    'version_number': self.ds_version_imagetags.version
-                }
-            }
-        }
-        ret = self.controller.create(mock.MagicMock(), body, mock.ANY)
-
-        self.assertEqual(200, ret.status)
-        mock_glance_client.images.list.assert_called_once_with(
-            filters={'tag': ['trove', 'mysql'], 'status': 'active'},
-            sort='created_at:desc', limit=1
-        )
-
-        mock_model_create.assert_called_once_with(
-            mock.ANY, name, flavor, image_id,
-            [], [],
-            mock.ANY, mock.ANY,
-            None, None, None, [], None, None,
-            replica_count=None, volume_type=None, modules=None, locality=None,
-            region_name=CONF.service_credentials.region_name, access=None
-        )
-        args = mock_model_create.call_args[0]
-        actual_ds_version = args[7]
-        self.assertEqual(self.ds_version_imagetags.name,
-                         actual_ds_version.name)
-        self.assertEqual(self.ds_version_imagetags.version,
-                         actual_ds_version.version)
-
-    def test_create_multiple_versions(self):
-        body = {
-            'instance': {
-                'name': self.random_name(name='instance',
-                                         prefix='TestInstanceController'),
-                'flavorRef': self.random_uuid(),
-                'datastore': {
-                    'type': self.ds_name,
-                    'version': 'test_version'
-                }
-            }
-        }
-
-        self.assertRaises(
-            exception.DatastoreVersionsNoUniqueMatch,
-            self.controller.create,
-            mock.MagicMock(), body, mock.ANY
-        )
-
-    @mock.patch.object(clients, 'create_nova_client',
-                       return_value=mock.MagicMock())
-    @mock.patch('trove.rpc.get_client')
-    def test_update_datastore_version(self, mock_get_rpc_client,
-                                      mock_create_nova_client):
-        # Create an instance in db.
-        instance = ins_models.DBInstance.create(
-            name=self.random_name('instance'),
-            flavor_id=self.random_uuid(),
-            tenant_id=self.random_uuid(),
-            volume_size=1,
-            datastore_version_id=self.ds_version_imageid.id,
-            task_status=ins_models.InstanceTasks.BUILDING,
-            compute_instance_id=self.random_uuid()
-        )
-        ins_models.InstanceServiceStatus.create(
-            instance_id=instance.id,
-            status=srvstatus.ServiceStatuses.NEW
-        )
-
-        # Create a new datastore version in db.
-        new_version_name = self.random_name('version')
-        ds_models.update_datastore_version(
-            self.ds_name, new_version_name,
-            'mysql', self.random_uuid(), [], '', 1
-        )
-        new_ds_version = ds_models.DatastoreVersion.load(
-            self.ds, new_version_name)
-
-        body = {
-            'instance': {
-                'datastore_version': new_ds_version.id
-            }
-        }
-        self.controller.update(mock.MagicMock(), instance.id, body, mock.ANY)
-
-        rpc_ctx = mock_get_rpc_client.return_value.prepare.return_value
-        rpc_ctx.cast.assert_called_once_with(
-            mock.ANY, "upgrade",
-            instance_id=instance.id,
-            datastore_version_id=new_ds_version.id)
-
-    @mock.patch('trove.instance.models.load_server_group_info')
-    @mock.patch('trove.instance.models.load_guest_info')
-    @mock.patch('trove.instance.models.load_simple_instance_addresses')
-    @mock.patch('trove.instance.models.load_simple_instance_server_status')
-    def test_show_with_restart_required(self, load_server_mock,
-                                        load_addr_mock, load_guest_mock,
-                                        load_server_grp_mock):
-        # Create an instance in db.
-        instance = ins_models.DBInstance.create(
-            name=self.random_name('instance'),
-            flavor_id=self.random_uuid(),
-            tenant_id=self.random_uuid(),
-            volume_size=1,
-            datastore_version_id=self.ds_version_imageid.id,
-            task_status=ins_models.InstanceTasks.NONE,
-            compute_instance_id=self.random_uuid(),
-            server_status='ACTIVE'
-        )
-        ins_models.InstanceServiceStatus.create(
-            instance_id=instance.id,
-            status=srvstatus.ServiceStatuses.RESTART_REQUIRED,
-        )
-
-        # workaround to reset updated_at field.
-        service_status = ins_models.InstanceServiceStatus.find_by(
-            instance_id=instance.id)
-        service_status.updated_at = timeutils.utcnow() - timedelta(
-            seconds=(CONF.agent_heartbeat_expiry + 60))
-        ins_models.get_db_api().save(service_status)
-
-        ret = self.controller.show(mock.MagicMock(), mock.ANY, instance.id)
-        self.assertEqual(200, ret.status)
-
-        ret_instance = ret.data(None)['instance']
-
-        self.assertEqual('ACTIVE', ret_instance.get('status'))
-        self.assertEqual('RESTART_REQUIRED',
-                         ret_instance.get('operating_status'))
-
-    @mock.patch('trove.instance.models.load_server_group_info')
-    @mock.patch('trove.instance.models.load_guest_info')
-    @mock.patch('trove.instance.models.load_simple_instance_addresses')
-    @mock.patch('trove.instance.models.load_simple_instance_server_status')
-    def test_show_without_restart_required(self, load_server_mock,
-                                           load_addr_mock, load_guest_mock,
-                                           load_server_grp_mock):
-        # Create an instance in db.
-        instance = ins_models.DBInstance.create(
-            name=self.random_name('instance'),
-            flavor_id=self.random_uuid(),
-            tenant_id=self.random_uuid(),
-            volume_size=1,
-            datastore_version_id=self.ds_version_imageid.id,
-            task_status=ins_models.InstanceTasks.NONE,
-            compute_instance_id=self.random_uuid(),
-            server_status='ACTIVE'
-        )
-        ins_models.InstanceServiceStatus.create(
-            instance_id=instance.id,
-            status=srvstatus.ServiceStatuses.HEALTHY,
-        )
-
-        # workaround to reset updated_at field.
-        service_status = ins_models.InstanceServiceStatus.find_by(
-            instance_id=instance.id)
-        service_status.updated_at = timeutils.utcnow() - timedelta(
-            seconds=(CONF.agent_heartbeat_expiry + 60))
-        ins_models.get_db_api().save(service_status)
-
-        ret = self.controller.show(mock.MagicMock(), mock.ANY, instance.id)
-        self.assertEqual(200, ret.status)
-
-        ret_instance = ret.data(None)['instance']
-
-        self.assertEqual('ACTIVE', ret_instance.get('status'))
-        self.assertEqual('ERROR', ret_instance.get('operating_status'))
+        """Create client for mgmt instance test."""
+        reqs = Requirements(is_admin=True)
+        self.user = CONFIG.users.find_user(reqs)
+        self.client = create_dbaas_client(self.user)
+
+    @test
+    def test_mgmt_instance_index_fields_present(self):
+        """
+        Verify that all the expected fields are returned by the index method.
+        """
+        expected_fields = [
+            'created',
+            'deleted',
+            'deleted_at',
+            'flavor',
+            'datastore',
+            'id',
+            'links',
+            'name',
+            'server',
+            'status',
+            'task_description',
+            'tenant_id',
+            'updated',
+            'region'
+        ]
+
+        if CONFIG.trove_volume_support:
+            expected_fields.append('volume')
+
+        index = self.client.management.index()
+
+        if not hasattr(index, "deleted"):
+            raise SkipTest("instance index must have a "
+                           "deleted label for this test")
+
+        for instance in index:
+            with Check() as check:
+                for field in expected_fields:
+                    check.true(hasattr(instance, field),
+                               "Index lacks field %s" % field)
+
+    @test
+    def test_mgmt_instance_index_check_filter(self):
+        """
+        Make sure that the deleted= filter works as expected, and no instances
+        are excluded.
+        """
+
+        if not hasattr(self.client.management.index, 'deleted'):
+            raise SkipTest("instance index must have a deleted "
+                           "label for this test")
+        instance_counts = []
+        for deleted_filter in (True, False):
+            filtered_index = self.client.management.index(
+                deleted=deleted_filter)
+            instance_counts.append(len(filtered_index))
+        for instance in filtered_index:
+                # Every instance listed here should have the proper value
+                # for 'deleted'.
+                assert_equal(deleted_filter, instance.deleted)
+        full_index = self.client.management.index()
+        # There should be no instances that are neither deleted or not-deleted.
+        assert_equal(len(full_index), sum(instance_counts))
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/module/test_module_controller.py` & `trove-8.0.1/trove/tests/unittests/module/test_module_controller.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/module/test_module_models.py` & `trove-8.0.1/trove/tests/unittests/module/test_module_models.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import copy
-from unittest.mock import Mock, patch
+from mock import Mock, patch
 
 from trove.common import crypto_utils
 from trove.common import exception
 from trove.datastore import models as datastore_models
 from trove.module import models
 from trove.taskmanager import api as task_api
 from trove.tests.unittests import trove_testtools
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/module/test_module_views.py` & `trove-8.0.1/trove/tests/unittests/module/test_module_views.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest.mock import Mock, patch
+from mock import Mock, patch
 from trove.datastore import models
 from trove.module.views import DetailedModuleView
 from trove.tests.unittests import trove_testtools
 
 
 class ModuleViewsTest(trove_testtools.TestCase):
 
@@ -36,15 +36,15 @@
         self.module = Mock()
         self.module.name = 'test_module'
         self.module.type = 'test'
         self.module.md5 = 'md5-hash'
         self.module.created = 'Yesterday'
         self.module.updated = 'Now'
         self.module.datastore = 'mysql'
-        self.module.datastore_version = '5.7'
+        self.module.datastore_version = '5.6'
         self.module.auto_apply = False
         self.module.tenant_id = 'my_tenant'
         self.module.is_admin = False
         self.module.priority_apply = False
         self.module.apply_order = 5
 
     def tearDown(self):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/mysql/test_common.py` & `trove-8.0.1/trove/tests/unittests/mysql/test_common.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/mysql/test_user_controller.py` & `trove-8.0.1/trove/tests/unittests/mysql/test_user_controller.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/quota/test_quota.py` & `trove-8.0.1/trove/tests/unittests/quota/test_quota.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import Mock, MagicMock, patch
+from mock import Mock, MagicMock, patch
 from testtools import skipIf
 
 from trove.common import cfg
 from trove.common import exception
 from trove.db.models import DatabaseModelBase
 from trove.extensions.mgmt.quota.service import QuotaController
 from trove.quota.models import Quota
@@ -140,23 +140,14 @@
                                             FAKE_TENANT2)
             self.assertEqual(0, instance_quota.save.call_count)
             self.assertNotIn('instances', result._data['quotas'])
             self.assertEqual(1, volume_quota.save.call_count)
             self.assertEqual(200, result.status)
             self.assertEqual(10, result._data['quotas']['volumes'])
 
-    def test_update_resource_with_invalid_negative_number(self):
-        quota = MagicMock(spec=Quota)
-        with patch.object(DatabaseModelBase, 'find_by', return_value=quota):
-            body = {'quotas': {'instances': -2}}
-            self.assertRaises(exception.QuotaLimitTooSmall,
-                              self.controller.update,
-                              self.req, body, FAKE_TENANT1,
-                              FAKE_TENANT2)
-
 
 class DbQuotaDriverTest(trove_testtools.TestCase):
 
     def setUp(self):
 
         super(DbQuotaDriverTest, self).setUp()
         self.driver = DbQuotaDriver(resources)
@@ -391,42 +382,14 @@
         self.assertEqual(0, usages[Resource.INSTANCES].in_use)
         self.assertEqual(0, usages[Resource.INSTANCES].reserved)
         self.assertEqual(FAKE_TENANT1, usages[Resource.VOLUMES].tenant_id)
         self.assertEqual(Resource.VOLUMES, usages[Resource.VOLUMES].resource)
         self.assertEqual(0, usages[Resource.VOLUMES].in_use)
         self.assertEqual(0, usages[Resource.VOLUMES].reserved)
 
-    def test_check_quota_with_unlimited_quota(self):
-
-        FAKE_QUOTA_USAGE = [QuotaUsage(id=1,
-                                       tenant_id=FAKE_TENANT1,
-                                       resource=Resource.INSTANCES,
-                                       in_use=1,
-                                       reserved=2),
-                            QuotaUsage(id=2,
-                                       tenant_id=FAKE_TENANT1,
-                                       resource=Resource.VOLUMES,
-                                       in_use=1,
-                                       reserved=1)]
-        FAKE_QUOTAS = [Quota(tenant_id=FAKE_TENANT1,
-                             resource=Resource.INSTANCES,
-                             hard_limit=-1),
-                       Quota(tenant_id=FAKE_TENANT1,
-                             resource=Resource.VOLUMES,
-                             hard_limit=-1)]
-
-        self.mock_quota_result.all = Mock(return_value=FAKE_QUOTAS)
-        self.mock_usage_result.all = Mock(return_value=FAKE_QUOTA_USAGE)
-        QuotaUsage.save = Mock()
-        Reservation.create = Mock()
-
-        delta = {'instances': 2, 'volumes': 3}
-        self.assertIsNone(self.driver.check_quotas(FAKE_TENANT1, resources,
-                                                   delta))
-
     def test_reserve(self):
 
         FAKE_QUOTAS = [QuotaUsage(id=1,
                                   tenant_id=FAKE_TENANT1,
                                   resource=Resource.INSTANCES,
                                   in_use=1,
                                   reserved=2),
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/router/test_router.py` & `trove-8.0.1/trove/tests/unittests/router/test_router.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 
 from routes import Mapper
 
 from trove.common.wsgi import Router, Fault
 from trove.tests.unittests import trove_testtools
 
 
-class FakeRequest(object):
+class FakeRequst(object):
     """A fake webob request object designed to cause 404.
 
     The dispatcher actually checks if the given request is a dict and throws
     an error if it is. This object wrapper tricks the dispatcher into
     handling the request like a regular request.
     """
 
@@ -40,12 +40,12 @@
     def setUp(self):
         super(TestRouter, self).setUp()
         self.mapper = Mapper()
 
     def test_404_is_fault(self):
         """Test that the dispatcher wraps 404's in a `Fault`."""
 
-        fake_request = FakeRequest()
+        fake_request = FakeRequst()
 
         response = Router._dispatch(fake_request)
 
         assert isinstance(response, Fault)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_api.py` & `trove-8.0.1/trove/tests/unittests/taskmanager/test_api.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,17 +10,16 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest import mock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from trove.common import context
 from trove.common import exception
 from trove.common.rpc.version import RPC_API_VERSION
 from trove.common.strategies.cluster.experimental.mongodb.taskmanager import (
     MongoDbTaskManagerAPI)
 from trove.guestagent import models as agent_models
@@ -47,30 +46,30 @@
     def _mock_rpc_client(self):
         self.call_context = trove_testtools.TroveTestContext(self)
         self.api.client.prepare = Mock(return_value=self.call_context)
         self.call_context.cast = Mock()
 
     @patch.object(task_api.API, '_transform_obj', Mock(return_value='flv-id'))
     def test_create_instance(self):
+        flavor = Mock()
         self.api.create_instance(
-            'inst-id', 'inst-name', mock.ANY, 'img-id', {'name': 'db1'},
+            'inst-id', 'inst-name', flavor, 'img-id', {'name': 'db1'},
             {'name': 'usr1'}, 'mysql', None, 1, backup_id='bk-id',
             availability_zone='az', root_password='pwd', nics=['nic-id'],
             overrides={}, slave_of_id='slv-id', cluster_config={},
             volume_type='type', modules=['mod-id'], locality='affinity')
-
         self._verify_rpc_prepare_before_cast()
         self._verify_cast(
             'create_instance', availability_zone='az', backup_id='bk-id',
             cluster_config={}, databases={'name': 'db1'},
             datastore_manager='mysql', flavor='flv-id', image_id='img-id',
             instance_id='inst-id', locality='affinity', modules=['mod-id'],
             name='inst-name', nics=['nic-id'], overrides={}, packages=None,
             root_password='pwd', slave_of_id='slv-id', users={'name': 'usr1'},
-            volume_size=1, volume_type='type', access=None, ds_version=None)
+            volume_size=1, volume_type='type')
 
     def test_detach_replica(self):
         self.api.detach_replica('some-instance-id')
 
         self._verify_rpc_prepare_before_cast()
         self._verify_cast('detach_replica', instance_id='some-instance-id')
 
@@ -115,17 +114,17 @@
         mock_agent_heart_beat.return_value.find_by_instance_id.side_effect = (
             exception.ModelNotFoundError)
         self.api._delete_heartbeat('some-cluster-id')
         mock_agent_heart_beat.return_value.delete.assert_not_called()
 
     def test_transform_obj(self):
         flavor = Mock()
-        self.assertRaisesRegex(ValueError,
-                               ('Could not transform %s' % flavor),
-                               self.api._transform_obj, flavor)
+        self.assertRaisesRegexp(ValueError,
+                                ('Could not transform %s' % flavor),
+                                self.api._transform_obj, flavor)
 
     def test_upgrade(self):
         self.api.upgrade('some-instance-id', 'some-datastore-version')
 
         self._verify_rpc_prepare_before_cast()
         self._verify_cast('upgrade',
                           instance_id='some-instance-id',
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_clusters.py` & `trove-8.0.1/trove/tests/unittests/taskmanager/test_clusters.py`

 * *Files 15% similar despite different names*

```diff
@@ -11,87 +11,32 @@
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import datetime
 
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from trove.cluster.models import ClusterTasks as ClusterTaskStatus
 from trove.cluster.models import DBCluster
 from trove.common.strategies.cluster.experimental.mongodb.taskmanager import (
     MongoDbClusterTasks as ClusterTasks)
 from trove.common import utils
 from trove.datastore import models as datastore_models
 from trove.instance.models import BaseInstance
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import InstanceTasks
-from trove.instance.service_status import ServiceStatuses
+from trove.taskmanager.models import ServiceStatuses
 from trove.tests.unittests import trove_testtools
 
 
-class CassandraClusterTasksTest(trove_testtools.TestCase):
-    def setUp(self):
-        super(CassandraClusterTasksTest, self).setUp()
-        self.cluster_id = "1234"
-        self.cluster_name = "test1"
-        self.tenant_id = "2345"
-        self.db_cluster = DBCluster(ClusterTaskStatus.NONE,
-                                    id=self.cluster_id,
-                                    created=str(datetime.date),
-                                    updated=str(datetime.date),
-                                    name=self.cluster_name,
-                                    task_id=ClusterTaskStatus.NONE._code,
-                                    tenant_id=self.tenant_id,
-                                    datastore_version_id="1",
-                                    deleted=False)
-        self.dbinst1 = DBInstance(InstanceTasks.NONE, id="1", name="member1",
-                                  compute_instance_id="compute-1",
-                                  task_id=InstanceTasks.NONE._code,
-                                  task_description=InstanceTasks.NONE._db_text,
-                                  volume_id="volume-1",
-                                  datastore_version_id="1",
-                                  cluster_id=self.cluster_id,
-                                  shard_id="shard-1",
-                                  type="member")
-        self.dbinst2 = DBInstance(InstanceTasks.NONE, id="2", name="member2",
-                                  compute_instance_id="compute-2",
-                                  task_id=InstanceTasks.NONE._code,
-                                  task_description=InstanceTasks.NONE._db_text,
-                                  volume_id="volume-2",
-                                  datastore_version_id="1",
-                                  cluster_id=self.cluster_id,
-                                  shard_id="shard-1",
-                                  type="member")
-        mock_ds1 = Mock()
-        mock_ds1.name = 'cassandra'
-        mock_dv1 = Mock()
-        mock_dv1.name = '4.0.0'
-        self.clustertasks = ClusterTasks(Mock(),
-                                         self.db_cluster,
-                                         datastore=mock_ds1,
-                                         datastore_version=mock_dv1)
-
-    @patch.object(DBInstance, 'find_by')
-    @patch.object(InstanceServiceStatus, 'find_by')
-    def test_all_instances_healthy(self, mock_find, mock_db_find):
-        (mock_find.return_value.
-         get_status.return_value) = ServiceStatuses.HEALTHY
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
-        ret_val = self.clustertasks._all_instances_healthy(["1", "2"],
-                                                           self.cluster_id)
-        self.assertTrue(ret_val)
-
-
 class MongoDbClusterTasksTest(trove_testtools.TestCase):
     def setUp(self):
         super(MongoDbClusterTasksTest, self).setUp()
         self.cluster_id = "1232"
         self.cluster_name = "Cluster-1234"
         self.tenant_id = "6789"
         self.db_cluster = DBCluster(ClusterTaskStatus.NONE,
@@ -146,52 +91,29 @@
         mock_dv1.name = '2.0.4'
         self.clustertasks = ClusterTasks(Mock(),
                                          self.db_cluster,
                                          datastore=mock_ds1,
                                          datastore_version=mock_dv1)
 
     @patch.object(ClusterTasks, 'update_statuses_on_failure')
-    @patch.object(DBInstance, 'find_by')
-    @patch.object(InstanceServiceStatus, 'find_by')
-    @patch('trove.taskmanager.models.LOG')
-    def test_all_instances_ready_with_server_error(self,
-                                                   mock_logging, mock_find,
-                                                   mock_db_find, mock_update):
-        (mock_find.return_value.
-         get_status.return_value) = ServiceStatuses.NEW
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.BUILDING_ERROR_SERVER
-        ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
-                                                         self.cluster_id)
-        mock_update.assert_called_with(self.cluster_id, None)
-        self.assertFalse(ret_val)
-
-    @patch.object(ClusterTasks, 'update_statuses_on_failure')
-    @patch.object(DBInstance, 'find_by')
     @patch.object(InstanceServiceStatus, 'find_by')
     @patch('trove.taskmanager.models.LOG')
     def test_all_instances_ready_bad_status(self, mock_logging,
-                                            mock_find, mock_db_find,
-                                            mock_update):
+                                            mock_find, mock_update):
         (mock_find.return_value.
          get_status.return_value) = ServiceStatuses.FAILED
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
         ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
                                                          self.cluster_id)
         mock_update.assert_called_with(self.cluster_id, None)
         self.assertFalse(ret_val)
 
-    @patch.object(DBInstance, 'find_by')
     @patch.object(InstanceServiceStatus, 'find_by')
-    def test_all_instances_ready(self, mock_find, mock_db_find):
+    def test_all_instances_ready(self, mock_find):
         (mock_find.return_value.
          get_status.return_value) = ServiceStatuses.INSTANCE_READY
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
         ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
                                                          self.cluster_id)
         self.assertTrue(ret_val)
 
     @patch.object(ClusterTasks, 'update_statuses_on_failure')
     @patch.object(ClusterTasks, 'get_guest')
     @patch.object(ClusterTasks, 'get_ip')
@@ -343,86 +265,14 @@
     def test_delete_cluster(self, mock_find_all, mock_find_by, mock_save):
         mock_find_all.return_value.all.return_value = []
         mock_find_by.return_value = self.db_cluster
         self.clustertasks.delete_cluster(Mock(), self.cluster_id)
         self.assertEqual(ClusterTaskStatus.NONE, self.db_cluster.task_status)
         mock_save.assert_called_with()
 
-    def test_rolling_upgrade_cluster_without_order_specified(self):
-        self._assert_rolling_upgrade_cluster(None, None)
-
-    def test_rolling_upgrade_cluster_with_order_specified(self):
-        ordering = {
-            1: 1,
-            2: 2,
-            3: 3,
-            4: 4,
-            5: 5
-        }
-
-        def ordering_function(instance):
-            return ordering[instance.id]
-
-        self._assert_rolling_upgrade_cluster(ordering_function, ordering)
-
-    @patch('trove.taskmanager.models.DBaaSInstanceUpgrade')
-    @patch('trove.taskmanager.models.BuiltInstanceTasks')
-    @patch('trove.taskmanager.models.EndNotification')
-    @patch('trove.taskmanager.models.StartNotification')
-    @patch('trove.taskmanager.models.Timeout')
-    @patch.object(ClusterTasks, 'reset_task')
-    @patch.object(DBInstance, 'find_all')
-    def _assert_rolling_upgrade_cluster(self,
-                                        ordering_function,
-                                        ordering,
-                                        mock_find_all,
-                                        mock_reset_task,
-                                        mock_timeout,
-                                        mock_start,
-                                        mock_end,
-                                        mock_instance_task,
-                                        mock_upgrade):
-        class MockInstance(Mock):
-            upgrade_counter = 0
-
-            def upgrade(self, _):
-                MockInstance.upgrade_counter += 1
-                self.upgrade_number = MockInstance.upgrade_counter
-
-        db_instances = [Mock() for _ in range(5)]
-        for i in range(5):
-            db_instances[i].id = i + 1
-
-        mock_find_all.return_value.all.return_value = db_instances
-        instances = []
-
-        def load_side_effect(_, instance_id):
-            return_value = MockInstance()
-            return_value.id = instance_id
-            instances.append(return_value)
-            return return_value
-
-        mock_instance_task.load.side_effect = load_side_effect
-        if ordering is None:
-            ordering = {
-                1: 1,
-                2: 2,
-                3: 3,
-                4: 4,
-                5: 5
-            }
-        self.clustertasks.rolling_upgrade_cluster(MagicMock(),
-                                                  Mock(),
-                                                  Mock(),
-                                                  ordering_function)
-        order_result = {inst.id: inst.upgrade_number for inst in instances}
-
-        self.assertEqual(ClusterTaskStatus.NONE, self.db_cluster.task_status)
-        self.assertDictEqual(ordering, order_result)
-
     @patch.object(ClusterTasks, 'reset_task')
     @patch.object(ClusterTasks, '_create_shard')
     @patch.object(ClusterTasks, 'get_guest')
     @patch.object(utils, 'generate_random_password', return_value='pwd')
     @patch.object(ClusterTasks, 'get_ip')
     @patch.object(Instance, 'load')
     @patch.object(ClusterTasks, '_all_instances_ready')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_galera_clusters.py` & `trove-8.0.1/trove/tests/unittests/taskmanager/test_galera_clusters.py`

 * *Files 9% similar despite different names*

```diff
@@ -9,31 +9,31 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import datetime
 
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from trove.cluster.models import ClusterTasks as ClusterTaskStatus
 from trove.cluster.models import DBCluster
 from trove.common.exception import GuestError
 from trove.common.strategies.cluster.experimental.galera_common.taskmanager \
     import GaleraCommonClusterTasks
 from trove.common.strategies.cluster.experimental.galera_common.taskmanager \
     import GaleraCommonTaskManagerStrategy
 from trove.datastore import models as datastore_models
 from trove.instance.models import BaseInstance
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import InstanceTasks
-from trove.instance.service_status import ServiceStatuses
+from trove.taskmanager.models import ServiceStatuses
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 
 class GaleraClusterTasksTest(trove_testtools.TestCase):
     def setUp(self):
         super(GaleraClusterTasksTest, self).setUp()
@@ -87,52 +87,29 @@
                 'password': "password",
             },
             'cluster_name': self.cluster_name,
             'admin_password': "admin_password"
         }
 
     @patch.object(GaleraCommonClusterTasks, 'update_statuses_on_failure')
-    @patch.object(DBInstance, 'find_by')
-    @patch.object(InstanceServiceStatus, 'find_by')
-    @patch('trove.taskmanager.models.LOG')
-    def test_all_instances_ready_with_server_error(self,
-                                                   mock_logging, mock_find,
-                                                   mock_db_find, mock_update):
-        (mock_find.return_value.
-         get_status.return_value) = ServiceStatuses.NEW
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.BUILDING_ERROR_SERVER
-        ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
-                                                         self.cluster_id)
-        mock_update.assert_called_with(self.cluster_id, None)
-        self.assertFalse(ret_val)
-
-    @patch.object(GaleraCommonClusterTasks, 'update_statuses_on_failure')
-    @patch.object(DBInstance, 'find_by')
     @patch.object(InstanceServiceStatus, 'find_by')
     @patch('trove.taskmanager.models.LOG')
     def test_all_instances_ready_bad_status(self, mock_logging,
-                                            mock_find, mock_db_find,
-                                            mock_update):
+                                            mock_find, mock_update):
         (mock_find.return_value.
          get_status.return_value) = ServiceStatuses.FAILED
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
         ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
                                                          self.cluster_id)
         mock_update.assert_called_with(self.cluster_id, None)
         self.assertFalse(ret_val)
 
-    @patch.object(DBInstance, 'find_by')
     @patch.object(InstanceServiceStatus, 'find_by')
-    def test_all_instances_ready(self, mock_find, mock_db_find):
+    def test_all_instances_ready(self, mock_find):
         (mock_find.return_value.
          get_status.return_value) = ServiceStatuses.INSTANCE_READY
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
         ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
                                                          self.cluster_id)
         self.assertTrue(ret_val)
 
     @patch.object(GaleraCommonClusterTasks, 'update_statuses_on_failure')
     @patch.object(GaleraCommonClusterTasks, '_all_instances_ready',
                   return_value=False)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_manager.py` & `trove-8.0.1/trove/tests/unittests/taskmanager/test_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
-from unittest.mock import MagicMock, Mock, patch, PropertyMock
+from mock import Mock, patch, PropertyMock
 from proboscis.asserts import assert_equal
 
 from trove.backup.models import Backup
 from trove.common.exception import TroveError, ReplicationSlaveAttachError
 from trove.common import server_group as srv_grp
 from trove.instance.tasks import InstanceTasks
 from trove.taskmanager.manager import Manager
@@ -59,16 +59,16 @@
 
         def test_case(txn_list, selected_master):
             with patch.object(self.manager, '_get_replica_txns',
                               return_value=txn_list):
                 result = self.manager._most_current_replica(master, None)
                 assert_equal(result, selected_master)
 
-        with self.assertRaisesRegex(TroveError,
-                                    'not all replicating from same'):
+        with self.assertRaisesRegexp(TroveError,
+                                     'not all replicating from same'):
             test_case([['a', '2a99e-32bf', 2], ['b', '2a', 1]], None)
 
         test_case([['a', '2a99e-32bf', 2]], 'a')
         test_case([['a', '2a', 1], ['b', '2a', 2]], 'b')
         test_case([['a', '2a', 2], ['b', '2a', 1]], 'a')
         test_case([['a', '2a', 1], ['b', '2a', 1]], 'a')
         test_case([['a', None, 0]], 'a')
@@ -90,21 +90,20 @@
                                        self.mock_slave2]):
             self.manager.promote_to_replica_source(
                 self.context, 'some-inst-id')
 
         self.mock_slave1.detach_replica.assert_called_with(
             self.mock_old_master, for_failover=True)
         self.mock_old_master.attach_replica.assert_called_with(
-            self.mock_slave1, restart=False)
+            self.mock_slave1)
         self.mock_slave1.make_read_only.assert_called_with(False)
 
         self.mock_slave2.detach_replica.assert_called_with(
             self.mock_old_master, for_failover=True)
-        self.mock_slave2.attach_replica.assert_called_with(self.mock_slave1,
-                                                           restart=True)
+        self.mock_slave2.attach_replica.assert_called_with(self.mock_slave1)
 
         self.mock_old_master.demote_replication_master.assert_any_call()
 
         mock_set_task_status.assert_called_with(([self.mock_old_master] +
                                                  [self.mock_slave1,
                                                   self.mock_slave2]),
                                                 InstanceTasks.NONE)
@@ -155,17 +154,17 @@
     def test_error_promote_to_replica_source(self, *args):
         self.mock_slave2.detach_replica = Mock(
             side_effect=RuntimeError('Error'))
 
         with patch.object(models.BuiltInstanceTasks, 'load',
                           side_effect=[self.mock_slave1, self.mock_old_master,
                                        self.mock_slave2]):
-            self.assertRaisesRegex(RuntimeError, 'Error',
-                                   self.manager.promote_to_replica_source,
-                                   self.context, 'some-inst-id')
+            self.assertRaisesRegexp(RuntimeError, 'Error',
+                                    self.manager.promote_to_replica_source,
+                                    self.context, 'some-inst-id')
 
     @patch('trove.taskmanager.manager.LOG')
     def test_error_demote_replication_master_promote_to_replica_source(
             self, mock_logging):
         self.mock_old_master.demote_replication_master = Mock(
             side_effect=RuntimeError('Error'))
 
@@ -182,17 +181,17 @@
                                         mock_set_task_status):
         self.mock_slave2.detach_replica = Mock(
             side_effect=RuntimeError('Error'))
         mock_most_current_replica.return_value = self.mock_slave1
         with patch.object(models.BuiltInstanceTasks, 'load',
                           side_effect=[self.mock_master, self.mock_slave1,
                                        self.mock_slave2]):
-            self.assertRaisesRegex(RuntimeError, 'Error',
-                                   self.manager.eject_replica_source,
-                                   self.context, 'some-inst-id')
+            self.assertRaisesRegexp(RuntimeError, 'Error',
+                                    self.manager.eject_replica_source,
+                                    self.context, 'some-inst-id')
 
     @patch.object(Backup, 'delete')
     @patch.object(models.BuiltInstanceTasks, 'load')
     def test_create_replication_slave(self, mock_load, mock_backup_delete):
         mock_tasks = Mock()
         mock_snapshot = {'dataset': {'snapshot_id': 'test-id'}}
         mock_tasks.get_replication_master_snapshot = Mock(
@@ -204,16 +203,16 @@
                                          mock_flavor, Mock(), None, None,
                                          'mysql', 'mysql-server', 2,
                                          'temp-backup-id', None,
                                          'some_password', None, Mock(),
                                          'some-master-id', None, None,
                                          None, None)
         mock_tasks.get_replication_master_snapshot.assert_called_with(
-            self.context, 'some-master-id', mock_flavor,
-            parent_backup_id='temp-backup-id')
+            self.context, 'some-master-id', mock_flavor, 'temp-backup-id',
+            replica_number=1)
         mock_backup_delete.assert_called_with(self.context, 'test-id')
 
     @patch.object(models.FreshInstanceTasks, 'load')
     @patch.object(Backup, 'delete')
     @patch.object(models.BuiltInstanceTasks, 'load')
     @patch('trove.taskmanager.manager.LOG')
     def test_exception_create_replication_slave(self, mock_logging, mock_tasks,
@@ -222,15 +221,15 @@
         self.assertRaises(TroveError, self.manager.create_instance,
                           self.context, ['id1', 'id2'], Mock(), Mock(),
                           Mock(), None, None, 'mysql', 'mysql-server', 2,
                           'temp-backup-id', None, 'some_password', None,
                           Mock(), 'some-master-id', None, None, None, None)
 
     def test_AttributeError_create_instance(self):
-        self.assertRaisesRegex(
+        self.assertRaisesRegexp(
             AttributeError, 'Cannot create multiple non-replica instances.',
             self.manager.create_instance, self.context, ['id1', 'id2'],
             Mock(), Mock(), Mock(), None, None, 'mysql', 'mysql-server', 2,
             'temp-backup-id', None, 'some_password', None, Mock(), None, None,
             None, None, None)
 
     def test_create_instance(self):
@@ -244,27 +243,24 @@
                           return_value=mock_tasks):
             with patch.object(srv_grp.ServerGroup, 'create', mock_csg):
                 self.manager.create_instance(
                     self.context, 'id1', 'inst1', mock_flavor,
                     'mysql-image-id', None, None, 'mysql', 'mysql-server', 2,
                     'temp-backup-id', None, 'password', None, mock_override,
                     None, None, None, None, 'affinity')
-
-        mock_tasks.create_instance.assert_called_with(
-            mock_flavor,
-            'mysql-image-id', None,
-            None, 'mysql',
-            'mysql-server', 2,
-            'temp-backup-id', None,
-            'password', None,
-            mock_override,
-            None, None, None, None,
-            {'group': 'sg-id'},
-            access=None, ds_version=None)
-        mock_tasks.wait_for_instance.assert_called_with(3600, mock_flavor)
+        mock_tasks.create_instance.assert_called_with(mock_flavor,
+                                                      'mysql-image-id', None,
+                                                      None, 'mysql',
+                                                      'mysql-server', 2,
+                                                      'temp-backup-id', None,
+                                                      'password', None,
+                                                      mock_override,
+                                                      None, None, None, None,
+                                                      {'group': 'sg-id'})
+        mock_tasks.wait_for_instance.assert_called_with(36000, mock_flavor)
 
     def test_create_cluster(self):
         mock_tasks = Mock()
         with patch.object(models, 'load_cluster_tasks',
                           return_value=mock_tasks):
             self.manager.create_cluster(self.context, 'some-cluster-id')
         mock_tasks.create_cluster.assert_called_with(self.context,
@@ -274,48 +270,12 @@
         mock_tasks = Mock()
         with patch.object(models, 'load_cluster_tasks',
                           return_value=mock_tasks):
             self.manager.delete_cluster(self.context, 'some-cluster-id')
         mock_tasks.delete_cluster.assert_called_with(self.context,
                                                      'some-cluster-id')
 
-    def test_shrink_cluster_with_success(self):
-        self._assert_shrink_cluster(True)
-
-    def test_shrink_cluster_with_error(self):
-        self._assert_shrink_cluster(False)
-
-    @patch('trove.taskmanager.manager.EndNotification')
-    @patch('trove.taskmanager.manager.models.load_cluster_tasks')
-    def _assert_shrink_cluster(self, success, mock_load, mock_notification):
-        if success:
-            mock_load.side_effect = Mock()
-        else:
-            mock_load.side_effect = Exception
-
-        end_notification = MagicMock()
-        mock_notification.return_value = end_notification
-        context = Mock()
-        cluster_id = Mock()
-        instance_ids = Mock()
-
-        try:
-            self.manager.shrink_cluster(context, cluster_id, instance_ids)
-            self.assertTrue(success)
-        except Exception:
-            self.assertFalse(success)
-
-        mock_load.assert_called_once_with(context, cluster_id)
-        mock_notification.assert_called_once_with(context,
-                                                  cluster_id=cluster_id,
-                                                  instance_ids=instance_ids)
-        exit_error_type = end_notification.__exit__.call_args_list[0][0][0]
-        if success:
-            self.assertFalse(exit_error_type)
-        else:
-            self.assertTrue(exit_error_type)
-
 
 class TestTaskManagerService(trove_testtools.TestCase):
     def test_app_factory(self):
         test_service = service.app_factory(Mock())
         self.assertIsInstance(test_service, service.TaskService)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_models.py` & `trove-8.0.1/trove/tests/unittests/taskmanager/test_models.py`

 * *Files 10% similar despite different names*

```diff
@@ -7,118 +7,107 @@
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
-import json
 import os
-
 from tempfile import NamedTemporaryFile
-from unittest import mock
-from unittest.mock import call
-from unittest.mock import MagicMock
-from unittest.mock import Mock
-from unittest.mock import patch
-from unittest.mock import PropertyMock
-
-import cinderclient.v3.client as cinderclient
-import neutronclient.v2_0.client as neutronclient
-import novaclient.v2.flavors
-import novaclient.v2.servers
+import uuid
 
 from cinderclient import exceptions as cinder_exceptions
-from cinderclient.v3 import volumes as cinderclient_volumes
+import cinderclient.v2.client as cinderclient
+from cinderclient.v2 import volumes as cinderclient_volumes
+from mock import Mock, MagicMock, patch, PropertyMock, call
 from novaclient import exceptions as nova_exceptions
-from oslo_config import cfg
+import novaclient.v2.flavors
+import novaclient.v2.servers
 from swiftclient.client import ClientException
-from testtools.matchers import Equals
-from testtools.matchers import Is
+from testtools.matchers import Equals, Is
 
 import trove.backup.models
-import trove.common.context
-import trove.common.template as template
-import trove.db.models
-import trove.guestagent.api
-
 from trove.backup import models as backup_models
 from trove.backup import state
-from trove.common import exception
+import trove.common.context
 from trove.common.exception import GuestError
+from trove.common.exception import MalformedSecurityGroupRuleError
 from trove.common.exception import PollTimeOut
 from trove.common.exception import TroveError
+from trove.common.instance import ServiceStatuses
+from trove.common.notification import TroveInstanceModifyVolume
+from trove.common import remote
+import trove.common.template as template
 from trove.common import timeutils
 from trove.common import utils
 from trove.datastore import models as datastore_models
+import trove.db.models
 from trove.extensions.common import models as common_models
 from trove.extensions.mysql import models as mysql_models
+import trove.guestagent.api
 from trove.instance.models import BaseInstance
 from trove.instance.models import DBInstance
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import InstanceStatus
-from trove.instance.service_status import ServiceStatuses
 from trove.instance.tasks import InstanceTasks
 from trove import rpc
 from trove.taskmanager import models as taskmanager_models
 from trove.tests.unittests import trove_testtools
 from trove.tests.unittests.util import util
 
 INST_ID = 'dbinst-id-1'
 VOLUME_ID = 'volume-id-1'
 
 
-class FakeOptGroup(object):
+class _fake_neutron_client(object):
+    def list_floatingips(self):
+        return {'floatingips': [{'floating_ip_address': '192.168.10.1'}]}
+
 
+class FakeOptGroup(object):
     def __init__(self, tcp_ports=['3306', '3301-3307'],
                  udp_ports=[], icmp=False):
         self.tcp_ports = tcp_ports
         self.udp_ports = udp_ports
         self.icmp = icmp
 
 
 class fake_Server(object):
-
     def __init__(self):
         self.id = None
         self.name = None
         self.image_id = None
         self.flavor_id = None
         self.files = None
         self.userdata = None
-        self.meta = None
-        self.block_device_mapping_v2 = None
-        self.status = 'HEALTHY'
-        self.key_name = None
+        self.security_groups = None
+        self.block_device_mapping = None
+        self.status = 'ACTIVE'
 
 
 class fake_ServerManager(object):
-
     def create(self, name, image_id, flavor_id, files, userdata,
-               block_device_mapping_v2=None,
-               availability_zone=None,
+               security_groups, block_device_mapping, availability_zone=None,
                nics=None, config_drive=False,
-               scheduler_hints=None, key_name=None, meta=None):
+               scheduler_hints=None):
         server = fake_Server()
         server.id = "server_id"
         server.name = name
         server.image_id = image_id
         server.flavor_id = flavor_id
         server.files = files
         server.userdata = userdata
-        server.block_device_mapping_v2 = block_device_mapping_v2
+        server.security_groups = security_groups
+        server.block_device_mapping = block_device_mapping
         server.availability_zone = availability_zone
         server.nics = nics
-        server.key_name = key_name
-        server.meta = meta
         return server
 
 
 class fake_nova_client(object):
-
     def __init__(self):
         self.servers = fake_ServerManager()
 
 
 class fake_InstanceServiceStatus(object):
 
     _instance = None
@@ -180,18 +169,18 @@
         self.deleted = True
         pass
 
     def is_deleted(self):
         return self.deleted
 
 
-class BaseFreshInstanceTasksTest(trove_testtools.TestCase):
+class FreshInstanceTasksTest(trove_testtools.TestCase):
 
     def setUp(self):
-        super(BaseFreshInstanceTasksTest, self).setUp()
+        super(FreshInstanceTasksTest, self).setUp()
         mock_instance = patch('trove.instance.models.FreshInstance')
         mock_instance.start()
         self.addCleanup(mock_instance.stop)
         mock_instance.id = Mock(return_value='instance_id')
         mock_instance.tenant_id = Mock(return_value="tenant_id")
         mock_instance.hostname = Mock(return_value="hostname")
         mock_instance.name = Mock(return_value='name')
@@ -215,411 +204,260 @@
                                 delete=False) as f:
             self.cloudinit = f.name
             f.write(self.userdata)
         with NamedTemporaryFile(mode="w", delete=False) as f:
             self.guestconfig = f.name
             f.write(self.guestconfig_content)
         self.freshinstancetasks = taskmanager_models.FreshInstanceTasks(
-            None, MagicMock(), None, None)
-        self.freshinstancetasks.context = trove.common.context.TroveContext(
-            user_id='test_user')
+            None, Mock(), None, None)
+        self.tm_sg_create_inst_patch = patch.object(
+            trove.taskmanager.models.SecurityGroup, 'create_for_instance',
+            Mock(return_value={'id': uuid.uuid4(), 'name': uuid.uuid4()}))
+        self.tm_sg_create_inst_mock = self.tm_sg_create_inst_patch.start()
+        self.addCleanup(self.tm_sg_create_inst_patch.stop)
+        self.tm_sgr_create_sgr_patch = patch.object(
+            trove.taskmanager.models.SecurityGroupRule,
+            'create_sec_group_rule')
+        self.tm_sgr_create_sgr_mock = self.tm_sgr_create_sgr_patch.start()
+        self.addCleanup(self.tm_sgr_create_sgr_patch.stop)
+        self.task_models_conf_patch = patch('trove.taskmanager.models.CONF')
+        self.task_models_conf_mock = self.task_models_conf_patch.start()
+        self.addCleanup(self.task_models_conf_patch.stop)
+        self.inst_models_conf_patch = patch('trove.instance.models.CONF')
+        self.inst_models_conf_mock = self.inst_models_conf_patch.start()
+        self.addCleanup(self.inst_models_conf_patch.stop)
 
     def tearDown(self):
-        super(BaseFreshInstanceTasksTest, self).tearDown()
+        super(FreshInstanceTasksTest, self).tearDown()
         os.remove(self.cloudinit)
         os.remove(self.guestconfig)
         InstanceServiceStatus.find_by = self.orig_ISS_find_by
         DBInstance.find_by = self.orig_DBI_find_by
 
-
-class FreshInstanceTasksTest(BaseFreshInstanceTasksTest):
-
     def test_create_instance_userdata(self):
         cloudinit_location = os.path.dirname(self.cloudinit)
         datastore_manager = os.path.splitext(os.path.basename(self.
                                                               cloudinit))[0]
 
-        cfg.CONF.set_override('cloudinit_location', cloudinit_location)
+        def fake_conf_getter(*args, **kwargs):
+            if args[0] == 'cloudinit_location':
+                return cloudinit_location
+            else:
+                return ''
+        self.task_models_conf_mock.get.side_effect = fake_conf_getter
 
         server = self.freshinstancetasks._create_server(
-            None, None, datastore_manager, None, None, None)
-
+            None, None, None, datastore_manager, None, None, None)
         self.assertEqual(server.userdata, self.userdata)
 
-    def test_create_instance_with_keypair(self):
-        cfg.CONF.set_override('nova_keypair', 'fake_keypair')
-
-        server = self.freshinstancetasks._create_server(
-            None, None, None, None, None, None)
-
-        self.assertEqual('fake_keypair', server.key_name)
-
-    def test_create_instance_prepare_cloud_config(self):
-        files = {"/tmp/fake_file": "fake text"}
-        user_data = self.freshinstancetasks.prepare_cloud_config(files)
-        self.assertTrue(user_data.startswith('#cloud-config'))
-
     @patch.object(DBInstance, 'get_by')
     def test_create_instance_guestconfig(self, patch_get_by):
-        cfg.CONF.set_override('guest_config', self.guestconfig)
-        cfg.CONF.set_override('guest_info', 'guest_info.conf')
-        cfg.CONF.set_override('injected_config_location', '/etc/trove/conf.d')
-        cfg.CONF.set_override('docker_insecure_registries',
-                              '127.0.0.1:4000,127.0.0.1:5000')
+        def fake_conf_getter(*args, **kwargs):
+            if args[0] == 'guest_config':
+                return self.guestconfig
+            if args[0] == 'guest_info':
+                return 'guest_info.conf'
+            if args[0] == 'injected_config_location':
+                return '/etc/trove/conf.d'
+            else:
+                return ''
 
+        self.inst_models_conf_mock.get.side_effect = fake_conf_getter
         # execute
-        files = self.freshinstancetasks.get_injected_files("test", 'test')
+        files = self.freshinstancetasks.get_injected_files("test")
         # verify
         self.assertTrue(
             '/etc/trove/conf.d/guest_info.conf' in files)
         self.assertTrue(
             '/etc/trove/conf.d/trove-guestagent.conf' in files)
         self.assertEqual(
             self.guestconfig_content,
             files['/etc/trove/conf.d/trove-guestagent.conf'])
-        deamon_json = json.loads(files.get('/etc/docker/daemon.json'))
-        self.assertIn('127.0.0.1:4000', deamon_json.get('insecure-registries'))
-        self.assertIn('127.0.0.1:5000', deamon_json.get('insecure-registries'))
 
     @patch.object(DBInstance, 'get_by')
     def test_create_instance_guestconfig_compat(self, patch_get_by):
-        cfg.CONF.set_override('guest_config', self.guestconfig)
-        cfg.CONF.set_override('guest_info', '/etc/guest_info')
-        cfg.CONF.set_override('injected_config_location', '/etc')
+        def fake_conf_getter(*args, **kwargs):
+            if args[0] == 'guest_config':
+                return self.guestconfig
+            if args[0] == 'guest_info':
+                return '/etc/guest_info'
+            if args[0] == 'injected_config_location':
+                return '/etc'
+            else:
+                return ''
 
+        self.inst_models_conf_mock.get.side_effect = fake_conf_getter
         # execute
-        files = self.freshinstancetasks.get_injected_files("test", 'test')
+        files = self.freshinstancetasks.get_injected_files("test")
         # verify
         self.assertTrue(
             '/etc/guest_info' in files)
         self.assertTrue(
             '/etc/trove-guestagent.conf' in files)
         self.assertEqual(
             self.guestconfig_content,
             files['/etc/trove-guestagent.conf'])
-        self.assertFalse(files.get('/etc/docker/daemon.json'))
 
     def test_create_instance_with_az_kwarg(self):
+        self.task_models_conf_mock.get.return_value = ''
         # execute
         server = self.freshinstancetasks._create_server(
-            None, None, None, None, availability_zone='nova', nics=None)
+            None, None, None, None, None, availability_zone='nova', nics=None)
         # verify
         self.assertIsNotNone(server)
 
     def test_create_instance_with_az(self):
+        self.task_models_conf_mock.get.return_value = ''
         # execute
         server = self.freshinstancetasks._create_server(
-            None, None, None, None, 'nova', None)
+            None, None, None, None, None, 'nova', None)
         # verify
         self.assertIsNotNone(server)
 
     def test_create_instance_with_az_none(self):
+        self.task_models_conf_mock.get.return_value = ''
         # execute
         server = self.freshinstancetasks._create_server(
-            None, None, None, None, None, None)
+            None, None, None, None, None, None, None)
         # verify
         self.assertIsNotNone(server)
 
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'hostname',
-                  new_callable=PropertyMock,
-                  return_value='fake-hostname')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'name',
-                  new_callable=PropertyMock,
-                  return_value='fake-name')
-    def test_servers_create_block_device_mapping_v2(self,
-                                                    mock_hostname,
-                                                    mock_name):
-        # This testcase is to test create_server with config_drive=True.
-        # We need set use_nova_server_config_drive True explicitly
-        # because use_nova_server_config_drive becomes False since Yoga
-        cfg.CONF.set_override('use_nova_server_config_drive', True)
-        self.freshinstancetasks.prepare_userdata = Mock(return_value=None)
-        mock_nova_client = self.freshinstancetasks.nova_client = Mock()
-        mock_servers_create = mock_nova_client.servers.create
-        self.freshinstancetasks._create_server('fake-flavor', 'fake-image',
-                                               'mysql', None, None, None)
-        meta = {'trove_project_id': self.freshinstancetasks.tenant_id,
-                'trove_user_id': 'test_user',
-                'trove_instance_id': self.freshinstancetasks.id}
-        mock_servers_create.assert_called_with(
-            'fake-name', 'fake-image',
-            'fake-flavor', files={},
-            userdata=None,
-            block_device_mapping_v2=None,
-            availability_zone=None,
-            nics=None,
-            config_drive=True,
-            scheduler_hints=None,
-            key_name=None,
-            meta=meta,
-        )
-
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'hostname',
-                  new_callable=PropertyMock,
-                  return_value='fake-hostname')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'name',
-                  new_callable=PropertyMock,
-                  return_value='fake-name')
-    def test_servers_create_block_device_mapping_v2_since_yoga(
-            self, mock_hostname, mock_name):
-
-        # This testcase is for testing with config_drive=False.
-        self.freshinstancetasks.prepare_userdata = Mock(
-            return_value="#cloud-config\nwrite_files:\n")
-        mock_nova_client = self.freshinstancetasks.nova_client = Mock()
-        mock_servers_create = mock_nova_client.servers.create
-        self.freshinstancetasks._create_server('fake-flavor', 'fake-image',
-                                               'mysql', None, None, None)
-        meta = {'trove_project_id': self.freshinstancetasks.tenant_id,
-                'trove_user_id': 'test_user',
-                'trove_instance_id': self.freshinstancetasks.id}
-
-        userdata = self.freshinstancetasks.prepare_userdata('mysql')
-        userdata = userdata + \
-            self.freshinstancetasks.prepare_cloud_config({})
-        mock_servers_create.assert_called_with(
-            'fake-name', 'fake-image',
-            'fake-flavor', files={},
-            userdata=userdata,
-            block_device_mapping_v2=None,
-            availability_zone=None,
-            nics=None,
-            config_drive=False,
-            scheduler_hints=None,
-            key_name=None,
-            meta=meta,
-        )
-
     @patch.object(InstanceServiceStatus, 'find_by',
                   return_value=fake_InstanceServiceStatus.find_by())
     @patch.object(DBInstance, 'find_by',
                   return_value=fake_DBInstance.find_by())
     @patch('trove.taskmanager.models.LOG')
     def test_update_status_of_instance_failure(
             self, mock_logging, dbi_find_by_mock, iss_find_by_mock):
+        self.task_models_conf_mock.get.return_value = ''
         self.freshinstancetasks.update_statuses_on_time_out()
         self.assertEqual(ServiceStatuses.FAILED_TIMEOUT_GUESTAGENT,
                          fake_InstanceServiceStatus.find_by().get_status())
         self.assertEqual(InstanceTasks.BUILDING_ERROR_TIMEOUT_GA,
                          fake_DBInstance.find_by().get_task_status())
 
+    def test_create_sg_rules_success(self):
+        datastore_manager = 'mysql'
+        self.task_models_conf_mock.get = Mock(return_value=FakeOptGroup())
+        self.freshinstancetasks._create_secgroup(datastore_manager)
+        self.assertEqual(2, taskmanager_models.SecurityGroupRule.
+                         create_sec_group_rule.call_count)
+
+    def test_create_sg_rules_format_exception_raised(self):
+        datastore_manager = 'mysql'
+        self.task_models_conf_mock.get = Mock(
+            return_value=FakeOptGroup(tcp_ports=['3306', '-3306']))
+        self.freshinstancetasks.update_db = Mock()
+        self.assertRaises(MalformedSecurityGroupRuleError,
+                          self.freshinstancetasks._create_secgroup,
+                          datastore_manager)
+
+    def test_create_sg_rules_success_with_duplicated_port_or_range(self):
+        datastore_manager = 'mysql'
+        self.task_models_conf_mock.get = Mock(
+            return_value=FakeOptGroup(
+                tcp_ports=['3306', '3306', '3306-3307', '3306-3307']))
+        self.freshinstancetasks.update_db = Mock()
+        self.freshinstancetasks._create_secgroup(datastore_manager)
+        self.assertEqual(2, taskmanager_models.SecurityGroupRule.
+                         create_sec_group_rule.call_count)
+
+    def test_create_sg_rules_exception_with_malformed_ports_or_range(self):
+        datastore_manager = 'mysql'
+        self.task_models_conf_mock.get = Mock(
+            return_value=FakeOptGroup(tcp_ports=['A', 'B-C']))
+        self.freshinstancetasks.update_db = Mock()
+        self.assertRaises(MalformedSecurityGroupRuleError,
+                          self.freshinstancetasks._create_secgroup,
+                          datastore_manager)
+
+    def test_create_sg_rules_icmp(self):
+        datastore_manager = 'mysql'
+        self.task_models_conf_mock.get = Mock(
+            return_value=FakeOptGroup(icmp=True))
+        self.freshinstancetasks.update_db = Mock()
+        self.freshinstancetasks._create_secgroup(datastore_manager)
+        self.assertEqual(3, taskmanager_models.SecurityGroupRule.
+                         create_sec_group_rule.call_count)
+
+    @patch.object(BaseInstance, 'update_db')
+    @patch('trove.taskmanager.models.CONF')
+    @patch('trove.taskmanager.models.LOG')
+    def test_error_sec_group_create_instance(self, mock_logging,
+                                             mock_conf, mock_update_db):
+        mock_conf.get = Mock(
+            return_value=FakeOptGroup(tcp_ports=['3306', '-3306']))
+        mock_flavor = {'id': 7, 'ram': 256, 'name': 'smaller_flavor'}
+        self.assertRaisesRegexp(
+            TroveError,
+            'Error creating security group for instance',
+            self.freshinstancetasks.create_instance, mock_flavor,
+            'mysql-image-id', None, None, 'mysql', 'mysql-server', 2,
+            None, None, None, None, Mock(), None, None, None, None, None)
+
+    @patch.object(BaseInstance, 'update_db')
+    @patch.object(backup_models.Backup, 'get_by_id')
+    @patch.object(taskmanager_models.FreshInstanceTasks, 'report_root_enabled')
+    @patch.object(taskmanager_models.FreshInstanceTasks, 'get_injected_files')
+    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_secgroup')
+    @patch.object(taskmanager_models.FreshInstanceTasks, '_build_volume_info')
+    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_server')
+    @patch.object(taskmanager_models.FreshInstanceTasks, '_guest_prepare')
+    @patch.object(template, 'SingleInstanceConfigTemplate')
+    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_dns_entry',
+                  side_effect=TroveError)
+    @patch('trove.taskmanager.models.LOG')
+    def test_error_create_dns_entry_create_instance(self, *args):
+        mock_flavor = {'id': 6, 'ram': 512, 'name': 'big_flavor'}
+        self.assertRaisesRegexp(
+            TroveError,
+            'Error creating DNS entry for instance',
+            self.freshinstancetasks.create_instance, mock_flavor,
+            'mysql-image-id', None, None, 'mysql', 'mysql-server',
+            2, Mock(), None, 'root_password', None, Mock(), None, None, None,
+            None, None)
+
     @patch.object(BaseInstance, 'update_db')
     @patch.object(taskmanager_models.FreshInstanceTasks, '_create_dns_entry')
     @patch.object(taskmanager_models.FreshInstanceTasks, 'get_injected_files')
     @patch.object(taskmanager_models.FreshInstanceTasks, '_create_server')
     @patch.object(taskmanager_models.FreshInstanceTasks, '_create_secgroup')
     @patch.object(taskmanager_models.FreshInstanceTasks, '_build_volume_info')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_root_volume')
     @patch.object(taskmanager_models.FreshInstanceTasks, '_guest_prepare')
     @patch.object(template, 'SingleInstanceConfigTemplate')
-    @patch('trove.taskmanager.models.FreshInstanceTasks._create_port')
     def test_create_instance(self,
-                             mock_create_port,
                              mock_single_instance_template,
                              mock_guest_prepare,
-                             mock_create_root_volume,
                              mock_build_volume_info,
                              mock_create_secgroup,
                              mock_create_server,
                              mock_get_injected_files,
                              *args):
-
-        cfg.CONF.set_override('tcp_ports', ['3306', '3301-3307'],
-                              group='mysql')
-
         mock_flavor = {'id': 8, 'ram': 768, 'name': 'bigger_flavor'}
         config_content = {'config_contents': 'some junk'}
         mock_single_instance_template.return_value.config_contents = (
             config_content)
         overrides = Mock()
-        mock_create_secgroup.return_value = 'fake_security_group_id'
-        mock_create_port.return_value = 'fake-port-id'
-
-        self.freshinstancetasks.create_instance(
-            mock_flavor, 'mysql-image-id', None,
-            None, 'mysql', 'mysql-server',
-            2, None, None,
-            None, [{'net-id': 'fake-net-id'}], overrides,
-            None, None, 'volume_type',
-            None, {'group': 'sg-id'}
-        )
-
-        mock_create_secgroup.assert_called_with('mysql', [])
-        mock_create_port.assert_called_once_with(
-            {'net-id': 'fake-net-id'},
-            ['fake_security_group_id'],
-            is_mgmt=False,
-            is_public=False
-        )
-        image_id = 'mysql-image-id'
-        if cfg.CONF.volume_rootdisk_support:
-            mock_create_root_volume.assert_called_with(
-                'mysql-image-id', 10, 'volume_type', None)
-            image_id = None
-
-        mock_build_volume_info.assert_called_with(
-            'mysql', availability_zone=None, volume_size=2,
-            volume_type='volume_type'
-        )
+        self.freshinstancetasks.create_instance(mock_flavor, 'mysql-image-id',
+                                                None, None, 'mysql',
+                                                'mysql-server', 2,
+                                                None, None, None, None,
+                                                overrides, None, None,
+                                                'volume_type', None,
+                                                {'group': 'sg-id'})
+        mock_create_secgroup.assert_called_with('mysql')
+        mock_build_volume_info.assert_called_with('mysql', volume_size=2,
+                                                  volume_type='volume_type')
         mock_guest_prepare.assert_called_with(
             768, mock_build_volume_info(), 'mysql-server', None, None, None,
-            config_content, None, overrides, None, None, None, ds_version=None
-        )
+            config_content, None, overrides, None, None, None)
         mock_create_server.assert_called_with(
-            8, image_id, 'mysql',
-            mock_build_volume_info()['block_device'], None,
-            [{'port-id': 'fake-port-id'}],
-            mock_get_injected_files(),
-            {'group': 'sg-id'}
-        )
-
-    @patch.object(BaseInstance, 'update_db')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_dns_entry')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'get_injected_files')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_server')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_build_volume_info')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_root_volume')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_guest_prepare')
-    @patch.object(template, 'SingleInstanceConfigTemplate')
-    @patch('trove.common.clients_admin.neutron_client_trove_admin')
-    def test_create_instance_with_mgmt_port(self,
-                                            mock_neutron_client,
-                                            mock_single_instance_template,
-                                            mock_guest_prepare,
-                                            mock_create_root_volume,
-                                            mock_build_volume_info,
-                                            mock_create_server,
-                                            mock_get_injected_files,
-                                            *args):
-        self.patch_conf_property('management_networks', ['fake-mgmt-uuid'])
-
-        mock_client = MagicMock()
-        mock_client.create_security_group.return_value = {
-            'security_group': {'id': 'fake-sg-id'}
-        }
-        mock_client.create_port.side_effect = [
-            {'port': {'id': 'fake-mgmt-port-id'}},
-            {
-                'port': {
-                    'id': 'fake-user-port-id',
-                    'fixed_ips': [{'subnet_id': 'fake-subnet-id'}]
-                }
-            }
-        ]
-        mock_client.list_networks.return_value = {
-            'networks': [{'id': 'fake-public-net-id'}]
-        }
-        mock_client.list_ports.return_value = {
-            'ports': [{'id': 'fake-port-id'}]
-        }
-        mock_neutron_client.return_value = mock_client
-
-        mock_flavor = {'id': 8, 'ram': 768, 'name': 'bigger_flavor'}
-        config_content = {'config_contents': 'some junk'}
-        mock_single_instance_template.return_value.config_contents = (
-            config_content)
-
-        self.freshinstancetasks.create_instance(
-            mock_flavor, 'mysql-image-id', None,
-            None, 'mysql', 'mysql-server',
-            2, None, None,
-            None, [{'net-id': 'fake-net-uuid'}, {'net-id': 'fake-mgmt-uuid'}],
-            mock.ANY,
-            None, None, 'volume_type',
-            None, {'group': 'sg-id'},
-            access={'is_public': True, 'allowed_cidrs': ['192.168.0.1/24']}
-        )
-
-        image_id = 'mysql-image-id'
-        if cfg.CONF.volume_rootdisk_support:
-            mock_create_root_volume.assert_called_with(
-                'mysql-image-id', 10, 'volume_type', None)
-            image_id = None
-        mock_build_volume_info.assert_called_with(
-            'mysql', availability_zone=None, volume_size=2,
-            volume_type='volume_type'
-        )
-        mock_guest_prepare.assert_called_with(
-            768, mock_build_volume_info(), 'mysql-server', None, None, None,
-            config_content, None, mock.ANY, None, None, None, ds_version=None)
-        mock_create_server.assert_called_with(
-            8, image_id, 'mysql',
-            mock_build_volume_info()['block_device'], None,
-            [
-                {'port-id': 'fake-user-port-id'},
-                {'port-id': 'fake-mgmt-port-id'}
-            ],
-            mock_get_injected_files(), {'group': 'sg-id'}
-        )
-        create_floatingip_param = {
-            "floatingip": {
-                'floating_network_id': 'fake-public-net-id',
-                'port_id': 'fake-user-port-id',
-                'project_id': mock.ANY
-            }
-        }
-        mock_client.create_floatingip.assert_called_once_with(
-            create_floatingip_param
-        )
-
-    @patch.object(BaseInstance, 'update_db')
-    @patch.object(taskmanager_models, 'create_cinder_client')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'device_path',
-                  new_callable=PropertyMock,
-                  return_value='fake-device-path')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'volume_support',
-                  new_callable=PropertyMock,
-                  return_value=True)
-    def test_build_volume_info(self, mock_volume_support, mock_device_path,
-                               mock_create_cinderclient, mock_update_db):
-        cfg.CONF.set_override('block_device_mapping', 'fake-bdm')
-        cfg.CONF.set_override('mount_point', 'fake-mount-point',
-                              group='mysql')
-        mock_cinderclient = mock_create_cinderclient.return_value
-        mock_volume = Mock(name='fake-vol', id='fake-vol-id',
-                           size=2, status='available')
-        mock_cinderclient.volumes.create.return_value = mock_volume
-        mock_cinderclient.volumes.get.return_value = mock_volume
-        volume_info = self.freshinstancetasks._build_volume_info(
-            'mysql', volume_size=2, volume_type='volume_type')
-        expected = {
-            'block_device': [{
-                'uuid': 'fake-vol-id',
-                'source_type': 'volume',
-                'destination_type': 'volume',
-                'device_name': 'fake-bdm',
-                'volume_size': 2,
-                'delete_on_termination': True}],
-            'device_path': 'fake-device-path',
-            'mount_point': 'fake-mount-point'
-        }
-        self.assertEqual(expected, volume_info)
-
-    @patch.object(BaseInstance, 'update_db')
-    @patch.object(taskmanager_models.FreshInstanceTasks, '_create_volume')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'device_path',
-                  new_callable=PropertyMock,
-                  return_value='fake-device-path')
-    @patch.object(taskmanager_models.FreshInstanceTasks, 'volume_support',
-                  new_callable=PropertyMock,
-                  return_value=False)
-    def test_build_volume_info_without_volume(self, mock_volume_support,
-                                              mock_device_path,
-                                              mock_create_volume,
-                                              mock_update_db):
-        cfg.CONF.set_override('mount_point', 'fake-mount-point',
-                              group='mysql')
-        volume_info = self.freshinstancetasks._build_volume_info('mysql')
-        self.assertFalse(mock_create_volume.called)
-        expected = {
-            'block_device': None,
-            'device_path': 'fake-device-path',
-            'mount_point': 'fake-mount-point'
-        }
-        self.assertEqual(expected, volume_info)
+            8, 'mysql-image-id', mock_create_secgroup(),
+            'mysql', mock_build_volume_info()['block_device'], None,
+            None, mock_get_injected_files(), {'group': 'sg-id'})
 
     @patch.object(trove.guestagent.api.API, 'attach_replication_slave')
     @patch.object(rpc, 'get_client')
     @patch.object(DBInstance, 'get_by')
     def test_attach_replication_slave(self, mock_get_by, mock_get_client,
                                       mock_attach_replication_slave):
         mock_flavor = {'id': 8, 'ram': 768, 'name': 'bigger_flavor'}
@@ -644,15 +482,15 @@
                   side_effect=GuestError)
     @patch('trove.taskmanager.models.LOG')
     @patch.object(DBInstance, 'get_by')
     def test_error_attach_replication_slave(self, *args):
         mock_flavor = {'id': 8, 'ram': 768, 'name': 'bigger_flavor'}
         snapshot = {'replication_strategy': 'MysqlGTIDReplication',
                     'master': {'id': 'master-id'}}
-        self.assertRaisesRegex(
+        self.assertRaisesRegexp(
             TroveError, 'Error attaching instance',
             self.freshinstancetasks.attach_replication_slave,
             snapshot, mock_flavor)
 
 
 class ResizeVolumeTest(trove_testtools.TestCase):
 
@@ -668,15 +506,14 @@
         self.old_vol_size = 1
         self.new_vol_size = 2
         self.action = taskmanager_models.ResizeVolumeAction(self.instance,
                                                             self.old_vol_size,
                                                             self.new_vol_size)
 
         class FakeGroup(object):
-
             def __init__(self):
                 self.mount_point = 'var/lib/mysql'
                 self.device_path = '/dev/vdb'
 
         self.taskmanager_models_CONF = patch.object(taskmanager_models, 'CONF')
         self.mock_conf = self.taskmanager_models_CONF.start()
         self.mock_conf.get = Mock(return_value=FakeGroup())
@@ -720,43 +557,53 @@
             self.instance.nova_client.volumes.create_server_volume.call_count)
         self.assertEqual(1, attach_count)
         self.assertEqual(1, self.instance.guest.mount_volume.call_count)
         self.assertEqual(1, self.instance.restart.call_count)
         self.instance.volume_client.volumes.extend.side_effect = None
         self.instance.reset_mock()
 
-    def test_resize_volume_verify_extend_no_volume(self):
+    @patch('trove.taskmanager.models.LOG')
+    def test_resize_volume_verify_extend_no_volume(self, mock_logging):
         self.instance.volume_client.volumes.get = Mock(
             return_value=None)
-        self.assertRaises(exception.TroveError,
+        self.assertRaises(cinder_exceptions.ClientException,
                           self.action._verify_extend)
         self.instance.reset_mock()
 
     @patch('trove.taskmanager.models.LOG')
     def test_resize_volume_poll_timeout(self, mock_logging):
         utils.poll_until = Mock(side_effect=PollTimeOut)
         self.assertRaises(PollTimeOut, self.action._verify_extend)
         self.assertEqual(2, self.instance.volume_client.volumes.get.call_count)
         utils.poll_until.side_effect = None
         self.instance.reset_mock()
 
+    @patch.object(TroveInstanceModifyVolume, 'notify')
     def test_resize_volume_active_server_succeeds(self, *args):
         server = Mock(status=InstanceStatus.ACTIVE)
         self.instance.attach_mock(server, 'server')
-
         self.action.execute()
-
+        self.assertEqual(1, self.instance.guest.stop_db.call_count)
+        self.assertEqual(1, self.instance.guest.unmount_volume.call_count)
+        detach_count = (
+            self.instance.nova_client.volumes.delete_server_volume.call_count)
+        self.assertEqual(1, detach_count)
         extend_count = self.instance.volume_client.volumes.extend.call_count
         self.assertEqual(1, extend_count)
+        attach_count = (
+            self.instance.nova_client.volumes.create_server_volume.call_count)
+        self.assertEqual(1, attach_count)
+        self.assertEqual(1, self.instance.guest.resize_fs.call_count)
+        self.assertEqual(1, self.instance.guest.mount_volume.call_count)
+        self.assertEqual(1, self.instance.restart.call_count)
         self.instance.reset_mock()
 
     def test_resize_volume_server_error_fails(self):
         server = Mock(status=InstanceStatus.ERROR)
         self.instance.attach_mock(server, 'server')
-
         self.assertRaises(TroveError, self.action.execute)
         self.instance.reset_mock()
 
 
 class BuiltInstanceTasksTest(trove_testtools.TestCase):
 
     def get_inst_service_status(self, status_id, statuses):
@@ -775,31 +622,18 @@
         self.instance_task.volume_client.volumes = stub_volume_mgr
         stub_volume_mgr.extend = MagicMock(return_value=None)
         stub_new_volume = cinderclient.volumes.Volume(
             stub_volume_mgr, {'status': 'available', 'size': 2}, True)
         stub_volume_mgr.get = MagicMock(return_value=stub_new_volume)
         stub_volume_mgr.attach = MagicMock(return_value=None)
 
-    def _stub_neutron_client(self):
-        stub_neutron_client = self.instance_task._neutron_client = MagicMock(
-            spec=neutronclient.Client)
-        stub_neutron_client.list_floatingips = MagicMock(
-            return_value={'floatingips': [{
-                'floating_ip_address': '192.168.10.1',
-                'id': 'fake-floatingip-id'}]})
-        stub_neutron_client.list_ports = MagicMock(
-            return_value={'ports': [{
-                'fixed_ips': [{'ip_address': '10.0.0.1'},
-                              {'ip_address': 'fd4a:7bef:d1ed:1::1'}],
-                'id': 'fake-port-id'}]})
-
     def setUp(self):
         super(BuiltInstanceTasksTest, self).setUp()
         self.new_flavor = {'id': 8, 'ram': 768, 'name': 'bigger_flavor'}
-        stub_nova_server = MagicMock(id='fake_id')
+        stub_nova_server = MagicMock()
         self.rpc_patches = patch.multiple(
             rpc, get_notifier=MagicMock(), get_client=MagicMock())
         self.rpc_mocks = self.rpc_patches.start()
         self.addCleanup(self.rpc_patches.stop)
         db_instance = DBInstance(InstanceTasks.NONE,
                                  id=INST_ID,
                                  name='resize-inst-name',
@@ -811,15 +645,15 @@
                                  updated=timeutils.utcnow(),
                                  compute_instance_id='computeinst-id-1',
                                  tenant_id='testresize-tenant-id',
                                  volume_size='1',
                                  volume_id=VOLUME_ID)
 
         # this is used during the final check of whether the resize successful
-        db_instance.server_status = 'HEALTHY'
+        db_instance.server_status = 'ACTIVE'
         self.db_instance = db_instance
         self.dm_dv_load_by_uuid_patch = patch.object(
             datastore_models.DatastoreVersion, 'load_by_uuid', MagicMock(
                 return_value=datastore_models.DatastoreVersion(db_instance)))
         self.dm_dv_load_by_uuid_mock = self.dm_dv_load_by_uuid_patch.start()
         self.addCleanup(self.dm_dv_load_by_uuid_patch.stop)
         self.dm_ds_load_patch = patch.object(
@@ -838,19 +672,18 @@
         self.instance_task._guest = MagicMock(spec=trove.guestagent.api.API)
         self.instance_task._nova_client = MagicMock(
             spec=novaclient.client)
         self.stub_server_mgr = MagicMock(
             spec=novaclient.v2.servers.ServerManager)
         self.stub_running_server = MagicMock(
             spec=novaclient.v2.servers.Server)
-        self.stub_running_server.status = 'HEALTHY'
+        self.stub_running_server.status = 'ACTIVE'
         self.stub_running_server.flavor = {'id': 6, 'ram': 512}
         self.stub_verifying_server = MagicMock(
             spec=novaclient.v2.servers.Server)
-        self.stub_verifying_server.id = 'fake_id'
         self.stub_verifying_server.status = 'VERIFY_RESIZE'
         self.stub_verifying_server.flavor = {'id': 8, 'ram': 768}
         self.stub_server_mgr.get = MagicMock(
             return_value=self.stub_verifying_server)
         self.instance_task._nova_client.servers = self.stub_server_mgr
         stub_flavor_manager = MagicMock(
             spec=novaclient.v2.flavors.FlavorManager)
@@ -898,31 +731,28 @@
             MagicMock(return_value=None))
         self.tbmb_running_mock = self.tbmb_running_patch.start()
         self.addCleanup(self.tbmb_running_patch.stop)
 
         if 'volume' in self._testMethodName:
             self._stub_volume_client()
 
-        if ('floating_ips' in self._testMethodName or
-                'public_ips' in self._testMethodName):
-            self._stub_neutron_client()
-
     def tearDown(self):
         super(BuiltInstanceTasksTest, self).tearDown()
 
     def test_resize_flavor(self):
         orig_server = self.instance_task.server
         self.instance_task.resize_flavor({'id': 1, 'ram': 512},
                                          self.new_flavor)
         # verify
         self.assertIsNot(self.instance_task.server, orig_server)
-        self.assertEqual(1, self.instance_task._guest.stop_db.call_count)
+        self.instance_task._guest.stop_db.assert_any_call(
+            do_not_start_on_reboot=True)
         orig_server.resize.assert_any_call(self.new_flavor['id'])
         self.assertThat(self.db_instance.task_status, Is(InstanceTasks.NONE))
-        self.assertEqual(2, self.stub_server_mgr.get.call_count)
+        self.assertEqual(1, self.stub_server_mgr.get.call_count)
         self.assertThat(self.db_instance.flavor_id, Is(self.new_flavor['id']))
 
     @patch('trove.taskmanager.models.LOG')
     def test_resize_flavor_resize_failure(self, mock_logging):
         orig_server = self.instance_task.server
         self.stub_verifying_server.status = 'ERROR'
         with patch.object(self.instance_task._nova_client.servers, 'get',
@@ -930,29 +760,45 @@
             # execute
             self.assertRaises(TroveError, self.instance_task.resize_flavor,
                               {'id': 1, 'ram': 512}, self.new_flavor)
             # verify
             self.assertTrue(self.stub_server_mgr.get.called)
             self.assertIs(self.instance_task.server,
                           self.stub_verifying_server)
-            self.assertEqual(1, self.instance_task._guest.stop_db.call_count)
+            self.instance_task._guest.stop_db.assert_any_call(
+                do_not_start_on_reboot=True)
             orig_server.resize.assert_any_call(self.new_flavor['id'])
             self.assertThat(self.db_instance.task_status,
                             Is(InstanceTasks.NONE))
             self.assertThat(self.db_instance.flavor_id, Is('6'))
 
     @patch.object(utils, 'poll_until')
     def test_reboot(self, mock_poll):
+        self.instance_task.datastore_status_matches = Mock(return_value=True)
+        self.instance_task._refresh_datastore_status = Mock()
         self.instance_task.server.reboot = Mock()
-
+        self.instance_task.set_datastore_status_to_paused = Mock()
         self.instance_task.reboot()
-
         self.instance_task._guest.stop_db.assert_any_call()
+        self.instance_task._refresh_datastore_status.assert_any_call()
         self.instance_task.server.reboot.assert_any_call()
-        self.instance_task._guest.restart.assert_any_call()
+        self.instance_task.set_datastore_status_to_paused.assert_any_call()
+
+    @patch.object(utils, 'poll_until')
+    @patch('trove.taskmanager.models.LOG')
+    def test_reboot_datastore_not_ready(self, mock_logging, mock_poll):
+        self.instance_task.datastore_status_matches = Mock(return_value=False)
+        self.instance_task._refresh_datastore_status = Mock()
+        self.instance_task.server.reboot = Mock()
+        self.instance_task.set_datastore_status_to_paused = Mock()
+        self.instance_task.reboot()
+        self.instance_task._guest.stop_db.assert_any_call()
+        self.instance_task._refresh_datastore_status.assert_any_call()
+        assert not self.instance_task.server.reboot.called
+        assert not self.instance_task.set_datastore_status_to_paused.called
 
     @patch.object(BaseInstance, 'update_db')
     def test_detach_replica(self, mock_update_db):
         with patch.object(self.instance_task, 'reset_task_status') as tr_mock:
             self.instance_task.detach_replica(Mock(), True)
             self.instance_task._guest.detach_replica.assert_called_with(True)
             mock_update_db.assert_called_with(slave_of_id=None)
@@ -1004,24 +850,45 @@
         replica_config.config_contents = config_content
 
         with patch.object(taskmanager_models.BuiltInstanceTasks,
                           '_render_replica_config',
                           return_value=replica_config):
             self.instance_task.attach_replica(master)
         self.instance_task._guest.attach_replica.assert_called_with(
-            replica_context, config_content, restart=False)
+            replica_context, config_content)
         mock_update_db.assert_called_with(slave_of_id=master.id)
 
     @patch('trove.taskmanager.models.LOG')
     def test_error_attach_replica(self, mock_logging):
         with patch.object(self.instance_task._guest, 'attach_replica',
                           side_effect=GuestError):
             self.assertRaises(GuestError, self.instance_task.attach_replica,
                               Mock())
 
+    def test_get_floating_ips(self):
+        with patch.object(remote, 'create_neutron_client',
+                          return_value=_fake_neutron_client()):
+            floating_ips = self.instance_task._get_floating_ips()
+            self.assertEqual('192.168.10.1',
+                             floating_ips['192.168.10.1'].get(
+                                 'floating_ip_address'))
+
+    @patch.object(BaseInstance, 'get_visible_ip_addresses',
+                  return_value=['192.168.10.1'])
+    def test_detach_public_ips(self, mock_address):
+        with patch.object(remote, 'create_neutron_client',
+                          return_value=_fake_neutron_client()):
+            removed_ips = self.instance_task.detach_public_ips()
+            self.assertEqual(['192.168.10.1'], removed_ips)
+
+    def test_attach_public_ips(self):
+        self.instance_task.attach_public_ips(['192.168.10.1'])
+        self.stub_verifying_server.add_floating_ip.assert_called_with(
+            '192.168.10.1')
+
     @patch.object(BaseInstance, 'update_db')
     def test_enable_as_master(self, mock_update_db):
         test_func = self.instance_task._guest.enable_as_master
         config_content = {'config_contents': 'some junk'}
         replica_source_config = MagicMock()
         replica_source_config.config_contents = config_content
         with patch.object(self.instance_task, '_render_replica_source_config',
@@ -1051,28 +918,35 @@
         self.instance_task.cleanup_source_on_replica_detach(replica_info)
         test_func.assert_called_with(replica_info)
 
     def test_demote_replication_master(self):
         self.instance_task.demote_replication_master()
         self.instance_task._guest.demote_replication_master.assert_any_call()
 
-    @patch('trove.taskmanager.models.BuiltInstanceTasks.set_service_status')
-    @patch('trove.taskmanager.models.BuiltInstanceTasks.is_service_healthy')
-    @patch('trove.taskmanager.models.BuiltInstanceTasks.reset_task_status')
-    def test_upgrade(self, mock_resetstatus, mock_check, mock_setstatus):
-        dsv = MagicMock()
-        attrs = {'name': 'new_version'}
-        dsv.configure_mock(**attrs)
-        mock_check.return_value = True
-        self.instance_task._guest.pre_upgrade.return_value = {}
-
-        self.instance_task.upgrade(dsv)
-
-        self.instance_task._guest.upgrade.assert_called_once_with(
-            {'datastore_version': 'new_version'})
+    @patch.multiple(taskmanager_models.BuiltInstanceTasks,
+                    get_injected_files=Mock(return_value="the-files"))
+    def test_upgrade(self, *args):
+        pre_rebuild_server = self.instance_task.server
+        dsv = Mock(image_id='foo_image')
+        mock_volume = Mock(attachments=[{'device': '/dev/mock_dev'}])
+        with patch.object(self.instance_task._volume_client.volumes, "get",
+                          Mock(return_value=mock_volume)):
+            mock_server = Mock(status='ACTIVE')
+            with patch.object(self.instance_task._nova_client.servers,
+                              'get', Mock(return_value=mock_server)):
+                with patch.multiple(self.instance_task._guest,
+                                    pre_upgrade=Mock(return_value={}),
+                                    post_upgrade=Mock()):
+                    self.instance_task.upgrade(dsv)
+
+                    self.instance_task._guest.pre_upgrade.assert_called_with()
+                    pre_rebuild_server.rebuild.assert_called_with(
+                        dsv.image_id, files="the-files")
+                    self.instance_task._guest.post_upgrade.assert_called_with(
+                        mock_volume.attachments[0])
 
     def test_fix_device_path(self):
         self.assertEqual("/dev/vdb", self.instance_task.
                          _fix_device_path("vdb"))
         self.assertEqual("/dev/dev", self.instance_task.
                          _fix_device_path("dev"))
         self.assertEqual("/dev/vdb/dev", self.instance_task.
@@ -1083,79 +957,90 @@
 
     def setUp(self):
         super(BackupTasksTest, self).setUp()
         self.backup = backup_models.DBBackup()
         self.backup.id = 'backup_id'
         self.backup.name = 'backup_test',
         self.backup.description = 'test desc'
-        self.backup.location = 'http://xxx/z_CLOUD/container/12e48.xbstream.gz'
+        self.backup.location = 'http://xxx/z_CLOUD/12e48.xbstream.gz'
         self.backup.instance_id = 'instance id'
         self.backup.created = 'yesterday'
         self.backup.updated = 'today'
         self.backup.size = 2.0
         self.backup.state = state.BackupState.NEW
+        self.container_content = (None,
+                                  [{'name': 'first'},
+                                   {'name': 'second'},
+                                   {'name': 'third'}])
         self.bm_backup_patches = patch.multiple(
             backup_models.Backup,
             delete=MagicMock(return_value=None),
             get_by_id=MagicMock(return_value=self.backup))
         self.bm_backup_mocks = self.bm_backup_patches.start()
         self.addCleanup(self.bm_backup_patches.stop)
         self.bm_DBBackup_patch = patch.object(
             backup_models.DBBackup, 'save',
             MagicMock(return_value=self.backup))
         self.bm_DBBackup_mock = self.bm_DBBackup_patch.start()
         self.addCleanup(self.bm_DBBackup_patch.stop)
         self.backup.delete = MagicMock(return_value=None)
+        self.swift_client = MagicMock()
+        self.create_swift_client_patch = patch.object(
+            remote, 'create_swift_client',
+            MagicMock(return_value=self.swift_client))
+        self.create_swift_client_mock = self.create_swift_client_patch.start()
+        self.addCleanup(self.create_swift_client_patch.stop)
+
+        self.swift_client.head_container = MagicMock(
+            side_effect=ClientException("foo"))
+        self.swift_client.head_object = MagicMock(
+            side_effect=ClientException("foo"))
+        self.swift_client.get_container = MagicMock(
+            return_value=self.container_content)
+        self.swift_client.delete_object = MagicMock(return_value=None)
+        self.swift_client.delete_container = MagicMock(return_value=None)
 
     def tearDown(self):
         super(BackupTasksTest, self).tearDown()
 
     def test_delete_backup_nolocation(self):
         self.backup.location = ''
         taskmanager_models.BackupTasks.delete_backup('dummy context',
                                                      self.backup.id)
+        self.backup.delete.assert_any_call()
 
-        self.assertTrue(self.backup.deleted)
-
-    @patch('trove.common.clients.create_swift_client')
-    def test_delete_backup_fail_delete_manifest(self, mock_swift_client):
-        client_mock = MagicMock()
-        client_mock.head_object.return_value = {}
-        client_mock.delete_object.side_effect = ClientException("foo")
-        mock_swift_client.return_value = client_mock
-
-        self.assertRaises(
-            TroveError,
-            taskmanager_models.BackupTasks.delete_backup,
-            'dummy context', self.backup.id
-        )
-        self.assertFalse(backup_models.Backup.delete.called)
-        self.assertEqual(
-            state.BackupState.DELETE_FAILED,
-            self.backup.state,
-            "backup should be in DELETE_FAILED status"
-        )
-
-    @patch('trove.common.clients.create_swift_client')
-    def test_delete_backup_delete_swift(self, mock_swift_client):
-        client_mock = MagicMock()
-        mock_swift_client.return_value = client_mock
-
-        taskmanager_models.BackupTasks.delete_backup(mock.ANY, self.backup.id)
-
-        client_mock.head_object.assert_called_once_with('container',
-                                                        '12e48.xbstream.gz')
-        client_mock.delete_object.assert_called_once_with('container',
-                                                          '12e48.xbstream.gz')
-
-    def test_delete_backup_restored(self):
-        self.backup.state = state.BackupState.RESTORED
-        taskmanager_models.BackupTasks.delete_backup(mock.ANY, self.backup.id)
+    @patch('trove.taskmanager.models.LOG')
+    def test_delete_backup_fail_delete_manifest(self, mock_logging):
+        with patch.object(self.swift_client, 'delete_object',
+                          side_effect=ClientException("foo")):
+            with patch.object(self.swift_client, 'head_object',
+                              return_value={}):
+                self.assertRaises(
+                    TroveError,
+                    taskmanager_models.BackupTasks.delete_backup,
+                    'dummy context', self.backup.id)
+                self.assertFalse(backup_models.Backup.delete.called)
+                self.assertEqual(
+                    state.BackupState.DELETE_FAILED,
+                    self.backup.state,
+                    "backup should be in DELETE_FAILED status")
 
-        self.assertTrue(self.backup.deleted)
+    @patch('trove.taskmanager.models.LOG')
+    def test_delete_backup_fail_delete_segment(self, mock_logging):
+        with patch.object(self.swift_client, 'delete_object',
+                          side_effect=ClientException("foo")):
+            self.assertRaises(
+                TroveError,
+                taskmanager_models.BackupTasks.delete_backup,
+                'dummy context', self.backup.id)
+            self.assertFalse(backup_models.Backup.delete.called)
+            self.assertEqual(
+                state.BackupState.DELETE_FAILED,
+                self.backup.state,
+                "backup should be in DELETE_FAILED status")
 
     def test_parse_manifest(self):
         manifest = 'container/prefix'
         cont, prefix = taskmanager_models.BackupTasks._parse_manifest(manifest)
         self.assertEqual('container', cont)
         self.assertEqual('prefix', prefix)
 
@@ -1175,15 +1060,14 @@
         manifest = 'container/'
         cont, prefix = taskmanager_models.BackupTasks._parse_manifest(manifest)
         self.assertEqual('container', cont)
         self.assertEqual('', prefix)
 
 
 class NotifyMixinTest(trove_testtools.TestCase):
-
     def test_get_service_id(self):
         id_map = {
             'mysql': '123',
             'percona': 'abc'
         }
         mixin = taskmanager_models.NotifyMixin()
         self.assertThat(mixin._get_service_id('mysql', id_map), Equals('123'))
@@ -1205,46 +1089,43 @@
         super(RootReportTest, self).setUp()
         util.init_db()
 
     def tearDown(self):
         super(RootReportTest, self).tearDown()
 
     def test_report_root_first_time(self):
-        context = Mock()
-        context.user_id = utils.generate_uuid()
         report = mysql_models.RootHistory.create(
-            context, utils.generate_uuid())
+            None, utils.generate_uuid(), 'root')
         self.assertIsNotNone(report)
 
     def test_report_root_double_create(self):
-        context = Mock()
-        context.user_id = utils.generate_uuid()
-        id = utils.generate_uuid()
-        history = mysql_models.RootHistory(id, context.user_id).save()
+        uuid = utils.generate_uuid()
+        history = mysql_models.RootHistory(uuid, 'root').save()
         with patch.object(mysql_models.RootHistory, 'load',
                           Mock(return_value=history)):
-            report = mysql_models.RootHistory.create(context, id)
+            report = mysql_models.RootHistory.create(
+                None, uuid, 'root')
             self.assertTrue(mysql_models.RootHistory.load.called)
             self.assertEqual(history.user, report.user)
             self.assertEqual(history.id, report.id)
 
 
 class ClusterRootTest(trove_testtools.TestCase):
 
     @patch.object(common_models.RootHistory, "create")
     @patch.object(common_models.Root, "create")
     def test_cluster_root_create(self, root_create, root_history_create):
         context = Mock()
-        context.user_id = utils.generate_uuid()
-        id = utils.generate_uuid()
+        uuid = utils.generate_uuid()
+        user = "root"
         password = "rootpassword"
         cluster_instances = [utils.generate_uuid(), utils.generate_uuid()]
-        common_models.ClusterRoot.create(context, id, password,
+        common_models.ClusterRoot.create(context, uuid, user, password,
                                          cluster_instances)
-        root_create.assert_called_with(context, id, password,
+        root_create.assert_called_with(context, uuid, user, password,
                                        cluster_instances_list=None)
         self.assertEqual(2, root_history_create.call_count)
         calls = [
-            call(context, cluster_instances[0]),
-            call(context, cluster_instances[1])
+            call(context, cluster_instances[0], user),
+            call(context, cluster_instances[1], user)
         ]
         root_history_create.assert_has_calls(calls)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/taskmanager/test_vertica_clusters.py` & `trove-8.0.1/trove/tests/unittests/taskmanager/test_vertica_clusters.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,16 +9,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import datetime
 
-from unittest.mock import Mock
-from unittest.mock import patch
+from mock import Mock
+from mock import patch
 
 from trove.cluster.models import ClusterTasks as ClusterTaskStatus
 from trove.cluster.models import DBCluster
 import trove.common.context as context
 from trove.common.exception import GuestError
 from trove.common.strategies.cluster.experimental.vertica.taskmanager import (
     VerticaClusterTasks as ClusterTasks)
@@ -28,16 +28,16 @@
     VerticaTaskManagerStrategy as task_strategy)
 from trove.datastore import models as datastore_models
 from trove.instance.models import BaseInstance
 from trove.instance.models import DBInstance
 from trove.instance.models import Instance
 from trove.instance.models import InstanceServiceStatus
 from trove.instance.models import InstanceTasks
-from trove.instance.service_status import ServiceStatuses
 from trove import rpc
+from trove.taskmanager.models import ServiceStatuses
 from trove.tests.unittests import trove_testtools
 
 
 class VerticaClusterTasksTest(trove_testtools.TestCase):
     def setUp(self):
         super(VerticaClusterTasksTest, self).setUp()
         self.cluster_id = "1232"
@@ -82,52 +82,29 @@
         mock_dv1.name = '7.1'
         self.clustertasks = ClusterTasks(Mock(),
                                          self.db_cluster,
                                          datastore=mock_ds1,
                                          datastore_version=mock_dv1)
 
     @patch.object(ClusterTasks, 'update_statuses_on_failure')
-    @patch.object(DBInstance, 'find_by')
-    @patch.object(InstanceServiceStatus, 'find_by')
-    @patch('trove.taskmanager.models.LOG')
-    def test_all_instances_ready_with_server_error(self,
-                                                   mock_logging, mock_find,
-                                                   mock_db_find, mock_update):
-        (mock_find.return_value.
-         get_status.return_value) = ServiceStatuses.NEW
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.BUILDING_ERROR_SERVER
-        ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
-                                                         self.cluster_id)
-        mock_update.assert_called_with(self.cluster_id, None)
-        self.assertFalse(ret_val)
-
-    @patch.object(ClusterTasks, 'update_statuses_on_failure')
-    @patch.object(DBInstance, 'find_by')
     @patch.object(InstanceServiceStatus, 'find_by')
     @patch('trove.taskmanager.models.LOG')
     def test_all_instances_ready_bad_status(self, mock_logging,
-                                            mock_find, mock_db_find,
-                                            mock_update):
+                                            mock_find, mock_update):
         (mock_find.return_value.
          get_status.return_value) = ServiceStatuses.FAILED
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
         ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
                                                          self.cluster_id)
         mock_update.assert_called_with(self.cluster_id, None)
         self.assertFalse(ret_val)
 
-    @patch.object(DBInstance, 'find_by')
     @patch.object(InstanceServiceStatus, 'find_by')
-    def test_all_instances_ready(self, mock_find, mock_db_find):
+    def test_all_instances_ready(self, mock_find):
         (mock_find.return_value.
          get_status.return_value) = ServiceStatuses.INSTANCE_READY
-        (mock_db_find.return_value.
-         get_task_status.return_value) = InstanceTasks.NONE
         ret_val = self.clustertasks._all_instances_ready(["1", "2", "3", "4"],
                                                          self.cluster_id)
         self.assertTrue(ret_val)
 
     @patch.object(ClusterTasks, 'reset_task')
     @patch.object(ClusterTasks, '_all_instances_ready', return_value=False)
     @patch.object(Instance, 'load')
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/upgrade/test_controller.py` & `trove-8.0.1/trove/tests/unittests/upgrade/test_controller.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 import jsonschema
-from unittest.mock import Mock, MagicMock, patch
+from mock import Mock, MagicMock, patch
 
 from trove.extensions.mgmt.upgrade.models import UpgradeMessageSender
 from trove.extensions.mgmt.upgrade.service import UpgradeController
 from trove.tests.unittests import trove_testtools
 
 
 class TestUpgradeController(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/upgrade/test_models.py` & `trove-8.0.1/trove/tests/unittests/upgrade/test_models.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,16 +9,15 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-from unittest.mock import patch
-
+from mock import patch
 from trove.extensions.mgmt.upgrade.models import UpgradeMessageSender
 from trove import rpc
 from trove.tests.unittests import trove_testtools
 
 
 class TestUpgradeModel(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/util/matchers.py` & `trove-8.0.1/trove/tests/unittests/util/matchers.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/util/util.py` & `trove-8.0.1/trove/guestagent/strategies/restore/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,44 +1,24 @@
-#    Copyright 2012 OpenStack Foundation
-#
-#    Licensed under the Apache License, Version 2.0 (the "License"); you may
-#    not use this file except in compliance with the License. You may obtain
-#    a copy of the License at
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
+
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
 #
-#         http://www.apache.org/licenses/LICENSE-2.0
+# http://www.apache.org/licenses/LICENSE-2.0
 #
-#    Unless required by applicable law or agreed to in writing, software
-#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
-#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
-#    License for the specific language governing permissions and limitations
-#    under the License.
-
-import threading
-
-from trove.common import cfg
-from trove.db import get_db_api
-from trove.db.sqlalchemy import session
-
-CONF = cfg.CONF
-DB_SETUP = None
-LOCK = threading.Lock()
-
-
-def init_db():
-    with LOCK:
-        global DB_SETUP
-        if not DB_SETUP:
-            CONF.set_override("connection",
-                              "sqlite:///trove_test.sqlite",
-                              "database")
-            db_api = get_db_api()
-            db_api.db_sync(CONF)
-            session.configure_db(CONF)
-            DB_SETUP = True
-
-
-def cleanup_db():
-    with LOCK:
-        global DB_SETUP
-        if DB_SETUP:
-            session.clean_db()
-            DB_SETUP = False
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from oslo_log import log as logging
+
+from trove.common.strategies.strategy import Strategy
+
+LOG = logging.getLogger(__name__)
+
+
+def get_restore_strategy(restore_driver, ns=__name__):
+    LOG.debug("Getting restore strategy: %s.", restore_driver)
+    return Strategy.get_strategy(restore_driver, ns)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/volume_type/test_volume_type.py` & `trove-8.0.1/trove/tests/unittests/volume_type/test_volume_type.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,17 +10,17 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest import mock
+import mock
 
-from trove.common import clients
+from trove.common import remote
 from trove.tests.unittests import trove_testtools
 from trove.volume_type import models
 
 
 class TestVolumeType(trove_testtools.TestCase):
 
     def test_volume_type(self):
@@ -34,15 +34,15 @@
 
         self.assertEqual(cinder_volume_type.id, volume_type.id)
         self.assertEqual(cinder_volume_type.name, volume_type.name)
         self.assertEqual(cinder_volume_type.is_public, volume_type.is_public)
         self.assertEqual(cinder_volume_type.description,
                          volume_type.description)
 
-    @mock.patch.object(clients, 'create_cinder_client')
+    @mock.patch.object(remote, 'create_cinder_client')
     def test_volume_types(self, mock_client):
         mock_context = mock.MagicMock()
         mock_types = [mock.MagicMock(), mock.MagicMock()]
 
         mock_client(mock_context).volume_types.list.return_value = mock_types
 
         volume_types = models.VolumeTypes(mock_context)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/unittests/volume_type/test_volume_type_views.py` & `trove-8.0.1/trove/tests/unittests/volume_type/test_volume_type_views.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
-from unittest import mock
+import mock
 
 from trove.tests.unittests import trove_testtools
 from trove.volume_type import views
 
 
 class TestVolumeTypeViews(trove_testtools.TestCase):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/__init__.py` & `trove-8.0.1/trove/tests/util/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -19,49 +19,58 @@
 
 .. automodule:: utils
    :platform: Unix
    :synopsis: Tests for Nova.
 """
 
 import subprocess
-from urllib.parse import unquote
 try:
     EVENT_AVAILABLE = True
 except ImportError:
     EVENT_AVAILABLE = False
 
-import glanceclient
-from keystoneauth1.identity import v3
-from keystoneauth1 import session
-from neutronclient.v2_0 import client as neutron_client
-from novaclient import client as nova_client
 from proboscis.asserts import assert_true
 from proboscis.asserts import Check
 from proboscis.asserts import fail
 from proboscis import SkipTest
+from six.moves.urllib.parse import unquote
 from sqlalchemy import create_engine
-from sqlalchemy.sql.expression import text
-import tenacity
 from troveclient.compat import Dbaas
+from troveclient.compat import exceptions
 
 from trove.common import cfg
 from trove.common.utils import import_class
 from trove.common.utils import import_object
 from trove.tests.config import CONFIG as test_config
-from trove.tests.util.client import TestClient
-from trove.tests.util import mysql
+from trove.tests.util.client import TestClient as TestClient
 from trove.tests.util import test_config as CONFIG
 from trove.tests.util.users import Requirements
 
 
 WHITE_BOX = test_config.white_box
-FLUSH = text("FLUSH PRIVILEGES;")
 CONF = cfg.CONF
 
 
+def assert_http_code(expected_http_code, func, *args, **kwargs):
+    try:
+        rtn_value = func(*args, **kwargs)
+        assert_equal(
+            expected_http_code,
+            200,
+            "Expected the function to return http code %s but instead got "
+            "no error (code 200?)." % expected_http_code)
+        return rtn_value
+    except exceptions.ClientException as ce:
+        assert_equal(
+            expected_http_code,
+            ce.code,
+            "Expected the function to return http code %s but instead got "
+            "code %s." % (expected_http_code, ce.code))
+
+
 def create_client(*args, **kwargs):
     """
     Using the User Requirements as arguments, finds a user and grabs a new
     DBAAS client.
     """
     reqs = Requirements(*args, **kwargs)
     user = test_config.users.find_user(reqs)
@@ -101,14 +110,15 @@
                               False):
         kwargs['service_url'] += "/" + user.tenant
 
     if auth_strategy == 'fake':
         from troveclient.compat import auth
 
         class FakeAuth(auth.Authenticator):
+
             def authenticate(self):
                 class FakeCatalog(object):
                     def __init__(self, auth):
                         self.auth = auth
 
                     def get_public_url(self):
                         return "%s/%s" % (test_config.dbaas_url,
@@ -144,58 +154,48 @@
             actual = dbaas.client.service_url
             msg = "Dbaas management url was expected to start with %s, but " \
                   "was %s." % (expected_prefix, actual)
             check.true(actual.startswith(expected_prefix), msg)
     return TestClient(dbaas)
 
 
-def create_keystone_session(user):
-    auth = v3.Password(username=user.auth_user,
-                       password=user.auth_key,
-                       project_id=user.tenant_id,
-                       user_domain_name='Default',
-                       project_domain_name='Default',
-                       auth_url=test_config.auth_url)
-    return session.Session(auth=auth)
-
-
 def create_nova_client(user, service_type=None):
+    """Creates a rich client for the Nova API using the test config."""
+    if test_config.nova_client is None:
+        raise SkipTest("No nova_client info specified in the Test Config "
+                       "so this test will be skipped.")
+    from novaclient.client import Client
     if not service_type:
-        service_type = CONF.nova_compute_service_type
-    openstack = nova_client.Client(
-        CONF.nova_client_version,
-        username=user.auth_user,
-        password=user.auth_key,
-        user_domain_name='Default',
-        project_id=user.tenant_id,
-        auth_url=CONFIG.auth_url,
-        service_type=service_type, os_cache=False,
-        cacert=test_config.values.get('cacert', None)
-    )
-
+        service_type = test_config.nova_client['nova_service_type']
+    openstack = Client(CONF.nova_client_version,
+                       user.auth_user,
+                       user.auth_key,
+                       project_name=user.tenant,
+                       auth_url=test_config.nova_client['auth_url'],
+                       service_type=service_type, os_cache=False,
+                       cacert=test_config.values.get('cacert', None))
+    openstack.authenticate()
     return TestClient(openstack)
 
 
-def create_neutron_client(user):
-    sess = create_keystone_session(user)
-    client = neutron_client.Client(
-        session=sess,
-        service_type=CONF.neutron_service_type,
-        region_name=CONFIG.trove_client_region_name,
-        insecure=CONF.neutron_api_insecure,
-        endpoint_type=CONF.neutron_endpoint_type
-    )
-
-    return TestClient(client)
-
-
 def create_glance_client(user):
-    sess = create_keystone_session(user)
-    glance = glanceclient.Client(CONF.glance_client_version, session=sess)
+    """Creates a rich client for the Glance API using the test config."""
+    if test_config.glance_client is None:
+        raise SkipTest("No glance_client info specified in the Test Config "
+                       "so this test will be skipped.")
+    from glanceclient import Client
+    from keystoneauth1.identity import v2
+    from keystoneauth1 import session
 
+    auth = v2.Password(username=user.auth_user,
+                       password=user.auth_key,
+                       tenant_name=user.tenant,
+                       auth_url=test_config.glance_client['auth_url'])
+    session = session.Session(auth=auth)
+    glance = Client(CONF.glance_client_version, session=session)
     return TestClient(glance)
 
 
 def dns_checker(mgmt_instance):
     """Given a MGMT instance, ensures DNS provisioning worked.
 
     Uses a helper class which, given a mgmt instance (returned by the mgmt
@@ -205,16 +205,18 @@
         checker = import_class(CONFIG.trove_dns_checker)
         checker()(mgmt_instance)
     else:
         raise SkipTest("Can't access DNS system to check if DNS provisioned.")
 
 
 def process(cmd):
-    output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
-    return output
+    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,
+                               stderr=subprocess.PIPE)
+    result = process.communicate()
+    return result
 
 
 def string_in_list(str, substr_list):
     """Returns True if the string appears in the list."""
     return any([str.find(x) >= 0 for x in substr_list])
 
 
@@ -259,35 +261,30 @@
                      "local.MySqlConnection")
     if cls == "local.MySqlConnection":
         return MySqlConnection()
     return import_object(cls)()
 
 
 class MySqlConnection(object):
+
     def assert_fails(self, ip, user_name, password):
+        from trove.tests.util import mysql
         try:
             with mysql.create_mysql_connection(ip, user_name, password):
                 pass
             fail("Should have failed to connect: mysql --host %s -u %s -p%s"
                  % (ip, user_name, password))
         except mysql.MySqlPermissionsFailure:
             return  # Good, this is what we wanted.
         except mysql.MySqlConnectionFailure as mcf:
             fail("Expected to see permissions failure. Instead got message:"
                  "%s" % mcf.message)
 
-    @tenacity.retry(
-        wait=tenacity.wait_fixed(3),
-        stop=tenacity.stop_after_attempt(5),
-        reraise=True
-    )
     def create(self, ip, user_name, password):
-        print("Connecting mysql, host: %s, user: %s, password: %s" %
-              (ip, user_name, password))
-
+        from trove.tests.util import mysql
         return mysql.create_mysql_connection(ip, user_name, password)
 
 
 class LocalSqlClient(object):
     """A sqlalchemy wrapper to manage transactions."""
 
     def __init__(self, engine, use_flush=True):
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/check.py` & `trove-8.0.1/trove/tests/util/check.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 
 from proboscis.asserts import assert_equal
 from proboscis.asserts import assert_false
 from proboscis.asserts import assert_not_equal
 from proboscis.asserts import assert_true
 from proboscis.asserts import ASSERTION_ERROR
 from proboscis.asserts import Check
+import six
 
 
 def get_stack_trace_of_caller(level_up):
     """Gets the stack trace at the point of the caller."""
     level_up += 1
     st = traceback.extract_stack()
     caller_index = len(st) - level_up
@@ -35,15 +36,15 @@
     new_st = st[0:caller_index]
     return new_st
 
 
 def raise_blame_caller(level_up, ex):
     """Raises an exception, changing the stack trace to point to the caller."""
     new_st = get_stack_trace_of_caller(level_up + 2)
-    raise ex.with_traceback(new_st)
+    six.reraise(type(ex), ex, new_st)
 
 
 class Checker(object):
 
     def __init__(self):
         self.messages = []
         self.odd = True
@@ -65,15 +66,15 @@
     def equal(self, *args, **kwargs):
         self._run_assertion(assert_equal, *args, **kwargs)
 
     def false(self, *args, **kwargs):
         self._run_assertion(assert_false, *args, **kwargs)
 
     def not_equal(self, *args, **kwargs):
-        self._run_assertion(assert_not_equal, *args, **kwargs)
+        _run_assertion(assert_not_equal, *args, **kwargs)
 
     def _run_assertion(self, assert_func, *args, **kwargs):
         """
         Runs an assertion method, but catches any failure and adds it as a
         string to the messages list.
         """
         if self.protected:
@@ -167,18 +168,18 @@
     """Checks for attributes in an object."""
 
     def __init__(self, name, instance):
         self.name = name
         self.instance = instance
         super(TypeCheck, self).__init__()
 
-    def _check_type(self, value, attribute_type):
+    def _check_type(value, attribute_type):
         if not isinstance(value, attribute_type):
             self.fail("%s attribute %s is of type %s (expected %s)."
-                      % (self.name, self.attribute_name, type(value),
+                      % (self.name, attribute_name, type(value),
                          attribute_type))
 
     def has_field(self, attribute_name, attribute_type,
                   additional_checks=None):
         if not hasattr(self.instance, attribute_name):
             self.fail("%s missing attribute %s." % (self.name, attribute_name))
         else:
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/event_simulator.py` & `trove-8.0.1/trove/tests/util/event_simulator.py`

 * *Files 1% similar despite different names*

```diff
@@ -177,15 +177,14 @@
         """Starts up the thread. Should be called from a different thread."""
         # Don't call this from the thread which it represents.
         assert eventlet.corolocal.get_ident() != self.id
         self.caller_sem = Semaphore(0)
         self.my_sem.release()
         self.caller_sem.acquire()  # Wait for it to finish.
 
-
 # Main global thread to run.
 main_greenlet = None
 
 # Stack of threads currently running or sleeping
 fake_threads = []
 
 # Allow a sleep method to be called at least this number of times before
@@ -224,26 +223,26 @@
         if ft['greenlet'].id == cr.id:
             ft['next_sleep_time'] = time_to_sleep
 
     cr.sleep()
 
 
 def fake_poll_until(retriever, condition=lambda value: value,
-                    sleep_time=1, time_out=0):
+                    sleep_time=1, time_out=None):
     """Fakes out poll until."""
     from trove.common import exception
     slept_time = 0
     while True:
         resource = retriever()
         if condition(resource):
             return resource
         fake_sleep(sleep_time)
         slept_time += sleep_time
         if time_out and slept_time >= time_out:
-            raise exception.PollTimeOut()
+                raise exception.PollTimeOut()
 
 
 def run_main(func):
     """Runs the given function as the initial thread of the event simulator."""
     global main_greenlet
     main_greenlet = Coroutine(main_loop)
     fake_spawn(0, func)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/mysql.py` & `trove-8.0.1/trove/tests/util/mysql.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
 
 import re
 
-from oslo_db.sqlalchemy import engines
+from oslo_db.sqlalchemy import session
 import pexpect
 from sqlalchemy.exc import OperationalError
 try:
     from sqlalchemy.exc import ResourceClosedError
 except ImportError:
     ResourceClosedError = Exception
 
@@ -69,18 +69,18 @@
                 raise MySqlPermissionsFailure(oe)
             else:
                 raise MySqlConnectionFailure(oe)
 
     @staticmethod
     def _exception_is_permissions_issue(msg):
         """Assert message cited a permissions issue and not something else."""
-        pos_error = re.compile(r".*Host '[\w\.]*' is not allowed to connect "
-                               "to this MySQL server.*")
+        pos_error = re.compile(".*Host '[\w\.]*' is not allowed to connect to "
+                               "this MySQL server.*")
         pos_error1 = re.compile(".*Access denied for user "
-                                r"'[\w\*\!\@\#\^\&]*'@'[\w\.]*'.*")
+                                "'[\w\*\!\@\#\^\&]*'@'[\w\.]*'.*")
         if (pos_error.match(msg) or pos_error1.match(msg)):
             return True
 
     def __enter__(self):
         try:
             self.conn = self.engine.connect()
         except OperationalError as oe:
@@ -110,30 +110,31 @@
                 self.trans.rollback()
             else:
                 self.trans.commit()
         self.conn.close()
 
     @staticmethod
     def _init_engine(user, password, host):
-        return engines.create_engine(
-            "mysql+pymysql://%s:%s@%s:3306" % (user, password, host))
+        return session.EngineFacade(
+            "mysql+pymysql://%s:%s@%s:3306" % (user, password, host)
+        ).get_engine()
 
 
 class PexpectMySqlConnection(object):
 
     TIME_OUT = 30
 
     def __init__(self, ssh_args, host, user, password):
         self.host = host
         self.user = user
         self.password = password
         cmd = '%s %s' % (tests.SSH_CMD, ssh_args)
         self.proc = pexpect.spawn(cmd)
         print(cmd)
-        self.proc.expect(r":~\$", timeout=self.TIME_OUT)
+        self.proc.expect(":~\$", timeout=self.TIME_OUT)
         cmd2 = "mysql --host '%s' -u '%s' '-p%s'\n" % \
                (self.host, self.user, self.password)
         print(cmd2)
         self.proc.send(cmd2)
         result = self.proc.expect([
             'mysql>',
             'Access denied',
@@ -147,15 +148,15 @@
     def __enter__(self):
         return self
 
     def __exit__(self, type, value, traceback):
         self.proc.close()
 
     def execute(self, cmd):
-        self.proc.send(cmd + "\\G\n")
+        self.proc.send(cmd + "\G\n")
         outcome = self.proc.expect(['Empty set', 'mysql>'],
                                    timeout=self.TIME_OUT)
         if outcome == 0:
             return []
         else:
             # This next line might be invaluable for long test runs.
             print("Interpreting output: %s" % self.proc.before)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/server_connection.py` & `trove-8.0.1/trove/tests/util/server_connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -9,70 +9,50 @@
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
+import netaddr
 import os
-import subprocess
-
 from proboscis.asserts import fail
-import tenacity
 
 from trove import tests
-from trove.tests.config import CONFIG
 from trove.tests import util
 from trove.tests.util.users import Requirements
 
 
-def create_server_connection(instance_id, ip_address=None):
+def create_server_connection(instance_id):
     if util.test_config.use_local_ovz:
         return OpenVZServerConnection(instance_id)
-    return ServerSSHConnection(instance_id, ip_address=ip_address)
+    return ServerSSHConnection(instance_id)
 
 
 class ServerSSHConnection(object):
-    def __init__(self, instance_id, ip_address=None):
-        if not ip_address:
-            req_admin = Requirements(is_admin=True)
-            user = util.test_config.users.find_user(req_admin)
-            dbaas_admin = util.create_dbaas_client(user)
-            instance = dbaas_admin.management.show(instance_id)
-
-            mgmt_interfaces = instance.server["addresses"].get(
-                CONFIG.trove_mgmt_network, []
-            )
-            mgmt_addresses = [str(inf["addr"]) for inf in mgmt_interfaces
-                              if inf["version"] == 4]
-
-            if len(mgmt_addresses) == 0:
-                fail("No IPV4 ip found for management network.")
-            else:
-                self.ip_address = mgmt_addresses[0]
-        else:
-            self.ip_address = ip_address
+    def __init__(self, instance_id):
+        self.instance_id = instance_id
+        req_admin = Requirements(is_admin=True)
+        self.user = util.test_config.users.find_user(req_admin)
+        self.dbaas_admin = util.create_dbaas_client(self.user)
+        self.instance = self.dbaas_admin.management.show(self.instance_id)
+        try:
+            self.ip_address = [str(ip) for ip in self.instance.ip
+                               if netaddr.valid_ipv4(ip)][0]
+        except Exception:
+            fail("No IPV4 ip found")
 
         TROVE_TEST_SSH_USER = os.environ.get('TROVE_TEST_SSH_USER')
         if TROVE_TEST_SSH_USER and '@' not in self.ip_address:
             self.ip_address = TROVE_TEST_SSH_USER + '@' + self.ip_address
 
-    @tenacity.retry(
-        wait=tenacity.wait_fixed(5),
-        stop=tenacity.stop_after_attempt(3),
-        retry=tenacity.retry_if_exception_type(subprocess.CalledProcessError)
-    )
     def execute(self, cmd):
-        exe_cmd = "%s %s '%s'" % (tests.SSH_CMD, self.ip_address, cmd)
+        exe_cmd = "%s %s %s" % (tests.SSH_CMD, self.ip_address, cmd)
         print("RUNNING COMMAND: %s" % exe_cmd)
-
-        output = util.process(exe_cmd)
-
-        print("OUTPUT: %s" % output)
-        return output
+        return util.process(exe_cmd)
 
 
 class OpenVZServerConnection(object):
     def __init__(self, instance_id):
         self.instance_id = instance_id
         req_admin = Requirements(is_admin=True)
         self.user = util.test_config.users.find_user(req_admin)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/usage.py` & `trove-8.0.1/trove/tests/util/usage.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove/tests/util/users.py` & `trove-8.0.1/trove/tests/util/users.py`

 * *Files 7% similar despite different names*

```diff
@@ -85,14 +85,22 @@
                                tenant_id=user_dict.get("tenant_id", None),
                                requirements=reqs)
             self.users.append(user)
 
     def find_all_users_who_satisfy(self, requirements, black_list=None):
         """Returns a list of all users who satisfy the given requirements."""
         black_list = black_list or []
+        print("Searching for a user who meets requirements %s in our list..."
+              % requirements)
+        print("Users:")
+        for user in self.users:
+            print("\t" + str(user))
+        print("Black list")
+        for item in black_list:
+            print("\t" + str(item))
         return (user for user in self.users
                 if user.auth_user not in black_list and
                 user.requirements.satisfies(requirements))
 
     def find_user(self, requirements, black_list=None):
         """Finds a user who meets the requirements and has been used least."""
         users = self.find_all_users_who_satisfy(requirements, black_list)
@@ -106,20 +114,19 @@
         return user
 
     def _find_user_by_condition(self, condition):
         users = (user for user in self.users if condition(user))
         try:
             user = min(users, key=lambda user: user.test_count)
         except ValueError:
-            raise RuntimeError('Did not find a user with specified condition')
+            raise RuntimeError('Did not find a user with name "%s".' % name)
         user.test_count += 1
         return user
 
     def find_user_by_name(self, name):
         """Finds a user who meets the requirements and has been used least."""
         condition = lambda user: user.auth_user == name
         return self._find_user_by_condition(condition)
 
     def find_user_by_tenant_id(self, tenant_id):
-        """Finds a user who meets the requirements and has been used least."""
         condition = lambda user: user.tenant_id == tenant_id
         return self._find_user_by_condition(condition)
```

### Comparing `trove-21.0.0.0rc2/trove/tests/util/utils.py` & `trove-8.0.1/trove/tests/util/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,14 +15,16 @@
 
 
 import time
 
 from functools import wraps
 from oslo_log import log as logging
 
+from trove.common.i18n import _
+
 
 LOG = logging.getLogger(__name__)
 
 
 def retry(expected_exception_cls, retries=3, delay_fun=lambda n: 3 * n):
     """Retry decorator.
     Executes the decorated function N times with a variable timeout
@@ -41,15 +43,15 @@
             remaining_attempts = retries
             while remaining_attempts > 1:
                 try:
                     return f(*args, **kwargs)
                 except expected_exception_cls:
                     remaining_attempts -= 1
                     delay = delay_fun(retries - remaining_attempts)
-                    LOG.exception(
+                    LOG.exception(_(
                         "Retrying in %(delay)d seconds "
-                        "(remaining attempts: %(remaining)d)..." %
+                        "(remaining attempts: %(remaining)d)...") %
                         {'delay': delay, 'remaining': remaining_attempts})
                     time.sleep(delay)
             return f(*args, **kwargs)
         return wrapper
     return retry_deco
```

### Comparing `trove-21.0.0.0rc2/trove/version.py` & `trove-8.0.1/trove/common/strategies/storage/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,18 +1,26 @@
-# Copyright 2011 OpenStack Foundation
+# Copyright 2013 Hewlett-Packard Development Company, L.P.
 # All Rights Reserved.
 #
 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
 #    not use this file except in compliance with the License. You may obtain
 #    a copy of the License at
 #
 #         http://www.apache.org/licenses/LICENSE-2.0
 #
 #    Unless required by applicable law or agreed to in writing, software
 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+
+from oslo_log import log as logging
+
+from trove.common.strategies.strategy import Strategy
+
+LOG = logging.getLogger(__name__)
 
-import pbr.version
 
-version_info = pbr.version.VersionInfo('trove')
+def get_storage_strategy(storage_driver, ns=__name__):
+    LOG.debug("Getting storage strategy: %s.", storage_driver)
+    return Strategy.get_strategy(storage_driver, ns)
```

### Comparing `trove-21.0.0.0rc2/trove/versions.py` & `trove-8.0.1/trove/versions.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,18 +12,17 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 import os
 import routes
 
-from trove.common import cfg
 from trove.common import wsgi
 
-CONF = cfg.CONF
+
 VERSIONS = {
     "1.0": {
         "id": "v1.0",
         "status": "CURRENT",
         "updated": "2012-08-01T00:00:00Z",
         "links": [],
     },
@@ -53,15 +52,15 @@
 
 
 class BaseVersion(object):
 
     def __init__(self, id, status, base_url, updated):
         self.id = id
         self.status = status
-        self.base_url = CONF.public_endpoint or base_url
+        self.base_url = base_url
         self.updated = updated
 
     def data(self):
         return {
             "id": self.id,
             "status": self.status,
             "updated": self.updated,
```

### Comparing `trove-21.0.0.0rc2/trove/volume_type/models.py` & `trove-8.0.1/trove/volume_type/models.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,17 +13,17 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
 
 """Model classes that form the core of volume-support functionality"""
 
 from cinderclient import exceptions as cinder_exception
-from trove.common import clients
 from trove.common import exception as trove_exception
 from trove.common import models
+from trove.common import remote
 
 
 class VolumeType(object):
 
     _data_fields = ['id', 'name', 'is_public', 'description']
 
     def __init__(self, volume_type=None):
@@ -32,15 +32,15 @@
 
     @classmethod
     def load(cls, volume_type_id, context=None, client=None):
         if not(client or context):
             raise trove_exception.InvalidModelError(
                 "client or context must be provided to load a volume_type")
         if not client:
-            client = clients.create_cinder_client(context)
+            client = remote.create_cinder_client(context)
         try:
             volume_type = client.volume_types.get(volume_type_id)
         except cinder_exception.NotFound:
             raise trove_exception.NotFound(uuid=volume_type_id)
         except cinder_exception.ClientException as ce:
             raise trove_exception.TroveError(str(ce))
         return cls(volume_type)
@@ -61,15 +61,14 @@
     def description(self):
         return self.volume_type.description
 
 
 class VolumeTypes(models.CinderRemoteModelBase):
 
     def __init__(self, context):
-        volume_types = clients.create_cinder_client(
-            context).volume_types.list()
+        volume_types = remote.create_cinder_client(context).volume_types.list()
         self.volume_types = [VolumeType(volume_type=item)
                              for item in volume_types]
 
     def __iter__(self):
         for item in self.volume_types:
             yield item
```

### Comparing `trove-21.0.0.0rc2/trove/volume_type/views.py` & `trove-8.0.1/trove/volume_type/views.py`

 * *Files identical despite different names*

### Comparing `trove-21.0.0.0rc2/trove.egg-info/PKG-INFO` & `trove-8.0.1/PKG-INFO`

 * *Files 17% similar despite different names*

```diff
@@ -1,79 +1,56 @@
-Metadata-Version: 1.2
+Metadata-Version: 1.1
 Name: trove
-Version: 21.0.0.0rc2
+Version: 8.0.1
 Summary: OpenStack DBaaS
-Home-page: https://docs.openstack.org/trove/latest/
+Home-page: https://wiki.openstack.org/wiki/Trove
 Author: OpenStack
-Author-email: openstack-discuss@lists.openstack.org
+Author-email: openstack-dev@lists.openstack.org
 License: UNKNOWN
 Description: =====
         Trove
         =====
         
-        .. image:: https://governance.openstack.org/tc/badges/trove.svg
-            :target: https://governance.openstack.org/tc/reference/tags/index.html
+        .. image:: http://governance.openstack.org/badges/trove.svg
+            :target: http://governance.openstack.org/reference/tags/index.html
         
         Trove is Database as a Service for OpenStack.
         
         Getting Started
         ---------------
         
         If you'd like to run from the master branch, you can clone the git repo:
         
-            git clone https://opendev.org/openstack/trove
+            git clone https://github.com/openstack/trove
         
-        For information on how to contribute to trove, please see
-        CONTRIBUTING.rst_ and HACKING.rst_
         
-        .. _CONTRIBUTING.rst: https://opendev.org/openstack/trove/src/branch/master/CONTRIBUTING.rst
-        .. _HACKING.rst: https://opendev.org/openstack/trove/src/branch/master/HACKING.rst
+        * Wiki: https://wiki.openstack.org/wiki/Trove
+        * Developer Docs: http://docs.openstack.org/developer/trove
         
-        * `Wiki <https://wiki.openstack.org/wiki/Trove>`_
-        * `Developer Docs <https://docs.openstack.org/trove/latest/>`_
-        
-        You can raise bugs here:
-        `Bug Tracker <https://storyboard.openstack.org/#!/project/openstack/trove>`_
-        
-        The plan for trove can be found at
-        `Trove Specs <https://specs.openstack.org/openstack/trove-specs/>`_
-        
-        Release notes for the project can be found at:
-          https://docs.openstack.org/releasenotes/trove
+        You can raise bugs here: https://bugs.launchpad.net/trove
         
         Python client
         -------------
-        Python-troveclient_ is a client for Trove.
-        
-        .. _Python-troveclient: https://opendev.org/openstack/python-troveclient
-        
-        Dashboard plugin
-        ----------------
-        Trove-dashboard_ is OpenStack dashbaord plugin for Trove.
-        
-        .. _Trove-dashboard: https://opendev.org/openstack/trove-dashboard
+        https://git.openstack.org/cgit/openstack/python-troveclient
         
         References
         ----------
         
-        * `Installation docs`_
-        * `Manual installation docs`_
-        * `Build guest image`_
-        
-        .. _Installation docs: https://docs.openstack.org/trove/latest/install/
-        .. _Manual installation docs: https://docs.openstack.org/trove/latest/install/install-manual.html
-        .. _Build guest image: https://docs.openstack.org/trove/latest/admin/building_guest_images.html
+        * Installation docs:
+          http://docs.openstack.org/developer/trove/dev/install.html
+        * Manual installation docs:
+          http://docs.openstack.org/developer/trove/dev/manual_install.html
+        * Build guest image:
+          http://docs.openstack.org/developer/trove/dev/building_guest_images.html
         
         
 Platform: UNKNOWN
 Classifier: Environment :: OpenStack
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: System Administrators
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 2
+Classifier: Programming Language :: Python :: 2.7
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Requires-Python: >= 3.8
+Classifier: Programming Language :: Python :: 3.5
```

### Comparing `trove-21.0.0.0rc2/trove.egg-info/SOURCES.txt` & `trove-8.0.1/trove.egg-info/SOURCES.txt`

 * *Files 27% similar despite different names*

```diff
@@ -1,215 +1,348 @@
 .coveragerc
 .stestr.conf
+.testr.conf
+.zuul.yaml
 AUTHORS
 CONTRIBUTING.rst
 ChangeLog
 HACKING.rst
 LICENSE
 README.rst
-bindep.txt
+babel.cfg
+blacklist-py3.txt
 generate_examples.py
 pylintrc
 requirements.txt
 run_tests.py
+run_tests.sh
 setup.cfg
 setup.py
 test-requirements.txt
 tox.ini
 api-ref/source/api-versions.inc
-api-ref/source/backup-strategy.inc
-api-ref/source/backups.inc
 api-ref/source/conf.py
 api-ref/source/configurations.inc
+api-ref/source/database-instance-actions.inc
+api-ref/source/database-instances.inc
 api-ref/source/databases.inc
-api-ref/source/datastore-versions.inc
 api-ref/source/datastores.inc
+api-ref/source/flavors.inc
 api-ref/source/index.rst
-api-ref/source/instance-actions.inc
-api-ref/source/instance-logs.inc
-api-ref/source/instances.inc
 api-ref/source/parameters.yaml
-api-ref/source/quotas.inc
-api-ref/source/users.inc
-api-ref/source/samples/backup-create-request.json
-api-ref/source/samples/backup-create-response.json
-api-ref/source/samples/backup-get-response.json
-api-ref/source/samples/backup-list-response.json
-api-ref/source/samples/backup-strategy-create-request.json
-api-ref/source/samples/backup-strategy-create-response.json
-api-ref/source/samples/backup-strategy-list-response.json
-api-ref/source/samples/config-group-create-request.json
-api-ref/source/samples/config-group-create-response.json
-api-ref/source/samples/config-group-list-instances-response.json
-api-ref/source/samples/config-group-patch-request.json
-api-ref/source/samples/config-group-put-request.json
-api-ref/source/samples/config-group-show-response.json
-api-ref/source/samples/config-groups-list-response.json
-api-ref/source/samples/databases-create-request.json
-api-ref/source/samples/databases-list-response.json
-api-ref/source/samples/datastore-list-response.json
-api-ref/source/samples/datastore-show-response.json
-api-ref/source/samples/datastore-version-create-request.json
-api-ref/source/samples/datastore-version-list-response.json
-api-ref/source/samples/datastore-version-mgmt-list-response.json
-api-ref/source/samples/datastore-version-mgmt-patch-request.json
-api-ref/source/samples/datastore-version-mgmt-show-response.json
-api-ref/source/samples/datastore-version-parameter-create-request.json
-api-ref/source/samples/datastore-version-parameter-create-response.json
-api-ref/source/samples/datastore-version-parameter-list-response.json
-api-ref/source/samples/datastore-version-parameter-show-response.json
-api-ref/source/samples/datastore-version-parameter-update-request.json
-api-ref/source/samples/datastore-version-parameter-update-response.json
-api-ref/source/samples/datastore-version-show-response.json
-api-ref/source/samples/instance-action-eject-replica-request.json
-api-ref/source/samples/instance-action-promote-replica-request.json
-api-ref/source/samples/instance-action-reset-status-request.json
-api-ref/source/samples/instance-action-resize-request.json
-api-ref/source/samples/instance-action-resize-volume-request.json
-api-ref/source/samples/instance-action-restart-request.json
-api-ref/source/samples/instance-backup-list-response.json
-api-ref/source/samples/instance-configuration-list-response.json
-api-ref/source/samples/instance-create-request.json
-api-ref/source/samples/instance-create-response.json
-api-ref/source/samples/instance-list-detail-response.json
-api-ref/source/samples/instance-list-response.json
-api-ref/source/samples/instance-log-disable-request.json
-api-ref/source/samples/instance-log-disable-response.json
-api-ref/source/samples/instance-log-discard-request.json
-api-ref/source/samples/instance-log-discard-response.json
-api-ref/source/samples/instance-log-enable-request.json
-api-ref/source/samples/instance-log-enable-response.json
-api-ref/source/samples/instance-log-list-response.json
-api-ref/source/samples/instance-log-publish-request.json
-api-ref/source/samples/instance-log-publish-response.json
-api-ref/source/samples/instance-log-show-request.json
-api-ref/source/samples/instance-log-show-response.json
-api-ref/source/samples/instance-mgmt-action-migrate-request.json
-api-ref/source/samples/instance-mgmt-action-reboot-request.json
-api-ref/source/samples/instance-mgmt-action-rebuild-instance-request.json
-api-ref/source/samples/instance-mgmt-action-reset-task-status-request.json
-api-ref/source/samples/instance-mgmt-action-stop-request.json
-api-ref/source/samples/instance-mgmt-list-response.json
-api-ref/source/samples/instance-mgmt-show-response.json
-api-ref/source/samples/instance-patch-detach-replica-request.json
-api-ref/source/samples/instance-patch-update-name-request.json
-api-ref/source/samples/instance-patch-upgrade-datastore-version-request.json
-api-ref/source/samples/instance-put-attach-config-group-request.json
-api-ref/source/samples/instance-show-response.json
-api-ref/source/samples/instance-update-access-request.json
-api-ref/source/samples/limit-show-response.json
-api-ref/source/samples/quota-show-response.json
-api-ref/source/samples/quota-update.json
-api-ref/source/samples/user-check-root-response.json
-api-ref/source/samples/user-create-request.json
-api-ref/source/samples/user-enable-root-response.json
-api-ref/source/samples/user-grant-databases-access-request.json
-api-ref/source/samples/user-list-response.json
-api-ref/source/samples/user-put-request.json
-api-ref/source/samples/user-root-enable-request.json
-api-ref/source/samples/user-show-response.json
-api-ref/source/samples/user-show-root-history-response.json
-api-ref/source/samples/users-put-request.json
-api-ref/source/samples/versions-response.json
-backup/Dockerfile
-backup/__init__.py
-backup/install.sh
-backup/main.py
-backup/requirements.txt
-backup/drivers/__init__.py
-backup/drivers/base.py
-backup/drivers/innobackupex.py
-backup/drivers/mariabackup.py
-backup/drivers/mysql_base.py
-backup/drivers/postgres.py
-backup/drivers/xtrabackup.py
-backup/storage/__init__.py
-backup/storage/base.py
-backup/storage/swift.py
-backup/tests/unittests/__init__.py
-backup/tests/unittests/drivers/__init__.py
-backup/tests/unittests/drivers/test_innobackupex.py
-backup/tests/unittests/drivers/test_mariadb.py
-backup/tests/unittests/drivers/test_postgres.py
-backup/tests/unittests/drivers/test_xtrabackup.py
-backup/utils/__init__.py
-backup/utils/postgresql.py
+api-ref/source/user-management.inc
+api-ref/source/samples/db-attach-config-group-request-json-http.txt
+api-ref/source/samples/db-attach-config-group-request.json
+api-ref/source/samples/db-attach-config-group-response-json-http.txt
+api-ref/source/samples/db-backup-create-incremental-request-json-http.txt
+api-ref/source/samples/db-backup-create-incremental-request.json
+api-ref/source/samples/db-backup-create-incremental-response-json-http.txt
+api-ref/source/samples/db-backup-create-incremental-response.json
+api-ref/source/samples/db-backup-create-request-json-http.txt
+api-ref/source/samples/db-backup-create-request.json
+api-ref/source/samples/db-backup-create-response-json-http.txt
+api-ref/source/samples/db-backup-create-response.json
+api-ref/source/samples/db-backup-delete-request-json-http.txt
+api-ref/source/samples/db-backup-delete-response-json-http.txt
+api-ref/source/samples/db-backup-get-request-json-http.txt
+api-ref/source/samples/db-backup-get-response-json-http.txt
+api-ref/source/samples/db-backup-get-response.json
+api-ref/source/samples/db-backup-list-request-json-http.txt
+api-ref/source/samples/db-backup-list-response-json-http.txt
+api-ref/source/samples/db-backup-list-response.json
+api-ref/source/samples/db-backup-restore-request-json-http.txt
+api-ref/source/samples/db-backup-restore-request.json
+api-ref/source/samples/db-backup-restore-response-json-http.txt
+api-ref/source/samples/db-backup-restore-response.json
+api-ref/source/samples/db-backups-by-instance-request-json-http.txt
+api-ref/source/samples/db-backups-by-instance-response-json-http.txt
+api-ref/source/samples/db-backups-by-instance-response.json
+api-ref/source/samples/db-change-user-attributes-request-json-http.txt
+api-ref/source/samples/db-change-user-attributes-request.json
+api-ref/source/samples/db-change-user-attributes-response-json-http.txt
+api-ref/source/samples/db-change-users-password-request-json-http.txt
+api-ref/source/samples/db-change-users-password-request.json
+api-ref/source/samples/db-change-users-password-response-json-http.txt
+api-ref/source/samples/db-check-root-user-request-json-http.txt
+api-ref/source/samples/db-check-root-user-response-json-http.txt
+api-ref/source/samples/db-check-root-user-response.json
+api-ref/source/samples/db-config-group-details-request-json-http.txt
+api-ref/source/samples/db-config-group-details-response-json-http.txt
+api-ref/source/samples/db-config-group-details-response.json
+api-ref/source/samples/db-config-group-instances-request-json-http.txt
+api-ref/source/samples/db-config-group-instances-response-json-http.txt
+api-ref/source/samples/db-config-group-instances-response.json
+api-ref/source/samples/db-configuration-attach-to-instance-request-json-http.txt
+api-ref/source/samples/db-configuration-attach-to-instance-request.json
+api-ref/source/samples/db-configuration-attach-to-instance-response-json-http.txt
+api-ref/source/samples/db-configuration-create-request-json-http.txt
+api-ref/source/samples/db-configuration-create-request.json
+api-ref/source/samples/db-configuration-create-response-json-http.txt
+api-ref/source/samples/db-configuration-create-response.json
+api-ref/source/samples/db-configuration-delete-request-json-http.txt
+api-ref/source/samples/db-configuration-delete-response-json-http.txt
+api-ref/source/samples/db-configuration-detach-from-instance-request-json-http.txt
+api-ref/source/samples/db-configuration-detach-from-instance-request.json
+api-ref/source/samples/db-configuration-detach-from-instance-response-json-http.txt
+api-ref/source/samples/db-configuration-details-request-json-http.txt
+api-ref/source/samples/db-configuration-details-response-json-http.txt
+api-ref/source/samples/db-configuration-details-response.json
+api-ref/source/samples/db-configuration-edit-parameters-request-json-http.txt
+api-ref/source/samples/db-configuration-edit-parameters-request.json
+api-ref/source/samples/db-configuration-edit-parameters-response-json-http.txt
+api-ref/source/samples/db-configuration-list-instances-request-json-http.txt
+api-ref/source/samples/db-configuration-list-instances-response-json-http.txt
+api-ref/source/samples/db-configuration-list-instances-response.json
+api-ref/source/samples/db-configuration-list-request-json-http.txt
+api-ref/source/samples/db-configuration-list-response-json-http.txt
+api-ref/source/samples/db-configuration-list-response.json
+api-ref/source/samples/db-configuration-parameter-for-datastore-version-request-json-http.txt
+api-ref/source/samples/db-configuration-parameter-for-datastore-version-response-json-http.txt
+api-ref/source/samples/db-configuration-parameter-for-datastore-version-response.json
+api-ref/source/samples/db-configuration-parameter-without-datastore-version-request-json-http.txt
+api-ref/source/samples/db-configuration-parameter-without-datastore-version-response-json-http.txt
+api-ref/source/samples/db-configuration-parameter-without-datastore-version-response.json
+api-ref/source/samples/db-configuration-parameters-for-datastore-version-request-json-http.txt
+api-ref/source/samples/db-configuration-parameters-for-datastore-version-response-json-http.txt
+api-ref/source/samples/db-configuration-parameters-for-datastore-version-response.json
+api-ref/source/samples/db-configuration-parameters-without-datastore-version-request-json-http.txt
+api-ref/source/samples/db-configuration-parameters-without-datastore-version-response-json-http.txt
+api-ref/source/samples/db-configuration-parameters-without-datastore-version-response.json
+api-ref/source/samples/db-configuration-update-parameters-request-json-http.txt
+api-ref/source/samples/db-configuration-update-parameters-request.json
+api-ref/source/samples/db-configuration-update-parameters-response-json-http.txt
+api-ref/source/samples/db-create-config-group-request-json-http.txt
+api-ref/source/samples/db-create-config-group-request.json
+api-ref/source/samples/db-create-config-group-response-json-http.txt
+api-ref/source/samples/db-create-config-group-response.json
+api-ref/source/samples/db-create-databases-request-json-http.txt
+api-ref/source/samples/db-create-databases-request.json
+api-ref/source/samples/db-create-databases-response-json-http.txt
+api-ref/source/samples/db-create-instance-request-json-http.txt
+api-ref/source/samples/db-create-instance-request.json
+api-ref/source/samples/db-create-instance-response-json-http.txt
+api-ref/source/samples/db-create-instance-response.json
+api-ref/source/samples/db-create-users-request-json-http.txt
+api-ref/source/samples/db-create-users-request.json
+api-ref/source/samples/db-create-users-response-json-http.txt
+api-ref/source/samples/db-datastore-by-id-request-json-http.txt
+api-ref/source/samples/db-datastore-by-id-response-json-http.txt
+api-ref/source/samples/db-datastore-by-id-response.json
+api-ref/source/samples/db-datastore-parameters-response.json
+api-ref/source/samples/db-datastore-version-by-id-request-json-http.txt
+api-ref/source/samples/db-datastore-version-by-id-response-json-http.txt
+api-ref/source/samples/db-datastore-version-by-id-response.json
+api-ref/source/samples/db-datastore-versions-list-request-json-http.txt
+api-ref/source/samples/db-datastore-versions-list-response-json-http.txt
+api-ref/source/samples/db-datastore-versions-list-response.json
+api-ref/source/samples/db-datastores-list-request-json-http.txt
+api-ref/source/samples/db-datastores-list-response-json-http.txt
+api-ref/source/samples/db-datastores-list-response.json
+api-ref/source/samples/db-delete-config-group-request-json-http.txt
+api-ref/source/samples/db-delete-databases-request-json-http.txt
+api-ref/source/samples/db-delete-databases-response-json-http.txt
+api-ref/source/samples/db-delete-instance-request-json-http.txt
+api-ref/source/samples/db-delete-instance-response-json-http.txt
+api-ref/source/samples/db-delete-root-request-json-http.txt
+api-ref/source/samples/db-delete-users-request-json-http.txt
+api-ref/source/samples/db-delete-users-response-json-http.txt
+api-ref/source/samples/db-detach-config-group-request-json-http.txt
+api-ref/source/samples/db-detach-config-group-request.json
+api-ref/source/samples/db-detach-config-group-response-json-http.txt
+api-ref/source/samples/db-detach-replica-request-json-http.txt
+api-ref/source/samples/db-detach-replica-request.json
+api-ref/source/samples/db-detach-replica-response-json-http.txt
+api-ref/source/samples/db-disable-root-user-request-json-http.txt
+api-ref/source/samples/db-disable-root-user-response-json-http.txt
+api-ref/source/samples/db-enable-root-user-request-json-http.txt
+api-ref/source/samples/db-enable-root-user-response-json-http.txt
+api-ref/source/samples/db-enable-root-user-response.json
+api-ref/source/samples/db-faults-badRequest.json
+api-ref/source/samples/db-faults-instanceFault.json
+api-ref/source/samples/db-faults-itemNotFound.json
+api-ref/source/samples/db-flavors-by-id-request-json-http.txt
+api-ref/source/samples/db-flavors-by-id-response-json-http.txt
+api-ref/source/samples/db-flavors-by-id-response.json
+api-ref/source/samples/db-flavors-request-json-http.txt
+api-ref/source/samples/db-flavors-response-json-http.txt
+api-ref/source/samples/db-flavors-response.json
+api-ref/source/samples/db-get-default-instance-configuration-request-json-http.txt
+api-ref/source/samples/db-get-default-instance-configuration-response-json-http.txt
+api-ref/source/samples/db-get-default-instance-configuration-response.json
+api-ref/source/samples/db-grant-user-access-request-json-http.txt
+api-ref/source/samples/db-grant-user-access-request.json
+api-ref/source/samples/db-grant-user-access-response-json-http.txt
+api-ref/source/samples/db-instance-eject-replica-request-json-http.txt
+api-ref/source/samples/db-instance-eject-replica-request.json
+api-ref/source/samples/db-instance-promote-replica-request-json-http.txt
+api-ref/source/samples/db-instance-promote-replica-request.json
+api-ref/source/samples/db-instance-reboot-request-json-http.txt
+api-ref/source/samples/db-instance-reboot-request.json
+api-ref/source/samples/db-instance-reboot-response-json-http.txt
+api-ref/source/samples/db-instance-resize-flavor-request-json-http.txt
+api-ref/source/samples/db-instance-resize-flavor-request.json
+api-ref/source/samples/db-instance-resize-flavor-response-json-http.txt
+api-ref/source/samples/db-instance-resize-instance-request-json-http.txt
+api-ref/source/samples/db-instance-resize-instance-request.json
+api-ref/source/samples/db-instance-resize-instance-response-json-http.txt
+api-ref/source/samples/db-instance-resize-volume-request-json-http.txt
+api-ref/source/samples/db-instance-resize-volume-request.json
+api-ref/source/samples/db-instance-resize-volume-response-json-http.txt
+api-ref/source/samples/db-instance-restart-request-json-http.txt
+api-ref/source/samples/db-instance-restart-request.json
+api-ref/source/samples/db-instance-restart-response-json-http.txt
+api-ref/source/samples/db-instance-status-detail-request-json-http.txt
+api-ref/source/samples/db-instance-status-detail-response-json-http.txt
+api-ref/source/samples/db-instance-status-detail-response.json
+api-ref/source/samples/db-instances-index-pagination-request-json-http.txt
+api-ref/source/samples/db-instances-index-pagination-response-json-http.txt
+api-ref/source/samples/db-instances-index-pagination-response.json
+api-ref/source/samples/db-instances-index-request-json-http.txt
+api-ref/source/samples/db-instances-index-response-json-http.txt
+api-ref/source/samples/db-instances-index-response.json
+api-ref/source/samples/db-list-cfg-defaults-request-json-http.txt
+api-ref/source/samples/db-list-cfg-defaults-response-json-http.txt
+api-ref/source/samples/db-list-cfg-defaults-response.json
+api-ref/source/samples/db-list-cfg-groups-request-json-http.txt
+api-ref/source/samples/db-list-cfg-groups-response-json-http.txt
+api-ref/source/samples/db-list-cfg-groups-response.json
+api-ref/source/samples/db-list-databases-pagination-request-json-http.txt
+api-ref/source/samples/db-list-databases-pagination-response-json-http.txt
+api-ref/source/samples/db-list-databases-pagination-response.json
+api-ref/source/samples/db-list-databases-request-json-http.txt
+api-ref/source/samples/db-list-databases-response-json-http.txt
+api-ref/source/samples/db-list-databases-response.json
+api-ref/source/samples/db-list-datastore-versions-request-json-http.txt
+api-ref/source/samples/db-list-datastore-versions-response-json-http.txt
+api-ref/source/samples/db-list-datastore-versions.json
+api-ref/source/samples/db-list-parameters-request-json-http.txt
+api-ref/source/samples/db-list-parameters-response-json-http.txt
+api-ref/source/samples/db-list-parameters-response.json
+api-ref/source/samples/db-list-user-access-request-json-http.txt
+api-ref/source/samples/db-list-user-access-response-json-http.txt
+api-ref/source/samples/db-list-user-access-response.json
+api-ref/source/samples/db-list-user-dbs-request-json-http.txt
+api-ref/source/samples/db-list-user-dbs-response-json-http.txt
+api-ref/source/samples/db-list-user-dbs-response.json
+api-ref/source/samples/db-list-users-pagination-request-json-http.txt
+api-ref/source/samples/db-list-users-pagination-response-json-http.txt
+api-ref/source/samples/db-list-users-pagination-response.json
+api-ref/source/samples/db-list-users-request-json-http.txt
+api-ref/source/samples/db-list-users-response-json-http.txt
+api-ref/source/samples/db-list-users-response.json
+api-ref/source/samples/db-mgmt-get-account-details-request-json-http.txt
+api-ref/source/samples/db-mgmt-get-account-details-response-json-http.txt
+api-ref/source/samples/db-mgmt-get-account-details-response.json
+api-ref/source/samples/db-mgmt-get-host-detail-request-json-http.txt
+api-ref/source/samples/db-mgmt-get-host-detail-response-json-http.txt
+api-ref/source/samples/db-mgmt-get-host-detail-response.json
+api-ref/source/samples/db-mgmt-get-instance-details-request-json-http.txt
+api-ref/source/samples/db-mgmt-get-instance-details-response-json-http.txt
+api-ref/source/samples/db-mgmt-get-instance-details-response.json
+api-ref/source/samples/db-mgmt-get-root-details-request-json-http.txt
+api-ref/source/samples/db-mgmt-get-root-details-response-json-http.txt
+api-ref/source/samples/db-mgmt-get-root-details-response.json
+api-ref/source/samples/db-mgmt-get-storage-request-json-http.txt
+api-ref/source/samples/db-mgmt-get-storage-response-json-http.txt
+api-ref/source/samples/db-mgmt-get-storage-response.json
+api-ref/source/samples/db-mgmt-instance-diagnostics-request-json-http.txt
+api-ref/source/samples/db-mgmt-instance-diagnostics-response-json-http.txt
+api-ref/source/samples/db-mgmt-instance-diagnostics-response.json
+api-ref/source/samples/db-mgmt-instance-index-request-json-http.txt
+api-ref/source/samples/db-mgmt-instance-index-response-json-http.txt
+api-ref/source/samples/db-mgmt-instance-index-response.json
+api-ref/source/samples/db-mgmt-list-accounts-request-json-http.txt
+api-ref/source/samples/db-mgmt-list-accounts-response-json-http.txt
+api-ref/source/samples/db-mgmt-list-accounts-response.json
+api-ref/source/samples/db-mgmt-list-hosts-request-json-http.txt
+api-ref/source/samples/db-mgmt-list-hosts-response-json-http.txt
+api-ref/source/samples/db-mgmt-list-hosts-response.json
+api-ref/source/samples/db-patch-config-group-request-json-http.txt
+api-ref/source/samples/db-patch-config-group-request.json
+api-ref/source/samples/db-patch-config-group-response-json-http.txt
+api-ref/source/samples/db-restore-delete-request-json-http.txt
+api-ref/source/samples/db-restore-delete-response-json-http.txt
+api-ref/source/samples/db-revoke-user-access-request-json-http.txt
+api-ref/source/samples/db-revoke-user-access-response-json-http.txt
+api-ref/source/samples/db-show-parameter-details-request-json-http.txt
+api-ref/source/samples/db-show-parameter-details-response-json-http.txt
+api-ref/source/samples/db-show-parameter-details.json
+api-ref/source/samples/db-update-config-group-request-json-http.txt
+api-ref/source/samples/db-update-config-group-request.json
+api-ref/source/samples/db-update-config-group-response-json-http.txt
+api-ref/source/samples/db-version-request-json-http.txt
+api-ref/source/samples/db-version-response-json-http.txt
+api-ref/source/samples/db-version-response.json
+api-ref/source/samples/db-versions-request-json-http.txt
+api-ref/source/samples/db-versions-response-json-http.txt
+api-ref/source/samples/db-versions-response.json
+apidocs/src/samples/db-get-default-instance-configuration-response-json.txt
 contrib/trove-guestagent
-contrib/trove-network-driver
 devstack/README.rst
 devstack/plugin.sh
 devstack/settings
 devstack/files/apache-trove-api.template
 devstack/files/debs/trove
 devstack/files/rpms/trove
 devstack/files/rpms-suse/trove
-doc/requirements.txt
 doc/source/conf.py
 doc/source/index.rst
+doc/source/admin/basics.rst
 doc/source/admin/building_guest_images.rst
-doc/source/admin/database_management.rst
-doc/source/admin/datastore.rst
+doc/source/admin/database_module_usage.rst
+doc/source/admin/guest_cloud_init.rst
 doc/source/admin/index.rst
-doc/source/admin/network_isolation.rst
-doc/source/admin/run_trove_in_production.rst
 doc/source/admin/secure_oslo_messaging.rst
-doc/source/admin/troubleshooting.rst
-doc/source/admin/upgrade.rst
 doc/source/cli/index.rst
 doc/source/cli/trove-manage.rst
-doc/source/cli/trove-status.rst
-doc/source/contributor/contributing.rst
 doc/source/contributor/design.rst
-doc/source/contributor/functional_test.rst
+doc/source/contributor/how_to_create_a_trove_instance.rst
 doc/source/contributor/index.rst
-doc/source/contributor/release-notes.rst
 doc/source/contributor/testing.rst
 doc/source/install/apache-mod-wsgi.rst
 doc/source/install/common_configure.txt
 doc/source/install/common_prerequisites.txt
+doc/source/install/conf.py
 doc/source/install/dashboard.rst
 doc/source/install/get_started.rst
 doc/source/install/index.rst
-doc/source/install/install-devstack.rst
-doc/source/install/install-manual.rst
-doc/source/install/install-redhat.rst
-doc/source/install/install-suse.rst
+doc/source/install/install-obs.rst
+doc/source/install/install-rdo.rst
 doc/source/install/install-ubuntu.rst
+doc/source/install/install.rst
+doc/source/install/manual_install.rst
 doc/source/install/next-steps.rst
 doc/source/install/verify.rst
-doc/source/reference/conf-file.rst
-doc/source/reference/conf.rst
 doc/source/reference/index.rst
 doc/source/reference/notifier.rst
-doc/source/reference/policy-file.rst
-doc/source/reference/policy.rst
 doc/source/reference/trove_api_extensions.rst
-doc/source/user/backup-db.rst
-doc/source/user/create-db.rst
-doc/source/user/index.rst
-doc/source/user/instance-status.rst
-doc/source/user/manage-db-and-users.rst
-doc/source/user/manage-db-config.rst
-doc/source/user/set-up-clustering.rst
-doc/source/user/set-up-replication.rst
-doc/source/user/upgrade-cluster-datastore.rst
-doc/source/user/upgrade-datastore.rst
+doc/votes/channel_logging
 etc/apache2/trove
 etc/tests/core.test.conf
 etc/tests/localhost.test.conf
-etc/trove/README-policy.generated.md
 etc/trove/api-paste.ini
 etc/trove/api-paste.ini.test
+etc/trove/policy.json
+etc/trove/trove-conductor.conf.sample
+etc/trove/trove-guestagent.conf.sample
 etc/trove/trove-logging-guestagent.conf
+etc/trove/trove-taskmanager.conf.sample
+etc/trove/trove-workbook.yaml
+etc/trove/trove.conf.sample
 etc/trove/trove.conf.test
+etc/trove/cloudinit/README
+etc/trove/conf.d/README
+etc/trove/conf.d/guest_info.conf
 integration/README.md
+integration/scripts/conf.json.example
 integration/scripts/create_vm
 integration/scripts/functions
 integration/scripts/functions_qemu
 integration/scripts/image-projects-list
+integration/scripts/local.conf.rc
 integration/scripts/localrc.rc
 integration/scripts/projects-list
 integration/scripts/reviews.rc
 integration/scripts/trovestack
 integration/scripts/trovestack.rc
 integration/scripts/conf/cassandra.conf
 integration/scripts/conf/couchbase.conf
@@ -223,333 +356,273 @@
 integration/scripts/conf/pxc.conf
 integration/scripts/conf/redis.conf
 integration/scripts/conf/test_begin.conf
 integration/scripts/conf/test_end.conf
 integration/scripts/conf/vertica.conf
 integration/scripts/files/trove-guest.systemd.conf
 integration/scripts/files/trove-guest.upstart.conf
-integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/15-trove-dep
-integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/20-guest-systemd
-integration/scripts/files/deprecated-elements/fedora-guest/extra-data.d/62-ssh-key
-integration/scripts/files/deprecated-elements/fedora-guest/install.d/15-trove-dep
-integration/scripts/files/deprecated-elements/fedora-guest/install.d/20-etc
-integration/scripts/files/deprecated-elements/fedora-guest/install.d/21-use-fedora-certificates
-integration/scripts/files/deprecated-elements/fedora-guest/install.d/50-user
-integration/scripts/files/deprecated-elements/fedora-guest/install.d/62-ssh-key
-integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/05-ipforwarding
-integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/62-trove-guest-sudoers
-integration/scripts/files/deprecated-elements/fedora-guest/post-install.d/90-yum-update
-integration/scripts/files/deprecated-elements/fedora-mariadb/README.md
-integration/scripts/files/deprecated-elements/fedora-mariadb/install.d/10-mariadb
-integration/scripts/files/deprecated-elements/fedora-mariadb/pre-install.d/10-percona-copr
-integration/scripts/files/deprecated-elements/fedora-mongodb/README.md
-integration/scripts/files/deprecated-elements/fedora-mongodb/install.d/10-mongodb
-integration/scripts/files/deprecated-elements/fedora-mongodb/install.d/25-trove-mongo-dep
-integration/scripts/files/deprecated-elements/fedora-mysql/README.md
-integration/scripts/files/deprecated-elements/fedora-mysql/install.d/10-mysql
-integration/scripts/files/deprecated-elements/fedora-mysql/install.d/40-xtrabackup
-integration/scripts/files/deprecated-elements/fedora-mysql/post-install.d/30-register-mysql-service
-integration/scripts/files/deprecated-elements/fedora-percona/install.d/05-percona-server
-integration/scripts/files/deprecated-elements/fedora-percona/install.d/10-mysql
-integration/scripts/files/deprecated-elements/fedora-postgresql/install.d/10-postgresql
-integration/scripts/files/deprecated-elements/fedora-redis/README.md
-integration/scripts/files/deprecated-elements/fedora-redis/install.d/10-redis
-integration/scripts/files/deprecated-elements/ubuntu-cassandra/install.d/10-cassandra
-integration/scripts/files/deprecated-elements/ubuntu-couchbase/install.d/10-couchbase
-integration/scripts/files/deprecated-elements/ubuntu-couchdb/install.d/10-couchdb
-integration/scripts/files/deprecated-elements/ubuntu-db2/README.md
-integration/scripts/files/deprecated-elements/ubuntu-db2/extra-data.d/20-copy-db2-pkgs
-integration/scripts/files/deprecated-elements/ubuntu-db2/install.d/10-db2
-integration/scripts/files/deprecated-elements/ubuntu-mongodb/README.md
-integration/scripts/files/deprecated-elements/ubuntu-mongodb/pre-install.d/10-mongodb-apt-key
-integration/scripts/files/deprecated-elements/ubuntu-percona/install.d/30-mysql
-integration/scripts/files/deprecated-elements/ubuntu-percona/pre-install.d/10-percona-apt-key
-integration/scripts/files/deprecated-elements/ubuntu-percona/pre-install.d/20-apparmor-mysql-local
-integration/scripts/files/deprecated-elements/ubuntu-pxc/install.d/30-mysql
-integration/scripts/files/deprecated-elements/ubuntu-pxc/pre-install.d/10-percona-apt-key
-integration/scripts/files/deprecated-elements/ubuntu-pxc/pre-install.d/20-apparmor-mysql-local
-integration/scripts/files/deprecated-elements/ubuntu-redis/README.md
-integration/scripts/files/deprecated-elements/ubuntu-redis/install.d/30-redis
-integration/scripts/files/deprecated-elements/ubuntu-redis/install.d/80-fix-in-guest-agent-env
-integration/scripts/files/deprecated-elements/ubuntu-vertica/README.md
-integration/scripts/files/deprecated-elements/ubuntu-vertica/extra-data.d/93-copy-vertica-deb
-integration/scripts/files/deprecated-elements/ubuntu-vertica/install.d/97-vertica
-integration/scripts/files/deprecated-elements/ubuntu-xenial-cassandra/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-couchbase/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-couchdb/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/10-mongodb-thp
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/20-mongodb
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/25-trove-mongo-dep
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/30-mongodb-conf
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/35-check-numa
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/40-check-numa-systemd
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/41-mongod-systemd
-integration/scripts/files/deprecated-elements/ubuntu-xenial-mongodb/install.d/42-mongos-systemd
-integration/scripts/files/deprecated-elements/ubuntu-xenial-percona/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-percona/post-install.d/10-fix-mycnf
-integration/scripts/files/deprecated-elements/ubuntu-xenial-pxc/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-pxc/install.d/31-fix-my-cnf
-integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/element-deps
-integration/scripts/files/deprecated-elements/ubuntu-xenial-redis/install.d/31-fix-init-file
-integration/scripts/files/elements/guest-agent/README.rst
-integration/scripts/files/elements/guest-agent/element-deps
-integration/scripts/files/elements/guest-agent/package-installs.yaml
-integration/scripts/files/elements/guest-agent/pkg-map
-integration/scripts/files/elements/guest-agent/source-repository-guest-agent
-integration/scripts/files/elements/guest-agent/svc-map
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/31-guest-agent-install
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/docker-hostnic-dev.service
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/docker-hostnic.service
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/docker-hostnic.socket
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent-dev.service
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.conf
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.init
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.logrotate
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-agent.service
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-log-collection.service
-integration/scripts/files/elements/guest-agent/install.d/guest-agent-source-install/guest-log-collection.timer
-integration/scripts/files/elements/guest-agent/post-install.d/31-enable-guest-agent-systemd
-integration/scripts/files/elements/guest-agent/post-install.d/99-clean-apt
-integration/scripts/files/elements/root-passwd/README.rst
-integration/scripts/files/elements/root-passwd/post-install.d/99-setup
-integration/scripts/files/elements/ubuntu-docker/element-deps
-integration/scripts/files/elements/ubuntu-docker/install.d/21-docker
-integration/scripts/files/elements/ubuntu-guest/extra-data.d/11-ssh-key-dev
-integration/scripts/files/elements/ubuntu-guest/install.d/11-user
-integration/scripts/files/elements/ubuntu-guest/install.d/12-ssh-key-dev
-integration/scripts/files/elements/ubuntu-guest/post-install.d/11-ipforwarding
-integration/scripts/files/elements/ubuntu-guest/post-install.d/12-ntp
-integration/scripts/files/elements/ubuntu-guest/post-install.d/13-trove-guest-sudoers
-integration/scripts/files/elements/ubuntu-guest/pre-install.d/11-baseline-tools
+integration/scripts/files/elements/apt-conf-dir/README.rst
+integration/scripts/files/elements/apt-conf-dir/extra-data.d/99-use-host-apt-confd
+integration/scripts/files/elements/fedora-guest/extra-data.d/15-trove-dep
+integration/scripts/files/elements/fedora-guest/extra-data.d/20-guest-systemd
+integration/scripts/files/elements/fedora-guest/extra-data.d/62-ssh-key
+integration/scripts/files/elements/fedora-guest/install.d/15-trove-dep
+integration/scripts/files/elements/fedora-guest/install.d/20-etc
+integration/scripts/files/elements/fedora-guest/install.d/50-user
+integration/scripts/files/elements/fedora-guest/install.d/62-ssh-key
+integration/scripts/files/elements/fedora-guest/post-install.d/05-ipforwarding
+integration/scripts/files/elements/fedora-guest/post-install.d/62-trove-guest-sudoers
+integration/scripts/files/elements/fedora-guest/post-install.d/90-yum-update
+integration/scripts/files/elements/fedora-mariadb/README.md
+integration/scripts/files/elements/fedora-mariadb/install.d/10-mariadb
+integration/scripts/files/elements/fedora-mariadb/pre-install.d/10-percona-copr
+integration/scripts/files/elements/fedora-mongodb/README.md
+integration/scripts/files/elements/fedora-mongodb/install.d/10-mongodb
+integration/scripts/files/elements/fedora-mongodb/install.d/25-trove-mongo-dep
+integration/scripts/files/elements/fedora-mysql/README.md
+integration/scripts/files/elements/fedora-mysql/install.d/10-mysql
+integration/scripts/files/elements/fedora-mysql/install.d/40-xtrabackup
+integration/scripts/files/elements/fedora-mysql/post-install.d/30-register-mysql-service
+integration/scripts/files/elements/fedora-percona/install.d/05-percona-server
+integration/scripts/files/elements/fedora-percona/install.d/10-mysql
+integration/scripts/files/elements/fedora-postgresql/install.d/10-postgresql
+integration/scripts/files/elements/fedora-redis/README.md
+integration/scripts/files/elements/fedora-redis/install.d/10-redis
+integration/scripts/files/elements/ubuntu-cassandra/install.d/10-cassandra
+integration/scripts/files/elements/ubuntu-couchbase/install.d/10-couchbase
+integration/scripts/files/elements/ubuntu-couchdb/install.d/10-couchdb
+integration/scripts/files/elements/ubuntu-db2/README.md
+integration/scripts/files/elements/ubuntu-db2/extra-data.d/20-copy-db2-pkgs
+integration/scripts/files/elements/ubuntu-db2/install.d/10-db2
+integration/scripts/files/elements/ubuntu-guest/extra-data.d/15-trove-dep
+integration/scripts/files/elements/ubuntu-guest/extra-data.d/62-ssh-key
+integration/scripts/files/elements/ubuntu-guest/install.d/05-base-apps
+integration/scripts/files/elements/ubuntu-guest/install.d/15-trove-dep
+integration/scripts/files/elements/ubuntu-guest/install.d/50-user
+integration/scripts/files/elements/ubuntu-guest/install.d/62-ssh-key
+integration/scripts/files/elements/ubuntu-guest/install.d/98-ssh
+integration/scripts/files/elements/ubuntu-guest/install.d/99-clean-apt
+integration/scripts/files/elements/ubuntu-guest/post-install.d/05-ipforwarding
+integration/scripts/files/elements/ubuntu-guest/post-install.d/10-ntp
+integration/scripts/files/elements/ubuntu-guest/post-install.d/62-trove-guest-sudoers
+integration/scripts/files/elements/ubuntu-guest/post-install.d/90-apt-get-update
+integration/scripts/files/elements/ubuntu-guest/pre-install.d/04-baseline-tools
+integration/scripts/files/elements/ubuntu-mariadb/README.md
+integration/scripts/files/elements/ubuntu-mariadb/pre-install.d/10-percona-apt-key
+integration/scripts/files/elements/ubuntu-mariadb/pre-install.d/20-apparmor-mysql-local
+integration/scripts/files/elements/ubuntu-mongodb/README.md
+integration/scripts/files/elements/ubuntu-mongodb/install.d/10-mongodb-thp
+integration/scripts/files/elements/ubuntu-mongodb/install.d/20-mongodb
+integration/scripts/files/elements/ubuntu-mongodb/install.d/25-trove-mongo-dep
+integration/scripts/files/elements/ubuntu-mongodb/install.d/30-mongodb-conf
+integration/scripts/files/elements/ubuntu-mongodb/install.d/41-mongod-init
+integration/scripts/files/elements/ubuntu-mongodb/install.d/42-mongos-init
+integration/scripts/files/elements/ubuntu-mongodb/pre-install.d/10-mongodb-apt-key
+integration/scripts/files/elements/ubuntu-mysql/README.md
 integration/scripts/files/elements/ubuntu-mysql/pre-install.d/10-percona-apt-key
-integration/scripts/files/elements/ubuntu-xenial-guest/install.d/22-decrease-networking-timeout
+integration/scripts/files/elements/ubuntu-mysql/pre-install.d/20-apparmor-mysql-local
+integration/scripts/files/elements/ubuntu-percona/install.d/30-mysql
+integration/scripts/files/elements/ubuntu-percona/pre-install.d/10-percona-apt-key
+integration/scripts/files/elements/ubuntu-percona/pre-install.d/20-apparmor-mysql-local
+integration/scripts/files/elements/ubuntu-postgresql/install.d/30-postgresql
+integration/scripts/files/elements/ubuntu-postgresql/pre-install.d/10-postgresql-repo
+integration/scripts/files/elements/ubuntu-pxc/install.d/30-mysql
+integration/scripts/files/elements/ubuntu-pxc/pre-install.d/10-percona-apt-key
+integration/scripts/files/elements/ubuntu-pxc/pre-install.d/20-apparmor-mysql-local
+integration/scripts/files/elements/ubuntu-redis/README.md
+integration/scripts/files/elements/ubuntu-redis/install.d/30-redis
+integration/scripts/files/elements/ubuntu-trusty-cassandra/element-deps
+integration/scripts/files/elements/ubuntu-trusty-couchbase/element-deps
+integration/scripts/files/elements/ubuntu-trusty-couchdb/element-deps
+integration/scripts/files/elements/ubuntu-trusty-db2/element-deps
+integration/scripts/files/elements/ubuntu-trusty-guest/element-deps
+integration/scripts/files/elements/ubuntu-trusty-guest/extra-data.d/20-guest-upstart
+integration/scripts/files/elements/ubuntu-trusty-guest/install.d/20-etc
+integration/scripts/files/elements/ubuntu-trusty-guest/pre-install.d/01-trim-pkgs
+integration/scripts/files/elements/ubuntu-trusty-mariadb/element-deps
+integration/scripts/files/elements/ubuntu-trusty-mariadb/install.d/30-mariadb
+integration/scripts/files/elements/ubuntu-trusty-mongodb/element-deps
+integration/scripts/files/elements/ubuntu-trusty-mysql/element-deps
+integration/scripts/files/elements/ubuntu-trusty-mysql/install.d/30-mysql
+integration/scripts/files/elements/ubuntu-trusty-percona/element-deps
+integration/scripts/files/elements/ubuntu-trusty-postgresql/element-deps
+integration/scripts/files/elements/ubuntu-trusty-pxc/element-deps
+integration/scripts/files/elements/ubuntu-trusty-redis/element-deps
+integration/scripts/files/elements/ubuntu-trusty-vertica/element-deps
+integration/scripts/files/elements/ubuntu-vertica/README.md
+integration/scripts/files/elements/ubuntu-vertica/extra-data.d/93-copy-vertica-deb
+integration/scripts/files/elements/ubuntu-vertica/install.d/97-vertica
+integration/scripts/files/elements/ubuntu-xenial-guest/element-deps
+integration/scripts/files/elements/ubuntu-xenial-guest/extra-data.d/20-guest-systemd
+integration/scripts/files/elements/ubuntu-xenial-guest/install.d/20-etc
+integration/scripts/files/elements/ubuntu-xenial-guest/pre-install.d/01-trim-pkgs
+integration/scripts/files/elements/ubuntu-xenial-mariadb/element-deps
+integration/scripts/files/elements/ubuntu-xenial-mariadb/install.d/30-mariadb
+integration/scripts/files/elements/ubuntu-xenial-mysql/element-deps
+integration/scripts/files/elements/ubuntu-xenial-mysql/install.d/30-mysql
+integration/scripts/files/elements/ubuntu-xenial-postgresql/element-deps
+integration/scripts/files/elements/ubuntu-xenial-postgresql/install.d/31-fix-init-script
+integration/scripts/files/elements/ubuntu-xenial-pxc/element-deps
+integration/scripts/files/elements/ubuntu-xenial-pxc/install.d/31-fix-my-cnf
+integration/scripts/files/elements/ubuntu-xenial-redis/element-deps
+integration/scripts/files/elements/ubuntu-xenial-redis/install.d/31-fix-init-file
 integration/scripts/files/keys/authorized_keys
 integration/scripts/files/keys/id_rsa
 integration/scripts/files/keys/id_rsa.pub
 integration/scripts/files/requirements/fedora-requirements.txt
 integration/scripts/files/requirements/ubuntu-requirements.txt
+integration/scripts/local.conf.d/ceilometer_cinder.conf.rc
+integration/scripts/local.conf.d/ceilometer_nova.conf.rc
+integration/scripts/local.conf.d/ceilometer_services.conf.rc
+integration/scripts/local.conf.d/sample.rc
+integration/scripts/local.conf.d/trove_services.conf.rc
+integration/scripts/local.conf.d/use_kvm.rc
+integration/scripts/local.conf.d/use_uuid_token.rc
+integration/scripts/local.conf.d/using_vagrant.rc
+integration/tests/examples/README
+integration/tests/examples/example_gen.sh
+integration/tests/examples/gendoc.sh
+integration/tests/examples/local.conf
+integration/tests/examples/setup.py
+integration/tests/examples/tox.ini
+integration/tests/examples/examples/client.py
+integration/tests/examples/examples/example_generation.py
+integration/tests/examples/examples/local.conf
 integration/tests/integration/core.test.conf
 integration/tests/integration/int_tests.py
 integration/tests/integration/localhost.test.conf
+integration/tests/integration/run_local.sh
+integration/tests/integration/setup.py
+integration/tests/integration/tox.ini
+integration/tests/integration/tests/README
 integration/tests/integration/tests/__init__.py
 integration/tests/integration/tests/colorizer.py
 integration/tests/integration/tests/initialize.py
+integration/tests/integration/tests/api/__init__.py
+integration/tests/integration/tests/api/delete_all.py
+integration/tests/integration/tests/api/instances_pagination.py
+integration/tests/integration/tests/api/instances_quotas.py
+integration/tests/integration/tests/api/instances_states.py
+integration/tests/integration/tests/dns/__init__.py
+integration/tests/integration/tests/dns/check_domain.py
+integration/tests/integration/tests/dns/concurrency.py
+integration/tests/integration/tests/dns/conversion.py
+integration/tests/integration/tests/dns/dns.py
+integration/tests/integration/tests/smoke/__init__.py
+integration/tests/integration/tests/smoke/instance.py
 integration/tests/integration/tests/util/__init__.py
 integration/tests/integration/tests/util/report.py
 integration/tests/integration/tests/util/rpc.py
 integration/tests/integration/tests/util/services.py
-playbooks/trove-devstack-base.yaml
-playbooks/image-build/docker-registry.yaml
-playbooks/image-build/post.yaml
-playbooks/image-build/run.yaml
+integration/tests/integration/tests/volumes/__init__.py
+integration/tests/integration/tests/volumes/driver.py
 releasenotes/notes/.placeholder
 releasenotes/notes/add-cassandra-log-retrieval-a295f3d0d4c56804.yaml
-releasenotes/notes/add-cinder-az-option-d4ff1968e6064ff2.yaml
 releasenotes/notes/add-cors-support-fe3ecbecb68f7efd.yaml
-releasenotes/notes/add-designate-v2-dns-driver-8d1be56ab2c71b83.yaml
 releasenotes/notes/add-icmp-flag-58937cce344e77d9.yaml
-releasenotes/notes/add-instance-detailed-list-e712dccf6c9091c0.yaml
 releasenotes/notes/add-max-prep-stmts-ac1056e127de7609.yaml
-releasenotes/notes/add-network-isolation-support-640f7105eb90651a.yaml
 releasenotes/notes/add-new-relic-license-driver-0f314edabb7561c4.yaml
-releasenotes/notes/add-support-create-instance-with-rootdisk-cinder.yaml
 releasenotes/notes/alter-user-portable-021f4b792e2c129b.yaml
 releasenotes/notes/associate-volume-type-datastore-97defb9279b61c1f.yaml
-releasenotes/notes/avoid-diverged-slave-when-migrating-mariadb-master-37e2429a1ea75913.yaml
 releasenotes/notes/cassandra-backup-and-restore-00de234de67ea5ee.yaml
 releasenotes/notes/cassandra-configuration-groups-e6bcf4014a79f14f.yaml
 releasenotes/notes/cassandra-user-functions-041abfa4f4baa591.yaml
 releasenotes/notes/cluster-configuration-groups-37f7de9e5a343165.yaml
-releasenotes/notes/cluster-notifications-fd205f5f0148b052.yaml
 releasenotes/notes/cluster-volume-type-901329a3b3667cb4.yaml
 releasenotes/notes/cluster_list_show_all_ips-3547635440.yaml
 releasenotes/notes/cluster_restart-bb5abb7372131ee0.yaml
 releasenotes/notes/couchdb-backup-restore-0cc3324c3088f947.yaml
 releasenotes/notes/couchdb-user-db-functions-fa41ac47fce095cb.yaml
 releasenotes/notes/datastore-manager-refactor-5aeac4e6bfa6e07b.yaml
 releasenotes/notes/db2-backup-restore-96ab214cddd15181.yaml
 releasenotes/notes/db2-configuration-groups-ca2164be741d35f9.yaml
 releasenotes/notes/db2-online-backup-restore-3783afe752562e70.yaml
 releasenotes/notes/dbaas-ceilometer-notifications-5a623d0d6520be72.yaml
-releasenotes/notes/deprecate-default_neutron_networks-84cd00224d6b7bc1.yaml
 releasenotes/notes/deprecate-long-query-time-b85af24772e2e7cb.yaml
 releasenotes/notes/disply_module_bools_properly-571cca9a87f28339.yaml
-releasenotes/notes/drop-py-2-7-010fe6df0c10352d.yaml
 releasenotes/notes/drop-python-26-support-39dff0c5636edc74.yaml
-releasenotes/notes/drop-python-3-6-and-3-7-51489f1a80c2e5e5.yaml
 releasenotes/notes/fix-apply-configuration-on-prepare-4cff827b7f3c4d33.yaml
 releasenotes/notes/fix-bad-swift-endpoint-in-guestlog-05f7483509dacbbf.yaml
-releasenotes/notes/fix-cluster-creation-error-c1fd7674e71dd6a2.yaml
 releasenotes/notes/fix-cluster-show-346798b3e3.yaml
 releasenotes/notes/fix-cluster-type-error-71cd846897dfd32e.yaml
-releasenotes/notes/fix-clustering-database-status-check-61fa0f49dd786c72.yaml
 releasenotes/notes/fix-deprecated-SafeConfigParse-ca3fd3e9f52a8cc8.yaml
-releasenotes/notes/fix-docker-start-failed-160e79b6e5494edd.yaml
-releasenotes/notes/fix-galera_common-cluster-shrink-e2c80913423772dd.yaml
 releasenotes/notes/fix-gtid-parsing-9f60ad6e9e8f173f.yaml
-releasenotes/notes/fix-guest-agent-config-missing.yaml
-releasenotes/notes/fix-illegal-value-be1acadc8c54c224.yaml
-releasenotes/notes/fix-ipv6-route-exists-b5f07569270b6066.yaml
-releasenotes/notes/fix-missing-request-id-in-log.yaml
 releasenotes/notes/fix-module-apply-after-remove-97c84c30fb320a46.yaml
 releasenotes/notes/fix-mongo-cluster-grow-8fa4788af0ce5309.yaml
 releasenotes/notes/fix-mysql-replication-bf2b131994a5a772.yaml
 releasenotes/notes/fix-mysql-replication-ca0928069c0bfab8.yaml
 releasenotes/notes/fix-postgres-pg-rewind-6eef0afb568439ce.yaml
-releasenotes/notes/fix-postgresql-database-create-failed-abd4f99cc7dde44c.yaml
-releasenotes/notes/fix-postgresql-socket-path-2028103b91543e4c.yaml
-releasenotes/notes/fix-postgress-create-database-75fbe03e3b4e296d.yaml
-releasenotes/notes/fix-prepare-mariadb-after-restore-472112bbcdcecdb9.yaml
 releasenotes/notes/fix-redis-configuration-f0543ede84f8aac3.yaml
-releasenotes/notes/fix-server-restart-bug-9746b2c2ed00a45a.yaml
-releasenotes/notes/fix-swift-connect-over-tls-c4e62213a8d38fe2.yaml
 releasenotes/notes/fix-trove-events-8ce54233504065cf.yaml
 releasenotes/notes/fix_mod_inst_cmd-3a46c7233e3.yaml
 releasenotes/notes/fix_module_apply-042fc6e61f721540.yaml
 releasenotes/notes/fix_module_driver_logging-666601f411db784a.yaml
-releasenotes/notes/fix_mysql_missing_configdir-fa9e2e647dd46846.yaml
-releasenotes/notes/fix_mysql_permission_problem-2698e6a4dcc6e444.yaml
 releasenotes/notes/fix_notification_err_msgs-e52771108633c9cf.yaml
 releasenotes/notes/fixes-mariadb-config-groups-b5fa4f44a8ed7b85.yaml
 releasenotes/notes/flavor-list-disk-6213c3760e374441.yaml
 releasenotes/notes/flavor-list-ephemeral-edf2dc35d5c247b3.yaml
 releasenotes/notes/flavor-list-vcpu-817b0f5715820377.yaml
 releasenotes/notes/force_delete-c2b06dbead554726.yaml
 releasenotes/notes/grow-cluster-nic-az-0e0fe4083666c300.yaml
 releasenotes/notes/guest-call-timeout-2781a57ca8feb89a.yaml
-releasenotes/notes/healthcheck-middleware-670a667bfb245123.yaml
 releasenotes/notes/implement-cassandra-clustering-9f7bc3ae6817c19e.yaml
 releasenotes/notes/implement-cassandra-root-b0870d23dbf1a848.yaml
 releasenotes/notes/implement-mariadb-clustering-088ac2f6012689fb.yaml
-releasenotes/notes/implement-redis-root-347b5ee0107debb5.yaml
 releasenotes/notes/improve-mysql-user-list-pagination-71457d934500f817.yaml
 releasenotes/notes/incremental_backup-1910ded0fc3474a3.yaml
 releasenotes/notes/instance-show-comp-vol-id-964db9f52a5ac9c1.yaml
 releasenotes/notes/instance-upgrade-7d464f85e025d729.yaml
 releasenotes/notes/locality-support-for-clusters-78bb74145d867df2.yaml
 releasenotes/notes/locality-support-for-replication-01d9b05d27b92d82.yaml
 releasenotes/notes/make-password-length-datastore-specific-7cdb1bfeab6e6227.yaml
 releasenotes/notes/mariadb-gtid-replication-1ea972bcfe909773.yaml
 releasenotes/notes/mask-configuration-passwords-317ff6d2415b2ca1.yaml
 releasenotes/notes/module-management-66d3979cc45ed440.yaml
 releasenotes/notes/module-ordering-92b6445a8ac3a3bf.yaml
 releasenotes/notes/module-support-for-clusters-87b41dd7648275bf.yaml
 releasenotes/notes/module_reapply-342c0965a4318d4e.yaml
 releasenotes/notes/module_reapply_update_values-1fb88dc58701368d.yaml
-releasenotes/notes/mongo-cluster-create-use-extended-perperties-ced87fde31c6c110.yaml
 releasenotes/notes/mongo-cluster-grow-use-az-and-nic-values-207b041113e7b4fb.yaml
 releasenotes/notes/mountpoint-detection-096734f0097eb75a.yaml
 releasenotes/notes/multi-region-cd8da560bfe00de5.yaml
 releasenotes/notes/mysql-config-preserve-types-77b970162bf6df08.yaml
 releasenotes/notes/mysql-root-fix-35079552e25170ca.yaml
 releasenotes/notes/mysql-user-list-pagination-9496c401c180f605.yaml
-releasenotes/notes/mysql8-6a81a8498ee2c229.yaml
 releasenotes/notes/percona-2.3-support-2eab8f12167e44bc.yaml
 releasenotes/notes/persist-error-message-fb69ddf885bcde84.yaml
 releasenotes/notes/pgsql-incremental-backup-acb4421f7de3ac09.yaml
 releasenotes/notes/pgsql-streaming-replication-f4df7e4047988b21.yaml
 releasenotes/notes/post-upgrade-fixes-828811607826d433.yaml
 releasenotes/notes/postgres-user-list-race-46624dc9e4420e02.yaml
 releasenotes/notes/postgresql-use-proper-guestagent-models-7ba601c7b4c001d6.yaml
 releasenotes/notes/pxc-cluster-root-enable-30c366e3b5bcda51.yaml
 releasenotes/notes/pxc-grow-shrink-0b1ee689cbc77743.yaml
 releasenotes/notes/quota-management-3792cbc25ebe16bb.yaml
-releasenotes/notes/redis-upgrade-63769ddb1b546cb9.yaml
-releasenotes/notes/remove-bionic-support-85f506117e566813.yaml
-releasenotes/notes/remove-global-default_password_length-a30b75abb0c5091f.yaml
-releasenotes/notes/remove-idle-timeout-e4a5db0d5ee524d7.yaml
 releasenotes/notes/remove-override-templates-85429da7f66e006a.yaml
-releasenotes/notes/remove-support-of-use-nova-server-volume-2a334f57d8213810.yaml
-releasenotes/notes/return-http-204-for-disable-root-api-a818fc41fd6e75eb.yaml
 releasenotes/notes/reuse-cassandra-connections-092cf2a762a2e796.yaml
 releasenotes/notes/secure-mongodb-instances-1e6d7df3febab8f4.yaml
-releasenotes/notes/separate-backup-docker-image-884165.yaml
 releasenotes/notes/slo-backups-3c35135316f837e1.yaml
-releasenotes/notes/support-backup-strategy.yaml
-releasenotes/notes/support-new-mariadb-versions-in-backup-docker-image-7e3106446dcb4dd0.yaml
-releasenotes/notes/support-nova-keypair-a2cdb2da5c1511e9.yaml
-releasenotes/notes/support-online-resize.yaml
-releasenotes/notes/support-subnet-and-ip-address.yaml
-releasenotes/notes/train-01-backup-filtering-90ff6deac7b411e9.yaml
-releasenotes/notes/train-02-management-security-group.yaml
-releasenotes/notes/train-03-public-trove-instance-8ec456bed46411e9.yaml
-releasenotes/notes/train-04-public-trove-images-127300c0df6c11e9.yaml
-releasenotes/notes/trove-status-upgrade-check-framework-b9d3d3e2463ec26d.yaml
 releasenotes/notes/update-myisam-recover-opt-232b9d680bc362bf.yaml
 releasenotes/notes/use-oslo-policy-bbd1b911e6487c36.yaml
 releasenotes/notes/use-osprofiler-options-58263c311617b127.yaml
-releasenotes/notes/ussuri-add-ip-addresses-for-instance.yaml
-releasenotes/notes/ussuri-add-service-status-updated.yaml
-releasenotes/notes/ussuri-admin-clients-a14514a835ae11ea.yaml
-releasenotes/notes/ussuri-database-instance-healthy.yaml
-releasenotes/notes/ussuri-delete-datastoredad784e2345711ea.yaml
-releasenotes/notes/ussuri-fix-delete-datastore-version.yaml
-releasenotes/notes/ussuri-service-credential-config.yaml
-releasenotes/notes/ussuri-support-xfs-disk-format.yaml
 releasenotes/notes/vertica-configuration-groups-710c892c1e3d6a90.yaml
 releasenotes/notes/vertica-grow-shrink-cluster-e32d48f5b2e1bfab.yaml
 releasenotes/notes/vertica-load-via-curl-call-4d47c4e0b1b53471.yaml
-releasenotes/notes/victoria-check-subnet-router-association.yaml
-releasenotes/notes/victoria-database-containerization.yaml
-releasenotes/notes/victoria-expired-database-status.yaml
-releasenotes/notes/victoria-list-project-backups.yaml
-releasenotes/notes/victoria-rebuild-instance.yaml
-releasenotes/notes/victoria-resize-vollume-for-replication.yaml
-releasenotes/notes/victoria-show-replicas-for-instance-list.yaml
-releasenotes/notes/victoria-show-update-instance-access.yaml
-releasenotes/notes/victoria-trove-manager-remove-config-params.yaml
-releasenotes/notes/wallaby-add-ram-quota-d8e64d0385b1429f.yaml
-releasenotes/notes/wallaby-datastore-version-image-tags.yaml
-releasenotes/notes/wallaby-deprecate-json-formatted-policy-file-21c88ff2ad490a2e.yaml
-releasenotes/notes/wallaby-docker-registry.yaml
-releasenotes/notes/wallaby-fix-deleting-volume.yaml
-releasenotes/notes/wallaby-fix-race-condition-create-delete.yaml
-releasenotes/notes/wallaby-mysql-8.yaml
-releasenotes/notes/wallaby-operating-status.yaml
-releasenotes/notes/wallaby-restore-backup.yaml
-releasenotes/notes/xena-add-iptables-persistent-package.yaml
-releasenotes/notes/xena-allow-project-show-resource-quota.yaml
-releasenotes/notes/xena-container-bridge-network.yaml
-releasenotes/notes/xena-fix-backup-custom-image-registry.yaml
-releasenotes/notes/xena-fix-postgresql-wal-archive-size.yaml
-releasenotes/notes/xena-fix-resize-instance.yaml
-releasenotes/notes/xena-show-network.yaml
-releasenotes/source/2023.1.rst
-releasenotes/source/2023.2.rst
 releasenotes/source/conf.py
 releasenotes/source/index.rst
 releasenotes/source/mitaka.rst
 releasenotes/source/newton.rst
 releasenotes/source/ocata.rst
-releasenotes/source/pike.rst
-releasenotes/source/queens.rst
-releasenotes/source/rocky.rst
-releasenotes/source/stein.rst
-releasenotes/source/train.rst
 releasenotes/source/unreleased.rst
-releasenotes/source/ussuri.rst
-releasenotes/source/victoria.rst
-releasenotes/source/wallaby.rst
-releasenotes/source/xena.rst
-releasenotes/source/yoga.rst
-releasenotes/source/zed.rst
 releasenotes/source/_static/.placeholder
 releasenotes/source/_templates/.placeholder
-releasenotes/source/locale/en_GB/LC_MESSAGES/releasenotes.po
-releasenotes/templates/feature.yml
-releasenotes/templates/fix.yml
-roles/trove-devstack/README
-roles/trove-devstack/defaults/main.yml
-roles/trove-devstack/tasks/main.yml
 tools/install_venv.py
 tools/start-fake-mode.sh
 tools/stop-fake-mode.sh
-tools/trove-config-generator.conf
-tools/trove-policy-generator.conf
+tools/test-setup.sh
 tools/trove-pylint.README
 tools/trove-pylint.config
 tools/trove-pylint.py
 tools/with_venv.sh
 trove/README
 trove/__init__.py
 trove/rpc.py
@@ -571,56 +644,50 @@
 trove/cluster/__init__.py
 trove/cluster/models.py
 trove/cluster/service.py
 trove/cluster/tasks.py
 trove/cluster/views.py
 trove/cmd/__init__.py
 trove/cmd/api.py
-trove/cmd/app_wsgi.py
+trove/cmd/app.wsgi
 trove/cmd/common.py
 trove/cmd/conductor.py
 trove/cmd/fakemode.py
 trove/cmd/guest.py
 trove/cmd/manage.py
-trove/cmd/network_driver.py
-trove/cmd/status.py
 trove/cmd/taskmanager.py
 trove/common/__init__.py
 trove/common/api.py
 trove/common/apischema.py
 trove/common/auth.py
 trove/common/base_exception.py
 trove/common/base_wsgi.py
-trove/common/cache.py
 trove/common/cfg.py
-trove/common/clients.py
-trove/common/clients_admin.py
 trove/common/configurations.py
-trove/common/constants.py
 trove/common/context.py
 trove/common/crypto_utils.py
 trove/common/debug_utils.py
 trove/common/exception.py
 trove/common/extensions.py
-trove/common/glance.py
+trove/common/glance_remote.py
 trove/common/i18n.py
+trove/common/instance.py
 trove/common/limits.py
 trove/common/local.py
 trove/common/models.py
-trove/common/neutron.py
 trove/common/notification.py
 trove/common/pagination.py
 trove/common/pastedeploy.py
 trove/common/policy.py
 trove/common/profile.py
-trove/common/schemata.py
+trove/common/remote.py
 trove/common/serializable_notification.py
 trove/common/server_group.py
+trove/common/single_tenant_remote.py
 trove/common/stream_codecs.py
-trove/common/swift.py
 trove/common/template.py
 trove/common/timeutils.py
 trove/common/trove_remote.py
 trove/common/utils.py
 trove/common/views.py
 trove/common/wsgi.py
 trove/common/xmlutils.py
@@ -633,38 +700,24 @@
 trove/common/db/mongodb/__init__.py
 trove/common/db/mongodb/models.py
 trove/common/db/mysql/__init__.py
 trove/common/db/mysql/data.py
 trove/common/db/mysql/models.py
 trove/common/db/postgresql/__init__.py
 trove/common/db/postgresql/models.py
-trove/common/db/redis/__init__.py
-trove/common/db/redis/models.py
-trove/common/policies/__init__.py
-trove/common/policies/backups.py
-trove/common/policies/base.py
-trove/common/policies/clusters.py
-trove/common/policies/configuration_parameters.py
-trove/common/policies/configurations.py
-trove/common/policies/databases.py
-trove/common/policies/datastores.py
-trove/common/policies/flavors.py
-trove/common/policies/instances.py
-trove/common/policies/limits.py
-trove/common/policies/modules.py
-trove/common/policies/root.py
-trove/common/policies/user_access.py
-trove/common/policies/users.py
 trove/common/rpc/__init__.py
 trove/common/rpc/conductor_guest_serializer.py
 trove/common/rpc/conductor_host_serializer.py
 trove/common/rpc/secure_serializer.py
 trove/common/rpc/serializer.py
 trove/common/rpc/service.py
 trove/common/rpc/version.py
+trove/common/schemas/atom-link.rng
+trove/common/schemas/atom.rng
+trove/common/schemas/v1.1/limits.rng
 trove/common/strategies/__init__.py
 trove/common/strategies/strategy.py
 trove/common/strategies/cluster/__init__.py
 trove/common/strategies/cluster/base.py
 trove/common/strategies/cluster/strategy.py
 trove/common/strategies/cluster/experimental/__init__.py
 trove/common/strategies/cluster/experimental/cassandra/__init__.py
@@ -683,14 +736,18 @@
 trove/common/strategies/cluster/experimental/redis/api.py
 trove/common/strategies/cluster/experimental/redis/guestagent.py
 trove/common/strategies/cluster/experimental/redis/taskmanager.py
 trove/common/strategies/cluster/experimental/vertica/__init__.py
 trove/common/strategies/cluster/experimental/vertica/api.py
 trove/common/strategies/cluster/experimental/vertica/guestagent.py
 trove/common/strategies/cluster/experimental/vertica/taskmanager.py
+trove/common/strategies/storage/__init__.py
+trove/common/strategies/storage/base.py
+trove/common/strategies/storage/swift.py
+trove/common/strategies/storage/experimental/__init__.py
 trove/conductor/__init__.py
 trove/conductor/api.py
 trove/conductor/manager.py
 trove/conductor/models.py
 trove/configuration/__init__.py
 trove/configuration/models.py
 trove/configuration/service.py
@@ -750,28 +807,28 @@
 trove/db/sqlalchemy/migrate_repo/versions/036_add_datastore_version_metadata.py
 trove/db/sqlalchemy/migrate_repo/versions/037_modules.py
 trove/db/sqlalchemy/migrate_repo/versions/038_instance_faults.py
 trove/db/sqlalchemy/migrate_repo/versions/039_region.py
 trove/db/sqlalchemy/migrate_repo/versions/040_module_priority.py
 trove/db/sqlalchemy/migrate_repo/versions/041_instance_keys.py
 trove/db/sqlalchemy/migrate_repo/versions/042_add_cluster_configuration_id.py
-trove/db/sqlalchemy/migrate_repo/versions/043_instance_ds_version_nullable.py
-trove/db/sqlalchemy/migrate_repo/versions/044_remove_datastore_configuration_parameters_deleted.py
-trove/db/sqlalchemy/migrate_repo/versions/045_add_backup_strategy.py
-trove/db/sqlalchemy/migrate_repo/versions/046_add_access_to_instance.py
-trove/db/sqlalchemy/migrate_repo/versions/047_image_tag_in_datastore_version.py
-trove/db/sqlalchemy/migrate_repo/versions/048_add_version_to_datastore_version.py
 trove/db/sqlalchemy/migrate_repo/versions/__init__.py
 trove/dns/__init__.py
 trove/dns/driver.py
 trove/dns/manager.py
 trove/dns/models.py
 trove/dns/designate/__init__.py
 trove/dns/designate/driver.py
 trove/extensions/__init__.py
+trove/extensions/account/__init__.py
+trove/extensions/account/models.py
+trove/extensions/account/service.py
+trove/extensions/account/views.py
+trove/extensions/cassandra/__init__.py
+trove/extensions/cassandra/service.py
 trove/extensions/common/__init__.py
 trove/extensions/common/models.py
 trove/extensions/common/service.py
 trove/extensions/common/views.py
 trove/extensions/mgmt/__init__.py
 trove/extensions/mgmt/clusters/__init__.py
 trove/extensions/mgmt/clusters/models.py
@@ -779,105 +836,183 @@
 trove/extensions/mgmt/clusters/views.py
 trove/extensions/mgmt/configuration/__init__.py
 trove/extensions/mgmt/configuration/service.py
 trove/extensions/mgmt/configuration/views.py
 trove/extensions/mgmt/datastores/__init__.py
 trove/extensions/mgmt/datastores/service.py
 trove/extensions/mgmt/datastores/views.py
+trove/extensions/mgmt/host/__init__.py
+trove/extensions/mgmt/host/models.py
+trove/extensions/mgmt/host/service.py
+trove/extensions/mgmt/host/views.py
+trove/extensions/mgmt/host/instance/__init__.py
+trove/extensions/mgmt/host/instance/service.py
 trove/extensions/mgmt/instances/__init__.py
 trove/extensions/mgmt/instances/models.py
 trove/extensions/mgmt/instances/service.py
 trove/extensions/mgmt/instances/views.py
 trove/extensions/mgmt/quota/__init__.py
 trove/extensions/mgmt/quota/service.py
 trove/extensions/mgmt/quota/views.py
 trove/extensions/mgmt/upgrade/__init__.py
 trove/extensions/mgmt/upgrade/models.py
 trove/extensions/mgmt/upgrade/service.py
+trove/extensions/mgmt/volume/__init__.py
+trove/extensions/mgmt/volume/models.py
+trove/extensions/mgmt/volume/service.py
+trove/extensions/mgmt/volume/views.py
 trove/extensions/mongodb/__init__.py
 trove/extensions/mongodb/service.py
 trove/extensions/mysql/__init__.py
 trove/extensions/mysql/common.py
 trove/extensions/mysql/models.py
 trove/extensions/mysql/service.py
 trove/extensions/mysql/views.py
+trove/extensions/postgresql/__init__.py
+trove/extensions/postgresql/service.py
 trove/extensions/pxc/__init__.py
 trove/extensions/pxc/service.py
-trove/extensions/redis/__init__.py
-trove/extensions/redis/models.py
-trove/extensions/redis/service.py
-trove/extensions/redis/views.py
 trove/extensions/routes/__init__.py
+trove/extensions/routes/account.py
 trove/extensions/routes/mgmt.py
 trove/extensions/routes/mysql.py
+trove/extensions/routes/security_group.py
 trove/extensions/security_group/__init__.py
 trove/extensions/security_group/models.py
+trove/extensions/security_group/service.py
+trove/extensions/security_group/views.py
 trove/extensions/vertica/__init__.py
 trove/extensions/vertica/service.py
 trove/flavor/__init__.py
 trove/flavor/models.py
 trove/flavor/service.py
 trove/flavor/views.py
 trove/guestagent/__init__.py
 trove/guestagent/api.py
 trove/guestagent/dbaas.py
 trove/guestagent/guest_log.py
 trove/guestagent/models.py
 trove/guestagent/pkg.py
 trove/guestagent/service.py
 trove/guestagent/volume.py
+trove/guestagent/backup/__init__.py
+trove/guestagent/backup/backupagent.py
 trove/guestagent/common/__init__.py
 trove/guestagent/common/configuration.py
 trove/guestagent/common/guestagent_utils.py
 trove/guestagent/common/operating_system.py
 trove/guestagent/common/sql_query.py
 trove/guestagent/datastore/__init__.py
 trove/guestagent/datastore/manager.py
 trove/guestagent/datastore/service.py
-trove/guestagent/datastore/mariadb/__init__.py
-trove/guestagent/datastore/mariadb/manager.py
-trove/guestagent/datastore/mariadb/service.py
+trove/guestagent/datastore/experimental/__init__.py
+trove/guestagent/datastore/experimental/cassandra/__init__.py
+trove/guestagent/datastore/experimental/cassandra/manager.py
+trove/guestagent/datastore/experimental/cassandra/service.py
+trove/guestagent/datastore/experimental/couchbase/__init__.py
+trove/guestagent/datastore/experimental/couchbase/manager.py
+trove/guestagent/datastore/experimental/couchbase/service.py
+trove/guestagent/datastore/experimental/couchbase/system.py
+trove/guestagent/datastore/experimental/couchdb/__init__.py
+trove/guestagent/datastore/experimental/couchdb/manager.py
+trove/guestagent/datastore/experimental/couchdb/service.py
+trove/guestagent/datastore/experimental/couchdb/system.py
+trove/guestagent/datastore/experimental/db2/__init__.py
+trove/guestagent/datastore/experimental/db2/manager.py
+trove/guestagent/datastore/experimental/db2/service.py
+trove/guestagent/datastore/experimental/db2/system.py
+trove/guestagent/datastore/experimental/mariadb/__init__.py
+trove/guestagent/datastore/experimental/mariadb/manager.py
+trove/guestagent/datastore/experimental/mariadb/service.py
+trove/guestagent/datastore/experimental/mongodb/__init__.py
+trove/guestagent/datastore/experimental/mongodb/manager.py
+trove/guestagent/datastore/experimental/mongodb/service.py
+trove/guestagent/datastore/experimental/mongodb/system.py
+trove/guestagent/datastore/experimental/percona/__init__.py
+trove/guestagent/datastore/experimental/percona/manager.py
+trove/guestagent/datastore/experimental/percona/service.py
+trove/guestagent/datastore/experimental/postgresql/__init__.py
+trove/guestagent/datastore/experimental/postgresql/manager.py
+trove/guestagent/datastore/experimental/postgresql/pgsql_query.py
+trove/guestagent/datastore/experimental/postgresql/service.py
+trove/guestagent/datastore/experimental/pxc/__init__.py
+trove/guestagent/datastore/experimental/pxc/manager.py
+trove/guestagent/datastore/experimental/pxc/service.py
+trove/guestagent/datastore/experimental/redis/__init__.py
+trove/guestagent/datastore/experimental/redis/manager.py
+trove/guestagent/datastore/experimental/redis/service.py
+trove/guestagent/datastore/experimental/redis/system.py
+trove/guestagent/datastore/experimental/vertica/__init__.py
+trove/guestagent/datastore/experimental/vertica/manager.py
+trove/guestagent/datastore/experimental/vertica/service.py
+trove/guestagent/datastore/experimental/vertica/system.py
+trove/guestagent/datastore/galera_common/__init__.py
+trove/guestagent/datastore/galera_common/manager.py
+trove/guestagent/datastore/galera_common/service.py
 trove/guestagent/datastore/mysql/__init__.py
 trove/guestagent/datastore/mysql/manager.py
 trove/guestagent/datastore/mysql/service.py
 trove/guestagent/datastore/mysql_common/__init__.py
 trove/guestagent/datastore/mysql_common/manager.py
 trove/guestagent/datastore/mysql_common/service.py
-trove/guestagent/datastore/postgres/__init__.py
-trove/guestagent/datastore/postgres/manager.py
-trove/guestagent/datastore/postgres/query.py
-trove/guestagent/datastore/postgres/service.py
+trove/guestagent/datastore/technical-preview/__init__.py
 trove/guestagent/module/__init__.py
 trove/guestagent/module/driver_manager.py
 trove/guestagent/module/module_manager.py
 trove/guestagent/module/drivers/__init__.py
 trove/guestagent/module/drivers/module_driver.py
 trove/guestagent/module/drivers/new_relic_license_driver.py
 trove/guestagent/module/drivers/ping_driver.py
 trove/guestagent/strategies/__init__.py
+trove/guestagent/strategies/backup/__init__.py
+trove/guestagent/strategies/backup/base.py
+trove/guestagent/strategies/backup/mysql_impl.py
+trove/guestagent/strategies/backup/experimental/__init__.py
+trove/guestagent/strategies/backup/experimental/cassandra_impl.py
+trove/guestagent/strategies/backup/experimental/couchbase_impl.py
+trove/guestagent/strategies/backup/experimental/couchdb_impl.py
+trove/guestagent/strategies/backup/experimental/db2_impl.py
+trove/guestagent/strategies/backup/experimental/mariadb_impl.py
+trove/guestagent/strategies/backup/experimental/mongo_impl.py
+trove/guestagent/strategies/backup/experimental/postgresql_impl.py
+trove/guestagent/strategies/backup/experimental/redis_impl.py
 trove/guestagent/strategies/replication/__init__.py
 trove/guestagent/strategies/replication/base.py
-trove/guestagent/strategies/replication/mariadb_gtid.py
 trove/guestagent/strategies/replication/mysql_base.py
+trove/guestagent/strategies/replication/mysql_binlog.py
 trove/guestagent/strategies/replication/mysql_gtid.py
-trove/guestagent/strategies/replication/postgresql.py
-trove/guestagent/utils/__init__.py
-trove/guestagent/utils/docker.py
-trove/guestagent/utils/mysql.py
+trove/guestagent/strategies/replication/experimental/__init__.py
+trove/guestagent/strategies/replication/experimental/mariadb_gtid.py
+trove/guestagent/strategies/replication/experimental/postgresql_impl.py
+trove/guestagent/strategies/replication/experimental/redis_sync.py
+trove/guestagent/strategies/restore/__init__.py
+trove/guestagent/strategies/restore/base.py
+trove/guestagent/strategies/restore/mysql_impl.py
+trove/guestagent/strategies/restore/experimental/__init__.py
+trove/guestagent/strategies/restore/experimental/cassandra_impl.py
+trove/guestagent/strategies/restore/experimental/couchbase_impl.py
+trove/guestagent/strategies/restore/experimental/couchdb_impl.py
+trove/guestagent/strategies/restore/experimental/db2_impl.py
+trove/guestagent/strategies/restore/experimental/mariadb_impl.py
+trove/guestagent/strategies/restore/experimental/mongo_impl.py
+trove/guestagent/strategies/restore/experimental/postgresql_impl.py
+trove/guestagent/strategies/restore/experimental/redis_impl.py
 trove/hacking/__init__.py
-trove/hacking/checks.py
+trove/hacking/translation_checks.py
 trove/instance/__init__.py
 trove/instance/models.py
 trove/instance/service.py
-trove/instance/service_status.py
 trove/instance/tasks.py
 trove/instance/views.py
 trove/limits/__init__.py
 trove/limits/service.py
 trove/limits/views.py
+trove/locale/fr/LC_MESSAGES/trove-log-error.po
+trove/locale/fr/LC_MESSAGES/trove-log-info.po
+trove/locale/fr/LC_MESSAGES/trove-log-warning.po
 trove/module/__init__.py
 trove/module/models.py
 trove/module/service.py
 trove/module/views.py
 trove/network/__init__.py
 trove/network/base.py
 trove/network/neutron.py
@@ -938,39 +1073,51 @@
 trove/tests/int_tests.py
 trove/tests/root_logger.py
 trove/tests/api/__init__.py
 trove/tests/api/backups.py
 trove/tests/api/configurations.py
 trove/tests/api/databases.py
 trove/tests/api/datastores.py
+trove/tests/api/flavors.py
+trove/tests/api/header.py
 trove/tests/api/instances.py
 trove/tests/api/instances_actions.py
 trove/tests/api/instances_delete.py
+trove/tests/api/instances_mysql_down.py
 trove/tests/api/instances_resize.py
 trove/tests/api/limits.py
 trove/tests/api/replication.py
 trove/tests/api/root.py
+trove/tests/api/root_on_create.py
 trove/tests/api/user_access.py
 trove/tests/api/users.py
 trove/tests/api/versions.py
 trove/tests/api/mgmt/__init__.py
+trove/tests/api/mgmt/accounts.py
+trove/tests/api/mgmt/admin_required.py
 trove/tests/api/mgmt/configurations.py
 trove/tests/api/mgmt/datastore_versions.py
+trove/tests/api/mgmt/hosts.py
+trove/tests/api/mgmt/instances.py
 trove/tests/api/mgmt/instances_actions.py
+trove/tests/api/mgmt/malformed_json.py
 trove/tests/api/mgmt/quotas.py
+trove/tests/api/mgmt/storage.py
 trove/tests/db/__init__.py
 trove/tests/db/migrations.py
+trove/tests/examples/__init__.py
+trove/tests/examples/client.py
+trove/tests/examples/snippets.py
 trove/tests/fakes/__init__.py
 trove/tests/fakes/common.py
 trove/tests/fakes/conf.py
 trove/tests/fakes/dns.py
 trove/tests/fakes/guestagent.py
 trove/tests/fakes/keystone.py
 trove/tests/fakes/limits.py
-trove/tests/fakes/neutron.py
 trove/tests/fakes/nova.py
 trove/tests/fakes/swift.py
 trove/tests/fakes/taskmanager.py
 trove/tests/scenario/__init__.py
 trove/tests/scenario/groups/__init__.py
 trove/tests/scenario/groups/backup_group.py
 trove/tests/scenario/groups/cluster_group.py
@@ -980,14 +1127,15 @@
 trove/tests/scenario/groups/instance_actions_group.py
 trove/tests/scenario/groups/instance_create_group.py
 trove/tests/scenario/groups/instance_delete_group.py
 trove/tests/scenario/groups/instance_error_create_group.py
 trove/tests/scenario/groups/instance_force_delete_group.py
 trove/tests/scenario/groups/instance_upgrade_group.py
 trove/tests/scenario/groups/module_group.py
+trove/tests/scenario/groups/negative_cluster_actions_group.py
 trove/tests/scenario/groups/replication_group.py
 trove/tests/scenario/groups/root_actions_group.py
 trove/tests/scenario/groups/test_group.py
 trove/tests/scenario/groups/user_actions_group.py
 trove/tests/scenario/helpers/__init__.py
 trove/tests/scenario/helpers/cassandra_helper.py
 trove/tests/scenario/helpers/couchbase_helper.py
@@ -1017,115 +1165,151 @@
 trove/tests/scenario/runners/instance_upgrade_runners.py
 trove/tests/scenario/runners/module_runners.py
 trove/tests/scenario/runners/negative_cluster_actions_runners.py
 trove/tests/scenario/runners/replication_runners.py
 trove/tests/scenario/runners/root_actions_runners.py
 trove/tests/scenario/runners/test_runners.py
 trove/tests/scenario/runners/user_actions_runners.py
+trove/tests/tempest/__init__.py
+trove/tests/tempest/config.py
+trove/tests/tempest/plugin.py
+trove/tests/tempest/services/__init__.py
+trove/tests/tempest/services/database/__init__.py
+trove/tests/tempest/services/database/json/__init__.py
+trove/tests/tempest/services/database/json/flavors_client.py
+trove/tests/tempest/services/database/json/limits_client.py
+trove/tests/tempest/services/database/json/versions_client.py
+trove/tests/tempest/tests/__init__.py
+trove/tests/tempest/tests/api/__init__.py
+trove/tests/tempest/tests/api/database/__init__.py
+trove/tests/tempest/tests/api/database/base.py
+trove/tests/tempest/tests/api/database/flavors/__init__.py
+trove/tests/tempest/tests/api/database/flavors/test_flavors.py
+trove/tests/tempest/tests/api/database/flavors/test_flavors_negative.py
+trove/tests/tempest/tests/api/database/limits/__init__.py
+trove/tests/tempest/tests/api/database/limits/test_limits.py
+trove/tests/tempest/tests/api/database/versions/__init__.py
+trove/tests/tempest/tests/api/database/versions/test_versions.py
 trove/tests/unittests/__init__.py
 trove/tests/unittests/trove_testtools.py
 trove/tests/unittests/api/__init__.py
 trove/tests/unittests/api/test_versions.py
 trove/tests/unittests/api/common/__init__.py
 trove/tests/unittests/api/common/test_extensions.py
 trove/tests/unittests/api/common/test_limits.py
 trove/tests/unittests/backup/__init__.py
 trove/tests/unittests/backup/test_backup_controller.py
 trove/tests/unittests/backup/test_backup_models.py
-trove/tests/unittests/backup/test_service.py
+trove/tests/unittests/backup/test_backupagent.py
+trove/tests/unittests/backup/test_storage.py
 trove/tests/unittests/cluster/__init__.py
 trove/tests/unittests/cluster/test_cassandra_cluster.py
 trove/tests/unittests/cluster/test_cluster.py
 trove/tests/unittests/cluster/test_cluster_controller.py
 trove/tests/unittests/cluster/test_cluster_models.py
 trove/tests/unittests/cluster/test_cluster_pxc_controller.py
 trove/tests/unittests/cluster/test_cluster_redis_controller.py
 trove/tests/unittests/cluster/test_cluster_vertica_controller.py
 trove/tests/unittests/cluster/test_cluster_views.py
 trove/tests/unittests/cluster/test_galera_cluster.py
 trove/tests/unittests/cluster/test_models.py
 trove/tests/unittests/cluster/test_mongodb_cluster.py
 trove/tests/unittests/cluster/test_redis_cluster.py
 trove/tests/unittests/cluster/test_vertica_cluster.py
-trove/tests/unittests/cmd/__init__.py
-trove/tests/unittests/cmd/test_status.py
 trove/tests/unittests/common/__init__.py
-trove/tests/unittests/common/test_auth.py
+trove/tests/unittests/common/test_common_extensions.py
 trove/tests/unittests/common/test_conductor_serializer.py
 trove/tests/unittests/common/test_context.py
 trove/tests/unittests/common/test_crypto_utils.py
 trove/tests/unittests/common/test_dbmodels.py
 trove/tests/unittests/common/test_exception.py
 trove/tests/unittests/common/test_notification.py
 trove/tests/unittests/common/test_pagination.py
 trove/tests/unittests/common/test_policy.py
+trove/tests/unittests/common/test_remote.py
 trove/tests/unittests/common/test_secure_serializer.py
 trove/tests/unittests/common/test_serializer.py
 trove/tests/unittests/common/test_server_group.py
 trove/tests/unittests/common/test_stream_codecs.py
 trove/tests/unittests/common/test_template.py
 trove/tests/unittests/common/test_timeutils.py
 trove/tests/unittests/common/test_utils.py
 trove/tests/unittests/common/test_wsgi.py
 trove/tests/unittests/conductor/__init__.py
 trove/tests/unittests/conductor/test_conf.py
 trove/tests/unittests/conductor/test_methods.py
 trove/tests/unittests/configuration/__init__.py
 trove/tests/unittests/configuration/test_configuration_controller.py
-trove/tests/unittests/configuration/test_service.py
 trove/tests/unittests/datastore/__init__.py
 trove/tests/unittests/datastore/base.py
 trove/tests/unittests/datastore/test_capability.py
 trove/tests/unittests/datastore/test_datastore.py
 trove/tests/unittests/datastore/test_datastore_version_metadata.py
 trove/tests/unittests/datastore/test_datastore_versions.py
 trove/tests/unittests/db/__init__.py
 trove/tests/unittests/db/test_migration_utils.py
 trove/tests/unittests/domain-name-service/__init__.py
 trove/tests/unittests/domain-name-service/test_designate_driver.py
-trove/tests/unittests/extensions/__init__.py
-trove/tests/unittests/extensions/common/__init__.py
-trove/tests/unittests/extensions/common/test_service.py
-trove/tests/unittests/extensions/mgmt/__init__.py
-trove/tests/unittests/extensions/mgmt/datastores/__init__.py
-trove/tests/unittests/extensions/mgmt/datastores/test_service.py
-trove/tests/unittests/extensions/mgmt/instances/__init__.py
-trove/tests/unittests/extensions/mgmt/instances/test_models.py
-trove/tests/unittests/extensions/mgmt/instances/test_service.py
-trove/tests/unittests/extensions/mgmt/quota/__init__.py
-trove/tests/unittests/extensions/mgmt/quota/test_service.py
 trove/tests/unittests/flavor/__init__.py
 trove/tests/unittests/flavor/test_flavor_views.py
 trove/tests/unittests/guestagent/__init__.py
-trove/tests/unittests/guestagent/datastore/__init__.py
-trove/tests/unittests/guestagent/datastore/test_service.py
-trove/tests/unittests/guestagent/datastore/postgres/__init__.py
-trove/tests/unittests/guestagent/datastore/postgres/test_manager.py
-trove/tests/unittests/guestagent/datastore/postgres/test_service.py
-trove/tests/unittests/guestagent/utils/__init__.py
-trove/tests/unittests/guestagent/utils/test_docker.py
+trove/tests/unittests/guestagent/test_agent_heartbeats_models.py
+trove/tests/unittests/guestagent/test_api.py
+trove/tests/unittests/guestagent/test_backups.py
+trove/tests/unittests/guestagent/test_cassandra_manager.py
+trove/tests/unittests/guestagent/test_configuration.py
+trove/tests/unittests/guestagent/test_couchbase_manager.py
+trove/tests/unittests/guestagent/test_couchdb_manager.py
+trove/tests/unittests/guestagent/test_datastore_manager.py
+trove/tests/unittests/guestagent/test_db2_manager.py
+trove/tests/unittests/guestagent/test_dbaas.py
+trove/tests/unittests/guestagent/test_galera_cluster_api.py
+trove/tests/unittests/guestagent/test_galera_manager.py
+trove/tests/unittests/guestagent/test_guestagent_utils.py
+trove/tests/unittests/guestagent/test_manager.py
+trove/tests/unittests/guestagent/test_mariadb_manager.py
+trove/tests/unittests/guestagent/test_models.py
+trove/tests/unittests/guestagent/test_mongodb_cluster_manager.py
+trove/tests/unittests/guestagent/test_mongodb_manager.py
+trove/tests/unittests/guestagent/test_mysql_manager.py
+trove/tests/unittests/guestagent/test_operating_system.py
+trove/tests/unittests/guestagent/test_pkg.py
+trove/tests/unittests/guestagent/test_query.py
+trove/tests/unittests/guestagent/test_redis_manager.py
+trove/tests/unittests/guestagent/test_service.py
+trove/tests/unittests/guestagent/test_vertica_api.py
+trove/tests/unittests/guestagent/test_vertica_manager.py
+trove/tests/unittests/guestagent/test_volume.py
 trove/tests/unittests/hacking/__init__.py
-trove/tests/unittests/hacking/test_check.py
+trove/tests/unittests/hacking/test_translation_checks.py
 trove/tests/unittests/instance/__init__.py
 trove/tests/unittests/instance/test_instance_controller.py
 trove/tests/unittests/instance/test_instance_models.py
 trove/tests/unittests/instance/test_instance_status.py
 trove/tests/unittests/instance/test_instance_views.py
-trove/tests/unittests/instance/test_service.py
+trove/tests/unittests/mgmt/__init__.py
+trove/tests/unittests/mgmt/test_clusters.py
+trove/tests/unittests/mgmt/test_datastore_controller.py
+trove/tests/unittests/mgmt/test_datastores.py
+trove/tests/unittests/mgmt/test_models.py
 trove/tests/unittests/module/__init__.py
 trove/tests/unittests/module/test_module_controller.py
 trove/tests/unittests/module/test_module_models.py
 trove/tests/unittests/module/test_module_views.py
 trove/tests/unittests/mysql/__init__.py
 trove/tests/unittests/mysql/test_common.py
 trove/tests/unittests/mysql/test_user_controller.py
+trove/tests/unittests/network/__init__.py
+trove/tests/unittests/network/test_neutron_driver.py
 trove/tests/unittests/quota/__init__.py
 trove/tests/unittests/quota/test_quota.py
 trove/tests/unittests/router/__init__.py
 trove/tests/unittests/router/test_router.py
+trove/tests/unittests/secgroups/__init__.py
+trove/tests/unittests/secgroups/test_security_group.py
 trove/tests/unittests/taskmanager/__init__.py
 trove/tests/unittests/taskmanager/test_api.py
 trove/tests/unittests/taskmanager/test_clusters.py
 trove/tests/unittests/taskmanager/test_galera_clusters.py
 trove/tests/unittests/taskmanager/test_manager.py
 trove/tests/unittests/taskmanager/test_models.py
 trove/tests/unittests/taskmanager/test_vertica_clusters.py
@@ -1145,12 +1329,9 @@
 trove/tests/util/mysql.py
 trove/tests/util/server_connection.py
 trove/tests/util/usage.py
 trove/tests/util/users.py
 trove/tests/util/utils.py
 trove/volume_type/__init__.py
 trove/volume_type/models.py
-trove/volume_type/views.py
-zuul.d/deprecated_jobs.yaml
-zuul.d/jobs.yaml
-zuul.d/nodesets.yaml
-zuul.d/projects.yaml
+trove/volume_type/service.py
+trove/volume_type/views.py
```

### Comparing `trove-21.0.0.0rc2/trove.egg-info/entry_points.txt` & `trove-8.0.1/trove.egg-info/entry_points.txt`

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,29 @@
 [console_scripts]
 trove-api = trove.cmd.api:main
 trove-conductor = trove.cmd.conductor:main
-trove-docker-plugin = trove.cmd.network_driver:main
 trove-fake-mode = trove.cmd.fakemode:main
 trove-guestagent = trove.cmd.guest:main
 trove-manage = trove.cmd.manage:main
 trove-mgmt-taskmanager = trove.cmd.taskmanager:mgmt_main
-trove-status = trove.cmd.status:main
 trove-taskmanager = trove.cmd.taskmanager:main
 
-[oslo.config.opts]
-trove.config = trove.common.cfg:list_opts
-
 [oslo.messaging.notify.drivers]
 trove.openstack.common.notifier.log_notifier = oslo_messaging.notify._impl_log:LogDriver
 trove.openstack.common.notifier.no_op_notifier = oslo_messaging.notify._impl_noop:NoOpDriver
 trove.openstack.common.notifier.rpc_notifier = oslo_messaging.notify.messaging:MessagingDriver
 trove.openstack.common.notifier.rpc_notifier2 = oslo_messaging.notify.messaging:MessagingV2Driver
 trove.openstack.common.notifier.test_notifier = oslo_messaging.notify._impl_test:TestDriver
 
-[oslo.policy.enforcer]
-trove = trove.common.policy:get_enforcer
-
-[oslo.policy.policies]
-trove = trove.common.policies:list_rules
+[tempest.test_plugins]
+trove_tests = trove.tests.tempest.plugin:TroveTempestPlugin
 
 [trove.api.extensions]
+account = trove.extensions.routes.account:Account
 mgmt = trove.extensions.routes.mgmt:Mgmt
 mysql = trove.extensions.routes.mysql:Mysql
+security_group = trove.extensions.routes.security_group:Security_group
 
 [trove.guestagent.module.drivers]
 new_relic_license = trove.guestagent.module.drivers.new_relic_license_driver:NewRelicLicenseDriver
 ping = trove.guestagent.module.drivers.ping_driver:PingDriver
 
-[wsgi_scripts]
-trove-wsgi = trove.cmd.app_wsgi:wsgimain
-
```

### Comparing `trove-21.0.0.0rc2/trove.egg-info/requires.txt` & `trove-8.0.1/trove.egg-info/requires.txt`

 * *Files 13% similar despite different names*

```diff
@@ -1,54 +1,48 @@
-Flask>=2.2.3
-Jinja2>=2.10
-Paste>=2.0.2
-PasteDeploy>=1.5.0
-PyMySQL>=0.7.6
-Routes>=2.3.1
+pbr!=2.1.0,>=2.0.0
 SQLAlchemy!=1.1.5,!=1.1.6,!=1.1.7,!=1.1.8,>=1.0.10
+eventlet!=0.18.3,!=0.20.1,<0.21.0,>=0.18.2
+keystonemiddleware>=4.12.0
+Routes>=2.3.1
 WebOb>=1.7.1
-cryptography>=2.1.4
-diskimage-builder!=1.6.0,!=1.7.0,!=1.7.1,>=1.1.2
-docker>=4.2.0
-eventlet!=0.18.3,!=0.20.1,>=0.18.2
-gunicorn>=20.1.0
-httplib2>=0.9.1
-iso8601>=0.1.11
-jsonschema>=3.2.0
-keystonemiddleware>=4.17.0
-lxml!=3.7.0,>=3.4.1
-netaddr>=0.7.18
-oslo.cache>=1.26.0
-oslo.concurrency>=3.26.0
-oslo.config>=6.8.0
-oslo.context>=4.0.0
-oslo.db>=4.27.0
-oslo.i18n>=3.15.3
-oslo.log>=3.36.0
-oslo.messaging>=14.1.0
-oslo.middleware>=3.31.0
-oslo.policy>=3.6.0
-oslo.serialization!=2.19.1,>=2.18.0
-oslo.service!=1.28.1,>=1.24.0
-oslo.upgradecheck>=1.3.0
-oslo.utils>=3.40.0
-osprofiler>=1.4.0
+PasteDeploy>=1.5.0
+Paste
+sqlalchemy-migrate>=0.11.0
+netaddr!=0.7.16,>=0.7.13
+httplib2>=0.7.5
+lxml!=3.7.0,>=2.3
 passlib>=1.7.0
-pbr!=2.1.0,>=2.0.0
-pexpect!=3.3,>=3.1
-psycopg2-binary>=2.6.2
-python-cinderclient>=3.3.0
-python-designateclient>=2.7.0
-python-glanceclient>=2.8.0
-python-heatclient>=1.10.0
+python-heatclient>=1.6.1
+python-novaclient>=9.0.0
+python-cinderclient>=2.1.0
 python-keystoneclient>=3.8.0
-python-neutronclient>=6.7.0
-python-novaclient>=9.1.0
 python-swiftclient>=3.2.0
-python-troveclient>=2.2.0
-semantic-version>=2.7.0
-sqlalchemy-migrate>=0.11.0
+python-designateclient>=1.5.0
+python-neutronclient>=6.3.0
+python-glanceclient>=2.7.0
+iso8601>=0.1.11
+jsonschema!=2.5.0,<3.0.0,>=2.0.0
+Jinja2!=2.9.0,!=2.9.1,!=2.9.2,!=2.9.3,!=2.9.4,>=2.8
+pexpect!=3.3,>=3.1
+oslo.config!=4.3.0,!=4.4.0,>=4.0.0
+oslo.context>=2.14.0
+oslo.i18n!=3.15.2,>=2.1.0
+oslo.middleware>=3.27.0
+oslo.serialization>=1.10.0
+oslo.service>=1.10.0
+oslo.utils>=3.20.0
+oslo.concurrency>=3.8.0
+PyMySQL>=0.7.6
+Babel!=2.4.0,>=2.3.4
+six>=1.9.0
 stevedore>=1.20.0
+oslo.messaging!=5.25.0,>=5.24.2
+osprofiler>=1.4.0
+oslo.log>=3.22.0
+oslo.db>=4.24.0
 xmltodict>=0.10.1
+pycrypto>=2.6
+oslo.policy>=1.23.0
+diskimage-builder!=1.6.0,!=1.7.0,!=1.7.1,>=1.1.2
 
-[:(sys_platform!='win32')]
-pyroute2>=0.7.7
+[:(python_version=='2.7' or python_version=='2.6' or python_version=='3.3')]
+enum34
```

