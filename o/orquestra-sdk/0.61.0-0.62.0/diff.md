# Comparing `tmp/orquestra_sdk-0.61.0-py3-none-any.whl.zip` & `tmp/orquestra_sdk-0.62.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,14 +1,14 @@
-Zip file size: 222744 bytes, number of entries: 110
+Zip file size: 224946 bytes, number of entries: 112
 -rw-r--r--  2.0 unx     1911 b- defN 20-Feb-02 00:00 orquestra/sdk/__init__.py
 -rw-r--r--  2.0 unx    10179 b- defN 20-Feb-02 00:00 orquestra/sdk/exceptions.py
 -rw-r--r--  2.0 unx        0 b- defN 20-Feb-02 00:00 orquestra/sdk/py.typed
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/__init__.py
 -rw-r--r--  2.0 unx     8254 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_ast.py
--rw-r--r--  2.0 unx    16463 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_config.py
+-rw-r--r--  2.0 unx    16509 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_config.py
 -rw-r--r--  2.0 unx     2880 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_dates.py
 -rw-r--r--  2.0 unx    42709 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_dsl.py
 -rw-r--r--  2.0 unx     3512 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_env.py
 -rw-r--r--  2.0 unx     2155 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_exec_ctx.py
 -rw-r--r--  2.0 unx     2577 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_factory.py
 -rw-r--r--  2.0 unx     3774 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_git_url_utils.py
 -rw-r--r--  2.0 unx     3858 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_graphs.py
@@ -17,34 +17,36 @@
 -rw-r--r--  2.0 unx     1144 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_regex.py
 -rw-r--r--  2.0 unx     1404 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_retry.py
 -rw-r--r--  2.0 unx     4948 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_services.py
 -rw-r--r--  2.0 unx    32576 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_traversal.py
 -rw-r--r--  2.0 unx     4608 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_viz.py
 -rw-r--r--  2.0 unx    23964 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_workflow.py
 -rw-r--r--  2.0 unx     8458 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/abc.py
--rw-r--r--  2.0 unx     4809 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/dispatch.py
+-rw-r--r--  2.0 unx     4826 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/dispatch.py
 -rw-r--r--  2.0 unx     6168 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/loader.py
--rw-r--r--  2.0 unx     7243 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/serde.py
+-rw-r--r--  2.0 unx     7032 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/serde.py
 -rw-r--r--  2.0 unx      763 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_api/__init__.py
 -rw-r--r--  2.0 unx    14594 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_api/_config.py
 -rw-r--r--  2.0 unx    12860 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_api/_task_run.py
 -rw-r--r--  2.0 unx    32163 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_api/_wf_run.py
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/__init__.py
 -rw-r--r--  2.0 unx    30006 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/_ce_runtime.py
--rw-r--r--  2.0 unx    51851 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/_client.py
+-rw-r--r--  2.0 unx    51898 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/_client.py
 -rw-r--r--  2.0 unx     6062 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/_exceptions.py
--rw-r--r--  2.0 unx    17822 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/_models.py
+-rw-r--r--  2.0 unx    17690 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_driver/_models.py
 -rw-r--r--  2.0 unx     3247 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_logs/_interfaces.py
 -rw-r--r--  2.0 unx     6791 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_logs/_markers.py
 -rw-r--r--  2.0 unx     1326 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_logs/_models.py
 -rw-r--r--  2.0 unx      849 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_logs/_regrouping.py
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_spaces/__init__.py
 -rw-r--r--  2.0 unx     2728 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_spaces/_api.py
 -rw-r--r--  2.0 unx     2357 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_spaces/_resolver.py
 -rw-r--r--  2.0 unx      704 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_spaces/_structs.py
+-rw-r--r--  2.0 unx      383 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_storage/__init__.py
+-rw-r--r--  2.0 unx     4496 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_storage/orqdantic.py
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_testing/__init__.py
 -rw-r--r--  2.0 unx     4061 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_testing/_connections.py
 -rw-r--r--  2.0 unx     9430 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_testing/_example_wfs.py
 -rw-r--r--  2.0 unx     1754 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_testing/_ipc.py
 -rw-r--r--  2.0 unx      261 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_testing/_long_import.py
 -rw-r--r--  2.0 unx      950 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/_testing/_reloaders.py
 -rw-r--r--  2.0 unx    18206 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_arg_resolvers.py
@@ -58,55 +60,55 @@
 -rw-r--r--  2.0 unx     2645 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_services/_down.py
 -rw-r--r--  2.0 unx     1323 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_services/_status.py
 -rw-r--r--  2.0 unx     2574 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_services/_up.py
 -rw-r--r--  2.0 unx     3226 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_task/_logs.py
 -rw-r--r--  2.0 unx     3614 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_task/_results.py
 -rw-r--r--  2.0 unx      373 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/__init__.py
 -rw-r--r--  2.0 unx     6308 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/_click_default_group.py
--rw-r--r--  2.0 unx     7125 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/_errors.py
+-rw-r--r--  2.0 unx     8358 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/_errors.py
 -rw-r--r--  2.0 unx     1686 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/_models.py
 -rw-r--r--  2.0 unx    17739 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/_presenters.py
 -rw-r--r--  2.0 unx    10907 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_ui/_prompts.py
 -rw-r--r--  2.0 unx     3983 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_workflow/_list.py
 -rw-r--r--  2.0 unx     4105 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_workflow/_logs.py
 -rw-r--r--  2.0 unx     3128 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_workflow/_results.py
 -rw-r--r--  2.0 unx     2594 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_workflow/_stop.py
 -rw-r--r--  2.0 unx     5198 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_workflow/_submit.py
 -rw-r--r--  2.0 unx     2132 b- defN 20-Feb-02 00:00 orquestra/sdk/_base/cli/_workflow/_view.py
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/__init__.py
--rw-r--r--  2.0 unx    25756 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_build_workflow.py
+-rw-r--r--  2.0 unx    25840 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_build_workflow.py
 -rw-r--r--  2.0 unx     8255 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_client.py
--rw-r--r--  2.0 unx    31768 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_dag.py
+-rw-r--r--  2.0 unx    30665 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_dag.py
 -rw-r--r--  2.0 unx      946 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_id_gen.py
 -rw-r--r--  2.0 unx     9527 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_ray_logs.py
--rw-r--r--  2.0 unx     1460 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_wf_metadata.py
+-rw-r--r--  2.0 unx     1482 b- defN 20-Feb-02 00:00 orquestra/sdk/_ray/_wf_metadata.py
 -rw-r--r--  2.0 unx      341 b- defN 20-Feb-02 00:00 orquestra/sdk/dremio/__init__.py
 -rw-r--r--  2.0 unx     2916 b- defN 20-Feb-02 00:00 orquestra/sdk/dremio/_api.py
 -rw-r--r--  2.0 unx      832 b- defN 20-Feb-02 00:00 orquestra/sdk/dremio/_env_var_reader.py
 -rw-r--r--  2.0 unx      662 b- defN 20-Feb-02 00:00 orquestra/sdk/dremio/_flight_facade.py
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/examples/__init__.py
--rw-r--r--  2.0 unx     1579 b- defN 20-Feb-02 00:00 orquestra/sdk/examples/exportable_wf.py
+-rw-r--r--  2.0 unx     1590 b- defN 20-Feb-02 00:00 orquestra/sdk/examples/exportable_wf.py
 -rw-r--r--  2.0 unx     1075 b- defN 20-Feb-02 00:00 orquestra/sdk/examples/workflow_defs.py
 -rw-r--r--  2.0 unx      318 b- defN 20-Feb-02 00:00 orquestra/sdk/kubernetes/__init__.py
 -rw-r--r--  2.0 unx     2449 b- defN 20-Feb-02 00:00 orquestra/sdk/kubernetes/quantity.py
 -rw-r--r--  2.0 unx      528 b- defN 20-Feb-02 00:00 orquestra/sdk/mlflow/__init__.py
--rw-r--r--  2.0 unx     8959 b- defN 20-Feb-02 00:00 orquestra/sdk/mlflow/_connection_utils.py
+-rw-r--r--  2.0 unx     8966 b- defN 20-Feb-02 00:00 orquestra/sdk/mlflow/_connection_utils.py
 -rw-r--r--  2.0 unx      427 b- defN 20-Feb-02 00:00 orquestra/sdk/packaging/__init__.py
 -rw-r--r--  2.0 unx     4139 b- defN 20-Feb-02 00:00 orquestra/sdk/packaging/_versions.py
 -rw-r--r--  2.0 unx      204 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/__init__.py
 -rw-r--r--  2.0 unx     1472 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/_compat.py
--rw-r--r--  2.0 unx     1620 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/configs.py
--rw-r--r--  2.0 unx    15228 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/ir.py
--rw-r--r--  2.0 unx     2107 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/responses.py
--rw-r--r--  2.0 unx     2992 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/workflow_run.py
+-rw-r--r--  2.0 unx     1630 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/configs.py
+-rw-r--r--  2.0 unx    15293 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/ir.py
+-rw-r--r--  2.0 unx     2137 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/responses.py
+-rw-r--r--  2.0 unx     3000 b- defN 20-Feb-02 00:00 orquestra/sdk/schema/workflow_run.py
 -rw-r--r--  2.0 unx     1102 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/__init__.py
 -rw-r--r--  2.0 unx     7595 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_api.py
 -rw-r--r--  2.0 unx     2500 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_auth.py
--rw-r--r--  2.0 unx     8282 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_client.py
+-rw-r--r--  2.0 unx     8350 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_client.py
 -rw-r--r--  2.0 unx     1249 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_exceptions.py
--rw-r--r--  2.0 unx     1773 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_models.py
-?rw-r--r--  2.0 unx    17544 b- defN 20-Feb-02 00:00 orquestra_sdk-0.61.0.dist-info/METADATA
-?rw-r--r--  2.0 unx       87 b- defN 20-Feb-02 00:00 orquestra_sdk-0.61.0.dist-info/WHEEL
-?rw-r--r--  2.0 unx       60 b- defN 20-Feb-02 00:00 orquestra_sdk-0.61.0.dist-info/entry_points.txt
-?rw-r--r--  2.0 unx    11357 b- defN 20-Feb-02 00:00 orquestra_sdk-0.61.0.dist-info/licenses/LICENSE
-?rw-r--r--  2.0 unx    10130 b- defN 20-Feb-02 00:00 orquestra_sdk-0.61.0.dist-info/RECORD
-110 files, 754922 bytes uncompressed, 206522 bytes compressed:  72.6%
+-rw-r--r--  2.0 unx     1767 b- defN 20-Feb-02 00:00 orquestra/sdk/secrets/_models.py
+?rw-r--r--  2.0 unx    17501 b- defN 20-Feb-02 00:00 orquestra_sdk-0.62.0.dist-info/METADATA
+?rw-r--r--  2.0 unx       87 b- defN 20-Feb-02 00:00 orquestra_sdk-0.62.0.dist-info/WHEEL
+?rw-r--r--  2.0 unx       60 b- defN 20-Feb-02 00:00 orquestra_sdk-0.62.0.dist-info/entry_points.txt
+?rw-r--r--  2.0 unx    11357 b- defN 20-Feb-02 00:00 orquestra_sdk-0.62.0.dist-info/licenses/LICENSE
+?rw-r--r--  2.0 unx    10324 b- defN 20-Feb-02 00:00 orquestra_sdk-0.62.0.dist-info/RECORD
+112 files, 760148 bytes uncompressed, 208410 bytes compressed:  72.6%
```

## zipnote {}

```diff
@@ -120,14 +120,20 @@
 
 Filename: orquestra/sdk/_base/_spaces/_resolver.py
 Comment: 
 
 Filename: orquestra/sdk/_base/_spaces/_structs.py
 Comment: 
 
+Filename: orquestra/sdk/_base/_storage/__init__.py
+Comment: 
+
+Filename: orquestra/sdk/_base/_storage/orqdantic.py
+Comment: 
+
 Filename: orquestra/sdk/_base/_testing/__init__.py
 Comment: 
 
 Filename: orquestra/sdk/_base/_testing/_connections.py
 Comment: 
 
 Filename: orquestra/sdk/_base/_testing/_example_wfs.py
@@ -309,23 +315,23 @@
 
 Filename: orquestra/sdk/secrets/_exceptions.py
 Comment: 
 
 Filename: orquestra/sdk/secrets/_models.py
 Comment: 
 
-Filename: orquestra_sdk-0.61.0.dist-info/METADATA
+Filename: orquestra_sdk-0.62.0.dist-info/METADATA
 Comment: 
 
-Filename: orquestra_sdk-0.61.0.dist-info/WHEEL
+Filename: orquestra_sdk-0.62.0.dist-info/WHEEL
 Comment: 
 
-Filename: orquestra_sdk-0.61.0.dist-info/entry_points.txt
+Filename: orquestra_sdk-0.62.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: orquestra_sdk-0.61.0.dist-info/licenses/LICENSE
+Filename: orquestra_sdk-0.62.0.dist-info/licenses/LICENSE
 Comment: 
 
-Filename: orquestra_sdk-0.61.0.dist-info/RECORD
+Filename: orquestra_sdk-0.62.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## orquestra/sdk/_base/_config.py

```diff
@@ -1,22 +1,22 @@
 ################################################################################
-# © Copyright 2022-2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """This is the internal module for saving and loading runtime configurations.
 
 See docs/runtime_configurations.rst for more information.
 """
 import os
 import pathlib
 from pathlib import Path
 from typing import Any, List, Mapping, Optional, Union
 from urllib.parse import ParseResult, urlparse
 
 import filelock
-from pydantic.error_wrappers import ValidationError
+from pydantic import ValidationError
 
 import orquestra.sdk.exceptions as exceptions
 from orquestra.sdk.schema.configs import (
     CONFIG_FILE_CURRENT_VERSION,
     ConfigName,
     RuntimeConfiguration,
     RuntimeConfigurationFile,
@@ -135,22 +135,23 @@
 
 def _open_config_file() -> RuntimeConfigurationFile:
     config_file = get_config_file_path()
     if not config_file.exists():
         raise exceptions.ConfigFileNotFoundError(
             f"Config file {config_file} not found."
         )
-    return RuntimeConfigurationFile.parse_file(config_file)
+    data: str = config_file.read_text()
+    return RuntimeConfigurationFile.model_validate_json(data)
 
 
 def _save_config_file(
     config_file_contents: RuntimeConfigurationFile,
 ):
     config_file: Path = get_config_file_path()
-    config_file.write_text(data=config_file_contents.json(indent=2))
+    config_file.write_text(data=config_file_contents.model_dump_json(indent=2))
 
 
 EMPTY_CONFIG_FILE = RuntimeConfigurationFile(
     version=CONFIG_FILE_CURRENT_VERSION,
     configs=dict(),
 )
 
@@ -171,15 +172,15 @@
     new_config_entry = RuntimeConfiguration(
         config_name=resolved_config_name,
         runtime_name=resolved_runtime_name,
         runtime_options=resolved_runtime_options,
     )
     new_config_file: RuntimeConfigurationFile
     if resolved_prev_config_file is not None:
-        new_config_file = resolved_prev_config_file.copy(deep=True)
+        new_config_file = resolved_prev_config_file.model_copy(deep=True)
     else:
         new_config_file = RuntimeConfigurationFile(
             version=CONFIG_FILE_CURRENT_VERSION,
             configs={},
         )
     new_config_file.configs[resolved_config_name] = new_config_entry
```

## orquestra/sdk/_base/dispatch.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2021-2023 Zapata Computing Inc.
+# © Copyright 2021 - 2024 Zapata Computing Inc.
 ################################################################################
 import importlib
 import importlib.util
 import os
 import sys
 import typing as t
 from functools import singledispatch
@@ -11,26 +11,26 @@
 from orquestra.sdk._base import serde
 from orquestra.sdk.schema import ir
 
 
 def _locate_callable(fn_ref_dict) -> t.Callable:
     fn_ref: ir.FunctionRef
     if fn_ref_dict["type"] == "MODULE_FUNCTION_REF":
-        fn_ref = ir.ModuleFunctionRef.parse_obj(fn_ref_dict)
+        fn_ref = ir.ModuleFunctionRef.model_validate(fn_ref_dict)
         return locate_fn_ref(fn_ref)
 
     elif fn_ref_dict["type"] == "FILE_FUNCTION_REF":
-        fn_ref = ir.FileFunctionRef.parse_obj(fn_ref_dict)
+        fn_ref = ir.FileFunctionRef.model_validate(fn_ref_dict)
         # There's some kerkuffle with prepending components to the script file path.
         # Here, we assume that the fn_ref already contains a valid filepath relative to
         # the pwd. Hence, we don't need to pass a custom search path to
         # `locate_fn_ref()`.
         return locate_fn_ref(fn_ref)
     elif fn_ref_dict["type"] == "INLINE_FUNCTION_REF":
-        fn_ref = ir.InlineFunctionRef.parse_obj(fn_ref_dict)
+        fn_ref = ir.InlineFunctionRef.model_validate(fn_ref_dict)
         # We need to deserialize the pickled function
         return locate_fn_ref(fn_ref)
     else:
         raise ValueError(
             f"fn_ref_dict needs to be consistent with {ir.FunctionRef}. "
             f"Got: {fn_ref_dict}"
         )
```

## orquestra/sdk/_base/serde.py

```diff
@@ -1,23 +1,24 @@
 ################################################################################
-# © Copyright 2021-2023 Zapata Computing Inc.
+# © Copyright 2021 - 2024 Zapata Computing Inc.
 ################################################################################
 import codecs
 import json
 import typing as t
 from contextlib import contextmanager
 from dataclasses import dataclass
 from functools import singledispatch
 from pathlib import Path
 
 import cloudpickle  # type: ignore
-import pydantic
 
 from orquestra.sdk.schema import ir, responses
 
+from .._base._storage import TypeAdapter
+
 CHUNK_SIZE = 40_000
 ENCODING = "base64"
 PICKLE_PROTOCOL = 4
 
 
 class _JSONTupleEncoder(json.JSONEncoder):
     @staticmethod
@@ -152,30 +153,26 @@
         raise NotImplementedError(
             "We only support AUTO, JSON, and ENCODED_PICKLE artifact serialization at"
             f" the moment, not {artifact_format}"
         )
 
 
 def value_from_result_dict(result_dict: t.Mapping) -> t.Any:
-    # Bug with mypy and Pydantic:
-    #   Unions cannot be passed to parse_obj_as: pydantic/pydantic#1847
-    result: responses.WorkflowResult = pydantic.parse_obj_as(
-        responses.WorkflowResult, result_dict  # type: ignore[arg-type]
+    result = t.cast(
+        responses.WorkflowResult,
+        TypeAdapter(responses.WorkflowResult).validate_python(result_dict),
     )
+
     return deserialize(result)
 
 
 def deserialize_constant(node: ir.ConstantNode):
-    # Bug with mypy and Pydantic:
-    #   Unions cannot be passed to parse_obj_as: pydantic/pydantic#1847
-    return deserialize(
-        pydantic.parse_obj_as(
-            responses.WorkflowResult, node.dict()  # type: ignore[arg-type]
-        )
-    )
+    constant = TypeAdapter(responses.WorkflowResult).validate_python(node.model_dump())
+
+    return deserialize(constant)
 
 
 def stringify_package_spec(package: ir.PackageSpec) -> str:
     parts: t.List[str] = [package.name]
 
     if package.extras:
         formatted_extras = ",".join(sorted(package.extras))
```

## orquestra/sdk/_base/_driver/_client.py

```diff
@@ -1,22 +1,22 @@
 ################################################################################
-# © Copyright 2022 - 2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Code for accessing the Workflow Driver API.
 
 Implemented API spec:
     https://github.com/zapatacomputing/workflow-driver/tree/2b3534/openapi
 """
 
 import io
 import re
 import zlib
 from datetime import timedelta
 from tarfile import TarFile
-from typing import Generic, List, Mapping, Optional, Tuple, TypeVar, Union
+from typing import Generic, List, Mapping, Optional, Tuple, TypeVar, Union, cast
 from urllib.parse import urljoin
 
 import pydantic
 import requests
 from requests import codes
 
 from orquestra.sdk import ProjectRef, exceptions
@@ -27,14 +27,15 @@
     State,
     WorkflowRun,
     WorkflowRunMinimal,
     WorkflowRunSummary,
     WorkspaceId,
 )
 
+from ..._base._storage import TypeAdapter
 from .._regex import VERSION_REGEX
 from . import _exceptions, _models
 
 
 def _match_unsupported_version(error_detail: str):
     # We try to match format of the error message to parse the supported and
     # submitted versions.
@@ -268,26 +269,26 @@
         Returns:
             the WorkflowDefID associated with the stored definition
         """
         query_params = (
             _models.CreateWorkflowDefsRequest(
                 workspaceId=project.workspace_id,
                 projectId=project.project_id,
-            ).dict()
+            ).model_dump()
             if project
             else None
         )
         resp = self._post(
             self._uri_provider.uri_for("create_workflow_def"),
-            body_params=workflow_def.dict(),
+            body_params=workflow_def.model_dump(),
             query_params=query_params,
         )
 
         if resp.status_code == codes.BAD_REQUEST:
-            error = _models.Error.parse_obj(resp.json())
+            error = _models.Error.model_validate(resp.json())
             raise _exceptions.InvalidWorkflowDef(
                 message=error.message, detail=error.detail
             )
 
         try:
             _handle_common_errors(resp)
         except (
@@ -295,15 +296,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         return (
             _models.Response[_models.CreateWorkflowDefResponse, _models.MetaEmpty]
-            .parse_obj(resp.json())
+            .model_validate(resp.json())
             .data.id
         )
 
     def list_workflow_defs(
         self, page_size: Optional[int] = None, page_token: Optional[str] = None
     ) -> Paginated[WorkflowDef]:
         """Lists all known workflow definitions.
@@ -322,29 +323,29 @@
                 error is raised by the remote cluster.
         """
         resp = self._get(
             self._uri_provider.uri_for("list_workflow_defs"),
             query_params=_models.ListWorkflowDefsRequest(
                 pageSize=page_size,
                 pageToken=page_token,
-            ).dict(),
+            ).model_dump(),
         )
 
         try:
             _handle_common_errors(resp)
         except (
             _exceptions.InvalidTokenError,
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.ListWorkflowDefsResponse, _models.Pagination
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
         contents = [d.workflow for d in parsed_response.data]
         if parsed_response.meta is not None:
             next_token = parsed_response.meta.nextPageToken
         else:
             next_token = None
 
         return Paginated(
@@ -428,15 +429,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_resp = _models.Response[
             _models.GetWorkflowDefResponse, _models.MetaEmpty
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         return parsed_resp.data
 
     def delete_workflow_def(self, workflow_def_id: _models.WorkflowDefID):
         """Gets a stored workflow definition.
 
         Args:
@@ -506,19 +507,19 @@
         resp = self._post(
             self._uri_provider.uri_for("create_workflow_run"),
             body_params=_models.CreateWorkflowRunRequest(
                 workflowDefinitionID=workflow_def_id,
                 resources=resources,
                 dryRun=dry_run,
                 headNodeResources=head_node_resources,
-            ).dict(),
+            ).model_dump(),
         )
 
         if resp.status_code == codes.BAD_REQUEST:
-            error = _models.Error.parse_obj(resp.json())
+            error = _models.Error.model_validate(resp.json())
             if error.code == _models.ErrorCode.SDK_VERSION_UNSUPPORTED:
                 requested, available = _match_unsupported_version(error.detail)
                 raise _exceptions.UnsupportedSDKVersion(requested, available)
             raise _exceptions.InvalidWorkflowRunRequest(
                 message=error.message, detail=error.detail
             )
 
@@ -529,15 +530,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         return (
             _models.Response[_models.CreateWorkflowRunResponse, _models.MetaEmpty]
-            .parse_obj(resp.json())
+            .model_validate(resp.json())
             .data.id
         )
 
     def list_workflow_runs(
         self,
         workflow_def_id: Optional[_models.WorkflowDefID] = None,
         page_size: Optional[int] = None,
@@ -581,15 +582,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.ListWorkflowRunsResponse, _models.Pagination
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         if parsed_response.meta is not None:
             next_token = parsed_response.meta.nextPageToken
         else:
             next_token = None
 
         workflow_runs = []
@@ -646,15 +647,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.ListWorkflowRunSummariesResponse, _models.Pagination
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         if parsed_response.meta is not None:
             next_token = parsed_response.meta.nextPageToken
         else:
             next_token = None
 
         workflow_runs = []
@@ -710,15 +711,15 @@
                 workflowDefinitionID=workflow_def_id,
                 pageSize=page_size,
                 pageToken=page_token,
                 workspaceId=workspace,
                 projectId=None,
                 maxAge=(int(max_age.total_seconds()) if max_age else None),
                 state=_get_state_query(state),
-            ).dict(),
+            ).model_dump(),
         )
 
         try:
             _handle_common_errors(resp)
         except (
             _exceptions.InvalidTokenError,
             _exceptions.ForbiddenError,
@@ -763,15 +764,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.WorkflowRunResponse, _models.MetaEmpty
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         workflow_def = self.get_workflow_def(parsed_response.data.definitionId)
 
         return parsed_response.data.to_ir(workflow_def.workflow)
 
     def terminate_workflow_run(
         self, wf_run_id: _models.WorkflowRunID, force: Optional[bool] = None
@@ -794,15 +795,15 @@
                 error is raised by the remote cluster.
         """
         resp = self._post(
             self._uri_provider.uri_for(
                 "terminate_workflow_run", parameters=(wf_run_id,)
             ),
             body_params=None,
-            query_params=_models.TerminateWorkflowRunRequest(force=force).dict(),
+            query_params=_models.TerminateWorkflowRunRequest(force=force).model_dump(),
         )
 
         if resp.status_code == codes.NOT_FOUND:
             raise _exceptions.WorkflowRunNotFound(wf_run_id)
 
         try:
             _handle_common_errors(resp)
@@ -837,15 +838,15 @@
         """
         resp = self._get(
             self._uri_provider.uri_for(
                 "get_workflow_run_artifacts",
             ),
             query_params=_models.GetWorkflowRunArtifactsRequest(
                 workflowRunId=wf_run_id
-            ).dict(),
+            ).model_dump(),
         )
 
         if resp.status_code == codes.NOT_FOUND:
             raise _exceptions.WorkflowRunNotFound(wf_run_id)
         elif resp.status_code == codes.BAD_REQUEST:
             raise _exceptions.InvalidWorkflowRunID(wf_run_id)
 
@@ -856,15 +857,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.GetWorkflowRunArtifactsResponse, _models.MetaEmpty
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         return parsed_response.data
 
     def get_workflow_run_artifact(
         self, artifact_id: _models.WorkflowRunArtifactID
     ) -> WorkflowResult:
         """Gets workflow run artifacts from the workflow driver.
@@ -899,18 +900,17 @@
         except (
             _exceptions.InvalidTokenError,
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
-        # Bug with mypy and Pydantic:
-        #   Unions cannot be passed to parse_obj_as: pydantic/pydantic#1847
-        return pydantic.parse_obj_as(
-            WorkflowResult, resp.json()  # type: ignore[arg-type]
+        return cast(
+            WorkflowResult,
+            TypeAdapter(WorkflowResult).validate_python(resp.json()),
         )
 
     # --- Workflow Run Results ---
 
     def get_workflow_run_results(
         self, wf_run_id: _models.WorkflowRunID
     ) -> List[_models.WorkflowRunResultID]:
@@ -931,15 +931,15 @@
             orquestra.sdk._base._driver._exceptions.UnknownHTTPError: when any other
                 error is raised by the remote cluster.
         """
         resp = self._get(
             self._uri_provider.uri_for("get_workflow_run_results"),
             query_params=_models.GetWorkflowRunResultsRequest(
                 workflowRunId=wf_run_id
-            ).dict(),
+            ).model_dump(),
         )
 
         if resp.status_code == codes.BAD_REQUEST:
             raise _exceptions.InvalidWorkflowRunID(wf_run_id)
         elif resp.status_code == codes.NOT_FOUND:
             raise _exceptions.WorkflowRunNotFound(wf_run_id)
 
@@ -950,15 +950,15 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.GetWorkflowRunResultsResponse, _models.MetaEmpty
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         return parsed_response.data
 
     def get_workflow_run_result(
         self, result_id: _models.WorkflowRunResultID
     ) -> Union[WorkflowResult, ComputeEngineWorkflowResult]:
         """Gets workflow run results from the workflow driver.
@@ -998,33 +998,33 @@
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         # To ensure the correct ordering of results, we serialize the results on CE as:
         # {
         #   "results": [
-        #       (JSONResult | PickleResult).json(),
-        #       (JSONResult | PickleResult).json(),
+        #       (JSONResult | PickleResult).model_dump_json(),
+        #       (JSONResult | PickleResult).model_dump_json(),
         #       ...
         #   ]
-        # } aka a ComputeEngineWorkflowResult.json()
+        # } aka a ComputeEngineWorkflowResult.model_dump_json()
         # For older workflows, we respond with:
-        # (JSONResult | PickleResult).json()
+        # (JSONResult | PickleResult).model_dump_json()
 
         json_response = resp.json()
         try:
             # Try an older response
-            # Bug with mypy and Pydantic:
-            #   Unions cannot be passed to parse_obj_as: pydantic/pydantic#1847
-            return pydantic.parse_obj_as(
-                WorkflowResult, json_response  # type: ignore[arg-type]
+            return cast(
+                WorkflowResult,
+                TypeAdapter(WorkflowResult).validate_python(json_response),
             )
+
         except pydantic.ValidationError:
             # If we fail, try parsing each part of a list separately
-            return ComputeEngineWorkflowResult.parse_obj(json_response)
+            return ComputeEngineWorkflowResult.model_validate(json_response)
 
     # --- Workflow Logs ---
     def get_workflow_run_logs(
         self, wf_run_id: _models.WorkflowRunID
     ) -> List[_models.WorkflowLogMessage]:
         """Gets the logs of a workflow run from the workflow driver.
 
@@ -1045,15 +1045,15 @@
             orquestra.sdk._base._driver._exceptions.WorkflowRunLogsNotReadable: when
                 the logs exist, but cannot be decoded.
         """
         resp = self._get(
             self._uri_provider.uri_for("get_workflow_run_logs"),
             query_params=_models.GetWorkflowRunLogsRequest(
                 workflowRunId=wf_run_id
-            ).dict(),
+            ).model_dump(),
         )
 
         # Handle errors
         if resp.status_code == codes.NOT_FOUND:
             raise _exceptions.WorkflowRunLogsNotFound(wf_run_id)
         elif resp.status_code == codes.BAD_REQUEST:
             raise _exceptions.InvalidWorkflowRunID(wf_run_id)
@@ -1079,15 +1079,15 @@
 
         # Parse the decoded data as logs
         messages = []
         for section_str in decoded.split("\n"):
             if len(section_str) < 1:
                 continue
 
-            events = pydantic.parse_raw_as(_models.WorkflowLogSection, section_str)
+            events = TypeAdapter(_models.WorkflowLogSection).validate_json(section_str)
 
             for event in events:
                 messages.append(event.message)
 
         return messages
 
     def get_task_run_logs(
@@ -1115,15 +1115,15 @@
             orquestra.sdk._base._driver._exceptions.UnknownHTTPError: when any other
                 error is raised by the remote cluster.
         """
         resp = self._get(
             self._uri_provider.uri_for("get_task_run_logs"),
             query_params=_models.GetTaskRunLogsRequest(
                 workflowRunId=wf_run_id, taskInvocationId=task_inv_id
-            ).dict(),
+            ).model_dump(),
         )
 
         # Handle errors
         if resp.status_code == codes.NOT_FOUND:
             raise _exceptions.TaskRunLogsNotFound(wf_run_id, task_inv_id)
         elif resp.status_code == codes.BAD_REQUEST:
             raise _exceptions.InvalidWorkflowRunID(wf_run_id)
@@ -1149,15 +1149,15 @@
 
         # Parse the decoded data as logs
         messages = []
         for section_str in decoded.split("\n"):
             if len(section_str) < 1:
                 continue
 
-            events = pydantic.parse_raw_as(_models.TaskLogSection, section_str)
+            events = TypeAdapter(_models.TaskLogSection).validate_json(section_str)
 
             for event in events:
                 messages.append(event.message)
 
         return messages
 
     def get_system_logs(self, wf_run_id: _models.WorkflowRunID) -> List[_models.SysLog]:
@@ -1182,15 +1182,15 @@
             NotImplementedError: when a log object's source_type is not a recognised
                 value, or is a value for a schema has not been defined.
         """
         resp = self._get(
             self._uri_provider.uri_for("get_workflow_run_system_logs"),
             query_params=_models.GetWorkflowRunLogsRequest(
                 workflowRunId=wf_run_id
-            ).dict(),
+            ).model_dump(),
         )
 
         # Handle errors
         if resp.status_code == codes.NOT_FOUND:
             raise _exceptions.WorkflowRunLogsNotFound(wf_run_id)
         elif resp.status_code == codes.BAD_REQUEST:
             raise _exceptions.InvalidWorkflowRunID(wf_run_id)
@@ -1216,15 +1216,16 @@
         assert untarred is not None
         decoded = untarred.read().decode()
 
         messages = []
         for section_str in decoded.split("\n"):
             if len(section_str) < 1:
                 continue
-            events = pydantic.parse_raw_as(_models.SysSection, section_str)
+
+            events = TypeAdapter(_models.SysSection).validate_json(section_str)
 
             for event in events:
                 messages.append(event.message)
 
         return messages
 
     def list_workspaces(self):
@@ -1248,16 +1249,16 @@
         except (
             _exceptions.InvalidTokenError,
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
-        parsed_response = pydantic.parse_obj_as(
-            _models.ListWorkspacesResponse, resp.json()
+        parsed_response = TypeAdapter(_models.ListWorkspacesResponse).validate_python(
+            resp.json()
         )
 
         return parsed_response
 
     def list_projects(self, workspace_id: WorkspaceId):
         """Gets the list of all projects in given workspace.
 
@@ -1287,16 +1288,16 @@
         except (
             _exceptions.InvalidTokenError,
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
-        parsed_response = pydantic.parse_obj_as(
-            _models.ListProjectResponse, resp.json()
+        parsed_response = TypeAdapter(_models.ListProjectResponse).validate_python(
+            resp.json()
         )
 
         return parsed_response
 
     def get_workflow_project(self, wf_run_id: _models.WorkflowRunID) -> ProjectRef:
         """Gets the status of a workflow run from the workflow driver.
 
@@ -1332,14 +1333,14 @@
             _exceptions.ForbiddenError,
             _exceptions.UnknownHTTPError,
         ):
             raise
 
         parsed_response = _models.Response[
             _models.WorkflowRunResponse, _models.MetaEmpty
-        ].parse_obj(resp.json())
+        ].model_validate(resp.json())
 
         workflow_def = self.get_workflow_def(parsed_response.data.definitionId)
 
         return ProjectRef(
             workspace_id=workflow_def.workspaceId, project_id=workflow_def.project
         )
```

## orquestra/sdk/_base/_driver/_models.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2022 - 2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Internal models for the workflow driver API."""
 from enum import Enum, IntEnum
 from typing import (
     Generic,
     List,
     Literal,
@@ -12,15 +12,14 @@
     NewType,
     Optional,
     TypeVar,
     Union,
 )
 
 import pydantic
-from pydantic.generics import GenericModel
 from typing_extensions import Annotated
 
 from orquestra.sdk._base._dates import Instant
 from orquestra.sdk.schema.ir import WorkflowDef
 from orquestra.sdk.schema.workflow_run import (
     ProjectId,
     RunStatus,
@@ -28,45 +27,47 @@
     TaskRun,
     WorkflowRun,
     WorkflowRunMinimal,
     WorkflowRunSummary,
     WorkspaceId,
 )
 
+from ..._base._storage import BaseModel
+
 WorkflowDefID = str
 WorkflowRunID = str
 TaskRunID = str
 TaskInvocationID = str
 WorkflowRunArtifactID = str
 WorkflowRunResultID = str
 
 
 # --- Generic responses and pagination ---
 
 DataT = TypeVar("DataT")
 MetaT = TypeVar("MetaT")
 
 
-class Pagination(pydantic.BaseModel):
+class Pagination(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/259481b9240547bccf4fa40df4e92bf6c617a25f/openapi/src/schemas/MetaSuccessPaginated.yaml.
     """  # noqa: D205, D212
 
     nextPageToken: str
 
 
-class Response(GenericModel, Generic[DataT, MetaT]):
+class Response(BaseModel, Generic[DataT, MetaT]):
     """A generic to help with the structure of driver responses."""
 
     data: DataT
-    meta: Optional[MetaT]
+    meta: Optional[MetaT] = None
 
 
-class MetaEmpty(pydantic.BaseModel):
+class MetaEmpty(BaseModel):
     pass
 
 
 class ErrorCode(IntEnum):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/59eb3985151a813c69fec5a4777e8ed5c9a1f718/openapi/src/resources/workflow-runs.yaml#L59.
@@ -76,70 +77,70 @@
     WORKFLOW_DEF_ID_MISSING = 2
     RESOURCE_REQUEST_INVALID = 3
     SDK_VERSION_UNSUPPORTED = 4
     WORKFLOW_DEF_ID_NOT_UUID = 5
     WORKFLOW_DEF_NOT_FOUND = 6
 
 
-class Error(pydantic.BaseModel):
+class Error(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/2b3534/openapi/src/schemas/Error.yaml.
     """  # noqa: D205, D212
 
-    code: Optional[int]
+    code: Optional[int] = None
     message: str
     detail: str
 
 
 # --- Workflow Definitions ---
 
 
-class CreateWorkflowDefResponse(pydantic.BaseModel):
+class CreateWorkflowDefResponse(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/2b3534/openapi/src/responses/CreateWorkflowDefinitionResponse.yaml.
     """  # noqa: D205, D212
 
     id: WorkflowDefID
 
 
-class GetWorkflowDefResponse(pydantic.BaseModel):
+class GetWorkflowDefResponse(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/cb61512e9f3da24addd933c7259aa4584ab04e4f/openapi/src/schemas/WorkflowDefinition.yaml.
     """  # noqa: D205, D212
 
     id: WorkflowDefID
     created: Instant
     owner: str
     workflow: WorkflowDef
     workspaceId: WorkspaceId
     project: ProjectId
     sdkVersion: str
 
 
-class ListWorkflowDefsRequest(pydantic.BaseModel):
+class ListWorkflowDefsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/cdb667ef6d1053876250daff27e19fb50374c0d4/openapi/src/resources/workflow-definitions.yaml#L8.
     """  # noqa: D205, D212
 
-    pageSize: Optional[int]
-    pageToken: Optional[str]
+    pageSize: Optional[int] = None
+    pageToken: Optional[str] = None
 
 
-class CreateWorkflowDefsRequest(pydantic.BaseModel):
+class CreateWorkflowDefsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/dc8a2a37d92324f099afefc048f6486a5061850f/openapi/src/resources/workflow-definitions.yaml#L39.
     """  # noqa: D205, D212
 
-    workspaceId: Optional[str]
-    projectId: Optional[str]
+    workspaceId: Optional[str] = None
+    projectId: Optional[str] = None
 
 
 ListWorkflowDefsResponse = List[GetWorkflowDefResponse]
 
 # --- Workflow Runs ---
 
 
@@ -158,41 +159,41 @@
     UNKNOWN = "UNKNOWN"
 
     @classmethod
     def _missing_(cls, value):
         return cls.UNKNOWN
 
 
-class RunStatusResponse(pydantic.BaseModel):
+class RunStatusResponse(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/schemas/RunStatus.yaml#L1.
     """  # noqa: D205, D212
 
     state: StateResponse
-    startTime: Optional[Instant]
-    endTime: Optional[Instant]
+    startTime: Optional[Instant] = None
+    endTime: Optional[Instant] = None
 
     def to_ir(self) -> RunStatus:
         return RunStatus(
             state=State(self.state),
             start_time=self.startTime,
             end_time=self.endTime,
         )
 
 
-class TaskRunResponse(pydantic.BaseModel):
+class TaskRunResponse(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/schemas/WorkflowRun.yaml#L17.
     """  # noqa: D205, D212
 
     id: TaskRunID
     invocationId: TaskInvocationID
-    status: Optional[RunStatusResponse]
+    status: Optional[RunStatusResponse] = None
 
     def to_ir(self) -> TaskRun:
         if self.status is None:
             status = RunStatus(
                 state=State.WAITING,
                 start_time=None,
                 end_time=None,
@@ -202,15 +203,15 @@
         return TaskRun(
             id=self.id,
             invocation_id=self.invocationId,
             status=status,
         )
 
 
-class MinimalWorkflowRunResponse(pydantic.BaseModel):
+class MinimalWorkflowRunResponse(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/schemas/WorkflowRun.yaml#L1.
     """  # noqa: D205, D212
 
     id: WorkflowRunID
     definitionId: WorkflowDefID
@@ -218,15 +219,15 @@
     def to_ir(self, workflow_def: WorkflowDef) -> WorkflowRunMinimal:
         return WorkflowRunMinimal(
             id=self.id,
             workflow_def=workflow_def,
         )
 
 
-class WorkflowRunSummaryResponse(pydantic.BaseModel):
+class WorkflowRunSummaryResponse(BaseModel):
     """Contains all of the information needed to give a basic overview of the workflow.
 
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/schemas/WorkflowRun.yaml#L1
     """
 
     id: WorkflowRunID
@@ -264,170 +265,170 @@
             id=self.id,
             status=self.status.to_ir(),
             task_runs=[t.to_ir() for t in self.taskRuns],
             workflow_def=workflow_def,
         )
 
 
-class Resources(pydantic.BaseModel):
+class Resources(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/580c8d8835b1cccd085ea716c514038e85eb28d7/openapi/src/schemas/Resources.yaml.
     """  # noqa: D205, D212
 
     # If this schema is changed, the documentation in
     # docs/guides/ce-resource-management.rst should also be updated.
 
-    nodes: Optional[int]
+    nodes: Optional[int] = None
     cpu: Optional[str] = pydantic.Field(
-        regex=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
+        pattern=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
     )
     memory: Optional[str] = pydantic.Field(
-        regex=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
+        pattern=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
     )
-    gpu: Optional[str] = pydantic.Field(regex="^[01]+$")
+    gpu: Optional[str] = pydantic.Field(pattern="^[01]+$")
 
 
-class HeadNodeResources(pydantic.BaseModel):
+class HeadNodeResources(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/ac1e97ea00fc3526c93187a1da02170bff45b74f/openapi/src/schemas/HeadNodeResources.yaml.
     """  # noqa: D205, D212
 
     cpu: Optional[str] = pydantic.Field(
-        regex=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
+        pattern=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
     )
     memory: Optional[str] = pydantic.Field(
-        regex=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
+        pattern=r"^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$"
     )
 
 
-class CreateWorkflowRunRequest(pydantic.BaseModel):
+class CreateWorkflowRunRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/ac1e97ea00fc3526c93187a1da02170bff45b74f/openapi/src/schemas/CreateWorkflowRunRequest.yaml.
     """  # noqa: D205, D212
 
     workflowDefinitionID: WorkflowDefID
     resources: Resources
     dryRun: bool
-    headNodeResources: Optional[HeadNodeResources]
+    headNodeResources: Optional[HeadNodeResources] = None
 
 
-class CreateWorkflowRunResponse(pydantic.BaseModel):
+class CreateWorkflowRunResponse(BaseModel):
     """Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/2e999a76019e8f8de8082409daddf7789dc2f430/pkg/server/server.go#L376.
     """  # noqa: D205, D212
 
     id: WorkflowRunID
 
 
-class ListWorkflowRunsRequest(pydantic.BaseModel):
+class ListWorkflowRunsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/c52013c0f4df066159fc32ad38d489b3eaff5850/openapi/src/resources/workflow-runs.yaml#L14.
     """  # noqa: D205, D212
 
-    workflowDefinitionID: Optional[WorkflowDefID]
-    pageSize: Optional[int]
-    pageToken: Optional[str]
-    workspaceId: Optional[WorkspaceId]
-    projectId: Optional[ProjectId]
-    maxAge: Optional[int]
-    state: Optional[str]
+    workflowDefinitionID: Optional[WorkflowDefID] = None
+    pageSize: Optional[int] = None
+    pageToken: Optional[str] = None
+    workspaceId: Optional[WorkspaceId] = None
+    projectId: Optional[ProjectId] = None
+    maxAge: Optional[int] = None
+    state: Optional[str] = None
 
 
 ListWorkflowRunsResponse = List[MinimalWorkflowRunResponse]
 
 ListWorkflowRunSummariesResponse = List[WorkflowRunSummaryResponse]
 
 
-class GetWorkflowRunResponse(pydantic.BaseModel):
+class GetWorkflowRunResponse(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/resources/workflow-run.yaml#L17.
     """  # noqa: D205, D212
 
     data: WorkflowRunResponse
 
 
-class TerminateWorkflowRunRequest(pydantic.BaseModel):
+class TerminateWorkflowRunRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/873437f8157226c451220306a6ce90c80e8c8f9e/openapi/src/resources/workflow-run-terminate.yaml#L12.
     """  # noqa: D205, D212
 
-    force: Optional[bool]
+    force: Optional[bool] = None
 
 
 # --- Workflow Artifacts ---
 
 
-class GetWorkflowRunArtifactsRequest(pydantic.BaseModel):
+class GetWorkflowRunArtifactsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/resources/artifacts.yaml#L10.
     """  # noqa: D205, D212
 
     workflowRunId: WorkflowRunID
 
 
 GetWorkflowRunArtifactsResponse = Mapping[TaskRunID, List[WorkflowRunArtifactID]]
 
 # --- Workflow Results ---
 
 
-class GetWorkflowRunResultsRequest(pydantic.BaseModel):
+class GetWorkflowRunResultsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/resources/run-results.yaml#L10.
     """  # noqa: D205, D212
 
     workflowRunId: WorkflowRunID
 
 
 GetWorkflowRunResultsResponse = List[WorkflowRunResultID]
 
 
 # --- Logs ---
 
 
-class GetWorkflowRunLogsRequest(pydantic.BaseModel):
+class GetWorkflowRunLogsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/34eba4253b56266772795a8a59d6ec7edf88c65a/openapi/src/resources/workflow-run-logs.yaml.
     """  # noqa: D205, D212
 
     workflowRunId: WorkflowRunID
 
 
-class GetTaskRunLogsRequest(pydantic.BaseModel):
+class GetTaskRunLogsRequest(BaseModel):
     """
     Implements:
     https://github.com/zapatacomputing/workflow-driver/blob/c7685a579eca1f9cb3eb27e2a8c2a9757a3cd021/openapi/src/resources/task-run-logs.yaml.
     """  # noqa: D205, D212
 
     workflowRunId: WorkflowRunID
     taskInvocationId: TaskInvocationID
 
 
-class CommonResourceMeta(pydantic.BaseModel):
+class CommonResourceMeta(BaseModel):
     type: str
     displayName: str
     description: str
     owner: str
     createdBy: str
     createdAt: str
     lastAccessed: str
     lastUpdated: str
     tags: List[str]
     status: str
 
 
-class ResourceIdentifier(pydantic.BaseModel):
+class ResourceIdentifier(BaseModel):
     tenantId: str
     resourceGroupId: str
     id: str
 
 
 class WorkspaceDetail(CommonResourceMeta, ResourceIdentifier):
     logo: str
@@ -463,15 +464,15 @@
 # file contains newline-separated chunks. Each chunk is a JSON-encoded list of events.
 
 
 LogFilename = NewType("LogFilename", str)
 RayFilename = NewType("RayFilename", str)
 
 
-class WorkflowLogMessage(pydantic.BaseModel):
+class WorkflowLogMessage(BaseModel):
     """Represents a single line indexed by the server side log service.
 
     Based on:
     https://github.com/zapatacomputing/workflow-driver/blob/972aaa3ca75780a52d01872bc294be419a761209/openapi/src/resources/workflow-run-logs.yaml#L25.
 
     The name is borrowed from Fluent Bit nomenclature:
     https://docs.fluentbit.io/manual/concepts/key-concepts#event-format.
@@ -511,15 +512,15 @@
     message: WorkflowLogMessage
     """A single indexed log line."""
 
 
 WorkflowLogSection = List[WorkflowLogEvent]
 
 
-class TaskLogMessage(pydantic.BaseModel):
+class TaskLogMessage(BaseModel):
     """Represents a single line indexed by the server side log service.
 
     Based on:
     https://github.com/zapatacomputing/workflow-driver/blob/f55bd3d5203a42ee42fc2405cd44cfaa94993f4a/openapi/src/resources/task-run-logs.yaml#L16
 
     The name is borrowed from Fluent Bit nomenclature:
     https://docs.fluentbit.io/manual/concepts/key-concepts#event-format.
@@ -574,52 +575,52 @@
     UNKNOWN = "UNKNOWN"
 
     @classmethod
     def _missing_(cls, *args, **kwargs):
         return cls.UNKNOWN
 
 
-class K8sEventLog(pydantic.BaseModel):
+class K8sEventLog(BaseModel):
     """A system-level log line produced by a K8S event."""
 
     tag: str
 
     log: dict
     """
     The keys in this dictionary are determined by Kubernetes.
     """
 
     source_type: Literal[SystemLogSourceType.K8S_EVENT] = SystemLogSourceType.K8S_EVENT
 
 
-class RayHeadNodeEventLog(pydantic.BaseModel):
+class RayHeadNodeEventLog(BaseModel):
     """A system-level log line produced by a Ray head node event."""
 
     tag: str
 
     log: str
 
     source_type: Literal[
         SystemLogSourceType.RAY_HEAD_NODE
     ] = SystemLogSourceType.RAY_HEAD_NODE
 
 
-class RayWorkerNodeEventLog(pydantic.BaseModel):
+class RayWorkerNodeEventLog(BaseModel):
     """A system-level log line produced by a Ray head node event."""
 
     tag: str
 
     log: str
 
     source_type: Literal[
         SystemLogSourceType.RAY_WORKER_NODE
     ] = SystemLogSourceType.RAY_WORKER_NODE
 
 
-class UnknownEventLog(pydantic.BaseModel):
+class UnknownEventLog(BaseModel):
     """Fallback option - the event type is unknown, so display the message as a str."""
 
     tag: str
 
     log: str
 
     source_type: Literal[SystemLogSourceType.UNKNOWN] = SystemLogSourceType.UNKNOWN
```

## orquestra/sdk/_base/cli/_ui/_errors.py

```diff
@@ -1,30 +1,73 @@
 ################################################################################
 # © Copyright 2023 Zapata Computing Inc.
 ################################################################################
 import sys
-import traceback
 from functools import singledispatch
+from pathlib import Path
+from types import TracebackType
+from typing import Optional
 
 import click
+import rich
 from rich.box import SIMPLE_HEAVY
 from rich.console import Console
 from rich.table import Column, Table
 
 from orquestra.sdk import exceptions
 from orquestra.sdk._base._config import IN_PROCESS_CONFIG_NAME, RAY_CONFIG_NAME_ALIAS
 from orquestra.sdk.schema.responses import ResponseStatusCode
 
 
-def _print_traceback(e: Exception):
-    # Newer Python versions like 3.10 allow passing just the exception object to
-    # traceback.format_exception(). Python 3.8 requires an explicit 3-argument form.
-
-    tb_lines = traceback.format_exception(type(e), e, e.__traceback__)
-    click.secho("".join(tb_lines), fg="red", file=sys.stderr)
+def _compact_tb(tb: TracebackType):
+    return "{}:{}:{}".format(
+        tb.tb_frame.f_code.co_name,
+        Path(tb.tb_frame.f_code.co_filename).name,
+        tb.tb_lineno,
+    )
+
+
+def _compact_exc(e: BaseException, prefix: str = ""):
+    tb = e.__traceback__
+    exc_message = f"{e}"
+    spacing = ": " if len(exc_message) > 0 else ""
+    file_details = f"({_compact_tb(tb)})" if tb is not None else ""
+    return "{}[red][b]{}{}[/b]{} {}[/red]".format(
+        prefix,
+        type(e).__name__,
+        spacing,
+        exc_message,
+        file_details,
+    )
+
+
+def _print_traceback(
+    e: BaseException, level: int = 0, console: Optional[rich.console.Console] = None
+):
+    _console = (
+        rich.console.Console(file=sys.stderr, highlight=False)
+        if console is None
+        else console
+    )
+    indent = "  " * level
+    _console.print(f"{indent}{_compact_exc(e)}")
+    if level == 0:
+        tb = e.__traceback__
+        while tb is not None:
+            _console.print(f"  [red]{_compact_tb(tb)}[/red]")
+            tb = tb.tb_next
+    next_exc = e.__cause__
+    context_exc = e.__context__
+    suppress_context = e.__suppress_context__
+    if next_exc is not None:
+        _console.print(f"{indent}[red b]Caused by:[/red b]")
+        _print_traceback(next_exc, level + 1, _console)
+    elif context_exc is not None and not suppress_context:
+        _console.print(f"{indent}[red b]While handling:[/red b]")
+        _print_traceback(context_exc, level + 1, _console)
 
 
 @singledispatch
 def pretty_print_exception(e: Exception) -> ResponseStatusCode:
     # The default case
     _print_traceback(e)
     click.echo(
```

## orquestra/sdk/_ray/_build_workflow.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2023 Zapata Computing Inc.
+# © Copyright 2023 - 2024 Zapata Computing Inc.
 ################################################################################
 """Translates IR workflow def into a Ray workflow."""
 import os
 import re
 import time
 import typing as t
 import warnings
@@ -283,19 +283,21 @@
                     if serialization
                     else wrapped_return
                 )
                 unpacked: t.Tuple[responses.WorkflowResult, ...]
 
                 if output_metadata is not None and output_metadata.n_outputs > 1:
                     unpacked = tuple(
-                        serde.result_from_artifact(
-                            wrapped_return[i], ir.ArtifactFormat.AUTO
+                        (
+                            serde.result_from_artifact(
+                                wrapped_return[i], ir.ArtifactFormat.AUTO
+                            )
+                            if serialization
+                            else wrapped_return[i]
                         )
-                        if serialization
-                        else wrapped_return[i]
                         for i in range(output_metadata.n_outputs)
                     )
                 else:
                     unpacked = (packed,)
                 return TaskResult(
                     packed=packed,
                     unpacked=unpacked,
@@ -646,15 +648,15 @@
     # dict.
     ray_task_name = client.get_current_task_id()
     task_meta: dict = client.get_task_metadata(
         workflow_id=wf_run_id, name=ray_task_name
     )
 
     try:
-        user_meta = InvUserMetadata.parse_obj(task_meta.get("user_metadata"))
+        user_meta = InvUserMetadata.model_validate(task_meta.get("user_metadata"))
     except pydantic.ValidationError:
         # This ray task wasn't annotated with InvUserMetadata. It happens when
         # `get_current_ids()` is used from a context that's not a regular Orquestra Task
         # run. One example is the one-off task that we use to construct Ray DAG inside a
         # Ray worker process.
         return wf_run_id, None, None
```

## orquestra/sdk/_ray/_dag.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2022-2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """RuntimeInterface implementation that uses Ray DAG/Ray Core API."""
 
 from __future__ import annotations
 
 import dataclasses
 import logging
@@ -103,43 +103,15 @@
         return State.WAITING
 
 
 def _workflow_state_from_ray_meta(
     wf_status: _client.WorkflowStatus,
     start_time: t.Optional[float],
     end_time: t.Optional[float],
-    ray_task_metas: t.List[t.Dict[str, t.Any]],
 ) -> State:
-    if wf_status == _client.WorkflowStatus.FAILED:
-        # If Ray said the workflow has failed, we'll check to see if all the tasks are
-        # in a completed state.
-        # Note that unlike when reporting states for individual tasks, we regard
-        # 'WAITING' as completed as we only want tasks that have actually started to
-        # prevent the workflow from being reported as FAILED.
-        tasks_completed = (
-            (
-                state := _task_state_from_ray_meta(
-                    wf_status,
-                    task_meta["stats"].get("start_time"),
-                    task_meta["stats"].get("end_time"),
-                    task_meta["stats"].get("failed"),
-                )
-            ).is_completed()
-            or state == State.WAITING
-            for task_meta in ray_task_metas
-        )
-
-        if all(tasks_completed):
-            # If all the tasks are completed, we will say the workflow failed.
-            return State.FAILED
-        else:
-            # If there is at least one task that is not in a completed state, we'll
-            # say the workflow is still running.
-            return State.RUNNING
-
     if start_time and end_time and wf_status == _client.WorkflowStatus.RUNNING:
         # If we ask Ray right after a workflow has been completed, Ray reports
         # workflow status as "RUNNING". This happens even after we await Ray
         # workflow completion via 'ray.get()'.
         return State.SUCCEEDED
 
     if not start_time and not end_time and wf_status == _client.WorkflowStatus.RUNNING:
@@ -250,15 +222,14 @@
     """
     # We'll use our workflow state heuristic to return a better
     # description of the workflow state.
     state = _workflow_state_from_ray_meta(
         wf_status=wf_status,
         start_time=start_time,
         end_time=end_time,
-        ray_task_metas=ray_task_metas,
     )
     if not state.is_completed() and end_time is not None:
         # If the workflow isn't completed and the metadata contained an end_time,
         # we'll use None as the end_time
         # This is because a "failed" workflow will have its end_time set, even if
         # there are tasks still running.
         _end_time = None
@@ -267,15 +238,18 @@
         # This is because the end_time stored with the workflow is when the workflow
         # was marked as failed, not when the last task ended. Note that workflows can
         # fail with waiting tasks, so we filter out any tasks that don't have a start
         # time.
         task_end_times = [
             task_meta["stats"].get("end_time")
             for task_meta in ray_task_metas
-            if task_meta["stats"].get("start_time")
+            if (
+                task_meta["stats"].get("start_time")
+                and task_meta["stats"].get("end_time")
+            )
         ]
         if len(task_end_times) == 0:
             # If there are no usable task end times, use the workflow end time.
             # This can occur if a workflow fails during environment setup.
             _end_time = end_time
         else:
             _end_time = max(task_end_times)
@@ -444,15 +418,15 @@
             wf_status = self._client.get_workflow_status(workflow_id=workflow_run_id)
             wf_meta = self._client.get_workflow_metadata(workflow_id=workflow_run_id)
         except (_client.workflow_exceptions.WorkflowNotFoundError, ValueError) as e:
             raise exceptions.WorkflowRunNotFoundError(
                 f"Workflow run {workflow_run_id} wasn't found"
             ) from e
 
-        wf_user_metadata = WfUserMetadata.parse_obj(wf_meta["user_metadata"])
+        wf_user_metadata = WfUserMetadata.model_validate(wf_meta["user_metadata"])
         wf_def = wf_user_metadata.workflow_def
 
         inv_ids = wf_def.task_invocations.keys()
         # We assume that:
         # - create_workflow_run() created a separate Ray Task for each IR's
         #   TaskInvocation
         # - each Ray Task's name was set to TaskInvocation.id
@@ -529,15 +503,15 @@
         """Set the current time as end_time for tasks and workflows that don't have one.
 
         Ray doesn't provide an end time for terminated tasks and workflows.
         This brought some issues on Workflow Driver, we so fill up the missing status
         fields with the current datetime for all terminated tasks and workflow.
         """
         now: _dates.Instant = _dates.now()
-        new_model = model.copy(deep=True)
+        new_model = model.model_copy(deep=True)
 
         if model.status.start_time is not None and model.status.end_time is None:
             assert now >= model.status.start_time
             new_model.status.end_time = now
 
         for task in new_model.task_runs:
             if task.status.start_time is not None and task.status.end_time is None:
@@ -632,17 +606,19 @@
 
         # We need to check if the task output was a TaskResult or any other value.
         # A TaskResult means this is a >=0.47.0 workflow and there is a serialized
         # value (WorkflowResult) in TaskResult.packed
         # Anything else is a <0.47.0 workflow and the value should be serialized
 
         serialized_succeeded_values = [
-            v.packed
-            if isinstance(v, TaskResult)
-            else serde.result_from_artifact(v, ir.ArtifactFormat.AUTO)
+            (
+                v.packed
+                if isinstance(v, TaskResult)
+                else serde.result_from_artifact(v, ir.ArtifactFormat.AUTO)
+            )
             for v in succeeded_values
         ]
 
         return dict(
             zip(
                 succeeded_inv_ids,
                 serialized_succeeded_values,
```

## orquestra/sdk/_ray/_wf_metadata.py

```diff
@@ -1,31 +1,30 @@
 ################################################################################
-# © Copyright 2023 Zapata Computing Inc.
+# © Copyright 2023 - 2024 Zapata Computing Inc.
 ################################################################################
 
 import json
 import typing as t
 
-import pydantic
-
+from .._base._storage import BaseModel
 from ..schema import ir, workflow_run
 
 
-class WfUserMetadata(pydantic.BaseModel):
+class WfUserMetadata(BaseModel):
     """Information about a workflow run we store as a Ray metadata dict.
 
     Pydantic helps us check that the thing we read from Ray is indeed a dictionary we
     set (i.e. it has proper fields).
     """
 
     # Full definition of the workflow that's being run.
     workflow_def: ir.WorkflowDef
 
 
-class InvUserMetadata(pydantic.BaseModel):
+class InvUserMetadata(BaseModel):
     """Information about a task invocation we store as a Ray metadata dict.
 
     Pydantic helps us check that the thing we read from Ray is indeed a dictionary we
     set (i.e. it has proper fields).
     """
 
     # Invocation ID. Scoped to a single workflow def. Allows to distinguish
@@ -36,8 +35,8 @@
     # (Hopefully) globally unique identifier of as single task execution. Allows
     # to distinguish invocations of the same task across workflow runs.
     task_run_id: workflow_run.TaskRunId
 
 
 def pydatic_to_json_dict(pydantic_obj) -> t.Dict[str, t.Any]:
     """Produces a JSON-serializable dict."""
-    return json.loads(pydantic_obj.json())
+    return json.loads(pydantic_obj.model_dump_json())
```

## orquestra/sdk/dremio/_api.py

```diff
@@ -19,15 +19,15 @@
     To obtain the values described above, follow these steps:
 
     #. Open Orquestra Portal—open your cluster's URI in a web browser.
     #. Select workspace.
     #. Open "Connectors".
     #. Click "Connect" on the "Dremio" tab.
     #. Click "Copy Flight Endpoint". This is the value for your ``ORQ_DREMIO_URI``.
-    #. Click "Lanuch".
+    #. Click "Launch".
     #. Inside Dremio, go to settings and configure your user account.
        ``ORQ_DREMIO_USER`` is your Dremio account email.
        ``ORQ_DREMIO_PASS`` is your Dremio account password.
     """
 
     @classmethod
     def from_env_vars(cls) -> "DremioClient":
```

## orquestra/sdk/examples/exportable_wf.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2022 - 2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Example of a workflow file that can be exported to v1 (yaml) format and run on
 Orquestra cluster.
 """  # noqa: D205
 import typing as t
 
 # public API
@@ -51,12 +51,12 @@
 
 def capitalize_no_decorator(text: str) -> str:
     return text.capitalize()
 
 
 def main():
     # output the intermediate workflow representation as a JSON
-    print(my_workflow.model.json())
+    print(my_workflow.model.model_dump_json())
 
 
 if __name__ == "__main__":
     main()
```

## orquestra/sdk/mlflow/_connection_utils.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2023 Zapata Computing Inc.
+# © Copyright 2023 - 2024 Zapata Computing Inc.
 ################################################################################
 
 """Utilities for communicating with mlflow."""
 
 import os
 import typing as t
 import warnings
```

## orquestra/sdk/schema/configs.py

```diff
@@ -1,14 +1,14 @@
 ################################################################################
-# © Copyright 2023 Zapata Computing Inc.
+# © Copyright 2023 - 2024 Zapata Computing Inc.
 ################################################################################
 from enum import Enum
 from typing import Any, Dict, Literal
 
-from pydantic.main import BaseModel
+from .._base._storage import BaseModel
 
 CONFIG_FILE_CURRENT_VERSION = "0.0.2"
 
 ConfigName = str
 
 
 class RuntimeName(str, Enum):
```

## orquestra/sdk/schema/ir.py

```diff
@@ -1,22 +1,24 @@
 ################################################################################
-# © Copyright 2021-2023 Zapata Computing Inc.
+# © Copyright 2021 - 2024 Zapata Computing Inc.
 ################################################################################
 """Workflow Intermediate Representation.
 
 The classes here are used only for purposes of schema definition. Every data
 structure here is JSON-serializable.
 """
 
 import enum
 import typing as t
 import warnings
 
 import pydantic
-from pydantic import BaseModel
+from typing_extensions import Annotated
+
+from .._base._storage import BaseModel, GpuResourceType, field_validator
 
 ImportId = str
 SecretNodeId = str
 
 
 class SecretNode(BaseModel):
     """A reference to a secret stored in an external secret/config service."""
@@ -36,31 +38,31 @@
     # This is used locally and remotely to get a secret from a specific workspace
     workspace_id: t.Optional[str] = None
 
 
 class GitURL(BaseModel):
     original_url: str
     protocol: str
-    user: t.Optional[str]
-    password: t.Optional[SecretNode]
+    user: t.Optional[str] = None
+    password: t.Optional[SecretNode] = None
     host: str
-    port: t.Optional[int]
+    port: t.Optional[int] = None
     path: str
-    query: t.Optional[str]
+    query: t.Optional[str] = None
 
 
 class GitImport(BaseModel):
     id: ImportId
     repo_url: GitURL
     git_ref: str
 
     # we need this in the JSON to know which class to use when deserializing
-    type: str = pydantic.Field(default="GIT_IMPORT", const=True)
+    type: t.Literal["GIT_IMPORT"] = "GIT_IMPORT"
 
-    @pydantic.validator("repo_url", pre=True)
+    @field_validator("repo_url", mode="before")
     def _backwards_compatible_repo_url(cls, v):
         """Allows older models with a string URL to be imported."""
         # Prevent circular imports
         from .._base._git_url_utils import parse_git_url
 
         if not isinstance(v, str):
             return v
@@ -73,20 +75,20 @@
 
     (e.g. not committed to any git repo).
     """
 
     id: ImportId
 
     # we need this in the JSON to know which class to use when deserializing
-    type: str = pydantic.Field(default="LOCAL_IMPORT", const=True)
+    type: t.Literal["LOCAL_IMPORT"] = "LOCAL_IMPORT"
 
 
 class InlineImport(BaseModel):
     id: ImportId
-    type: str = pydantic.Field(default="INLINE_IMPORT", const=True)
+    type: t.Literal["INLINE_IMPORT"] = "INLINE_IMPORT"
 
 
 class PackageSpec(BaseModel):
     # noqa E501
     """Representation of single package import.
 
     The fields in this class are based on:
@@ -107,15 +109,15 @@
     id: ImportId
 
     # List of pip packages
     packages: t.List[PackageSpec]
     # List of pip options to put at start of the requirements
     pip_options: t.List[str]
 
-    type: str = pydantic.Field(default="PYTHON_IMPORT", const=True)
+    type: t.Literal["PYTHON_IMPORT"] = "PYTHON_IMPORT"
 
 
 # If we need more import types, add them here.
 Import = t.Union[GitImport, LocalImport, PythonImports, InlineImport]
 
 
 TaskDefId = str
@@ -127,48 +129,49 @@
     function_name: str
 
     # Needed by Orquestra Studio to power jump-to-definition.
     file_path: t.Optional[str] = None
     line_number: t.Optional[int] = None
 
     # We need this in the JSON to know which class to use when deserializing
-    type: str = pydantic.Field(default="MODULE_FUNCTION_REF", const=True)
+    type: t.Literal["MODULE_FUNCTION_REF"] = "MODULE_FUNCTION_REF"
 
 
 class FileFunctionRef(BaseModel):
     # Required to dereference function for execution.
     file_path: str
     function_name: str
 
     # Needed by Orquestra Studio to power jump-to-definition. `file_path` can
     # be used for both execution and jump-to-definition.
     line_number: t.Optional[int] = None
 
     # We need this in the JSON to know which class to use when deserializing
-    type: str = pydantic.Field(default="FILE_FUNCTION_REF", const=True)
+    type: t.Literal["FILE_FUNCTION_REF"] = "FILE_FUNCTION_REF"
 
 
 class InlineFunctionRef(BaseModel):
     function_name: str
     # Required to dereference function for execution. The function object is serialized
     # using `dill`, base64-encoded, and chunked to workaround JSON string length limits.
     encoded_function: t.List[str]
 
     # We need this in the JSON to know which class to use when deserializing
-    type: str = pydantic.Field(default="INLINE_FUNCTION_REF", const=True)
+    type: t.Literal["INLINE_FUNCTION_REF"] = "INLINE_FUNCTION_REF"
 
 
 FunctionRef = t.Union[ModuleFunctionRef, FileFunctionRef, InlineFunctionRef]
 
 
 class Resources(BaseModel):
     cpu: t.Optional[str] = None
     memory: t.Optional[str] = None
     disk: t.Optional[str] = None
-    gpu: t.Optional[str] = None
+    gpu: GpuResourceType = None
+
     # nodes should be a positive integer representing the number of nodes assigned
     # to a workflow. If None, the runtime will choose.
     # This only applies to workflows and not tasks.
     nodes: t.Optional[int] = None
 
 
 class DataAggregation(BaseModel):
@@ -218,15 +221,15 @@
     # `source_import_id` & `dependency_import_ids` are references to imports defined in
     # Workflow.imports.
 
     # List of abstract inputs that the task requires, but without the values yet.
     # Kinda like function signature.
     # None means we do not know the parameters for this Task (e.g. an external task)
     # An empty list [] means a Task with no parameters
-    parameters: t.Optional[t.List[TaskParameter]]
+    parameters: t.Optional[t.List[TaskParameter]] = None
 
     # Statically inferred from the task function. See also `TaskOutputMetadata`'s
     # docstring.
     # 'None' for IRs generated with orquestra-sdk<=0.45.1. Not empty for newer SDK
     # versions.
     output_metadata: t.Optional[TaskOutputMetadata] = None
 
@@ -301,18 +304,15 @@
     """
 
     # Workflow-scope unique ID used to refer from task invocations
     id: ConstantNodeId
 
     # Serialized value
     value: str
-    serialization_format: ArtifactFormat = pydantic.Field(
-        default=ArtifactFormat.JSON,
-        const=True,
-    )
+    serialization_format: t.Literal[ArtifactFormat.JSON] = ArtifactFormat.JSON
 
     # Human-readable string that can be rendered on the UI to represent the value.
     value_preview: pydantic.constr(max_length=12)  # type: ignore
 
 
 class ConstantNodePickle(BaseModel):
     """Piece of data that already exists at workflow submission time.
@@ -323,18 +323,17 @@
     """
 
     # Workflow-scope unique ID used to refer from task invocations
     id: ConstantNodeId
 
     # Serialized value
     chunks: t.List[str]
-    serialization_format: ArtifactFormat = pydantic.Field(
-        default=ArtifactFormat.ENCODED_PICKLE,
-        const=True,
-    )
+    serialization_format: t.Literal[
+        ArtifactFormat.ENCODED_PICKLE
+    ] = ArtifactFormat.ENCODED_PICKLE
 
     # Human-readable string that can be rendered on the UI to represent the value.
     value_preview: pydantic.constr(max_length=12)  # type: ignore
 
 
 # General ConstantNode that can hold constants that are not JSON-serializable
 ConstantNode = t.Union[ConstantNodeJSON, ConstantNodePickle]
@@ -355,19 +354,19 @@
     # Value: id of the constant or artifact that will contain the value at runtime.
     kwargs_ids: t.Dict[ParameterName, ArgumentId]
 
     # Where to store the returned values.
     output_ids: t.List[ArtifactNodeId]
 
     # TaskInvocation specific resources
-    resources: t.Optional[Resources]
+    resources: t.Optional[Resources] = None
 
     # Specification for custom image more scoped than TaskDef custom_image field
     # if not set, will fall back to TaskDef custom_image
-    custom_image: t.Optional[str]
+    custom_image: t.Optional[str] = None
 
 
 WorkflowDefName = str
 
 
 class Version(BaseModel):
     original: str
@@ -408,24 +407,26 @@
     # of arguments or returned values.
     task_invocations: t.Dict[TaskInvocationId, TaskInvocation]
 
     # IDs of the nodes that are considered as workflow outputs. The referenced nodes
     # can be constants or artifacts.
     output_ids: t.List[ArgumentId]
 
-    data_aggregation: t.Optional[DataAggregation]
+    data_aggregation: t.Optional[DataAggregation] = None
 
     # Metadata defaults to None to allow older JSON to be loaded
-    metadata: t.Optional[WorkflowMetadata] = None
+    metadata: Annotated[
+        t.Optional[WorkflowMetadata], pydantic.Field(validate_default=True)
+    ] = None
 
     # The resources that are available for the workflow to use.
     # If none, the runtime will decide.
     resources: t.Optional[Resources] = None
 
-    @pydantic.validator("metadata", always=True)
+    @field_validator("metadata", mode="after")
     def sdk_version_up_to_date(cls, v: t.Optional[WorkflowMetadata]):
         # Workaround for circular imports
         from orquestra.sdk import exceptions
         from orquestra.sdk.packaging import _versions
         from orquestra.sdk.schema import _compat
 
         current_version = _versions.get_current_sdk_version()
```

## orquestra/sdk/schema/responses.py

```diff
@@ -1,22 +1,23 @@
 ################################################################################
-# © Copyright 2022-2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Models for responses from the CLI.
 
 The classes here are used only for purposes of schema definition. Every data
 structure here is JSON-serializable.
 """
 
 import enum
 import typing as t
 
-from pydantic import BaseModel, Field
+from pydantic import Field
 from typing_extensions import Annotated
 
+from .._base._storage import BaseModel
 from .ir import ArtifactFormat
 
 
 class ResponseFormat(enum.Enum):
     PLAIN_TEXT = "text"
     JSON = "json"
     DEFAULT = PLAIN_TEXT
```

## orquestra/sdk/schema/workflow_run.py

```diff
@@ -1,25 +1,25 @@
 ################################################################################
-# © Copyright 2022 - 2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Workflow Run model.
 
 The classes here are used only for purposes of schema definition. Every data
 structure here is JSON-serializable.
 """
 
 import enum
 import typing as t
 import warnings
 
-from pydantic import BaseModel
-
 from orquestra.sdk._base._dates import Instant
 from orquestra.sdk.schema.ir import TaskInvocationId, WorkflowDef
 
+from .._base._storage import BaseModel
+
 WorkflowRunId = str
 TaskRunId = str
 WorkspaceId = str
 ProjectId = str
 
 
 class State(enum.Enum):
```

## orquestra/sdk/secrets/_client.py

```diff
@@ -1,9 +1,9 @@
 ################################################################################
-# © Copyright 2022 - 2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Code for accessing the Config Service API.
 
 API spec: https://github.com/zapatacomputing/config-service/tree/main/openapi/src
 """
 import typing as t
 from urllib.parse import urljoin
@@ -120,15 +120,15 @@
             raise _exceptions.SecretNotFoundError(secret_name=name)
 
         try:
             _handle_common_errors(resp)
         except (_exceptions.InvalidTokenError, _exceptions.UnknownHTTPError):
             raise
 
-        return SecretDefinition.parse_obj(resp.json()["data"]["details"])
+        return SecretDefinition.model_validate(resp.json()["data"]["details"])
 
     def list_secrets(
         self, workspace_id: t.Optional[WorkspaceId]
     ) -> t.Sequence[SecretNameObj]:
         """List the available secrets.
 
         Args:
@@ -138,24 +138,26 @@
             orquestra.sdk.secrets._exceptions.InvalidTokenError: when the authorization
                 token is rejected by the remote cluster.
             orquestra.sdk.secrets._exceptions.UnknownHTTPError: when any other error is
                 raised by the remote cluster.
         """
         resp = self._get(
             API_ACTIONS["list_secrets"],
-            query_params=ListSecretsRequest(workspace=workspace_id).dict()
-            if workspace_id
-            else None,
+            query_params=(
+                ListSecretsRequest(workspace=workspace_id).model_dump()
+                if workspace_id
+                else None
+            ),
         )
         try:
             _handle_common_errors(resp)
         except (_exceptions.InvalidTokenError, _exceptions.UnknownHTTPError):
             raise
 
-        return [SecretNameObj.parse_obj(d) for d in resp.json()["data"]]
+        return [SecretNameObj.model_validate(d) for d in resp.json()["data"]]
 
     # --- mutations ---
 
     def create_secret(self, new_secret: SecretDefinition):
         """Post a new secret.
 
         Args:
@@ -167,15 +169,15 @@
             orquestra.sdk.secrets._exceptions.InvalidTokenError: when the authorization
                 token is rejected by the remote cluster.
             orquestra.sdk.secrets._exceptions.UnknownHTTPError: when any other error is
                 raised by the remote cluster
         """
         resp = self._post(
             API_ACTIONS["create_secret"],
-            body_params={"data": new_secret.dict()},
+            body_params={"data": new_secret.model_dump()},
         )
 
         if resp.status_code == codes.BAD_REQUEST:
             raise _exceptions.SecretAlreadyExistsError(secret_name=new_secret.name)
 
         try:
             _handle_common_errors(resp)
@@ -196,15 +198,15 @@
                 token is rejected by the remote cluster.
             orquestra.sdk.secrets._exceptions.UnknownHTTPError: when any other error is
                 raised by the remote cluster
         """
         obj = SecretValueObj(value=value)
         resp = self._post(
             API_ACTIONS["update_secret"].format(name),
-            body_params={"data": obj.dict()},
+            body_params={"data": obj.model_dump()},
         )
 
         if resp.status_code == codes.NOT_FOUND:
             raise _exceptions.SecretNotFoundError(secret_name=name)
 
         try:
             _handle_common_errors(resp)
```

## orquestra/sdk/secrets/_models.py

```diff
@@ -1,59 +1,59 @@
 ################################################################################
-# © Copyright 2022 - 2023 Zapata Computing Inc.
+# © Copyright 2022 - 2024 Zapata Computing Inc.
 ################################################################################
 """Models for accessing the Config Service API.
 
 API spec: https://github.com/zapatacomputing/config-service/tree/main/openapi/src
 """
 from typing import Optional
 
-import pydantic
+from .._base._storage import BaseModel
 
 SecretName = str
 SecretValue = str
 ResourceGroup = str
 WorkspaceId = str
 
 
-class SecretNameObj(pydantic.BaseModel):
+class SecretNameObj(BaseModel):
     """
     Model for:
     https://github.com/zapatacomputing/config-service/blob/3f275a52149fb2b74c6a8c01726cce4f390a1533/openapi/src/schemas/SecretName.yaml.
 
     Named 'SecretNameObj' instead of 'SecretName' to avoid clash with the field type.
     alias.
     """  # noqa: D205, D212
 
     name: SecretName
 
 
-class SecretValueObj(pydantic.BaseModel):
+class SecretValueObj(BaseModel):
     """
     Model for:
     https://github.com/zapatacomputing/config-service/blob/3f275a52149fb2b74c6a8c01726cce4f390a1533/openapi/src/schemas/SecretValue.yaml.
 
     Named 'SecretValueObj' instead of 'SecretValue' to avoid clash with the field type.
     alias.
     """  # noqa: D205, D212
 
     value: SecretValue
 
 
-class SecretDefinition(pydantic.BaseModel):
+class SecretDefinition(BaseModel):
     """
     Model for:
     https://github.com/zapatacomputing/config-service/blob/3f275a52149fb2b74c6a8c01726cce4f390a1533/openapi/src/schemas/SecretDefinition.yaml.
     """  # noqa: D205, D212
 
     name: SecretName
     value: SecretValue
-    resourceGroup: Optional[ResourceGroup]
+    resourceGroup: Optional[ResourceGroup] = None
 
 
-class ListSecretsRequest(pydantic.BaseModel):
+class ListSecretsRequest(BaseModel):
     """
     Model for:
     https://github.com/zapatacomputing/config-service/blob/fbfc4627450bc9a460278b242738e55210e7bf03/openapi/src/parameters/query/workspace.yaml.
     """  # noqa: D205, D212
 
     workspace: WorkspaceId
```

## Comparing `orquestra_sdk-0.61.0.dist-info/METADATA` & `orquestra_sdk-0.62.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-Metadata-Version: 2.1
+Metadata-Version: 2.3
 Name: orquestra-sdk
-Version: 0.61.0
+Version: 0.62.0
 Summary: Compose Orquestra workflows using a Python DSL
 Project-URL: Homepage, https://github.com/zapatacomputing/orquestra-workflow-sdk
 Project-URL: Documentation, https://docs.orquestra.io
 Project-URL: Repository, https://github.com/zapatacomputing/orquestra-workflow-sdk.git
 Project-URL: Issues, https://github.com/zapatacomputing/orquestra-workflow-sdk/issues
 Author-email: "Zapata Computing Inc." <info@zapatacomputing.com>
 License:                                  Apache License
@@ -212,35 +212,35 @@
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering
 Requires-Python: >=3.8
 Requires-Dist: aiohttp>=3.9.0
 Requires-Dist: argcomplete
-Requires-Dist: click!=8.1.4,~=8.0
+Requires-Dist: click!=8.1.4,>=8.0
 Requires-Dist: cloudpickle==2.2.1
-Requires-Dist: cloup~=2.0
+Requires-Dist: cloup>=2.0
 Requires-Dist: dill==0.3.6
 Requires-Dist: filelock>=3.3.2
 Requires-Dist: gitpython
 Requires-Dist: graphviz
-Requires-Dist: importlib-metadata~=4.0
-Requires-Dist: inquirer~=3.0
+Requires-Dist: importlib-metadata>=4.0
+Requires-Dist: inquirer>=3.0
 Requires-Dist: packaging>=21
 Requires-Dist: pandas>=1.4
-Requires-Dist: pip-api==0.0.30
+Requires-Dist: pip-api>=0.0.30
 Requires-Dist: pyarrow>=10.0
-Requires-Dist: pydantic<2,>=1.10.8
-Requires-Dist: pyjwt~=2.6
-Requires-Dist: requests~=2.28
-Requires-Dist: rich~=13.5
+Requires-Dist: pydantic>1.10.7
+Requires-Dist: pyjwt>=2.6
+Requires-Dist: requests>=2.28
+Requires-Dist: rich>=13.5
 Requires-Dist: tabulate
 Requires-Dist: typing-extensions>=4.1.0
 Requires-Dist: wrapt
-Requires-Dist: wurlitzer~=3.0
+Requires-Dist: wurlitzer>=3.0
 Provides-Extra: all
 Requires-Dist: orquestra-sdk[ray]; extra == 'all'
 Provides-Extra: dev
 Requires-Dist: black~=23.7; extra == 'dev'
 Requires-Dist: diff-cover; extra == 'dev'
 Requires-Dist: flake8-pyproject>=1.2.3; extra == 'dev'
 Requires-Dist: flake8>=4.0.0; extra == 'dev'
@@ -253,16 +253,16 @@
 Requires-Dist: pydoclint; extra == 'dev'
 Requires-Dist: pymarkdownlnt; extra == 'dev'
 Requires-Dist: pyright!=1.1.340,!=1.1.341; extra == 'dev'
 Requires-Dist: pytest-cov>=2.12; extra == 'dev'
 Requires-Dist: pytest-dependency; extra == 'dev'
 Requires-Dist: pytest-httpserver; extra == 'dev'
 Requires-Dist: pytest-timeout>=2.0.0; extra == 'dev'
-Requires-Dist: pytest~=6.2; extra == 'dev'
-Requires-Dist: responses!=0.24,~=0.20; extra == 'dev'
+Requires-Dist: pytest>=6.2; extra == 'dev'
+Requires-Dist: responses!=0.24,>=0.20; extra == 'dev'
 Requires-Dist: ruff; extra == 'dev'
 Requires-Dist: scikit-learn; extra == 'dev'
 Requires-Dist: types-psutil; extra == 'dev'
 Requires-Dist: types-pygments; extra == 'dev'
 Requires-Dist: types-requests; extra == 'dev'
 Requires-Dist: types-setuptools; extra == 'dev'
 Requires-Dist: types-tabulate; extra == 'dev'
@@ -277,15 +277,14 @@
 Requires-Dist: sphinx-togglebutton==0.3.2; extra == 'docs'
 Requires-Dist: sphinx<7.0,>6.0; extra == 'docs'
 Requires-Dist: sphinxcontrib-autoprogram==0.1.8; extra == 'docs'
 Requires-Dist: sphinxcontrib-youtube==1.2.0; extra == 'docs'
 Requires-Dist: sphinxemoji==0.2.0; extra == 'docs'
 Provides-Extra: ray
 Requires-Dist: async-timeout; extra == 'ray'
-Requires-Dist: pyarrow; extra == 'ray'
 Requires-Dist: ray[default]==2.9.0; extra == 'ray'
 Description-Content-Type: text/markdown
 
 # Orquestra Workflow SDK
 
 ## What is it?
```

## Comparing `orquestra_sdk-0.61.0.dist-info/licenses/LICENSE` & `orquestra_sdk-0.62.0.dist-info/licenses/LICENSE`

 * *Files identical despite different names*

## Comparing `orquestra_sdk-0.61.0.dist-info/RECORD` & `orquestra_sdk-0.62.0.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 orquestra/sdk/__init__.py,sha256=-U6jbKwuTgUGDZImeRg3AGysjCwi1ekfe5hQZSxWk9M,1911
 orquestra/sdk/exceptions.py,sha256=6Nk1hj-MynAS0aIUE1KMG7X_7aBIhPANA5CGkbG2ZKA,10179
 orquestra/sdk/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 orquestra/sdk/_base/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
 orquestra/sdk/_base/_ast.py,sha256=jEMiu5rpT_yGYPKwOoZuS9Bf17VYKra5jm9PbGgDysQ,8254
-orquestra/sdk/_base/_config.py,sha256=0YezB_LFKnSrC07vvHnfVt4YIW6gD8Zo1o2KlNp-V6Q,16463
+orquestra/sdk/_base/_config.py,sha256=i090dkDJ93VdKZ3VXaBxTCVBHbC7bL_rORY_lB1Tgv8,16509
 orquestra/sdk/_base/_dates.py,sha256=X4tviCvHYWnoNQiWboZyOOWMu8GA7hehydD85pF_OZU,2880
 orquestra/sdk/_base/_dsl.py,sha256=o3zKLkcXctE1lbUm1meLFDmOWBlK4WohIc3VgDyaMSU,42709
 orquestra/sdk/_base/_env.py,sha256=RQ_FXHcTBHuCSEk9jPfQA8PQAz5iUxPg6kgtUmWdYQ0,3512
 orquestra/sdk/_base/_exec_ctx.py,sha256=IynhETUYMjDbBDXE5vLJxxtRS-5byuyY_S4_vEw2nBM,2155
 orquestra/sdk/_base/_factory.py,sha256=eQcSU96XvOnYpomjxDjjeNwqS2wC55uq0WjxfiVUgxU,2577
 orquestra/sdk/_base/_git_url_utils.py,sha256=nqt46uxslor-kxgkhPYKcdUABhfQucnpdWcHEuFHnQc,3774
 orquestra/sdk/_base/_graphs.py,sha256=CrVtMkIIv9y7Rg2nHL0z2lClC3_Hy5cm4HgTKoHPL5E,3858
@@ -16,34 +16,36 @@
 orquestra/sdk/_base/_regex.py,sha256=lPsAFfR4b-PWfuzXiyTfw0y3F3lTXv4MUDIyClbAFUI,1144
 orquestra/sdk/_base/_retry.py,sha256=zRYVUAAtpzV_w-tv3AuKPL2xYmoPb50VoIc-Q2qzLEo,1404
 orquestra/sdk/_base/_services.py,sha256=DgpImHoEBNlcgP511SZUXzj_jPqHB9ogQ_ThDlUm9bk,4948
 orquestra/sdk/_base/_traversal.py,sha256=8SyZMbuubAVEyt20fSthBV26fdagaDugcUAhlHobmTQ,32576
 orquestra/sdk/_base/_viz.py,sha256=JgtO4oHX1zcZZZOzu2ru4IKw8wpLMhtPYBTm-chUVVo,4608
 orquestra/sdk/_base/_workflow.py,sha256=11HwUf9kCJf29YX0FIoMJ5M7-xAOq2nsmC2ZKi1P40Q,23964
 orquestra/sdk/_base/abc.py,sha256=v59pBUBDiwRyLP5OtT7hkakKPJxfMACtGZ-aCyKHOwQ,8458
-orquestra/sdk/_base/dispatch.py,sha256=3nutW_xSeZ5qSTrwciTKfOPMaJYnXmHyog3luBKAvxU,4809
+orquestra/sdk/_base/dispatch.py,sha256=QNCmilmACPwL1rJSxA24e5VQeCp_Sf2VFU8TzIas7Mw,4826
 orquestra/sdk/_base/loader.py,sha256=-ijGK_9zyUX6LHaMiSS13TpMDxE0SF8Vb0ecbFekMQY,6168
-orquestra/sdk/_base/serde.py,sha256=80Wdrcdj98icdo0mULImc0LNfQHFvXhMVvmGTwYS9sc,7243
+orquestra/sdk/_base/serde.py,sha256=3WAJLgb9Jh1i0OaNP5RrH-Fgj2qdxYUCRGk5y_f_Pqc,7032
 orquestra/sdk/_base/_api/__init__.py,sha256=wghBSXivpMof8K_CECbWz3culPm5EhmRqobDyYFoY5w,763
 orquestra/sdk/_base/_api/_config.py,sha256=D6CmuDwYIIxkDTOyXTE4NSsd21gf_wFYbLPICOjT6qI,14594
 orquestra/sdk/_base/_api/_task_run.py,sha256=K7pHWhz_pdP7aFU6reaSfRBJu9ccJI2CHFeP1y0o2JM,12860
 orquestra/sdk/_base/_api/_wf_run.py,sha256=yyhrU9hEixjE_gvVnTnmrr52nUaWRp-mayYBlbVZgig,32163
 orquestra/sdk/_base/_driver/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
 orquestra/sdk/_base/_driver/_ce_runtime.py,sha256=PnoJ0fWurXYu-lODmLNpNR6A4ED5VA5VcN2qWGvkf60,30006
-orquestra/sdk/_base/_driver/_client.py,sha256=9L-JbJC-GJFZrUA5et6r7Bl5wuN9xtoVdeICUUoUHNw,51851
+orquestra/sdk/_base/_driver/_client.py,sha256=LkTRHP5FzJYgRm7nfqUrGM1KfbIBq08N0yKwwgYwVeM,51898
 orquestra/sdk/_base/_driver/_exceptions.py,sha256=eX2Cd_rIYwgmX0-0AtmIbpAgFw0WbcN7PtfGDX0UmZM,6062
-orquestra/sdk/_base/_driver/_models.py,sha256=FVTCuCnSrCoz2PtRnmcJqfB-k4FDhRkGKNO-lVePi3o,17822
+orquestra/sdk/_base/_driver/_models.py,sha256=YkqMU9tHruP-CjZO3debh0BKt2GxZKPm5OMkz34pT5w,17690
 orquestra/sdk/_base/_logs/_interfaces.py,sha256=Qhxw442jY9Lhs0b4m_P6N71gH6tEN72v3sTxgUA4aAk,3247
 orquestra/sdk/_base/_logs/_markers.py,sha256=jf2PZmXdvH5jhRzg3OWwraPpRuF78aOxqMwRfxffHuE,6791
 orquestra/sdk/_base/_logs/_models.py,sha256=nen_w1ik1p6_3-4TXqxW67wKFG46FtyJ3GfqlNHESKo,1326
 orquestra/sdk/_base/_logs/_regrouping.py,sha256=YLXsHuMvLQyt5ypbW1vAsdeUVB1WTmth5WA0RyMIUk8,849
 orquestra/sdk/_base/_spaces/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
 orquestra/sdk/_base/_spaces/_api.py,sha256=dBEYBv1EC3FoLHVQ9Nmyh3TtCLhFGH63-eIlj0jUrdU,2728
 orquestra/sdk/_base/_spaces/_resolver.py,sha256=IgB2PmgU5tBHtsWnhnvWimUXf3lU9gWTiArgOg2EPQ4,2357
 orquestra/sdk/_base/_spaces/_structs.py,sha256=8EdEm9QOQ7tm2aUSkAIycSHOoiuSmGANCn29wPXWcMw,704
+orquestra/sdk/_base/_storage/__init__.py,sha256=OsoXm3KCXMwrh99mpDYeMzIQLJy61uHZ2-YVp2-_tHs,383
+orquestra/sdk/_base/_storage/orqdantic.py,sha256=KjTtgvlCHASTeNZS3v-eRSDjo3XeQZYpZCOGwzomU2g,4496
 orquestra/sdk/_base/_testing/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
 orquestra/sdk/_base/_testing/_connections.py,sha256=4ik5Ok6s4CKs2o93oWqs0sjTR8Uee_oNLbEbMeEk5_A,4061
 orquestra/sdk/_base/_testing/_example_wfs.py,sha256=tp93cW7jdQGl_MBVeYOpLBNhhKHS8pCH-e84UeMzXfw,9430
 orquestra/sdk/_base/_testing/_ipc.py,sha256=jDpwNAfGBuYj-Vf8k5C-8JQnFvoBVldkEswshfnvIzY,1754
 orquestra/sdk/_base/_testing/_long_import.py,sha256=CQ2vLXnvZmmSrX0c4vi9agcWVox7S7Bxuy7eouQ_FZA,261
 orquestra/sdk/_base/_testing/_reloaders.py,sha256=Kf1aMo-bolm4uAK3tZkjirz5txbix-3NB0ghRKTc2P4,950
 orquestra/sdk/_base/cli/_arg_resolvers.py,sha256=GuxLbzRhL4T1z0KxrcdJeM9_W7Dj0SsOTEMvHh41eDQ,18206
@@ -57,54 +59,54 @@
 orquestra/sdk/_base/cli/_services/_down.py,sha256=S-XFErTJ1izBvO92hZ6I7VL4us2stndhu_qsUr7-yOY,2645
 orquestra/sdk/_base/cli/_services/_status.py,sha256=XTLcjAUm98bQTZfUghZe0nZUOkXBkIVTPo2W7lVHe4Y,1323
 orquestra/sdk/_base/cli/_services/_up.py,sha256=5hi1a0msT0cFawPuJKp18SZKk5SnDQqwnbvk_48cQzU,2574
 orquestra/sdk/_base/cli/_task/_logs.py,sha256=9kQhDGDCT3G-T3IX6IHPM7NNN49v2cqXG1sdHoqR2Vk,3226
 orquestra/sdk/_base/cli/_task/_results.py,sha256=CAT8sGTGQnjqZdYjNyCa5IA__RWNukshyiOHRKXSOd4,3614
 orquestra/sdk/_base/cli/_ui/__init__.py,sha256=q15KdpaIU_GcUihbkqOAHL6l5Fs0DLWV_F-AZ-M3Wl4,373
 orquestra/sdk/_base/cli/_ui/_click_default_group.py,sha256=ZwRAztjPw1bhTvPBEtY8VdEqKXcjUNGC5HQg-apobxg,6308
-orquestra/sdk/_base/cli/_ui/_errors.py,sha256=Sa-zq5Q2hRHed6uWEg62VeAZMKJJW7xugjxTqBBOq_4,7125
+orquestra/sdk/_base/cli/_ui/_errors.py,sha256=JfnnCsG_vEt29hHTJV_XAlAeSmTbw8rdKRjNYKwp2x4,8358
 orquestra/sdk/_base/cli/_ui/_models.py,sha256=ZleNCrTj7bS_nhSMcSzZXZs4cNzgJ9KkJX5IwT5q2PU,1686
 orquestra/sdk/_base/cli/_ui/_presenters.py,sha256=06-yaIZO9UwgUF4rLIiG7fzPl5Xzln8nYAl0Jx4Lne8,17739
 orquestra/sdk/_base/cli/_ui/_prompts.py,sha256=Hk2u89FgtJJUYGWy0HjZoukUI_4pULHaP1a8ILxE1J4,10907
 orquestra/sdk/_base/cli/_workflow/_list.py,sha256=fb5nREzz7hml7isdxn6FCPpki2dioTI3gjz0q8h1pyE,3983
 orquestra/sdk/_base/cli/_workflow/_logs.py,sha256=n-_HnQLhDrUvdYNxrSDEoaA8yZAVNAvEMtgYRARHZfc,4105
 orquestra/sdk/_base/cli/_workflow/_results.py,sha256=amozsdLnJd3GnbX8w0nASPc2NAqi_WpvMUJOQNNdiC8,3128
 orquestra/sdk/_base/cli/_workflow/_stop.py,sha256=7hWJl0cfumsQpctDAqE1LO4yzIBF2QR54nfNga6qP30,2594
 orquestra/sdk/_base/cli/_workflow/_submit.py,sha256=aqUYijdAdv3-XKatodGL9Rpa6gQ10X30oz9r8gsxS20,5198
 orquestra/sdk/_base/cli/_workflow/_view.py,sha256=f5KJZJgc-ls76OVZonGmJsrpe4odmf5iOiQlr6DkN6Y,2132
 orquestra/sdk/_ray/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
-orquestra/sdk/_ray/_build_workflow.py,sha256=iiSt7m9NOuZBVU0iCmh0PnJisMuiy3xDKHaU_V6UBYo,25756
+orquestra/sdk/_ray/_build_workflow.py,sha256=I_JOGPoWk0R5teujkCLB5-e8Nk96RghM4bF2ib91mWo,25840
 orquestra/sdk/_ray/_client.py,sha256=f-Fl_0j-GBv-ELY401w-0z6DAG4CvdnxpTALXO_E3E0,8255
-orquestra/sdk/_ray/_dag.py,sha256=ssmwzl88gCw1D0N2I2mJtxsxeufXAYrWi2bS7EPuoNQ,31768
+orquestra/sdk/_ray/_dag.py,sha256=PyvEvC0PS-_AQdzoORved4eaQ3mgb7ENNNl6telnfmQ,30665
 orquestra/sdk/_ray/_id_gen.py,sha256=59hPSI9DB6DacE-uwtKnbjX1B10mj3MM_Cq6PIujY8k,946
 orquestra/sdk/_ray/_ray_logs.py,sha256=foeeFJ-uzMTqPHBtNS_mimf10r5x-VqkGzGvoHdKhNQ,9527
-orquestra/sdk/_ray/_wf_metadata.py,sha256=Or3H-ZpKsr8WdhgjU2aMuWcG2xHC9S0SlYWJamxtr1E,1460
+orquestra/sdk/_ray/_wf_metadata.py,sha256=9fWjJtwc0Yi5AOmiL4a4JVkIM_mG9oCjQg57k86MttU,1482
 orquestra/sdk/dremio/__init__.py,sha256=s21wyNLd2PkcYuA7qNJBtqxGbiwNm3DASxjK_zSLWU4,341
-orquestra/sdk/dremio/_api.py,sha256=kNNhQSZM2GVBSzXRg4_dVxL-InDULrV6foctwxHtwpQ,2916
+orquestra/sdk/dremio/_api.py,sha256=yo5JrjP67vNrX3eeqsIDerkK88WGAK0gqZZkQ45524s,2916
 orquestra/sdk/dremio/_env_var_reader.py,sha256=kY-jT7Hpjgkr0AZg6Ue86AIQqH-S2-GfdneXW0lADkY,832
 orquestra/sdk/dremio/_flight_facade.py,sha256=fAFl4cIV9Offuh5-2p7EPHQiPyNWRlOZy2fvew42sS4,662
 orquestra/sdk/examples/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
-orquestra/sdk/examples/exportable_wf.py,sha256=akPKVKSltqFtU_SxuSc71Srflni3TepwkoODJqHugPo,1579
+orquestra/sdk/examples/exportable_wf.py,sha256=Shcb-nc6vB21smrUwi5rgqW43RKGYnBiQ-67wC3dx-0,1590
 orquestra/sdk/examples/workflow_defs.py,sha256=tmAPpDVM7-6szg_6RznA7fOO7UtqDcLR2iosMkjIflA,1075
 orquestra/sdk/kubernetes/__init__.py,sha256=fmqiLNRiTWLjlWlynWbcgaBq6N5YkP-qoM2qgZluuM4,318
 orquestra/sdk/kubernetes/quantity.py,sha256=hezpYYjew8wpooRqJXjUvd1tPQBgfrDnC9AtnBYF2wU,2449
 orquestra/sdk/mlflow/__init__.py,sha256=NlgzlPQI5iBdAnVYul0ocUXdHS6QBPr_AYWrwVE_eLU,528
-orquestra/sdk/mlflow/_connection_utils.py,sha256=ySB2AkD9oA3eLi3L3EeBAr2m253NJiJGgITOZOlovX4,8959
+orquestra/sdk/mlflow/_connection_utils.py,sha256=7UtxLqH8KPY6ik_mRkF0cmCXpb1wLw1qRAEPAOp3vRc,8966
 orquestra/sdk/packaging/__init__.py,sha256=fj3mB81q3BZa_-_o8UvipKA6Yvr41x6OtCv56p4HPls,427
 orquestra/sdk/packaging/_versions.py,sha256=iR7s4FZLNZoTTVNjg2-sIIFBTYYQ3XDbz2ZrEArigHI,4139
 orquestra/sdk/schema/__init__.py,sha256=w3lKi4G8NQ88XMU5vXiUdq-vpO0RLzNE91LrXiZ_gj8,204
 orquestra/sdk/schema/_compat.py,sha256=ojx9IuATi_UdJZt8v4ykdHezflLkQI2p-Y1fbKVlXQw,1472
-orquestra/sdk/schema/configs.py,sha256=mJcFNkECgsORMe0QoW9reJcKzNdj3NeXdV7k7qdFfAI,1620
-orquestra/sdk/schema/ir.py,sha256=BOlvPoD8g0ubgSuYn5EdUWGlCw3o-aW5PWTtdPFrvaI,15228
-orquestra/sdk/schema/responses.py,sha256=c9LleDNCExJdrV9VfOqb4RlxrIFFz2dx6jb4w3somvA,2107
-orquestra/sdk/schema/workflow_run.py,sha256=JtKMu4Yjg9m0xxLn9dw1qlvMP2UY62wEDIOCll5r8Ec,2992
+orquestra/sdk/schema/configs.py,sha256=qXNf82i7RVDyhkk74dhwOQbRRT2BKkDxv0jGugX0mu8,1630
+orquestra/sdk/schema/ir.py,sha256=2ajT9TVE_kzfrHtji_mKaPG8dOhTWbb0PNMee3oEZCU,15293
+orquestra/sdk/schema/responses.py,sha256=tXBeweGAZT01-pe_Cxyqda9P95gaDsZjzUvoDt_VXKI,2137
+orquestra/sdk/schema/workflow_run.py,sha256=PLVdYJ95x-J_OH1ZyqltEex9uE7n3Yxfi6aqfKEBF2k,3000
 orquestra/sdk/secrets/__init__.py,sha256=LKFVfYX1UNmV20EFZmNeRBKPtTAMlS5ZbIEVWnhUDcs,1102
 orquestra/sdk/secrets/_api.py,sha256=MEwoDL_WypTSkPh0VAwvfRHmy2-AleyEfFRorTJDEik,7595
 orquestra/sdk/secrets/_auth.py,sha256=iWUn01l6pkiDtndIUSitXzK11gum1lm6GWyWCJ1Dwow,2500
-orquestra/sdk/secrets/_client.py,sha256=kU8gcWBjhzwQe55LM8oASlOvIW6t8cBxZkYDwzWRZV4,8282
+orquestra/sdk/secrets/_client.py,sha256=vJoDUeXaLgPclfwYyGFNYHVgx18rNRUN8Wi4DOxGNr8,8350
 orquestra/sdk/secrets/_exceptions.py,sha256=I8Hgize2RV39k5mLxSU-FeHWQpZWThP_t6IjgQ95Fl8,1249
-orquestra/sdk/secrets/_models.py,sha256=EC9TWNWnVVE_SzN0i2sXhJjhHcKm7nePrnXUG9iUXi8,1773
-orquestra_sdk-0.61.0.dist-info/METADATA,sha256=JU-uyWpAjqEGAif5hPc635qimAldo7lPTvVtciGVqLs,17544
-orquestra_sdk-0.61.0.dist-info/WHEEL,sha256=TJPnKdtrSue7xZ_AVGkp9YXcvDrobsjBds1du3Nx6dc,87
-orquestra_sdk-0.61.0.dist-info/entry_points.txt,sha256=rsHTLDZNiyLXSSUtXteDzuvyV6gPETOXhdN1NF2oylQ,60
-orquestra_sdk-0.61.0.dist-info/licenses/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-orquestra_sdk-0.61.0.dist-info/RECORD,,
+orquestra/sdk/secrets/_models.py,sha256=WRfEUrrM4c24LzNAwISjM3CG14PP39ddsc9L6bmtkrc,1767
+orquestra_sdk-0.62.0.dist-info/METADATA,sha256=r2JHrYQGHZMuo6njS-jNgvustN4BO_gGU6gU2jNGYn8,17501
+orquestra_sdk-0.62.0.dist-info/WHEEL,sha256=uNdcs2TADwSd5pVaP0Z_kcjcvvTUklh2S7bxZMF8Uj0,87
+orquestra_sdk-0.62.0.dist-info/entry_points.txt,sha256=rsHTLDZNiyLXSSUtXteDzuvyV6gPETOXhdN1NF2oylQ,60
+orquestra_sdk-0.62.0.dist-info/licenses/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+orquestra_sdk-0.62.0.dist-info/RECORD,,
```

