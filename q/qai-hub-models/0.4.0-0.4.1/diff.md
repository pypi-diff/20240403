# Comparing `tmp/qai_hub_models-0.4.0-py3-none-any.whl.zip` & `tmp/qai_hub_models-0.4.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,912 +1,905 @@
-Zip file size: 967754 bytes, number of entries: 910
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/__init__.py
--rw-r--r--  2.0 unx      281 b- defN 24-Mar-18 23:35 qai_hub_models/_version.py
--rw-r--r--  2.0 unx      620 b- defN 24-Mar-18 23:37 qai_hub_models/asset_bases.yaml
--rw-r--r--  2.0 unx      734 b- defN 24-Mar-18 23:35 qai_hub_models/conftest.py
--rw-r--r--  2.0 unx      877 b- defN 24-Mar-18 23:35 qai_hub_models/global_requirements.txt
--rw-r--r--  2.0 unx      395 b- defN 24-Mar-18 23:35 qai_hub_models/requirements-dev.txt
--rw-r--r--  2.0 unx      426 b- defN 24-Mar-18 23:35 qai_hub_models/requirements.txt
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/datasets/__init__.py
--rw-r--r--  2.0 unx     4757 b- defN 24-Mar-18 23:35 qai_hub_models/datasets/bsd300.py
--rw-r--r--  2.0 unx     4054 b- defN 24-Mar-18 23:35 qai_hub_models/datasets/coco.py
--rw-r--r--  2.0 unx     1546 b- defN 24-Mar-18 23:35 qai_hub_models/datasets/common.py
--rw-r--r--  2.0 unx     3203 b- defN 24-Mar-18 23:35 qai_hub_models/datasets/imagenette.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/evaluators/__init__.py
--rw-r--r--  2.0 unx     6212 b- defN 24-Mar-18 23:35 qai_hub_models/evaluators/base_evaluators.py
--rw-r--r--  2.0 unx     1326 b- defN 24-Mar-18 23:35 qai_hub_models/evaluators/classification_evaluator.py
--rw-r--r--  2.0 unx     3355 b- defN 24-Mar-18 23:35 qai_hub_models/evaluators/detection_evaluator.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Mar-18 23:35 qai_hub_models/evaluators/image_evaluator.py
--rw-r--r--  2.0 unx     2181 b- defN 24-Mar-18 23:35 qai_hub_models/evaluators/superres_evaluator.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/__init__.py
--rw-r--r--  2.0 unx      559 b- defN 24-Mar-18 23:35 qai_hub_models/models/common.py
--rw-r--r--  2.0 unx     6690 b- defN 24-Mar-18 23:35 qai_hub_models/models/protocols.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/__init__.py
--rw-r--r--  2.0 unx     1655 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py
--rw-r--r--  2.0 unx     4346 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/cityscapes_segmentation/app.py
--rw-r--r--  2.0 unx     2904 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py
--rw-r--r--  2.0 unx      890 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py
--rw-r--r--  2.0 unx     2888 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/cityscapes_segmentation/model.py
--rw-r--r--  2.0 unx     8565 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/deeplab/__init__.py
--rw-r--r--  2.0 unx     2651 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/deeplab/app.py
--rw-r--r--  2.0 unx     2252 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/deeplab/demo.py
--rw-r--r--  2.0 unx      915 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/deeplab/evaluator.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/detr/__init__.py
--rw-r--r--  2.0 unx     4243 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/detr/app.py
--rw-r--r--  2.0 unx     3565 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/detr/coco_label_map.py
--rw-r--r--  2.0 unx     1944 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/detr/demo.py
--rw-r--r--  2.0 unx     2152 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/detr/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/fastsam/__init__.py
--rw-r--r--  2.0 unx     4883 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/fastsam/app.py
--rw-r--r--  2.0 unx     2022 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/fastsam/demo.py
--rw-r--r--  2.0 unx     1935 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/fastsam/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/ffnet/__init__.py
--rw-r--r--  2.0 unx     4445 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/ffnet/model.py
--rw-r--r--  2.0 unx     1487 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/ffnet/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/ffnet_quantized/__init__.py
--rw-r--r--  2.0 unx     1165 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json
--rw-r--r--  2.0 unx     3080 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/ffnet_quantized/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/imagenet_classifier/__init__.py
--rw-r--r--  2.0 unx     2272 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/imagenet_classifier/app.py
--rw-r--r--  2.0 unx     2432 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/imagenet_classifier/demo.py
--rw-r--r--  2.0 unx     3404 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/imagenet_classifier/model.py
--rw-r--r--  2.0 unx     3781 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/mediapipe/__init__.py
--rw-r--r--  2.0 unx    30293 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/mediapipe/app.py
--rw-r--r--  2.0 unx     4394 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/mediapipe/utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/quicksrnet/__init__.py
--rw-r--r--  2.0 unx      985 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/quicksrnet/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/repaint/__init__.py
--rw-r--r--  2.0 unx     3366 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/repaint/app.py
--rw-r--r--  2.0 unx     2229 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/repaint/demo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/sesr/__init__.py
--rw-r--r--  2.0 unx     1029 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/sesr/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/super_resolution/__init__.py
--rw-r--r--  2.0 unx     2143 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/super_resolution/app.py
--rw-r--r--  2.0 unx     2775 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/super_resolution/demo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/swin/__init__.py
--rw-r--r--  2.0 unx     9175 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/swin/swin_transformer.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/video_classifier/__init__.py
--rw-r--r--  2.0 unx    11183 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/video_classifier/app.py
--rw-r--r--  2.0 unx     1581 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/video_classifier/demo.py
--rw-r--r--  2.0 unx     1969 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/video_classifier/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/whisper/__init__.py
--rw-r--r--  2.0 unx     9600 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/whisper/app.py
--rw-r--r--  2.0 unx     1302 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/whisper/demo.py
--rw-r--r--  2.0 unx    13419 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/whisper/model.py
--rw-r--r--  2.0 unx     2598 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/whisper/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/yolo/__init__.py
--rw-r--r--  2.0 unx     5686 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/yolo/app.py
--rw-r--r--  2.0 unx     2364 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/yolo/demo.py
--rw-r--r--  2.0 unx     4183 b- defN 24-Mar-18 23:35 qai_hub_models/models/_shared/yolo/utils.py
--rw-r--r--  2.0 unx      450 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/__init__.py
--rw-r--r--  2.0 unx      870 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/conftest.py
--rw-r--r--  2.0 unx      597 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/demo.py
--rw-r--r--  2.0 unx     8136 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/export.py
--rw-r--r--  2.0 unx     1023 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/info.yaml
--rw-r--r--  2.0 unx     4838 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/model.py
--rw-r--r--  2.0 unx     2740 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/perf.yaml
--rw-r--r--  2.0 unx     2006 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/test.py
--rw-r--r--  2.0 unx      532 b- defN 24-Mar-18 23:35 qai_hub_models/models/aotgan/patches/layer_norm.diff
--rw-r--r--  2.0 unx     1983 b- defN 24-Mar-18 23:35 qai_hub_models/models/baichuan_7b_quantized/info.yaml
--rw-r--r--  2.0 unx     2036 b- defN 24-Mar-18 23:35 qai_hub_models/models/baichuan_7b_quantized/perf.yaml
--rw-r--r--  2.0 unx      559 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/__init__.py
--rw-r--r--  2.0 unx     9705 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/app.py
--rw-r--r--  2.0 unx     6397 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/demo.py
--rw-r--r--  2.0 unx     7622 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/export.py
--rw-r--r--  2.0 unx     1296 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/info.yaml
--rw-r--r--  2.0 unx     5426 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/model.py
--rw-r--r--  2.0 unx     3323 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/perf.yaml
--rw-r--r--  2.0 unx       46 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/requirements.txt
--rw-r--r--  2.0 unx     1556 b- defN 24-Mar-18 23:35 qai_hub_models/models/controlnet_quantized/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/__init__.py
--rw-r--r--  2.0 unx      798 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/conftest.py
--rw-r--r--  2.0 unx      543 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/demo.py
--rw-r--r--  2.0 unx     7895 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/export.py
--rw-r--r--  2.0 unx     1287 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/info.yaml
--rw-r--r--  2.0 unx      708 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/model.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/perf.yaml
--rw-r--r--  2.0 unx      857 b- defN 24-Mar-18 23:35 qai_hub_models/models/convnext_tiny/test.py
--rw-r--r--  2.0 unx      398 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/__init__.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/app.py
--rw-r--r--  2.0 unx      884 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/conftest.py
--rw-r--r--  2.0 unx     2128 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/demo.py
--rw-r--r--  2.0 unx     8173 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/export.py
--rw-r--r--  2.0 unx     1334 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/info.yaml
--rw-r--r--  2.0 unx     3831 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/model.py
--rw-r--r--  2.0 unx     2684 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/perf.yaml
--rw-r--r--  2.0 unx     1790 b- defN 24-Mar-18 23:35 qai_hub_models/models/ddrnet23_slim/test.py
--rw-r--r--  2.0 unx      451 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/__init__.py
--rw-r--r--  2.0 unx      894 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/conftest.py
--rw-r--r--  2.0 unx     1026 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/demo.py
--rw-r--r--  2.0 unx     8048 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/export.py
--rw-r--r--  2.0 unx     1278 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/info.yaml
--rw-r--r--  2.0 unx     2886 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/model.py
--rw-r--r--  2.0 unx     2744 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/perf.yaml
--rw-r--r--  2.0 unx     2086 b- defN 24-Mar-18 23:35 qai_hub_models/models/deeplabv3_resnet50/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/__init__.py
--rw-r--r--  2.0 unx      794 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/demo.py
--rw-r--r--  2.0 unx     7868 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/info.yaml
--rw-r--r--  2.0 unx      698 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/model.py
--rw-r--r--  2.0 unx     2736 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/perf.yaml
--rw-r--r--  2.0 unx      841 b- defN 24-Mar-18 23:35 qai_hub_models/models/densenet121/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/__init__.py
--rw-r--r--  2.0 unx      800 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/conftest.py
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/demo.py
--rw-r--r--  2.0 unx     7885 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/export.py
--rw-r--r--  2.0 unx     1188 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/info.yaml
--rw-r--r--  2.0 unx      661 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/model.py
--rw-r--r--  2.0 unx     2698 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/requirements.txt
--rw-r--r--  2.0 unx     1316 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/__init__.py
--rw-r--r--  2.0 unx      808 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/conftest.py
--rw-r--r--  2.0 unx      906 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/demo.py
--rw-r--r--  2.0 unx     7901 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/export.py
--rw-r--r--  2.0 unx     1215 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/info.yaml
--rw-r--r--  2.0 unx      668 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/model.py
--rw-r--r--  2.0 unx     2702 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/requirements.txt
--rw-r--r--  2.0 unx     1373 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet101_dc5/test.py
--rw-r--r--  2.0 unx      481 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/__init__.py
--rw-r--r--  2.0 unx      798 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/conftest.py
--rw-r--r--  2.0 unx      893 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/demo.py
--rw-r--r--  2.0 unx     7881 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/export.py
--rw-r--r--  2.0 unx     1185 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/info.yaml
--rw-r--r--  2.0 unx      659 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/model.py
--rw-r--r--  2.0 unx     2695 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/requirements.txt
--rw-r--r--  2.0 unx     1636 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50/test.py
--rw-r--r--  2.0 unx      488 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/__init__.py
--rw-r--r--  2.0 unx      806 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/conftest.py
--rw-r--r--  2.0 unx      903 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/demo.py
--rw-r--r--  2.0 unx     7897 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/export.py
--rw-r--r--  2.0 unx     1212 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/info.yaml
--rw-r--r--  2.0 unx      666 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/model.py
--rw-r--r--  2.0 unx     2698 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/requirements.txt
--rw-r--r--  2.0 unx     1344 b- defN 24-Mar-18 23:35 qai_hub_models/models/detr_resnet50_dc5/test.py
--rw-r--r--  2.0 unx      477 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/__init__.py
--rw-r--r--  2.0 unx      802 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/conftest.py
--rw-r--r--  2.0 unx      549 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/demo.py
--rw-r--r--  2.0 unx     7883 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/export.py
--rw-r--r--  2.0 unx     1361 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/info.yaml
--rw-r--r--  2.0 unx      714 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/model.py
--rw-r--r--  2.0 unx     2737 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/perf.yaml
--rw-r--r--  2.0 unx      867 b- defN 24-Mar-18 23:35 qai_hub_models/models/efficientnet_b0/test.py
--rw-r--r--  2.0 unx      463 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/__init__.py
--rw-r--r--  2.0 unx      870 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/conftest.py
--rw-r--r--  2.0 unx      939 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/demo.py
--rw-r--r--  2.0 unx     7982 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/export.py
--rw-r--r--  2.0 unx     1117 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/info.yaml
--rw-r--r--  2.0 unx     3473 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/model.py
--rw-r--r--  2.0 unx     2747 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/perf.yaml
--rw-r--r--  2.0 unx     1831 b- defN 24-Mar-18 23:35 qai_hub_models/models/esrgan/test.py
--rw-r--r--  2.0 unx      418 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/app.py
--rw-r--r--  2.0 unx      892 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/conftest.py
--rw-r--r--  2.0 unx     3172 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/demo.py
--rw-r--r--  2.0 unx     7650 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/export.py
--rw-r--r--  2.0 unx     1070 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/info.yaml
--rw-r--r--  2.0 unx     2414 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/model.py
--rw-r--r--  2.0 unx     2701 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/perf.yaml
--rw-r--r--  2.0 unx       74 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/requirements.txt
--rw-r--r--  2.0 unx     2492 b- defN 24-Mar-18 23:35 qai_hub_models/models/facebook_denoiser/test.py
--rw-r--r--  2.0 unx      440 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/conftest.py
--rw-r--r--  2.0 unx      762 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/demo.py
--rw-r--r--  2.0 unx     8244 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/export.py
--rw-r--r--  2.0 unx     1301 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/info.yaml
--rw-r--r--  2.0 unx      683 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/model.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/requirements.txt
--rw-r--r--  2.0 unx     1332 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_s/test.py
--rw-r--r--  2.0 unx      440 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/conftest.py
--rw-r--r--  2.0 unx      762 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/demo.py
--rw-r--r--  2.0 unx     8244 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/export.py
--rw-r--r--  2.0 unx     1300 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/info.yaml
--rw-r--r--  2.0 unx      683 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/model.py
--rw-r--r--  2.0 unx     2686 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/requirements.txt
--rw-r--r--  2.0 unx     1332 b- defN 24-Mar-18 23:35 qai_hub_models/models/fastsam_x/test.py
--rw-r--r--  2.0 unx      410 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/__init__.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/app.py
--rw-r--r--  2.0 unx      882 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/conftest.py
--rw-r--r--  2.0 unx     2315 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/demo.py
--rw-r--r--  2.0 unx     8149 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/export.py
--rw-r--r--  2.0 unx     1241 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/info.yaml
--rw-r--r--  2.0 unx     1989 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/model.py
--rw-r--r--  2.0 unx     2737 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/perf.yaml
--rw-r--r--  2.0 unx     1637 b- defN 24-Mar-18 23:35 qai_hub_models/models/fcn_resnet50/test.py
--rw-r--r--  2.0 unx      487 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/__init__.py
--rw-r--r--  2.0 unx      894 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/conftest.py
--rw-r--r--  2.0 unx      607 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/demo.py
--rw-r--r--  2.0 unx     8030 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/export.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/info.yaml
--rw-r--r--  2.0 unx      629 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/model.py
--rw-r--r--  2.0 unx     2746 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt
--rw-r--r--  2.0 unx      804 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_122ns_lowres/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/demo.py
--rw-r--r--  2.0 unx     7994 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/export.py
--rw-r--r--  2.0 unx     1298 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/info.yaml
--rw-r--r--  2.0 unx      563 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/model.py
--rw-r--r--  2.0 unx     2739 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/__init__.py
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/demo.py
--rw-r--r--  2.0 unx     8447 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/info.yaml
--rw-r--r--  2.0 unx     1164 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/model.py
--rw-r--r--  2.0 unx     2684 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_40s_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/demo.py
--rw-r--r--  2.0 unx     7994 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/export.py
--rw-r--r--  2.0 unx     1275 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/info.yaml
--rw-r--r--  2.0 unx      563 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/model.py
--rw-r--r--  2.0 unx     2746 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/__init__.py
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/demo.py
--rw-r--r--  2.0 unx     8447 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/info.yaml
--rw-r--r--  2.0 unx     1139 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/model.py
--rw-r--r--  2.0 unx     2689 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_54s_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/demo.py
--rw-r--r--  2.0 unx     7994 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/export.py
--rw-r--r--  2.0 unx     1279 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/info.yaml
--rw-r--r--  2.0 unx      563 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/model.py
--rw-r--r--  2.0 unx     2746 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s/test.py
--rw-r--r--  2.0 unx      485 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/__init__.py
--rw-r--r--  2.0 unx      890 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/conftest.py
--rw-r--r--  2.0 unx      601 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/demo.py
--rw-r--r--  2.0 unx     8022 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/export.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/info.yaml
--rw-r--r--  2.0 unx      623 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/model.py
--rw-r--r--  2.0 unx     2739 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/requirements.txt
--rw-r--r--  2.0 unx      794 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_lowres/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/__init__.py
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/demo.py
--rw-r--r--  2.0 unx     8447 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/info.yaml
--rw-r--r--  2.0 unx     1139 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/model.py
--rw-r--r--  2.0 unx     2689 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Mar-18 23:35 qai_hub_models/models/ffnet_78s_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/demo.py
--rw-r--r--  2.0 unx     7859 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/export.py
--rw-r--r--  2.0 unx     1295 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/info.yaml
--rw-r--r--  2.0 unx      743 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/model.py
--rw-r--r--  2.0 unx     2720 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet/test.py
--rw-r--r--  2.0 unx      573 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/__init__.py
--rw-r--r--  2.0 unx      810 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/demo.py
--rw-r--r--  2.0 unx     8339 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/export.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/info.yaml
--rw-r--r--  2.0 unx     4310 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/model.py
--rw-r--r--  2.0 unx     2729 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/perf.yaml
--rw-r--r--  2.0 unx      885 b- defN 24-Mar-18 23:35 qai_hub_models/models/googlenet_quantized/test.py
--rw-r--r--  2.0 unx      430 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/__init__.py
--rw-r--r--  2.0 unx     5644 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/app.py
--rw-r--r--  2.0 unx      878 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/conftest.py
--rw-r--r--  2.0 unx     1739 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/demo.py
--rw-r--r--  2.0 unx     8154 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/export.py
--rw-r--r--  2.0 unx     1195 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/info.yaml
--rw-r--r--  2.0 unx     2801 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/model.py
--rw-r--r--  2.0 unx     2733 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/perf.yaml
--rw-r--r--  2.0 unx       51 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/requirements.txt
--rw-r--r--  2.0 unx     1420 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose/test.py
--rw-r--r--  2.0 unx      441 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/__init__.py
--rw-r--r--  2.0 unx      898 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/conftest.py
--rw-r--r--  2.0 unx     1830 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/demo.py
--rw-r--r--  2.0 unx     8606 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/export.py
--rw-r--r--  2.0 unx     1227 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/info.yaml
--rw-r--r--  2.0 unx     3132 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/model.py
--rw-r--r--  2.0 unx     2686 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/perf.yaml
--rw-r--r--  2.0 unx       51 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/requirements.txt
--rw-r--r--  2.0 unx     1588 b- defN 24-Mar-18 23:35 qai_hub_models/models/hrnet_pose_quantized/test.py
--rw-r--r--  2.0 unx      434 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
--rw-r--r--  2.0 unx     2133 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/app.py
--rw-r--r--  2.0 unx      912 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
--rw-r--r--  2.0 unx     1517 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py
--rw-r--r--  2.0 unx     7547 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/export.py
--rw-r--r--  2.0 unx     1248 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml
--rw-r--r--  2.0 unx     7886 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/model.py
--rw-r--r--  2.0 unx     2719 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml
--rw-r--r--  2.0 unx       72 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt
--rw-r--r--  2.0 unx     2560 b- defN 24-Mar-18 23:35 qai_hub_models/models/huggingface_wavlm_base_plus/test.py
--rw-r--r--  2.0 unx      477 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/__init__.py
--rw-r--r--  2.0 unx      796 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/conftest.py
--rw-r--r--  2.0 unx      546 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/demo.py
--rw-r--r--  2.0 unx     7871 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/export.py
--rw-r--r--  2.0 unx     1359 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/info.yaml
--rw-r--r--  2.0 unx      756 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/model.py
--rw-r--r--  2.0 unx     2735 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/perf.yaml
--rw-r--r--  2.0 unx      861 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3/test.py
--rw-r--r--  2.0 unx      584 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/__init__.py
--rw-r--r--  2.0 unx      816 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/conftest.py
--rw-r--r--  2.0 unx      591 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/demo.py
--rw-r--r--  2.0 unx     8372 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/export.py
--rw-r--r--  2.0 unx     1556 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/info.yaml
--rw-r--r--  2.0 unx     8094 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/model.py
--rw-r--r--  2.0 unx     2684 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/perf.yaml
--rw-r--r--  2.0 unx      901 b- defN 24-Mar-18 23:35 qai_hub_models/models/inception_v3_quantized/test.py
--rw-r--r--  2.0 unx      455 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/__init__.py
--rw-r--r--  2.0 unx      882 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/conftest.py
--rw-r--r--  2.0 unx      911 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/demo.py
--rw-r--r--  2.0 unx     8159 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/export.py
--rw-r--r--  2.0 unx     1082 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/info.yaml
--rw-r--r--  2.0 unx     4977 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/model.py
--rw-r--r--  2.0 unx     2752 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/perf.yaml
--rw-r--r--  2.0 unx      171 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/requirements.txt
--rw-r--r--  2.0 unx     2074 b- defN 24-Mar-18 23:35 qai_hub_models/models/lama_dilated/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/__init__.py
--rw-r--r--  2.0 unx     3986 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/app.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/conftest.py
--rw-r--r--  2.0 unx     2072 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/demo.py
--rw-r--r--  2.0 unx     7618 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/export.py
--rw-r--r--  2.0 unx     1112 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/info.yaml
--rw-r--r--  2.0 unx     3788 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/model.py
--rw-r--r--  2.0 unx     2687 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/perf.yaml
--rw-r--r--  2.0 unx       39 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/requirements.txt
--rw-r--r--  2.0 unx     1714 b- defN 24-Mar-18 23:35 qai_hub_models/models/litehrnet/test.py
--rw-r--r--  2.0 unx     1993 b- defN 24-Mar-18 23:35 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml
--rw-r--r--  2.0 unx     2037 b- defN 24-Mar-18 23:35 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml
--rw-r--r--  2.0 unx      412 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/__init__.py
--rw-r--r--  2.0 unx     2099 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/app.py
--rw-r--r--  2.0 unx      886 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/conftest.py
--rw-r--r--  2.0 unx     2862 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/demo.py
--rw-r--r--  2.0 unx     9984 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/export.py
--rw-r--r--  2.0 unx     1469 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/info.yaml
--rw-r--r--  2.0 unx     7669 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/model.py
--rw-r--r--  2.0 unx     4826 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/perf.yaml
--rw-r--r--  2.0 unx     1441 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_face/test.py
--rw-r--r--  2.0 unx      412 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/__init__.py
--rw-r--r--  2.0 unx     9819 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/app.py
--rw-r--r--  2.0 unx      886 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/conftest.py
--rw-r--r--  2.0 unx     2832 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/demo.py
--rw-r--r--  2.0 unx     9984 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/export.py
--rw-r--r--  2.0 unx     1374 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/info.yaml
--rw-r--r--  2.0 unx     6017 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/model.py
--rw-r--r--  2.0 unx     4829 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/perf.yaml
--rw-r--r--  2.0 unx     1442 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_hand/test.py
--rw-r--r--  2.0 unx      412 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/__init__.py
--rw-r--r--  2.0 unx     4430 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/app.py
--rw-r--r--  2.0 unx      886 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/conftest.py
--rw-r--r--  2.0 unx     2889 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/demo.py
--rw-r--r--  2.0 unx     9985 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/export.py
--rw-r--r--  2.0 unx     1387 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/info.yaml
--rw-r--r--  2.0 unx     5801 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/model.py
--rw-r--r--  2.0 unx     4828 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/perf.yaml
--rw-r--r--  2.0 unx     1443 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_pose/test.py
--rw-r--r--  2.0 unx      362 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/__init__.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/app.py
--rw-r--r--  2.0 unx      804 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/conftest.py
--rw-r--r--  2.0 unx     2674 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/demo.py
--rw-r--r--  2.0 unx     8178 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/export.py
--rw-r--r--  2.0 unx     1455 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/info.yaml
--rw-r--r--  2.0 unx    12352 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/model.py
--rw-r--r--  2.0 unx     2750 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/perf.yaml
--rw-r--r--  2.0 unx       15 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/requirements.txt
--rw-r--r--  2.0 unx     1396 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/test.py
--rw-r--r--  2.0 unx     2492 b- defN 24-Mar-18 23:35 qai_hub_models/models/mediapipe_selfie/utils.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/demo.py
--rw-r--r--  2.0 unx     7859 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/export.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/info.yaml
--rw-r--r--  2.0 unx      699 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/model.py
--rw-r--r--  2.0 unx     2720 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/perf.yaml
--rw-r--r--  2.0 unx      882 b- defN 24-Mar-18 23:35 qai_hub_models/models/mnasnet05/test.py
--rw-r--r--  2.0 unx      474 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/__init__.py
--rw-r--r--  2.0 unx      882 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/conftest.py
--rw-r--r--  2.0 unx      540 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/demo.py
--rw-r--r--  2.0 unx     7871 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/export.py
--rw-r--r--  2.0 unx     1380 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/info.yaml
--rw-r--r--  2.0 unx     2595 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/model.py
--rw-r--r--  2.0 unx     2728 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/perf.yaml
--rw-r--r--  2.0 unx     1091 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2/test.py
--rw-r--r--  2.0 unx      485 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/__init__.py
--rw-r--r--  2.0 unx      902 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/conftest.py
--rw-r--r--  2.0 unx      585 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/demo.py
--rw-r--r--  2.0 unx     8352 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/export.py
--rw-r--r--  2.0 unx     1362 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/info.yaml
--rw-r--r--  2.0 unx     4311 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/model.py
--rw-r--r--  2.0 unx     2735 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/perf.yaml
--rw-r--r--  2.0 unx     1002 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v2_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/__init__.py
--rw-r--r--  2.0 unx      808 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/conftest.py
--rw-r--r--  2.0 unx      556 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/demo.py
--rw-r--r--  2.0 unx     7915 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/export.py
--rw-r--r--  2.0 unx     1340 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/info.yaml
--rw-r--r--  2.0 unx      721 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/model.py
--rw-r--r--  2.0 unx     2684 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/perf.yaml
--rw-r--r--  2.0 unx      879 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large/test.py
--rw-r--r--  2.0 unx      607 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py
--rw-r--r--  2.0 unx      828 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py
--rw-r--r--  2.0 unx      748 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py
--rw-r--r--  2.0 unx     8084 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/export.py
--rw-r--r--  2.0 unx     1374 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml
--rw-r--r--  2.0 unx     3096 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/model.py
--rw-r--r--  2.0 unx     2694 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml
--rw-r--r--  2.0 unx      917 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_large_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/__init__.py
--rw-r--r--  2.0 unx      808 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/conftest.py
--rw-r--r--  2.0 unx      556 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/demo.py
--rw-r--r--  2.0 unx     7915 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/export.py
--rw-r--r--  2.0 unx     1338 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/info.yaml
--rw-r--r--  2.0 unx      721 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/model.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/perf.yaml
--rw-r--r--  2.0 unx      879 b- defN 24-Mar-18 23:35 qai_hub_models/models/mobilenet_v3_small/test.py
--rw-r--r--  2.0 unx      394 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/__init__.py
--rw-r--r--  2.0 unx     3958 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/app.py
--rw-r--r--  2.0 unx      880 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/conftest.py
--rw-r--r--  2.0 unx     3262 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/demo.py
--rw-r--r--  2.0 unx     9895 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/export.py
--rw-r--r--  2.0 unx     1494 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/info.yaml
--rw-r--r--  2.0 unx     5295 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/model.py
--rw-r--r--  2.0 unx     4829 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/perf.yaml
--rw-r--r--  2.0 unx       29 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/requirements.txt
--rw-r--r--  2.0 unx     2118 b- defN 24-Mar-18 23:35 qai_hub_models/models/openai_clip/test.py
--rw-r--r--  2.0 unx      402 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/__init__.py
--rw-r--r--  2.0 unx    12008 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/app.py
--rw-r--r--  2.0 unx      874 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/conftest.py
--rw-r--r--  2.0 unx     2053 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/demo.py
--rw-r--r--  2.0 unx     8151 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/export.py
--rw-r--r--  2.0 unx     1246 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/info.yaml
--rw-r--r--  2.0 unx     5084 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/model.py
--rw-r--r--  2.0 unx     2738 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/requirements.txt
--rw-r--r--  2.0 unx     1321 b- defN 24-Mar-18 23:35 qai_hub_models/models/openpose/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/__init__.py
--rw-r--r--  2.0 unx      888 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/conftest.py
--rw-r--r--  2.0 unx      972 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/demo.py
--rw-r--r--  2.0 unx     8161 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/export.py
--rw-r--r--  2.0 unx     1248 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/info.yaml
--rw-r--r--  2.0 unx     3170 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/model.py
--rw-r--r--  2.0 unx     2715 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/perf.yaml
--rw-r--r--  2.0 unx     1422 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py
--rw-r--r--  2.0 unx      908 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py
--rw-r--r--  2.0 unx      891 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/demo.py
--rw-r--r--  2.0 unx     8614 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/export.py
--rw-r--r--  2.0 unx     1274 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml
--rw-r--r--  2.0 unx     4244 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/model.py
--rw-r--r--  2.0 unx     2688 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml
--rw-r--r--  2.0 unx     2921 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetlarge_quantized/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/__init__.py
--rw-r--r--  2.0 unx      890 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/conftest.py
--rw-r--r--  2.0 unx      976 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/demo.py
--rw-r--r--  2.0 unx     8165 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/export.py
--rw-r--r--  2.0 unx     1242 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/info.yaml
--rw-r--r--  2.0 unx     3177 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/model.py
--rw-r--r--  2.0 unx     2725 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/perf.yaml
--rw-r--r--  2.0 unx     1428 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium/test.py
--rw-r--r--  2.0 unx      484 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py
--rw-r--r--  2.0 unx      910 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py
--rw-r--r--  2.0 unx      900 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/demo.py
--rw-r--r--  2.0 unx     8618 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/export.py
--rw-r--r--  2.0 unx     1282 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml
--rw-r--r--  2.0 unx     4254 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/model.py
--rw-r--r--  2.0 unx     2689 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml
--rw-r--r--  2.0 unx     2912 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetmedium_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/__init__.py
--rw-r--r--  2.0 unx      888 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/conftest.py
--rw-r--r--  2.0 unx      972 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/demo.py
--rw-r--r--  2.0 unx     8161 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/export.py
--rw-r--r--  2.0 unx     1238 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/info.yaml
--rw-r--r--  2.0 unx     3170 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/model.py
--rw-r--r--  2.0 unx     2727 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/perf.yaml
--rw-r--r--  2.0 unx     1422 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py
--rw-r--r--  2.0 unx      908 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py
--rw-r--r--  2.0 unx      891 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/demo.py
--rw-r--r--  2.0 unx     8614 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/export.py
--rw-r--r--  2.0 unx     1278 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml
--rw-r--r--  2.0 unx     4234 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/model.py
--rw-r--r--  2.0 unx     2688 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml
--rw-r--r--  2.0 unx     2859 b- defN 24-Mar-18 23:35 qai_hub_models/models/quicksrnetsmall_quantized/test.py
--rw-r--r--  2.0 unx      481 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py
--rw-r--r--  2.0 unx      906 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py
--rw-r--r--  2.0 unx     1280 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/demo.py
--rw-r--r--  2.0 unx     8197 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/export.py
--rw-r--r--  2.0 unx     1206 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml
--rw-r--r--  2.0 unx     5226 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/model.py
--rw-r--r--  2.0 unx     2746 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml
--rw-r--r--  2.0 unx       44 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt
--rw-r--r--  2.0 unx     1480 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_general_x4v3/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/__init__.py
--rw-r--r--  2.0 unx      894 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/conftest.py
--rw-r--r--  2.0 unx     1256 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/demo.py
--rw-r--r--  2.0 unx     7634 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/export.py
--rw-r--r--  2.0 unx     1330 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/info.yaml
--rw-r--r--  2.0 unx     4443 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/model.py
--rw-r--r--  2.0 unx     2695 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/perf.yaml
--rw-r--r--  2.0 unx       44 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/requirements.txt
--rw-r--r--  2.0 unx     1440 b- defN 24-Mar-18 23:35 qai_hub_models/models/real_esrgan_x4plus/test.py
--rw-r--r--  2.0 unx      469 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/__init__.py
--rw-r--r--  2.0 unx      784 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/conftest.py
--rw-r--r--  2.0 unx      524 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/demo.py
--rw-r--r--  2.0 unx     7847 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/export.py
--rw-r--r--  2.0 unx     1291 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/info.yaml
--rw-r--r--  2.0 unx      635 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/model.py
--rw-r--r--  2.0 unx     2729 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/perf.yaml
--rw-r--r--  2.0 unx      984 b- defN 24-Mar-18 23:35 qai_hub_models/models/regnet/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/demo.py
--rw-r--r--  2.0 unx     7859 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/export.py
--rw-r--r--  2.0 unx     1312 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/info.yaml
--rw-r--r--  2.0 unx      609 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/model.py
--rw-r--r--  2.0 unx     2736 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/perf.yaml
--rw-r--r--  2.0 unx      961 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/__init__.py
--rw-r--r--  2.0 unx      810 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/demo.py
--rw-r--r--  2.0 unx     8339 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/export.py
--rw-r--r--  2.0 unx     1346 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/info.yaml
--rw-r--r--  2.0 unx     3551 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/model.py
--rw-r--r--  2.0 unx     2741 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/perf.yaml
--rw-r--r--  2.0 unx      921 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet101_quantized/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/__init__.py
--rw-r--r--  2.0 unx      788 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/conftest.py
--rw-r--r--  2.0 unx      530 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/demo.py
--rw-r--r--  2.0 unx     7855 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/info.yaml
--rw-r--r--  2.0 unx      607 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/model.py
--rw-r--r--  2.0 unx     2720 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/perf.yaml
--rw-r--r--  2.0 unx      955 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18/test.py
--rw-r--r--  2.0 unx      482 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/__init__.py
--rw-r--r--  2.0 unx      808 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/conftest.py
--rw-r--r--  2.0 unx      562 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/demo.py
--rw-r--r--  2.0 unx     8335 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/export.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/info.yaml
--rw-r--r--  2.0 unx     3353 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/model.py
--rw-r--r--  2.0 unx     2728 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/perf.yaml
--rw-r--r--  2.0 unx      917 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet18_quantized/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/__init__.py
--rw-r--r--  2.0 unx      788 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/conftest.py
--rw-r--r--  2.0 unx      530 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/demo.py
--rw-r--r--  2.0 unx     7855 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/export.py
--rw-r--r--  2.0 unx     1303 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/info.yaml
--rw-r--r--  2.0 unx      607 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/model.py
--rw-r--r--  2.0 unx     2722 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/perf.yaml
--rw-r--r--  2.0 unx      955 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnet50/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/__init__.py
--rw-r--r--  2.0 unx      792 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/conftest.py
--rw-r--r--  2.0 unx      536 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/demo.py
--rw-r--r--  2.0 unx     7863 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/export.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/info.yaml
--rw-r--r--  2.0 unx      617 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/model.py
--rw-r--r--  2.0 unx     2735 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/perf.yaml
--rw-r--r--  2.0 unx      897 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101/test.py
--rw-r--r--  2.0 unx      484 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/__init__.py
--rw-r--r--  2.0 unx      812 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/conftest.py
--rw-r--r--  2.0 unx      581 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/demo.py
--rw-r--r--  2.0 unx     8363 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/export.py
--rw-r--r--  2.0 unx     1365 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/info.yaml
--rw-r--r--  2.0 unx     3358 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/model.py
--rw-r--r--  2.0 unx     2687 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/perf.yaml
--rw-r--r--  2.0 unx      925 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext101_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/demo.py
--rw-r--r--  2.0 unx     7859 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/export.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/info.yaml
--rw-r--r--  2.0 unx      704 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/model.py
--rw-r--r--  2.0 unx     2729 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/__init__.py
--rw-r--r--  2.0 unx      810 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/demo.py
--rw-r--r--  2.0 unx     8359 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/export.py
--rw-r--r--  2.0 unx     1362 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/info.yaml
--rw-r--r--  2.0 unx     3349 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/model.py
--rw-r--r--  2.0 unx     2681 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/perf.yaml
--rw-r--r--  2.0 unx      921 b- defN 24-Mar-18 23:35 qai_hub_models/models/resnext50_quantized/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/__init__.py
--rw-r--r--  2.0 unx     5101 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/app.py
--rw-r--r--  2.0 unx      905 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/conftest.py
--rw-r--r--  2.0 unx     3088 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/demo.py
--rw-r--r--  2.0 unx    10107 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/export.py
--rw-r--r--  2.0 unx     1391 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/info.yaml
--rw-r--r--  2.0 unx    12012 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/model.py
--rw-r--r--  2.0 unx     2685 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/perf.yaml
--rw-r--r--  2.0 unx       37 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/requirements.txt
--rw-r--r--  2.0 unx     3062 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/test.py
--rw-r--r--  2.0 unx      826 b- defN 24-Mar-18 23:35 qai_hub_models/models/sam/utils.py
--rw-r--r--  2.0 unx      464 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/__init__.py
--rw-r--r--  2.0 unx      872 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/conftest.py
--rw-r--r--  2.0 unx      923 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/demo.py
--rw-r--r--  2.0 unx     7986 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/export.py
--rw-r--r--  2.0 unx     1104 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/info.yaml
--rw-r--r--  2.0 unx     2984 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/model.py
--rw-r--r--  2.0 unx     2722 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/perf.yaml
--rw-r--r--  2.0 unx     1471 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/__init__.py
--rw-r--r--  2.0 unx      892 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/conftest.py
--rw-r--r--  2.0 unx      990 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/demo.py
--rw-r--r--  2.0 unx     8147 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/export.py
--rw-r--r--  2.0 unx     1151 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/info.yaml
--rw-r--r--  2.0 unx     3926 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/model.py
--rw-r--r--  2.0 unx     2680 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/perf.yaml
--rw-r--r--  2.0 unx     2927 b- defN 24-Mar-18 23:35 qai_hub_models/models/sesr_m5_quantized/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/__init__.py
--rw-r--r--  2.0 unx      798 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/conftest.py
--rw-r--r--  2.0 unx      543 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/demo.py
--rw-r--r--  2.0 unx     7875 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/export.py
--rw-r--r--  2.0 unx     1353 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/info.yaml
--rw-r--r--  2.0 unx      713 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/model.py
--rw-r--r--  2.0 unx     2731 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/perf.yaml
--rw-r--r--  2.0 unx      857 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2/test.py
--rw-r--r--  2.0 unx      584 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/__init__.py
--rw-r--r--  2.0 unx      818 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/conftest.py
--rw-r--r--  2.0 unx      588 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/demo.py
--rw-r--r--  2.0 unx     8355 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/export.py
--rw-r--r--  2.0 unx     1383 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/info.yaml
--rw-r--r--  2.0 unx     6330 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/model.py
--rw-r--r--  2.0 unx     2740 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/perf.yaml
--rw-r--r--  2.0 unx      899 b- defN 24-Mar-18 23:35 qai_hub_models/models/shufflenet_v2_quantized/test.py
--rw-r--r--  2.0 unx      396 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/__init__.py
--rw-r--r--  2.0 unx     3793 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/app.py
--rw-r--r--  2.0 unx      868 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/conftest.py
--rw-r--r--  2.0 unx     1657 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/demo.py
--rw-r--r--  2.0 unx     8121 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/export.py
--rw-r--r--  2.0 unx     1260 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/info.yaml
--rw-r--r--  2.0 unx     4770 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/model.py
--rw-r--r--  2.0 unx     2725 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/perf.yaml
--rw-r--r--  2.0 unx     1355 b- defN 24-Mar-18 23:35 qai_hub_models/models/sinet/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/__init__.py
--rw-r--r--  2.0 unx      798 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/conftest.py
--rw-r--r--  2.0 unx      539 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/demo.py
--rw-r--r--  2.0 unx     7876 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/export.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/info.yaml
--rw-r--r--  2.0 unx      696 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/model.py
--rw-r--r--  2.0 unx     2724 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/perf.yaml
--rw-r--r--  2.0 unx      851 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1/test.py
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/__init__.py
--rw-r--r--  2.0 unx      818 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/conftest.py
--rw-r--r--  2.0 unx      584 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/demo.py
--rw-r--r--  2.0 unx     8308 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/export.py
--rw-r--r--  2.0 unx     1358 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/info.yaml
--rw-r--r--  2.0 unx     3364 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/model.py
--rw-r--r--  2.0 unx     2734 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
--rw-r--r--  2.0 unx      895 b- defN 24-Mar-18 23:35 qai_hub_models/models/squeezenet1_1_quantized/test.py
--rw-r--r--  2.0 unx      540 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/__init__.py
--rw-r--r--  2.0 unx     7966 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/app.py
--rw-r--r--  2.0 unx     5765 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/demo.py
--rw-r--r--  2.0 unx     7432 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/export.py
--rw-r--r--  2.0 unx     1355 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/info.yaml
--rw-r--r--  2.0 unx     3604 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/model.py
--rw-r--r--  2.0 unx     2638 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/perf.yaml
--rw-r--r--  2.0 unx       46 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/requirements.txt
--rw-r--r--  2.0 unx     1599 b- defN 24-Mar-18 23:35 qai_hub_models/models/stable_diffusion_quantized/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/__init__.py
--rw-r--r--  2.0 unx     4155 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/app.py
--rw-r--r--  2.0 unx      876 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/conftest.py
--rw-r--r--  2.0 unx     2847 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/demo.py
--rw-r--r--  2.0 unx     7742 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/export.py
--rw-r--r--  2.0 unx     1102 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/info.yaml
--rw-r--r--  2.0 unx     8418 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/model.py
--rw-r--r--  2.0 unx     2700 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/perf.yaml
--rw-r--r--  2.0 unx       11 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/requirements.txt
--rw-r--r--  2.0 unx     2497 b- defN 24-Mar-18 23:35 qai_hub_models/models/stylegan2/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/conftest.py
--rw-r--r--  2.0 unx      531 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/demo.py
--rw-r--r--  2.0 unx     7879 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/export.py
--rw-r--r--  2.0 unx     1383 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/info.yaml
--rw-r--r--  2.0 unx     1241 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/model.py
--rw-r--r--  2.0 unx     2685 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/perf.yaml
--rw-r--r--  2.0 unx     1251 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_base/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/__init__.py
--rw-r--r--  2.0 unx      792 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/conftest.py
--rw-r--r--  2.0 unx      534 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/demo.py
--rw-r--r--  2.0 unx     7883 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/export.py
--rw-r--r--  2.0 unx     1378 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/info.yaml
--rw-r--r--  2.0 unx     1242 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/model.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/perf.yaml
--rw-r--r--  2.0 unx     1257 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_small/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/__init__.py
--rw-r--r--  2.0 unx      790 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/conftest.py
--rw-r--r--  2.0 unx      531 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/demo.py
--rw-r--r--  2.0 unx     7879 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/export.py
--rw-r--r--  2.0 unx     1376 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/info.yaml
--rw-r--r--  2.0 unx     1241 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/model.py
--rw-r--r--  2.0 unx     2680 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/perf.yaml
--rw-r--r--  2.0 unx     1369 b- defN 24-Mar-18 23:35 qai_hub_models/models/swin_tiny/test.py
--rw-r--r--  2.0 unx      396 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/__init__.py
--rw-r--r--  2.0 unx    10207 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/app.py
--rw-r--r--  2.0 unx      782 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/conftest.py
--rw-r--r--  2.0 unx     1779 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/demo.py
--rw-r--r--  2.0 unx     9887 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/export.py
--rw-r--r--  2.0 unx     1370 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/info.yaml
--rw-r--r--  2.0 unx    10488 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/model.py
--rw-r--r--  2.0 unx     4707 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/perf.yaml
--rw-r--r--  2.0 unx       42 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/requirements.txt
--rw-r--r--  2.0 unx     2357 b- defN 24-Mar-18 23:35 qai_hub_models/models/trocr/test.py
--rw-r--r--  2.0 unx      348 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/__init__.py
--rw-r--r--  2.0 unx     1305 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/app.py
--rw-r--r--  2.0 unx      806 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/conftest.py
--rw-r--r--  2.0 unx     2509 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/demo.py
--rw-r--r--  2.0 unx     8169 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/info.yaml
--rw-r--r--  2.0 unx     2666 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/model.py
--rw-r--r--  2.0 unx     2753 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/perf.yaml
--rw-r--r--  2.0 unx     1215 b- defN 24-Mar-18 23:35 qai_hub_models/models/unet_segmentation/test.py
--rw-r--r--  2.0 unx      466 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/__init__.py
--rw-r--r--  2.0 unx      778 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/conftest.py
--rw-r--r--  2.0 unx      515 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/demo.py
--rw-r--r--  2.0 unx     7888 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/export.py
--rw-r--r--  2.0 unx     1342 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/info.yaml
--rw-r--r--  2.0 unx      685 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/model.py
--rw-r--r--  2.0 unx     2676 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/perf.yaml
--rw-r--r--  2.0 unx      807 b- defN 24-Mar-18 23:35 qai_hub_models/models/vit/test.py
--rw-r--r--  2.0 unx      444 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/__init__.py
--rw-r--r--  2.0 unx      802 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/conftest.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/demo.py
--rw-r--r--  2.0 unx     9662 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/export.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/info.yaml
--rw-r--r--  2.0 unx      558 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/model.py
--rw-r--r--  2.0 unx     4720 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_base_en/test.py
--rw-r--r--  2.0 unx      445 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/__init__.py
--rw-r--r--  2.0 unx      804 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/conftest.py
--rw-r--r--  2.0 unx      486 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/demo.py
--rw-r--r--  2.0 unx     9666 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/export.py
--rw-r--r--  2.0 unx     1848 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/info.yaml
--rw-r--r--  2.0 unx      560 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/model.py
--rw-r--r--  2.0 unx     4697 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/perf.yaml
--rw-r--r--  2.0 unx       38 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_en/test.py
--rw-r--r--  2.0 unx      217 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_small_multi/code-gen.yaml
--rw-r--r--  2.0 unx      444 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/__init__.py
--rw-r--r--  2.0 unx      802 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/conftest.py
--rw-r--r--  2.0 unx      483 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/demo.py
--rw-r--r--  2.0 unx     9662 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/export.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/info.yaml
--rw-r--r--  2.0 unx      558 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/model.py
--rw-r--r--  2.0 unx     4712 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Mar-18 23:35 qai_hub_models/models/whisper_tiny_en/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/__init__.py
--rw-r--r--  2.0 unx      796 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/conftest.py
--rw-r--r--  2.0 unx      542 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/demo.py
--rw-r--r--  2.0 unx     7871 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/export.py
--rw-r--r--  2.0 unx     1298 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/info.yaml
--rw-r--r--  2.0 unx      710 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/model.py
--rw-r--r--  2.0 unx     2736 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/perf.yaml
--rw-r--r--  2.0 unx      855 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50/test.py
--rw-r--r--  2.0 unx      582 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/__init__.py
--rw-r--r--  2.0 unx      816 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/conftest.py
--rw-r--r--  2.0 unx      587 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/demo.py
--rw-r--r--  2.0 unx     8304 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/export.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/info.yaml
--rw-r--r--  2.0 unx     3555 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/model.py
--rw-r--r--  2.0 unx     2737 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/perf.yaml
--rw-r--r--  2.0 unx      932 b- defN 24-Mar-18 23:35 qai_hub_models/models/wideresnet50_quantized/test.py
--rw-r--r--  2.0 unx      461 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/__init__.py
--rw-r--r--  2.0 unx      866 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/conftest.py
--rw-r--r--  2.0 unx      742 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/demo.py
--rw-r--r--  2.0 unx     7974 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/export.py
--rw-r--r--  2.0 unx     1156 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/info.yaml
--rw-r--r--  2.0 unx     3403 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/model.py
--rw-r--r--  2.0 unx     2722 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/perf.yaml
--rw-r--r--  2.0 unx     1402 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/__init__.py
--rw-r--r--  2.0 unx      886 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/conftest.py
--rw-r--r--  2.0 unx      956 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/demo.py
--rw-r--r--  2.0 unx     8427 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/export.py
--rw-r--r--  2.0 unx     1191 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/info.yaml
--rw-r--r--  2.0 unx     3572 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/model.py
--rw-r--r--  2.0 unx     2676 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/perf.yaml
--rw-r--r--  2.0 unx     1607 b- defN 24-Mar-18 23:35 qai_hub_models/models/xlsr_quantized/test.py
--rw-r--r--  2.0 unx      436 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/__init__.py
--rw-r--r--  2.0 unx     1071 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/app.py
--rw-r--r--  2.0 unx      870 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/conftest.py
--rw-r--r--  2.0 unx     1027 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/demo.py
--rw-r--r--  2.0 unx     7877 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/export.py
--rw-r--r--  2.0 unx     1168 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/info.yaml
--rw-r--r--  2.0 unx     4009 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/model.py
--rw-r--r--  2.0 unx     2734 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/perf.yaml
--rw-r--r--  2.0 unx     1845 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov6/test.py
--rw-r--r--  2.0 unx      436 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/__init__.py
--rw-r--r--  2.0 unx     1071 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/app.py
--rw-r--r--  2.0 unx      870 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/conftest.py
--rw-r--r--  2.0 unx      909 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/demo.py
--rw-r--r--  2.0 unx     7897 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/export.py
--rw-r--r--  2.0 unx     1131 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/info.yaml
--rw-r--r--  2.0 unx     8997 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/model.py
--rw-r--r--  2.0 unx     2685 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/perf.yaml
--rw-r--r--  2.0 unx       47 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/requirements.txt
--rw-r--r--  2.0 unx     2323 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov7/test.py
--rw-r--r--  2.0 unx      415 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/__init__.py
--rw-r--r--  2.0 unx      609 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/app.py
--rw-r--r--  2.0 unx      792 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/conftest.py
--rw-r--r--  2.0 unx      926 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/demo.py
--rw-r--r--  2.0 unx     7931 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/export.py
--rw-r--r--  2.0 unx     1163 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/info.yaml
--rw-r--r--  2.0 unx     3684 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/model.py
--rw-r--r--  2.0 unx     2748 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/requirements.txt
--rw-r--r--  2.0 unx     2080 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_det/test.py
--rw-r--r--  2.0 unx      419 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/__init__.py
--rw-r--r--  2.0 unx     7698 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/app.py
--rw-r--r--  2.0 unx      792 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/conftest.py
--rw-r--r--  2.0 unx     3155 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/demo.py
--rw-r--r--  2.0 unx     7954 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/export.py
--rw-r--r--  2.0 unx     1273 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/info.yaml
--rw-r--r--  2.0 unx     4663 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/model.py
--rw-r--r--  2.0 unx     2689 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/requirements.txt
--rw-r--r--  2.0 unx     2536 b- defN 24-Mar-18 23:35 qai_hub_models/models/yolov8_seg/test.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/test/__init__.py
--rw-r--r--  2.0 unx     1041 b- defN 24-Mar-18 23:35 qai_hub_models/test/test_async_compile_jobs.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/test/e2e/__init__.py
--rw-r--r--  2.0 unx     1661 b- defN 24-Mar-18 23:35 qai_hub_models/test/e2e/test_aimet_compile.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/test/test_utils/__init__.py
--rw-r--r--  2.0 unx     1493 b- defN 24-Mar-18 23:35 qai_hub_models/test/test_utils/perf.yaml
--rw-r--r--  2.0 unx     3229 b- defN 24-Mar-18 23:35 qai_hub_models/test/test_utils/test_info_specs.py
--rw-r--r--  2.0 unx     6515 b- defN 24-Mar-18 23:35 qai_hub_models/test/test_utils/test_perf_summary.py
--rw-r--r--  2.0 unx     3295 b- defN 24-Mar-18 23:35 qai_hub_models/test/test_utils/test_qai_hub_helpers.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/utils/__init__.py
--rw-r--r--  2.0 unx    14838 b- defN 24-Mar-18 23:35 qai_hub_models/utils/args.py
--rw-r--r--  2.0 unx    33721 b- defN 24-Mar-18 23:35 qai_hub_models/utils/asset_loaders.py
--rw-r--r--  2.0 unx     5966 b- defN 24-Mar-18 23:35 qai_hub_models/utils/base_model.py
--rw-r--r--  2.0 unx     8707 b- defN 24-Mar-18 23:35 qai_hub_models/utils/bounding_box_processing.py
--rw-r--r--  2.0 unx     1771 b- defN 24-Mar-18 23:35 qai_hub_models/utils/camera_capture.py
--rw-r--r--  2.0 unx     5222 b- defN 24-Mar-18 23:35 qai_hub_models/utils/compare.py
--rw-r--r--  2.0 unx    29522 b- defN 24-Mar-18 23:35 qai_hub_models/utils/config_loaders.py
--rw-r--r--  2.0 unx     3066 b- defN 24-Mar-18 23:35 qai_hub_models/utils/display.py
--rw-r--r--  2.0 unx     6403 b- defN 24-Mar-18 23:35 qai_hub_models/utils/draw.py
--rw-r--r--  2.0 unx     1549 b- defN 24-Mar-18 23:35 qai_hub_models/utils/huggingface.py
--rw-r--r--  2.0 unx    12490 b- defN 24-Mar-18 23:35 qai_hub_models/utils/image_processing.py
--rw-r--r--  2.0 unx    12482 b- defN 24-Mar-18 23:35 qai_hub_models/utils/inference.py
--rw-r--r--  2.0 unx     1308 b- defN 24-Mar-18 23:35 qai_hub_models/utils/input_spec.py
--rw-r--r--  2.0 unx     4559 b- defN 24-Mar-18 23:35 qai_hub_models/utils/measurement.py
--rw-r--r--  2.0 unx     1425 b- defN 24-Mar-18 23:35 qai_hub_models/utils/model_adapters.py
--rw-r--r--  2.0 unx    12621 b- defN 24-Mar-18 23:35 qai_hub_models/utils/model_card.py
--rw-r--r--  2.0 unx     1406 b- defN 24-Mar-18 23:35 qai_hub_models/utils/path_helpers.py
--rw-r--r--  2.0 unx    11378 b- defN 24-Mar-18 23:35 qai_hub_models/utils/perf_summary.py
--rw-r--r--  2.0 unx     4760 b- defN 24-Mar-18 23:35 qai_hub_models/utils/printing.py
--rw-r--r--  2.0 unx     5365 b- defN 24-Mar-18 23:35 qai_hub_models/utils/qai_hub_helpers.py
--rw-r--r--  2.0 unx     1463 b- defN 24-Mar-18 23:35 qai_hub_models/utils/qnn_helpers.py
--rw-r--r--  2.0 unx     2170 b- defN 24-Mar-18 23:35 qai_hub_models/utils/quantization.py
--rw-r--r--  2.0 unx    12085 b- defN 24-Mar-18 23:35 qai_hub_models/utils/quantization_aimet.py
--rw-r--r--  2.0 unx      754 b- defN 24-Mar-18 23:35 qai_hub_models/utils/test_compare.py
--rw-r--r--  2.0 unx     3173 b- defN 24-Mar-18 23:35 qai_hub_models/utils/testing.py
--rw-r--r--  2.0 unx      259 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/config_loader.py
--rw-r--r--  2.0 unx      993 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/default_config.json
--rw-r--r--  2.0 unx      946 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/default_config_legacy_v1.json
--rw-r--r--  2.0 unx      955 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/default_config_legacy_v2.json
--rw-r--r--  2.0 unx      919 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
--rw-r--r--  2.0 unx     1187 b- defN 24-Mar-18 23:35 qai_hub_models/utils/aimet/repo.py
--rw-r--r--  2.0 unx     1481 b- defN 24-Mar-18 23:37 qai_hub_models-0.4.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    40833 b- defN 24-Mar-18 23:37 qai_hub_models-0.4.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-18 23:37 qai_hub_models-0.4.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 24-Mar-18 23:37 qai_hub_models-0.4.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    93092 b- defN 24-Mar-18 23:37 qai_hub_models-0.4.0.dist-info/RECORD
-910 files, 2447155 bytes uncompressed, 815198 bytes compressed:  66.7%
+Zip file size: 961398 bytes, number of entries: 903
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/__init__.py
+-rw-r--r--  2.0 unx      281 b- defN 24-Apr-02 22:37 qai_hub_models/_version.py
+-rw-r--r--  2.0 unx      620 b- defN 24-Apr-02 22:41 qai_hub_models/asset_bases.yaml
+-rw-r--r--  2.0 unx      734 b- defN 24-Apr-02 22:37 qai_hub_models/conftest.py
+-rw-r--r--  2.0 unx      877 b- defN 24-Apr-02 22:37 qai_hub_models/global_requirements.txt
+-rw-r--r--  2.0 unx      395 b- defN 24-Apr-02 22:37 qai_hub_models/requirements-dev.txt
+-rw-r--r--  2.0 unx      426 b- defN 24-Apr-02 22:37 qai_hub_models/requirements.txt
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/datasets/__init__.py
+-rw-r--r--  2.0 unx     4757 b- defN 24-Apr-02 22:37 qai_hub_models/datasets/bsd300.py
+-rw-r--r--  2.0 unx     4054 b- defN 24-Apr-02 22:37 qai_hub_models/datasets/coco.py
+-rw-r--r--  2.0 unx     1546 b- defN 24-Apr-02 22:37 qai_hub_models/datasets/common.py
+-rw-r--r--  2.0 unx     3203 b- defN 24-Apr-02 22:37 qai_hub_models/datasets/imagenette.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/evaluators/__init__.py
+-rw-r--r--  2.0 unx     6212 b- defN 24-Apr-02 22:37 qai_hub_models/evaluators/base_evaluators.py
+-rw-r--r--  2.0 unx     1326 b- defN 24-Apr-02 22:37 qai_hub_models/evaluators/classification_evaluator.py
+-rw-r--r--  2.0 unx     3355 b- defN 24-Apr-02 22:37 qai_hub_models/evaluators/detection_evaluator.py
+-rw-r--r--  2.0 unx     2434 b- defN 24-Apr-02 22:37 qai_hub_models/evaluators/image_evaluator.py
+-rw-r--r--  2.0 unx     2181 b- defN 24-Apr-02 22:37 qai_hub_models/evaluators/superres_evaluator.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/__init__.py
+-rw-r--r--  2.0 unx      571 b- defN 24-Apr-02 22:37 qai_hub_models/models/common.py
+-rw-r--r--  2.0 unx     7882 b- defN 24-Apr-02 22:37 qai_hub_models/models/protocols.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/__init__.py
+-rw-r--r--  2.0 unx     1655 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py
+-rw-r--r--  2.0 unx     4346 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/cityscapes_segmentation/app.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py
+-rw-r--r--  2.0 unx      890 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py
+-rw-r--r--  2.0 unx     2888 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/cityscapes_segmentation/model.py
+-rw-r--r--  2.0 unx     8565 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/deeplab/__init__.py
+-rw-r--r--  2.0 unx     2651 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/deeplab/app.py
+-rw-r--r--  2.0 unx     2252 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/deeplab/demo.py
+-rw-r--r--  2.0 unx      915 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/deeplab/evaluator.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/detr/__init__.py
+-rw-r--r--  2.0 unx     4243 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/detr/app.py
+-rw-r--r--  2.0 unx     3565 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/detr/coco_label_map.py
+-rw-r--r--  2.0 unx     1944 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/detr/demo.py
+-rw-r--r--  2.0 unx     2152 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/detr/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/fastsam/__init__.py
+-rw-r--r--  2.0 unx     4883 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/fastsam/app.py
+-rw-r--r--  2.0 unx     2022 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/fastsam/demo.py
+-rw-r--r--  2.0 unx     1935 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/fastsam/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/ffnet/__init__.py
+-rw-r--r--  2.0 unx     4548 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/ffnet/model.py
+-rw-r--r--  2.0 unx     1569 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/ffnet/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/ffnet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1165 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json
+-rw-r--r--  2.0 unx     2372 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/ffnet_quantized/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/imagenet_classifier/__init__.py
+-rw-r--r--  2.0 unx     2272 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/imagenet_classifier/app.py
+-rw-r--r--  2.0 unx     2432 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/imagenet_classifier/demo.py
+-rw-r--r--  2.0 unx     3404 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/imagenet_classifier/model.py
+-rw-r--r--  2.0 unx     3781 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/mediapipe/__init__.py
+-rw-r--r--  2.0 unx    30293 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/mediapipe/app.py
+-rw-r--r--  2.0 unx     4394 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/mediapipe/utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/quicksrnet/__init__.py
+-rw-r--r--  2.0 unx      985 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/quicksrnet/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/repaint/__init__.py
+-rw-r--r--  2.0 unx     3366 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/repaint/app.py
+-rw-r--r--  2.0 unx     2229 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/repaint/demo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/sesr/__init__.py
+-rw-r--r--  2.0 unx     1029 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/sesr/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/super_resolution/__init__.py
+-rw-r--r--  2.0 unx     2143 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/super_resolution/app.py
+-rw-r--r--  2.0 unx     2775 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/super_resolution/demo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/swin/__init__.py
+-rw-r--r--  2.0 unx     9175 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/swin/swin_transformer.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/video_classifier/__init__.py
+-rw-r--r--  2.0 unx    11183 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/video_classifier/app.py
+-rw-r--r--  2.0 unx     1581 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/video_classifier/demo.py
+-rw-r--r--  2.0 unx     1969 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/video_classifier/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/whisper/__init__.py
+-rw-r--r--  2.0 unx    10188 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/whisper/app.py
+-rw-r--r--  2.0 unx     1302 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/whisper/demo.py
+-rw-r--r--  2.0 unx    14119 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/whisper/model.py
+-rw-r--r--  2.0 unx     2848 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/whisper/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/yolo/__init__.py
+-rw-r--r--  2.0 unx     7069 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/yolo/app.py
+-rw-r--r--  2.0 unx     2475 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/yolo/demo.py
+-rw-r--r--  2.0 unx     4190 b- defN 24-Apr-02 22:37 qai_hub_models/models/_shared/yolo/utils.py
+-rw-r--r--  2.0 unx      450 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/__init__.py
+-rw-r--r--  2.0 unx      870 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/conftest.py
+-rw-r--r--  2.0 unx      597 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/demo.py
+-rw-r--r--  2.0 unx     8156 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/export.py
+-rw-r--r--  2.0 unx     1023 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/info.yaml
+-rw-r--r--  2.0 unx     4838 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/model.py
+-rw-r--r--  2.0 unx     2768 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/perf.yaml
+-rw-r--r--  2.0 unx     2006 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/test.py
+-rw-r--r--  2.0 unx      532 b- defN 24-Apr-02 22:37 qai_hub_models/models/aotgan/patches/layer_norm.diff
+-rw-r--r--  2.0 unx     1983 b- defN 24-Apr-02 22:37 qai_hub_models/models/baichuan_7b_quantized/info.yaml
+-rw-r--r--  2.0 unx     2036 b- defN 24-Apr-02 22:37 qai_hub_models/models/baichuan_7b_quantized/perf.yaml
+-rw-r--r--  2.0 unx      559 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/__init__.py
+-rw-r--r--  2.0 unx     9705 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/app.py
+-rw-r--r--  2.0 unx     6397 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/demo.py
+-rw-r--r--  2.0 unx     7622 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/export.py
+-rw-r--r--  2.0 unx     1329 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/info.yaml
+-rw-r--r--  2.0 unx     5426 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/model.py
+-rw-r--r--  2.0 unx     8427 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/perf.yaml
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1556 b- defN 24-Apr-02 22:37 qai_hub_models/models/controlnet_quantized/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/__init__.py
+-rw-r--r--  2.0 unx      798 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/conftest.py
+-rw-r--r--  2.0 unx      543 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/demo.py
+-rw-r--r--  2.0 unx     7915 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/export.py
+-rw-r--r--  2.0 unx     1287 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/info.yaml
+-rw-r--r--  2.0 unx      708 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/model.py
+-rw-r--r--  2.0 unx     2707 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/perf.yaml
+-rw-r--r--  2.0 unx      857 b- defN 24-Apr-02 22:37 qai_hub_models/models/convnext_tiny/test.py
+-rw-r--r--  2.0 unx      398 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/__init__.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/app.py
+-rw-r--r--  2.0 unx      884 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/conftest.py
+-rw-r--r--  2.0 unx     2128 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/demo.py
+-rw-r--r--  2.0 unx     8193 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/export.py
+-rw-r--r--  2.0 unx     1334 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/info.yaml
+-rw-r--r--  2.0 unx     3831 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/model.py
+-rw-r--r--  2.0 unx     2707 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/perf.yaml
+-rw-r--r--  2.0 unx     1790 b- defN 24-Apr-02 22:37 qai_hub_models/models/ddrnet23_slim/test.py
+-rw-r--r--  2.0 unx      451 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/__init__.py
+-rw-r--r--  2.0 unx      894 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/conftest.py
+-rw-r--r--  2.0 unx     1026 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/demo.py
+-rw-r--r--  2.0 unx     8068 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/export.py
+-rw-r--r--  2.0 unx     1278 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/info.yaml
+-rw-r--r--  2.0 unx     2886 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/model.py
+-rw-r--r--  2.0 unx     2761 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/perf.yaml
+-rw-r--r--  2.0 unx     2086 b- defN 24-Apr-02 22:37 qai_hub_models/models/deeplabv3_resnet50/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/__init__.py
+-rw-r--r--  2.0 unx      794 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/demo.py
+-rw-r--r--  2.0 unx     7888 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/info.yaml
+-rw-r--r--  2.0 unx      698 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/model.py
+-rw-r--r--  2.0 unx     2758 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/perf.yaml
+-rw-r--r--  2.0 unx      841 b- defN 24-Apr-02 22:37 qai_hub_models/models/densenet121/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/__init__.py
+-rw-r--r--  2.0 unx      800 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/conftest.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/demo.py
+-rw-r--r--  2.0 unx     7905 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/export.py
+-rw-r--r--  2.0 unx     1188 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/info.yaml
+-rw-r--r--  2.0 unx      661 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/model.py
+-rw-r--r--  2.0 unx     2711 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/requirements.txt
+-rw-r--r--  2.0 unx     1316 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/__init__.py
+-rw-r--r--  2.0 unx      808 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/conftest.py
+-rw-r--r--  2.0 unx      906 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/demo.py
+-rw-r--r--  2.0 unx     7921 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/export.py
+-rw-r--r--  2.0 unx     1215 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/info.yaml
+-rw-r--r--  2.0 unx      668 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/model.py
+-rw-r--r--  2.0 unx     2719 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/requirements.txt
+-rw-r--r--  2.0 unx     1373 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet101_dc5/test.py
+-rw-r--r--  2.0 unx      481 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/__init__.py
+-rw-r--r--  2.0 unx      798 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/conftest.py
+-rw-r--r--  2.0 unx      893 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/demo.py
+-rw-r--r--  2.0 unx     7901 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/export.py
+-rw-r--r--  2.0 unx     1185 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/info.yaml
+-rw-r--r--  2.0 unx      659 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/model.py
+-rw-r--r--  2.0 unx     2713 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/requirements.txt
+-rw-r--r--  2.0 unx     1636 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50/test.py
+-rw-r--r--  2.0 unx      488 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/__init__.py
+-rw-r--r--  2.0 unx      806 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/conftest.py
+-rw-r--r--  2.0 unx      903 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/demo.py
+-rw-r--r--  2.0 unx     7917 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/export.py
+-rw-r--r--  2.0 unx     1212 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/info.yaml
+-rw-r--r--  2.0 unx      666 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/model.py
+-rw-r--r--  2.0 unx     2718 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/requirements.txt
+-rw-r--r--  2.0 unx     1344 b- defN 24-Apr-02 22:37 qai_hub_models/models/detr_resnet50_dc5/test.py
+-rw-r--r--  2.0 unx      477 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/__init__.py
+-rw-r--r--  2.0 unx      802 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/conftest.py
+-rw-r--r--  2.0 unx      549 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/demo.py
+-rw-r--r--  2.0 unx     7903 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/export.py
+-rw-r--r--  2.0 unx     1361 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/info.yaml
+-rw-r--r--  2.0 unx      714 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/model.py
+-rw-r--r--  2.0 unx     2756 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/perf.yaml
+-rw-r--r--  2.0 unx      867 b- defN 24-Apr-02 22:37 qai_hub_models/models/efficientnet_b0/test.py
+-rw-r--r--  2.0 unx      463 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/__init__.py
+-rw-r--r--  2.0 unx      870 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/conftest.py
+-rw-r--r--  2.0 unx      939 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/demo.py
+-rw-r--r--  2.0 unx     8002 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/export.py
+-rw-r--r--  2.0 unx     1117 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/info.yaml
+-rw-r--r--  2.0 unx     3473 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/model.py
+-rw-r--r--  2.0 unx     2772 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/perf.yaml
+-rw-r--r--  2.0 unx     1831 b- defN 24-Apr-02 22:37 qai_hub_models/models/esrgan/test.py
+-rw-r--r--  2.0 unx      418 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/app.py
+-rw-r--r--  2.0 unx      892 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/conftest.py
+-rw-r--r--  2.0 unx     3172 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/demo.py
+-rw-r--r--  2.0 unx     7670 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/export.py
+-rw-r--r--  2.0 unx     1070 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/info.yaml
+-rw-r--r--  2.0 unx     2414 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/model.py
+-rw-r--r--  2.0 unx     2724 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/perf.yaml
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/requirements.txt
+-rw-r--r--  2.0 unx     2492 b- defN 24-Apr-02 22:37 qai_hub_models/models/facebook_denoiser/test.py
+-rw-r--r--  2.0 unx      440 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/conftest.py
+-rw-r--r--  2.0 unx      762 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/demo.py
+-rw-r--r--  2.0 unx     8264 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/export.py
+-rw-r--r--  2.0 unx     1301 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/info.yaml
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/model.py
+-rw-r--r--  2.0 unx     2706 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/requirements.txt
+-rw-r--r--  2.0 unx     1332 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_s/test.py
+-rw-r--r--  2.0 unx      440 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/conftest.py
+-rw-r--r--  2.0 unx      762 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/demo.py
+-rw-r--r--  2.0 unx     8264 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/export.py
+-rw-r--r--  2.0 unx     1300 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/info.yaml
+-rw-r--r--  2.0 unx      683 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/model.py
+-rw-r--r--  2.0 unx     2707 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/requirements.txt
+-rw-r--r--  2.0 unx     1332 b- defN 24-Apr-02 22:37 qai_hub_models/models/fastsam_x/test.py
+-rw-r--r--  2.0 unx      410 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/__init__.py
+-rw-r--r--  2.0 unx     2683 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/app.py
+-rw-r--r--  2.0 unx      882 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/conftest.py
+-rw-r--r--  2.0 unx     2315 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/demo.py
+-rw-r--r--  2.0 unx     8169 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/export.py
+-rw-r--r--  2.0 unx     1241 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/info.yaml
+-rw-r--r--  2.0 unx     1989 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/model.py
+-rw-r--r--  2.0 unx     2761 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/perf.yaml
+-rw-r--r--  2.0 unx     1637 b- defN 24-Apr-02 22:37 qai_hub_models/models/fcn_resnet50/test.py
+-rw-r--r--  2.0 unx      487 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/__init__.py
+-rw-r--r--  2.0 unx      894 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/conftest.py
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/demo.py
+-rw-r--r--  2.0 unx     8050 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/export.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/info.yaml
+-rw-r--r--  2.0 unx      648 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/model.py
+-rw-r--r--  2.0 unx     2771 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt
+-rw-r--r--  2.0 unx      804 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_122ns_lowres/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/demo.py
+-rw-r--r--  2.0 unx     8014 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/export.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/model.py
+-rw-r--r--  2.0 unx     2764 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/__init__.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/demo.py
+-rw-r--r--  2.0 unx     8467 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1169 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/model.py
+-rw-r--r--  2.0 unx     2710 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_40s_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/demo.py
+-rw-r--r--  2.0 unx     8014 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/export.py
+-rw-r--r--  2.0 unx     1275 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/model.py
+-rw-r--r--  2.0 unx     2772 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/__init__.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/demo.py
+-rw-r--r--  2.0 unx     8467 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/model.py
+-rw-r--r--  2.0 unx     2714 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_54s_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/demo.py
+-rw-r--r--  2.0 unx     8014 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/export.py
+-rw-r--r--  2.0 unx     1279 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/model.py
+-rw-r--r--  2.0 unx     2772 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s/test.py
+-rw-r--r--  2.0 unx      485 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/__init__.py
+-rw-r--r--  2.0 unx      890 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/conftest.py
+-rw-r--r--  2.0 unx      601 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/demo.py
+-rw-r--r--  2.0 unx     8042 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/export.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/info.yaml
+-rw-r--r--  2.0 unx      640 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/model.py
+-rw-r--r--  2.0 unx     2767 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/requirements.txt
+-rw-r--r--  2.0 unx      794 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_lowres/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/__init__.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/demo.py
+-rw-r--r--  2.0 unx     8467 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/model.py
+-rw-r--r--  2.0 unx     2713 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-02 22:37 qai_hub_models/models/ffnet_78s_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/demo.py
+-rw-r--r--  2.0 unx     7879 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/export.py
+-rw-r--r--  2.0 unx     1295 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/info.yaml
+-rw-r--r--  2.0 unx      743 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/model.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/perf.yaml
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet/test.py
+-rw-r--r--  2.0 unx      573 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/__init__.py
+-rw-r--r--  2.0 unx      810 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/demo.py
+-rw-r--r--  2.0 unx     8359 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/export.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/info.yaml
+-rw-r--r--  2.0 unx     4079 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/model.py
+-rw-r--r--  2.0 unx     2753 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/perf.yaml
+-rw-r--r--  2.0 unx      885 b- defN 24-Apr-02 22:37 qai_hub_models/models/googlenet_quantized/test.py
+-rw-r--r--  2.0 unx      430 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/__init__.py
+-rw-r--r--  2.0 unx     5644 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/app.py
+-rw-r--r--  2.0 unx      878 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/conftest.py
+-rw-r--r--  2.0 unx     1739 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/demo.py
+-rw-r--r--  2.0 unx     8160 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/export.py
+-rw-r--r--  2.0 unx     1195 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/info.yaml
+-rw-r--r--  2.0 unx     2801 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/model.py
+-rw-r--r--  2.0 unx     2755 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/perf.yaml
+-rw-r--r--  2.0 unx       51 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/requirements.txt
+-rw-r--r--  2.0 unx     1420 b- defN 24-Apr-02 22:37 qai_hub_models/models/hrnet_pose/test.py
+-rw-r--r--  2.0 unx      434 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
+-rw-r--r--  2.0 unx     2133 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/app.py
+-rw-r--r--  2.0 unx      912 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
+-rw-r--r--  2.0 unx     1517 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py
+-rw-r--r--  2.0 unx     7567 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/export.py
+-rw-r--r--  2.0 unx     1248 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml
+-rw-r--r--  2.0 unx     7587 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/model.py
+-rw-r--r--  2.0 unx     2735 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml
+-rw-r--r--  2.0 unx       72 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt
+-rw-r--r--  2.0 unx     2560 b- defN 24-Apr-02 22:37 qai_hub_models/models/huggingface_wavlm_base_plus/test.py
+-rw-r--r--  2.0 unx      477 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/__init__.py
+-rw-r--r--  2.0 unx      796 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/conftest.py
+-rw-r--r--  2.0 unx      546 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/demo.py
+-rw-r--r--  2.0 unx     7891 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/export.py
+-rw-r--r--  2.0 unx     1359 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/info.yaml
+-rw-r--r--  2.0 unx      756 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/model.py
+-rw-r--r--  2.0 unx     2760 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/perf.yaml
+-rw-r--r--  2.0 unx      861 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3/test.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/__init__.py
+-rw-r--r--  2.0 unx      816 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/conftest.py
+-rw-r--r--  2.0 unx      591 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/demo.py
+-rw-r--r--  2.0 unx     8392 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/export.py
+-rw-r--r--  2.0 unx     1556 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/info.yaml
+-rw-r--r--  2.0 unx     7543 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/model.py
+-rw-r--r--  2.0 unx     2712 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/perf.yaml
+-rw-r--r--  2.0 unx      901 b- defN 24-Apr-02 22:37 qai_hub_models/models/inception_v3_quantized/test.py
+-rw-r--r--  2.0 unx      455 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/__init__.py
+-rw-r--r--  2.0 unx      882 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/conftest.py
+-rw-r--r--  2.0 unx      911 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/demo.py
+-rw-r--r--  2.0 unx     8179 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/export.py
+-rw-r--r--  2.0 unx     1082 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/info.yaml
+-rw-r--r--  2.0 unx     4977 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/model.py
+-rw-r--r--  2.0 unx     2776 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/perf.yaml
+-rw-r--r--  2.0 unx      171 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/requirements.txt
+-rw-r--r--  2.0 unx     2074 b- defN 24-Apr-02 22:37 qai_hub_models/models/lama_dilated/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/__init__.py
+-rw-r--r--  2.0 unx     3986 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/app.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/conftest.py
+-rw-r--r--  2.0 unx     2072 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/demo.py
+-rw-r--r--  2.0 unx     7638 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/export.py
+-rw-r--r--  2.0 unx     1112 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/info.yaml
+-rw-r--r--  2.0 unx     3788 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/model.py
+-rw-r--r--  2.0 unx     2711 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/perf.yaml
+-rw-r--r--  2.0 unx       39 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/requirements.txt
+-rw-r--r--  2.0 unx     1714 b- defN 24-Apr-02 22:37 qai_hub_models/models/litehrnet/test.py
+-rw-r--r--  2.0 unx     1993 b- defN 24-Apr-02 22:37 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml
+-rw-r--r--  2.0 unx     2037 b- defN 24-Apr-02 22:37 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/__init__.py
+-rw-r--r--  2.0 unx     2099 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/app.py
+-rw-r--r--  2.0 unx      886 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/conftest.py
+-rw-r--r--  2.0 unx     2862 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/demo.py
+-rw-r--r--  2.0 unx     9755 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/export.py
+-rw-r--r--  2.0 unx     1469 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/info.yaml
+-rw-r--r--  2.0 unx     7669 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/model.py
+-rw-r--r--  2.0 unx     4857 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/perf.yaml
+-rw-r--r--  2.0 unx     1441 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_face/test.py
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/__init__.py
+-rw-r--r--  2.0 unx     9819 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/app.py
+-rw-r--r--  2.0 unx      886 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/conftest.py
+-rw-r--r--  2.0 unx     2832 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/demo.py
+-rw-r--r--  2.0 unx     9755 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/export.py
+-rw-r--r--  2.0 unx     1374 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/info.yaml
+-rw-r--r--  2.0 unx     6017 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/model.py
+-rw-r--r--  2.0 unx     4858 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/perf.yaml
+-rw-r--r--  2.0 unx     1442 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_hand/test.py
+-rw-r--r--  2.0 unx      412 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/__init__.py
+-rw-r--r--  2.0 unx     4430 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/app.py
+-rw-r--r--  2.0 unx      886 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/conftest.py
+-rw-r--r--  2.0 unx     2889 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/demo.py
+-rw-r--r--  2.0 unx     9756 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/export.py
+-rw-r--r--  2.0 unx     1387 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/info.yaml
+-rw-r--r--  2.0 unx     5801 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/model.py
+-rw-r--r--  2.0 unx     4849 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/perf.yaml
+-rw-r--r--  2.0 unx     1443 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_pose/test.py
+-rw-r--r--  2.0 unx      362 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/app.py
+-rw-r--r--  2.0 unx      804 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/conftest.py
+-rw-r--r--  2.0 unx     2674 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/demo.py
+-rw-r--r--  2.0 unx     8198 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/export.py
+-rw-r--r--  2.0 unx     1455 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/info.yaml
+-rw-r--r--  2.0 unx    12352 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/model.py
+-rw-r--r--  2.0 unx     2775 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/perf.yaml
+-rw-r--r--  2.0 unx       15 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/requirements.txt
+-rw-r--r--  2.0 unx     1396 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/test.py
+-rw-r--r--  2.0 unx     2492 b- defN 24-Apr-02 22:37 qai_hub_models/models/mediapipe_selfie/utils.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/demo.py
+-rw-r--r--  2.0 unx     7879 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/export.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/info.yaml
+-rw-r--r--  2.0 unx      699 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/model.py
+-rw-r--r--  2.0 unx     2751 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/perf.yaml
+-rw-r--r--  2.0 unx      882 b- defN 24-Apr-02 22:37 qai_hub_models/models/mnasnet05/test.py
+-rw-r--r--  2.0 unx      474 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/__init__.py
+-rw-r--r--  2.0 unx      882 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/conftest.py
+-rw-r--r--  2.0 unx      540 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/demo.py
+-rw-r--r--  2.0 unx     7891 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/export.py
+-rw-r--r--  2.0 unx     1380 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/info.yaml
+-rw-r--r--  2.0 unx     2595 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/model.py
+-rw-r--r--  2.0 unx     2755 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/perf.yaml
+-rw-r--r--  2.0 unx     1091 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2/test.py
+-rw-r--r--  2.0 unx      485 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/__init__.py
+-rw-r--r--  2.0 unx      902 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/conftest.py
+-rw-r--r--  2.0 unx      585 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/demo.py
+-rw-r--r--  2.0 unx     8372 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/export.py
+-rw-r--r--  2.0 unx     1362 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/info.yaml
+-rw-r--r--  2.0 unx     3760 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/model.py
+-rw-r--r--  2.0 unx     2759 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1002 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v2_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/__init__.py
+-rw-r--r--  2.0 unx      808 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/conftest.py
+-rw-r--r--  2.0 unx      556 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/demo.py
+-rw-r--r--  2.0 unx     7935 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/export.py
+-rw-r--r--  2.0 unx     1340 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/info.yaml
+-rw-r--r--  2.0 unx      721 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/model.py
+-rw-r--r--  2.0 unx     2708 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/perf.yaml
+-rw-r--r--  2.0 unx      879 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large/test.py
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py
+-rw-r--r--  2.0 unx      828 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py
+-rw-r--r--  2.0 unx      748 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py
+-rw-r--r--  2.0 unx     8104 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/export.py
+-rw-r--r--  2.0 unx     1374 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml
+-rw-r--r--  2.0 unx     2865 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/model.py
+-rw-r--r--  2.0 unx     2723 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml
+-rw-r--r--  2.0 unx      917 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_large_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/__init__.py
+-rw-r--r--  2.0 unx      808 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/conftest.py
+-rw-r--r--  2.0 unx      556 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/demo.py
+-rw-r--r--  2.0 unx     7935 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/export.py
+-rw-r--r--  2.0 unx     1338 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/info.yaml
+-rw-r--r--  2.0 unx      721 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/model.py
+-rw-r--r--  2.0 unx     2707 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/perf.yaml
+-rw-r--r--  2.0 unx      879 b- defN 24-Apr-02 22:37 qai_hub_models/models/mobilenet_v3_small/test.py
+-rw-r--r--  2.0 unx      394 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/__init__.py
+-rw-r--r--  2.0 unx     3958 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/app.py
+-rw-r--r--  2.0 unx      880 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/conftest.py
+-rw-r--r--  2.0 unx     3262 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/demo.py
+-rw-r--r--  2.0 unx     9666 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/export.py
+-rw-r--r--  2.0 unx     1494 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/info.yaml
+-rw-r--r--  2.0 unx     5295 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/model.py
+-rw-r--r--  2.0 unx     4855 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/perf.yaml
+-rw-r--r--  2.0 unx       29 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/requirements.txt
+-rw-r--r--  2.0 unx     2118 b- defN 24-Apr-02 22:37 qai_hub_models/models/openai_clip/test.py
+-rw-r--r--  2.0 unx      402 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/__init__.py
+-rw-r--r--  2.0 unx    12008 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/app.py
+-rw-r--r--  2.0 unx      874 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/conftest.py
+-rw-r--r--  2.0 unx     2053 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/demo.py
+-rw-r--r--  2.0 unx     8171 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/export.py
+-rw-r--r--  2.0 unx     1246 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/info.yaml
+-rw-r--r--  2.0 unx     5084 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/model.py
+-rw-r--r--  2.0 unx     2762 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/requirements.txt
+-rw-r--r--  2.0 unx     1321 b- defN 24-Apr-02 22:37 qai_hub_models/models/openpose/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/__init__.py
+-rw-r--r--  2.0 unx      888 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/conftest.py
+-rw-r--r--  2.0 unx      972 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/demo.py
+-rw-r--r--  2.0 unx     8181 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/export.py
+-rw-r--r--  2.0 unx     1248 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/info.yaml
+-rw-r--r--  2.0 unx     3170 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/model.py
+-rw-r--r--  2.0 unx     2753 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/perf.yaml
+-rw-r--r--  2.0 unx     1422 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py
+-rw-r--r--  2.0 unx      908 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py
+-rw-r--r--  2.0 unx      891 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/demo.py
+-rw-r--r--  2.0 unx     8634 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/export.py
+-rw-r--r--  2.0 unx     1274 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml
+-rw-r--r--  2.0 unx     4587 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/model.py
+-rw-r--r--  2.0 unx     2712 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2921 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetlarge_quantized/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/__init__.py
+-rw-r--r--  2.0 unx      890 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/conftest.py
+-rw-r--r--  2.0 unx      976 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/demo.py
+-rw-r--r--  2.0 unx     8185 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/export.py
+-rw-r--r--  2.0 unx     1242 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/info.yaml
+-rw-r--r--  2.0 unx     3177 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/model.py
+-rw-r--r--  2.0 unx     2754 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/perf.yaml
+-rw-r--r--  2.0 unx     1428 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium/test.py
+-rw-r--r--  2.0 unx      484 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py
+-rw-r--r--  2.0 unx      910 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py
+-rw-r--r--  2.0 unx      900 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/demo.py
+-rw-r--r--  2.0 unx     8638 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/export.py
+-rw-r--r--  2.0 unx     1282 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml
+-rw-r--r--  2.0 unx     4597 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/model.py
+-rw-r--r--  2.0 unx     2711 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2912 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetmedium_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/__init__.py
+-rw-r--r--  2.0 unx      888 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/conftest.py
+-rw-r--r--  2.0 unx      972 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/demo.py
+-rw-r--r--  2.0 unx     8181 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/export.py
+-rw-r--r--  2.0 unx     1238 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/info.yaml
+-rw-r--r--  2.0 unx     3170 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/model.py
+-rw-r--r--  2.0 unx     2752 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/perf.yaml
+-rw-r--r--  2.0 unx     1422 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py
+-rw-r--r--  2.0 unx      908 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py
+-rw-r--r--  2.0 unx      891 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/demo.py
+-rw-r--r--  2.0 unx     8634 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/export.py
+-rw-r--r--  2.0 unx     1278 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml
+-rw-r--r--  2.0 unx     4577 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/model.py
+-rw-r--r--  2.0 unx     2710 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2859 b- defN 24-Apr-02 22:37 qai_hub_models/models/quicksrnetsmall_quantized/test.py
+-rw-r--r--  2.0 unx      481 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py
+-rw-r--r--  2.0 unx      906 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py
+-rw-r--r--  2.0 unx     1280 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/demo.py
+-rw-r--r--  2.0 unx     8217 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/export.py
+-rw-r--r--  2.0 unx     1206 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml
+-rw-r--r--  2.0 unx     5226 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/model.py
+-rw-r--r--  2.0 unx     2765 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml
+-rw-r--r--  2.0 unx       44 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt
+-rw-r--r--  2.0 unx     1480 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_general_x4v3/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/__init__.py
+-rw-r--r--  2.0 unx      894 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/conftest.py
+-rw-r--r--  2.0 unx     1256 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/demo.py
+-rw-r--r--  2.0 unx     7654 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/export.py
+-rw-r--r--  2.0 unx     1330 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/info.yaml
+-rw-r--r--  2.0 unx     4443 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/model.py
+-rw-r--r--  2.0 unx     2720 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/perf.yaml
+-rw-r--r--  2.0 unx       44 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/requirements.txt
+-rw-r--r--  2.0 unx     1440 b- defN 24-Apr-02 22:37 qai_hub_models/models/real_esrgan_x4plus/test.py
+-rw-r--r--  2.0 unx      469 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/__init__.py
+-rw-r--r--  2.0 unx      784 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/conftest.py
+-rw-r--r--  2.0 unx      524 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/demo.py
+-rw-r--r--  2.0 unx     7867 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/export.py
+-rw-r--r--  2.0 unx     1291 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/info.yaml
+-rw-r--r--  2.0 unx      635 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/model.py
+-rw-r--r--  2.0 unx     2755 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/perf.yaml
+-rw-r--r--  2.0 unx      984 b- defN 24-Apr-02 22:37 qai_hub_models/models/regnet/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/demo.py
+-rw-r--r--  2.0 unx     7879 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/export.py
+-rw-r--r--  2.0 unx     1312 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/info.yaml
+-rw-r--r--  2.0 unx      609 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/model.py
+-rw-r--r--  2.0 unx     2759 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/perf.yaml
+-rw-r--r--  2.0 unx      961 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/__init__.py
+-rw-r--r--  2.0 unx      810 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/demo.py
+-rw-r--r--  2.0 unx     8359 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/export.py
+-rw-r--r--  2.0 unx     1346 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/info.yaml
+-rw-r--r--  2.0 unx     3000 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/model.py
+-rw-r--r--  2.0 unx     2765 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/perf.yaml
+-rw-r--r--  2.0 unx      921 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet101_quantized/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/__init__.py
+-rw-r--r--  2.0 unx      788 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/conftest.py
+-rw-r--r--  2.0 unx      530 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/demo.py
+-rw-r--r--  2.0 unx     7875 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/info.yaml
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/model.py
+-rw-r--r--  2.0 unx     2746 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/perf.yaml
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18/test.py
+-rw-r--r--  2.0 unx      482 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/__init__.py
+-rw-r--r--  2.0 unx      808 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/conftest.py
+-rw-r--r--  2.0 unx      562 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/demo.py
+-rw-r--r--  2.0 unx     8355 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/export.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/info.yaml
+-rw-r--r--  2.0 unx     2802 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/model.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/perf.yaml
+-rw-r--r--  2.0 unx      917 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet18_quantized/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/__init__.py
+-rw-r--r--  2.0 unx      788 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/conftest.py
+-rw-r--r--  2.0 unx      530 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/demo.py
+-rw-r--r--  2.0 unx     7875 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/export.py
+-rw-r--r--  2.0 unx     1303 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/info.yaml
+-rw-r--r--  2.0 unx      607 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/model.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/perf.yaml
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnet50/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/__init__.py
+-rw-r--r--  2.0 unx      792 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/conftest.py
+-rw-r--r--  2.0 unx      536 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/demo.py
+-rw-r--r--  2.0 unx     7883 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/export.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/info.yaml
+-rw-r--r--  2.0 unx      617 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/model.py
+-rw-r--r--  2.0 unx     2761 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/perf.yaml
+-rw-r--r--  2.0 unx      897 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101/test.py
+-rw-r--r--  2.0 unx      484 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/__init__.py
+-rw-r--r--  2.0 unx      812 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/conftest.py
+-rw-r--r--  2.0 unx      581 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/demo.py
+-rw-r--r--  2.0 unx     8383 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/export.py
+-rw-r--r--  2.0 unx     1365 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/info.yaml
+-rw-r--r--  2.0 unx     2807 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/model.py
+-rw-r--r--  2.0 unx     2708 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/perf.yaml
+-rw-r--r--  2.0 unx      925 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext101_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/demo.py
+-rw-r--r--  2.0 unx     7879 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/export.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/info.yaml
+-rw-r--r--  2.0 unx      704 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/model.py
+-rw-r--r--  2.0 unx     2754 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/perf.yaml
+-rw-r--r--  2.0 unx      840 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/__init__.py
+-rw-r--r--  2.0 unx      810 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/demo.py
+-rw-r--r--  2.0 unx     8379 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/export.py
+-rw-r--r--  2.0 unx     1362 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/info.yaml
+-rw-r--r--  2.0 unx     2798 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/model.py
+-rw-r--r--  2.0 unx     2705 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/perf.yaml
+-rw-r--r--  2.0 unx      921 b- defN 24-Apr-02 22:37 qai_hub_models/models/resnext50_quantized/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/__init__.py
+-rw-r--r--  2.0 unx     5101 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/app.py
+-rw-r--r--  2.0 unx      905 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/conftest.py
+-rw-r--r--  2.0 unx     3088 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/demo.py
+-rw-r--r--  2.0 unx    10152 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/export.py
+-rw-r--r--  2.0 unx     1391 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/info.yaml
+-rw-r--r--  2.0 unx    12000 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/model.py
+-rw-r--r--  2.0 unx     2708 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/perf.yaml
+-rw-r--r--  2.0 unx       37 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/requirements.txt
+-rw-r--r--  2.0 unx     3062 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/test.py
+-rw-r--r--  2.0 unx      826 b- defN 24-Apr-02 22:37 qai_hub_models/models/sam/utils.py
+-rw-r--r--  2.0 unx      464 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/__init__.py
+-rw-r--r--  2.0 unx      872 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/conftest.py
+-rw-r--r--  2.0 unx      923 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/demo.py
+-rw-r--r--  2.0 unx     8006 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/export.py
+-rw-r--r--  2.0 unx     1104 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/info.yaml
+-rw-r--r--  2.0 unx     2984 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/model.py
+-rw-r--r--  2.0 unx     2745 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/perf.yaml
+-rw-r--r--  2.0 unx     1471 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/__init__.py
+-rw-r--r--  2.0 unx      892 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/conftest.py
+-rw-r--r--  2.0 unx      990 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/demo.py
+-rw-r--r--  2.0 unx     8167 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/export.py
+-rw-r--r--  2.0 unx     1151 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/info.yaml
+-rw-r--r--  2.0 unx     4269 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/model.py
+-rw-r--r--  2.0 unx     2704 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2927 b- defN 24-Apr-02 22:37 qai_hub_models/models/sesr_m5_quantized/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/__init__.py
+-rw-r--r--  2.0 unx      798 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/conftest.py
+-rw-r--r--  2.0 unx      543 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/demo.py
+-rw-r--r--  2.0 unx     7895 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/export.py
+-rw-r--r--  2.0 unx     1353 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/info.yaml
+-rw-r--r--  2.0 unx      713 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/model.py
+-rw-r--r--  2.0 unx     2757 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/perf.yaml
+-rw-r--r--  2.0 unx      857 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2/test.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/__init__.py
+-rw-r--r--  2.0 unx      818 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/conftest.py
+-rw-r--r--  2.0 unx      588 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/demo.py
+-rw-r--r--  2.0 unx     8375 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/export.py
+-rw-r--r--  2.0 unx     1383 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/info.yaml
+-rw-r--r--  2.0 unx     5779 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/model.py
+-rw-r--r--  2.0 unx     2767 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/perf.yaml
+-rw-r--r--  2.0 unx      899 b- defN 24-Apr-02 22:37 qai_hub_models/models/shufflenet_v2_quantized/test.py
+-rw-r--r--  2.0 unx      396 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/__init__.py
+-rw-r--r--  2.0 unx     3793 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/app.py
+-rw-r--r--  2.0 unx      868 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/conftest.py
+-rw-r--r--  2.0 unx     1657 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/demo.py
+-rw-r--r--  2.0 unx     8141 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/export.py
+-rw-r--r--  2.0 unx     1260 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/info.yaml
+-rw-r--r--  2.0 unx     4770 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/model.py
+-rw-r--r--  2.0 unx     2746 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/perf.yaml
+-rw-r--r--  2.0 unx     1355 b- defN 24-Apr-02 22:37 qai_hub_models/models/sinet/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/__init__.py
+-rw-r--r--  2.0 unx      798 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/conftest.py
+-rw-r--r--  2.0 unx      539 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/demo.py
+-rw-r--r--  2.0 unx     7896 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/export.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/info.yaml
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/model.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/perf.yaml
+-rw-r--r--  2.0 unx      851 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1/test.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/__init__.py
+-rw-r--r--  2.0 unx      818 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/conftest.py
+-rw-r--r--  2.0 unx      584 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/demo.py
+-rw-r--r--  2.0 unx     8328 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/export.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/info.yaml
+-rw-r--r--  2.0 unx     2813 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/model.py
+-rw-r--r--  2.0 unx     2757 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
+-rw-r--r--  2.0 unx      895 b- defN 24-Apr-02 22:37 qai_hub_models/models/squeezenet1_1_quantized/test.py
+-rw-r--r--  2.0 unx      540 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/__init__.py
+-rw-r--r--  2.0 unx     7966 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/app.py
+-rw-r--r--  2.0 unx     5765 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/demo.py
+-rw-r--r--  2.0 unx     7432 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/export.py
+-rw-r--r--  2.0 unx     1355 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/info.yaml
+-rw-r--r--  2.0 unx     3604 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/model.py
+-rw-r--r--  2.0 unx     6384 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/perf.yaml
+-rw-r--r--  2.0 unx       46 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1599 b- defN 24-Apr-02 22:37 qai_hub_models/models/stable_diffusion_quantized/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/__init__.py
+-rw-r--r--  2.0 unx     4155 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/app.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/conftest.py
+-rw-r--r--  2.0 unx     2847 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/demo.py
+-rw-r--r--  2.0 unx     7762 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/export.py
+-rw-r--r--  2.0 unx     1102 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/info.yaml
+-rw-r--r--  2.0 unx     8406 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/model.py
+-rw-r--r--  2.0 unx     2722 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/perf.yaml
+-rw-r--r--  2.0 unx       11 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/requirements.txt
+-rw-r--r--  2.0 unx     2497 b- defN 24-Apr-02 22:37 qai_hub_models/models/stylegan2/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/conftest.py
+-rw-r--r--  2.0 unx      531 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/demo.py
+-rw-r--r--  2.0 unx     7899 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/export.py
+-rw-r--r--  2.0 unx     1383 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/info.yaml
+-rw-r--r--  2.0 unx     1241 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/model.py
+-rw-r--r--  2.0 unx     2709 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/perf.yaml
+-rw-r--r--  2.0 unx     1251 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_base/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/__init__.py
+-rw-r--r--  2.0 unx      792 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/conftest.py
+-rw-r--r--  2.0 unx      534 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/demo.py
+-rw-r--r--  2.0 unx     7903 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/export.py
+-rw-r--r--  2.0 unx     1378 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/info.yaml
+-rw-r--r--  2.0 unx     1242 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/model.py
+-rw-r--r--  2.0 unx     2709 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/perf.yaml
+-rw-r--r--  2.0 unx     1257 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_small/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/__init__.py
+-rw-r--r--  2.0 unx      790 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/conftest.py
+-rw-r--r--  2.0 unx      531 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/demo.py
+-rw-r--r--  2.0 unx     7899 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/export.py
+-rw-r--r--  2.0 unx     1376 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/info.yaml
+-rw-r--r--  2.0 unx     1241 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/model.py
+-rw-r--r--  2.0 unx     2703 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/perf.yaml
+-rw-r--r--  2.0 unx     1369 b- defN 24-Apr-02 22:37 qai_hub_models/models/swin_tiny/test.py
+-rw-r--r--  2.0 unx      396 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/__init__.py
+-rw-r--r--  2.0 unx    10207 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/app.py
+-rw-r--r--  2.0 unx      782 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/conftest.py
+-rw-r--r--  2.0 unx     1779 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/demo.py
+-rw-r--r--  2.0 unx     9655 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/export.py
+-rw-r--r--  2.0 unx     1370 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/info.yaml
+-rw-r--r--  2.0 unx    10482 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/model.py
+-rw-r--r--  2.0 unx     4735 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/perf.yaml
+-rw-r--r--  2.0 unx       42 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/requirements.txt
+-rw-r--r--  2.0 unx     2357 b- defN 24-Apr-02 22:37 qai_hub_models/models/trocr/test.py
+-rw-r--r--  2.0 unx      348 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/__init__.py
+-rw-r--r--  2.0 unx     1305 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/app.py
+-rw-r--r--  2.0 unx      806 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/conftest.py
+-rw-r--r--  2.0 unx     2509 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/demo.py
+-rw-r--r--  2.0 unx     8189 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/info.yaml
+-rw-r--r--  2.0 unx     2666 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/model.py
+-rw-r--r--  2.0 unx     2774 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/perf.yaml
+-rw-r--r--  2.0 unx     1215 b- defN 24-Apr-02 22:37 qai_hub_models/models/unet_segmentation/test.py
+-rw-r--r--  2.0 unx      466 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/__init__.py
+-rw-r--r--  2.0 unx      778 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/conftest.py
+-rw-r--r--  2.0 unx      515 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/demo.py
+-rw-r--r--  2.0 unx     7908 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/export.py
+-rw-r--r--  2.0 unx     1342 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/info.yaml
+-rw-r--r--  2.0 unx      685 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/model.py
+-rw-r--r--  2.0 unx     2701 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/perf.yaml
+-rw-r--r--  2.0 unx      807 b- defN 24-Apr-02 22:37 qai_hub_models/models/vit/test.py
+-rw-r--r--  2.0 unx      444 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/__init__.py
+-rw-r--r--  2.0 unx      802 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/conftest.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/demo.py
+-rw-r--r--  2.0 unx     9707 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/export.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/info.yaml
+-rw-r--r--  2.0 unx      558 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/model.py
+-rw-r--r--  2.0 unx     4743 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_base_en/test.py
+-rw-r--r--  2.0 unx      445 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/__init__.py
+-rw-r--r--  2.0 unx      804 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/conftest.py
+-rw-r--r--  2.0 unx      486 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/demo.py
+-rw-r--r--  2.0 unx     9711 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/export.py
+-rw-r--r--  2.0 unx     1848 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/info.yaml
+-rw-r--r--  2.0 unx      560 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/model.py
+-rw-r--r--  2.0 unx     4753 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/perf.yaml
+-rw-r--r--  2.0 unx       38 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_small_en/test.py
+-rw-r--r--  2.0 unx      444 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/__init__.py
+-rw-r--r--  2.0 unx      802 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/conftest.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/demo.py
+-rw-r--r--  2.0 unx     9707 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/export.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/info.yaml
+-rw-r--r--  2.0 unx      558 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/model.py
+-rw-r--r--  2.0 unx     4733 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-Apr-02 22:37 qai_hub_models/models/whisper_tiny_en/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/__init__.py
+-rw-r--r--  2.0 unx      796 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/conftest.py
+-rw-r--r--  2.0 unx      542 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/demo.py
+-rw-r--r--  2.0 unx     7891 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/export.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/info.yaml
+-rw-r--r--  2.0 unx      710 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/model.py
+-rw-r--r--  2.0 unx     2760 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/perf.yaml
+-rw-r--r--  2.0 unx      855 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50/test.py
+-rw-r--r--  2.0 unx      582 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/__init__.py
+-rw-r--r--  2.0 unx      816 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/conftest.py
+-rw-r--r--  2.0 unx      587 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/demo.py
+-rw-r--r--  2.0 unx     8324 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/export.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/info.yaml
+-rw-r--r--  2.0 unx     3004 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/model.py
+-rw-r--r--  2.0 unx     2760 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/perf.yaml
+-rw-r--r--  2.0 unx      932 b- defN 24-Apr-02 22:37 qai_hub_models/models/wideresnet50_quantized/test.py
+-rw-r--r--  2.0 unx      461 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/__init__.py
+-rw-r--r--  2.0 unx      866 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/conftest.py
+-rw-r--r--  2.0 unx      742 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/demo.py
+-rw-r--r--  2.0 unx     7994 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/export.py
+-rw-r--r--  2.0 unx     1156 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/info.yaml
+-rw-r--r--  2.0 unx     3403 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/model.py
+-rw-r--r--  2.0 unx     2743 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/perf.yaml
+-rw-r--r--  2.0 unx     1402 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/__init__.py
+-rw-r--r--  2.0 unx      886 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/conftest.py
+-rw-r--r--  2.0 unx      956 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/demo.py
+-rw-r--r--  2.0 unx     8447 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/export.py
+-rw-r--r--  2.0 unx     1191 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/info.yaml
+-rw-r--r--  2.0 unx     3915 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/model.py
+-rw-r--r--  2.0 unx     2701 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1607 b- defN 24-Apr-02 22:37 qai_hub_models/models/xlsr_quantized/test.py
+-rw-r--r--  2.0 unx      436 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/__init__.py
+-rw-r--r--  2.0 unx     1071 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/app.py
+-rw-r--r--  2.0 unx      870 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/conftest.py
+-rw-r--r--  2.0 unx     1027 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/demo.py
+-rw-r--r--  2.0 unx     7897 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/export.py
+-rw-r--r--  2.0 unx     1168 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/info.yaml
+-rw-r--r--  2.0 unx     4686 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/model.py
+-rw-r--r--  2.0 unx     2760 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/perf.yaml
+-rw-r--r--  2.0 unx     1845 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov6/test.py
+-rw-r--r--  2.0 unx      436 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/__init__.py
+-rw-r--r--  2.0 unx     1071 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/app.py
+-rw-r--r--  2.0 unx      870 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/conftest.py
+-rw-r--r--  2.0 unx      909 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/demo.py
+-rw-r--r--  2.0 unx     7917 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/export.py
+-rw-r--r--  2.0 unx     1131 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/info.yaml
+-rw-r--r--  2.0 unx     9640 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/model.py
+-rw-r--r--  2.0 unx     2707 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/perf.yaml
+-rw-r--r--  2.0 unx       47 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/requirements.txt
+-rw-r--r--  2.0 unx     2323 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov7/test.py
+-rw-r--r--  2.0 unx      415 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/__init__.py
+-rw-r--r--  2.0 unx      892 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/app.py
+-rw-r--r--  2.0 unx      792 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/conftest.py
+-rw-r--r--  2.0 unx      926 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/demo.py
+-rw-r--r--  2.0 unx     7951 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/export.py
+-rw-r--r--  2.0 unx     1163 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/info.yaml
+-rw-r--r--  2.0 unx     4637 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/model.py
+-rw-r--r--  2.0 unx     2768 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/requirements.txt
+-rw-r--r--  2.0 unx     2080 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_det/test.py
+-rw-r--r--  2.0 unx      419 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/__init__.py
+-rw-r--r--  2.0 unx     7698 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/app.py
+-rw-r--r--  2.0 unx      792 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/conftest.py
+-rw-r--r--  2.0 unx     3155 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/demo.py
+-rw-r--r--  2.0 unx     7974 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/export.py
+-rw-r--r--  2.0 unx     1273 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/info.yaml
+-rw-r--r--  2.0 unx     4663 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/model.py
+-rw-r--r--  2.0 unx     2710 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/requirements.txt
+-rw-r--r--  2.0 unx     2536 b- defN 24-Apr-02 22:37 qai_hub_models/models/yolov8_seg/test.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/test/__init__.py
+-rw-r--r--  2.0 unx     1043 b- defN 24-Apr-02 22:37 qai_hub_models/test/test_async_compile_jobs.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/test/e2e/__init__.py
+-rw-r--r--  2.0 unx     1661 b- defN 24-Apr-02 22:37 qai_hub_models/test/e2e/test_aimet_compile.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/test/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1493 b- defN 24-Apr-02 22:37 qai_hub_models/test/test_utils/perf.yaml
+-rw-r--r--  2.0 unx     3229 b- defN 24-Apr-02 22:37 qai_hub_models/test/test_utils/test_info_specs.py
+-rw-r--r--  2.0 unx     6515 b- defN 24-Apr-02 22:37 qai_hub_models/test/test_utils/test_perf_summary.py
+-rw-r--r--  2.0 unx     3295 b- defN 24-Apr-02 22:37 qai_hub_models/test/test_utils/test_qai_hub_helpers.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/utils/__init__.py
+-rw-r--r--  2.0 unx    16452 b- defN 24-Apr-02 22:37 qai_hub_models/utils/args.py
+-rw-r--r--  2.0 unx    33565 b- defN 24-Apr-02 22:37 qai_hub_models/utils/asset_loaders.py
+-rw-r--r--  2.0 unx     6052 b- defN 24-Apr-02 22:37 qai_hub_models/utils/base_model.py
+-rw-r--r--  2.0 unx     8707 b- defN 24-Apr-02 22:37 qai_hub_models/utils/bounding_box_processing.py
+-rw-r--r--  2.0 unx     1771 b- defN 24-Apr-02 22:37 qai_hub_models/utils/camera_capture.py
+-rw-r--r--  2.0 unx     5222 b- defN 24-Apr-02 22:37 qai_hub_models/utils/compare.py
+-rw-r--r--  2.0 unx    32743 b- defN 24-Apr-02 22:37 qai_hub_models/utils/config_loaders.py
+-rw-r--r--  2.0 unx     3066 b- defN 24-Apr-02 22:37 qai_hub_models/utils/display.py
+-rw-r--r--  2.0 unx     6403 b- defN 24-Apr-02 22:37 qai_hub_models/utils/draw.py
+-rw-r--r--  2.0 unx     1549 b- defN 24-Apr-02 22:37 qai_hub_models/utils/huggingface.py
+-rw-r--r--  2.0 unx    12490 b- defN 24-Apr-02 22:37 qai_hub_models/utils/image_processing.py
+-rw-r--r--  2.0 unx    12482 b- defN 24-Apr-02 22:37 qai_hub_models/utils/inference.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-Apr-02 22:37 qai_hub_models/utils/input_spec.py
+-rw-r--r--  2.0 unx     4559 b- defN 24-Apr-02 22:37 qai_hub_models/utils/measurement.py
+-rw-r--r--  2.0 unx     1577 b- defN 24-Apr-02 22:37 qai_hub_models/utils/model_adapters.py
+-rw-r--r--  2.0 unx    12621 b- defN 24-Apr-02 22:37 qai_hub_models/utils/model_card.py
+-rw-r--r--  2.0 unx     1406 b- defN 24-Apr-02 22:37 qai_hub_models/utils/path_helpers.py
+-rw-r--r--  2.0 unx    11378 b- defN 24-Apr-02 22:37 qai_hub_models/utils/perf_summary.py
+-rw-r--r--  2.0 unx     4858 b- defN 24-Apr-02 22:37 qai_hub_models/utils/printing.py
+-rw-r--r--  2.0 unx     5365 b- defN 24-Apr-02 22:37 qai_hub_models/utils/qai_hub_helpers.py
+-rw-r--r--  2.0 unx     1463 b- defN 24-Apr-02 22:37 qai_hub_models/utils/qnn_helpers.py
+-rw-r--r--  2.0 unx     2170 b- defN 24-Apr-02 22:37 qai_hub_models/utils/quantization.py
+-rw-r--r--  2.0 unx    11791 b- defN 24-Apr-02 22:37 qai_hub_models/utils/quantization_aimet.py
+-rw-r--r--  2.0 unx      754 b- defN 24-Apr-02 22:37 qai_hub_models/utils/test_compare.py
+-rw-r--r--  2.0 unx     3173 b- defN 24-Apr-02 22:37 qai_hub_models/utils/testing.py
+-rw-r--r--  2.0 unx      259 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/config_loader.py
+-rw-r--r--  2.0 unx      993 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/default_config.json
+-rw-r--r--  2.0 unx      946 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/default_config_legacy_v1.json
+-rw-r--r--  2.0 unx      955 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/default_config_legacy_v2.json
+-rw-r--r--  2.0 unx      919 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
+-rw-r--r--  2.0 unx     1187 b- defN 24-Apr-02 22:37 qai_hub_models/utils/aimet/repo.py
+-rw-r--r--  2.0 unx     1481 b- defN 24-Apr-02 22:41 qai_hub_models-0.4.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    40686 b- defN 24-Apr-02 22:41 qai_hub_models-0.4.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-02 22:41 qai_hub_models-0.4.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 24-Apr-02 22:41 qai_hub_models-0.4.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    92338 b- defN 24-Apr-02 22:41 qai_hub_models-0.4.1.dist-info/RECORD
+903 files, 2444307 bytes uncompressed, 810084 bytes compressed:  66.9%
```

## zipnote {}

```diff
@@ -738,14 +738,17 @@
 
 Filename: qai_hub_models/models/ffnet_40s_quantized/model.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_40s_quantized/perf.yaml
 Comment: 
 
+Filename: qai_hub_models/models/ffnet_40s_quantized/requirements.txt
+Comment: 
+
 Filename: qai_hub_models/models/ffnet_40s_quantized/test.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_54s/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_54s/conftest.py
@@ -789,14 +792,17 @@
 
 Filename: qai_hub_models/models/ffnet_54s_quantized/model.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_54s_quantized/perf.yaml
 Comment: 
 
+Filename: qai_hub_models/models/ffnet_54s_quantized/requirements.txt
+Comment: 
+
 Filename: qai_hub_models/models/ffnet_54s_quantized/test.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_78s/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_78s/conftest.py
@@ -867,14 +873,17 @@
 
 Filename: qai_hub_models/models/ffnet_78s_quantized/model.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_78s_quantized/perf.yaml
 Comment: 
 
+Filename: qai_hub_models/models/ffnet_78s_quantized/requirements.txt
+Comment: 
+
 Filename: qai_hub_models/models/ffnet_78s_quantized/test.py
 Comment: 
 
 Filename: qai_hub_models/models/googlenet/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/googlenet/conftest.py
@@ -948,41 +957,14 @@
 
 Filename: qai_hub_models/models/hrnet_pose/requirements.txt
 Comment: 
 
 Filename: qai_hub_models/models/hrnet_pose/test.py
 Comment: 
 
-Filename: qai_hub_models/models/hrnet_pose_quantized/__init__.py
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/conftest.py
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/demo.py
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/export.py
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/info.yaml
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/model.py
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/perf.yaml
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/requirements.txt
-Comment: 
-
-Filename: qai_hub_models/models/hrnet_pose_quantized/test.py
-Comment: 
-
 Filename: qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/huggingface_wavlm_base_plus/app.py
 Comment: 
 
 Filename: qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
@@ -2340,17 +2322,14 @@
 
 Filename: qai_hub_models/models/whisper_small_en/requirements.txt
 Comment: 
 
 Filename: qai_hub_models/models/whisper_small_en/test.py
 Comment: 
 
-Filename: qai_hub_models/models/whisper_small_multi/code-gen.yaml
-Comment: 
-
 Filename: qai_hub_models/models/whisper_tiny_en/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/whisper_tiny_en/conftest.py
 Comment: 
 
 Filename: qai_hub_models/models/whisper_tiny_en/demo.py
@@ -2709,23 +2688,23 @@
 
 Filename: qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
 Comment: 
 
 Filename: qai_hub_models/utils/aimet/repo.py
 Comment: 
 
-Filename: qai_hub_models-0.4.0.dist-info/LICENSE
+Filename: qai_hub_models-0.4.1.dist-info/LICENSE
 Comment: 
 
-Filename: qai_hub_models-0.4.0.dist-info/METADATA
+Filename: qai_hub_models-0.4.1.dist-info/METADATA
 Comment: 
 
-Filename: qai_hub_models-0.4.0.dist-info/WHEEL
+Filename: qai_hub_models-0.4.1.dist-info/WHEEL
 Comment: 
 
-Filename: qai_hub_models-0.4.0.dist-info/top_level.txt
+Filename: qai_hub_models-0.4.1.dist-info/top_level.txt
 Comment: 
 
-Filename: qai_hub_models-0.4.0.dist-info/RECORD
+Filename: qai_hub_models-0.4.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qai_hub_models/_version.py

```diff
@@ -1,5 +1,5 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
-__version__ = "0.4.0"
+__version__ = "0.4.1"
```

## qai_hub_models/models/common.py

```diff
@@ -7,14 +7,15 @@
 
 import numpy as np
 
 
 class TargetRuntime(Enum):
     TFLITE = 0
     QNN = 1
+    ORT = 2
 
     def __str__(self):
         return self.name.lower()
 
 
 class SourceModelFormat(Enum):
     ONNX = 0
```

## qai_hub_models/models/protocols.py

```diff
@@ -15,20 +15,25 @@
 checker that the class that inherits the mixin must implement HubModelProtocol.
 
 These are type checked at compile time.
 """
 from __future__ import annotations
 
 from abc import abstractmethod
-from typing import Protocol, Type, TypeVar, runtime_checkable
+from pathlib import Path
+from typing import Any, Protocol, Type, TypeVar, runtime_checkable
 
-from qai_hub.client import DatasetEntries
+from qai_hub.client import DatasetEntries, SourceModel
 
 from qai_hub_models.evaluators.base_evaluators import BaseEvaluator, _DataLoader
-from qai_hub_models.models.common import SampleInputsType, TargetRuntime
+from qai_hub_models.models.common import (
+    SampleInputsType,
+    SourceModelFormat,
+    TargetRuntime,
+)
 from qai_hub_models.utils.input_spec import InputSpec
 
 FromPretrainedTypeVar = TypeVar("FromPretrainedTypeVar", bound="FromPretrainedProtocol")
 
 FromPrecompiledTypeVar = TypeVar(
     "FromPrecompiledTypeVar", bound="FromPrecompiledProtocol"
 )
@@ -71,18 +76,18 @@
     """
 
     @abstractmethod
     def quantize(
         self,
         data: _DataLoader,
         num_samples: int | None = None,
-        evaluator: BaseEvaluator | None = None,
         device: str = "cpu",
         requantize_model_weights=False,
-    ) -> float | None:
+        data_has_gt=False,
+    ) -> None:
         """
         Compute quantization encodings for this model with the given dataset and model evaluator.
 
         This model will be updated with a new set of quantization parameters. Future calls to
         forward() and export_...() will take these quantization parameters into account.
 
         Parameters:
@@ -97,26 +102,23 @@
                           ground_truth: Collection[torch.Tensor] | torch.Tensor]
                         )
 
             num_samples: int | None
                 Number of samples to use for evaluation. One sample is one iteration from iter(data).
                 If none, defaults to the number of samples in the dataset.
 
-            evaluator: BaseModelEvaluator | None
-                Evaluator to populate while quantizing the data.
-                If not provided, an evaluator is not used.
-
             device: str
                 Name of device on which inference should be run.
 
             requantize_model_weights: bool
                 If a weight is quantized, recompute its quantization parameters.
 
-        Returns:
-            If an evaluator is provided, returns its accuracy score. No return value otherwise.
+            data_has_gt: bool
+                Set to true if the data loader passed in also provides ground truth data.
+                The ground truth data will be discarded for quantization.
         """
         ...
 
     @abstractmethod
     def get_calibration_data(
         self,
         target_runtime: TargetRuntime,
@@ -171,14 +173,59 @@
         pretrained model. While this function may take arguments, all arguments
         should have default values specified, so that all classes can be invoked
         with `cls.from_pretrained()` and always have it return something reasonable.
         """
         ...
 
 
+class PretrainedHubModelProtocol(HubModelProtocol, FromPretrainedProtocol, Protocol):
+    """
+    All pretrained AI Hub Models must, at minimum, implement this interface.
+    """
+
+    @abstractmethod
+    def convert_to_torchscript(
+        self, input_spec: InputSpec | None = None, check_trace: bool = True
+    ) -> Any:
+        """
+        Converts the torch module to a torchscript trace, which
+        is the format expected by qai hub.
+
+        This is a default implementation that may be overriden by a subclass.
+        """
+        ...
+
+    def convert_to_hub_source_model(
+        self,
+        target_runtime: TargetRuntime,
+        output_path: str | Path,
+        input_spec: InputSpec | None = None,
+        check_trace: bool = True,
+    ) -> SourceModel:
+        ...
+
+    def get_hub_compile_options(
+        self,
+        target_runtime: TargetRuntime,
+        other_compile_options: str = "",
+    ) -> str:
+        """
+        AI Hub compile options recommended for the model.
+        """
+        ...
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        """
+        Source model format preferred for conversion on AI Hub.
+        """
+        ...
+
+
 class FromPrecompiledProtocol(Protocol):
     """
     Models follow this protocol if they can be initiated from a precompiled torch model.
     """
 
     @classmethod
     @abstractmethod
```

## qai_hub_models/models/_shared/ffnet/model.py

```diff
@@ -2,14 +2,15 @@
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
 from importlib import reload
+from typing import Type, TypeVar
 
 import torch
 
 from qai_hub_models.models._shared.cityscapes_segmentation.model import (
     FFNET_SOURCE_PATCHES,
     FFNET_SOURCE_REPO_COMMIT,
     FFNET_SOURCE_REPOSITORY,
@@ -54,20 +55,22 @@
     "segmentation_ffnet122NS_CCC_mobile_pre_down": (
         "ffnet122NS",
         "ffnet122NS_CCC_cityscapes_state_dict_quarts_pre_down.pth",
         "ffnet122NS_CCC_cityscapes_state_dict_quarts.pth",
     ),
 }
 
+FFNetType = TypeVar("FFNetType", bound="FFNet")
+
 
 class FFNet(CityscapesSegmentor):
     """Exportable FFNet fuss-free Cityscapes segmentation model."""
 
     @classmethod
-    def from_pretrained(cls, variant_name: str) -> FFNet:
+    def from_pretrained(cls: Type[FFNetType], variant_name: str) -> FFNetType:
         model = _load_ffnet_source_model(variant_name)
         model.eval()
 
         return cls(model)
 
 
 def _load_ffnet_source_model(variant_name) -> torch.nn.Module:
```

## qai_hub_models/models/_shared/ffnet/test_utils.py

```diff
@@ -1,26 +1,29 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
+from typing import Type
+
 import numpy as np
 import torch
 
 from qai_hub_models.models._shared.cityscapes_segmentation.demo import (
     TEST_CITYSCAPES_LIKE_IMAGE_ASSET,
 )
-from qai_hub_models.models._shared.ffnet.model import FFNet, _load_ffnet_source_model
+from qai_hub_models.models._shared.ffnet.model import _load_ffnet_source_model
 from qai_hub_models.utils.asset_loaders import load_image
+from qai_hub_models.utils.base_model import BaseModel
 from qai_hub_models.utils.image_processing import preprocess_PIL_image
 
 
 def run_test_off_target_numerical(
-    ffnet_cls: FFNet, variant_name: str, relax_numerics: bool = False
+    ffnet_cls: Type[BaseModel], variant_name: str, relax_numerics: bool = False
 ):
     """Verify that raw (numeric) outputs of both (qaism and non-qaism) networks are the same."""
     processed_sample_image = preprocess_PIL_image(
         load_image(TEST_CITYSCAPES_LIKE_IMAGE_ASSET)
     )
     source_model = _load_ffnet_source_model(variant_name)
     qaism_model = ffnet_cls.from_pretrained()
```

## qai_hub_models/models/_shared/ffnet_quantized/model.py

```diff
@@ -9,30 +9,30 @@
 from qai_hub_models.utils.quantization_aimet import (
     AIMETQuantizableMixin,
 )
 
 # isort: on
 
 import os
+from typing import Type, TypeVar
 
 import torch
 from aimet_torch.batch_norm_fold import fold_all_batch_norms
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
-from qai_hub.client import DatasetEntries
 
 from qai_hub_models.models._shared.ffnet.model import FFNet
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
-from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
 FFNET_AIMET_CONFIG = os.path.abspath(
     os.path.join(os.path.dirname(__file__), "aimet_config.json")
 )
 
+FFNetQuantizableType = TypeVar("FFNetQuantizableType", bound="FFNetQuantizable")
+
 
 class FFNetQuantizable(AIMETQuantizableMixin, FFNet):
     """
     FFNet with post train quantization support.
 
     Supports only 8-bit weights and activations.
     """
@@ -40,32 +40,24 @@
     def __init__(
         self,
         ffnet_model: FFNet,
     ) -> None:
         FFNet.__init__(self, ffnet_model.model)
         AIMETQuantizableMixin.__init__(self, ffnet_model)
 
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
-
     @classmethod
     def default_aimet_encodings(cls) -> str:
         raise NotImplementedError()
 
     @classmethod
     def from_pretrained(
-        cls,
+        cls: Type[FFNetQuantizableType],
         variant_name: str,
         aimet_encodings: str | None = "DEFAULT",
-    ) -> "FFNetQuantizable":
+    ) -> FFNetQuantizableType:
         ffnet = FFNet.from_pretrained(variant_name).model
 
         input_shape = FFNetQuantizable.get_input_spec()["image"][0]
 
         fold_all_batch_norms(ffnet, [input_shape])
 
         ffnet = prepare_model(ffnet)
@@ -81,18 +73,7 @@
         if aimet_encodings:
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = cls.default_aimet_encodings()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
-    def get_calibration_data(
-        self, target_runtime: TargetRuntime, input_spec: InputSpec | None = None
-    ) -> DatasetEntries | None:
-        # Do not provide calibration data
-        return None
```

## qai_hub_models/models/_shared/whisper/app.py

```diff
@@ -30,23 +30,25 @@
     OpenAI Whisper.
     """
 
     def __init__(self, whisper: Whisper):
         decoder = whisper.decoder.to("cpu")
         encoder = whisper.encoder.to("cpu")
         self.num_decoder_blocks = whisper.num_decoder_blocks
+        self.num_decoder_heads = whisper.num_decoder_heads
         self.attention_dim = whisper.attention_dim
+        self.max_decode_len = whisper.max_decode_len
 
         # Wraps torch Module so it takes np ndarray as input and outputs
         if isinstance(encoder, torch.nn.Module):
             self.encoder = TorchNumpyAdapter(encoder)
         else:
             self.encoder = encoder
         if isinstance(decoder, torch.nn.Module):
-            self.decoder = TorchNumpyAdapter(decoder)
+            self.decoder = TorchNumpyAdapter(decoder.eval())
         else:
             self.decoder = decoder
 
     def predict(self, *args, **kwargs):
         # See transcribe.
         return self.transcribe(*args, **kwargs)
 
@@ -63,26 +65,35 @@
         - transcribed texts
         """
         cross_attn_cache = self.encoder(mel_input)
         # Start decoding
         # coreml only takes float tensors
         x = np.array([[TOKEN_SOT]])
         decoded_tokens = [TOKEN_SOT]
-        cache_tensor = np.array([], dtype=np.float32).reshape(
-            (1, 0, self.attention_dim)
-        )
+        sample_len = self.max_decode_len  # max # of tokens to sample
+        cache_tensor = np.zeros((1, sample_len, self.attention_dim)).astype(np.float32)
         self_attn_cache = [cache_tensor] * 2 * self.num_decoder_blocks
 
-        sample_len = 224  # max # of tokens to sample
         sum_logprobs = 0
         for i in range(sample_len):
-            decoder_out = self.decoder(x, *cross_attn_cache, *self_attn_cache)
+            # Using i to index inside the decoder model hurts the
+            # the model performance.
+            # index - used to get positional embedding correctly.
+            index = torch.zeros([1, 1], dtype=torch.int32)
+            index[0, 0] = i
+            # Use mask to get the k_cache updated with new key
+            mask = torch.zeros(1, sample_len, self.attention_dim, dtype=torch.bool)
+            mask[:, i, :] = 1
+            decoder_out = self.decoder(
+                x, index, mask, *cross_attn_cache, *self_attn_cache
+            )
             # logit has shape (1, decoded_len, 51864)
             logits = decoder_out[0]
             self_attn_cache = decoder_out[1:]  # type: ignore
+
             # logit has shape (51864,)
             logits = logits[0, -1]  # consider only the last token
 
             # Filters
             # SuppressBlank
             if i == 0:
                 logits[[TOKEN_EOT, TOKEN_BLANK]] = -np.inf
```

## qai_hub_models/models/_shared/whisper/model.py

```diff
@@ -25,32 +25,36 @@
 class Whisper(CollectionModel):
     def __init__(
         self,
         encoder: Callable[[torch.Tensor], List[torch.Tensor]],
         decoder: Callable[..., Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]],
         num_decoder_blocks: int,
         attention_dim: int,
+        num_heads: int,
     ):
         self.encoder = encoder
         self.decoder = decoder
         self.num_decoder_blocks = num_decoder_blocks
         self.attention_dim = attention_dim
+        self.num_decoder_heads = num_heads
+        self.max_decode_len = MAX_DECODE_LEN
 
     @classmethod
     def from_pretrained(cls, model: str = "tiny.en"):
         # For other model sizes, see https://github.com/openai/whisper/blob/main/whisper/__init__.py#L17
         return cls.from_source_model(whisper.load_model(model))
 
     @classmethod
     def from_source_model(cls, whisper_model: Any):
         encoder = WhisperEncoderInf(whisper_model)
         decoder = WhisperDecoderInf(whisper_model.decoder)
         num_decoder_blocks = len(decoder.blocks)
         attention_dim = decoder.attention_dim
-        return cls(encoder, decoder, num_decoder_blocks, attention_dim)  # type: ignore
+        num_heads = decoder.num_heads
+        return cls(encoder, decoder, num_decoder_blocks, attention_dim, num_heads)  # type: ignore
 
 
 class WhisperEncoderInf(BaseModel):
     """
     WhisperEncoder optimized for export and inference.
 
     It takes audio input (mel) and directly produce cross attention
@@ -78,22 +82,14 @@
         """
         return dict(audio=((1, 80, 3000), "float32"))
 
     @classmethod
     def from_pretrained(cls):
         return Whisper.from_pretrained().encoder
 
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --compute_unit gpu"
-
     def get_hub_profile_options(
         self, target_runtime: TargetRuntime, other_profile_options: str = ""
     ) -> str:
         profile_options = super().get_hub_profile_options(
             target_runtime, other_profile_options
         )
         return profile_options + " --max_profiler_iterations 10" + " --compute_unit gpu"
@@ -124,67 +120,83 @@
         for p in ["positional_embedding"]:
             self.register_parameter(p, getattr(model, p))
 
     @property
     def attention_dim(self):
         return self.blocks[0].attn_ln.weight.shape[0]
 
-    def forward(self, x: torch.Tensor, *kv_cache_args, **kv_cache_kwargs):
+    @property
+    def num_heads(self):
+        return self.blocks[0].attn.n_head
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        index: torch.Tensor,
+        mask: torch.Tensor,
+        *kv_cache_args,
+        **kv_cache_kwargs,
+    ):
         """
         Args:
 
         - x: torch.LongTensor, shape = (batch_size, <= n_ctx)
             the text tokens
 
+        - index: torch.tensor, shape = (1, 1)
+            index to get the positional encoding for x.
+
+        - mask: torch.tensor, shape = (1, max_sample_length, attn_dim)
+            Mask helps create kv_cache while keeping the size consistent.
+
         - kv_cache_args: Tuple of length 4 * num_decoder_blocks. Elements are:
 
             b{i}_cross_attn_k: [1, 1500, attn_dim]
             b{i}_cross_attn_v: [1, 1500, attn_dim]
 
             for i = 0, ..., num_blocks
 
             followed by
 
-            b{i}_self_attn_k: [1, decoded_len, attn_dim]
-            b{i}_self_attn_v: [1, decoded_len, attn_dim]
+            b{i}_self_attn_k: [1, max_sample_length, attn_dim]
+            b{i}_self_attn_v: [1, max_sample_length, attn_dim]
 
             for i = 0, ..., num_blocks
 
         Returns:
 
         - logits: of shape [1, 1, 51864]
         - b0_self_attn_k, b0_self_attn_v, b1_self_attn_k, ...: Updated self attn cache.
           2*num_decoder_blocks
         """
+
         if not kv_cache_args:
             kv_cache_args = list(kv_cache_kwargs.values())
+
         assert isinstance(self.token_embedding, torch.nn.Module)  # for mypy
         assert isinstance(self.ln, torch.nn.Module)  # for mypy
         assert isinstance(self.positional_embedding, torch.nn.Parameter)  # for mypy
         # Set up kv_cache
         kv_cache = {}  # torch.nn.Module -> torch.Tensor
         for i, block in enumerate(self.blocks):
             kv_cache.update(
                 {
                     block.attn.key: kv_cache_args[2 * self.num_blocks + i * 2],
                     block.attn.value: kv_cache_args[2 * self.num_blocks + i * 2 + 1],
                     block.cross_attn.key: kv_cache_args[i * 2],
                     block.cross_attn.value: kv_cache_args[i * 2 + 1],
                 }
             )
-        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0
-        x = (
-            self.token_embedding(x)
-            + self.positional_embedding[offset : offset + x.shape[-1]]
-        )
+
+        x = self.token_embedding(x) + self.positional_embedding[index.long()]
 
         # x shape: (1, 1, 384)
         kv_cache_new = []
         for block in self.blocks:
-            x, k_cache, v_cache = block(x, kv_cache=kv_cache)
+            x, k_cache, v_cache = block(x, index, mask, kv_cache=kv_cache)
             kv_cache_new.append(k_cache.float())
             kv_cache_new.append(v_cache.float())
 
         x = self.ln(x)
         logits = (
             x
             @ torch.transpose(
@@ -192,41 +204,46 @@
             )
         ).float()
 
         # shape: [1, 1, 51864]
         return (logits,) + tuple(kv_cache_new)
 
     @staticmethod
-    def get_input_spec(num_blocks: int, attention_dim: int) -> InputSpec:
+    def get_input_spec(
+        num_blocks: int, attention_dim: int, num_heads: int
+    ) -> InputSpec:
         """
         Returns the input specification (name -> (shape, type). This can be
         used to submit profiling job on Qualcomm AI Hub.
         """
-        specs = dict(x=((1, 1), "int32"))
+        specs = dict(
+            x=((1, 1), "int32"),
+            index=((1, 1), "int32"),
+            mask=((1, MAX_DECODE_LEN, attention_dim), "int32"),
+        )
         for i in range(num_blocks):
             specs[f"b{i}_cross_attn_k"] = ((1, 1500, attention_dim), "float32")
             specs[f"b{i}_cross_attn_v"] = ((1, 1500, attention_dim), "float32")
 
-        # Use mean length for profiling
-        mean_decode_len = MAX_DECODE_LEN // 2
-
         for i in range(num_blocks):
             specs[f"b{i}_self_attn_k"] = (
-                (1, mean_decode_len, attention_dim),
+                (1, MAX_DECODE_LEN, attention_dim),
                 "float32",
             )
             specs[f"b{i}_self_attn_v"] = (
-                (1, mean_decode_len, attention_dim),
+                (1, MAX_DECODE_LEN, attention_dim),
                 "float32",
             )
 
         return specs
 
-    def _get_input_spec_for_model_instance(self) -> InputSpec:
-        return self.__class__.get_input_spec(len(self.blocks), self.attention_dim)
+    def _get_input_spec_for_instance(self) -> InputSpec:
+        return self.__class__.get_input_spec(
+            len(self.blocks), self.attention_dim, self.num_heads
+        )
 
     @classmethod
     def from_pretrained(cls):
         return Whisper.from_pretrained().decoder
 
 
 class MHAWrapper(torch.nn.Module):
@@ -254,14 +271,16 @@
         self.n_head = model.n_head
         for m in ["query", "key", "value", "out"]:
             self.add_module(m, getattr(model, m))
 
     def forward(
         self,
         x: torch.Tensor,
+        index: torch.Tensor,
+        mask: torch.Tensor,
         kv_cache: Dict[torch.nn.Module, torch.Tensor],
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
         """
         Args:
 
         - x: shape [1, 1, attention_dim]. Input feature.
 
@@ -277,26 +296,27 @@
         - updated k, v cache: of shape [1, decoded_len+1, attention_dim]
         """
         assert isinstance(self.query, torch.nn.Module)  # for mypy
         assert isinstance(self.key, torch.nn.Module)  # for mypy
         assert isinstance(self.value, torch.nn.Module)  # for mypy
         assert isinstance(self.out, torch.nn.Module)  # for mypy
         q = self.query(x)
-
         if self.attn_type == "self_attention":
             k_cache = kv_cache[self.key]
             v_cache = kv_cache[self.value]
-            k = self.key(x)
-            v = self.value(x)
-            k = torch.cat([k_cache, k], dim=1)
-            v = torch.cat([v_cache, v], dim=1)
+            k = torch.zeros(k_cache.shape)
+            v = torch.zeros(v_cache.shape)
+            k = mask * self.key(x) + k_cache
+            v = mask * self.value(x) + v_cache
+            new_index = torch.tensor([index[0, 0] + 1]).long()
+            wv = qkv_attention(q, k[:, :new_index], v[:, :new_index], self.n_head)
         else:  # cross_attention
             k, v = kv_cache[self.key], kv_cache[self.value]
+            wv = qkv_attention(q, k, v, self.n_head)
 
-        wv = qkv_attention(q, k, v, self.n_head)
         # Return updated kv cache
         return self.out(wv), k.detach(), v.detach()
 
 
 def qkv_attention(
     q: torch.Tensor,
     k: torch.Tensor,
@@ -304,22 +324,24 @@
     n_head: int,
     mask: Optional[torch.Tensor] = None,
 ) -> torch.Tensor:
     """
     Adapted from whisper.model.MultiHeadAttention.qkv_attention
     """
     n_batch, n_ctx, n_state = q.shape
+
     scale = (n_state // n_head) ** -0.25
     q = q.view(*q.shape[:2], n_head, -1).permute(0, 2, 1, 3) * scale
     k = k.view(*k.shape[:2], n_head, -1).permute(0, 2, 3, 1) * scale
     v = v.view(*v.shape[:2], n_head, -1).permute(0, 2, 1, 3)
 
     qk = q @ k
     if mask is not None:
-        qk = qk + mask[:n_ctx, :n_ctx]
+        qk = qk + mask
+    # Use negative infinity to mask the zeros when doing the softmax.
     qk = qk.float()
 
     w = torch.nn.functional.softmax(qk, dim=-1).to(q.dtype)
     return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)
 
 
 class ResidualAttentionBlockWrapper(torch.nn.Module):
@@ -338,31 +360,35 @@
         self.cross_attn = MHAWrapper(model.cross_attn, "cross_attention")
         for m in ["attn_ln", "cross_attn_ln", "mlp", "mlp_ln"]:
             self.add_module(m, getattr(model, m))
 
     def forward(
         self,
         x: torch.Tensor,
+        index: torch.Tensor,
+        mask: torch.Tensor,
         kv_cache: Dict[torch.nn.Module, torch.Tensor],
     ):
         """
         Args: Same as MHAWrapper
         Returns: Same as MHAWrapper
         """
         # Get updated self attention kv cache
         assert isinstance(self.attn, torch.nn.Module)  # for mypy
         assert isinstance(self.attn_ln, torch.nn.Module)  # for mypy
         assert isinstance(self.cross_attn_ln, torch.nn.Module)  # for mypy
         assert isinstance(self.cross_attn, torch.nn.Module)  # for mypy
         assert isinstance(self.mlp, torch.nn.Module)  # for mypy
         assert isinstance(self.mlp_ln, torch.nn.Module)  # for mypy
-        x_attn, k_cache, v_cache = self.attn(self.attn_ln(x), kv_cache=kv_cache)
+        x_attn, k_cache, v_cache = self.attn(
+            self.attn_ln(x), index=index, mask=mask, kv_cache=kv_cache
+        )
         x = x + x_attn
         if self.cross_attn:
             # Ignore cross attn kv cache which is constant (pre-computed in
             # `WhisperCrossAttnKVCacheTorch`)
             x_cross_attn, _, _ = self.cross_attn(
-                self.cross_attn_ln(x), kv_cache=kv_cache
+                self.cross_attn_ln(x), index=index, mask=mask, kv_cache=kv_cache
             )
             x = x + x_cross_attn
         x = x + self.mlp(self.mlp_ln(x))
         return x, k_cache, v_cache
```

## qai_hub_models/models/_shared/whisper/test_utils.py

```diff
@@ -9,14 +9,15 @@
 from qai_hub_models.models._shared.whisper.app import (
     WhisperApp,
     load_audio,
     load_mel_filter,
 )
 from qai_hub_models.models._shared.whisper.demo import TEST_AUDIO_PATH
 from qai_hub_models.models._shared.whisper.model import (
+    MAX_DECODE_LEN,
     MEL_FILTER_PATH,
     Whisper,
     WhisperDecoderInf,
     WhisperEncoderInf,
 )
 
 
@@ -45,19 +46,24 @@
         logits_orig = model.decoder(tokens, audio_features).detach().numpy()
 
     # QAIHM
     encoder = WhisperEncoderInf(model)
     decoder = WhisperDecoderInf(model.decoder)
 
     cross_attn_cache = encoder(mel_input)
-    cache_tensor = np.array([], dtype=np.float32).reshape((1, 0, decoder.attention_dim))
-    self_attn_cache = [torch.from_numpy(cache_tensor)] * 2 * decoder.num_blocks
-
-    decoder_out = decoder(tokens, *cross_attn_cache, *self_attn_cache)
-    logits = decoder_out[0].detach().numpy()
+    sample_len = MAX_DECODE_LEN
+    cache_tensor = np.zeros([1, sample_len, decoder.attention_dim]).astype(np.float32)
+    index = torch.zeros([1, 1], dtype=torch.int32)
+    index[0, 0] = 0
+    mask = torch.zeros(1, sample_len, decoder.attention_dim, dtype=torch.bool)
+    mask[:, 0, :] = 1
+    self_attn_cache = [cache_tensor] * 2 * decoder.num_blocks
+    with torch.no_grad():
+        decoder_out = decoder(tokens, index, mask, *cross_attn_cache, *self_attn_cache)
+        logits = decoder_out[0].detach().numpy()
 
     np.testing.assert_allclose(logits_orig, logits)
 
 
 def run_test_transcribe(whisper_version):
     """
     Test that WhisperApp produces end to end transcription results that
```

## qai_hub_models/models/_shared/yolo/app.py

```diff
@@ -6,14 +6,15 @@
 
 from typing import Callable, List, Tuple
 
 import numpy as np
 import torch
 from PIL.Image import Image
 
+from qai_hub_models.models._shared.yolo.utils import detect_postprocess
 from qai_hub_models.utils.bounding_box_processing import batched_nms
 from qai_hub_models.utils.draw import draw_box_from_xyxy
 from qai_hub_models.utils.image_processing import app_to_net_image_inputs
 
 
 class YoloObjectDetectionApp:
     """
@@ -34,14 +35,15 @@
     def __init__(
         self,
         model: Callable[
             [torch.Tensor], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         ],
         nms_score_threshold: float = 0.45,
         nms_iou_threshold: float = 0.7,
+        model_includes_postprocessing: bool = True,
     ):
         """
         Initialize a YoloObjectDetectionApp application.
 
         Parameters:
             model: torch.Tensor
                 Yolo object detection model.
@@ -59,18 +61,22 @@
                                 of the most probable class of the prediction.
 
             nms_score_threshold
                 Score threshold for non maximum suppression.
 
             nms_iou_threshold
                 Intersection over Union threshold for non maximum suppression.
+
+            model_includes_postprocessing
+                Whether the model includes postprocessing steps beyond the detector.
         """
         self.model = model
         self.nms_score_threshold = nms_score_threshold
         self.nms_iou_threshold = nms_iou_threshold
+        self.model_includes_postprocessing = model_includes_postprocessing
 
     def check_image_size(self, pixel_values: torch.Tensor) -> None:
         """
         Verify image size is valid model input.
         """
         raise NotImplementedError
 
@@ -116,15 +122,20 @@
         # Input Prep
         NHWC_int_numpy_frames, NCHW_fp32_torch_frames = app_to_net_image_inputs(
             pixel_values_or_image
         )
         self.check_image_size(NCHW_fp32_torch_frames)
 
         # Run prediction
-        pred_boxes, pred_scores, pred_class_idx = self.model(NCHW_fp32_torch_frames)
+        if self.model_includes_postprocessing:
+            pred_boxes, pred_scores, pred_class_idx = self.model(NCHW_fp32_torch_frames)
+        else:
+            pred_boxes, pred_scores, pred_class_idx = self.pre_nms_postprocess(
+                self.model(NCHW_fp32_torch_frames)
+            )
 
         # Non Maximum Suppression on each batch
         pred_boxes, pred_scores, pred_class_idx = batched_nms(
             self.nms_iou_threshold,
             self.nms_score_threshold,
             pred_boxes,
             pred_scores,
@@ -144,7 +155,27 @@
                     box[0:2].int(),
                     box[2:4].int(),
                     color=(0, 255, 0),
                     size=2,
                 )
 
         return NHWC_int_numpy_frames
+
+    def pre_nms_postprocess(
+        self, prediction: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        """
+        Process the output of the YOLO detector for input to NMS.
+
+        Parameters:
+            detector_output: torch.Tensor
+                The output of Yolo detection model. Tensor shape varies by model implementation.
+
+        Returns:
+            boxes: torch.Tensor
+                Bounding box locations. Shape is [batch, num preds, 4] where 4 == (x1, y1, x2, y2)
+            scores: torch.Tensor
+                class scores multiplied by confidence: Shape is [batch, num_preds]
+            class_idx: torch.tensor
+                Shape is [batch, num_preds] where the last dim is the index of the most probable class of the prediction.
+        """
+        return detect_postprocess(prediction)
```

## qai_hub_models/models/_shared/yolo/demo.py

```diff
@@ -12,22 +12,22 @@
 from qai_hub_models.utils.args import (
     demo_model_from_cli_args,
     get_model_cli_parser,
     get_on_device_demo_parser,
     validate_on_device_demo_args,
 )
 from qai_hub_models.utils.asset_loaders import CachedWebAsset, load_image
-from qai_hub_models.utils.base_model import BaseModel
+from qai_hub_models.utils.base_model import HubModel
 from qai_hub_models.utils.display import display_or_save_image
 
 
 # Run Yolo end-to-end on a sample image.
 # The demo will display a image with the predicted bounding boxes.
 def yolo_detection_demo(
-    model_type: Type[BaseModel],
+    model_type: Type[HubModel],
     model_id: str,
     app_type: Callable[..., YoloObjectDetectionApp],
     default_image: str | CachedWebAsset,
     stride_multiple: int | None = None,
     is_test: bool = False,
 ):
     # Demo parameters
@@ -45,19 +45,27 @@
     )
     parser.add_argument(
         "--iou-threshold",
         type=float,
         default=0.7,
         help="Intersection over Union (IoU) threshold for NonMaximumSuppression",
     )
-    args = parser.parse_args([] if is_test else None)
+    pargs = parser.parse_args([] if is_test else None)
+    args = pargs
+
     validate_on_device_demo_args(args, model_id)
 
     model = demo_model_from_cli_args(model_type, model_id, args)
 
-    app = app_type(model, args.score_threshold, args.iou_threshold)
+    app = app_type(
+        model,
+        args.score_threshold,
+        args.iou_threshold,
+        args.include_postprocessing if not is_test else True,
+    )
+
     print("Model Loaded")
     image = load_image(args.image)
     pred_images = app.predict_boxes_from_image(image)
     out = Image.fromarray(pred_images[0])
     if not is_test:
         display_or_save_image(out, args.output_dir, "yolo_demo_output.png")
```

## qai_hub_models/models/_shared/yolo/utils.py

```diff
@@ -45,16 +45,16 @@
         such as bounding boxes, classes, and confidence.
 
     Parameters:
         detector_output: torch.Tensor
             The output of Yolo Detection model
             Shape is [batch, num_preds, k]
                 where, k = # of classes + 5
-                k is structured as follows [boxes (4) : conf (1) : # of classes]
-                and boxes are co-ordinates [x_center, y_center, w, h]
+                k is structured as follows [box_coordinates (4) , conf (1) , # of classes]
+                and box_coordinates are [x_center, y_center, w, h]
 
     Returns:
         boxes: torch.Tensor
             Bounding box locations. Shape is [batch, num preds, 4] where 4 == (x1, y1, x2, y2)
         scores: torch.Tensor
             class scores multiplied by confidence: Shape is [batch, num_preds]
         class_idx: torch.tensor
```

## qai_hub_models/models/aotgan/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/aotgan/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: AOT-GAN
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 172836.0
-      throughput: 5.785831655442153
+      inference_time: 172572.0
+      throughput: 5.79468279906358
       estimated_peak_memory_range:
-        min: 3305472
-        max: 6628872
+        min: 2220032
+        max: 5310760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: jqpyel4gy
+      job_id: jw562w2vg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:15:11.791489Z'
+    timestamp: '2024-04-02T16:05:49.549048Z'
     torchscript_onnx_qnn:
-      inference_time: 162909.0
-      throughput: 6.138396282587212
+      inference_time: 162522.0
+      throughput: 6.15301313053002
       estimated_peak_memory_range:
-        min: 4268032
-        max: 33754568
+        min: 3313664
+        max: 38238512
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 275
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 275
-      job_id: j1p8on8g9
+      job_id: j1pvq7q7g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 127366.0
-      throughput: 7.851388910698303
+      inference_time: 126409.0
+      throughput: 7.910829134001535
       estimated_peak_memory_range:
-        min: 2334720
-        max: 227053936
+        min: 2404352
+        max: 254900160
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: j2p0ywegw
+      job_id: jwgoz8z4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:17:12.829523Z'
+    timestamp: '2024-04-02T16:07:27.833498Z'
     torchscript_onnx_qnn:
-      inference_time: 120027.0
-      throughput: 8.331458755113433
+      inference_time: 119294.0
+      throughput: 8.382651264942076
       estimated_peak_memory_range:
-        min: 0
-        max: 140852624
+        min: 3862528
+        max: 165145744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 275
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 275
-      job_id: jogkz1ogd
+      job_id: j7gjdqd7g
       job_status: Passed
```

## qai_hub_models/models/controlnet_quantized/info.yaml

```diff
@@ -13,14 +13,15 @@
 research_paper: https://arxiv.org/abs/2302.05543
 research_paper_title: Adding Conditional Control to Text-to-Image Diffusion Models
 license: https://github.com/lllyasviel/ControlNet/blob/main/LICENSE
 deploy_license: https://github.com/lllyasviel/ControlNet/blob/main/LICENSE
 source_repo: https://github.com/lllyasviel/ControlNet
 technical_details:
   Input: Text prompt and input image as a reference
+  Conditioning Input: Canny-Edge
   QNN-SDK: '2.19'
   Text Encoder Number of parameters: 340M
   UNet Number of parameters: 865M
   VAE Decoder Number of parameters: 83M
   ControlNet Number of parameters: 361M
   Model size: 1.4GB
 applicable_scenarios:
```

## qai_hub_models/models/controlnet_quantized/perf.yaml

```diff
@@ -1,127 +1,325 @@
+aggregated:
+  supported_oses:
+  - Android
+  supported_devices:
+  - Samsung Galaxy S23
+  - Samsung Galaxy S23 Ultra
+  - Samsung Galaxy S23+
+  - Samsung Galaxy S24
+  - Samsung Galaxy S24 Ultra
+  supported_chipsets:
+  - Snapdragon 8 Gen 2
+  - Snapdragon 8 Gen 3
 models:
 - name: TextEncoder_Quantized
   performance_metrics:
-  - reference_device_info:
-      name: Samsung Galaxy S23 Ultra
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T01:42:16.523507Z'
     torchscript_onnx_qnn:
-      inference_time: 11369
-      throughput: 87.95
+      inference_time: 11394.0
+      throughput: 87.76549060909251
       estimated_peak_memory_range:
-        min: 57344
-        max: 34869152
+        min: 53248
+        max: 77484736
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
         layers_on_npu: 570
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 570
-      precision: uint16
+      job_id: j2p0zr695
+      job_status: Passed
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T01:43:37.533078Z'
+    torchscript_onnx_qnn:
+      inference_time: 8080.0
+      throughput: 123.76237623762377
+      estimated_peak_memory_range:
+        min: 12288
+        max: 143897312
       primary_compute_unit: NPU
-      job_id: jz5w40nzg
+      precision: uint16
+      layer_info:
+        layers_on_npu: 570
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 570
+      job_id: jnp148j8p
       job_status: Passed
-- name: VAEDecoder_Quantized
+- name: UNet_Quantized
   performance_metrics:
-  - reference_device_info:
-      name: Samsung Galaxy S23 Ultra
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T01:45:12.974733Z'
     torchscript_onnx_qnn:
-      inference_time: 386746
-      throughput: 2.58
+      inference_time: 262520.0
+      throughput: 3.8092335822032606
       estimated_peak_memory_range:
-        min: 122880
-        max: 4489392
+        min: 11612160
+        max: 17312944
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
-        layers_on_npu: 409
+        layers_on_npu: 5434
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 409
-      precision: uint16
+        total_layers: 5434
+      job_id: jogkey8wg
+      job_status: Passed
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T01:46:51.297059Z'
+    torchscript_onnx_qnn:
+      inference_time: 192789.0
+      throughput: 5.187017931520989
+      estimated_peak_memory_range:
+        min: 2793472
+        max: 1307148416
       primary_compute_unit: NPU
-      job_id: jnp16kxk5
+      precision: uint16
+      layer_info:
+        layers_on_npu: 5434
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 5434
+      job_id: jz5we97m5
       job_status: Passed
-- name: UNet_Quantized
+- name: VAEDecoder_Quantized
   performance_metrics:
-  - reference_device_info:
-      name: Samsung Galaxy S23 Ultra
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T01:48:15.428311Z'
     torchscript_onnx_qnn:
-      inference_time: 259981
-      throughput: 3.84
+      inference_time: 390243.0
+      throughput: 2.5625059257949534
       estimated_peak_memory_range:
-        min: 13058048
-        max: 15044232
+        min: 233472
+        max: 38117408
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
-        layers_on_npu: 5434
+        layers_on_npu: 409
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 5434
-      precision: uint16
+        total_layers: 409
+      job_id: j1p8q71kp
+      job_status: Passed
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T01:49:38.841493Z'
+    torchscript_onnx_qnn:
+      inference_time: 294404.0
+      throughput: 3.3966929797149494
+      estimated_peak_memory_range:
+        min: 212992
+        max: 92622064
       primary_compute_unit: NPU
-      job_id: jmg9d7eq5
+      precision: uint16
+      layer_info:
+        layers_on_npu: 409
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 409
+      job_id: jvgdxv3rp
       job_status: Passed
 - name: ControlNet_Quantized
   performance_metrics:
-  - reference_device_info:
-      name: Samsung Galaxy S23 Ultra
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T01:52:25.960823Z'
     torchscript_onnx_qnn:
-      inference_time: 103748
-      throughput: 9.63
+      inference_time: 100330.0
+      throughput: 9.96710854181202
       estimated_peak_memory_range:
-        min: 200704
-        max: 23278088
+        min: 1900544
+        max: 71013288
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
         layers_on_npu: 2406
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 2406
-      precision: uint16
-      primary_compute_unit: NPU
-      job_id: jw56w9jng
+      job_id: jn5q62vnp
       job_status: Passed
-aggregated:
-  supported_devices:
-  - Samsung Galaxy S23 Ultra
-  supported_oses:
-  - Android
-  supported_chipsets:
-  - Snapdragon 8 Gen 2
-  performance_metrics:
-  - reference_device_info:
-    name: Samsung Galaxy S23 Ultra
-    os: '13'
-    form_factor: Phone
-    os_name: Android
-    manufacturer: Samsung
-    chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
-    torchscript_onnx_qnn:
-      inference_time: 761844
-      throughput: 1.31
-      estimated_peak_memory_range:
-        min: 13058048
-        max: 34869152
-      precision: int16
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T01:53:56.368554Z'
+    torchscript_onnx_qnn:
+      inference_time: 76940.0
+      throughput: 12.997140629061606
+      estimated_peak_memory_range:
+        min: 0
+        max: 558479712
       primary_compute_unit: NPU
-      job_id: ""
+      precision: uint16
+      layer_info:
+        layers_on_npu: 2406
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 2406
+      job_id: jmg9l4m8g
       job_status: Passed
```

## qai_hub_models/models/convnext_tiny/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/convnext_tiny/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ConvNext-Tiny
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 11538.0
-      throughput: 86.67013347200555
+      inference_time: 11504.0
+      throughput: 86.92628650904034
       estimated_peak_memory_range:
-        min: 53248
-        max: 2750320
+        min: 32768
+        max: 2493040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 380
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 380
-      job_id: jnp10l25q
+      job_id: jlpeoyo7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:26:23.235644Z'
+    timestamp: '2024-04-02T15:30:19.195043Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 8123.0
-      throughput: 123.10722639418934
+      inference_time: 8139.0
+      throughput: 122.86521685710775
       estimated_peak_memory_range:
-        min: 40960
-        max: 205818960
+        min: 20480
+        max: 209217264
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 380
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 380
-      job_id: jvgdw9e5j
+      job_id: jygz2n2zg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:26:23.235670Z'
+    timestamp: '2024-04-02T15:30:19.195057Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/ddrnet23_slim/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ddrnet23_slim/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DDRNet23-Slim
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6741.0
-      throughput: 148.3459427384661
+      inference_time: 6702.0
+      throughput: 149.20919128618323
       estimated_peak_memory_range:
-        min: 1024000
-        max: 28696320
+        min: 1003520
+        max: 2797288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: jz5wo7zp1
+      job_id: jz5ww4wz5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:17:36.932886Z'
+    timestamp: '2024-04-02T15:51:44.265591Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 4644.0
-      throughput: 215.33161068044788
+      inference_time: 4785.0
+      throughput: 208.9864158829676
       estimated_peak_memory_range:
-        min: 45056
-        max: 68954288
+        min: 36864
+        max: 71748864
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: jmg9vmq57
+      job_id: jmg90d0qg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:17:36.932896Z'
+    timestamp: '2024-04-02T15:51:44.265604Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/deeplabv3_resnet50/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/deeplabv3_resnet50/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DeepLabV3-ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 57559.0
-      throughput: 17.373477649021005
+      inference_time: 58066.0
+      throughput: 17.221782110012743
       estimated_peak_memory_range:
-        min: 106496
-        max: 3561872
+        min: 12288
+        max: 171781856
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 96
         layers_on_cpu: 0
         total_layers: 96
-      job_id: jw5663y5o
+      job_id: jqpyzmrrg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:49:36.627925Z'
+    timestamp: '2024-04-02T15:16:34.908612Z'
     torchscript_onnx_qnn:
-      inference_time: 145372.0
-      throughput: 6.878903777893955
+      inference_time: 145873.0
+      throughput: 6.855278221466619
       estimated_peak_memory_range:
-        min: 724992
-        max: 17276040
+        min: 811008
+        max: 9257648
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 82
         layers_on_cpu: 0
         total_layers: 82
-      job_id: jwgoy1k58
+      job_id: j1p8210zp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 40153.0
-      throughput: 24.904739371902473
+      inference_time: 40355.0
+      throughput: 24.780076818238136
       estimated_peak_memory_range:
-        min: 4358144
-        max: 29236608
+        min: 0
+        max: 28183312
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 96
         layers_on_cpu: 0
         total_layers: 96
-      job_id: j1p3k4n52
+      job_id: j2p04632g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:51:31.429028Z'
+    timestamp: '2024-04-02T15:19:14.249826Z'
     torchscript_onnx_qnn:
-      inference_time: 104457.0
-      throughput: 9.573317250160352
+      inference_time: 104946.0
+      throughput: 9.52871000323976
       estimated_peak_memory_range:
-        min: 675840
-        max: 24520160
+        min: 700416
+        max: 26619552
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 82
         layers_on_cpu: 0
         total_layers: 82
-      job_id: j1pv31r5x
+      job_id: jogkv87yp
       job_status: Passed
```

## qai_hub_models/models/densenet121/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/densenet121/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DenseNet-121
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1603.0
-      throughput: 623.8303181534623
+      inference_time: 1615.0
+      throughput: 619.1950464396285
       estimated_peak_memory_range:
-        min: 16384
-        max: 20547528
+        min: 20480
+        max: 2339568
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 310
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 310
-      job_id: jqpyen0gy
+      job_id: jn5q0ve7p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:35:22.500705Z'
+    timestamp: '2024-04-02T15:16:44.466434Z'
     torchscript_onnx_qnn:
-      inference_time: 1436.0
-      throughput: 696.3788300835655
+      inference_time: 1442.0
+      throughput: 693.4812760055479
       estimated_peak_memory_range:
-        min: 618496
-        max: 5887960
+        min: 20480
+        max: 9456304
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 371
+        layers_on_npu: 370
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 371
-      job_id: j1p8o6qg9
+        total_layers: 370
+      job_id: jw562wevg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1114.0
-      throughput: 897.6660682226212
+      inference_time: 1112.0
+      throughput: 899.2805755395683
       estimated_peak_memory_range:
         min: 12288
-        max: 93424064
+        max: 95054176
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 310
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 310
-      job_id: j2p0yd0gw
+      job_id: j1gl4l6e5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:39:50.803809Z'
+    timestamp: '2024-04-02T15:19:29.601699Z'
     torchscript_onnx_qnn:
-      inference_time: 985.0
-      throughput: 1015.2284263959391
+      inference_time: 977.0
+      throughput: 1023.5414534288639
       estimated_peak_memory_range:
         min: 618496
-        max: 142978448
+        max: 148303712
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 371
+        layers_on_npu: 370
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 371
-      job_id: jn5q8ze57
+        total_layers: 370
+      job_id: j1p3n6vx5
       job_status: Passed
```

## qai_hub_models/models/detr_resnet101/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet101/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DETR-ResNet101
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 563957.0
-      throughput: 1.7731848350140171
+      inference_time: 53317.0
+      throughput: 18.755743946583642
       estimated_peak_memory_range:
-        min: 102526976
-        max: 112477944
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 77824
+        max: 8355272
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 957
+        layers_on_npu: 954
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 957
-      job_id: jmg9v8m57
+      job_id: j1pvq707g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:10:49.800332Z'
+    timestamp: '2024-04-02T15:52:17.373948Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 489867.0
-      throughput: 2.0413704127855112
+      inference_time: 39536.0
+      throughput: 25.29340348037232
       estimated_peak_memory_range:
-        min: 109977600
-        max: 266823568
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 1413120
+        max: 263608576
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 957
+        layers_on_npu: 954
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 957
-      job_id: jnp103n5q
+      job_id: j7gjdqz7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:10:49.800340Z'
+    timestamp: '2024-04-02T15:52:17.373962Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/detr_resnet101_dc5/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet101_dc5/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DETR-ResNet101-DC5
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 976351.0
-      throughput: 1.0242218218652923
+      inference_time: 439506.0
+      throughput: 2.2752817936501435
       estimated_peak_memory_range:
-        min: 168345600
-        max: 171158408
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 8531968
+        max: 17800792
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 958
+        layers_on_npu: 955
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 958
-      job_id: jep28v6p6
+      job_id: jlpeoye7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:03:43.829001Z'
+    timestamp: '2024-04-02T15:53:12.170672Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 777938.0
-      throughput: 1.2854494831207628
+      inference_time: 331401.0
+      throughput: 3.017492403462874
       estimated_peak_memory_range:
-        min: 175112192
-        max: 339555616
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 106496
+        max: 457171760
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 958
+        layers_on_npu: 955
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 958
-      job_id: jqpye70gy
+      job_id: jygz2nozg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:03:43.829010Z'
+    timestamp: '2024-04-02T15:53:12.170686Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/detr_resnet50/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet50/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DETR-ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 365312.0
-      throughput: 2.737386124737211
+      inference_time: 49534.0
+      throughput: 20.188153591472524
       estimated_peak_memory_range:
-        min: 109416448
-        max: 444976064
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 1585152
+        max: 11362840
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 889
+        layers_on_npu: 886
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 889
-      job_id: j1p3k7x52
+      job_id: jz5ww42z5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:08:31.933833Z'
+    timestamp: '2024-04-02T15:30:45.384076Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 287302.0
-      throughput: 3.480657983585217
+      inference_time: 36491.0
+      throughput: 27.404017428955086
       estimated_peak_memory_range:
-        min: 108204032
-        max: 196940032
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 135168
+        max: 216736992
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 889
+        layers_on_npu: 886
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 889
-      job_id: jwgoyw458
+      job_id: jmg90djqg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:08:31.933846Z'
+    timestamp: '2024-04-02T15:30:45.384090Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/detr_resnet50_dc5/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet50_dc5/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: DETR-ResNet50-DC5
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 692168.0
-      throughput: 1.4447359600559402
+      inference_time: 428409.0
+      throughput: 2.3342180019560748
       estimated_peak_memory_range:
-        min: 117583872
-        max: 529905552
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6443008
+        max: 14635248
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 890
+        layers_on_npu: 887
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 890
-      job_id: jqp4q2lgo
+      job_id: jnp126ykg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:27:50.803823Z'
+    timestamp: '2024-04-02T15:36:20.165124Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 521991.0
-      throughput: 1.9157418422923
+      inference_time: 326693.0
+      throughput: 3.060977737508915
       estimated_peak_memory_range:
-        min: 178831360
-        max: 279734112
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 147456
+        max: 420422096
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 0
-        layers_on_cpu: 890
+        layers_on_npu: 887
+        layers_on_gpu: 2
+        layers_on_cpu: 1
         total_layers: 890
-      job_id: j0pxvz9g7
+      job_id: jvgdn2ek5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:27:50.803834Z'
+    timestamp: '2024-04-02T15:36:20.165138Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/efficientnet_b0/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/efficientnet_b0/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: EfficientNet-B0
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2174.0
-      throughput: 459.9816007359706
+      inference_time: 1218.0
+      throughput: 821.0180623973728
       estimated_peak_memory_range:
-        min: 24576
-        max: 2273464
+        min: 16384
+        max: 2283088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 243
-      job_id: jlpe9l8gr
+      job_id: jz57290qp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:37:36.573638Z'
+    timestamp: '2024-04-02T15:50:13.918223Z'
     torchscript_onnx_qnn:
-      inference_time: 2173.0
-      throughput: 460.1932811780948
+      inference_time: 1223.0
+      throughput: 817.6614881439084
       estimated_peak_memory_range:
-        min: 16384
-        max: 87349280
+        min: 622592
+        max: 7343432
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 242
+        layers_on_npu: 241
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 242
-      job_id: jz5wo14p1
+        total_layers: 241
+      job_id: j0px9xnjp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1524.0
-      throughput: 656.1679790026246
+      inference_time: 907.0
+      throughput: 1102.5358324145534
       estimated_peak_memory_range:
         min: 12288
-        max: 70874656
+        max: 70459040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 243
-      job_id: jygze44g8
+      job_id: jqp4n3kqg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:42:10.776325Z'
+    timestamp: '2024-04-02T15:52:57.259137Z'
     torchscript_onnx_qnn:
-      inference_time: 1508.0
-      throughput: 663.1299734748011
+      inference_time: 886.0
+      throughput: 1128.6681715575621
       estimated_peak_memory_range:
-        min: 618496
-        max: 79231776
+        min: 0
+        max: 70990016
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 242
+        layers_on_npu: 241
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 242
-      job_id: jmg9vxm57
+        total_layers: 241
+      job_id: jo5me8qyp
       job_status: Passed
```

## qai_hub_models/models/esrgan/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/esrgan/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ESRGAN
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 74047.0
-      throughput: 13.504936054127784
+      inference_time: 73806.0
+      throughput: 13.54903395387909
       estimated_peak_memory_range:
-        min: 12288
-        max: 4695144
+        min: 3256320
+        max: 5857168
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1024
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1024
-      job_id: jnp10rl5q
+      job_id: jegn0kmv5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:41:56.326001Z'
+    timestamp: '2024-04-02T15:50:06.444234Z'
     torchscript_onnx_qnn:
-      inference_time: 65507.0
-      throughput: 15.265544140320882
+      inference_time: 69637.0
+      throughput: 14.360182087108864
       estimated_peak_memory_range:
-        min: 57344
-        max: 55933800
+        min: 143360
+        max: 108258128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1027
+        layers_on_npu: 1026
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1027
-      job_id: jz5woo6p1
+        total_layers: 1026
+      job_id: j2p04622g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 53553.0
-      throughput: 18.673090209698803
+      inference_time: 50712.0
+      throughput: 19.71919861176842
       estimated_peak_memory_range:
-        min: 3276800
-        max: 574983152
+        min: 77824
+        max: 582298832
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1024
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1024
-      job_id: jvgdwjl5j
+      job_id: jqpyzmjrg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:46:30.960659Z'
+    timestamp: '2024-04-02T15:52:55.582636Z'
     torchscript_onnx_qnn:
-      inference_time: 50563.0
-      throughput: 19.777307517354586
+      inference_time: 49723.0
+      throughput: 20.11141725157372
       estimated_peak_memory_range:
-        min: 86016
-        max: 240922112
+        min: 1306624
+        max: 256079456
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1027
+        layers_on_npu: 1026
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1027
-      job_id: jmg9vvl57
+        total_layers: 1026
+      job_id: j1p821mzp
       job_status: Passed
```

## qai_hub_models/models/facebook_denoiser/export.py

```diff
@@ -180,14 +180,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/facebook_denoiser/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Facebook-Denoiser
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 711384.0
-      throughput: 1.4057105585731475
+      inference_time: 746968.0
+      throughput: 1.338745434878067
       estimated_peak_memory_range:
-        min: 236318720
-        max: 349174920
+        min: 379867136
+        max: 382919144
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 209
         total_layers: 209
-      job_id: j1p3kwm52
+      job_id: jogkv8qyp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:10:56.043154Z'
+    timestamp: '2024-04-02T15:25:02.878241Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 670316.0
-      throughput: 1.4918337023135357
+      inference_time: 692152.0
+      throughput: 1.4447693570198454
       estimated_peak_memory_range:
-        min: 481374208
-        max: 504692832
+        min: 372510720
+        max: 393584320
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 209
         total_layers: 209
-      job_id: jwgoy4158
+      job_id: jn5q0vr7p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:10:56.043167Z'
+    timestamp: '2024-04-02T15:25:02.878255Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/fastsam_s/export.py

```diff
@@ -195,14 +195,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fastsam_s/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FastSam-S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 13114.0
-      throughput: 76.25438462711605
+      inference_time: 8735.0
+      throughput: 114.48196908986834
       estimated_peak_memory_range:
-        min: 7823360
-        max: 25444440
+        min: 7831552
+        max: 10552872
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 288
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 288
-      job_id: jegn21vgo
+      job_id: jw562wzvg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:39:15.450027Z'
+    timestamp: '2024-04-02T16:06:17.970106Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 9234.0
-      throughput: 108.29542993285683
+      inference_time: 6461.0
+      throughput: 154.7748026621266
       estimated_peak_memory_range:
-        min: 6332416
-        max: 79756208
+        min: 6328320
+        max: 76883760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 288
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 288
-      job_id: joprkxv50
+      job_id: j1p3n61x5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:39:15.450036Z'
+    timestamp: '2024-04-02T16:06:17.970119Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/fastsam_x/export.py

```diff
@@ -195,14 +195,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fastsam_x/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FastSam-X
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 64155.0
-      throughput: 15.587249629802821
+      inference_time: 51073.0
+      throughput: 19.579817124508057
       estimated_peak_memory_range:
-        min: 9207808
-        max: 14058240
+        min: 9240576
+        max: 13971912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 420
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 420
-      job_id: jw566k75o
+      job_id: jwgoz8n4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:56:58.796143Z'
+    timestamp: '2024-04-02T15:25:26.476231Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 47867.0
-      throughput: 20.891219420477572
+      inference_time: 36142.0
+      throughput: 27.66864036301256
       estimated_peak_memory_range:
-        min: 7962624
-        max: 152777152
+        min: 98304
+        max: 142182032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 420
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 420
-      job_id: j1p3kyz52
+      job_id: j1pvq7r7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:56:58.796153Z'
+    timestamp: '2024-04-02T15:25:26.476245Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/fcn_resnet50/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fcn_resnet50/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FCN_ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 8550.0
-      throughput: 116.95906432748538
+      inference_time: 8557.0
+      throughput: 116.86338670094659
       estimated_peak_memory_range:
-        min: 4263936
-        max: 6443424
+        min: 159744
+        max: 7109192
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jn5q8dm57
+      job_id: jygz2njzg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:32:21.594233Z'
+    timestamp: '2024-04-02T15:38:36.923072Z'
     torchscript_onnx_qnn:
-      inference_time: 7881.0
-      throughput: 126.8874508311128
+      inference_time: 7883.0
+      throughput: 126.85525815045034
       estimated_peak_memory_range:
         min: 20480
-        max: 13250472
+        max: 10311800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 126
+        layers_on_npu: 125
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 126
-      job_id: jw566075o
+        total_layers: 125
+      job_id: jmg90dyqg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 6407.0
-      throughput: 156.07928827844546
+      inference_time: 6324.0
+      throughput: 158.12776723592663
       estimated_peak_memory_range:
-        min: 4251648
-        max: 76376944
+        min: 2187264
+        max: 78458400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: j1glnqlpv
+      job_id: jz5ww43z5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:34:20.486125Z'
+    timestamp: '2024-04-02T15:41:20.209217Z'
     torchscript_onnx_qnn:
-      inference_time: 5846.0
-      throughput: 171.05713308244952
+      inference_time: 5820.0
+      throughput: 171.82130584192439
       estimated_peak_memory_range:
-        min: 638976
-        max: 55934880
+        min: 618496
+        max: 59720272
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 126
+        layers_on_npu: 125
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 126
-      job_id: j1p3krz52
+        total_layers: 125
+      job_id: jnp126wkg
       job_status: Passed
```

## qai_hub_models/models/ffnet_122ns_lowres/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_122ns_lowres/model.py

```diff
@@ -7,11 +7,11 @@
 from qai_hub_models.models._shared.ffnet.model import FFNetLowRes
 
 MODEL_ID = __name__.split(".")[-2]
 
 
 class FFNet122NSLowRes(FFNetLowRes):
     @classmethod
-    def from_pretrained(cls) -> FFNet122NSLowRes:
-        return FFNetLowRes.from_pretrained.__func__(
-            cls, "segmentation_ffnet122NS_CCC_mobile_pre_down"
+    def from_pretrained(cls) -> FFNet122NSLowRes:  # type: ignore
+        return super(FFNet122NSLowRes, cls).from_pretrained(
+            "segmentation_ffnet122NS_CCC_mobile_pre_down"
         )
```

## qai_hub_models/models/ffnet_122ns_lowres/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-122NS-LowRes
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 10407.0
-      throughput: 96.08917075045642
+      inference_time: 9649.0
+      throughput: 103.6376826614157
       estimated_peak_memory_range:
-        min: 12288
-        max: 2345904
+        min: 647168
+        max: 2901040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 216
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 216
-      job_id: jmg9vel57
+      job_id: jvgdn2qk5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:45:44.022843Z'
+    timestamp: '2024-04-02T15:16:46.197377Z'
     torchscript_onnx_qnn:
-      inference_time: 10785.0
-      throughput: 92.7213722763097
+      inference_time: 10810.0
+      throughput: 92.50693802035153
       estimated_peak_memory_range:
-        min: 6205440
-        max: 39312144
+        min: 6344704
+        max: 40462128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 349
+        layers_on_npu: 348
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 349
-      job_id: jvgdwle5j
+        total_layers: 348
+      job_id: jqp4n3dqg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 7373.0
-      throughput: 135.63000135630003
+      inference_time: 6923.0
+      throughput: 144.4460494005489
       estimated_peak_memory_range:
-        min: 643072
-        max: 58158976
+        min: 405504
+        max: 60494448
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 216
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 216
-      job_id: jnp10x25q
+      job_id: jz5729lqp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:47:44.631260Z'
+    timestamp: '2024-04-02T15:19:30.896664Z'
     torchscript_onnx_qnn:
-      inference_time: 7627.0
-      throughput: 131.1131506490101
+      inference_time: 7600.0
+      throughput: 131.57894736842104
       estimated_peak_memory_range:
-        min: 6311936
-        max: 85982464
+        min: 6307840
+        max: 86339936
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 349
+        layers_on_npu: 348
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 349
-      job_id: jz57z3lp3
+        total_layers: 348
+      job_id: j0px9x6jp
       job_status: Passed
```

## qai_hub_models/models/ffnet_40s/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_40s/model.py

```diff
@@ -7,9 +7,9 @@
 from qai_hub_models.models._shared.ffnet.model import FFNet
 
 MODEL_ID = __name__.split(".")[-2]
 
 
 class FFNet40S(FFNet):
     @classmethod
-    def from_pretrained(cls) -> FFNet40S:
-        return FFNet.from_pretrained.__func__(cls, "segmentation_ffnet40S_dBBB_mobile")
+    def from_pretrained(cls) -> FFNet40S:  # type: ignore
+        return super(FFNet40S, cls).from_pretrained("segmentation_ffnet40S_dBBB_mobile")
```

## qai_hub_models/models/ffnet_40s/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-40S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 22513.0
-      throughput: 44.41878026029405
+      inference_time: 22812.0
+      throughput: 43.836577240049095
       estimated_peak_memory_range:
-        min: 2539520
-        max: 5190832
+        min: 2555904
+        max: 5191296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 92
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 92
-      job_id: jwgoyl458
+      job_id: jo5me86yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:39.279085Z'
+    timestamp: '2024-04-02T16:00:28.301836Z'
     torchscript_onnx_qnn:
-      inference_time: 17466.0
-      throughput: 57.25409366769724
+      inference_time: 17334.0
+      throughput: 57.69008884273682
       estimated_peak_memory_range:
-        min: 25210880
-        max: 48310168
+        min: 25214976
+        max: 45212320
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 141
+        layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 141
-      job_id: j7gjxr7pd
+        total_layers: 140
+      job_id: jep2xelxg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 16613.0
-      throughput: 60.19382411364594
+      inference_time: 16599.0
+      throughput: 60.24459304777396
       estimated_peak_memory_range:
-        min: 61440
-        max: 100488656
+        min: 16384
+        max: 106444032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 92
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 92
-      job_id: j1pv3l75x
+      job_id: jopr6wevp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:15:49.686166Z'
+    timestamp: '2024-04-02T16:02:52.508368Z'
     torchscript_onnx_qnn:
-      inference_time: 12681.0
-      throughput: 78.85813421654444
+      inference_time: 12563.0
+      throughput: 79.59882193743533
       estimated_peak_memory_range:
-        min: 25182208
-        max: 82551136
+        min: 25210880
+        max: 86653840
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 141
+        layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 141
-      job_id: jlpe977gr
+        total_layers: 140
+      job_id: jqpyzm6rg
       job_status: Passed
```

## qai_hub_models/models/ffnet_40s_quantized/export.py

```diff
@@ -198,14 +198,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_40s_quantized/model.py

```diff
@@ -11,20 +11,19 @@
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 DEFAULT_ENCODINGS = "encodings.json"
 
 
 class FFNet40SQuantizable(FFNetQuantizable, FFNet40S):
     @classmethod
-    def from_pretrained(
+    def from_pretrained(  # type: ignore
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> FFNet40SQuantizable:
-        return FFNetQuantizable.from_pretrained.__func__(
-            cls,
+        return super(FFNet40SQuantizable, cls).from_pretrained(
             "segmentation_ffnet40S_dBBB_mobile",
             aimet_encodings=aimet_encodings,
         )
 
     @classmethod
     def default_aimet_encodings(cls) -> str:
         return CachedWebModelAsset.from_asset_store(
```

## qai_hub_models/models/ffnet_40s_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-40S-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6439.0
-      throughput: 155.3036185743128
+      inference_time: 6451.0
+      throughput: 155.0147263990079
       estimated_peak_memory_range:
-        min: 888832
-        max: 2660784
+        min: 872448
+        max: 25600304
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 97
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 97
-      job_id: jqp4q92go
+      job_id: jogkv83yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:35:32.125659Z'
+    timestamp: '2024-04-02T15:52:22.278215Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 4671.0
-      throughput: 214.08691928923142
+      inference_time: 4634.0
+      throughput: 215.79628830384118
       estimated_peak_memory_range:
-        min: 16384
-        max: 65022448
+        min: 180224
+        max: 67612432
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 97
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 97
-      job_id: j0pxvd8g7
+      job_id: jn5q0v37p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:35:32.125673Z'
+    timestamp: '2024-04-02T15:52:22.278229Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/ffnet_54s/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_54s/model.py

```diff
@@ -7,9 +7,9 @@
 from qai_hub_models.models._shared.ffnet.model import FFNet
 
 MODEL_ID = __name__.split(".")[-2]
 
 
 class FFNet54S(FFNet):
     @classmethod
-    def from_pretrained(cls) -> FFNet54S:
-        return FFNet.from_pretrained.__func__(cls, "segmentation_ffnet54S_dBBB_mobile")
+    def from_pretrained(cls) -> FFNet54S:  # type: ignore
+        return super(FFNet54S, cls).from_pretrained("segmentation_ffnet54S_dBBB_mobile")
```

## qai_hub_models/models/ffnet_54s/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-54S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 24853.0
-      throughput: 40.23659115599727
+      inference_time: 25516.0
+      throughput: 39.191095783038094
       estimated_peak_memory_range:
-        min: 2572288
-        max: 4947328
+        min: 3219456
+        max: 5162680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 113
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 113
-      job_id: j0pxv38g7
+      job_id: jw562wnvg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:47:57.765081Z'
+    timestamp: '2024-04-02T16:00:41.754790Z'
     torchscript_onnx_qnn:
-      inference_time: 19975.0
-      throughput: 50.06257822277847
+      inference_time: 20433.0
+      throughput: 48.94043948514658
       estimated_peak_memory_range:
-        min: 25214976
-        max: 52299192
+        min: 25186304
+        max: 50574640
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 176
+        layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 176
-      job_id: joprkok50
+        total_layers: 175
+      job_id: jwgoz834p
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 18421.0
-      throughput: 54.28586938819825
+      inference_time: 18562.0
+      throughput: 53.87350501023597
       estimated_peak_memory_range:
-        min: 462848
-        max: 113159440
+        min: 2244608
+        max: 122307680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 113
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 113
-      job_id: jo5mro7gk
+      job_id: j1p3n6ex5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:49:56.432155Z'
+    timestamp: '2024-04-02T16:03:03.931513Z'
     torchscript_onnx_qnn:
-      inference_time: 14570.0
-      throughput: 68.63417982155113
+      inference_time: 14524.0
+      throughput: 68.85155604516662
       estimated_peak_memory_range:
-        min: 154132480
-        max: 217703424
+        min: 231440384
+        max: 301103936
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 176
+        layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 176
-      job_id: jep2846p6
+        total_layers: 175
+      job_id: j1pvq7v7g
       job_status: Passed
```

## qai_hub_models/models/ffnet_54s_quantized/export.py

```diff
@@ -198,14 +198,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_54s_quantized/model.py

```diff
@@ -11,20 +11,20 @@
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 DEFAULT_ENCODINGS = "encodings.json"
 
 
 class FFNet54SQuantizable(FFNetQuantizable, FFNet54S):
     @classmethod
-    def from_pretrained(
+    def from_pretrained(  # type: ignore
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> FFNet54SQuantizable:
-        return FFNetQuantizable.from_pretrained.__func__(
-            cls, "segmentation_ffnet54S_dBBB_mobile", aimet_encodings=aimet_encodings
+        return super(FFNet54SQuantizable, cls).from_pretrained(
+            "segmentation_ffnet54S_dBBB_mobile", aimet_encodings=aimet_encodings
         )
 
     @classmethod
     def default_aimet_encodings(cls) -> str:
         return CachedWebModelAsset.from_asset_store(
             MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
         ).fetch()
```

## qai_hub_models/models/ffnet_54s_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-54S-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7127.0
-      throughput: 140.31149151115477
+      inference_time: 7122.0
+      throughput: 140.40999719180004
       estimated_peak_memory_range:
-        min: 712704
-        max: 2530520
+        min: 823296
+        max: 9540112
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: j7gjxeepd
+      job_id: j7gjdqe7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:16:07.677264Z'
+    timestamp: '2024-04-02T15:24:42.915036Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 5136.0
-      throughput: 194.70404984423675
+      inference_time: 5147.0
+      throughput: 194.28793471925394
       estimated_peak_memory_range:
-        min: 16384
-        max: 71676704
+        min: 233472
+        max: 74819648
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: jnp10e75q
+      job_id: jz5ww4qz5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:16:07.677274Z'
+    timestamp: '2024-04-02T15:24:42.915050Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/ffnet_78s/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_78s/model.py

```diff
@@ -7,9 +7,9 @@
 from qai_hub_models.models._shared.ffnet.model import FFNet
 
 MODEL_ID = __name__.split(".")[-2]
 
 
 class FFNet78S(FFNet):
     @classmethod
-    def from_pretrained(cls) -> FFNet78S:
-        return FFNet.from_pretrained.__func__(cls, "segmentation_ffnet78S_dBBB_mobile")
+    def from_pretrained(cls) -> FFNet78S:  # type: ignore
+        return super(FFNet78S, cls).from_pretrained("segmentation_ffnet78S_dBBB_mobile")
```

## qai_hub_models/models/ffnet_78s/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-78S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 28993.0
-      throughput: 34.49108405477184
+      inference_time: 29260.0
+      throughput: 34.17634996582365
       estimated_peak_memory_range:
-        min: 2699264
-        max: 4868664
+        min: 2568192
+        max: 5238920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: j0pxvq9g7
+      job_id: jmg90dwqg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:58:52.803970Z'
+    timestamp: '2024-04-02T15:27:40.352259Z'
     torchscript_onnx_qnn:
-      inference_time: 23765.0
-      throughput: 42.07868714496108
+      inference_time: 23452.0
+      throughput: 42.64028654272557
       estimated_peak_memory_range:
-        min: 25214976
-        max: 45434792
+        min: 24997888
+        max: 45509104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 236
+        layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 236
-      job_id: joprkre50
+        total_layers: 235
+      job_id: jvgdn2ok5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 21479.0
-      throughput: 46.557102285953725
+      inference_time: 21325.0
+      throughput: 46.893317702227435
       estimated_peak_memory_range:
-        min: 2478080
-        max: 130875008
+        min: 1220608
+        max: 135944608
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jegn24mgo
+      job_id: jnp126ekg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:00:58.594801Z'
+    timestamp: '2024-04-02T15:30:22.141498Z'
     torchscript_onnx_qnn:
-      inference_time: 17826.0
-      throughput: 56.09783462358353
+      inference_time: 17797.0
+      throughput: 56.18924537843457
       estimated_peak_memory_range:
-        min: 25219072
-        max: 99798224
+        min: 228487168
+        max: 306923024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 236
+        layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 236
-      job_id: jep281mp6
+        total_layers: 235
+      job_id: jz5ww4qj5
       job_status: Passed
```

## qai_hub_models/models/ffnet_78s_lowres/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_78s_lowres/model.py

```diff
@@ -7,11 +7,11 @@
 from qai_hub_models.models._shared.ffnet.model import FFNetLowRes
 
 MODEL_ID = __name__.split(".")[-2]
 
 
 class FFNet78SLowRes(FFNetLowRes):
     @classmethod
-    def from_pretrained(cls) -> FFNet78SLowRes:
-        return FFNetLowRes.from_pretrained.__func__(
-            cls, "segmentation_ffnet78S_BCC_mobile_pre_down"
+    def from_pretrained(cls) -> FFNet78SLowRes:  # type: ignore
+        return super(FFNet78SLowRes, cls).from_pretrained(
+            "segmentation_ffnet78S_BCC_mobile_pre_down"
         )
```

## qai_hub_models/models/ffnet_78s_lowres/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-78S-LowRes
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 10810.0
-      throughput: 92.50693802035153
+      inference_time: 10717.0
+      throughput: 93.30969487729774
       estimated_peak_memory_range:
-        min: 0
-        max: 1890472
+        min: 663552
+        max: 2911376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jegn2dmgo
+      job_id: jmg90dwvg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:53:13.887710Z'
+    timestamp: '2024-04-02T15:16:39.163437Z'
     torchscript_onnx_qnn:
-      inference_time: 11408.0
-      throughput: 87.6577840112202
+      inference_time: 11424.0
+      throughput: 87.53501400560224
       estimated_peak_memory_range:
-        min: 16384
-        max: 52414400
+        min: 40960
+        max: 53367328
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 237
+        layers_on_npu: 236
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 237
-      job_id: jep28qmp6
+        total_layers: 236
+      job_id: jvgdn2ol5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 7768.0
-      throughput: 128.73326467559218
+      inference_time: 7571.0
+      throughput: 132.0829480914014
       estimated_peak_memory_range:
-        min: 540672
-        max: 52237632
+        min: 45056
+        max: 50924912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: joprkme50
+      job_id: jnp126elg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:57:44.327749Z'
+    timestamp: '2024-04-02T15:19:20.871561Z'
     torchscript_onnx_qnn:
-      inference_time: 8084.0
-      throughput: 123.70113805047006
+      inference_time: 7980.0
+      throughput: 125.31328320802005
       estimated_peak_memory_range:
-        min: 6328320
-        max: 72586224
+        min: 6307840
+        max: 71292128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 237
+        layers_on_npu: 236
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 237
-      job_id: jqpyek4gy
+        total_layers: 236
+      job_id: jz5729drp
       job_status: Passed
```

## qai_hub_models/models/ffnet_78s_quantized/export.py

```diff
@@ -198,14 +198,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_78s_quantized/model.py

```diff
@@ -11,20 +11,20 @@
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 DEFAULT_ENCODINGS = "encodings.json"
 
 
 class FFNet78SQuantizable(FFNetQuantizable, FFNet78S):
     @classmethod
-    def from_pretrained(
+    def from_pretrained(  # type: ignore
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> FFNet78SQuantizable:
-        return FFNetQuantizable.from_pretrained.__func__(
-            cls, "segmentation_ffnet78S_dBBB_mobile", aimet_encodings=aimet_encodings
+        return super(FFNet78SQuantizable, cls).from_pretrained(
+            "segmentation_ffnet78S_dBBB_mobile", aimet_encodings=aimet_encodings
         )
 
     @classmethod
     def default_aimet_encodings(cls) -> str:
         return CachedWebModelAsset.from_asset_store(
             MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
         ).fetch()
```

## qai_hub_models/models/ffnet_78s_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: FFNet-78S-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 8368.0
-      throughput: 119.50286806883365
+      inference_time: 8383.0
+      throughput: 119.28903733746868
       estimated_peak_memory_range:
-        min: 663552
-        max: 2264096
+        min: 692224
+        max: 40285240
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 154
-      job_id: jegn2jmgo
+      job_id: jqp4n3wlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:31:42.853131Z'
+    timestamp: '2024-04-02T15:19:19.002436Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 6095.0
-      throughput: 164.06890894175552
+      inference_time: 5978.0
+      throughput: 167.2800267648043
       estimated_peak_memory_range:
-        min: 16384
-        max: 84212448
+        min: 28672
+        max: 87145904
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 154
-      job_id: jep282mp6
+      job_id: j0px9x19p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:31:42.853166Z'
+    timestamp: '2024-04-02T15:19:19.002449Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/googlenet/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/googlenet/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: GoogLeNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1041.0
-      throughput: 960.6147934678194
+      inference_time: 1043.0
+      throughput: 958.7727708533077
       estimated_peak_memory_range:
         min: 12288
-        max: 1836376
+        max: 2222648
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: joprq3950
+      job_id: j2p046reg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:24:33.473846Z'
+    timestamp: '2024-04-02T15:44:36.230603Z'
     torchscript_onnx_qnn:
-      inference_time: 1083.0
-      throughput: 923.3610341643582
+      inference_time: 1085.0
+      throughput: 921.6589861751152
       estimated_peak_memory_range:
-        min: 32768
-        max: 26497136
+        min: 28672
+        max: 26694664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 144
+        layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 144
-      job_id: jqpyw37gy
+        total_layers: 143
+      job_id: jogkv8yop
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 647.0
-      throughput: 1545.595054095827
+      inference_time: 685.0
+      throughput: 1459.85401459854
       estimated_peak_memory_range:
-        min: 16384
-        max: 45415536
+        min: 12288
+        max: 45701264
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jep26y4g6
+      job_id: j1p82178p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:28:59.659531Z'
+    timestamp: '2024-04-02T15:47:18.152147Z'
     torchscript_onnx_qnn:
-      inference_time: 682.0
-      throughput: 1466.275659824047
+      inference_time: 694.0
+      throughput: 1440.922190201729
       estimated_peak_memory_range:
-        min: 0
-        max: 49977664
+        min: 618496
+        max: 54284752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 144
+        layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 144
-      job_id: j2p0q065w
+        total_layers: 143
+      job_id: jn5q0v2mp
       job_status: Passed
```

## qai_hub_models/models/googlenet_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/googlenet_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.googlenet.model import GoogLeNet
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 from qai_hub_models.utils.quantization_aimet import tie_aimet_observer_groups
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 DEFAULT_ENCODINGS = "googlenet_quantized_encodings.json"
 
 
@@ -40,19 +39,14 @@
     ) -> None:
         GoogLeNet.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "GoogLeNetQuantizable":
         """
         Parameters:
```

## qai_hub_models/models/googlenet_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: GoogLeNetQuantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 331.0
-      throughput: 3021.1480362537764
+      inference_time: 290.0
+      throughput: 3448.2758620689656
       estimated_peak_memory_range:
         min: 12288
-        max: 1926544
+        max: 1574472
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 87
+        layers_on_npu: 85
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 87
-      job_id: jnp109l5q
+        total_layers: 85
+      job_id: j1gl4lkl5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:54:15.624495Z'
+    timestamp: '2024-04-02T16:10:55.550466Z'
     torchscript_onnx_qnn:
-      inference_time: 365.0
-      throughput: 2739.72602739726
+      inference_time: 337.0
+      throughput: 2967.359050445104
       estimated_peak_memory_range:
-        min: 638976
-        max: 5546832
+        min: 73728
+        max: 4963272
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 89
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 89
-      job_id: jqp4qzlgo
+        total_layers: 86
+      job_id: jwgoz8vdp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 248.0
-      throughput: 4032.2580645161293
+      inference_time: 208.0
+      throughput: 4807.692307692308
       estimated_peak_memory_range:
-        min: 16384
-        max: 32361600
+        min: 12288
+        max: 33584240
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 87
+        layers_on_npu: 85
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 87
-      job_id: jz57zqrp3
+        total_layers: 85
+      job_id: j1p3n6mz5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:00:04.109028Z'
+    timestamp: '2024-04-02T16:12:28.573393Z'
     torchscript_onnx_qnn:
-      inference_time: 258.0
-      throughput: 3875.968992248062
+      inference_time: 248.0
+      throughput: 4032.2580645161293
       estimated_peak_memory_range:
-        min: 618496
-        max: 47357168
+        min: 163840
+        max: 44949040
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 89
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 89
-      job_id: j0pxvw9g7
+        total_layers: 86
+      job_id: j1pvq7wmg
       job_status: Passed
```

## qai_hub_models/models/hrnet_pose/export.py

```diff
@@ -112,15 +112,15 @@
     # Trace the model
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
         target_runtime,
         compile_options
-        + " --force_channel_last_input image_tensor"
+        + " --force_channel_last_input image"
         + " --force_channel_last_output output_0",
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub.Device(device),
@@ -152,15 +152,15 @@
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
         hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+            "image", sample_inputs, target_runtime
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub.Device(device),
             name=model_name,
             options=profile_options_all,
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/hrnet_pose/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: HRNetPose
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2519.0
-      throughput: 396.9829297340214
+      inference_time: 2297.0
+      throughput: 435.35045711798
       estimated_peak_memory_range:
-        min: 24576
-        max: 3015464
+        min: 16384
+        max: 2784976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 515
+        layers_on_npu: 514
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 515
-      job_id: jep28oxp6
+        total_layers: 514
+      job_id: j7gjdql8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:57:53.421052Z'
+    timestamp: '2024-04-02T15:27:57.614569Z'
     torchscript_onnx_qnn:
-      inference_time: 2608.0
-      throughput: 383.4355828220859
+      inference_time: 2295.0
+      throughput: 435.7298474945534
       estimated_peak_memory_range:
-        min: 49152
-        max: 58039344
+        min: 12288
+        max: 58975320
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 747
+        layers_on_npu: 745
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 747
-      job_id: j1p8ojzg9
+        total_layers: 745
+      job_id: jygz2n76g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1878.0
-      throughput: 532.4813631522896
+      inference_time: 1723.0
+      throughput: 580.3830528148578
       estimated_peak_memory_range:
         min: 16384
-        max: 103402912
+        max: 106291456
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 515
+        layers_on_npu: 514
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 515
-      job_id: j2p0yo2gw
+        total_layers: 514
+      job_id: jlpeoyv0g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:59:59.942614Z'
+    timestamp: '2024-04-02T15:30:43.560020Z'
     torchscript_onnx_qnn:
-      inference_time: 1922.0
-      throughput: 520.2913631633714
+      inference_time: 1715.0
+      throughput: 583.0903790087464
       estimated_peak_memory_range:
         min: 606208
-        max: 178228720
+        max: 177690672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 747
+        layers_on_npu: 745
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 747
-      job_id: j1glnwepv
+        total_layers: 745
+      job_id: jz5ww49j5
       job_status: Passed
```

## qai_hub_models/models/huggingface_wavlm_base_plus/export.py

```diff
@@ -176,14 +176,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/huggingface_wavlm_base_plus/model.py

```diff
@@ -72,14 +72,22 @@
         batch_size: int = 1,
         sample_length: int = 80000,
     ) -> InputSpec:
         # This can be used with the qai_hub python API to declare
         # the model input specification upon submitting a profile job.
         return {"input": ((batch_size, sample_length), "float32")}
 
+    def get_hub_profile_options(
+        self, target_runtime: TargetRuntime, other_profile_options: str = ""
+    ) -> str:
+        profile_options = super().get_hub_profile_options(
+            target_runtime, other_profile_options
+        )
+        return profile_options + " --compute_unit cpu"
+
 
 # Modules used to override Huggingface WavLM to be NPU friendly
 class SliceConv1d(torch.nn.Module):
     def __init__(self, orig_module: torch.nn.Conv1d, slice_size: int = 16000):
         """Slice inputs to conv1d to limit the input size to any conv"""
         super().__init__()
         assert isinstance(orig_module, torch.nn.Conv1d)
@@ -165,30 +173,14 @@
 
         # apply group norm
         x = self.orig_module.layer_norm(x)
         x = self.orig_module.activation(x)
         x = torch.concat(torch.unbind(x, axis=2), axis=-1)
         return x[:, :, :-1]
 
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --compute_unit gpu"
-
-    def get_hub_profile_options(
-        self, target_runtime: TargetRuntime, other_profile_options: str = ""
-    ) -> str:
-        profile_options = super().get_hub_profile_options(
-            target_runtime, other_profile_options
-        )
-        return profile_options + " --compute_unit gpu"
-
 
 def convert_to_wavlm_npu(model: WavLMModel):
     """
     Apply changes to make model NPU friendly
     """
     assert isinstance(model, WavLMModel)
     conv_layer = model.feature_extractor.conv_layers[0]
```

## qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: HuggingFace-WavLM-Base-Plus
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 237767939.0
-      throughput: 0.0042057815036197965
+      inference_time: 911873.0
+      throughput: 1.0966439405487387
       estimated_peak_memory_range:
-        min: 11886592
-        max: 15703120
-      primary_compute_unit: NPU
-      precision: fp16
+        min: 149282816
+        max: 153276888
+      primary_compute_unit: CPU
+      precision: fp32
       layer_info:
-        layers_on_npu: 848
+        layers_on_npu: 0
         layers_on_gpu: 0
-        layers_on_cpu: 0
+        layers_on_cpu: 848
         total_layers: 848
-      job_id: jlpe928gr
+      job_id: jz57yq4q5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:13:01.265817Z'
+    timestamp: '2024-03-26T15:30:16.725161Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 174470189.0
-      throughput: 0.005731638199807303
+      inference_time: 712941.0
+      throughput: 1.4026406112146728
       estimated_peak_memory_range:
-        min: 11321344
-        max: 711668304
-      primary_compute_unit: NPU
-      precision: fp16
+        min: 147787776
+        max: 179693888
+      primary_compute_unit: CPU
+      precision: fp32
       layer_info:
-        layers_on_npu: 848
+        layers_on_npu: 0
         layers_on_gpu: 0
-        layers_on_cpu: 0
+        layers_on_cpu: 848
         total_layers: 848
-      job_id: jygzew4g8
+      job_id: joprvzlvg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:13:01.265830Z'
+    timestamp: '2024-03-26T15:30:16.725174Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/inception_v3/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/inception_v3/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Inception-v3
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1435.0
-      throughput: 696.8641114982578
+      inference_time: 1428.0
+      throughput: 700.2801120448179
       estimated_peak_memory_range:
-        min: 20480
-        max: 1921832
+        min: 28672
+        max: 2409000
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: jqpyeorgy
+      job_id: jqp4n3xlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:30:27.461416Z'
+    timestamp: '2024-04-02T15:16:46.144805Z'
     torchscript_onnx_qnn:
-      inference_time: 1475.0
-      throughput: 677.9661016949152
+      inference_time: 1458.0
+      throughput: 685.8710562414266
       estimated_peak_memory_range:
-        min: 20480
-        max: 148512392
+        min: 622592
+        max: 149278208
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 220
+        layers_on_npu: 219
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 220
-      job_id: j1p8oezg9
+        total_layers: 219
+      job_id: jo5me8wqp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1069.0
-      throughput: 935.4536950420954
+      inference_time: 1047.0
+      throughput: 955.1098376313277
       estimated_peak_memory_range:
         min: 12288
-        max: 50854560
+        max: 51670896
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: j2p0ym2gw
+      job_id: j0px9x79p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:33:41.582505Z'
+    timestamp: '2024-04-02T15:19:30.685618Z'
     torchscript_onnx_qnn:
-      inference_time: 1082.0
-      throughput: 924.2144177449168
+      inference_time: 1083.0
+      throughput: 923.3610341643582
       estimated_peak_memory_range:
         min: 618496
-        max: 68383952
+        max: 66991296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 220
+        layers_on_npu: 219
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 220
-      job_id: jogkz2ygd
+        total_layers: 219
+      job_id: jegn0k9m5
       job_status: Passed
```

## qai_hub_models/models/inception_v3_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/inception_v3_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.inception_v3.model import InceptionNetV3
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 from qai_hub_models.utils.quantization_aimet import tie_aimet_observer_groups
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 4
 DEFAULT_ENCODINGS = "inception_v3_quantized_encodings.json"
 
 
@@ -43,19 +42,14 @@
     ) -> None:
         InceptionNetV3.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "InceptionNetV3Quantizable":
         """
         Parameters:
@@ -192,15 +186,7 @@
                 n.Mixed_7c.branch_pool.module_relu_93,
                 n.Mixed_7c.module_cat_12,
                 n.Mixed_7c.module_cat_13,
                 n.Mixed_7c.module_cat_14,
             ],
         ]
         tie_aimet_observer_groups(groups)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/inception_v3_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Inception-v3-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 615.0
-      throughput: 1626.0162601626016
+      inference_time: 633.0
+      throughput: 1579.778830963665
       estimated_peak_memory_range:
-        min: 36864
-        max: 2508048
+        min: 12288
+        max: 1553272
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 144
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 144
-      job_id: jz57zj9p3
+      job_id: jopr6w4ep
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:41:04.203939Z'
+    timestamp: '2024-04-02T15:19:51.517901Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 487.0
-      throughput: 2053.388090349076
+      inference_time: 461.0
+      throughput: 2169.1973969631235
       estimated_peak_memory_range:
-        min: 0
-        max: 63551712
+        min: 12288
+        max: 64115632
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 144
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 144
-      job_id: j0pxv7lg7
+      job_id: jep2xe7mg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:41:04.203947Z'
+    timestamp: '2024-04-02T15:19:51.517914Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/lama_dilated/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/lama_dilated/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: LaMa-Dilated
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 88628.0
-      throughput: 11.283115945299453
+      inference_time: 87826.0
+      throughput: 11.386149887277115
       estimated_peak_memory_range:
-        min: 3252224
-        max: 140731056
+        min: 3280896
+        max: 139026816
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 346
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 346
-      job_id: j2p0yv0gw
+      job_id: jqpyzm44g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:53:06.506039Z'
+    timestamp: '2024-04-02T15:27:34.825319Z'
     torchscript_onnx_qnn:
-      inference_time: 84164.0
-      throughput: 11.881564564421843
+      inference_time: 82023.0
+      throughput: 12.191702327395975
       estimated_peak_memory_range:
-        min: 4321280
-        max: 33964280
+        min: 667648
+        max: 36691936
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 333
+        layers_on_npu: 332
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 333
-      job_id: j1gln12pv
+        total_layers: 332
+      job_id: jogkv8lop
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 62025.0
-      throughput: 16.12253123740427
+      inference_time: 61367.0
+      throughput: 16.295403066794858
       estimated_peak_memory_range:
-        min: 225280
-        max: 245293744
+        min: 36864
+        max: 268387328
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 346
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 346
-      job_id: jogkz9vgd
+      job_id: j1p82138p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:58:58.279247Z'
+    timestamp: '2024-04-02T15:30:17.633943Z'
     torchscript_onnx_qnn:
-      inference_time: 58950.0
-      throughput: 16.963528413910094
+      inference_time: 57731.0
+      throughput: 17.321716235644626
       estimated_peak_memory_range:
-        min: 78331904
-        max: 243926976
+        min: 155123712
+        max: 340846160
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 333
+        layers_on_npu: 332
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 333
-      job_id: jw566dn5o
+        total_layers: 332
+      job_id: jn5q0v7mp
       job_status: Passed
```

## qai_hub_models/models/litehrnet/export.py

```diff
@@ -180,14 +180,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/litehrnet/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: LiteHRNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 15866.0
-      throughput: 63.02785831337451
+      inference_time: 15544.0
+      throughput: 64.33350488934637
       estimated_peak_memory_range:
-        min: 6811648
-        max: 10391632
+        min: 6557696
+        max: 21204936
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1226
         layers_on_gpu: 0
         layers_on_cpu: 10
         total_layers: 1236
-      job_id: jn5q83o57
+      job_id: j1gl4l0l5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:41:50.802497Z'
+    timestamp: '2024-04-02T15:47:29.705067Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 10704.0
-      throughput: 93.42301943198804
+      inference_time: 10368.0
+      throughput: 96.45061728395062
       estimated_peak_memory_range:
         min: 20480
-        max: 71674208
+        max: 72953920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1226
         layers_on_gpu: 0
         layers_on_cpu: 10
         total_layers: 1236
-      job_id: j1glnkmpv
+      job_id: jw562w37g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:41:50.802505Z'
+    timestamp: '2024-04-02T15:47:29.705081Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/mediapipe_face/export.py

```diff
@@ -23,15 +23,14 @@
 from qai_hub_models.utils.printing import (
     print_inference_metrics,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
-    transpose_channel_first_to_last,
 )
 
 ALL_COMPONENTS = ["MediaPipeFaceDetector", "MediaPipeFaceLandmarkDetector"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
@@ -123,15 +122,15 @@
         input_spec = component.get_input_spec()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options + " --force_channel_last_input image"
+            target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub.Device(device),
             name=f"{model_name}_{component_name}",
@@ -166,21 +165,17 @@
             print(
                 f"Running inference for {component_name} on a hosted device with example inputs."
             )
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
-            # Convert inputs from channel first to channel last
-            hub_inputs = transpose_channel_first_to_last(
-                "image", sample_inputs, target_runtime
-            )
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
-                inputs=hub_inputs,
+                inputs=sample_inputs,
                 device=hub.Device(device),
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
@@ -219,14 +214,16 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
+    parser = export_parser(
+        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
+    )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_face/perf.yaml

```diff
@@ -15,172 +15,173 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MediaPipeFaceDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 532.0
-      throughput: 1879.6992481203008
+      inference_time: 785.0
+      throughput: 1273.8853503184714
       estimated_peak_memory_range:
         min: 12288
-        max: 1591696
+        max: 1644744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 111
+        layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 111
-      job_id: jn5q8nm57
+        total_layers: 112
+      job_id: j1p3n64z5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:50.277943Z'
+    timestamp: '2024-04-02T15:54:57.036020Z'
     torchscript_onnx_qnn:
-      inference_time: 535.0
-      throughput: 1869.1588785046729
+      inference_time: 836.0
+      throughput: 1196.1722488038276
       estimated_peak_memory_range:
-        min: 16384
-        max: 4401872
+        min: 811008
+        max: 7017760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 111
+        layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 111
-      job_id: jwgoyxd58
+        total_layers: 148
+      job_id: jlpeoyr0g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 380.0
-      throughput: 2631.5789473684213
+      inference_time: 546.0
+      throughput: 1831.5018315018315
       estimated_peak_memory_range:
-        min: 12288
-        max: 27416464
+        min: 16384
+        max: 29604672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 111
+        layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 111
-      job_id: jw566x75o
+        total_layers: 112
+      job_id: j1pvq71mg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:15:59.596663Z'
+    timestamp: '2024-04-02T15:57:38.738298Z'
     torchscript_onnx_qnn:
-      inference_time: 381.0
-      throughput: 2624.6719160104985
+      inference_time: 594.0
+      throughput: 1683.5016835016836
       estimated_peak_memory_range:
         min: 12288
-        max: 26948416
+        max: 43350624
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 111
+        layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 111
-      job_id: j7gjx98pd
+        total_layers: 148
+      job_id: jz5ww4dj5
       job_status: Passed
 - name: MediaPipeFaceLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 211.0
-      throughput: 4739.336492890995
+      inference_time: 309.0
+      throughput: 3236.2459546925566
       estimated_peak_memory_range:
-        min: 24576
-        max: 1810232
+        min: 12288
+        max: 1598880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 100
+        layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 100
-      job_id: j1glndlpv
+        total_layers: 101
+      job_id: jwgoz81dp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:24:30.398348Z'
+    timestamp: '2024-04-02T16:04:19.041124Z'
     torchscript_onnx_qnn:
-      inference_time: 210.0
-      throughput: 4761.9047619047615
+      inference_time: 394.0
+      throughput: 2538.0710659898477
       estimated_peak_memory_range:
-        min: 28672
-        max: 1684984
+        min: 471040
+        max: 59853120
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 100
+        layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 100
-      job_id: j1pv38m5x
+        total_layers: 107
+      job_id: jygz2nx6g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 159.0
-      throughput: 6289.308176100629
+      inference_time: 253.0
+      throughput: 3952.5691699604745
       estimated_peak_memory_range:
         min: 12288
-        max: 24695408
+        max: 25742560
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 100
+        layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 100
-      job_id: j1p3kdz52
+        total_layers: 101
+      job_id: j7gjdq08g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:28:56.546828Z'
+    timestamp: '2024-04-02T16:06:04.371420Z'
     torchscript_onnx_qnn:
-      inference_time: 156.0
-      throughput: 6410.25641025641
+      inference_time: 284.0
+      throughput: 3521.1267605633802
       estimated_peak_memory_range:
-        min: 16384
-        max: 24996560
+        min: 12288
+        max: 37168176
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 100
+        layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 100
-      job_id: jlpe9q0gr
+        total_layers: 107
+      job_id: jmg90d3vg
       job_status: Passed
```

## qai_hub_models/models/mediapipe_hand/export.py

```diff
@@ -23,15 +23,14 @@
 from qai_hub_models.utils.printing import (
     print_inference_metrics,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
-    transpose_channel_first_to_last,
 )
 
 ALL_COMPONENTS = ["MediaPipeHandDetector", "MediaPipeHandLandmarkDetector"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
@@ -123,15 +122,15 @@
         input_spec = component.get_input_spec()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options + " --force_channel_last_input image"
+            target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub.Device(device),
             name=f"{model_name}_{component_name}",
@@ -166,21 +165,17 @@
             print(
                 f"Running inference for {component_name} on a hosted device with example inputs."
             )
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
-            # Convert inputs from channel first to channel last
-            hub_inputs = transpose_channel_first_to_last(
-                "image", sample_inputs, target_runtime
-            )
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
-                inputs=hub_inputs,
+                inputs=sample_inputs,
                 device=hub.Device(device),
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
@@ -219,14 +214,16 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
+    parser = export_parser(
+        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
+    )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_hand/perf.yaml

```diff
@@ -15,172 +15,173 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MediaPipeHandDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 765.0
-      throughput: 1307.18954248366
+      inference_time: 963.0
+      throughput: 1038.4215991692627
       estimated_peak_memory_range:
-        min: 12288
-        max: 12061368
+        min: 24576
+        max: 3475384
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 151
+        layers_on_npu: 152
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 151
-      job_id: jep28dxp6
+        total_layers: 152
+      job_id: jnp126dlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:23:15.414918Z'
+    timestamp: '2024-04-02T15:16:50.064065Z'
     torchscript_onnx_qnn:
-      inference_time: 763.0
-      throughput: 1310.615989515072
+      inference_time: 1013.0
+      throughput: 987.1668311944719
       estimated_peak_memory_range:
-        min: 12288
-        max: 1709784
+        min: 806912
+        max: 21114408
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 151
+        layers_on_npu: 197
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 151
-      job_id: jogkz0ygd
+        total_layers: 197
+      job_id: j0px9xe9p
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 571.0
-      throughput: 1751.3134851138354
+      inference_time: 679.0
+      throughput: 1472.7540500736377
       estimated_peak_memory_range:
-        min: 12288
-        max: 51661744
+        min: 16384
+        max: 52478672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 151
+        layers_on_npu: 152
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 151
-      job_id: j2p0y92gw
+        total_layers: 152
+      job_id: jz5729vrp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:27:44.926097Z'
+    timestamp: '2024-04-02T15:19:31.887081Z'
     torchscript_onnx_qnn:
-      inference_time: 547.0
-      throughput: 1828.1535648994516
+      inference_time: 722.0
+      throughput: 1385.0415512465374
       estimated_peak_memory_range:
-        min: 12288
-        max: 52066480
+        min: 802816
+        max: 55474400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 151
+        layers_on_npu: 197
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 151
-      job_id: j1gln8epv
+        total_layers: 197
+      job_id: jegn0krm5
       job_status: Passed
 - name: MediaPipeHandLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1047.0
-      throughput: 955.1098376313277
+      inference_time: 1204.0
+      throughput: 830.5647840531561
       estimated_peak_memory_range:
-        min: 28672
-        max: 2017000
+        min: 20480
+        max: 2109720
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 158
+        layers_on_npu: 159
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 158
-      job_id: jqpye2rgy
+        total_layers: 159
+      job_id: jvgdn2rl5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:35:00.465711Z'
+    timestamp: '2024-04-02T15:27:36.652846Z'
     torchscript_onnx_qnn:
-      inference_time: 996.0
-      throughput: 1004.0160642570281
+      inference_time: 1286.0
+      throughput: 777.6049766718507
       estimated_peak_memory_range:
-        min: 24576
-        max: 10650592
+        min: 16384
+        max: 10322344
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 158
+        layers_on_npu: 210
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 158
-      job_id: jn5q81757
+        total_layers: 210
+      job_id: jo5me8vqp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 749.0
-      throughput: 1335.1134846461948
+      inference_time: 892.0
+      throughput: 1121.0762331838564
       estimated_peak_memory_range:
-        min: 16384
-        max: 54372320
+        min: 12288
+        max: 56429024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 158
+        layers_on_npu: 159
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 158
-      job_id: j1p8orzg9
+        total_layers: 159
+      job_id: jqp4n3jlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:36:54.085694Z'
+    timestamp: '2024-04-02T15:30:18.255922Z'
     torchscript_onnx_qnn:
-      inference_time: 747.0
-      throughput: 1338.6880856760374
+      inference_time: 967.0
+      throughput: 1034.126163391934
       estimated_peak_memory_range:
-        min: 12288
-        max: 53941536
+        min: 802816
+        max: 63039088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 158
+        layers_on_npu: 210
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 158
-      job_id: jw566mv5o
+        total_layers: 210
+      job_id: jopr6w1ep
       job_status: Passed
```

## qai_hub_models/models/mediapipe_pose/export.py

```diff
@@ -23,15 +23,14 @@
 from qai_hub_models.utils.printing import (
     print_inference_metrics,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
-    transpose_channel_first_to_last,
 )
 
 ALL_COMPONENTS = ["MediaPipePoseDetector", "MediaPipePoseLandmarkDetector"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
@@ -123,15 +122,15 @@
         input_spec = component.get_input_spec()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options + " --force_channel_last_input image"
+            target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub.Device(device),
             name=f"{model_name}_{component_name}",
@@ -166,21 +165,17 @@
             print(
                 f"Running inference for {component_name} on a hosted device with example inputs."
             )
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
-            # Convert inputs from channel first to channel last
-            hub_inputs = transpose_channel_first_to_last(
-                "image", sample_inputs, target_runtime
-            )
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
-                inputs=hub_inputs,
+                inputs=sample_inputs,
                 device=hub.Device(device),
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
@@ -219,14 +214,16 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
+    parser = export_parser(
+        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
+    )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_pose/perf.yaml

```diff
@@ -15,172 +15,173 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MediaPipePoseDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 806.0
-      throughput: 1240.6947890818858
+      inference_time: 832.0
+      throughput: 1201.923076923077
       estimated_peak_memory_range:
-        min: 24576
-        max: 1736000
+        min: 12288
+        max: 4854072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 106
+        layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 106
-      job_id: jygzelzg8
+        total_layers: 107
+      job_id: jep2xe3mg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:54:08.518654Z'
+    timestamp: '2024-04-02T16:08:14.918160Z'
     torchscript_onnx_qnn:
-      inference_time: 808.0
-      throughput: 1237.6237623762377
+      inference_time: 888.0
+      throughput: 1126.126126126126
       estimated_peak_memory_range:
-        min: 28672
-        max: 4909504
+        min: 12288
+        max: 15880848
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 106
+        layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 106
-      job_id: jvgdwdk5j
+        total_layers: 140
+      job_id: jogkv8rop
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 577.0
-      throughput: 1733.102253032929
+      inference_time: 595.0
+      throughput: 1680.672268907563
       estimated_peak_memory_range:
-        min: 65536
-        max: 39641680
+        min: 61440
+        max: 40000256
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 106
+        layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 106
-      job_id: jmg9vzq57
+        total_layers: 107
+      job_id: j2p046eeg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:58:30.195464Z'
+    timestamp: '2024-04-02T16:09:48.929940Z'
     torchscript_onnx_qnn:
-      inference_time: 577.0
-      throughput: 1733.102253032929
+      inference_time: 635.0
+      throughput: 1574.8031496062993
       estimated_peak_memory_range:
-        min: 61440
-        max: 40004608
+        min: 0
+        max: 42314640
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 106
+        layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 106
-      job_id: jqp4qyqgo
+        total_layers: 140
+      job_id: j1gl4lel5
       job_status: Passed
 - name: MediaPipePoseLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1052.0
-      throughput: 950.5703422053232
+      inference_time: 1234.0
+      throughput: 810.3727714748784
       estimated_peak_memory_range:
-        min: 16384
-        max: 2847296
+        min: 24576
+        max: 2060312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 230
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: jz5wolzp1
+        total_layers: 230
+      job_id: jqpyzmv4g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:04:16.645350Z'
+    timestamp: '2024-04-02T16:14:29.934608Z'
     torchscript_onnx_qnn:
-      inference_time: 1063.0
-      throughput: 940.7337723424271
+      inference_time: 1299.0
+      throughput: 769.8229407236336
       estimated_peak_memory_range:
-        min: 12288
-        max: 2768272
+        min: 16384
+        max: 16124312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 306
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: jz57zeqp3
+        total_layers: 306
+      job_id: jn5q0v9mp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 756.0
-      throughput: 1322.7513227513227
+      inference_time: 897.0
+      throughput: 1114.8272017837235
       estimated_peak_memory_range:
-        min: 12288
-        max: 84633232
+        min: 16384
+        max: 87742400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 230
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: jnp10nk5q
+        total_layers: 230
+      job_id: j1p821w8p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:06:07.166564Z'
+    timestamp: '2024-04-02T16:16:04.740529Z'
     torchscript_onnx_qnn:
-      inference_time: 772.0
-      throughput: 1295.3367875647668
+      inference_time: 944.0
+      throughput: 1059.322033898305
       estimated_peak_memory_range:
-        min: 12288
-        max: 84377840
+        min: 815104
+        max: 87180432
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 306
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: j0pxvljg7
+        total_layers: 306
+      job_id: jw562wq7g
       job_status: Passed
```

## qai_hub_models/models/mediapipe_selfie/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_selfie/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MediaPipe-Selfie-Segmentation
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 821.0
-      throughput: 1218.026796589525
+      inference_time: 811.0
+      throughput: 1233.0456226880394
       estimated_peak_memory_range:
         min: 12288
-        max: 2051880
+        max: 1889216
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: j1p3kox52
+      job_id: j1p3n6qz5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:06:02.750038Z'
+    timestamp: '2024-04-02T16:00:20.583232Z'
     torchscript_onnx_qnn:
-      inference_time: 805.0
-      throughput: 1242.2360248447205
+      inference_time: 774.0
+      throughput: 1291.9896640826873
       estimated_peak_memory_range:
-        min: 815104
-        max: 4449664
+        min: 802816
+        max: 90946416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 139
+        layers_on_npu: 138
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 139
-      job_id: j1pv3275x
+        total_layers: 138
+      job_id: j1pvq7zmg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 555.0
-      throughput: 1801.8018018018017
+      inference_time: 554.0
+      throughput: 1805.0541516245487
       estimated_peak_memory_range:
         min: 12288
-        max: 22552848
+        max: 22551568
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: jwgoyd458
+      job_id: jwgoz8edp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:09:04.960914Z'
+    timestamp: '2024-04-02T16:02:48.415603Z'
     torchscript_onnx_qnn:
-      inference_time: 550.0
-      throughput: 1818.1818181818182
+      inference_time: 529.0
+      throughput: 1890.359168241966
       estimated_peak_memory_range:
         min: 176128
-        max: 42597216
+        max: 41833568
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 139
+        layers_on_npu: 138
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 139
-      job_id: jlpe967gr
+        total_layers: 138
+      job_id: j7gjdqk8g
       job_status: Passed
```

## qai_hub_models/models/mnasnet05/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mnasnet05/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MNASNet05
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 383.0
-      throughput: 2610.9660574412533
+      inference_time: 370.0
+      throughput: 2702.7027027027025
       estimated_peak_memory_range:
         min: 20480
-        max: 1718480
+        max: 2386016
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 69
-      job_id: j1p8o1qg9
+      job_id: jlpeoy40g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:31:16.872390Z'
+    timestamp: '2024-04-02T16:00:21.394093Z'
     torchscript_onnx_qnn:
-      inference_time: 358.0
-      throughput: 2793.2960893854747
+      inference_time: 362.0
+      throughput: 2762.4309392265195
       estimated_peak_memory_range:
-        min: 634880
-        max: 4722696
+        min: 12288
+        max: 120863224
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 102
+        layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 102
-      job_id: jn5q8ve57
+        total_layers: 101
+      job_id: jz5ww4mj5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 282.0
-      throughput: 3546.099290780142
+      inference_time: 277.0
+      throughput: 3610.1083032490974
       estimated_peak_memory_range:
-        min: 12288
-        max: 44089552
+        min: 24576
+        max: 44606688
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 69
-      job_id: jogkz8vgd
+      job_id: jygz2nv6g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:34:31.653300Z'
+    timestamp: '2024-04-02T16:02:47.948402Z'
     torchscript_onnx_qnn:
-      inference_time: 260.0
-      throughput: 3846.153846153846
+      inference_time: 258.0
+      throughput: 3875.968992248062
       estimated_peak_memory_range:
-        min: 0
-        max: 33635600
+        min: 618496
+        max: 37367488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 102
+        layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 102
-      job_id: j1glnl2pv
+        total_layers: 101
+      job_id: jmg90d9vg
       job_status: Passed
```

## qai_hub_models/models/mobilenet_v2/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v2/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MobileNet-v2
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 540.0
-      throughput: 1851.851851851852
+      inference_time: 549.0
+      throughput: 1821.4936247723133
       estimated_peak_memory_range:
         min: 12288
-        max: 1921936
+        max: 1985288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: jygzeyzg8
+      job_id: jnp126qlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:53:26.997975Z'
+    timestamp: '2024-04-02T15:43:39.892190Z'
     torchscript_onnx_qnn:
-      inference_time: 808.0
-      throughput: 1237.6237623762377
+      inference_time: 805.0
+      throughput: 1242.2360248447205
       estimated_peak_memory_range:
-        min: 622592
-        max: 6011376
+        min: 12288
+        max: 197702240
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 104
+        layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 104
-      job_id: jmg9v2q57
+        total_layers: 103
+      job_id: jz57296rp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 393.0
-      throughput: 2544.529262086514
+      inference_time: 394.0
+      throughput: 2538.0710659898477
       estimated_peak_memory_range:
         min: 12288
-        max: 55502880
+        max: 56118336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: jz5wozzp1
+      job_id: jvgdn27l5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:57:53.996541Z'
+    timestamp: '2024-04-02T15:46:19.700460Z'
     torchscript_onnx_qnn:
-      inference_time: 537.0
-      throughput: 1862.1973929236499
+      inference_time: 535.0
+      throughput: 1869.1588785046729
       estimated_peak_memory_range:
         min: 618496
-        max: 37101856
+        max: 36988752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 104
+        layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 104
-      job_id: jnp101k5q
+        total_layers: 103
+      job_id: jqp4n38lg
       job_status: Passed
```

## qai_hub_models/models/mobilenet_v2_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v2_quantized/model.py

```diff
@@ -19,15 +19,14 @@
 
 from qai_hub_models.models.mobilenet_v2.model import (
     MobileNetV2,
     _load_mobilenet_v2_source_model,
 )
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 from qai_hub_models.utils.quantization_aimet import convert_all_depthwise_to_per_tensor
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 3
 
 # Weights downloaded from https://github.com/quic/aimet-model-zoo/releases/download/phase_2_january_artifacts/torch_mobilenetv2_w8a8_state_dict.pth
 QUANTIZED_WEIGHTS = "torch_mobilenetv2_w8a8_state_dict.pth"
@@ -43,19 +42,14 @@
     ) -> None:
         MobileNetV2.__init__(self, quant_sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             quant_sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "MobileNetV2Quantizable":
         """
         Parameters:
@@ -100,15 +94,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/mobilenet_v2_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MobileNet-v2-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 237.0
-      throughput: 4219.4092827004215
+      inference_time: 234.0
+      throughput: 4273.504273504273
       estimated_peak_memory_range:
         min: 12288
-        max: 1520264
+        max: 1572504
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: j1p3klz52
+      job_id: j0px9xm9p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:42:55.837359Z'
+    timestamp: '2024-04-02T16:00:56.088303Z'
     torchscript_onnx_qnn:
-      inference_time: 352.0
-      throughput: 2840.909090909091
+      inference_time: 349.0
+      throughput: 2865.3295128939826
       estimated_peak_memory_range:
-        min: 135168
-        max: 94316568
+        min: 167936
+        max: 46798608
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 69
+        layers_on_npu: 68
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 69
-      job_id: j1pv3ym5x
+        total_layers: 68
+      job_id: jegn0kxm5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 168.0
-      throughput: 5952.380952380952
+      inference_time: 196.0
+      throughput: 5102.040816326531
       estimated_peak_memory_range:
         min: 12288
-        max: 35960128
+        max: 36730112
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: jwgoy7d58
+      job_id: jo5me84qp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:47:22.207861Z'
+    timestamp: '2024-04-02T16:03:12.952170Z'
     torchscript_onnx_qnn:
-      inference_time: 253.0
-      throughput: 3952.5691699604745
+      inference_time: 245.0
+      throughput: 4081.6326530612246
       estimated_peak_memory_range:
         min: 163840
-        max: 35983856
+        max: 32368720
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 69
+        layers_on_npu: 68
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 69
-      job_id: j7gjx68pd
+        total_layers: 68
+      job_id: jopr6w9ep
       job_status: Passed
```

## qai_hub_models/models/mobilenet_v3_large/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v3_large/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MobileNet-v3-Large
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 603.0
-      throughput: 1658.374792703151
+      inference_time: 600.0
+      throughput: 1666.6666666666667
       estimated_peak_memory_range:
         min: 12288
-        max: 2319320
+        max: 1649368
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 134
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 134
-      job_id: jnp10025q
+      job_id: jep2xejmg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:10:33.448407Z'
+    timestamp: '2024-04-02T16:02:57.989143Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 433.0
-      throughput: 2309.4688221709007
+      inference_time: 424.0
+      throughput: 2358.490566037736
       estimated_peak_memory_range:
-        min: 12288
-        max: 60000912
+        min: 16384
+        max: 60523168
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 134
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 134
-      job_id: jvgdwwe5j
+      job_id: jqpyzmn4g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:10:33.448414Z'
+    timestamp: '2024-04-02T16:02:57.989157Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/mobilenet_v3_large_quantized/export.py

```diff
@@ -189,14 +189,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v3_large_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.mobilenet_v3_large.model import MobileNetV3Large
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 DEFAULT_ENCODINGS = "mobilenet_v3_large_quantized_encodings.json"
 
 
 class MobileNetV3LargeQuantizable(AIMETQuantizableMixin, MobileNetV3Large):
@@ -39,19 +38,14 @@
     ) -> None:
         MobileNetV3Large.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "MobileNetV3LargeQuantizable":
         """
         Parameters:
```

## qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MobileNet-v3-Large-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2972.0
-      throughput: 336.47375504710635
+      inference_time: 2909.0
+      throughput: 343.7607425232039
       estimated_peak_memory_range:
-        min: 12288
-        max: 3564432
+        min: 1351680
+        max: 5759640
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 136
+        layers_on_npu: 134
         layers_on_gpu: 0
         layers_on_cpu: 15
-        total_layers: 151
-      job_id: j1pv3m75x
+        total_layers: 149
+      job_id: j2p046keg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:39:39.924043Z'
+    timestamp: '2024-04-02T15:35:49.786504Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 2352.0
-      throughput: 425.1700680272109
+      inference_time: 2580.0
+      throughput: 387.5968992248062
       estimated_peak_memory_range:
-        min: 0
-        max: 46180704
+        min: 12288
+        max: 45919040
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 136
+        layers_on_npu: 134
         layers_on_gpu: 0
         layers_on_cpu: 15
-        total_layers: 151
-      job_id: jlpe9x7gr
+        total_layers: 149
+      job_id: j1p82188p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:39:39.924051Z'
+    timestamp: '2024-04-02T15:35:49.786517Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/mobilenet_v3_small/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v3_small/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: MobileNet-v3-Small
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 424.0
-      throughput: 2358.490566037736
+      inference_time: 421.0
+      throughput: 2375.296912114014
       estimated_peak_memory_range:
         min: 36864
-        max: 1921728
+        max: 8536712
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 122
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 122
-      job_id: jlpe900gr
+      job_id: jogkv8dop
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:22:40.354876Z'
+    timestamp: '2024-04-02T15:24:45.951383Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 288.0
-      throughput: 3472.222222222222
+      inference_time: 312.0
+      throughput: 3205.128205128205
       estimated_peak_memory_range:
         min: 12288
-        max: 40067360
+        max: 40933232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 122
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 122
-      job_id: jygzeq6g8
+      job_id: jn5q0vwmp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:22:40.354885Z'
+    timestamp: '2024-04-02T15:24:45.951396Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/openai_clip/export.py

```diff
@@ -23,15 +23,14 @@
 from qai_hub_models.utils.printing import (
     print_inference_metrics,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
-    transpose_channel_first_to_last,
 )
 
 ALL_COMPONENTS = ["CLIPTextEncoder", "CLIPImageEncoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
@@ -123,15 +122,15 @@
         input_spec = component.get_input_spec()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options + " --force_channel_last_input image"
+            target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub.Device(device),
             name=f"{model_name}_{component_name}",
@@ -166,21 +165,17 @@
             print(
                 f"Running inference for {component_name} on a hosted device with example inputs."
             )
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
-            # Convert inputs from channel first to channel last
-            hub_inputs = transpose_channel_first_to_last(
-                "image", sample_inputs, target_runtime
-            )
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
-                inputs=hub_inputs,
+                inputs=sample_inputs,
                 device=hub.Device(device),
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
@@ -219,14 +214,16 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
+    parser = export_parser(
+        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
+    )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/openai_clip/perf.yaml

```diff
@@ -15,172 +15,173 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: CLIPTextEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 15516.0
-      throughput: 64.44960041247744
+      inference_time: 15437.0
+      throughput: 64.77942605428515
       estimated_peak_memory_range:
-        min: 49152
-        max: 3267008
+        min: 16384
+        max: 3773072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 574
         layers_on_gpu: 0
         layers_on_cpu: 2
         total_layers: 576
-      job_id: jz5worjp1
+      job_id: j1gl4l7l5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:47:17.422656Z'
+    timestamp: '2024-04-02T15:38:20.337723Z'
     torchscript_onnx_qnn:
-      inference_time: 15586.0
-      throughput: 64.16014371872193
+      inference_time: 8102.0
+      throughput: 123.42631449024933
       estimated_peak_memory_range:
-        min: 45056
-        max: 2975720
+        min: 32768
+        max: 20779640
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 574
+        layers_on_npu: 377
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 576
-      job_id: jz57z1rp3
+        layers_on_cpu: 0
+        total_layers: 377
+      job_id: j1pvq74mg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 11115.0
-      throughput: 89.9685110211426
+      inference_time: 11180.0
+      throughput: 89.44543828264759
       estimated_peak_memory_range:
         min: 16384
-        max: 204316144
+        max: 221118656
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 574
         layers_on_gpu: 0
         layers_on_cpu: 2
         total_layers: 576
-      job_id: jnp10ml5q
+      job_id: j1p3n68z5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:49:22.781059Z'
+    timestamp: '2024-04-02T15:41:03.797053Z'
     torchscript_onnx_qnn:
-      inference_time: 11246.0
-      throughput: 88.92050506846878
+      inference_time: 5698.0
+      throughput: 175.5001755001755
       estimated_peak_memory_range:
-        min: 40960
-        max: 205502128
+        min: 12288
+        max: 143619840
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 574
+        layers_on_npu: 377
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 576
-      job_id: j0pxv89g7
+        layers_on_cpu: 0
+        total_layers: 377
+      job_id: jlpeoy20g
       job_status: Passed
 - name: CLIPImageEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 128196.0
-      throughput: 7.800555399544447
+      inference_time: 126791.0
+      throughput: 7.886995133724002
       estimated_peak_memory_range:
-        min: 143360
-        max: 3847064
+        min: 163840
+        max: 4397144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 575
+        layers_on_npu: 576
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 575
-      job_id: jmg9vqv57
+        total_layers: 576
+      job_id: jw562wv7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:59:18.769511Z'
+    timestamp: '2024-04-02T15:49:16.432598Z'
     torchscript_onnx_qnn:
-      inference_time: 127795.0
-      throughput: 7.825032278258147
+      inference_time: 50465.0
+      throughput: 19.815713861091847
       estimated_peak_memory_range:
-        min: 180224
-        max: 4074336
+        min: 57344
+        max: 62046320
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 575
+        layers_on_npu: 371
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 575
-      job_id: jqp4q6lgo
+        total_layers: 371
+      job_id: j7gjdq18g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 98556.0
-      throughput: 10.14651568651325
+      inference_time: 96475.0
+      throughput: 10.365379632029024
       estimated_peak_memory_range:
-        min: 163840
-        max: 781391856
+        min: 266240
+        max: 867371232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 575
+        layers_on_npu: 576
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 575
-      job_id: jvgdwml5j
+        total_layers: 576
+      job_id: jwgoz8mdp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:01:23.890974Z'
+    timestamp: '2024-04-02T15:52:00.458152Z'
     torchscript_onnx_qnn:
-      inference_time: 97281.0
-      throughput: 10.279499593959766
+      inference_time: 38292.0
+      throughput: 26.115115428810196
       estimated_peak_memory_range:
-        min: 237568
-        max: 783870384
+        min: 643072
+        max: 228425888
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 575
+        layers_on_npu: 371
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 575
-      job_id: jo5mr1qgk
+        total_layers: 371
+      job_id: jygz2nw6g
       job_status: Passed
```

## qai_hub_models/models/openpose/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/openpose/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: OpenPose
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 11718.0
-      throughput: 85.33879501621438
+      inference_time: 11734.0
+      throughput: 85.22243054371911
       estimated_peak_memory_range:
-        min: 229376
-        max: 2888976
+        min: 237568
+        max: 2523304
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: j1pvokj5x
+      job_id: jw562wm6g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:20:33.773079Z'
+    timestamp: '2024-04-02T15:49:44.284847Z'
     torchscript_onnx_qnn:
-      inference_time: 11832.0
-      throughput: 84.51656524678837
+      inference_time: 11894.0
+      throughput: 84.07600470825626
       estimated_peak_memory_range:
-        min: 643072
-        max: 242325320
+        min: 618496
+        max: 231521112
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 187
+        layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 187
-      job_id: jlpe1m15r
+        total_layers: 186
+      job_id: jwgoz8wqp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 8755.0
-      throughput: 114.22044545973729
+      inference_time: 8768.0
+      throughput: 114.05109489051095
       estimated_peak_memory_range:
-        min: 192512
-        max: 33307600
+        min: 217088
+        max: 33805040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: j7gjmnxgd
+      job_id: j1p3n6735
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:22:29.557459Z'
+    timestamp: '2024-04-02T15:52:25.633149Z'
     torchscript_onnx_qnn:
-      inference_time: 8772.0
-      throughput: 113.99908800729594
+      inference_time: 8773.0
+      throughput: 113.98609369656901
       estimated_peak_memory_range:
         min: 618496
-        max: 53437584
+        max: 53251968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 187
+        layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 187
-      job_id: jygz9dk58
+        total_layers: 186
+      job_id: j1pvq7nkg
       job_status: Passed
```

## qai_hub_models/models/quicksrnetlarge/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetlarge/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: QuickSRNetLarge
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2500.0
-      throughput: 400.0
+      inference_time: 2479.0
+      throughput: 403.3884630899556
       estimated_peak_memory_range:
         min: 16384
-        max: 1492864
+        max: 2082976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: jn5q8l757
+      job_id: jz5ww4xj5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:06:24.010143Z'
+    timestamp: '2024-04-02T15:38:35.151948Z'
     torchscript_onnx_qnn:
-      inference_time: 2109.0
-      throughput: 474.158368895211
+      inference_time: 2103.0
+      throughput: 475.51117451260103
       estimated_peak_memory_range:
-        min: 16384
-        max: 5120280
+        min: 212992
+        max: 70526200
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 32
+        layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 32
-      job_id: jw5668v5o
+        total_layers: 31
+      job_id: jnp1263lg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1780.0
-      throughput: 561.7977528089888
+      inference_time: 1724.0
+      throughput: 580.046403712297
       estimated_peak_memory_range:
-        min: 20480
-        max: 27633264
+        min: 16384
+        max: 28078224
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: j1glnyepv
+      job_id: jmg90d8vg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:10:42.992618Z'
+    timestamp: '2024-04-02T15:41:14.086310Z'
     torchscript_onnx_qnn:
-      inference_time: 1506.0
-      throughput: 664.0106241699867
+      inference_time: 1489.0
+      throughput: 671.591672263264
       estimated_peak_memory_range:
-        min: 208896
-        max: 18546960
+        min: 204800
+        max: 18038672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 32
+        layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 32
-      job_id: j1p3kzx52
+        total_layers: 31
+      job_id: jvgdn20l5
       job_status: Passed
```

## qai_hub_models/models/quicksrnetlarge_quantized/export.py

```diff
@@ -202,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetlarge_quantized/model.py

```diff
@@ -12,14 +12,15 @@
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
+from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.quicksrnetlarge.model import QuickSRNetLarge
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config_legacy_v2
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 
@@ -93,7 +94,15 @@
                     MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        if target_runtime == TargetRuntime.QNN:
+            return SourceModelFormat.ONNX
+        else:
+            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: QuickSRNetLarge-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1868.0
-      throughput: 535.3319057815846
+      inference_time: 1505.0
+      throughput: 664.4518272425249
       estimated_peak_memory_range:
-        min: 12288
-        max: 1533296
+        min: 20480
+        max: 1674904
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 30
+        layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 33
-      job_id: jygze66g8
+        total_layers: 31
+      job_id: jz5ww4865
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:21:00.166706Z'
+    timestamp: '2024-04-02T15:46:59.414680Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 1484.0
-      throughput: 673.8544474393531
+      inference_time: 1194.0
+      throughput: 837.5209380234506
       estimated_peak_memory_range:
-        min: 20480
-        max: 25007104
+        min: 12288
+        max: 25612592
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 30
+        layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 33
-      job_id: jz5wokjp1
+        total_layers: 31
+      job_id: jmg90dklg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:21:00.166728Z'
+    timestamp: '2024-04-02T15:46:59.414694Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/quicksrnetmedium/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetmedium/perf.yaml

```diff
@@ -1,85 +1,85 @@
 models:
 - name: QuickSRNetMedium
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1398.0
-      throughput: 715.307582260372
+      inference_time: 1386.0
+      throughput: 721.5007215007215
       estimated_peak_memory_range:
-        min: 16384
-        max: 8236496
+        min: 24576
+        max: 8284584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jwgoy9d58
+      job_id: jnp12672g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:36.328807Z'
+    timestamp: '2024-04-02T15:43:54.390470Z'
     torchscript_onnx_qnn:
-      inference_time: 989.0
-      throughput: 1011.1223458038422
+      inference_time: 998.0
+      throughput: 1002.0040080160321
       estimated_peak_memory_range:
         min: 212992
-        max: 7267624
+        max: 7518912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 18
+        layers_on_npu: 17
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 18
-      job_id: j7gjx88pd
+        total_layers: 17
+      job_id: jz5729klp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 935.0
-      throughput: 1069.51871657754
+      inference_time: 899.0
+      throughput: 1112.3470522803113
       estimated_peak_memory_range:
         min: 16384
-        max: 19630352
+        max: 19609168
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: j1pv3nm5x
+      job_id: jvgdn28e5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:15:44.251341Z'
+    timestamp: '2024-04-02T15:46:31.914821Z'
     torchscript_onnx_qnn:
-      inference_time: 648.0
-      throughput: 1543.20987654321
+      inference_time: 651.0
+      throughput: 1536.0983102918588
       estimated_peak_memory_range:
         min: 208896
-        max: 14213744
+        max: 15417120
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 18
+        layers_on_npu: 17
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 18
-      job_id: jlpe9n0gr
+        total_layers: 17
+      job_id: jqp4n3mvg
       job_status: Passed
 aggregated:
   supported_oses:
   - Android
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
@@ -94,14 +94,15 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
```

## qai_hub_models/models/quicksrnetmedium_quantized/export.py

```diff
@@ -202,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetmedium_quantized/model.py

```diff
@@ -12,14 +12,15 @@
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
+from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.quicksrnetmedium.model import QuickSRNetMedium
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config_legacy_v2
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 
@@ -92,7 +93,15 @@
                     MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        if target_runtime == TargetRuntime.QNN:
+            return SourceModelFormat.ONNX
+        else:
+            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml

```diff
@@ -1,33 +1,33 @@
 models:
 - name: QuickSRNetMedium-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1411.0
-      throughput: 708.7172218284904
+      inference_time: 1054.0
+      throughput: 948.7666034155598
       estimated_peak_memory_range:
-        min: 28672
-        max: 1545320
+        min: 12288
+        max: 1550760
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: joprkj950
+        total_layers: 17
+      job_id: j0px9x31p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:24:06.170051Z'
+    timestamp: '2024-04-02T15:22:27.279987Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -36,36 +36,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 1149.0
-      throughput: 870.3220191470845
+      inference_time: 854.0
+      throughput: 1170.96018735363
       estimated_peak_memory_range:
-        min: 20480
-        max: 20002352
+        min: 12288
+        max: 19670544
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: jep28n4p6
+        total_layers: 17
+      job_id: jo5me8owp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:24:06.170059Z'
+    timestamp: '2024-04-02T15:22:27.280002Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -94,14 +94,15 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
```

## qai_hub_models/models/quicksrnetsmall/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetsmall/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: QuickSRNetSmall
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1338.0
-      throughput: 747.3841554559043
+      inference_time: 1324.0
+      throughput: 755.2870090634441
       estimated_peak_memory_range:
-        min: 24576
-        max: 1376064
+        min: 16384
+        max: 15227496
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: jygzezzg8
+      job_id: jegn0kor5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:09:24.184304Z'
+    timestamp: '2024-04-02T15:38:53.058069Z'
     torchscript_onnx_qnn:
-      inference_time: 1025.0
-      throughput: 975.609756097561
+      inference_time: 1010.0
+      throughput: 990.0990099009902
       estimated_peak_memory_range:
-        min: 212992
-        max: 37245776
+        min: 225280
+        max: 8292184
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 12
+        layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 12
-      job_id: jnp10ok5q
+        total_layers: 11
+      job_id: jep2xe44g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 839.0
-      throughput: 1191.8951132300358
+      inference_time: 939.0
+      throughput: 1064.9627263045793
       estimated_peak_memory_range:
         min: 16384
-        max: 17771072
+        max: 18004576
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: jmg9voq57
+      job_id: jopr6wo9p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:13:39.690790Z'
+    timestamp: '2024-04-02T15:41:32.292172Z'
     torchscript_onnx_qnn:
-      inference_time: 616.0
-      throughput: 1623.3766233766235
+      inference_time: 621.0
+      throughput: 1610.3059581320451
       estimated_peak_memory_range:
-        min: 212992
-        max: 14001568
+        min: 229376
+        max: 14179680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 12
+        layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 12
-      job_id: jz57zoqp3
+        total_layers: 11
+      job_id: jqpyzmq7g
       job_status: Passed
```

## qai_hub_models/models/quicksrnetsmall_quantized/export.py

```diff
@@ -202,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetsmall_quantized/model.py

```diff
@@ -12,14 +12,15 @@
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
+from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.quicksrnetsmall.model import QuickSRNetSmall
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config_legacy_v2
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 
@@ -91,7 +92,15 @@
                     MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        if target_runtime == TargetRuntime.QNN:
+            return SourceModelFormat.ONNX
+        else:
+            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml

```diff
@@ -1,33 +1,33 @@
 models:
 - name: QuickSRNetSmall-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1355.0
-      throughput: 738.0073800738007
+      inference_time: 992.0
+      throughput: 1008.0645161290323
       estimated_peak_memory_range:
         min: 20480
-        max: 2224928
+        max: 3845112
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 13
-      job_id: jz57zknp3
+        total_layers: 11
+      job_id: j2p046d6g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:22:40.346377Z'
+    timestamp: '2024-04-02T15:30:25.941729Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -36,36 +36,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 1099.0
-      throughput: 909.9181073703367
+      inference_time: 806.0
+      throughput: 1240.6947890818858
       estimated_peak_memory_range:
-        min: 20480
-        max: 20205264
+        min: 12288
+        max: 18429184
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 13
-      job_id: jqp4qm2go
+        total_layers: 11
+      job_id: j1p8216xp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:22:40.346384Z'
+    timestamp: '2024-04-02T15:30:25.941742Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -94,14 +94,15 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
```

## qai_hub_models/models/real_esrgan_general_x4v3/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Real-ESRGAN-General-x4v3
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7285.0
-      throughput: 137.26835964310226
+      inference_time: 7135.0
+      throughput: 140.1541695865452
       estimated_peak_memory_range:
-        min: 15745024
-        max: 20241416
+        min: 15785984
+        max: 26631064
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 72
-      job_id: j1glno2pv
+      job_id: jn5q0vz4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:36.720476Z'
+    timestamp: '2024-04-02T15:40:09.101637Z'
     torchscript_onnx_qnn:
-      inference_time: 6983.0
-      throughput: 143.20492624946297
+      inference_time: 7001.0
+      throughput: 142.836737608913
       estimated_peak_memory_range:
-        min: 12288
-        max: 10852600
+        min: 57344
+        max: 12072040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 73
+        layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 73
-      job_id: j1p3kxm52
+        total_layers: 72
+      job_id: jw562wr0g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 5660.0
-      throughput: 176.67844522968198
+      inference_time: 5501.0
+      throughput: 181.78512997636793
       estimated_peak_memory_range:
-        min: 57344
-        max: 53042192
+        min: 20480
+        max: 55158288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 72
-      job_id: jw566rn5o
+      job_id: j1gl4lo85
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:17:08.081378Z'
+    timestamp: '2024-04-02T15:42:50.185115Z'
     torchscript_onnx_qnn:
-      inference_time: 4939.0
-      throughput: 202.47013565499088
+      inference_time: 4936.0
+      throughput: 202.5931928687196
       estimated_peak_memory_range:
-        min: 208896
-        max: 32676160
+        min: 12288
+        max: 31063632
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 73
+        layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 73
-      job_id: jwgoyo158
+        total_layers: 72
+      job_id: j1p3n6xl5
       job_status: Passed
```

## qai_hub_models/models/real_esrgan_x4plus/export.py

```diff
@@ -180,14 +180,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/real_esrgan_x4plus/perf.yaml

```diff
@@ -15,14 +15,15 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
@@ -48,29 +49,29 @@
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:52.767646Z'
+    timestamp: '2024-04-02T16:05:08.717805Z'
     torchscript_onnx_qnn:
-      inference_time: 66635.0
-      throughput: 15.007128385983343
+      inference_time: 66817.0
+      throughput: 14.966251103761019
       estimated_peak_memory_range:
-        min: 94208
-        max: 104137800
+        min: 139264
+        max: 108335072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1031
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1031
-      job_id: jz57zzlp3
+      job_id: j1pvq7ejg
       job_status: Passed
   - torchscript_onnx_tflite:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
@@ -86,23 +87,23 @@
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:16:11.670851Z'
+    timestamp: '2024-04-02T16:06:57.492677Z'
     torchscript_onnx_qnn:
-      inference_time: 50978.0
-      throughput: 19.61630507277649
+      inference_time: 51292.0
+      throughput: 19.49621773375965
       estimated_peak_memory_range:
-        min: 90112
-        max: 248878432
+        min: 73728
+        max: 258670240
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1031
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1031
-      job_id: jqp4qqvgo
+      job_id: jygz2n8kg
       job_status: Passed
```

## qai_hub_models/models/regnet/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/regnet/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: RegNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1974.0
-      throughput: 506.5856129685917
+      inference_time: 1895.0
+      throughput: 527.7044854881267
       estimated_peak_memory_range:
-        min: 32768
-        max: 1789416
+        min: 180224
+        max: 45308848
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 112
-      job_id: jqpyey4gy
+      job_id: jmg90dxlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:08:16.252038Z'
+    timestamp: '2024-04-02T15:33:39.883573Z'
     torchscript_onnx_qnn:
-      inference_time: 1675.0
-      throughput: 597.0149253731344
+      inference_time: 1662.0
+      throughput: 601.6847172081829
       estimated_peak_memory_range:
-        min: 241664
-        max: 59486296
+        min: 622592
+        max: 60403520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 187
+        layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 187
-      job_id: j1p8ok8g9
+        total_layers: 186
+      job_id: jvgdn2ze5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1359.0
-      throughput: 735.8351729212657
+      inference_time: 1348.0
+      throughput: 741.839762611276
       estimated_peak_memory_range:
-        min: 16384
-        max: 131931280
+        min: 12288
+        max: 134684864
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 112
-      job_id: j2p0yxegw
+      job_id: jnp126v2g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:11:23.127753Z'
+    timestamp: '2024-04-02T15:36:21.856849Z'
     torchscript_onnx_qnn:
-      inference_time: 1197.0
-      throughput: 835.421888053467
+      inference_time: 1192.0
+      throughput: 838.9261744966443
       estimated_peak_memory_range:
         min: 618496
-        max: 68520544
+        max: 71874880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 187
+        layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 187
-      job_id: jogkzkogd
+        total_layers: 186
+      job_id: j0px9xd1p
       job_status: Passed
```

## qai_hub_models/models/resnet101/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet101/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNet101
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2993.0
-      throughput: 334.1129301703976
+      inference_time: 2981.0
+      throughput: 335.4579000335458
       estimated_peak_memory_range:
-        min: 28672
-        max: 1903408
+        min: 20480
+        max: 2256968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 145
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 145
-      job_id: j7gjxmxpd
+      job_id: j2p046v6g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:51:59.720577Z'
+    timestamp: '2024-04-02T15:33:00.839758Z'
     torchscript_onnx_qnn:
-      inference_time: 2921.0
-      throughput: 342.3485107839781
+      inference_time: 2909.0
+      throughput: 343.7607425232039
       estimated_peak_memory_range:
-        min: 622592
-        max: 226849752
+        min: 626688
+        max: 228487056
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 244
+        layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 244
-      job_id: jygze9kg8
+        total_layers: 243
+      job_id: jogkv892p
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 2221.0
-      throughput: 450.24763619990995
+      inference_time: 2205.0
+      throughput: 453.51473922902494
       estimated_peak_memory_range:
-        min: 16384
-        max: 103000720
+        min: 12288
+        max: 105917360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 145
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 145
-      job_id: jlpe911gr
+      job_id: j1p8214xp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:54:07.329383Z'
+    timestamp: '2024-04-02T15:35:41.997410Z'
     torchscript_onnx_qnn:
-      inference_time: 2126.0
-      throughput: 470.36688617121354
+      inference_time: 2129.0
+      throughput: 469.7040864255519
       estimated_peak_memory_range:
         min: 618496
-        max: 71779728
+        max: 73551600
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 244
+        layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 244
-      job_id: jz5won6p1
+        total_layers: 243
+      job_id: jn5q0vm4p
       job_status: Passed
```

## qai_hub_models/models/resnet101_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet101_quantized/model.py

```diff
@@ -19,15 +19,14 @@
 )
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.resnet101.model import ResNet101
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 4
 DEFAULT_ENCODINGS = "resnet101_quantized_encodings.json"
 
 
 class ResNet101Quantizable(
@@ -45,19 +44,14 @@
     ) -> None:
         ResNet101.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "ResNet101Quantizable":
         """
         Parameters:
@@ -87,15 +81,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/resnet101_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNet101Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1122.0
-      throughput: 891.2655971479501
+      inference_time: 1103.0
+      throughput: 906.6183136899365
       estimated_peak_memory_range:
-        min: 12288
-        max: 2141424
+        min: 40960
+        max: 1823040
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 146
-      job_id: jvgdw7z5j
+      job_id: j1gl4l185
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:05:31.212967Z'
+    timestamp: '2024-04-02T15:59:57.701225Z'
     torchscript_onnx_qnn:
-      inference_time: 1101.0
-      throughput: 908.2652134423251
+      inference_time: 1097.0
+      throughput: 911.5770282588878
       estimated_peak_memory_range:
-        min: 12288
-        max: 196790880
+        min: 20480
+        max: 197523472
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 144
+        layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 144
-      job_id: jvgdw765j
+        total_layers: 143
+      job_id: jwgoz84xp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 839.0
-      throughput: 1191.8951132300358
+      inference_time: 863.0
+      throughput: 1158.7485515643104
       estimated_peak_memory_range:
-        min: 12288
-        max: 91234848
+        min: 20480
+        max: 92174016
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 146
-      job_id: jmg9v9m57
+      job_id: jw562wd0g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:07:20.101134Z'
+    timestamp: '2024-04-02T16:02:25.700326Z'
     torchscript_onnx_qnn:
-      inference_time: 830.0
-      throughput: 1204.8192771084337
+      inference_time: 817.0
+      throughput: 1223.9902080783354
       estimated_peak_memory_range:
         min: 167936
-        max: 53969312
+        max: 53912720
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 144
+        layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 144
-      job_id: jo5mrv7gk
+        total_layers: 143
+      job_id: j1pvq79jg
       job_status: Passed
```

## qai_hub_models/models/resnet18/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet18/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNet18
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1053.0
-      throughput: 949.667616334283
+      inference_time: 1038.0
+      throughput: 963.3911368015414
       estimated_peak_memory_range:
-        min: 32768
-        max: 2028832
+        min: 16384
+        max: 1682008
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 36
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 36
-      job_id: j2p0y8egw
+      job_id: j7gjdqwxg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:41:20.341762Z'
+    timestamp: '2024-04-02T15:32:51.123052Z'
     torchscript_onnx_qnn:
-      inference_time: 989.0
-      throughput: 1011.1223458038422
+      inference_time: 985.0
+      throughput: 1015.2284263959391
       estimated_peak_memory_range:
-        min: 12288
-        max: 84688848
+        min: 16384
+        max: 95517680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 52
+        layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 52
-      job_id: jogkzwogd
+        total_layers: 51
+      job_id: jygz2n4kg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 771.0
-      throughput: 1297.0168612191958
+      inference_time: 772.0
+      throughput: 1295.3367875647668
       estimated_peak_memory_range:
         min: 12288
-        max: 23627952
+        max: 23648144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 36
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 36
-      job_id: j1p8od8g9
+      job_id: jlpeoyl1g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:45:48.504221Z'
+    timestamp: '2024-04-02T15:35:30.038080Z'
     torchscript_onnx_qnn:
-      inference_time: 717.0
-      throughput: 1394.700139470014
+      inference_time: 716.0
+      throughput: 1396.6480446927374
       estimated_peak_memory_range:
-        min: 630784
-        max: 25268288
+        min: 622592
+        max: 26671488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 52
+        layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 52
-      job_id: jn5q8xm57
+        total_layers: 51
+      job_id: jz5ww4465
       job_status: Passed
```

## qai_hub_models/models/resnet18_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet18_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.resnet18.model import ResNet18
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 7
 DEFAULT_ENCODINGS = "resnet18_quantized_encodings.json"
 
 
 class ResNet18Quantizable(AIMETQuantizableMixin, ResNet18):
@@ -39,19 +38,14 @@
     ) -> None:
         ResNet18.__init__(self, resnet18_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             resnet18_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "ResNet18Quantizable":
         """
         Parameters:
@@ -79,15 +73,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/resnet18_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNet18Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 356.0
-      throughput: 2808.9887640449438
+      inference_time: 355.0
+      throughput: 2816.9014084507044
       estimated_peak_memory_range:
         min: 12288
-        max: 1529808
+        max: 1475416
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 37
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 37
-      job_id: j1p3k8m52
+      job_id: jmg90ddlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:59:49.885782Z'
+    timestamp: '2024-04-02T15:27:39.960726Z'
     torchscript_onnx_qnn:
-      inference_time: 354.0
-      throughput: 2824.858757062147
+      inference_time: 368.0
+      throughput: 2717.391304347826
       estimated_peak_memory_range:
-        min: 20480
-        max: 62738248
+        min: 0
+        max: 206530616
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 35
+        layers_on_npu: 34
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 35
-      job_id: j1pv34z5x
+        total_layers: 34
+      job_id: jvgdn22e5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 301.0
-      throughput: 3322.2591362126245
+      inference_time: 294.0
+      throughput: 3401.360544217687
       estimated_peak_memory_range:
         min: 12288
-        max: 23414560
+        max: 23142320
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 37
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 37
-      job_id: jwgoym158
+      job_id: jnp12662g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:03:01.789875Z'
+    timestamp: '2024-04-02T15:30:23.722899Z'
     torchscript_onnx_qnn:
-      inference_time: 282.0
-      throughput: 3546.099290780142
+      inference_time: 287.0
+      throughput: 3484.320557491289
       estimated_peak_memory_range:
         min: 12288
-        max: 21538672
+        max: 22112096
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 35
+        layers_on_npu: 34
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 35
-      job_id: j7gjx11pd
+        total_layers: 34
+      job_id: jz57299lp
       job_status: Passed
```

## qai_hub_models/models/resnet50/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet50/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1898.0
-      throughput: 526.8703898840885
+      inference_time: 1893.0
+      throughput: 528.2620179609086
       estimated_peak_memory_range:
-        min: 36864
-        max: 2234848
+        min: 24576
+        max: 2236144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 77
-      job_id: j2p0yk0gw
+      job_id: jqp4n33vg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:27:02.086108Z'
+    timestamp: '2024-04-02T16:00:02.772259Z'
     torchscript_onnx_qnn:
-      inference_time: 1790.0
-      throughput: 558.659217877095
+      inference_time: 1782.0
+      throughput: 561.1672278338945
       estimated_peak_memory_range:
-        min: 626688
-        max: 186659664
+        min: 618496
+        max: 186769968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jogkzdvgd
+        total_layers: 124
+      job_id: jo5me88wp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1392.0
-      throughput: 718.3908045977012
+      inference_time: 1410.0
+      throughput: 709.2198581560284
       estimated_peak_memory_range:
-        min: 16384
-        max: 68731008
+        min: 0
+        max: 68342016
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 77
-      job_id: j1p8o8qg9
+      job_id: j0px9xx1p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:28:58.275338Z'
+    timestamp: '2024-04-02T16:02:27.391913Z'
     torchscript_onnx_qnn:
-      inference_time: 1307.0
-      throughput: 765.1109410864575
+      inference_time: 1303.0
+      throughput: 767.4597083653108
       estimated_peak_memory_range:
-        min: 0
-        max: 45987408
+        min: 618496
+        max: 49585360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jn5q8we57
+        total_layers: 124
+      job_id: jegn0kkr5
       job_status: Passed
```

## qai_hub_models/models/resnext101/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext101/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNeXt101
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6315.0
-      throughput: 158.3531274742676
+      inference_time: 6465.0
+      throughput: 154.67904098994586
       estimated_peak_memory_range:
-        min: 28672
-        max: 2570472
+        min: 32768
+        max: 2912504
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 145
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 145
-      job_id: j2p0yrngw
+      job_id: j1pvq77jg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:40:16.043830Z'
+    timestamp: '2024-04-02T15:51:13.215900Z'
     torchscript_onnx_qnn:
-      inference_time: 6079.0
-      throughput: 164.50074025333114
+      inference_time: 6084.0
+      throughput: 164.3655489809336
       estimated_peak_memory_range:
         min: 16384
-        max: 34444952
+        max: 36270640
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 244
+        layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 244
-      job_id: jogkzyngd
+        total_layers: 243
+      job_id: jlpeoyy1g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 4552.0
-      throughput: 219.6836555360281
+      inference_time: 4520.0
+      throughput: 221.23893805309734
       estimated_peak_memory_range:
         min: 20480
-        max: 357156576
+        max: 364641136
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 145
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 145
-      job_id: j1p8o7og9
+      job_id: j7gjdqqxg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:42:21.825443Z'
+    timestamp: '2024-04-02T15:53:55.219027Z'
     torchscript_onnx_qnn:
-      inference_time: 4377.0
-      throughput: 228.4669865204478
+      inference_time: 4424.0
+      throughput: 226.03978300180833
       estimated_peak_memory_range:
         min: 618496
-        max: 123852368
+        max: 130132064
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 244
+        layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 244
-      job_id: jn5q82o57
+        total_layers: 243
+      job_id: jygz2nnkg
       job_status: Passed
```

## qai_hub_models/models/resnext101_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext101_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.resnext101.model import ResNeXt101
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 4
 DEFAULT_ENCODINGS = "resnext101_quantized_encodings.json"
 
 
 class ResNeXt101Quantizable(AIMETQuantizableMixin, ResNeXt101):
@@ -39,19 +38,14 @@
     ) -> None:
         ResNeXt101.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "ResNeXt101Quantizable":
         """
         Parameters:
@@ -79,15 +73,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/resnext101_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNeXt101Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2842.0
-      throughput: 351.8648838845883
+      inference_time: 2844.0
+      throughput: 351.6174402250352
       estimated_peak_memory_range:
-        min: 16384
-        max: 1739432
+        min: 0
+        max: 2204768
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 146
-      job_id: jygzekkg8
+      job_id: jz5ww4765
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:42:18.013006Z'
+    timestamp: '2024-04-02T15:41:28.974979Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 2088.0
-      throughput: 478.9272030651341
+      inference_time: 2070.0
+      throughput: 483.09178743961354
       estimated_peak_memory_range:
-        min: 36864
-        max: 251955536
+        min: 12288
+        max: 261887168
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 146
-      job_id: jnp1lz25q
+      job_id: jmg90dmlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:42:18.013015Z'
+    timestamp: '2024-04-02T15:41:28.974992Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/resnext50/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext50/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNeXt50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2118.0
-      throughput: 472.14353163361665
+      inference_time: 2186.0
+      throughput: 457.45654162854527
       estimated_peak_memory_range:
-        min: 16384
-        max: 2846256
+        min: 20480
+        max: 2564264
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 77
-      job_id: j7gjxq1pd
+      job_id: jnp126j2g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:21:42.740361Z'
+    timestamp: '2024-04-02T16:00:40.598047Z'
     torchscript_onnx_qnn:
-      inference_time: 2081.0
-      throughput: 480.5382027871216
+      inference_time: 2074.0
+      throughput: 482.1600771456123
       estimated_peak_memory_range:
-        min: 12288
-        max: 67945728
+        min: 622592
+        max: 68996704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jygzen4g8
+        total_layers: 124
+      job_id: jz57294lp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1551.0
-      throughput: 644.7453255963894
+      inference_time: 1561.0
+      throughput: 640.6149903907751
       estimated_peak_memory_range:
-        min: 16384
-        max: 161276560
+        min: 12288
+        max: 164341728
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 77
-      job_id: jlpe9y8gr
+      job_id: jvgdn23e5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:23:42.732818Z'
+    timestamp: '2024-04-02T16:03:00.522367Z'
     torchscript_onnx_qnn:
       inference_time: 1518.0
       throughput: 658.7615283267457
       estimated_peak_memory_range:
         min: 618496
-        max: 57881488
+        max: 60133216
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jz5wo44p1
+        total_layers: 124
+      job_id: jqp4n31vg
       job_status: Passed
```

## qai_hub_models/models/resnext50_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext50_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.resnext50.model import ResNeXt50
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 DEFAULT_ENCODINGS = "resnext50_quantized_encodings.json"
 
 
 class ResNeXt50Quantizable(AIMETQuantizableMixin, ResNeXt50):
@@ -39,19 +38,14 @@
     ) -> None:
         ResNeXt50.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "ResNeXt50Quantizable":
         """
         Parameters:
@@ -79,15 +73,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/resnext50_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: ResNeXt50Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 874.0
-      throughput: 1144.1647597254005
+      inference_time: 879.0
+      throughput: 1137.6564277588168
       estimated_peak_memory_range:
         min: 12288
-        max: 1920376
+        max: 1573712
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 78
-      job_id: jegn27jgo
+      job_id: j0px9x41p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:10:31.822073Z'
+    timestamp: '2024-04-02T15:13:52.082055Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 656.0
-      throughput: 1524.3902439024391
+      inference_time: 683.0
+      throughput: 1464.1288433382138
       estimated_peak_memory_range:
         min: 12288
-        max: 96222112
+        max: 98876096
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 78
-      job_id: joprknk50
+      job_id: jo5me8mwp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:10:31.822087Z'
+    timestamp: '2024-04-02T15:13:52.082068Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/sam/export.py

```diff
@@ -226,15 +226,18 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_qnn=False
+        model_cls=Model,
+        components=ALL_COMPONENTS,
+        supports_qnn=False,
+        supports_ort=False,
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sam/model.py

```diff
@@ -113,15 +113,15 @@
                    3-channel Color Space: RGB
 
         Returns:
             image_embeddings
         """
         return self.sam.image_encoder(image)
 
-    def _get_input_spec_for_model_instance(
+    def _get_input_spec_for_instance(
         self,
         batch_size: int = 1,
         num_channels: int = 3,
     ) -> InputSpec:
         """
         Override for model.get_input_spec() when called on instances of this class.
 
@@ -223,15 +223,15 @@
         Where,
             k = number of points
         """
         return self.sam_decoder(
             image_embeddings, point_coords, point_labels, mask_input, has_mask_input
         )
 
-    def _get_input_spec_for_model_instance(
+    def _get_input_spec_for_instance(
         self,
         num_of_points: int = 1,
     ) -> InputSpec:
         """
         Override for model.get_input_spec() when called on instances of this class.
 
         The initializer for BaseModel will automatically override get_input_spec
```

## qai_hub_models/models/sam/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: SAMDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 16761.0
-      throughput: 59.66231131794046
+      inference_time: 57777.0
+      throughput: 17.30792529899441
       estimated_peak_memory_range:
-        min: 42115072
-        max: 92806968
-      primary_compute_unit: GPU
+        min: 5091328
+        max: 14286200
+      primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 356
-        layers_on_cpu: 9
+        layers_on_npu: 364
+        layers_on_gpu: 1
+        layers_on_cpu: 0
         total_layers: 365
-      job_id: jmg9vkm57
+      job_id: jegn0knr5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:10:36.748428Z'
+    timestamp: '2024-04-02T15:41:53.968079Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 13794.0
-      throughput: 72.4952878062926
+      inference_time: 39989.0
+      throughput: 25.006876891145065
       estimated_peak_memory_range:
-        min: 41951232
-        max: 94062064
-      primary_compute_unit: GPU
+        min: 16384
+        max: 209265456
+      primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 356
-        layers_on_cpu: 9
+        layers_on_npu: 364
+        layers_on_gpu: 1
+        layers_on_cpu: 0
         total_layers: 365
-      job_id: jnp107n5q
+      job_id: jopr6w09p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:10:36.748439Z'
+    timestamp: '2024-04-02T15:41:53.968093Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/sesr_m5/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sesr_m5/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: SESR-M5
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2245.0
-      throughput: 445.43429844097994
+      inference_time: 2254.0
+      throughput: 443.6557231588287
       estimated_peak_memory_range:
-        min: 28672
-        max: 9857128
+        min: 24576
+        max: 8961568
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 22
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 25
-      job_id: jwgoyjd58
+      job_id: jep2xew4g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:02:55.130462Z'
+    timestamp: '2024-04-02T15:43:53.479475Z'
     torchscript_onnx_qnn:
-      inference_time: 2136.0
-      throughput: 468.1647940074906
+      inference_time: 2137.0
+      throughput: 467.94571829667757
       estimated_peak_memory_range:
-        min: 221184
-        max: 3873216
+        min: 24576
+        max: 6179792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 32
+        layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 32
-      job_id: j7gjxj8pd
+        total_layers: 31
+      job_id: j2p046j6g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1533.0
-      throughput: 652.3157208088714
+      inference_time: 1589.0
+      throughput: 629.3266205160478
       estimated_peak_memory_range:
         min: 16384
-        max: 23601872
+        max: 24462352
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 22
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 25
-      job_id: j1pv3jm5x
+      job_id: jqpyzmx7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:04:43.299283Z'
+    timestamp: '2024-04-02T15:46:31.476167Z'
     torchscript_onnx_qnn:
-      inference_time: 1462.0
-      throughput: 683.9945280437756
+      inference_time: 1456.0
+      throughput: 686.8131868131868
       estimated_peak_memory_range:
-        min: 208896
-        max: 20706112
+        min: 212992
+        max: 22416672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 32
+        layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 32
-      job_id: jlpe9j0gr
+        total_layers: 31
+      job_id: j1p821xxp
       job_status: Passed
```

## qai_hub_models/models/sesr_m5_quantized/export.py

```diff
@@ -190,14 +190,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sesr_m5_quantized/model.py

```diff
@@ -13,14 +13,15 @@
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models._shared.sesr.common import _load_sesr_source_model
+from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.sesr_m5.model import (
     NUM_CHANNELS,
     NUM_LBLOCKS,
     SCALING_FACTOR,
     SESR_M5,
 )
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
@@ -93,7 +94,15 @@
                     MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        if target_runtime == TargetRuntime.QNN:
+            return SourceModelFormat.ONNX
+        else:
+            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/sesr_m5_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: SESR-M5-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1749.0
-      throughput: 571.7552887364208
+      inference_time: 1357.0
+      throughput: 736.9196757553427
       estimated_peak_memory_range:
-        min: 28672
-        max: 6325016
+        min: 12288
+        max: 3744312
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 13
+        layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 16
-      job_id: joprk1k50
+        total_layers: 14
+      job_id: jogkv842p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:59:28.460705Z'
+    timestamp: '2024-04-02T15:13:49.881073Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 1403.0
-      throughput: 712.7583749109052
+      inference_time: 1112.0
+      throughput: 899.2805755395683
       estimated_peak_memory_range:
-        min: 20480
-        max: 21054176
+        min: 12288
+        max: 22134720
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 13
+        layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 16
-      job_id: jep2836p6
+        total_layers: 14
+      job_id: jn5q0vy4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:59:28.460714Z'
+    timestamp: '2024-04-02T15:13:49.881086Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/shufflenet_v2/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/shufflenet_v2/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Shufflenet-v2
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 919.0
-      throughput: 1088.139281828074
+      inference_time: 917.0
+      throughput: 1090.5125408942204
       estimated_peak_memory_range:
         min: 12288
-        max: 2065312
+        max: 6661376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 202
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 202
-      job_id: j2p0y1ngw
+      job_id: j1gl4lx85
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:00:07.928895Z'
+    timestamp: '2024-04-02T15:55:19.973155Z'
     torchscript_onnx_qnn:
-      inference_time: 322.0
-      throughput: 3105.590062111801
+      inference_time: 310.0
+      throughput: 3225.8064516129034
       estimated_peak_memory_range:
-        min: 626688
-        max: 3731328
+        min: 12288
+        max: 87860112
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 157
+        layers_on_npu: 156
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 157
-      job_id: jogkzlngd
+        total_layers: 156
+      job_id: j1p3n69l5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 586.0
-      throughput: 1706.4846416382252
+      inference_time: 583.0
+      throughput: 1715.2658662092624
       estimated_peak_memory_range:
-        min: 16384
-        max: 32832960
+        min: 12288
+        max: 33769008
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 202
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 202
-      job_id: j1p8o3og9
+      job_id: jw562w70g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:03:17.860163Z'
+    timestamp: '2024-04-02T15:58:01.979025Z'
     torchscript_onnx_qnn:
-      inference_time: 225.0
-      throughput: 4444.444444444444
+      inference_time: 223.0
+      throughput: 4484.304932735426
       estimated_peak_memory_range:
         min: 12288
-        max: 48449136
+        max: 48783840
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 157
+        layers_on_npu: 156
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 157
-      job_id: jn5q87o57
+        total_layers: 156
+      job_id: jwgoz8rxp
       job_status: Passed
```

## qai_hub_models/models/shufflenet_v2_quantized/export.py

```diff
@@ -196,14 +196,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/shufflenet_v2_quantized/model.py

```diff
@@ -19,15 +19,14 @@
 )
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.shufflenet_v2.model import ShufflenetV2
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 from qai_hub_models.utils.quantization_aimet import (
     convert_all_depthwise_to_per_tensor,
     tie_aimet_observer_groups,
 )
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
@@ -49,19 +48,14 @@
     ) -> None:
         ShufflenetV2.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "ShufflenetV2Quantizable":
         """
         Parameters:
@@ -94,22 +88,14 @@
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
 
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
-
     @classmethod
     def _tie_pre_concat_quantizers(cls, sim: QuantizationSimModel):
         """
         This ties together the output quantizers prior to concatenations. This
         prevents unnecessary re-quantization during the concatenation.
         """
         n = sim.model.net
```

## qai_hub_models/models/shufflenet_v2_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Shufflenet-v2Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 579.0
-      throughput: 1727.1157167530225
+      inference_time: 557.0
+      throughput: 1795.3321364452424
       estimated_peak_memory_range:
-        min: 16384
-        max: 4558296
+        min: 12288
+        max: 1902352
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 203
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 203
-      job_id: j1p89yxg9
+      job_id: j1pvq7djg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:32:39.995361Z'
+    timestamp: '2024-04-02T15:16:43.037345Z'
     torchscript_onnx_qnn:
-      inference_time: 279.0
-      throughput: 3584.2293906810037
+      inference_time: 274.0
+      throughput: 3649.6350364963505
       estimated_peak_memory_range:
-        min: 0
-        max: 75494608
+        min: 12288
+        max: 69529408
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 120
+        layers_on_npu: 119
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 120
-      job_id: j1glzm8pv
+        total_layers: 119
+      job_id: jlpeoyz1g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 385.0
-      throughput: 2597.4025974025976
+      inference_time: 404.0
+      throughput: 2475.2475247524753
       estimated_peak_memory_range:
         min: 12288
-        max: 21664192
+        max: 22095680
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 203
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 203
-      job_id: jn5qkq457
+      job_id: j7gjdq7xg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:34:41.782968Z'
+    timestamp: '2024-04-02T15:19:24.193795Z'
     torchscript_onnx_qnn:
-      inference_time: 204.0
-      throughput: 4901.9607843137255
+      inference_time: 194.0
+      throughput: 5154.639175257732
       estimated_peak_memory_range:
         min: 163840
-        max: 41738848
+        max: 40609808
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 120
+        layers_on_npu: 119
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 120
-      job_id: jw56j40po
+        total_layers: 119
+      job_id: jygz2nmkg
       job_status: Passed
```

## qai_hub_models/models/sinet/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sinet/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: SINet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1809.0
-      throughput: 552.791597567717
+      inference_time: 1813.0
+      throughput: 551.5719801434087
       estimated_peak_memory_range:
-        min: 20480
-        max: 2244048
+        min: 28672
+        max: 2078752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 240
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 240
-      job_id: jw566wn5o
+      job_id: jz5ww4l65
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:17:54.436410Z'
+    timestamp: '2024-04-02T15:16:44.533757Z'
     torchscript_onnx_qnn:
-      inference_time: 1193.0
-      throughput: 838.2229673093043
+      inference_time: 1195.0
+      throughput: 836.8200836820083
       estimated_peak_memory_range:
-        min: 20480
-        max: 25094232
+        min: 622592
+        max: 58073808
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 187
+        layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 187
-      job_id: jwgoy8158
+        total_layers: 186
+      job_id: jnp126n2g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1170.0
-      throughput: 854.7008547008547
+      inference_time: 1197.0
+      throughput: 835.421888053467
       estimated_peak_memory_range:
         min: 12288
-        max: 24922736
+        max: 25406400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 240
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 240
-      job_id: j1p3k6m52
+      job_id: jmg90dzlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:22:20.419307Z'
+    timestamp: '2024-04-02T15:19:24.421326Z'
     torchscript_onnx_qnn:
-      inference_time: 802.0
-      throughput: 1246.8827930174564
+      inference_time: 798.0
+      throughput: 1253.1328320802006
       estimated_peak_memory_range:
-        min: 12288
-        max: 65545232
+        min: 0
+        max: 67803232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 187
+        layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 187
-      job_id: j1pv37z5x
+        total_layers: 186
+      job_id: jvgdn2de5
       job_status: Passed
```

## qai_hub_models/models/squeezenet1_1/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/squeezenet1_1/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: SqueezeNet-1_1
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 225.0
-      throughput: 4444.444444444444
+      inference_time: 223.0
+      throughput: 4484.304932735426
       estimated_peak_memory_range:
         min: 24576
-        max: 1431872
+        max: 1673088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 39
-      job_id: j1p8ol8g9
+      job_id: jz5ww4l35
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:32:29.373813Z'
+    timestamp: '2024-04-02T15:16:32.269958Z'
     torchscript_onnx_qnn:
-      inference_time: 278.0
-      throughput: 3597.122302158273
+      inference_time: 274.0
+      throughput: 3649.6350364963505
       estimated_peak_memory_range:
-        min: 20480
-        max: 53223728
+        min: 16384
+        max: 8208760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 69
+        layers_on_npu: 68
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 69
-      job_id: jn5q8jm57
+        total_layers: 68
+      job_id: jnp126n8g
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 181.0
-      throughput: 5524.861878453039
+      inference_time: 182.0
+      throughput: 5494.505494505494
       estimated_peak_memory_range:
         min: 12288
-        max: 21672448
+        max: 21808400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 39
-      job_id: jogkzjogd
+      job_id: jmg90dzwg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:34:27.758337Z'
+    timestamp: '2024-04-02T15:19:11.667370Z'
     torchscript_onnx_qnn:
       inference_time: 199.0
       throughput: 5025.125628140703
       estimated_peak_memory_range:
         min: 618496
-        max: 28404384
+        max: 28004672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 69
+        layers_on_npu: 68
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 69
-      job_id: j1glnjlpv
+        total_layers: 68
+      job_id: jvgdn2dr5
       job_status: Passed
```

## qai_hub_models/models/squeezenet1_1_quantized/export.py

```diff
@@ -194,14 +194,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/squeezenet1_1_quantized/model.py

```diff
@@ -16,15 +16,14 @@
 from aimet_torch.cross_layer_equalization import equalize_model
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.squeezenet1_1.model import SqueezeNet
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 DEFAULT_ENCODINGS = "squeezenet1_1_quantized_encodings.json"
 
 
 class SqueezeNetQuantizable(AIMETQuantizableMixin, SqueezeNet):
@@ -39,19 +38,14 @@
     ) -> None:
         SqueezeNet.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "SqueezeNetQuantizable":
         """
         Parameters:
@@ -79,15 +73,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/squeezenet1_1_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: SqueezeNet-1_1Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 176.0
-      throughput: 5681.818181818182
+      inference_time: 150.0
+      throughput: 6666.666666666667
       estimated_peak_memory_range:
         min: 12288
-        max: 2498992
+        max: 1450328
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 39
-      job_id: jnp10jk5q
+      job_id: jz5729evp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:48:25.505884Z'
+    timestamp: '2024-04-02T16:00:15.491720Z'
     torchscript_onnx_qnn:
-      inference_time: 185.0
-      throughput: 5405.405405405405
+      inference_time: 179.0
+      throughput: 5586.592178770949
       estimated_peak_memory_range:
-        min: 172032
-        max: 55116856
+        min: 12288
+        max: 73066960
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 43
+        layers_on_npu: 42
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 43
-      job_id: jqp4q1qgo
+        total_layers: 42
+      job_id: j0px9xl3p
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 135.0
-      throughput: 7407.407407407408
+      inference_time: 130.0
+      throughput: 7692.307692307692
       estimated_peak_memory_range:
         min: 12288
-        max: 21511824
+        max: 21769728
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 39
-      job_id: jz57z4qp3
+      job_id: jqp4n3y8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:51:35.601938Z'
+    timestamp: '2024-04-02T16:02:39.628312Z'
     torchscript_onnx_qnn:
-      inference_time: 146.0
-      throughput: 6849.315068493151
+      inference_time: 155.0
+      throughput: 6451.612903225807
       estimated_peak_memory_range:
-        min: 159744
-        max: 18650384
+        min: 184320
+        max: 20075776
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 43
+        layers_on_npu: 42
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 43
-      job_id: jo5mrmygk
+        total_layers: 42
+      job_id: jo5me80dp
       job_status: Passed
```

## qai_hub_models/models/stable_diffusion_quantized/perf.yaml

```diff
@@ -1,102 +1,247 @@
+aggregated:
+  supported_oses:
+  - Android
+  supported_devices:
+  - Samsung Galaxy S23
+  - Samsung Galaxy S23 Ultra
+  - Samsung Galaxy S23+
+  - Samsung Galaxy S24
+  - Samsung Galaxy S24 Ultra
+  supported_chipsets:
+  - Snapdragon 8 Gen 2
+  - Snapdragon 8 Gen 3
 models:
 - name: TextEncoder_Quantized
   performance_metrics:
-  - reference_device_info:
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T01:59:20.073771Z'
     torchscript_onnx_qnn:
-      inference_time: 11362
-      throughput: 88.01
+      inference_time: 11371.0
+      throughput: 87.9430129276229
       estimated_peak_memory_range:
-        min: 53248
-        max: 44039432
+        min: 40960
+        max: 32706224
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
         layers_on_npu: 570
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 570
-      precision: uint16
-      primary_compute_unit: NPU
-      job_id: jo5m87owp
+      job_id: jz5we9435
       job_status: Passed
-- name: VAEDecoder_Quantized
-  performance_metrics:
-  - reference_device_info:
-      name: Samsung Galaxy S23
-      os: '13'
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
-      chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T02:00:38.776739Z'
     torchscript_onnx_qnn:
-      inference_time: 393878
-      throughput: 2.53
+      inference_time: 8080.0
+      throughput: 123.76237623762377
       estimated_peak_memory_range:
-        min: 225280
-        max: 11689680
+        min: 12288
+        max: 143750784
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
-        layers_on_npu: 409
+        layers_on_npu: 570
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 409
-      precision: uint16
-      primary_compute_unit: NPU
-      job_id: joprwro95
+        total_layers: 570
+      job_id: j1pv6wdk5
       job_status: Passed
 - name: UNet_Quantized
   performance_metrics:
-  - reference_device_info:
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T02:02:10.559200Z'
     torchscript_onnx_qnn:
-      inference_time: 256698
-      throughput: 3.89
+      inference_time: 255354.0
+      throughput: 3.9161321146330192
       estimated_peak_memory_range:
-        min: 143360
-        max: 12844792
+        min: 339968
+        max: 47217488
+      primary_compute_unit: NPU
+      precision: uint16
       layer_info:
         layers_on_npu: 5421
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 5421
-      precision: uint16
+      job_id: jnp14868p
+      job_status: Passed
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T02:03:42.305288Z'
+    torchscript_onnx_qnn:
+      inference_time: 188594.0
+      throughput: 5.302395622342174
+      estimated_peak_memory_range:
+        min: 356352
+        max: 1302707040
       primary_compute_unit: NPU
-      job_id: jegnk4org
+      precision: uint16
+      layer_info:
+        layers_on_npu: 5421
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 5421
+      job_id: jlpedvzo5
       job_status: Passed
-aggregated:
-  supported_devices:
-  - Samsung Galaxy S23 Ultra
-  supported_oses:
-  - Android
-  supported_chipsets:
-  - Snapdragon 8 Gen 2
+- name: VAEDecoder_Quantized
   performance_metrics:
-  - reference_device_info:
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-02-14T05:29:28.928297Z'
+    timestamp: '2024-03-19T02:04:59.572576Z'
     torchscript_onnx_qnn:
-      inference_time: 661938
-      throughput: 1.51
+      inference_time: 392074.0
+      throughput: 2.5505389288756715
       estimated_peak_memory_range:
-        min: 225280
-        max: 44039432
+        min: 278528
+        max: 26216704
+      primary_compute_unit: NPU
       precision: uint16
+      layer_info:
+        layers_on_npu: 409
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 409
+      job_id: jmg9l4dwg
+      job_status: Passed
+  - torchscript_onnx_tflite:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: ''
+      job_status: Skipped
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-03-19T02:06:20.192396Z'
+    torchscript_onnx_qnn:
+      inference_time: 295055.0
+      throughput: 3.389198623985359
+      estimated_peak_memory_range:
+        min: 192512
+        max: 91839552
       primary_compute_unit: NPU
-      job_id: ""
+      precision: uint16
+      layer_info:
+        layers_on_npu: 409
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 409
+      job_id: j7gjvl7vg
       job_status: Passed
```

## qai_hub_models/models/stylegan2/export.py

```diff
@@ -181,14 +181,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/stylegan2/model.py

```diff
@@ -91,24 +91,24 @@
         used to submit a profiling job on Qualcomm AI Hub.
         """
         inputs = {"image_noise": ((batch_size, output_size), "float32")}
         if num_classes != 0:
             inputs["classes"] = ((batch_size, num_classes), "float32")
         return inputs
 
-    def _get_input_spec_for_model_instance(self, batch_size: int = 1) -> InputSpec:
+    def _get_input_spec_for_instance(self, batch_size: int = 1) -> InputSpec:
         return self.__class__.get_input_spec(
             self.output_size, self.num_classes, batch_size
         )
 
     def sample_inputs(
         self, input_spec: InputSpec | None = None, seed=None
     ) -> Dict[str, List[np.ndarray]]:
         if not input_spec:
-            input_spec = self._get_input_spec_for_model_instance()
+            input_spec = self._get_input_spec_for_instance()
 
         inputs = {
             "image_noise": [
                 np.random.RandomState(seed)
                 .randn(*input_spec["image_noise"][0])
                 .astype(np.float32)
             ]
```

## qai_hub_models/models/stylegan2/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: StyleGAN2
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1218362.0
-      throughput: 0.8207741213202644
+      inference_time: 1245465.0
+      throughput: 0.8029129682488066
       estimated_peak_memory_range:
-        min: 1358295040
-        max: 1361471248
+        min: 1583226880
+        max: 1586523400
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 89
-        layers_on_cpu: 492
-        total_layers: 581
-      job_id: jlpe988gr
+        layers_on_cpu: 478
+        total_layers: 567
+      job_id: jegn0kzk5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:03:19.171321Z'
+    timestamp: '2024-04-02T15:46:54.066866Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 980347.0
-      throughput: 1.0200469833640537
+      inference_time: 1030564.0
+      throughput: 0.970342453258604
       estimated_peak_memory_range:
-        min: 1110478848
-        max: 1142166720
+        min: 897953792
+        max: 928847488
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 89
-        layers_on_cpu: 492
-        total_layers: 581
-      job_id: jz5wo84p1
+        layers_on_cpu: 478
+        total_layers: 567
+      job_id: jopr6wl0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:03:19.171331Z'
+    timestamp: '2024-04-02T15:46:54.066880Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/swin_base/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/swin_base/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Swin-Base
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 66948.0
-      throughput: 14.936966003465376
+      inference_time: 66984.0
+      throughput: 14.928938253911381
       estimated_peak_memory_range:
-        min: 28672
-        max: 6112608
+        min: 118784
+        max: 4254288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1614
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1614
-      job_id: jogkzm2gd
+      job_id: jep2xerrg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:23:06.160602Z'
+    timestamp: '2024-04-02T15:14:19.208239Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 43458.0
-      throughput: 23.010722996916563
+      inference_time: 43260.0
+      throughput: 23.11604253351826
       estimated_peak_memory_range:
-        min: 69632
-        max: 472671520
+        min: 90112
+        max: 512533392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1614
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1614
-      job_id: jn5q8o457
+      job_id: jqpyzmo8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:23:06.160610Z'
+    timestamp: '2024-04-02T15:14:19.208253Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/swin_small/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/swin_small/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Swin-Small
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 50143.0
-      throughput: 19.94296312546118
+      inference_time: 50305.0
+      throughput: 19.87873968790379
       estimated_peak_memory_range:
-        min: 90112
-        max: 3612056
+        min: 114688
+        max: 3114440
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1609
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1609
-      job_id: jo5mr9ygk
+      job_id: j2p046m9g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:53:07.887698Z'
+    timestamp: '2024-04-02T15:59:03.699659Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 33054.0
-      throughput: 30.2535245356084
+      inference_time: 32975.0
+      throughput: 30.32600454890068
       estimated_peak_memory_range:
         min: 45056
-        max: 454274336
+        max: 479723312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1609
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1609
-      job_id: jegn2qvgo
+      job_id: j1p821ekp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:53:07.887705Z'
+    timestamp: '2024-04-02T15:59:03.699672Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/swin_tiny/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/swin_tiny/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Swin-Tiny
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 31313.0
-      throughput: 31.935617794526234
+      inference_time: 31126.0
+      throughput: 32.12748184797275
       estimated_peak_memory_range:
-        min: 81920
-        max: 3482152
+        min: 53248
+        max: 3289744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 859
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 859
-      job_id: j0pxvv1g7
+      job_id: jogkv82wp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:34:33.080588Z'
+    timestamp: '2024-04-02T15:46:54.220773Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 20716.0
-      throughput: 48.27186715582159
+      inference_time: 20461.0
+      throughput: 48.87346659498558
       estimated_peak_memory_range:
-        min: 49152
-        max: 274521296
+        min: 45056
+        max: 293868864
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 859
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 859
-      job_id: jo5mrrwgk
+      job_id: jn5q0vlnp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:34:33.080597Z'
+    timestamp: '2024-04-02T15:46:54.220786Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/trocr/export.py

```diff
@@ -23,15 +23,14 @@
 from qai_hub_models.utils.printing import (
     print_inference_metrics,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
-    transpose_channel_first_to_last,
 )
 
 ALL_COMPONENTS = ["TrOCREncoder", "TrOCRDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
@@ -123,15 +122,15 @@
         input_spec = component.get_input_spec()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options + " --force_channel_last_input pixel_values"
+            target_runtime, compile_options
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub.Device(device),
             name=f"{model_name}_{component_name}",
@@ -166,21 +165,17 @@
             print(
                 f"Running inference for {component_name} on a hosted device with example inputs."
             )
             profile_options_all = components_dict[
                 component_name
             ].get_hub_profile_options(target_runtime, profile_options)
             sample_inputs = components_dict[component_name].sample_inputs()
-            # Convert inputs from channel first to channel last
-            hub_inputs = transpose_channel_first_to_last(
-                "pixel_values", sample_inputs, target_runtime
-            )
             submitted_inference_job = hub.submit_inference_job(
                 model=compile_jobs[component_name].get_target_model(),
-                inputs=hub_inputs,
+                inputs=sample_inputs,
                 device=hub.Device(device),
                 name=f"{model_name}_{component_name}",
                 options=profile_options_all,
             )
             inference_jobs[component_name] = cast(
                 hub.client.InferenceJob, submitted_inference_job
             )
@@ -220,15 +215,18 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_qnn=False
+        model_cls=Model,
+        components=ALL_COMPONENTS,
+        supports_qnn=False,
+        supports_ort=False,
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/trocr/model.py

```diff
@@ -252,15 +252,15 @@
             decoder_input_specs[f"kv_{i}_attn_key"] = attn_cache_spec
             decoder_input_specs[f"kv_{i}_attn_val"] = attn_cache_spec
             decoder_input_specs[f"kv_{i}_cross_attn_key"] = cross_attn_cache_spec
             decoder_input_specs[f"kv_{i}_cross_attn_val"] = cross_attn_cache_spec
 
         return decoder_input_specs
 
-    def _get_input_spec_for_model_instance(self) -> InputSpec:
+    def _get_input_spec_for_instance(self) -> InputSpec:
         return self.__class__.get_input_spec(
             self.decoder_attention_heads,
             self.embeddings_per_head,
             self.num_decoder_layers,
         )
 
     @classmethod
```

## qai_hub_models/models/trocr/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: TrOCREncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 243976.0
-      throughput: 4.098763812834049
+      inference_time: 243112.0
+      throughput: 4.1133304814241995
       estimated_peak_memory_range:
-        min: 7221248
-        max: 10173368
+        min: 7290880
+        max: 10682856
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 627
+        layers_on_npu: 628
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 627
-      job_id: j7gjxxxpd
+        total_layers: 628
+      job_id: j1gl4lyj5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:46:04.016709Z'
+    timestamp: '2024-04-02T15:25:12.572315Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 182193.0
-      throughput: 5.48868507571641
+      inference_time: 182195.0
+      throughput: 5.488624825050084
       estimated_peak_memory_range:
-        min: 20480
-        max: 305620528
+        min: 6701056
+        max: 331600272
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 627
+        layers_on_npu: 628
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 627
-      job_id: jygzeekg8
+        total_layers: 628
+      job_id: j1p3n6z35
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:46:04.016721Z'
+    timestamp: '2024-04-02T15:25:12.572329Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -105,36 +106,36 @@
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
 - name: TrOCRDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2810.0
-      throughput: 355.87188612099646
+      inference_time: 2781.0
+      throughput: 359.5828838547285
       estimated_peak_memory_range:
-        min: 12288
-        max: 2353880
+        min: 28672
+        max: 2706376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 394
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 394
-      job_id: jlpe991gr
+      job_id: jw562w86g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:51:23.352323Z'
+    timestamp: '2024-04-02T15:30:38.723405Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -143,36 +144,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 2018.0
-      throughput: 495.5401387512388
+      inference_time: 1988.0
+      throughput: 503.01810865191146
       estimated_peak_memory_range:
         min: 12288
-        max: 193404384
+        max: 194199920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 394
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 394
-      job_id: jz5wov6p1
+      job_id: jwgoz8lqp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:51:23.352351Z'
+    timestamp: '2024-04-02T15:30:38.723420Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/unet_segmentation/export.py

```diff
@@ -193,14 +193,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/unet_segmentation/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Unet-Segmentation
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 160694.0
-      throughput: 6.223007704083538
+      inference_time: 159721.0
+      throughput: 6.260917474846764
       estimated_peak_memory_range:
         min: 6688768
-        max: 229291048
+        max: 230831992
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jlpe9rvgr
+      job_id: j1pvq72kg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:14:31.434457Z'
+    timestamp: '2024-04-02T16:00:44.075564Z'
     torchscript_onnx_qnn:
-      inference_time: 146509.0
-      throughput: 6.825519251377049
+      inference_time: 143885.0
+      throughput: 6.949994787503909
       estimated_peak_memory_range:
-        min: 10952704
-        max: 44981480
+        min: 9871360
+        max: 36840448
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 52
+        layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 52
-      job_id: jmg9v3857
+        total_layers: 51
+      job_id: jlpeoy6og
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 118830.0
-      throughput: 8.415383320710259
+      inference_time: 113226.0
+      throughput: 8.831893734654585
       estimated_peak_memory_range:
-        min: 6234112
-        max: 344093584
+        min: 4681728
+        max: 361817664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jz5wodmp1
+      job_id: j7gjdq3vg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:16:26.988161Z'
+    timestamp: '2024-04-02T16:03:02.410135Z'
     torchscript_onnx_qnn:
-      inference_time: 110459.0
-      throughput: 9.053132836618111
+      inference_time: 110489.0
+      throughput: 9.050674727800958
       estimated_peak_memory_range:
-        min: 328994816
-        max: 420473984
+        min: 9871360
+        max: 113176352
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 52
+        layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 52
-      job_id: jnp10d75q
+        total_layers: 51
+      job_id: jygz2nzog
       job_status: Passed
```

## qai_hub_models/models/vit/export.py

```diff
@@ -187,14 +187,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/vit/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: VIT
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 136110.0
-      throughput: 7.346998751010212
+      inference_time: 135551.0
+      throughput: 7.3772971058863455
       estimated_peak_memory_range:
-        min: 86016
-        max: 3893632
+        min: 167936
+        max: 4072768
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 557
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 557
-      job_id: j1gln9lpv
+      job_id: jz5ww4y35
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:40:01.517909Z'
+    timestamp: '2024-04-02T15:59:30.586343Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 100287.0
-      throughput: 9.971382133277494
+      inference_time: 100385.0
+      throughput: 9.96164765652239
       estimated_peak_memory_range:
-        min: 163840
-        max: 401162112
+        min: 172032
+        max: 414376288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 557
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 557
-      job_id: jw566975o
+      job_id: jmg90dowg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:40:01.517918Z'
+    timestamp: '2024-04-02T15:59:30.586357Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/whisper_base_en/export.py

```diff
@@ -215,15 +215,18 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_qnn=False
+        model_cls=Model,
+        components=ALL_COMPONENTS,
+        supports_qnn=False,
+        supports_ort=False,
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/whisper_base_en/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: WhisperEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 154406.0
-      throughput: 6.476432262994962
+      inference_time: 154210.0
+      throughput: 6.484663770183516
       estimated_peak_memory_range:
-        min: 36892672
-        max: 232224176
+        min: 11546624
+        max: 113388704
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 315
         layers_on_cpu: 0
         total_layers: 315
-      job_id: jqp4q0vgo
+      job_id: jnp126o8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:10:43.748935Z'
+    timestamp: '2024-04-02T15:35:51.877964Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 120437.0
-      throughput: 8.303096224582147
+      inference_time: 124136.0
+      throughput: 8.055680866146806
       estimated_peak_memory_range:
-        min: 36777984
-        max: 66087104
+        min: 35241984
+        max: 63999568
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 315
         layers_on_cpu: 0
         total_layers: 315
-      job_id: jo5mrywgk
+      job_id: jz5729ovp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:10:43.748943Z'
+    timestamp: '2024-04-02T15:35:51.877979Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -105,36 +106,36 @@
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 14139.0
-      throughput: 70.72635971426551
+      inference_time: 14069.0
+      throughput: 71.0782571611344
       estimated_peak_memory_range:
-        min: 3051520
-        max: 5712920
+        min: 5812224
+        max: 8998208
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 433
+        layers_on_npu: 459
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 433
-      job_id: j0pxv21g7
+        layers_on_cpu: 2
+        total_layers: 461
+      job_id: jvgdn26r5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:16:05.499826Z'
+    timestamp: '2024-04-02T15:41:26.044149Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -143,36 +144,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 10614.0
-      throughput: 94.21518748822311
+      inference_time: 10562.0
+      throughput: 94.6790380609733
       estimated_peak_memory_range:
-        min: 2019328
-        max: 96045024
+        min: 4571136
+        max: 110220208
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 433
+        layers_on_npu: 459
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 433
-      job_id: jegn28rgo
+        layers_on_cpu: 2
+        total_layers: 461
+      job_id: jqp4n3e8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:16:05.499836Z'
+    timestamp: '2024-04-02T15:41:26.044164Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/whisper_small_en/export.py

```diff
@@ -215,15 +215,18 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_qnn=False
+        model_cls=Model,
+        components=ALL_COMPONENTS,
+        supports_qnn=False,
+        supports_ort=False,
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/whisper_small_en/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: WhisperEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 602022.0
-      throughput: 1.6610688645929883
+      inference_time: 598326.0
+      throughput: 1.6713296764640013
       estimated_peak_memory_range:
-        min: 12288
-        max: 448965896
+        min: 95817728
+        max: 535321856
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 609
         layers_on_cpu: 0
         total_layers: 609
-      job_id: jvgdw4k5j
+      job_id: j0px9x03p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:29:10.773412Z'
+    timestamp: '2024-04-02T15:36:02.927472Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 461601.0
-      throughput: 2.1663731231084853
+      inference_time: 469347.0
+      throughput: 2.1306197759866365
       estimated_peak_memory_range:
-        min: 14163968
-        max: 46674320
+        min: 28250112
+        max: 60097088
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 609
         layers_on_cpu: 0
         total_layers: 609
-      job_id: jnp101l5q
+      job_id: jegn0k1k5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:29:10.773421Z'
+    timestamp: '2024-04-02T15:36:02.927486Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -105,36 +106,36 @@
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 45479.0
-      throughput: 21.988170364343983
+      inference_time: 46381.0
+      throughput: 21.560552812574116
       estimated_peak_memory_range:
-        min: 8577024
-        max: 12019040
+        min: 16228352
+        max: 19790512
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 853
+        layers_on_npu: 903
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 853
-      job_id: jz5wozjp1
+        layers_on_cpu: 2
+        total_layers: 905
+      job_id: jo5me89dp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:33:07.115194Z'
+    timestamp: '2024-04-02T15:41:42.406080Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -143,36 +144,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 34412.0
+      throughput: 29.059630361501803
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 20180992
+        max: 1716349552
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 903
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 0
-      job_id: jvgdw4l5j
-      job_status: Failed
+        layers_on_cpu: 2
+        total_layers: 905
+      job_id: jopr6wx0p
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:33:07.115203Z'
+    timestamp: '2024-04-02T15:41:42.406095Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/whisper_tiny_en/export.py

```diff
@@ -215,15 +215,18 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_qnn=False
+        model_cls=Model,
+        components=ALL_COMPONENTS,
+        supports_qnn=False,
+        supports_ort=False,
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/whisper_tiny_en/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: WhisperEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 67350.0
-      throughput: 14.847809948032666
+      inference_time: 69083.0
+      throughput: 14.475341256170115
       estimated_peak_memory_range:
-        min: 11608064
-        max: 57976544
+        min: 2011136
+        max: 68110808
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 217
         layers_on_cpu: 0
         total_layers: 217
-      job_id: jz57zx9p3
+      job_id: jep2xeorg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:03:16.946141Z'
+    timestamp: '2024-04-02T15:14:01.654956Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 53449.0
-      throughput: 18.709423936836984
+      inference_time: 53036.0
+      throughput: 18.855117278829475
       estimated_peak_memory_range:
         min: 0
-        max: 27656928
+        max: 25669392
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 217
         layers_on_cpu: 0
         total_layers: 217
-      job_id: jegn23qgo
+      job_id: j2p046o9g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:03:16.946150Z'
+    timestamp: '2024-04-02T15:14:01.654971Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -105,36 +106,36 @@
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7423.0
-      throughput: 134.71642193183348
+      inference_time: 7365.0
+      throughput: 135.77732518669382
       estimated_peak_memory_range:
-        min: 1634304
-        max: 4170776
+        min: 3002368
+        max: 5408984
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 293
+        layers_on_npu: 311
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 293
-      job_id: jqp4qv1go
+        layers_on_cpu: 2
+        total_layers: 313
+      job_id: jqpyzm88g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:09:31.853789Z'
+    timestamp: '2024-04-02T15:19:28.253886Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -143,36 +144,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 5570.0
-      throughput: 179.53321364452424
+      inference_time: 5492.0
+      throughput: 182.0830298616169
       estimated_peak_memory_range:
-        min: 466944
-        max: 230273920
+        min: 20480
+        max: 233538800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 293
+        layers_on_npu: 311
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 293
-      job_id: joprke750
+        layers_on_cpu: 2
+        total_layers: 313
+      job_id: j1p821jkp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:09:31.853814Z'
+    timestamp: '2024-04-02T15:19:28.253901Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/wideresnet50/export.py

```diff
@@ -185,14 +185,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/wideresnet50/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: WideResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 4401.0
-      throughput: 227.22108611679164
+      inference_time: 4786.0
+      throughput: 208.94274968658587
       estimated_peak_memory_range:
-        min: 20480
-        max: 2132848
+        min: 28672
+        max: 170961592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 77
-      job_id: jwgoyr458
+      job_id: jogkv86wp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:38.602998Z'
+    timestamp: '2024-04-02T15:27:33.204984Z'
     torchscript_onnx_qnn:
-      inference_time: 4580.0
-      throughput: 218.34061135371178
+      inference_time: 4614.0
+      throughput: 216.7316861725184
       estimated_peak_memory_range:
-        min: 618496
-        max: 323904968
+        min: 643072
+        max: 314223792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: j7gjx77pd
+        total_layers: 124
+      job_id: j1gl4lwj5
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 3306.0
-      throughput: 302.48033877797945
+      inference_time: 3605.0
+      throughput: 277.39251040221916
       estimated_peak_memory_range:
-        min: 16384
-        max: 94385296
+        min: 20480
+        max: 96019088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 77
-      job_id: j1pv3d75x
+      job_id: jn5q0v4np
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:15:48.204812Z'
+    timestamp: '2024-04-02T15:30:15.130717Z'
     torchscript_onnx_qnn:
-      inference_time: 3413.0
-      throughput: 292.99736302373276
+      inference_time: 3410.0
+      throughput: 293.2551319648094
       estimated_peak_memory_range:
         min: 618496
-        max: 52379088
+        max: 53769616
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jlpe9z7gr
+        total_layers: 124
+      job_id: jw562wo6g
       job_status: Passed
```

## qai_hub_models/models/wideresnet50_quantized/export.py

```diff
@@ -194,14 +194,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/wideresnet50_quantized/model.py

```diff
@@ -19,15 +19,14 @@
 )
 from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models.wideresnet50.model import WideResNet50
 from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
-from qai_hub_models.utils.base_model import SourceModelFormat, TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 DEFAULT_ENCODINGS = "wideresnet50_quantized_encodings.json"
 
 
 class WideResNet50Quantizable(AIMETQuantizableMixin, WideResNet50):
@@ -42,19 +41,14 @@
     ) -> None:
         WideResNet50.__init__(self, sim_model.model)
         AIMETQuantizableMixin.__init__(
             self,
             sim_model,
         )
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        return SourceModelFormat.ONNX
-
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "WideResNet50Quantizable":
         """
         Parameters:
@@ -84,15 +78,7 @@
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
         return cls(sim)
-
-    def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
-    ) -> str:
-        compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
-        )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
```

## qai_hub_models/models/wideresnet50_quantized/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: WideResNet50-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1767.0
-      throughput: 565.9309564233164
+      inference_time: 1771.0
+      throughput: 564.652738565782
       estimated_peak_memory_range:
-        min: 24576
-        max: 1759936
+        min: 12288
+        max: 2057600
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 78
-      job_id: jz5wo4zp1
+      job_id: jwgoz8dqp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:12:41.207435Z'
+    timestamp: '2024-04-02T15:50:01.927438Z'
     torchscript_onnx_qnn:
-      inference_time: 1707.0
-      throughput: 585.8230814294083
+      inference_time: 1722.0
+      throughput: 580.7200929152149
       estimated_peak_memory_range:
-        min: 28672
-        max: 479496224
+        min: 16384
+        max: 480044216
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 76
+        layers_on_npu: 75
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 76
-      job_id: j0pxvxjg7
+        total_layers: 75
+      job_id: j7gjdqyvg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1322.0
-      throughput: 756.4296520423601
+      inference_time: 1346.0
+      throughput: 742.9420505200594
       estimated_peak_memory_range:
-        min: 16384
-        max: 54559456
+        min: 12288
+        max: 55534992
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 78
-      job_id: jvgdw2k5j
+      job_id: j1pvq7mkg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:17:15.136644Z'
+    timestamp: '2024-04-02T15:52:43.890803Z'
     torchscript_onnx_qnn:
-      inference_time: 1291.0
-      throughput: 774.5933384972889
+      inference_time: 1290.0
+      throughput: 775.1937984496124
       estimated_peak_memory_range:
         min: 167936
-        max: 41865680
+        max: 41464352
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 76
+        layers_on_npu: 75
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 76
-      job_id: jogkz4ygd
+        total_layers: 75
+      job_id: jlpeoyxog
       job_status: Passed
```

## qai_hub_models/models/xlsr/export.py

```diff
@@ -189,14 +189,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/xlsr/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: XLSR
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2508.0
-      throughput: 398.72408293460927
+      inference_time: 2520.0
+      throughput: 396.8253968253968
       estimated_peak_memory_range:
-        min: 16384
-        max: 9569248
+        min: 28672
+        max: 1367248
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 16
-      job_id: jz57z6np3
+      job_id: jygz2nyog
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:14:58.320277Z'
+    timestamp: '2024-04-02T15:55:01.326907Z'
     torchscript_onnx_qnn:
-      inference_time: 987.0
-      throughput: 1013.1712259371834
+      inference_time: 971.0
+      throughput: 1029.8661174047375
       estimated_peak_memory_range:
-        min: 2121728
-        max: 10203592
+        min: 217088
+        max: 67726144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 22
+        layers_on_npu: 21
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 22
-      job_id: j0pxvm8g7
+        total_layers: 21
+      job_id: jmg90d2wg
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 1996.0
-      throughput: 501.00200400801606
+      inference_time: 1798.0
+      throughput: 556.1735261401557
       estimated_peak_memory_range:
         min: 16384
-        max: 19879696
+        max: 19705360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 16
-      job_id: jqp4q82go
+      job_id: jz5ww4z35
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:16:54.078428Z'
+    timestamp: '2024-04-02T15:57:38.953477Z'
     torchscript_onnx_qnn:
-      inference_time: 631.0
-      throughput: 1584.7860538827258
+      inference_time: 628.0
+      throughput: 1592.3566878980891
       estimated_peak_memory_range:
-        min: 225280
-        max: 18045792
+        min: 208896
+        max: 18068960
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 22
+        layers_on_npu: 21
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 22
-      job_id: jegn2xjgo
+        total_layers: 21
+      job_id: jnp12618g
       job_status: Passed
```

## qai_hub_models/models/xlsr_quantized/export.py

```diff
@@ -198,14 +198,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/xlsr_quantized/model.py

```diff
@@ -11,14 +11,15 @@
 )
 
 # isort: on
 
 import torch
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
+from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.xlsr.model import XLSR, _load_xlsr_source_model
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 # Weights and config stored in S3 are sourced from
 # https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/xlsr/model/model_cards/xlsr_4x_w8a8.json:
@@ -84,7 +85,15 @@
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         return cls(sim)
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        if target_runtime == TargetRuntime.QNN:
+            return SourceModelFormat.ONNX
+        else:
+            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/xlsr_quantized/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: XLSR-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1349.0
-      throughput: 741.2898443291327
+      inference_time: 1152.0
+      throughput: 868.0555555555555
       estimated_peak_memory_range:
-        min: 28672
-        max: 1726904
+        min: 77824
+        max: 1569664
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: j1p3k3l52
+        total_layers: 17
+      job_id: jvgdn24r5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:58:09.460010Z'
+    timestamp: '2024-04-02T15:52:42.432436Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 1084.0
-      throughput: 922.509225092251
+      inference_time: 927.0
+      throughput: 1078.7486515641856
       estimated_peak_memory_range:
-        min: 20480
-        max: 21010912
+        min: 16384
+        max: 20315072
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: jwgoy0x58
+        total_layers: 17
+      job_id: jz5729nvp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:58:09.460020Z'
+    timestamp: '2024-04-02T15:52:42.432450Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/yolov6/export.py

```diff
@@ -187,14 +187,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov6/model.py

```diff
@@ -27,43 +27,58 @@
 WEIGHTS_PATH = "https://github.com/meituan/YOLOv6/releases/download/0.4.0/"
 DEFAULT_WEIGHTS = "yolov6n.pt"
 
 
 class YoloV6(BaseModel):
     """Exportable YoloV6 bounding box detector, end-to-end."""
 
-    def __init__(self, model: nn.Module) -> None:
+    def __init__(self, model: nn.Module, include_postprocessing: bool = True) -> None:
         super().__init__()
         self.model = model
+        self.include_postprocessing = include_postprocessing
 
     # All image input spatial dimensions should be a multiple of this stride.
     STRIDE_MULTIPLE = 32
 
     @classmethod
-    def from_pretrained(cls, ckpt_name: str = DEFAULT_WEIGHTS):
+    def from_pretrained(
+        cls, ckpt_name: str = DEFAULT_WEIGHTS, include_postprocessing: bool = True
+    ):
         model_url = f"{WEIGHTS_PATH}{ckpt_name}"
         asset = CachedWebModelAsset(model_url, MODEL_ID, MODEL_ASSET_VERSION, ckpt_name)
         model = _load_yolov6_source_model_from_weights(asset)
-        return cls(model)
+        return cls(model, include_postprocessing)
 
     def forward(self, image: torch.Tensor):
         """
         Run YoloV6 on `image`, and produce a predicted set of bounding boxes and associated class probabilities.
 
         Parameters:
             image: Pixel values pre-processed for encoder consumption.
                    Range: float[0, 1]
                    3-channel Color Space: RGB
 
         Returns:
-            boxes: Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
-            class scores multiplied by confidence: Shape [batch, num_preds, # of classes (typically 80)]
+            If self.include_postprocessing:
+                boxes: Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
+                classes: class scores multiplied by confidence: Shape [batch, num_preds, # of classes (typically 80)]
+
+            Otherwise:
+                detector_output: torch.Tensor
+                    Shape is [batch, num_preds, k]
+                        where, k = # of classes + 5
+                        k is structured as follows [box_coordinates (4) , conf (1) , # of classes]
+                        and box_coordinates are [x_center, y_center, w, h]
         """
         predictions = self.model(image)
-        return detect_postprocess(predictions)
+        return (
+            detect_postprocess(predictions)
+            if self.include_postprocessing
+            else predictions
+        )
 
     @staticmethod
     def get_input_spec(
         batch_size: int = 1,
         num_channels: int = 3,
         height: int = 640,
         width: int = 640,
```

## qai_hub_models/models/yolov6/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Yolo-v6
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 8480.0
-      throughput: 117.9245283018868
+      inference_time: 7224.0
+      throughput: 138.42746400885935
       estimated_peak_memory_range:
-        min: 24576
-        max: 3130456
+        min: 53248
+        max: 8291064
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 182
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 182
-      job_id: jz5wo0jp1
+      job_id: jqp4n348g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:29:36.439969Z'
+    timestamp: '2024-04-02T15:38:46.956110Z'
     torchscript_onnx_qnn:
-      inference_time: 7275.0
-      throughput: 137.4570446735395
+      inference_time: 6898.0
+      throughput: 144.96955639315743
       estimated_peak_memory_range:
-        min: 4939776
-        max: 18286232
+        min: 5578752
+        max: 19291464
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 230
+        layers_on_npu: 229
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 230
-      job_id: jnp10kl5q
+        total_layers: 229
+      job_id: jo5me8kdp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 6051.0
-      throughput: 165.26194017517767
+      inference_time: 5152.0
+      throughput: 194.09937888198758
       estimated_peak_memory_range:
-        min: 16384
-        max: 74357488
+        min: 36864
+        max: 82013648
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 182
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 182
-      job_id: jmg9v7v57
+      job_id: j0px9xr3p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:32:50.147901Z'
+    timestamp: '2024-04-02T15:41:29.144196Z'
     torchscript_onnx_qnn:
-      inference_time: 5175.0
-      throughput: 193.23671497584542
+      inference_time: 4871.0
+      throughput: 205.29665366454526
       estimated_peak_memory_range:
-        min: 4931584
-        max: 94425040
+        min: 4947968
+        max: 93426784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 230
+        layers_on_npu: 229
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 230
-      job_id: jz57zmrp3
+        total_layers: 229
+      job_id: jegn0kqk5
       job_status: Passed
```

## qai_hub_models/models/yolov7/export.py

```diff
@@ -187,14 +187,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov7/model.py

```diff
@@ -28,26 +28,29 @@
 class YoloV7(BaseModel):
     """Exportable YoloV7 bounding box detector, end-to-end."""
 
     def __init__(
         self,
         yolov7_feature_extractor: torch.nn.Module,
         yolov7_detector: torch.nn.Module,
+        include_postprocessing: bool = True,
     ) -> None:
         super().__init__()
         self.yolov7_feature_extractor = yolov7_feature_extractor
         self.yolov7_detector = yolov7_detector
+        self.include_postprocessing = include_postprocessing
 
     # All image input spatial dimensions should be a multiple of this stride.
     STRIDE_MULTIPLE = 32
 
     @classmethod
     def from_pretrained(
         cls,
         weights_name: Optional[str] = DEFAULT_WEIGHTS,
+        include_postprocessing: bool = True,
     ):
         """Load YoloV7 from a weightfile created by the source YoloV7 repository."""
         # Load PyTorch model from disk
         yolov7_model = _load_yolov7_source_model_from_weights(weights_name)
 
         yolov7_model.profile = False
 
@@ -62,37 +65,46 @@
             -1
         ].f  # Previous (input) node indices in sequential model
         detector_head_state_dict["i"] = yolov7_model.model[
             -1
         ].i  # Index in sequential model
         yolov7_detect = _YoloV7Detector.from_yolov7_state_dict(detector_head_state_dict)
 
-        return cls(
-            yolov7_model,
-            yolov7_detect,
-        )
+        return cls(yolov7_model, yolov7_detect, include_postprocessing)
 
     def forward(self, image: torch.Tensor):
         """
         Run YoloV7 on `image`, and produce a predicted set of bounding boxes and associated class probabilities.
 
         Parameters:
             image: Pixel values pre-processed for encoder consumption.
                    Range: float[0, 1]
                    3-channel Color Space: BGR
 
         Returns:
-            boxes: Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
-            class scores multiplied by confidence: Shape [batch, num_preds, # of classes (typically 80)]
+            If self.include_postprocessing:
+                boxes: Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
+                classes: class scores multiplied by confidence: Shape [batch, num_preds, # of classes (typically 80)]
+
+            Otherwise:
+                detector_output: torch.Tensor
+                    Shape is [batch, num_preds, k]
+                        where, k = # of classes + 5
+                        k is structured as follows [box_coordinates (4) , conf (1) , # of classes]
+                        and box_coordinates are [x_center, y_center, w, h]
         """
         feature_extraction_output = (
             *self.yolov7_feature_extractor(image),
         )  # Convert output list to Tuple, for exportability
         prediction = self.yolov7_detector(feature_extraction_output)
-        return detect_postprocess(prediction)
+        return (
+            detect_postprocess(prediction)
+            if self.include_postprocessing
+            else prediction
+        )
 
     @staticmethod
     def get_input_spec(
         batch_size: int = 1,
         num_channels: int = 3,
         height: int = 640,
         width: int = 640,
```

## qai_hub_models/models/yolov7/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: Yolo-v7
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 24023.0
-      throughput: 41.626774341256294
+      inference_time: 25218.0
+      throughput: 39.65421524308034
       estimated_peak_memory_range:
-        min: 9568256
-        max: 12076232
+        min: 9555968
+        max: 43269368
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 286
         layers_on_gpu: 0
         layers_on_cpu: 21
         total_layers: 307
-      job_id: jqpye94gy
+      job_id: jopr6wd0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:22:25.772406Z'
+    timestamp: '2024-04-02T15:35:57.654637Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 17674.0
-      throughput: 56.580287427860135
+      inference_time: 19396.0
+      throughput: 51.557022066405445
       estimated_peak_memory_range:
-        min: 327680
-        max: 113867968
+        min: 12288
+        max: 131497776
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 286
         layers_on_gpu: 0
         layers_on_cpu: 21
         total_layers: 307
-      job_id: j2p0ynegw
+      job_id: jep2xedrg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:22:25.772414Z'
+    timestamp: '2024-04-02T15:35:57.654650Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/models/yolov8_det/app.py

```diff
@@ -1,17 +1,25 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
+from typing import Tuple
+
 import torch
 
 from qai_hub_models.models._shared.yolo.app import YoloObjectDetectionApp
+from qai_hub_models.models.yolov8_det.model import yolov8_detect_postprocess
 
 
 class YoloV8DetectionApp(YoloObjectDetectionApp):
     def check_image_size(self, pixel_values: torch.Tensor) -> None:
         """
         YoloV8 does not check for spatial dim shapes for input image
         """
         pass
+
+    def pre_nms_postprocess(
+        self, prediction: torch.Tensor
+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
+        return yolov8_detect_postprocess(prediction)
```

## qai_hub_models/models/yolov8_det/export.py

```diff
@@ -189,14 +189,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model)
+    parser = export_parser(model_cls=Model, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov8_det/model.py

```diff
@@ -27,38 +27,57 @@
 ]
 DEFAULT_WEIGHTS = "yolov8n.pt"
 
 
 class YoloV8Detector(BaseModel):
     """Exportable YoloV8 bounding box detector, end-to-end."""
 
-    def __init__(self, model: nn.Module) -> None:
+    def __init__(self, model: nn.Module, include_postprocessing: bool = True) -> None:
         super().__init__()
         self.model = model
+        self.include_postprocessing = include_postprocessing
 
     @classmethod
-    def from_pretrained(cls, ckpt_name: str = DEFAULT_WEIGHTS):
+    def from_pretrained(
+        cls, ckpt_name: str = DEFAULT_WEIGHTS, include_postprocessing: bool = True
+    ):
         model = ultralytics_YOLO(ckpt_name).model
         model.eval()
-        return cls(model)
+        return cls(model, include_postprocessing)
 
     def forward(self, image: torch.Tensor):
         """
         Run YoloV8 on `image`, and produce a predicted set of bounding boxes and associated class probabilities.
 
         Parameters:
             image: Pixel values pre-processed for encoder consumption.
                     Range: float[0, 1]
                     3-channel Color Space: RGB
 
         Returns:
-            boxes: Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
-            class scores multiplied by confidence: Shape [batch, num_preds, # of classes (typically 80)]
+            If self.include_postprocessing:
+                boxes: torch.Tensor
+                    Bounding box locations. Shape is [batch, num preds, 4] where 4 == (x1, y1, x2, y2)
+                scores: torch.Tensor
+                    class scores multiplied by confidence: Shape is [batch, num_preds]
+                class_idx: torch.tensor
+                    Shape is [batch, num_preds] where the last dim is the index of the most probable class of the prediction.
+
+            Otherwise:
+                predictions: torch.Tensor
+                    Shape is [batch, k, num_preds]
+                        Where, k = # of classes + 4
+                        The array dimension k is structured as follows:
+                            [box coordintes, # of classes]
+                        where box coordinates are [x_center, y_center, w, h]
         """
         predictions, *_ = self.model(image)
+        if not self.include_postprocessing:
+            return predictions
+
         boxes, scores, classes = yolov8_detect_postprocess(predictions)
         return boxes, scores, classes
 
     @staticmethod
     def get_input_spec(
         batch_size: int = 1,
         num_channels: int = 3,
@@ -77,17 +96,18 @@
     Post processing to break YoloV8 detector output into multiple, consumable tensors (eg. for NMS).
         such as bounding boxes, scores and classes.
 
     Parameters:
         detector_output: torch.Tensor
             The output of Yolo Detection model
             Shape is [batch, k, num_preds]
-                where, k = # of classes + 4
-                k is structured as follows [boxes (4) : # of classes]
-                and boxes are co-ordinates [x_center, y_center, w, h]
+                Where, k = # of classes + 4
+                The array dimension k is structured as follows:
+                    [box coordintes, # of classes]
+                where box coordinates are [x_center, y_center, w, h]
 
     Returns:
         boxes: torch.Tensor
             Bounding box locations. Shape is [batch, num preds, 4] where 4 == (x1, y1, x2, y2)
         scores: torch.Tensor
             class scores multiplied by confidence: Shape is [batch, num_preds]
         class_idx: torch.tensor
```

## qai_hub_models/models/yolov8_det/perf.yaml

```diff
@@ -15,94 +15,95 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: YOLOv8-Detection
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 9217.0
-      throughput: 108.49517196484756
+      inference_time: 6113.0
+      throughput: 163.5858007524947
       estimated_peak_memory_range:
-        min: 262144
-        max: 19308896
+        min: 233472
+        max: 8968336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 300
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 300
-      job_id: jo5mrw9gk
+      job_id: jqpyzm28g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-14T23:28:16.047386Z'
+    timestamp: '2024-04-02T15:27:43.907101Z'
     torchscript_onnx_qnn:
-      inference_time: 7039.0
-      throughput: 142.06563432305725
+      inference_time: 5316.0
+      throughput: 188.11136192626034
       estimated_peak_memory_range:
-        min: 4984832
-        max: 18803744
+        min: 4935680
+        max: 19108344
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 294
+        layers_on_npu: 293
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 294
-      job_id: joprk4750
+        total_layers: 293
+      job_id: j1p821rkp
       job_status: Passed
   - torchscript_onnx_tflite:
-      inference_time: 6502.0
-      throughput: 153.79883112888342
+      inference_time: 4320.0
+      throughput: 231.4814814814815
       estimated_peak_memory_range:
-        min: 24576
-        max: 83870080
+        min: 73728
+        max: 88723920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 300
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 300
-      job_id: jegn29qgo
+      job_id: j2p04699g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-14T23:30:19.085764Z'
+    timestamp: '2024-04-02T15:30:24.719256Z'
     torchscript_onnx_qnn:
-      inference_time: 4840.0
-      throughput: 206.61157024793388
+      inference_time: 3677.0
+      throughput: 271.9608376393799
       estimated_peak_memory_range:
-        min: 4947968
-        max: 123420640
+        min: 4931584
+        max: 110753456
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 294
+        layers_on_npu: 293
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 294
-      job_id: jep287qp6
+        total_layers: 293
+      job_id: jogkv80wp
       job_status: Passed
```

## qai_hub_models/models/yolov8_seg/export.py

```diff
@@ -189,14 +189,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov8_seg/perf.yaml

```diff
@@ -15,48 +15,49 @@
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
+  - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
 models:
 - name: YOLOv8-Segmentation
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 10665.0
-      throughput: 93.76465072667604
+      inference_time: 7056.0
+      throughput: 141.7233560090703
       estimated_peak_memory_range:
-        min: 4616192
-        max: 6990768
+        min: 4612096
+        max: 14526392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 337
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 337
-      job_id: j1glnxepv
+      job_id: jn5q0v1np
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-03-15T00:08:48.972058Z'
+    timestamp: '2024-04-02T15:25:01.845719Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
@@ -65,36 +66,36 @@
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
       job_id: ''
       job_status: Skipped
   - torchscript_onnx_tflite:
-      inference_time: 7417.0
-      throughput: 134.8254011055683
+      inference_time: 5151.0
+      throughput: 194.1370607649
       estimated_peak_memory_range:
-        min: 53248
-        max: 91611328
+        min: 16384
+        max: 98489488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 337
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 337
-      job_id: jw5667v5o
+      job_id: j1gl4l8j5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-03-15T00:08:48.972071Z'
+    timestamp: '2024-04-02T15:25:01.845733Z'
     torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
```

## qai_hub_models/test/test_async_compile_jobs.py

```diff
@@ -1,27 +1,27 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
 
 import qai_hub as hub
-import yaml
+
+from qai_hub_models.utils.asset_loaders import load_yaml
 
 
 def test_compile_jobs_success():
     """
     When testing compilation in CI, synchronously waiting for each compile_job to
     finish is too slow. Instead, job ids are written to a file upon submission,
     and success is validated all at once in the end using this test.
     """
     if os.stat(os.environ["COMPILE_JOBS_FILE"]).st_size == 0:
         return
-    with open(os.environ["COMPILE_JOBS_FILE"], "r") as f:
-        job_ids = yaml.safe_load(f.read())
+    job_ids = load_yaml(os.environ["COMPILE_JOBS_FILE"])
     failed_jobs = {}
     for name, job_id in job_ids.items():
         result = hub.get_job(job_id).wait()
         if not result.success:
             failed_jobs[name] = job_id
     if failed_jobs:
         raise ValueError(f"The following jobs failed to compile: {failed_jobs}")
```

## qai_hub_models/utils/args.py

```diff
@@ -163,28 +163,61 @@
     if not parser:
         parser = get_parser()
 
     from_pretrained_sig = inspect.signature(cls.from_pretrained)
     for name, param in from_pretrained_sig.parameters.items():
         if name == "cls":
             continue
+
+        help = (
+            f"For documentation, see {cls.__name__}::from_pretrained::parameter {name}."
+        )
+
         # Determining type from param.annotation is non-trivial (it can be a
         # strings like "Optional[str]" or "bool | None").
+        bool_action = None
+        arg_name = f"--{name.replace('_', '-')}"
         if param.default is not None:
             type_ = type(param.default)
+
+            if type_ == bool:
+                if param.default:
+                    bool_action = "store_false"
+                    # If the default is true, and the arg name does not start with no_,
+                    # then add the no- to the argument (as it should be passed as --no-enable-flag, not --enable-flag)
+                    if name.startswith("no_"):
+                        arg_name = f"--{name[3:].replace('_', '-')}"
+                    else:
+                        arg_name = f"--no-{name.replace('_', '-')}"
+                    help = (
+                        f"{help} Setting this flag will set parameter {name} to False."
+                    )
+                else:
+                    bool_action = "store_true"
+                    # If the default is false, and the arg name starts with no_,
+                    # then remove the no- from the argument (as it should be passed as --enable-flag, not --no-enable-flag)
+                    arg_name = f"--{name.replace('_', '-')}"
+                    help = (
+                        f"{help} Setting this flag will set parameter {name} to True."
+                    )
         elif param.annotation == "bool":
             type_ = bool
         else:
             type_ = str
-        parser.add_argument(
-            f"--{name.replace('_', '-')}",
-            type=type_,
-            default=param.default,
-            help=f"For documentation, see {cls.__name__}::from_pretrained.",
-        )
+
+        if bool_action:
+            parser.add_argument(arg_name, dest=name, action=bool_action, help=help)
+        else:
+            parser.add_argument(
+                arg_name,
+                dest=name,
+                type=type_,
+                default=param.default,
+                help=help,
+            )
     return parser
 
 
 def get_model_kwargs(
     model_cls: Type[FromPretrainedTypeVar], args_dict: Mapping[str, Any]
 ) -> Mapping[str, Any]:
     """
@@ -244,14 +277,15 @@
             export_output = export_module.export_model(
                 device=device.name,
                 skip_profiling=True,
                 skip_inferencing=True,
                 skip_downloading=True,
                 skip_summary=True,
                 target_runtime=cli_args.target_runtime,
+                **get_model_kwargs(model_cls, vars(cli_args)),
             )
 
             if len(export_output) == 0 or isinstance(export_output[0], str):
                 # The export returned local file paths, which mean Hub credentials were not found.
                 raise NotImplementedError(
                     f"Please sign-up for {_AIHUB_NAME} to continue the demo with on-device inference."
                 )
@@ -330,28 +364,32 @@
     return model.get_input_spec(**get_input_spec_kwargs(model, vars(cli_args)))
 
 
 def export_parser(
     model_cls: Type[FromPretrainedTypeVar] | Type[FromPrecompiledTypeVar],
     components: Optional[List[str]] = None,
     supports_qnn=True,
+    supports_ort=True,
     exporting_compiled_model=False,
 ) -> argparse.ArgumentParser:
     """
     Arg parser to be used in export scripts.
 
     Parameters:
         model_cls: Class of the model to be exported. Used to add additional
             args for model instantiation.
         components: Some models have multiple components that need to be
             compiled separately. This represents the list of options for the user to
             select which components they want to compile.
         supports_qnn:
             Whether QNN export is supported.
             Default=True.
+        supports_ort:
+            Whether ORT export is supported.
+            Default=True.
         exporting_compiled_model:
             True when exporting compiled model.
             If set, removing skip_profiling flag from export arguments.
             Default = False.
 
     Returns:
         Arg parser object.
```

## qai_hub_models/utils/asset_loaders.py

```diff
@@ -63,255 +63,14 @@
     try:
         logger.setLevel(log_level)
         yield
     finally:
         logger.setLevel(old_level)
 
 
-class QAIHM_WEB_ASSET(Enum):
-    STATIC_IMG = 0
-    ANIMATED_MOV = 1
-
-
-class ModelZooAssetConfig:
-    def __init__(
-        self,
-        asset_url: str,
-        web_asset_folder: str,
-        static_web_banner_filename: str,
-        animated_web_banner_filename: str,
-        model_asset_folder: str,
-        dataset_asset_folder: str,
-        local_store_path: str,
-        qaihm_repo: str,
-        example_use: str,
-        huggingface_path: str,
-        repo_url: str,
-        models_website_url: str,
-        models_website_relative_path: str,
-    ) -> None:
-        self.local_store_path = local_store_path
-        self.asset_url = asset_url
-        self.web_asset_folder = web_asset_folder
-        self.static_web_banner_filename = static_web_banner_filename
-        self.animated_web_banner_filename = animated_web_banner_filename
-        self.model_asset_folder = model_asset_folder
-        self.dataset_asset_folder = dataset_asset_folder
-        self.qaihm_repo = qaihm_repo
-        self.example_use = example_use
-        self.huggingface_path = huggingface_path
-        self.repo_url = repo_url
-        self.models_website_url = models_website_url
-        self.models_website_relative_path = models_website_relative_path
-
-        # Validation
-        for name in [
-            self.asset_url,
-            self.web_asset_folder,
-            self.model_asset_folder,
-            self.static_web_banner_filename,
-            self.animated_web_banner_filename,
-            self.local_store_path,
-            self.qaihm_repo,
-            self.example_use,
-            self.huggingface_path,
-            self.models_website_relative_path,
-        ]:
-            assert not name.endswith("/") and not name.endswith("\\")
-        for name in [
-            self.static_web_banner_filename,
-            self.animated_web_banner_filename,
-        ]:
-            assert not name.startswith("/") and not name.startswith("\\")
-
-        for name in [self.repo_url, self.models_website_url]:
-            assert not name.endswith("/"), "URLs should not end with a slash"
-
-    def get_hugging_face_url(self, model_name: str) -> str:
-        return f"https://huggingface.co/{self.get_huggingface_path(model_name)}"
-
-    def get_huggingface_path(self, model_name: str) -> str:
-        return self.huggingface_path.replace("{model_name}", str(model_name))
-
-    def get_web_asset_url(self, model_id: str, type: QAIHM_WEB_ASSET):
-        if type == QAIHM_WEB_ASSET.STATIC_IMG:
-            file = self.static_web_banner_filename
-        elif type == QAIHM_WEB_ASSET.ANIMATED_MOV:
-            file = self.animated_web_banner_filename
-        else:
-            raise NotImplementedError("unsupported web asset type")
-        return f"{self.asset_url}/{ModelZooAssetConfig._replace_path_keywords(self.web_asset_folder, model_id=model_id)}/{file}"
-
-    def get_local_store_model_path(
-        self, model_name: str, version: VersionType, filename: str
-    ) -> str:
-        model_dir = os.path.join(
-            self.local_store_path,
-            self.get_relative_model_asset_path(model_name, version, filename),
-        )
-        return model_dir
-
-    def get_local_store_dataset_path(
-        self, dataset_name: str, version: VersionType, filename: str
-    ) -> str:
-        model_dir = os.path.join(
-            self.local_store_path,
-            self.get_relative_dataset_asset_path(dataset_name, version, filename),
-        )
-        return model_dir
-
-    def get_relative_model_asset_path(
-        self, model_id: str, version: Union[int, str], file_name: str
-    ):
-        assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{ModelZooAssetConfig._replace_path_keywords(self.model_asset_folder, model_id=model_id, version=version)}/{file_name}"
-
-    def get_relative_dataset_asset_path(
-        self, dataset_id: str, version: Union[int, str], file_name: str
-    ):
-        assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{ModelZooAssetConfig._replace_path_keywords(self.dataset_asset_folder, dataset_id=dataset_id, version=version)}/{file_name}"
-
-    def get_model_asset_url(
-        self, model_id: str, version: Union[int, str], file_name: str
-    ):
-        assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{self.asset_url}/{self.get_relative_model_asset_path(model_id, version, file_name)}"
-
-    def get_dataset_asset_url(
-        self, dataset_id: str, version: Union[int, str], file_name: str
-    ):
-        assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{self.asset_url}/{self.get_relative_dataset_asset_path(dataset_id, version, file_name)}"
-
-    def get_qaihm_repo(self, model_id: str, relative=True):
-        relative_path = f"{ModelZooAssetConfig._replace_path_keywords(self.qaihm_repo, model_id=model_id)}"
-        if not relative:
-            return self.repo_url + "/" + relative_path
-
-        return relative_path
-
-    def get_website_url(self, model_id: str, relative=False):
-        relative_path = f"{ModelZooAssetConfig._replace_path_keywords(self.models_website_relative_path, model_id=model_id)}"
-        if not relative:
-            return self.models_website_url + "/" + relative_path
-        return relative_path
-
-    def get_example_use(self, model_id: str):
-        return f"{ModelZooAssetConfig._replace_path_keywords(self.example_use, model_id=model_id)}"
-
-    ###
-    # Helpers
-    ###
-    @staticmethod
-    def _replace_path_keywords(
-        path: str,
-        model_id: Optional[str] = None,
-        dataset_id: Optional[str] = None,
-        version: Optional[Union[int, str]] = None,
-    ):
-        if model_id:
-            path = path.replace("{model_id}", model_id)
-        if dataset_id:
-            path = path.replace("{dataset_id}", dataset_id)
-        if version:
-            path = path.replace("{version}", str(version))
-        return path
-
-    ###
-    # Load from CFG
-    ###
-    @staticmethod
-    def from_cfg(
-        asset_cfg_path: str = ASSET_BASES_DEFAULT_PATH,
-        local_store_path: str = LOCAL_STORE_DEFAULT_PATH,
-        verify_env_has_all_variables: bool = False,
-    ):
-        # Load CFG and params
-        asset_cfg = ModelZooAssetConfig.load_asset_cfg(
-            asset_cfg_path, verify_env_has_all_variables
-        )
-
-        return ModelZooAssetConfig(
-            asset_cfg["store_url"],
-            asset_cfg["web_asset_folder"],
-            asset_cfg["static_web_banner_filename"],
-            asset_cfg["animated_web_banner_filename"],
-            asset_cfg["model_asset_folder"],
-            asset_cfg["dataset_asset_folder"],
-            local_store_path,
-            asset_cfg["qaihm_repo"],
-            asset_cfg["example_use"],
-            asset_cfg["huggingface_path"],
-            asset_cfg["repo_url"],
-            asset_cfg["models_website_url"],
-            asset_cfg["models_website_relative_path"],
-        )
-
-    ASSET_CFG_SCHEMA = Schema(
-        And(
-            {
-                "store_url": str,
-                "web_asset_folder": str,
-                "dataset_asset_folder": str,
-                "static_web_banner_filename": str,
-                "animated_web_banner_filename": str,
-                "model_asset_folder": str,
-                "qaihm_repo": str,
-                "example_use": str,
-                "huggingface_path": str,
-                "repo_url": str,
-                "models_website_url": str,
-                "models_website_relative_path": str,
-            }
-        )
-    )
-
-    @staticmethod
-    def load_asset_cfg(path, verify_env_has_all_variables: bool = False):
-        with open(path) as f:
-            data = yaml.safe_load(f)
-            try:
-                # Validate high level-schema
-                ModelZooAssetConfig.ASSET_CFG_SCHEMA.validate(data)
-            except SchemaError as e:
-                assert 0, f"{e.code} in {path}"
-
-            for key, value in data.items():
-                # Environment variable replacement
-                if isinstance(value, str) and value.startswith("env::"):
-                    values = value.split("::")
-                    if len(values) == 2:
-                        _, env_var_name = values
-                        default = value
-                    elif len(values) == 3:
-                        _, env_var_name, default = values
-                    else:
-                        raise NotImplementedError(
-                            "Environment vars should be specified in asset_bases "
-                            "using format env::<var_name>::<default>"
-                        )
-
-                    data[key] = os.environ.get(env_var_name, default)
-                    if (
-                        verify_env_has_all_variables
-                        and default == value
-                        and env_var_name not in os.environ
-                    ):
-                        raise ValueError(
-                            f"Environment variable '{env_var_name}' was specified in "
-                            f"asset_bases.yaml for key '{key}', but is not defined."
-                        )
-
-            return data
-
-
-ASSET_CONFIG = ModelZooAssetConfig.from_cfg()
-
-
 def _query_yes_no(question, default="yes"):
     """
     Ask a yes/no question and return their answer.
 
     "question" is a string that is presented to the user.
     "default" is the presumed answer if the user just hits <Enter>.
             It must be "yes" (the default), "no" or None (meaning
@@ -533,14 +292,254 @@
             inplace=True,
             backup=".bak",
         ) as file:
             for line in file:
                 print(line.replace(find_str, replace_str), end="")
 
 
+class QAIHM_WEB_ASSET(Enum):
+    STATIC_IMG = 0
+    ANIMATED_MOV = 1
+
+
+class ModelZooAssetConfig:
+    def __init__(
+        self,
+        asset_url: str,
+        web_asset_folder: str,
+        static_web_banner_filename: str,
+        animated_web_banner_filename: str,
+        model_asset_folder: str,
+        dataset_asset_folder: str,
+        local_store_path: str,
+        qaihm_repo: str,
+        example_use: str,
+        huggingface_path: str,
+        repo_url: str,
+        models_website_url: str,
+        models_website_relative_path: str,
+    ) -> None:
+        self.local_store_path = local_store_path
+        self.asset_url = asset_url
+        self.web_asset_folder = web_asset_folder
+        self.static_web_banner_filename = static_web_banner_filename
+        self.animated_web_banner_filename = animated_web_banner_filename
+        self.model_asset_folder = model_asset_folder
+        self.dataset_asset_folder = dataset_asset_folder
+        self.qaihm_repo = qaihm_repo
+        self.example_use = example_use
+        self.huggingface_path = huggingface_path
+        self.repo_url = repo_url
+        self.models_website_url = models_website_url
+        self.models_website_relative_path = models_website_relative_path
+
+        # Validation
+        for name in [
+            self.asset_url,
+            self.web_asset_folder,
+            self.model_asset_folder,
+            self.static_web_banner_filename,
+            self.animated_web_banner_filename,
+            self.local_store_path,
+            self.qaihm_repo,
+            self.example_use,
+            self.huggingface_path,
+            self.models_website_relative_path,
+        ]:
+            assert not name.endswith("/") and not name.endswith("\\")
+        for name in [
+            self.static_web_banner_filename,
+            self.animated_web_banner_filename,
+        ]:
+            assert not name.startswith("/") and not name.startswith("\\")
+
+        for name in [self.repo_url, self.models_website_url]:
+            assert not name.endswith("/"), "URLs should not end with a slash"
+
+    def get_hugging_face_url(self, model_name: str) -> str:
+        return f"https://huggingface.co/{self.get_huggingface_path(model_name)}"
+
+    def get_huggingface_path(self, model_name: str) -> str:
+        return self.huggingface_path.replace("{model_name}", str(model_name))
+
+    def get_web_asset_url(self, model_id: str, type: QAIHM_WEB_ASSET):
+        if type == QAIHM_WEB_ASSET.STATIC_IMG:
+            file = self.static_web_banner_filename
+        elif type == QAIHM_WEB_ASSET.ANIMATED_MOV:
+            file = self.animated_web_banner_filename
+        else:
+            raise NotImplementedError("unsupported web asset type")
+        return f"{self.asset_url}/{ModelZooAssetConfig._replace_path_keywords(self.web_asset_folder, model_id=model_id)}/{file}"
+
+    def get_local_store_model_path(
+        self, model_name: str, version: VersionType, filename: str
+    ) -> str:
+        model_dir = os.path.join(
+            self.local_store_path,
+            self.get_relative_model_asset_path(model_name, version, filename),
+        )
+        return model_dir
+
+    def get_local_store_dataset_path(
+        self, dataset_name: str, version: VersionType, filename: str
+    ) -> str:
+        model_dir = os.path.join(
+            self.local_store_path,
+            self.get_relative_dataset_asset_path(dataset_name, version, filename),
+        )
+        return model_dir
+
+    def get_relative_model_asset_path(
+        self, model_id: str, version: Union[int, str], file_name: str
+    ):
+        assert not file_name.startswith("/") and not file_name.startswith("\\")
+        return f"{ModelZooAssetConfig._replace_path_keywords(self.model_asset_folder, model_id=model_id, version=version)}/{file_name}"
+
+    def get_relative_dataset_asset_path(
+        self, dataset_id: str, version: Union[int, str], file_name: str
+    ):
+        assert not file_name.startswith("/") and not file_name.startswith("\\")
+        return f"{ModelZooAssetConfig._replace_path_keywords(self.dataset_asset_folder, dataset_id=dataset_id, version=version)}/{file_name}"
+
+    def get_model_asset_url(
+        self, model_id: str, version: Union[int, str], file_name: str
+    ):
+        assert not file_name.startswith("/") and not file_name.startswith("\\")
+        return f"{self.asset_url}/{self.get_relative_model_asset_path(model_id, version, file_name)}"
+
+    def get_dataset_asset_url(
+        self, dataset_id: str, version: Union[int, str], file_name: str
+    ):
+        assert not file_name.startswith("/") and not file_name.startswith("\\")
+        return f"{self.asset_url}/{self.get_relative_dataset_asset_path(dataset_id, version, file_name)}"
+
+    def get_qaihm_repo(self, model_id: str, relative=True):
+        relative_path = f"{ModelZooAssetConfig._replace_path_keywords(self.qaihm_repo, model_id=model_id)}"
+        if not relative:
+            return self.repo_url + "/" + relative_path
+
+        return relative_path
+
+    def get_website_url(self, model_id: str, relative=False):
+        relative_path = f"{ModelZooAssetConfig._replace_path_keywords(self.models_website_relative_path, model_id=model_id)}"
+        if not relative:
+            return self.models_website_url + "/" + relative_path
+        return relative_path
+
+    def get_example_use(self, model_id: str):
+        return f"{ModelZooAssetConfig._replace_path_keywords(self.example_use, model_id=model_id)}"
+
+    ###
+    # Helpers
+    ###
+    @staticmethod
+    def _replace_path_keywords(
+        path: str,
+        model_id: Optional[str] = None,
+        dataset_id: Optional[str] = None,
+        version: Optional[Union[int, str]] = None,
+    ):
+        if model_id:
+            path = path.replace("{model_id}", model_id)
+        if dataset_id:
+            path = path.replace("{dataset_id}", dataset_id)
+        if version:
+            path = path.replace("{version}", str(version))
+        return path
+
+    ###
+    # Load from CFG
+    ###
+    @staticmethod
+    def from_cfg(
+        asset_cfg_path: str = ASSET_BASES_DEFAULT_PATH,
+        local_store_path: str = LOCAL_STORE_DEFAULT_PATH,
+        verify_env_has_all_variables: bool = False,
+    ):
+        # Load CFG and params
+        asset_cfg = ModelZooAssetConfig.load_asset_cfg(
+            asset_cfg_path, verify_env_has_all_variables
+        )
+
+        return ModelZooAssetConfig(
+            asset_cfg["store_url"],
+            asset_cfg["web_asset_folder"],
+            asset_cfg["static_web_banner_filename"],
+            asset_cfg["animated_web_banner_filename"],
+            asset_cfg["model_asset_folder"],
+            asset_cfg["dataset_asset_folder"],
+            local_store_path,
+            asset_cfg["qaihm_repo"],
+            asset_cfg["example_use"],
+            asset_cfg["huggingface_path"],
+            asset_cfg["repo_url"],
+            asset_cfg["models_website_url"],
+            asset_cfg["models_website_relative_path"],
+        )
+
+    ASSET_CFG_SCHEMA = Schema(
+        And(
+            {
+                "store_url": str,
+                "web_asset_folder": str,
+                "dataset_asset_folder": str,
+                "static_web_banner_filename": str,
+                "animated_web_banner_filename": str,
+                "model_asset_folder": str,
+                "qaihm_repo": str,
+                "example_use": str,
+                "huggingface_path": str,
+                "repo_url": str,
+                "models_website_url": str,
+                "models_website_relative_path": str,
+            }
+        )
+    )
+
+    @staticmethod
+    def load_asset_cfg(path, verify_env_has_all_variables: bool = False):
+        data = load_yaml(path)
+        try:
+            # Validate high level-schema
+            ModelZooAssetConfig.ASSET_CFG_SCHEMA.validate(data)
+        except SchemaError as e:
+            assert 0, f"{e.code} in {path}"
+
+        for key, value in data.items():
+            # Environment variable replacement
+            if isinstance(value, str) and value.startswith("env::"):
+                values = value.split("::")
+                if len(values) == 2:
+                    _, env_var_name = values
+                    default = value
+                elif len(values) == 3:
+                    _, env_var_name, default = values
+                else:
+                    raise NotImplementedError(
+                        "Environment vars should be specified in asset_bases "
+                        "using format env::<var_name>::<default>"
+                    )
+
+                data[key] = os.environ.get(env_var_name, default)
+                if (
+                    verify_env_has_all_variables
+                    and default == value
+                    and env_var_name not in os.environ
+                ):
+                    raise ValueError(
+                        f"Environment variable '{env_var_name}' was specified in "
+                        f"asset_bases.yaml for key '{key}', but is not defined."
+                    )
+
+        return data
+
+
+ASSET_CONFIG = ModelZooAssetConfig.from_cfg()
+
+
 class CachedWebAsset:
     """
     Helper class for downloading files for storage in the QAIHM asset cache.
     """
 
     def __init__(
         self,
```

## qai_hub_models/utils/base_model.py

```diff
@@ -14,16 +14,16 @@
     SampleInputsType,
     SourceModelFormat,
     TargetRuntime,
 )
 from qai_hub_models.models.protocols import (
     ExecutableModelProtocol,
     FromPrecompiledProtocol,
-    FromPretrainedProtocol,
     HubModelProtocol,
+    PretrainedHubModelProtocol,
 )
 from qai_hub_models.utils.input_spec import InputSpec, make_torch_inputs
 
 
 class CollectionModel:
     """
     Model that glues together several BaseModels
@@ -34,23 +34,23 @@
 
 class HubModel(HubModelProtocol):
     """
     Base interface for AI Hub models.
     """
 
     def __init__(self):
-        # Change self.get_input_spec() to call _get_input_spec_for_model_instance() instead.
+        # Change self.get_input_spec() to call _get_input_spec_for_instance() instead.
         #
-        # _get_input_spec_for_model_instance() is an override that allows get_input_spec()
+        # _get_input_spec_for_instance() is an override that allows get_input_spec()
         # to access instance variables. This may be used in case input shape is "hard-coded"
         # based on parameters passed to the model upon initialization.
         #
-        self.get_input_spec = self._get_input_spec_for_model_instance
+        self.get_input_spec = self._get_input_spec_for_instance
 
-    def _get_input_spec_for_model_instance(self, *args, **kwargs) -> InputSpec:
+    def _get_input_spec_for_instance(self, *args, **kwargs) -> InputSpec:
         """
         Get the input specifications for an instance of this model.
 
         Typically this will pre-fill inputs of get_input_spec
         with values determined by instance members of the model class.
 
         The initializer for BaseModel will automatically override get_input_spec
@@ -89,15 +89,15 @@
         """
         return other_profile_options
 
 
 class BaseModel(
     torch.nn.Module,
     HubModel,
-    FromPretrainedProtocol,
+    PretrainedHubModelProtocol,
     ExecutableModelProtocol,
 ):
     """
     A pre-trained PyTorch model with helpers for submission to AI Hub.
     """
 
     def __init__(self):
@@ -150,14 +150,16 @@
     ) -> str:
         """
         AI Hub compile options recommended for the model.
         """
         compile_options = ""
         if target_runtime == TargetRuntime.QNN:
             compile_options = "--target_runtime qnn_lib_aarch64_android"
+        if target_runtime == TargetRuntime.ORT:
+            compile_options = "--target_runtime onnx"
         if other_compile_options != "":
             return compile_options + " " + other_compile_options
         return compile_options
 
     def preferred_hub_source_model_format(
         self, target_runtime: TargetRuntime
     ) -> SourceModelFormat:
```

## qai_hub_models/utils/config_loaders.py

```diff
@@ -4,24 +4,23 @@
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
 from dataclasses import dataclass
 from enum import Enum
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
 import requests
-import yaml
 from qai_hub.util.session import create_session
 from schema import And
 from schema import Optional as OptionalSchema
 from schema import Schema, SchemaError
 
-from qai_hub_models.utils.asset_loaders import ASSET_CONFIG, QAIHM_WEB_ASSET
+from qai_hub_models.utils.asset_loaders import ASSET_CONFIG, QAIHM_WEB_ASSET, load_yaml
 from qai_hub_models.utils.base_model import TargetRuntime
 from qai_hub_models.utils.path_helpers import (
     MODELS_PACKAGE_NAME,
     QAIHM_PACKAGE_NAME,
     get_qaihm_models_root,
     get_qaihm_package_root,
 )
@@ -244,46 +243,43 @@
         self.skip_qnn = False
         self.tflite_row = (
             "| Samsung Galaxy S23 Ultra (Android 13) | Snapdragon 8 Gen 2 |"
         )
         self.qnn_row = "| Samsung Galaxy S23 Ultra (Android 13) | Snapdragon 8 Gen 2 |"
 
         if os.path.exists(self.perf_yaml_path):
-            with open(self.perf_yaml_path, "r") as perf_file:
-                self.perf_details = yaml.safe_load(perf_file)
-                num_models = len(self.perf_details["models"])
-
-                # Get TFLite summary from perf.yaml
-                try:
-                    self.tflite_summary = []
-                    for model in self.perf_details["models"]:
-                        self.tflite_summary.append(
-                            model["performance_metrics"][0][TFLITE_PATH]
-                        )
-                except Exception:
-                    self.skip_tflite = True
-
-                if not self.skip_overall and not self.skip_tflite:
-                    for num in range(num_models):
-                        if isinstance(self.tflite_summary[num]["inference_time"], str):
-                            self.skip_tflite = True
-
-                # Get QNN summary from perf.yaml
-                try:
-                    self.qnn_summary = []
-                    for model in self.perf_details["models"]:
-                        self.qnn_summary.append(
-                            model["performance_metrics"][0][QNN_PATH]
-                        )
-                except Exception:
-                    self.skip_qnn = True
-                if not self.skip_overall and not self.skip_qnn:
-                    for num in range(num_models):
-                        if isinstance(self.qnn_summary[num]["inference_time"], str):
-                            self.skip_qnn = True
+            self.perf_details = load_yaml(self.perf_yaml_path)
+            num_models = len(self.perf_details["models"])
+
+            # Get TFLite summary from perf.yaml
+            try:
+                self.tflite_summary = []
+                for model in self.perf_details["models"]:
+                    self.tflite_summary.append(
+                        model["performance_metrics"][0][TFLITE_PATH]
+                    )
+            except Exception:
+                self.skip_tflite = True
+
+            if not self.skip_overall and not self.skip_tflite:
+                for num in range(num_models):
+                    if isinstance(self.tflite_summary[num]["inference_time"], str):
+                        self.skip_tflite = True
+
+            # Get QNN summary from perf.yaml
+            try:
+                self.qnn_summary = []
+                for model in self.perf_details["models"]:
+                    self.qnn_summary.append(model["performance_metrics"][0][QNN_PATH])
+            except Exception:
+                self.skip_qnn = True
+            if not self.skip_overall and not self.skip_qnn:
+                for num in range(num_models):
+                    if isinstance(self.qnn_summary[num]["inference_time"], str):
+                        self.skip_qnn = True
         else:
             self.skip_overall = True
 
     def _get_runtime_type(self, model_type):
         if model_type == "tflite":
             return "TFLite"
         if model_type == "so":
@@ -472,14 +468,137 @@
 
             if name not in perf_details.keys():
                 perf_details[name] = None
 
         return perf_details
 
 
+class QAIHMModelCodeGen:
+    def __init__(
+        self,
+        is_aimet: bool,
+        has_on_target_demo: bool,
+        qnn_export_failure_reason: str,
+        tflite_export_failure_reason: str,
+        has_demo: bool,
+        check_trace: bool,
+        channel_last_input: List[str],
+        channel_last_output: List[str],
+        outputs_to_skip_validation: List[str],
+        export_test_model_kwargs: Dict[str, str],
+        components: Dict[str, Any],
+        default_components: List[str],
+        skip_tests: bool,
+        is_precompiled: bool,
+        no_assets: bool,
+        global_requirements_incompatible: bool,
+        torchscript_opt: List[str],
+        inference_metrics: str,
+        supports_ort: bool,
+    ) -> None:
+        self.is_aimet = is_aimet
+        self.has_on_target_demo = has_on_target_demo
+        self.qnn_export_failure_reason = qnn_export_failure_reason
+        self.tflite_export_failure_reason = tflite_export_failure_reason
+        self.has_demo = has_demo
+        self.check_trace = check_trace
+        self.channel_last_input = channel_last_input
+        self.channel_last_output = channel_last_output
+        self.outputs_to_skip_validation = outputs_to_skip_validation
+        self.export_test_model_kwargs = export_test_model_kwargs
+        self.components = components
+        self.default_components = default_components
+        self.skip_tests = skip_tests
+        self.is_precompiled = is_precompiled
+        self.no_assets = no_assets
+        self.global_requirements_incompatible = global_requirements_incompatible
+        self.torchscript_opt = torchscript_opt
+        self.inference_metrics = inference_metrics
+        self.supports_ort = supports_ort
+
+    def validate(self) -> Tuple[bool, Optional[str]]:
+        """Returns false with a reason if the info spec for this model is not valid."""
+        return True, None
+
+    @classmethod
+    def from_model(cls: Type[QAIHMModelCodeGen], model_id: str) -> QAIHMModelCodeGen:
+        code_gen_path = QAIHM_MODELS_ROOT / model_id / "code-gen.yaml"
+        if not os.path.exists(code_gen_path):
+            raise ValueError(f"{model_id} does not exist")
+        return cls.from_yaml(code_gen_path)
+
+    @classmethod
+    def from_yaml(
+        cls: Type[QAIHMModelCodeGen], code_gen_path: str | Path | None = None
+    ) -> QAIHMModelCodeGen:
+        # Load CFG and params
+        code_gen_config = QAIHMModelCodeGen.load_code_gen_yaml(code_gen_path)
+        return cls(
+            code_gen_config["is_aimet"],
+            code_gen_config["has_on_target_demo"],
+            code_gen_config["qnn_export_failure_reason"],
+            code_gen_config["tflite_export_failure_reason"],
+            code_gen_config["has_demo"],
+            code_gen_config["check_trace"],
+            code_gen_config["channel_last_input"],
+            code_gen_config["channel_last_output"],
+            code_gen_config["outputs_to_skip_validation"],
+            code_gen_config["export_test_model_kwargs"],
+            code_gen_config["components"],
+            code_gen_config["default_components"],
+            code_gen_config["skip_tests"],
+            code_gen_config["is_precompiled"],
+            code_gen_config["no_assets"],
+            code_gen_config["global_requirements_incompatible"],
+            code_gen_config["torchscript_opt"],
+            code_gen_config["inference_metrics"],
+            code_gen_config["supports_ort"],
+        )
+
+    # Schema for code-gen.yaml
+    CODE_GEN_YAML_SCHEMA = Schema(
+        And(
+            {
+                OptionalSchema("has_components", default=""): str,
+                OptionalSchema("is_aimet", default=False): bool,
+                OptionalSchema("has_on_target_demo", default=False): bool,
+                OptionalSchema("qnn_export_failure_reason", default=""): str,
+                OptionalSchema("tflite_export_failure_reason", default=""): str,
+                OptionalSchema("has_demo", default=True): bool,
+                OptionalSchema("check_trace", default=True): bool,
+                OptionalSchema("channel_last_input", default=[]): list,
+                OptionalSchema("channel_last_output", default=[]): list,
+                OptionalSchema("outputs_to_skip_validation", default=[]): list,
+                OptionalSchema("export_test_model_kwargs", default={}): dict,
+                OptionalSchema("components", default={}): dict,
+                OptionalSchema("default_components", default=[]): list,
+                OptionalSchema("skip_tests", default=False): bool,
+                OptionalSchema("is_precompiled", default=False): bool,
+                OptionalSchema("no_assets", default=False): bool,
+                OptionalSchema("global_requirements_incompatible", default=False): bool,
+                OptionalSchema("torchscript_opt", default=[]): list,
+                OptionalSchema("inference_metrics", default="psnr"): str,
+                OptionalSchema("supports_ort", default=False): bool,
+            }
+        )
+    )
+
+    @staticmethod
+    def load_code_gen_yaml(path: str | Path | None = None):
+        if not path or not os.path.exists(path):
+            return QAIHMModelCodeGen.CODE_GEN_YAML_SCHEMA.validate({})  # Default Schema
+        data = load_yaml(path)
+        try:
+            # Validate high level-schema
+            data = QAIHMModelCodeGen.CODE_GEN_YAML_SCHEMA.validate(data)
+        except SchemaError as e:
+            assert 0, f"{e.code} in {path}"
+        return data
+
+
 class QAIHMModelInfo:
     def __init__(
         self,
         name: str,
         id: str,
         status: MODEL_STATUS,
         status_reason: str | None,
@@ -494,15 +613,15 @@
         deploy_license: str,
         source_repo: str,
         applicable_scenarios: List[str],
         related_models: List[str],
         form_factors: List[FORM_FACTOR],
         has_static_banner: bool,
         has_animated_banner: bool,
-        code_gen_config: Dict[str, str | bool],
+        code_gen_config: QAIHMModelCodeGen,
         license_type: str,
         deploy_license_type: str,
         dataset: List[str],
         technical_details: Dict[str, str],
     ) -> None:
         self.name = name
         self.id = id
@@ -593,17 +712,18 @@
             )
 
         # Required assets exist
         if self.status == MODEL_STATUS.PUBLIC:
             if not os.path.exists(self.get_package_path() / "info.yaml"):
                 return False, "All public models must have an info.yaml"
 
-            if self.code_gen_config.get(
-                "tflite_export_failure_reason", False
-            ) and self.code_gen_config.get("qnn_export_failure_reason", False):
+            if (
+                self.code_gen_config.tflite_export_failure_reason
+                and self.code_gen_config.qnn_export_failure_reason
+            ):
                 return False, "Public models must support at least one export path"
 
         session = create_session()
         if self.has_static_banner:
             static_banner_url = ASSET_CONFIG.get_web_asset_url(
                 self.id, QAIHM_WEB_ASSET.STATIC_IMG
             )
@@ -684,28 +804,31 @@
 
     def get_requirements_path(self, root: Path = QAIHM_PACKAGE_ROOT):
         return self.get_package_path(root) / "requirements.txt"
 
     def has_model_requirements(self, root: Path = QAIHM_PACKAGE_ROOT):
         return os.path.exists(self.get_requirements_path(root))
 
-    @staticmethod
-    def from_model(model_id: str):
+    @classmethod
+    def from_model(cls: Type[QAIHMModelInfo], model_id: str) -> QAIHMModelInfo:
         schema_path = QAIHM_MODELS_ROOT / model_id / "info.yaml"
         code_gen_path = QAIHM_MODELS_ROOT / model_id / "code-gen.yaml"
         if not os.path.exists(schema_path):
             raise ValueError(f"{model_id} does not exist")
-        return QAIHMModelInfo.from_yaml(schema_path, code_gen_path)
+        return cls.from_yaml(schema_path, code_gen_path)
 
-    @staticmethod
-    def from_yaml(info_path: str | Path, code_gen_path: str | Path | None = None):
+    @classmethod
+    def from_yaml(
+        cls: Type[QAIHMModelInfo],
+        info_path: str | Path,
+        code_gen_path: str | Path | None = None,
+    ) -> QAIHMModelInfo:
         # Load CFG and params
         info_yaml = QAIHMModelInfo.load_info_yaml(info_path)
-        code_gen_config = QAIHMModelInfo.load_code_gen_yaml(code_gen_path)
-        return QAIHMModelInfo(
+        return cls(
             info_yaml["name"],
             info_yaml["id"],
             MODEL_STATUS.from_string(info_yaml["status"]),
             info_yaml.get("status_reason", None),
             info_yaml["headline"],
             MODEL_DOMAIN.from_string(info_yaml["domain"]),
             info_yaml["description"],
@@ -717,15 +840,15 @@
             info_yaml["deploy_license"],
             info_yaml["source_repo"],
             info_yaml["applicable_scenarios"],
             info_yaml["related_models"],
             [FORM_FACTOR.from_string(ff) for ff in info_yaml["form_factors"]],
             info_yaml["has_static_banner"],
             info_yaml["has_animated_banner"],
-            code_gen_config,
+            QAIHMModelCodeGen.from_yaml(code_gen_path),
             info_yaml["license_type"],
             info_yaml["deploy_license_type"],
             info_yaml["dataset"],
             info_yaml["technical_details"],
         )
 
     # Schema for info.yaml
@@ -755,57 +878,16 @@
                 "license_type": str,
                 "deploy_license_type": str,
                 "dataset": list,
             }
         )
     )
 
-    # Schema for code-gen.yaml
-    CODE_GEN_YAML_SCHEMA = Schema(
-        And(
-            {
-                OptionalSchema("has_components", default=""): str,
-                OptionalSchema("is_aimet", default=False): bool,
-                OptionalSchema("has_on_target_demo", default=False): bool,
-                OptionalSchema("qnn_export_failure_reason", default=""): str,
-                OptionalSchema("tflite_export_failure_reason", default=""): str,
-                OptionalSchema("has_demo", default=True): bool,
-                OptionalSchema("check_trace", default=True): bool,
-                OptionalSchema("channel_last_input", default=""): str,
-                OptionalSchema("channel_last_output", default=""): str,
-                OptionalSchema("outputs_to_skip_validation", default=[]): list,
-                OptionalSchema("export_test_model_kwargs", default={}): dict,
-                OptionalSchema("components", default={}): dict,
-                OptionalSchema("default_components", default=[]): list,
-                OptionalSchema("skip_tests", default=False): bool,
-                OptionalSchema("is_precompiled", default=False): bool,
-                OptionalSchema("no_assets", default=False): bool,
-                OptionalSchema("global_requirements_incompatible", default=False): bool,
-                OptionalSchema("torchscript_opt", default=[]): list,
-                OptionalSchema("inference_metrics", default="psnr"): str,
-            }
-        )
-    )
-
     @staticmethod
     def load_info_yaml(path: str | Path) -> Dict[str, Any]:
-        with open(path) as f:
-            data = yaml.safe_load(f)
-            try:
-                # Validate high level-schema
-                data = QAIHMModelInfo.INFO_YAML_SCHEMA.validate(data)
-            except SchemaError as e:
-                assert 0, f"{e.code} in {path}"
-            return data
-
-    @staticmethod
-    def load_code_gen_yaml(path: str | Path | None):
-        if not path or not os.path.exists(path):
-            return QAIHMModelInfo.CODE_GEN_YAML_SCHEMA.validate({})  # Default Schema
-        with open(path) as f:
-            data = yaml.safe_load(f)
-            try:
-                # Validate high level-schema
-                data = QAIHMModelInfo.CODE_GEN_YAML_SCHEMA.validate(data)
-            except SchemaError as e:
-                assert 0, f"{e.code} in {path}"
-            return data
+        data = load_yaml(path)
+        try:
+            # Validate high level-schema
+            data = QAIHMModelInfo.INFO_YAML_SCHEMA.validate(data)
+        except SchemaError as e:
+            assert 0, f"{e.code} in {path}"
+        return data
```

## qai_hub_models/utils/model_adapters.py

```diff
@@ -27,15 +27,21 @@
         """
         Wraps torch models to use numpy input / outputs
         """
         assert isinstance(base_model, (torch.jit.ScriptModule, torch.nn.Module))
         self.base_model = base_model
 
     def __call__(self, *args) -> Tuple[np.ndarray, ...]:
-        input_data = tuple(torch.from_numpy(t) for t in args)
+        inp = []
+        for t in args:
+            if not isinstance(t, np.ndarray):
+                inp.append(t)
+            else:
+                inp.append(torch.from_numpy(t))
+        input_data = tuple(inp)
         res = self.base_model(*input_data)
         if isinstance(res, torch.Tensor):
             output = res.detach().numpy()
         else:
             output = tuple(t.detach().numpy() for t in flatten(res))
         if isinstance(output, tuple) and len(output) == 1:
             return output[0]
```

## qai_hub_models/utils/printing.py

```diff
@@ -71,14 +71,16 @@
     print(f"Performance results on-device for {profile_job.name.title()}.")
     print(_INFO_DASH)
 
     if profile_job.model.model_type == SourceModelType.TFLITE:
         runtime = TargetRuntime.TFLITE
     elif is_qnn_hub_model(profile_job.model):
         runtime = TargetRuntime.QNN
+    elif profile_job.model.model_type == SourceModelType.ORT:
+        runtime = TargetRuntime.ORT
     else:
         raise NotImplementedError()
 
     print_profile_metrics(
         QAIHMModelPerf.ModelRuntimePerformanceDetails(
             profile_job.model.name,
             profile_job.device.name,
```

## qai_hub_models/utils/quantization_aimet.py

```diff
@@ -33,24 +33,19 @@
 from pathlib import Path
 from typing import Any, List
 from zipfile import ZipFile
 
 import torch
 from qai_hub.client import DatasetEntries
 
-from qai_hub_models.evaluators.base_evaluators import (
-    BaseEvaluator,
-    _DataLoader,
-    _for_each_batch,
-)
+from qai_hub_models.evaluators.base_evaluators import _DataLoader, _for_each_batch
 from qai_hub_models.models._shared.common import apply_module_function_recursively
 from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.protocols import (
-    EvalModelProtocol,
-    HubModelProtocol,
+    PretrainedHubModelProtocol,
     QuantizableModelProtocol,
 )
 from qai_hub_models.utils.input_spec import InputSpec, make_torch_inputs
 
 
 def tie_aimet_observer_groups(groups: List[List[Any]]):
     """
@@ -99,45 +94,37 @@
                 )
 
     apply_module_function_recursively(
         module, torch.nn.Conv2d, convert_depthwise_to_per_tensor
     )
 
 
-class AIMETQuantizableMixin(HubModelProtocol, QuantizableModelProtocol):
+class AIMETQuantizableMixin(PretrainedHubModelProtocol, QuantizableModelProtocol):
     """
     Mixin that allows a model to be quantized & exported to disk using AIMET.
 
     Inheritor must implement HubModel for this mixin to function.
     """
 
     def __init__(
         self,
         quant_sim: QuantizationSimModel,
         needs_onnx_direct_aimet_export: bool = False,
     ):
         self.quant_sim = quant_sim
         self.needs_onnx_direct_aimet_export = needs_onnx_direct_aimet_export
 
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        if target_runtime == TargetRuntime.QNN:
-            return SourceModelFormat.ONNX
-        else:
-            return SourceModelFormat.TORCHSCRIPT
-
     def quantize(
         self,
         data: _DataLoader,
         num_samples: int | None = None,
-        evaluator: BaseEvaluator | None = None,
         device: str = "cpu",
         requantize_model_weights=False,
-    ) -> float | None:
+        data_has_gt=False,
+    ) -> None:
         """
         Compute quantization encodings for this model with the given dataset and model evaluator.
 
         This model will be updated with a new set of quantization parameters. Future calls to
         forward() and export_...() will take these quantization parameters into account.
 
         Parameters:
@@ -152,29 +139,24 @@
                           ground_truth: Collection[torch.Tensor] | torch.Tensor]
                         )
 
             num_samples: int | None
                 Number of samples to use for evaluation. One sample is one iteration from iter(data).
                 If none, defaults to the number of samples in the dataset.
 
-            evaluator: BaseModelEvaluator | None
-                Evaluator to populate while quantizing the data.
-                If not provided, an evaluator is not used.
-
             device: str
                 Name of device on which inference should be run.
 
             requantize_model_weights: bool
                 If a weight is quantized, recompute its quantization parameters.
 
-        Returns:
-            If an evaluator is provided, returns its accuracy score. No return value otherwise.
+            data_has_gt: bool
+                Set to true if the data loader passed in also provides ground truth data.
+                The ground truth data will be discarded for quantization.
         """
-        if not evaluator and isinstance(self, EvalModelProtocol):
-            evaluator = self.get_evaluator()
 
         # Enable or disable quantization for model parameters (model weights).
         # Activations are always re-quantized.
         for quant_module in self.quant_sim.model.modules():
             if isinstance(quant_module, QcQuantizeWrapper):
                 for param_quantizer in quant_module.param_quantizers.values():
                     if not requantize_model_weights:
@@ -183,33 +165,23 @@
                         except RuntimeError:
                             # Encoding is not set, so it can't be frozen.
                             pass
                     else:
                         # Un-freeze the quantizer.
                         param_quantizer._is_encoding_frozen = False
 
-        # Reset evaluator if applicable
-        if evaluator:
-            evaluator.reset()
-
         # Define evaluator function for this model.
-        def evaluator_func(model: torch.nn.Module, args):
+        def batched_forward(model: torch.nn.Module, args):
             # This function is defined because AIMET does not unwrap
             # the arguments you pass to `compute_encodings`.
-            return (
-                evaluator.add_from_dataset(model, *args)
-                if evaluator
-                else _for_each_batch(model, *args)
-            )
+            data, num_samples, device = args
+            _for_each_batch(model, data, num_samples, device, data_has_gt=data_has_gt)
 
         # Compute the new encodings.
-        self.quant_sim.compute_encodings(evaluator_func, [data, num_samples, device])
-
-        # Return accuracy score if applicable
-        return evaluator.get_accuracy_score() if evaluator else None
+        self.quant_sim.compute_encodings(batched_forward, [data, num_samples, device])
 
     def convert_to_torchscript_and_aimet_encodings(
         self,
         output_dir: str | Path,
         input_spec: InputSpec | None = None,
         model_name: str | None = None,
     ) -> str:
@@ -316,7 +288,20 @@
         """
         Calibration dataset for this model and input spec.
         """
         if not input_spec:
             input_spec = self.get_input_spec()
         inputs = make_torch_inputs(input_spec)
         return {k: v.numpy() for k, v in zip(input_spec.keys(), inputs)}
+
+    def get_hub_compile_options(
+        self, target_runtime: TargetRuntime, other_compile_options: str = ""
+    ) -> str:
+        compile_options = super().get_hub_compile_options(  # type: ignore
+            target_runtime, other_compile_options
+        )
+        return compile_options + " --quantize_full_type int8 --quantize_io"
+
+    def preferred_hub_source_model_format(
+        self, target_runtime: TargetRuntime
+    ) -> SourceModelFormat:
+        return SourceModelFormat.ONNX
```

## Comparing `qai_hub_models-0.4.0.dist-info/LICENSE` & `qai_hub_models-0.4.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qai_hub_models-0.4.0.dist-info/METADATA` & `qai_hub_models-0.4.1.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qai-hub-models
-Version: 0.4.0
+Version: 0.4.1
 Summary: Models optimized for export to run on device.
 Home-page: https://github.com/quic/ai-hub-models
 Author: Qualcomm Technologies, Inc.
 License: BSD-3
 Platform: UNKNOWN
 Requires-Python: >=3.8, <3.11
 Description-Content-Type: text/markdown
@@ -107,50 +107,52 @@
 Requires-Dist: seaborn ==0.11.0 ; extra == 'fastsam_x'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'fastsam_x'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'fastsam_x'
 Provides-Extra: ffnet-122ns-lowres
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-122ns-lowres'
 Provides-Extra: ffnet-40s
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-40s'
+Provides-Extra: ffnet-40s-quantized
+Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-40s-quantized'
 Provides-Extra: ffnet-54s
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-54s'
+Provides-Extra: ffnet-54s-quantized
+Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-54s-quantized'
 Provides-Extra: ffnet-78s
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-78s'
 Provides-Extra: ffnet-78s-lowres
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-78s-lowres'
+Provides-Extra: ffnet-78s-quantized
+Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet-78s-quantized'
 Provides-Extra: ffnet_122ns_lowres
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_122ns_lowres'
 Provides-Extra: ffnet_40s
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_40s'
+Provides-Extra: ffnet_40s_quantized
+Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_40s_quantized'
 Provides-Extra: ffnet_54s
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_54s'
+Provides-Extra: ffnet_54s_quantized
+Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_54s_quantized'
 Provides-Extra: ffnet_78s
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_78s'
 Provides-Extra: ffnet_78s_lowres
 Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_78s_lowres'
+Provides-Extra: ffnet_78s_quantized
+Requires-Dist: scikit-image ==0.21.0 ; extra == 'ffnet_78s_quantized'
 Provides-Extra: hrnet-pose
 Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet-pose'
 Requires-Dist: mmpose ==1.2.0 ; extra == 'hrnet-pose'
 Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet-pose'
 Requires-Dist: mmdet ==3.2.0 ; extra == 'hrnet-pose'
-Provides-Extra: hrnet-pose-quantized
-Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet-pose-quantized'
-Requires-Dist: mmpose ==1.2.0 ; extra == 'hrnet-pose-quantized'
-Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet-pose-quantized'
-Requires-Dist: mmdet ==3.2.0 ; extra == 'hrnet-pose-quantized'
 Provides-Extra: hrnet_pose
 Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet_pose'
 Requires-Dist: mmpose ==1.2.0 ; extra == 'hrnet_pose'
 Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet_pose'
 Requires-Dist: mmdet ==3.2.0 ; extra == 'hrnet_pose'
-Provides-Extra: hrnet_pose_quantized
-Requires-Dist: yacs ==0.1.8 ; extra == 'hrnet_pose_quantized'
-Requires-Dist: mmpose ==1.2.0 ; extra == 'hrnet_pose_quantized'
-Requires-Dist: mmcv ==2.1.0 ; extra == 'hrnet_pose_quantized'
-Requires-Dist: mmdet ==3.2.0 ; extra == 'hrnet_pose_quantized'
 Provides-Extra: huggingface-wavlm-base-plus
 Requires-Dist: transformers ==4.27.4 ; extra == 'huggingface-wavlm-base-plus'
 Requires-Dist: soundfile ==0.12.1 ; extra == 'huggingface-wavlm-base-plus'
 Requires-Dist: librosa ==0.10.1 ; extra == 'huggingface-wavlm-base-plus'
 Requires-Dist: datasets ==2.14.5 ; extra == 'huggingface-wavlm-base-plus'
 Provides-Extra: huggingface_wavlm_base_plus
 Requires-Dist: transformers ==4.27.4 ; extra == 'huggingface_wavlm_base_plus'
@@ -521,137 +523,136 @@
 
 ### Computer Vision
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
 | **Image Classification**
-| [VIT](https://aihub.qualcomm.com/models/vit) | [qai_hub_models.models.vit](qai_hub_models/models/vit/README.md) |  |  | 
-| [Inception-v3-Quantized](https://aihub.qualcomm.com/models/inception_v3_quantized) | [qai_hub_models.models.inception_v3_quantized](qai_hub_models/models/inception_v3_quantized/README.md) |  |  | 
+| [ConvNext-Tiny](https://aihub.qualcomm.com/models/convnext_tiny) | [qai_hub_models.models.convnext_tiny](qai_hub_models/models/convnext_tiny/README.md) |  |  | 
+| [DenseNet-121](https://aihub.qualcomm.com/models/densenet121) | [qai_hub_models.models.densenet121](qai_hub_models/models/densenet121/README.md) |  |  | 
+| [EfficientNet-B0](https://aihub.qualcomm.com/models/efficientnet_b0) | [qai_hub_models.models.efficientnet_b0](qai_hub_models/models/efficientnet_b0/README.md) |  |  | 
+| [GoogLeNet](https://aihub.qualcomm.com/models/googlenet) | [qai_hub_models.models.googlenet](qai_hub_models/models/googlenet/README.md) |  |  | 
+| [GoogLeNetQuantized](https://aihub.qualcomm.com/models/googlenet_quantized) | [qai_hub_models.models.googlenet_quantized](qai_hub_models/models/googlenet_quantized/README.md) |  |  | 
 | [Inception-v3](https://aihub.qualcomm.com/models/inception_v3) | [qai_hub_models.models.inception_v3](qai_hub_models/models/inception_v3/README.md) |  |  | 
+| [Inception-v3-Quantized](https://aihub.qualcomm.com/models/inception_v3_quantized) | [qai_hub_models.models.inception_v3_quantized](qai_hub_models/models/inception_v3_quantized/README.md) |  |  | 
+| [MNASNet05](https://aihub.qualcomm.com/models/mnasnet05) | [qai_hub_models.models.mnasnet05](qai_hub_models/models/mnasnet05/README.md) |  |  | 
+| [MobileNet-v2](https://aihub.qualcomm.com/models/mobilenet_v2) | [qai_hub_models.models.mobilenet_v2](qai_hub_models/models/mobilenet_v2/README.md) |  |  | 
+| [MobileNet-v2-Quantized](https://aihub.qualcomm.com/models/mobilenet_v2_quantized) | [qai_hub_models.models.mobilenet_v2_quantized](qai_hub_models/models/mobilenet_v2_quantized/README.md) |  |  | 
 | [MobileNet-v3-Large](https://aihub.qualcomm.com/models/mobilenet_v3_large) | [qai_hub_models.models.mobilenet_v3_large](qai_hub_models/models/mobilenet_v3_large/README.md) |  |  | 
-| [GoogLeNet](https://aihub.qualcomm.com/models/googlenet) | [qai_hub_models.models.googlenet](qai_hub_models/models/googlenet/README.md) |  |  | 
+| [MobileNet-v3-Large-Quantized](https://aihub.qualcomm.com/models/mobilenet_v3_large_quantized) | [qai_hub_models.models.mobilenet_v3_large_quantized](qai_hub_models/models/mobilenet_v3_large_quantized/README.md) |  |  | 
+| [MobileNet-v3-Small](https://aihub.qualcomm.com/models/mobilenet_v3_small) | [qai_hub_models.models.mobilenet_v3_small](qai_hub_models/models/mobilenet_v3_small/README.md) |  |  | 
+| [RegNet](https://aihub.qualcomm.com/models/regnet) | [qai_hub_models.models.regnet](qai_hub_models/models/regnet/README.md) |  |  | 
 | [ResNeXt101](https://aihub.qualcomm.com/models/resnext101) | [qai_hub_models.models.resnext101](qai_hub_models/models/resnext101/README.md) |  |  | 
-| [ResNet50](https://aihub.qualcomm.com/models/resnet50) | [qai_hub_models.models.resnet50](qai_hub_models/models/resnet50/README.md) |  |  | 
-| [ResNeXt50](https://aihub.qualcomm.com/models/resnext50) | [qai_hub_models.models.resnext50](qai_hub_models/models/resnext50/README.md) |  |  | 
-| [SqueezeNet-1_1](https://aihub.qualcomm.com/models/squeezenet1_1) | [qai_hub_models.models.squeezenet1_1](qai_hub_models/models/squeezenet1_1/README.md) |  |  | 
 | [ResNeXt101Quantized](https://aihub.qualcomm.com/models/resnext101_quantized) | [qai_hub_models.models.resnext101_quantized](qai_hub_models/models/resnext101_quantized/README.md) |  |  | 
-| [Shufflenet-v2Quantized](https://aihub.qualcomm.com/models/shufflenet_v2_quantized) | [qai_hub_models.models.shufflenet_v2_quantized](qai_hub_models/models/shufflenet_v2_quantized/README.md) |  |  | 
-| [Shufflenet-v2](https://aihub.qualcomm.com/models/shufflenet_v2) | [qai_hub_models.models.shufflenet_v2](qai_hub_models/models/shufflenet_v2/README.md) |  |  | 
-| [ResNet18](https://aihub.qualcomm.com/models/resnet18) | [qai_hub_models.models.resnet18](qai_hub_models/models/resnet18/README.md) |  |  | 
+| [ResNeXt50](https://aihub.qualcomm.com/models/resnext50) | [qai_hub_models.models.resnext50](qai_hub_models/models/resnext50/README.md) |  |  | 
 | [ResNeXt50Quantized](https://aihub.qualcomm.com/models/resnext50_quantized) | [qai_hub_models.models.resnext50_quantized](qai_hub_models/models/resnext50_quantized/README.md) |  |  | 
-| [DenseNet-121](https://aihub.qualcomm.com/models/densenet121) | [qai_hub_models.models.densenet121](qai_hub_models/models/densenet121/README.md) |  |  | 
-| [Swin-Base](https://aihub.qualcomm.com/models/swin_base) | [qai_hub_models.models.swin_base](qai_hub_models/models/swin_base/README.md) |  |  | 
 | [ResNet101](https://aihub.qualcomm.com/models/resnet101) | [qai_hub_models.models.resnet101](qai_hub_models/models/resnet101/README.md) |  |  | 
-| [EfficientNet-B0](https://aihub.qualcomm.com/models/efficientnet_b0) | [qai_hub_models.models.efficientnet_b0](qai_hub_models/models/efficientnet_b0/README.md) |  |  | 
 | [ResNet101Quantized](https://aihub.qualcomm.com/models/resnet101_quantized) | [qai_hub_models.models.resnet101_quantized](qai_hub_models/models/resnet101_quantized/README.md) |  |  | 
-| [WideResNet50-Quantized](https://aihub.qualcomm.com/models/wideresnet50_quantized) | [qai_hub_models.models.wideresnet50_quantized](qai_hub_models/models/wideresnet50_quantized/README.md) |  |  | 
-| [SqueezeNet-1_1Quantized](https://aihub.qualcomm.com/models/squeezenet1_1_quantized) | [qai_hub_models.models.squeezenet1_1_quantized](qai_hub_models/models/squeezenet1_1_quantized/README.md) |  |  | 
-| [MNASNet05](https://aihub.qualcomm.com/models/mnasnet05) | [qai_hub_models.models.mnasnet05](qai_hub_models/models/mnasnet05/README.md) |  |  | 
-| [MobileNet-v3-Small](https://aihub.qualcomm.com/models/mobilenet_v3_small) | [qai_hub_models.models.mobilenet_v3_small](qai_hub_models/models/mobilenet_v3_small/README.md) |  |  | 
+| [ResNet18](https://aihub.qualcomm.com/models/resnet18) | [qai_hub_models.models.resnet18](qai_hub_models/models/resnet18/README.md) |  |  | 
 | [ResNet18Quantized](https://aihub.qualcomm.com/models/resnet18_quantized) | [qai_hub_models.models.resnet18_quantized](qai_hub_models/models/resnet18_quantized/README.md) |  |  | 
+| [ResNet50](https://aihub.qualcomm.com/models/resnet50) | [qai_hub_models.models.resnet50](qai_hub_models/models/resnet50/README.md) |  |  | 
+| [Shufflenet-v2](https://aihub.qualcomm.com/models/shufflenet_v2) | [qai_hub_models.models.shufflenet_v2](qai_hub_models/models/shufflenet_v2/README.md) |  |  | 
+| [Shufflenet-v2Quantized](https://aihub.qualcomm.com/models/shufflenet_v2_quantized) | [qai_hub_models.models.shufflenet_v2_quantized](qai_hub_models/models/shufflenet_v2_quantized/README.md) |  |  | 
+| [SqueezeNet-1_1](https://aihub.qualcomm.com/models/squeezenet1_1) | [qai_hub_models.models.squeezenet1_1](qai_hub_models/models/squeezenet1_1/README.md) |  |  | 
+| [SqueezeNet-1_1Quantized](https://aihub.qualcomm.com/models/squeezenet1_1_quantized) | [qai_hub_models.models.squeezenet1_1_quantized](qai_hub_models/models/squeezenet1_1_quantized/README.md) |  |  | 
+| [Swin-Base](https://aihub.qualcomm.com/models/swin_base) | [qai_hub_models.models.swin_base](qai_hub_models/models/swin_base/README.md) |  |  | 
 | [Swin-Small](https://aihub.qualcomm.com/models/swin_small) | [qai_hub_models.models.swin_small](qai_hub_models/models/swin_small/README.md) |  |  | 
-| [GoogLeNetQuantized](https://aihub.qualcomm.com/models/googlenet_quantized) | [qai_hub_models.models.googlenet_quantized](qai_hub_models/models/googlenet_quantized/README.md) |  |  | 
-| [MobileNet-v3-Large-Quantized](https://aihub.qualcomm.com/models/mobilenet_v3_large_quantized) | [qai_hub_models.models.mobilenet_v3_large_quantized](qai_hub_models/models/mobilenet_v3_large_quantized/README.md) |  |  | 
-| [MobileNet-v2](https://aihub.qualcomm.com/models/mobilenet_v2) | [qai_hub_models.models.mobilenet_v2](qai_hub_models/models/mobilenet_v2/README.md) |  |  | 
-| [WideResNet50](https://aihub.qualcomm.com/models/wideresnet50) | [qai_hub_models.models.wideresnet50](qai_hub_models/models/wideresnet50/README.md) |  |  | 
-| [RegNet](https://aihub.qualcomm.com/models/regnet) | [qai_hub_models.models.regnet](qai_hub_models/models/regnet/README.md) |  |  | 
 | [Swin-Tiny](https://aihub.qualcomm.com/models/swin_tiny) | [qai_hub_models.models.swin_tiny](qai_hub_models/models/swin_tiny/README.md) |  |  | 
-| [ConvNext-Tiny](https://aihub.qualcomm.com/models/convnext_tiny) | [qai_hub_models.models.convnext_tiny](qai_hub_models/models/convnext_tiny/README.md) |  |  | 
-| [MobileNet-v2-Quantized](https://aihub.qualcomm.com/models/mobilenet_v2_quantized) | [qai_hub_models.models.mobilenet_v2_quantized](qai_hub_models/models/mobilenet_v2_quantized/README.md) |  |  | 
+| [VIT](https://aihub.qualcomm.com/models/vit) | [qai_hub_models.models.vit](qai_hub_models/models/vit/README.md) |  |  | 
+| [WideResNet50](https://aihub.qualcomm.com/models/wideresnet50) | [qai_hub_models.models.wideresnet50](qai_hub_models/models/wideresnet50/README.md) |  |  | 
+| [WideResNet50-Quantized](https://aihub.qualcomm.com/models/wideresnet50_quantized) | [qai_hub_models.models.wideresnet50_quantized](qai_hub_models/models/wideresnet50_quantized/README.md) |  |  | 
 | | | | |
 | **Image Editing**
-| [LaMa-Dilated](https://aihub.qualcomm.com/models/lama_dilated) | [qai_hub_models.models.lama_dilated](qai_hub_models/models/lama_dilated/README.md) |  |  | 
 | [AOT-GAN](https://aihub.qualcomm.com/models/aotgan) | [qai_hub_models.models.aotgan](qai_hub_models/models/aotgan/README.md) |  |  | 
+| [LaMa-Dilated](https://aihub.qualcomm.com/models/lama_dilated) | [qai_hub_models.models.lama_dilated](qai_hub_models/models/lama_dilated/README.md) |  |  | 
 | | | | |
 | **Image Generation**
 | [StyleGAN2](https://aihub.qualcomm.com/models/stylegan2) | [qai_hub_models.models.stylegan2](qai_hub_models/models/stylegan2/README.md) |  |  | 
 | | | | |
 | **Super Resolution**
-| [XLSR-Quantized](https://aihub.qualcomm.com/models/xlsr_quantized) | [qai_hub_models.models.xlsr_quantized](qai_hub_models/models/xlsr_quantized/README.md) |  |  | 
+| [ESRGAN](https://aihub.qualcomm.com/models/esrgan) | [qai_hub_models.models.esrgan](qai_hub_models/models/esrgan/README.md) |  |  | 
+| [QuickSRNetLarge](https://aihub.qualcomm.com/models/quicksrnetlarge) | [qai_hub_models.models.quicksrnetlarge](qai_hub_models/models/quicksrnetlarge/README.md) |  |  | 
 | [QuickSRNetLarge-Quantized](https://aihub.qualcomm.com/models/quicksrnetlarge_quantized) | [qai_hub_models.models.quicksrnetlarge_quantized](qai_hub_models/models/quicksrnetlarge_quantized/README.md) |  |  | 
-| [QuickSRNetMedium-Quantized](https://aihub.qualcomm.com/models/quicksrnetmedium_quantized) | [qai_hub_models.models.quicksrnetmedium_quantized](qai_hub_models/models/quicksrnetmedium_quantized/README.md) |  |  | 
-| [Real-ESRGAN-x4plus](https://aihub.qualcomm.com/models/real_esrgan_x4plus) | [qai_hub_models.models.real_esrgan_x4plus](qai_hub_models/models/real_esrgan_x4plus/README.md) |  |  | 
-| [Real-ESRGAN-General-x4v3](https://aihub.qualcomm.com/models/real_esrgan_general_x4v3) | [qai_hub_models.models.real_esrgan_general_x4v3](qai_hub_models/models/real_esrgan_general_x4v3/README.md) |  |  | 
 | [QuickSRNetMedium](https://aihub.qualcomm.com/models/quicksrnetmedium) | [qai_hub_models.models.quicksrnetmedium](qai_hub_models/models/quicksrnetmedium/README.md) |  |  | 
-| [ESRGAN](https://aihub.qualcomm.com/models/esrgan) | [qai_hub_models.models.esrgan](qai_hub_models/models/esrgan/README.md) |  |  | 
+| [QuickSRNetMedium-Quantized](https://aihub.qualcomm.com/models/quicksrnetmedium_quantized) | [qai_hub_models.models.quicksrnetmedium_quantized](qai_hub_models/models/quicksrnetmedium_quantized/README.md) |  |  | 
 | [QuickSRNetSmall](https://aihub.qualcomm.com/models/quicksrnetsmall) | [qai_hub_models.models.quicksrnetsmall](qai_hub_models/models/quicksrnetsmall/README.md) |  |  | 
+| [QuickSRNetSmall-Quantized](https://aihub.qualcomm.com/models/quicksrnetsmall_quantized) | [qai_hub_models.models.quicksrnetsmall_quantized](qai_hub_models/models/quicksrnetsmall_quantized/README.md) |  |  | 
+| [Real-ESRGAN-General-x4v3](https://aihub.qualcomm.com/models/real_esrgan_general_x4v3) | [qai_hub_models.models.real_esrgan_general_x4v3](qai_hub_models/models/real_esrgan_general_x4v3/README.md) |  |  | 
+| [Real-ESRGAN-x4plus](https://aihub.qualcomm.com/models/real_esrgan_x4plus) | [qai_hub_models.models.real_esrgan_x4plus](qai_hub_models/models/real_esrgan_x4plus/README.md) |  |  | 
 | [SESR-M5](https://aihub.qualcomm.com/models/sesr_m5) | [qai_hub_models.models.sesr_m5](qai_hub_models/models/sesr_m5/README.md) |  |  | 
 | [SESR-M5-Quantized](https://aihub.qualcomm.com/models/sesr_m5_quantized) | [qai_hub_models.models.sesr_m5_quantized](qai_hub_models/models/sesr_m5_quantized/README.md) |  |  | 
-| [QuickSRNetSmall-Quantized](https://aihub.qualcomm.com/models/quicksrnetsmall_quantized) | [qai_hub_models.models.quicksrnetsmall_quantized](qai_hub_models/models/quicksrnetsmall_quantized/README.md) |  |  | 
 | [XLSR](https://aihub.qualcomm.com/models/xlsr) | [qai_hub_models.models.xlsr](qai_hub_models/models/xlsr/README.md) |  |  | 
-| [QuickSRNetLarge](https://aihub.qualcomm.com/models/quicksrnetlarge) | [qai_hub_models.models.quicksrnetlarge](qai_hub_models/models/quicksrnetlarge/README.md) |  |  | 
+| [XLSR-Quantized](https://aihub.qualcomm.com/models/xlsr_quantized) | [qai_hub_models.models.xlsr_quantized](qai_hub_models/models/xlsr_quantized/README.md) |  |  | 
 | | | | |
 | **Semantic Segmentation**
-| [FFNet-54S-Quantized](https://aihub.qualcomm.com/models/ffnet_54s_quantized) | [qai_hub_models.models.ffnet_54s_quantized](qai_hub_models/models/ffnet_54s_quantized/README.md) |  |  | 
-| [FFNet-40S-Quantized](https://aihub.qualcomm.com/models/ffnet_40s_quantized) | [qai_hub_models.models.ffnet_40s_quantized](qai_hub_models/models/ffnet_40s_quantized/README.md) |  |  | 
+| [DDRNet23-Slim](https://aihub.qualcomm.com/models/ddrnet23_slim) | [qai_hub_models.models.ddrnet23_slim](qai_hub_models/models/ddrnet23_slim/README.md) |  |  | 
+| [DeepLabV3-ResNet50](https://aihub.qualcomm.com/models/deeplabv3_resnet50) | [qai_hub_models.models.deeplabv3_resnet50](qai_hub_models/models/deeplabv3_resnet50/README.md) |  |  | 
 | [FCN_ResNet50](https://aihub.qualcomm.com/models/fcn_resnet50) | [qai_hub_models.models.fcn_resnet50](qai_hub_models/models/fcn_resnet50/README.md) |  |  | 
+| [FFNet-122NS-LowRes](https://aihub.qualcomm.com/models/ffnet_122ns_lowres) | [qai_hub_models.models.ffnet_122ns_lowres](qai_hub_models/models/ffnet_122ns_lowres/README.md) |  |  | 
+| [FFNet-40S](https://aihub.qualcomm.com/models/ffnet_40s) | [qai_hub_models.models.ffnet_40s](qai_hub_models/models/ffnet_40s/README.md) |  |  | 
+| [FFNet-40S-Quantized](https://aihub.qualcomm.com/models/ffnet_40s_quantized) | [qai_hub_models.models.ffnet_40s_quantized](qai_hub_models/models/ffnet_40s_quantized/README.md) |  |  | 
+| [FFNet-54S](https://aihub.qualcomm.com/models/ffnet_54s) | [qai_hub_models.models.ffnet_54s](qai_hub_models/models/ffnet_54s/README.md) |  |  | 
+| [FFNet-54S-Quantized](https://aihub.qualcomm.com/models/ffnet_54s_quantized) | [qai_hub_models.models.ffnet_54s_quantized](qai_hub_models/models/ffnet_54s_quantized/README.md) |  |  | 
+| [FFNet-78S](https://aihub.qualcomm.com/models/ffnet_78s) | [qai_hub_models.models.ffnet_78s](qai_hub_models/models/ffnet_78s/README.md) |  |  | 
+| [FFNet-78S-LowRes](https://aihub.qualcomm.com/models/ffnet_78s_lowres) | [qai_hub_models.models.ffnet_78s_lowres](qai_hub_models/models/ffnet_78s_lowres/README.md) |  |  | 
+| [FFNet-78S-Quantized](https://aihub.qualcomm.com/models/ffnet_78s_quantized) | [qai_hub_models.models.ffnet_78s_quantized](qai_hub_models/models/ffnet_78s_quantized/README.md) |  |  | 
+| [FastSam-S](https://aihub.qualcomm.com/models/fastsam_s) | [qai_hub_models.models.fastsam_s](qai_hub_models/models/fastsam_s/README.md) |  |  | 
 | [FastSam-X](https://aihub.qualcomm.com/models/fastsam_x) | [qai_hub_models.models.fastsam_x](qai_hub_models/models/fastsam_x/README.md) |  |  | 
 | [MediaPipe-Selfie-Segmentation](https://aihub.qualcomm.com/models/mediapipe_selfie) | [qai_hub_models.models.mediapipe_selfie](qai_hub_models/models/mediapipe_selfie/README.md) |  |  | 
+| [SINet](https://aihub.qualcomm.com/models/sinet) | [qai_hub_models.models.sinet](qai_hub_models/models/sinet/README.md) |  |  | 
 | [Segment-Anything-Model](https://aihub.qualcomm.com/models/sam) | [qai_hub_models.models.sam](qai_hub_models/models/sam/README.md) |  |  | 
 | [Unet-Segmentation](https://aihub.qualcomm.com/models/unet_segmentation) | [qai_hub_models.models.unet_segmentation](qai_hub_models/models/unet_segmentation/README.md) |  |  | 
-| [FFNet-40S](https://aihub.qualcomm.com/models/ffnet_40s) | [qai_hub_models.models.ffnet_40s](qai_hub_models/models/ffnet_40s/README.md) |  |  | 
-| [DDRNet23-Slim](https://aihub.qualcomm.com/models/ddrnet23_slim) | [qai_hub_models.models.ddrnet23_slim](qai_hub_models/models/ddrnet23_slim/README.md) |  |  | 
-| [DeepLabV3-ResNet50](https://aihub.qualcomm.com/models/deeplabv3_resnet50) | [qai_hub_models.models.deeplabv3_resnet50](qai_hub_models/models/deeplabv3_resnet50/README.md) |  |  | 
-| [SINet](https://aihub.qualcomm.com/models/sinet) | [qai_hub_models.models.sinet](qai_hub_models/models/sinet/README.md) |  |  | 
-| [FFNet-78S-Quantized](https://aihub.qualcomm.com/models/ffnet_78s_quantized) | [qai_hub_models.models.ffnet_78s_quantized](qai_hub_models/models/ffnet_78s_quantized/README.md) |  |  | 
-| [FFNet-54S](https://aihub.qualcomm.com/models/ffnet_54s) | [qai_hub_models.models.ffnet_54s](qai_hub_models/models/ffnet_54s/README.md) |  |  | 
-| [FFNet-122NS-LowRes](https://aihub.qualcomm.com/models/ffnet_122ns_lowres) | [qai_hub_models.models.ffnet_122ns_lowres](qai_hub_models/models/ffnet_122ns_lowres/README.md) |  |  | 
-| [FFNet-78S-LowRes](https://aihub.qualcomm.com/models/ffnet_78s_lowres) | [qai_hub_models.models.ffnet_78s_lowres](qai_hub_models/models/ffnet_78s_lowres/README.md) |  |  | 
 | [YOLOv8-Segmentation](https://aihub.qualcomm.com/models/yolov8_seg) | [qai_hub_models.models.yolov8_seg](qai_hub_models/models/yolov8_seg/README.md) |  |  | 
-| [FFNet-78S](https://aihub.qualcomm.com/models/ffnet_78s) | [qai_hub_models.models.ffnet_78s](qai_hub_models/models/ffnet_78s/README.md) |  |  | 
-| [FastSam-S](https://aihub.qualcomm.com/models/fastsam_s) | [qai_hub_models.models.fastsam_s](qai_hub_models/models/fastsam_s/README.md) |  |  | 
 | | | | |
 | **Object Detection**
 | [DETR-ResNet101](https://aihub.qualcomm.com/models/detr_resnet101) | [qai_hub_models.models.detr_resnet101](qai_hub_models/models/detr_resnet101/README.md) |  |  | 
-| [MediaPipe-Face-Detection](https://aihub.qualcomm.com/models/mediapipe_face) | [qai_hub_models.models.mediapipe_face](qai_hub_models/models/mediapipe_face/README.md) |  |  | 
-| [DETR-ResNet50-DC5](https://aihub.qualcomm.com/models/detr_resnet50_dc5) | [qai_hub_models.models.detr_resnet50_dc5](qai_hub_models/models/detr_resnet50_dc5/README.md) |  |  | 
-| [Yolo-v7](https://aihub.qualcomm.com/models/yolov7) | [qai_hub_models.models.yolov7](qai_hub_models/models/yolov7/README.md) |  |  | 
-| [YOLOv8-Detection](https://aihub.qualcomm.com/models/yolov8_det) | [qai_hub_models.models.yolov8_det](qai_hub_models/models/yolov8_det/README.md) |  |  | 
-| [Yolo-v6](https://aihub.qualcomm.com/models/yolov6) | [qai_hub_models.models.yolov6](qai_hub_models/models/yolov6/README.md) |  |  | 
 | [DETR-ResNet101-DC5](https://aihub.qualcomm.com/models/detr_resnet101_dc5) | [qai_hub_models.models.detr_resnet101_dc5](qai_hub_models/models/detr_resnet101_dc5/README.md) |  |  | 
 | [DETR-ResNet50](https://aihub.qualcomm.com/models/detr_resnet50) | [qai_hub_models.models.detr_resnet50](qai_hub_models/models/detr_resnet50/README.md) |  |  | 
+| [DETR-ResNet50-DC5](https://aihub.qualcomm.com/models/detr_resnet50_dc5) | [qai_hub_models.models.detr_resnet50_dc5](qai_hub_models/models/detr_resnet50_dc5/README.md) |  |  | 
+| [MediaPipe-Face-Detection](https://aihub.qualcomm.com/models/mediapipe_face) | [qai_hub_models.models.mediapipe_face](qai_hub_models/models/mediapipe_face/README.md) |  |  | 
 | [MediaPipe-Hand-Detection](https://aihub.qualcomm.com/models/mediapipe_hand) | [qai_hub_models.models.mediapipe_hand](qai_hub_models/models/mediapipe_hand/README.md) |  |  | 
+| [YOLOv8-Detection](https://aihub.qualcomm.com/models/yolov8_det) | [qai_hub_models.models.yolov8_det](qai_hub_models/models/yolov8_det/README.md) |  |  | 
+| [Yolo-v6](https://aihub.qualcomm.com/models/yolov6) | [qai_hub_models.models.yolov6](qai_hub_models/models/yolov6/README.md) |  |  | 
+| [Yolo-v7](https://aihub.qualcomm.com/models/yolov7) | [qai_hub_models.models.yolov7](qai_hub_models/models/yolov7/README.md) |  |  | 
 | | | | |
 | **Pose Estimation**
+| [HRNetPose](https://aihub.qualcomm.com/models/hrnet_pose) | [qai_hub_models.models.hrnet_pose](qai_hub_models/models/hrnet_pose/README.md) |  |  | 
+| [LiteHRNet](https://aihub.qualcomm.com/models/litehrnet) | [qai_hub_models.models.litehrnet](qai_hub_models/models/litehrnet/README.md) |  |  | 
 | [MediaPipe-Pose-Estimation](https://aihub.qualcomm.com/models/mediapipe_pose) | [qai_hub_models.models.mediapipe_pose](qai_hub_models/models/mediapipe_pose/README.md) |  |  | 
 | [OpenPose](https://aihub.qualcomm.com/models/openpose) | [qai_hub_models.models.openpose](qai_hub_models/models/openpose/README.md) |  |  | 
-| [LiteHRNet](https://aihub.qualcomm.com/models/litehrnet) | [qai_hub_models.models.litehrnet](qai_hub_models/models/litehrnet/README.md) |  |  | 
-| [HRNetPose](https://aihub.qualcomm.com/models/hrnet_pose) | [qai_hub_models.models.hrnet_pose](qai_hub_models/models/hrnet_pose/README.md) |  |  | 
-| [HRNetPoseQuantized](https://aihub.qualcomm.com/models/hrnet_pose_quantized) | [qai_hub_models.models.hrnet_pose_quantized](qai_hub_models/models/hrnet_pose_quantized/README.md) |  |  | 
 
 ### Audio
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
 | **Speech Recognition**
+| [HuggingFace-WavLM-Base-Plus](https://aihub.qualcomm.com/models/huggingface_wavlm_base_plus) | [qai_hub_models.models.huggingface_wavlm_base_plus](qai_hub_models/models/huggingface_wavlm_base_plus/README.md) |  |  | 
+| [Whisper-Base-En](https://aihub.qualcomm.com/models/whisper_base_en) | [qai_hub_models.models.whisper_base_en](qai_hub_models/models/whisper_base_en/README.md) |  |  | 
 | [Whisper-Small-En](https://aihub.qualcomm.com/models/whisper_small_en) | [qai_hub_models.models.whisper_small_en](qai_hub_models/models/whisper_small_en/README.md) |  |  | 
 | [Whisper-Tiny-En](https://aihub.qualcomm.com/models/whisper_tiny_en) | [qai_hub_models.models.whisper_tiny_en](qai_hub_models/models/whisper_tiny_en/README.md) |  |  | 
-| [Whisper-Base-En](https://aihub.qualcomm.com/models/whisper_base_en) | [qai_hub_models.models.whisper_base_en](qai_hub_models/models/whisper_base_en/README.md) |  |  | 
-| [HuggingFace-WavLM-Base-Plus](https://aihub.qualcomm.com/models/huggingface_wavlm_base_plus) | [qai_hub_models.models.huggingface_wavlm_base_plus](qai_hub_models/models/huggingface_wavlm_base_plus/README.md) |  |  | 
 | | | | |
 | **Audio Enhancement**
 | [Facebook-Denoiser](https://aihub.qualcomm.com/models/facebook_denoiser) | [qai_hub_models.models.facebook_denoiser](qai_hub_models/models/facebook_denoiser/README.md) |  |  | 
 
 ### Multimodal
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
-| [TrOCR](https://aihub.qualcomm.com/models/trocr) | [qai_hub_models.models.trocr](qai_hub_models/models/trocr/README.md) |  |  | 
 | [OpenAI-Clip](https://aihub.qualcomm.com/models/openai_clip) | [qai_hub_models.models.openai_clip](qai_hub_models/models/openai_clip/README.md) |  |  | 
+| [TrOCR](https://aihub.qualcomm.com/models/trocr) | [qai_hub_models.models.trocr](qai_hub_models/models/trocr/README.md) |  |  | 
 
 ### Generative Ai
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
 | **Image Generation**
 | [ControlNet](https://aihub.qualcomm.com/models/controlnet_quantized) | [qai_hub_models.models.controlnet_quantized](qai_hub_models/models/controlnet_quantized/README.md) |  |  | 
 | [Stable-Diffusion](https://aihub.qualcomm.com/models/stable_diffusion_quantized) | [qai_hub_models.models.stable_diffusion_quantized](qai_hub_models/models/stable_diffusion_quantized/README.md) |  |  | 
 | | | | |
 | **Text Generation**
-| [Llama-v2-7B-Chat](https://aihub.qualcomm.com/models/llama_v2_7b_chat_quantized) | [qai_hub_models.models.llama_v2_7b_chat_quantized](qai_hub_models/models/llama_v2_7b_chat_quantized/README.md) |  |  | 
 | [Baichuan-7B](https://aihub.qualcomm.com/models/baichuan_7b_quantized) | [qai_hub_models.models.baichuan_7b_quantized](qai_hub_models/models/baichuan_7b_quantized/README.md) |  |  | 
+| [Llama-v2-7B-Chat](https://aihub.qualcomm.com/models/llama_v2_7b_chat_quantized) | [qai_hub_models.models.llama_v2_7b_chat_quantized](qai_hub_models/models/llama_v2_7b_chat_quantized/README.md) |  |  | 
```

## Comparing `qai_hub_models-0.4.0.dist-info/RECORD` & `qai_hub_models-0.4.1.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 qai_hub_models/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/_version.py,sha256=WzOuR2xnmQE5OkcQ6TVyO5jBuai9YZT88_6W-bonfHw,281
+qai_hub_models/_version.py,sha256=cA83SEu5KF1vaKslxBJXqHJmN_NpCmBqkhyTWtwK1xQ,281
 qai_hub_models/asset_bases.yaml,sha256=53T7xh4bysMolqjwBOIxm4tm23szHhVQPgOKo7JzCvo,620
 qai_hub_models/conftest.py,sha256=STs4po9vrxUPOpDmX0XbVCVSIHtsbKou_Y11coTBAbU,734
 qai_hub_models/global_requirements.txt,sha256=vJFzTNzkE3URaOOQTaqyY_iSHjc-pMYdAa5f-GYg8jM,877
 qai_hub_models/requirements-dev.txt,sha256=z81nhpASF1QUiIvaj3ZEXr75ifaWkSstpB-Y-3jOJuo,395
 qai_hub_models/requirements.txt,sha256=m_Ltyf69NVmnBlAgWGLMetiz8hlbqAkZVVzvR6pE5Wc,426
 qai_hub_models/datasets/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/datasets/bsd300.py,sha256=ndX_-l3KFRQeOPxsq8AjyIwDb0ozGYQpP7N84lpQwtk,4757
@@ -13,16 +13,16 @@
 qai_hub_models/evaluators/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/evaluators/base_evaluators.py,sha256=vFG_4HpZkx3bHBluZq17GvP7iBCWkGphy07__pkvaww,6212
 qai_hub_models/evaluators/classification_evaluator.py,sha256=ccjbu9vcVTn8UBB0iUM_fRxvTlbarwfFWlXIR90gFAU,1326
 qai_hub_models/evaluators/detection_evaluator.py,sha256=QQJSry7nlD9TDJZ69kpNNeNNU2qC_0AZ9I48KNB6mrw,3355
 qai_hub_models/evaluators/image_evaluator.py,sha256=nPMHIeyo7WreD6LEa-eqdHdvIZQA1NbPhYoVrOQw3DY,2434
 qai_hub_models/evaluators/superres_evaluator.py,sha256=7NwBcPFJCjCbnmVpjB0cctRPNc2Am6PCL7z8vYzcDUY,2181
 qai_hub_models/models/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/common.py,sha256=Qqvomv5_0e9Oe9nUBoCNdIhLECmdz0zZKqJrPMFo5aU,559
-qai_hub_models/models/protocols.py,sha256=8RlDU96cEjge9SK5S2V464dfwrh79CGzbQckzwgO444,6690
+qai_hub_models/models/common.py,sha256=ynBwesML8zxw-ibpwPk2xHIY6On5zU1_L8zLpBeoLao,571
+qai_hub_models/models/protocols.py,sha256=4CntVWA7YWOYsnIrj6VgEUDQFk4Hswhl3UJHZGQrB0U,7882
 qai_hub_models/models/_shared/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/common.py,sha256=LHJiXmHRCP2A6GynzbwoLabojxZZlZOXwMtu4FMZcbg,1655
 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/cityscapes_segmentation/app.py,sha256=2n2tPq3bmKmAudJaQGoewvOZAX_RybjjLJnBmvfWKEM,4346
 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py,sha256=Hzjw4Bo2hKUbPkHQTk3-extZweVbAaL7bd7LAHeVy5w,2904
 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py,sha256=25G3q93XoJhT9-1sKh6p0cstO1vhAI13hOR91IucKmE,890
 qai_hub_models/models/_shared/cityscapes_segmentation/model.py,sha256=z-F-WrxlqJDdv2TNXwJW73pKgx-S0fSR2gQK0BqOoBU,2888
@@ -37,19 +37,19 @@
 qai_hub_models/models/_shared/detr/demo.py,sha256=06s1enzCfZU3GVyl2PUlkb2QlnQmG26yK4mGPCvRzQs,1944
 qai_hub_models/models/_shared/detr/model.py,sha256=BPRRhWAjfaeEb61P49Gq0NSm5nYQskhSw6xnQtTmRcM,2152
 qai_hub_models/models/_shared/fastsam/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/fastsam/app.py,sha256=VcGV3iloO_1t3g694bkV02PIPXIbnBaAoL6emPK-eRA,4883
 qai_hub_models/models/_shared/fastsam/demo.py,sha256=m9HSdl3LenmUHRF8lh8Gux66CLF6CyoHjXzP9SsZ4Q0,2022
 qai_hub_models/models/_shared/fastsam/model.py,sha256=Vr1iTFJ-UU0jxpwD2LCQ_mU-5fPCpw0NnPsbpGoOYzc,1935
 qai_hub_models/models/_shared/ffnet/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/ffnet/model.py,sha256=frRgqRiMGqPD3PCbie6cWjId1LUsPsZuJr29EK8k89s,4445
-qai_hub_models/models/_shared/ffnet/test_utils.py,sha256=kY_SKoQ5wmQMUOb7nNDDPkHwumfo-EZHCaE4o0m0S9s,1487
+qai_hub_models/models/_shared/ffnet/model.py,sha256=7fO0THAtD3QTphtGxRxIZbqnRAa88niDBN8xyX2xVYM,4548
+qai_hub_models/models/_shared/ffnet/test_utils.py,sha256=y-8cmFjaP_mCZIeSs4if8gKYMv-DW9mXKS_hIB_engc,1569
 qai_hub_models/models/_shared/ffnet_quantized/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json,sha256=RwkXlIJFptwSaAqRENlmu1spoW2oyFlhv2X20GGvIWM,1165
-qai_hub_models/models/_shared/ffnet_quantized/model.py,sha256=u2SksXg6AMdyNzZ91R0spAb0xPizsJO-89mrucN3ucI,3080
+qai_hub_models/models/_shared/ffnet_quantized/model.py,sha256=ppaDEceU2tbqohLZG9rKAw1f4-neaL-PhfyO6XDRZj8,2372
 qai_hub_models/models/_shared/imagenet_classifier/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/imagenet_classifier/app.py,sha256=af3jgCf_zfqi4c49oP6Ssn44yOMyyfuH6gZtgte3PVw,2272
 qai_hub_models/models/_shared/imagenet_classifier/demo.py,sha256=8o3ad2SJUbkVzg5jecHin4N2J_YxMNQsQ-ioImN3ufU,2432
 qai_hub_models/models/_shared/imagenet_classifier/model.py,sha256=MqTs01pdsUe7IpYIzIKwc7sEtBm2CXdR4qrbkmzqF5o,3404
 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py,sha256=MnxFDMDKULF87n4p4N1eYrUJ854zECi-nPqPLqakFZA,3781
 qai_hub_models/models/_shared/mediapipe/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/mediapipe/app.py,sha256=Sj7U03ljI1MHXkigQvvhSOf7taPFrBrpKKiLa-KLBcc,30293
@@ -67,844 +67,837 @@
 qai_hub_models/models/_shared/swin/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/swin/swin_transformer.py,sha256=E0EHaEh4k_FaVreqfNaj6iYu2HJ6qGjNhIiP6f1O1cs,9175
 qai_hub_models/models/_shared/video_classifier/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/video_classifier/app.py,sha256=DAfACj-oPdmLkJ5cohQbdz5JwmWrttKXOt2TiiuQExk,11183
 qai_hub_models/models/_shared/video_classifier/demo.py,sha256=tO2sX2PIFsT8-Lps2ZGHiW1YaCELejYmwbAs7tFC5sk,1581
 qai_hub_models/models/_shared/video_classifier/model.py,sha256=obQoYEt1AGWxrfm3tD12j1gmj6WlOGeBbATiXoEFTds,1969
 qai_hub_models/models/_shared/whisper/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/whisper/app.py,sha256=6oHmqsMQkNSSfPKdocvG_Za9AXD7Own6akKM8KgUBHU,9600
+qai_hub_models/models/_shared/whisper/app.py,sha256=FG9-3wysMYdfh7tcH6miAO5xf3EayCqVbCzLsSP05XQ,10188
 qai_hub_models/models/_shared/whisper/demo.py,sha256=7zi12g3CskAHOndf2nZIl_QVFLYp0n-P3VtO-ylEECU,1302
-qai_hub_models/models/_shared/whisper/model.py,sha256=-Aa-ikUcN4eFmBrMlR4Js1h4rR5tS0pjlhTCezuMylg,13419
-qai_hub_models/models/_shared/whisper/test_utils.py,sha256=tbaW6hdiy081MOsbV9OKwoNOCIgXf4I6neStu-FRUzc,2598
+qai_hub_models/models/_shared/whisper/model.py,sha256=8NfDTyLWl2Hp2t8lqc2fbNos65RsnwHLWxVXcelUT_8,14119
+qai_hub_models/models/_shared/whisper/test_utils.py,sha256=MVNzlRySllNxozJt9_biaPIii2hLNJHNga7weTv1mdU,2848
 qai_hub_models/models/_shared/yolo/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/yolo/app.py,sha256=yPy4f2OGc7lmedUxTKvKom8zpsZ1GDwDBW8pD0YST2g,5686
-qai_hub_models/models/_shared/yolo/demo.py,sha256=ho-XJmANdVHcxXMygI88xyARdooJvGRBU7rTvLfp4Ts,2364
-qai_hub_models/models/_shared/yolo/utils.py,sha256=jru1RpgsQTGd2c8wGdfvEm3T3MOcSxfYdmj98bDlc_s,4183
+qai_hub_models/models/_shared/yolo/app.py,sha256=T6yNzcH8Z5AY7Nml-TLLqz10lop1Idg09k0a22IXBp0,7069
+qai_hub_models/models/_shared/yolo/demo.py,sha256=ZtQmTD4vs424i5ZSuRmf80xvicWbbTJd_5JIGsHrzho,2475
+qai_hub_models/models/_shared/yolo/utils.py,sha256=ZjnOQ68V6pZMtm_6iDeQDctyf7nf1ljGiYLwAmRebj8,4190
 qai_hub_models/models/aotgan/__init__.py,sha256=BaaLR3zQEAxIdLMsE_bj1ifMQKJs6_HBVeTvC601C8Q,450
 qai_hub_models/models/aotgan/conftest.py,sha256=IK65Meta4wuUMjG-2FJtQOVtbFs6sxUEjpF-BkV5hVI,870
 qai_hub_models/models/aotgan/demo.py,sha256=pNMiZ4xPMXki0cep2i7KrSCYpbkFjVA7AjWJK3WLROw,597
-qai_hub_models/models/aotgan/export.py,sha256=rnxPCAn1HDx6GNhRu7mzyxUMtIs5LWyTxgDDS5IIBJ4,8136
+qai_hub_models/models/aotgan/export.py,sha256=HRVUYynOlKuoGa5cSUBxu5ilBFfX00mzOhuljik0uXk,8156
 qai_hub_models/models/aotgan/info.yaml,sha256=n2LVDkDfhrNh60R-XxQiZexVaNz3DHlmtJ7yuJ3MJ5U,1023
 qai_hub_models/models/aotgan/model.py,sha256=mj9Zn0MkmSh1Frsw9ESCB-pL1LjJoE_EHhNVPhfbqyo,4838
-qai_hub_models/models/aotgan/perf.yaml,sha256=O5xmet8Kc13MQ0frPdD34jIvi9N4wjQGg75SZsD5gMo,2740
+qai_hub_models/models/aotgan/perf.yaml,sha256=3wrjWMMZl0WBUnN00w3Rcbp1N1YoCFG8dsHexZBr4Xk,2768
 qai_hub_models/models/aotgan/test.py,sha256=sPojPTH2xmn5YoLMpTt8rurhiF1BDS1PMgRhXK01Fio,2006
 qai_hub_models/models/aotgan/patches/layer_norm.diff,sha256=NbhINLgR1N0wfrI3x89m7JheBbxT2MTNkEm9dpedBeY,532
 qai_hub_models/models/baichuan_7b_quantized/info.yaml,sha256=f4r1NJYFIzd7MJNcrOIkfg07OKkIjWFDFbViUT2umWU,1983
 qai_hub_models/models/baichuan_7b_quantized/perf.yaml,sha256=An29D-fEHO6H1ihUWR43hePTiHDuEP2VxUfHhloCkkk,2036
 qai_hub_models/models/controlnet_quantized/__init__.py,sha256=aCoQX1Qz9SrPHhuSJr4vD7f9nJtkhCzAa9__HBvcg4g,559
 qai_hub_models/models/controlnet_quantized/app.py,sha256=5wCKVlYNyeZ_m1PfqCOQlIq80bToaYsTdxw2Fk4l05U,9705
 qai_hub_models/models/controlnet_quantized/demo.py,sha256=rPSPdSGbFMUJ9jdMj5nLEypRqsbmrMaitON-j6zgzds,6397
 qai_hub_models/models/controlnet_quantized/export.py,sha256=SfnuhRIGIfCTGSP6tUhhlQ3TRJYkPovKHNsI2rowUZ8,7622
-qai_hub_models/models/controlnet_quantized/info.yaml,sha256=XBhGGAkja52iFEKplhUFbpRYo9B6hoQ1Qh3wrsQZe04,1296
+qai_hub_models/models/controlnet_quantized/info.yaml,sha256=ivhi6oY9IC0gGtw9REooBoV7aJQqUIvHw-g9YfE-qVw,1329
 qai_hub_models/models/controlnet_quantized/model.py,sha256=8BO-diXqlxH6wuNRs7VeYUlSxhDGWYCemAT_XgEzVt8,5426
-qai_hub_models/models/controlnet_quantized/perf.yaml,sha256=TmEm4hICGESKSV5Qaz-RUUxuthZiUYHh2bHQTNnvhFc,3323
+qai_hub_models/models/controlnet_quantized/perf.yaml,sha256=a0C641UKT6i75a3Tu0x_zsyVtB07oWBzsM3P2PSftAc,8427
 qai_hub_models/models/controlnet_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
 qai_hub_models/models/controlnet_quantized/test.py,sha256=ilelb6gq5P7_1RveAB09vfppdhzrVbWpHYiB-twI55w,1556
 qai_hub_models/models/convnext_tiny/__init__.py,sha256=HCfwohVDc0VtmT7yLoa43BTiBAEkKXtFE9-jfRy3XrM,475
 qai_hub_models/models/convnext_tiny/conftest.py,sha256=ZQRy7bLSIl_c8HcIH6pIdu6k-EqGJzgyL19za_byjKs,798
 qai_hub_models/models/convnext_tiny/demo.py,sha256=22oIp3py29Uzbsh0ob2ak4ic4ZdetMiqwgyo3I2fqEY,543
-qai_hub_models/models/convnext_tiny/export.py,sha256=du7MCwkLRbmM1Av6n2vDyr4mQpdsi8WaOu5nmYqGCM8,7895
+qai_hub_models/models/convnext_tiny/export.py,sha256=f6daP73JHRI7gBvlspn_eE8Kd86aEWLRt3hwyXyX8dY,7915
 qai_hub_models/models/convnext_tiny/info.yaml,sha256=C017TbQv8k22-OVMYW8RnesPhOAtRXZVqKANe2fAP7s,1287
 qai_hub_models/models/convnext_tiny/model.py,sha256=TohaPxU5EWTTBKMEjZusg238O3MEaA9MNKH36v6SoFw,708
-qai_hub_models/models/convnext_tiny/perf.yaml,sha256=Cw2U49fc3rMk2oRgRX4fC6jzWNfkr65ro8MHzOAT9dE,2683
+qai_hub_models/models/convnext_tiny/perf.yaml,sha256=Fg5TvSc1qlKNVDJEGh8C2aABL4YgG955krRLgO5yg7M,2707
 qai_hub_models/models/convnext_tiny/test.py,sha256=cijDPmavZsw6N6okJvEeQFhiC8zoWymxjT1QvbPUNhg,857
 qai_hub_models/models/ddrnet23_slim/__init__.py,sha256=fz62JiNfyn7NSLR8F95JKyRtjB6CQFJSxx8Vt2YQwyc,398
 qai_hub_models/models/ddrnet23_slim/app.py,sha256=bUGwG2F79praX6iTM5vHKxUlVlPyrUVl4FzmX3U9uWk,3750
 qai_hub_models/models/ddrnet23_slim/conftest.py,sha256=MXoAWwG-hVmyZQjF9uyBpDhZyy3DT-qQhoKWDW2CBuE,884
 qai_hub_models/models/ddrnet23_slim/demo.py,sha256=DtX3jP0RGrtig0qNjNSrhuhYErtopHTmsVcArT_HDrg,2128
-qai_hub_models/models/ddrnet23_slim/export.py,sha256=w4ONEFePaZtjf5ycC3Y3Is1QbmFcDrOtZ3mRQmQJSC8,8173
+qai_hub_models/models/ddrnet23_slim/export.py,sha256=qCMq8B7iRlRZqM2GwkpiA9lkuEPVgXAWj47y19ru_8M,8193
 qai_hub_models/models/ddrnet23_slim/info.yaml,sha256=J8BrgX-ROS7IN6jqb1Co6mC7mJxBO3VFVlCGHQyqIfc,1334
 qai_hub_models/models/ddrnet23_slim/model.py,sha256=o19xSxPaoEKCcYRotMWrqZyqPHI3iwigwU7OLXUKQ4M,3831
-qai_hub_models/models/ddrnet23_slim/perf.yaml,sha256=msK7HCFA_1YD9MZpirUcBB4JemJe_JYKhsk4VVIkpjc,2684
+qai_hub_models/models/ddrnet23_slim/perf.yaml,sha256=omKtcMXy8E3WaaCRoLWIpXHf-zSeXYxI_QKBgKfqa9A,2707
 qai_hub_models/models/ddrnet23_slim/test.py,sha256=Km5AirOt-YDL_qfBSKMuco1rFWR5-4MpK-o1GRBqPBY,1790
 qai_hub_models/models/deeplabv3_resnet50/__init__.py,sha256=RgC9JWAyPRQugvGjLMpeeWh68mvBeK7KxBnIvMtA_P8,451
 qai_hub_models/models/deeplabv3_resnet50/conftest.py,sha256=KlED3VIE0L0dNlTfwe0EsnYmiMY7AVO7DzcaNYxbuzM,894
 qai_hub_models/models/deeplabv3_resnet50/demo.py,sha256=pxsw91eJcacHy5-N-4POOM_vv8er47uTUIpIGM91P1k,1026
-qai_hub_models/models/deeplabv3_resnet50/export.py,sha256=epS9Lz36Uw_AEoCItgmo8QhLxnW6nQsxsHQcJtHw3Uc,8048
+qai_hub_models/models/deeplabv3_resnet50/export.py,sha256=Wuwe7Zq8MSoDuoCaohLFIbiCGf1xMZiWRArCtK9I_T8,8068
 qai_hub_models/models/deeplabv3_resnet50/info.yaml,sha256=xJyhJArERFP4UdDYrFc1dc9NLLJqkeQTi4EM_vZFaxQ,1278
 qai_hub_models/models/deeplabv3_resnet50/model.py,sha256=NLurkhtOrORi268v1t9_IlBzSTk3Lyy3jdZ2wlDHJUU,2886
-qai_hub_models/models/deeplabv3_resnet50/perf.yaml,sha256=z0yS15qEZtlmHoJdotOpWFTZMeIc6s-Z9IQtt2SI_7A,2744
+qai_hub_models/models/deeplabv3_resnet50/perf.yaml,sha256=XM55xjcK8x-GfGvBflwdagojbiA3SBC4dy1s2JjanRk,2761
 qai_hub_models/models/deeplabv3_resnet50/test.py,sha256=yXJY6pGC_QkjLGavh40vSP4flUm33H83FhWUFXTLzxM,2086
 qai_hub_models/models/densenet121/__init__.py,sha256=TxCV8tdZZ-D4d15naxmzu-0l5CHW_N3BkJiIV5HRzbY,471
 qai_hub_models/models/densenet121/conftest.py,sha256=wMqLTSpvvQHq1ZODIMnA-MHgV22YRf4ECxnMY2qrYNU,794
 qai_hub_models/models/densenet121/demo.py,sha256=kmSL8S4QnCR2AUdz-josBJzF3ehyQ5fOgHcsbgaIN2g,533
-qai_hub_models/models/densenet121/export.py,sha256=z_cQNCMMlVhbKueBNIRqe--GZ-LM0Ky-1ERnlr46yIM,7868
+qai_hub_models/models/densenet121/export.py,sha256=FrgJZ5oUwDfnRGuRYiyeHRd9l44-mRLl81Yl2I9wXGg,7888
 qai_hub_models/models/densenet121/info.yaml,sha256=RRS9Pk3XMI-kwohy4GZbzLePqaFD_03Iky3W4adFvX8,1310
 qai_hub_models/models/densenet121/model.py,sha256=Lp5noeTPApNrJL6z_VmsZfEHDuKCZNl_WUY-Lr7xatM,698
-qai_hub_models/models/densenet121/perf.yaml,sha256=zrT39FqfSXn5wdUrSKXtaY0DShsHhOunveQxoO4z0SE,2736
+qai_hub_models/models/densenet121/perf.yaml,sha256=6quM5-K__jVxKOK7j_NemdsII8Dru8rGOTHeBwn5rcM,2758
 qai_hub_models/models/densenet121/test.py,sha256=iCxswqgBQ_GXA2vfr-ShiukFZS2Gj38OV9zY6Jm3Gvk,841
 qai_hub_models/models/detr_resnet101/__init__.py,sha256=X6U_xy5Tp6diGFQfn7Na0XebaI6Ojc0wFv6xSoh_sKY,483
 qai_hub_models/models/detr_resnet101/conftest.py,sha256=6vZWUem8K0tAcN6Q7pswikMT1iLBsFWiiUe5HyAk4zY,800
 qai_hub_models/models/detr_resnet101/demo.py,sha256=9o_p8fmwrZycE5JQAsWCfskglZNfyrJOAVV49KJwChg,896
-qai_hub_models/models/detr_resnet101/export.py,sha256=RtP0O2Uq7VtkoU12ThzK9JT-07RINJwIAQ-d01Pv19g,7885
+qai_hub_models/models/detr_resnet101/export.py,sha256=20hH6wkFADuLHYxelRN7gcw6TBGJDcgQg_s3d3pSguk,7905
 qai_hub_models/models/detr_resnet101/info.yaml,sha256=YP4G-_ATugwYJnXGz9uJ0ghrH38exu_7SQfOq-2_OLE,1188
 qai_hub_models/models/detr_resnet101/model.py,sha256=f6SJr1m53RdUirPG1GOVeLDluBBxLIsOV-z2hSgF20E,661
-qai_hub_models/models/detr_resnet101/perf.yaml,sha256=Z-q-6FekTMqUcqDQq58KxYU9tH7vcGPq60FFiwhJ1O8,2698
+qai_hub_models/models/detr_resnet101/perf.yaml,sha256=yxurEsPzilPkhry0Ni8OHeTyx-cH5UluPWxYsrEH0P0,2711
 qai_hub_models/models/detr_resnet101/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet101/test.py,sha256=gDbI8asjHE5ONZLujGFXIcO_h-EC0OtEkg1t4SATMd8,1316
 qai_hub_models/models/detr_resnet101_dc5/__init__.py,sha256=u76QZ7QpcYASS-4FZptRsGd5jROIardr7dRIEktZ0Ds,490
 qai_hub_models/models/detr_resnet101_dc5/conftest.py,sha256=FaglsXCmZLy9we4l2PhNZD_xm-1cRgyS1zWeKsNOAG8,808
 qai_hub_models/models/detr_resnet101_dc5/demo.py,sha256=ie0VdX_40vFCnnIWRCnjxH9SRsxlGpGIJpvoteK6rNI,906
-qai_hub_models/models/detr_resnet101_dc5/export.py,sha256=-vrlHuHtULjCLxTMy6A94bGTHi8WAdPisRMgbPzzJT0,7901
+qai_hub_models/models/detr_resnet101_dc5/export.py,sha256=Vo7EJDf9pRoMNM72GSWzwUWtE9Ipb4ZCTYUOZQx2QCM,7921
 qai_hub_models/models/detr_resnet101_dc5/info.yaml,sha256=otqz2oAdErfBmbBgnQNQgRXlBMcw_nI-p_UTJr4nYsw,1215
 qai_hub_models/models/detr_resnet101_dc5/model.py,sha256=A4OU7F9N_JMElzRdKUH7VKkXmqlmvBxlZ71B6kN1Yyo,668
-qai_hub_models/models/detr_resnet101_dc5/perf.yaml,sha256=aqAJy-lqzhaXM264u-tq9GZ8hx5Cg5r4La-eB0MhmbQ,2702
+qai_hub_models/models/detr_resnet101_dc5/perf.yaml,sha256=95axI9nj9kF8HUEd6hDfRn-tylN04_ahpskux1aLKhw,2719
 qai_hub_models/models/detr_resnet101_dc5/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet101_dc5/test.py,sha256=E8USeF-ehryQQ7bnGoXsNmZ6ULJI9RHBhuGbHdj1fjc,1373
 qai_hub_models/models/detr_resnet50/__init__.py,sha256=atV57qR8yJou6gYUi56f7UTiBsbDVyQAvbwgHFuXI0c,481
 qai_hub_models/models/detr_resnet50/conftest.py,sha256=grxSREEFJM0IFk6__LgoDUMr68UYV7QgmtEIHJkH7d8,798
 qai_hub_models/models/detr_resnet50/demo.py,sha256=X9w6CWF9U1rYaN3Xo1RIDLgxk4l-CP8Kdrbp-wYaWBQ,893
-qai_hub_models/models/detr_resnet50/export.py,sha256=_p_F-7i2Yw-C-AxhwCXw43PXOiay7mARjqcM1TBhni4,7881
+qai_hub_models/models/detr_resnet50/export.py,sha256=UN230OEgT-Sv9r44fpLHPkcCqSaWCaLPKvmhI7zYbx0,7901
 qai_hub_models/models/detr_resnet50/info.yaml,sha256=Zgk7l86YchB6yOV1EHo072l0o8pOroZ79-VHvZbQhdQ,1185
 qai_hub_models/models/detr_resnet50/model.py,sha256=f7rWSFyK5_LybX4KkfnxZVlHpCCVk6hRfbx6CyEs7dg,659
-qai_hub_models/models/detr_resnet50/perf.yaml,sha256=4vlzYH3A1Hksk6Y95xjAmwIw2RjfvxCGARPts8A3fFw,2695
+qai_hub_models/models/detr_resnet50/perf.yaml,sha256=C8B6yFqDh1IXqbZGnWye9CvNiieb16TWz5kA9G68ZCY,2713
 qai_hub_models/models/detr_resnet50/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet50/test.py,sha256=O0P_osmIHBpoaSgCSsyPgMrS0hEhyWYiQiGfhVZzdMg,1636
 qai_hub_models/models/detr_resnet50_dc5/__init__.py,sha256=EOUQu8Eybv_S6dv2MxT06c5fP8fb-zbNd-MGM6Yo4DI,488
 qai_hub_models/models/detr_resnet50_dc5/conftest.py,sha256=VAVUi3nrH__WGTuTMDKNQamDlhNAMIDSiqjg1UskgBE,806
 qai_hub_models/models/detr_resnet50_dc5/demo.py,sha256=BFw09UhIreglAFeM1lS9xnKYumTh4fnS3hgqp0io7Es,903
-qai_hub_models/models/detr_resnet50_dc5/export.py,sha256=2CSbfbGChSJ7RR7U97AKqOM5nan9Yp4LDqzZ8A_fKNs,7897
+qai_hub_models/models/detr_resnet50_dc5/export.py,sha256=sF6QZQLdns8vQTkEnrR1bd5pccZttziRrexO0NFi_Fc,7917
 qai_hub_models/models/detr_resnet50_dc5/info.yaml,sha256=QrFUKwhno5ciqd0fpkPTis2SvXi2Nmqg9DkZTk6PaUE,1212
 qai_hub_models/models/detr_resnet50_dc5/model.py,sha256=8bZ0js6wEEb64RPwCHfwJtt9OrgTyk2KaRaS_9QxIAo,666
-qai_hub_models/models/detr_resnet50_dc5/perf.yaml,sha256=nMpxEcaqpgWgd5ogN3ETPmiadulmjlbni5bIuE4Nmrs,2698
+qai_hub_models/models/detr_resnet50_dc5/perf.yaml,sha256=5OlrDAYcoiJFRnoWnWcA-iOmDjs7bejFSxt7unSloRg,2718
 qai_hub_models/models/detr_resnet50_dc5/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet50_dc5/test.py,sha256=xt60UK2gLEMyw460kHLE14y2zgzHhhaM3peD7PfIb7Y,1344
 qai_hub_models/models/efficientnet_b0/__init__.py,sha256=7iMYCbJmA6I13XjCyUEWsizqKWjp0PMumYfobTRAhww,477
 qai_hub_models/models/efficientnet_b0/conftest.py,sha256=Le4ZI0-1wlpx5DkBPRM6sOxtbCiAMkjeYZ7jp2Rf-OM,802
 qai_hub_models/models/efficientnet_b0/demo.py,sha256=aZVZDgd4XTbaL3_3NKFMSHP15gKWYwe2qX6DEwJRPuM,549
-qai_hub_models/models/efficientnet_b0/export.py,sha256=Bsm0m24TXeiHxFV5KbEQZCdTFsgWYRG9CdxHghoh21s,7883
+qai_hub_models/models/efficientnet_b0/export.py,sha256=VOsj4Jwn6jCYO2YamVKQ82c77IMFQ3hTtPwgVqr2dEY,7903
 qai_hub_models/models/efficientnet_b0/info.yaml,sha256=qHfAm4KiPtXcdb3GmVePXiZZmsOE07Gbzl8c7Hg268c,1361
 qai_hub_models/models/efficientnet_b0/model.py,sha256=3miNRC3gRr5pfkWrwYdLFmx7Wjw0eI1CIKEOHLlB7jQ,714
-qai_hub_models/models/efficientnet_b0/perf.yaml,sha256=t-03Zwf-7NC1y5paMbElUc5V1aNcVa2Qzrgs4fbi7DM,2737
+qai_hub_models/models/efficientnet_b0/perf.yaml,sha256=o6uSWx1GV03STwYLRM6dNj1DIsOOdLGatIkZKivP-Yo,2756
 qai_hub_models/models/efficientnet_b0/test.py,sha256=RyrRTanG4fCwWk8d_EaFn9j5GVqLdDcubsEzKBM16mI,867
 qai_hub_models/models/esrgan/__init__.py,sha256=BYOPJKlrdJCd_dkZlkI9_rvh_Ip3TtrD70sT4hk6mWM,463
 qai_hub_models/models/esrgan/conftest.py,sha256=3QzbcoCBUzujSCNlw_lO8KdbslI8iFVr25PRBjcM77k,870
 qai_hub_models/models/esrgan/demo.py,sha256=QL3qaDz7O56O938cS7fMOPl_bb1gvcfJhP8onfAcMf4,939
-qai_hub_models/models/esrgan/export.py,sha256=J-xCCSUpEr-dP18a4U1fMAkN5o7WvgjO7xgv-XbVK7Q,7982
+qai_hub_models/models/esrgan/export.py,sha256=vjeek7I4d4RvbA0YcTW7FWmv_8A7o5FkAInYRoxdkqM,8002
 qai_hub_models/models/esrgan/info.yaml,sha256=uO-MC4Ex0o9haQh7iX2SBYKmtjEHkeko_xZmkGRAlPs,1117
 qai_hub_models/models/esrgan/model.py,sha256=zitbXArjzUEyP9_u5Z4ePgLFGYxG4DFKiWBRPdGy9Ow,3473
-qai_hub_models/models/esrgan/perf.yaml,sha256=R6Utw72qnBdis-4mGkoZtmBq8QXLNSkGT8yshGFwu10,2747
+qai_hub_models/models/esrgan/perf.yaml,sha256=3_FDeQlttQJXLalHojI6luKKMdvpsESfY_fmqaB2120,2772
 qai_hub_models/models/esrgan/test.py,sha256=b4pQrPN0z66v__gKsgAcXqIfEViysY9yhstuOaiNXi0,1831
 qai_hub_models/models/facebook_denoiser/__init__.py,sha256=2p_IKIllP1Me0a2Ga3OfzoAWS-sarWBUoedNgVhknoY,418
 qai_hub_models/models/facebook_denoiser/app.py,sha256=FNIaoA5NYrjXxrOoL3kiDS7TI_MpuxKPldE3ru_tA_w,3207
 qai_hub_models/models/facebook_denoiser/conftest.py,sha256=Pd2tGIihU72bq0JCljUMBnGRgdNjP_i2Pos3wAGcxxA,892
 qai_hub_models/models/facebook_denoiser/demo.py,sha256=pwZ7sKCYgl2YcRWBD6L4WChECUCwu_OrUmMWcfoqQOc,3172
-qai_hub_models/models/facebook_denoiser/export.py,sha256=3mxnjPyR1Oz67GuMtU4WubfLW7OA8u9HaHe853N7aQM,7650
+qai_hub_models/models/facebook_denoiser/export.py,sha256=z2MgZsaxc7s-1Di66Bt8lqZcn1a3uehI3gQynrE0W0c,7670
 qai_hub_models/models/facebook_denoiser/info.yaml,sha256=uBJBGRUnVPzhU6dK-LACvms0FCVSzJh2qYiLtv3JoGo,1070
 qai_hub_models/models/facebook_denoiser/model.py,sha256=_mrUIQsgrhGH_5HH103yFgEfpe1GEgU_FZe1qfbVTQo,2414
-qai_hub_models/models/facebook_denoiser/perf.yaml,sha256=6FlSLJr9EJA4bmyhlS46wVPzTMARLvKixDfLpC18viE,2701
+qai_hub_models/models/facebook_denoiser/perf.yaml,sha256=RsLoV8fwasIjupZTHpMKSqbbp-LDiaYplGiSUABqDSU,2724
 qai_hub_models/models/facebook_denoiser/requirements.txt,sha256=o34BIQCgYdBbS6Yp3eup9VorcrIfdNiUoGtrK3AoiPg,74
 qai_hub_models/models/facebook_denoiser/test.py,sha256=kmnG_zoQOVGd45TqH-pJcRTDrHq_Y0TYymSPg2r7PdY,2492
 qai_hub_models/models/fastsam_s/__init__.py,sha256=t-LgQ3KECa94clPW6e9afDjg0_iE3_AT5wwGh9u0ViE,440
 qai_hub_models/models/fastsam_s/conftest.py,sha256=q8KoGAe7ot0_LiqUvHpbGFrhqoAHTi2wtR0zeyjtOd0,790
 qai_hub_models/models/fastsam_s/demo.py,sha256=Ja3h5Trq_ZHYvEpQLp6AVki87esrcrrymRv0RpNp35g,762
-qai_hub_models/models/fastsam_s/export.py,sha256=g4mNvuXCO5XH52kNrbNHBny9vHCcl5d2uIQb-kS-d9w,8244
+qai_hub_models/models/fastsam_s/export.py,sha256=iGdi4Hd_-rAGxyRg35qXCrlt5fQJsfyUHWwSlCslszI,8264
 qai_hub_models/models/fastsam_s/info.yaml,sha256=BGCkK4ixZsZXeXRq_6IUUBn-cc3mV-1Zcq73p3TjTlQ,1301
 qai_hub_models/models/fastsam_s/model.py,sha256=-ynkCngyRJIeb85n8GIsAmAWmAt_fiRcZ85pz1c0nPw,683
-qai_hub_models/models/fastsam_s/perf.yaml,sha256=4W0sEFW6wqoOspEGje39AWM1sDBQAz7cjFaqlRlRlMc,2683
+qai_hub_models/models/fastsam_s/perf.yaml,sha256=FephWJLI4JSxQr4Q8LbwsvdjyV6mVNslpYhNi9z6J1k,2706
 qai_hub_models/models/fastsam_s/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/fastsam_s/test.py,sha256=KJD_DBh8sto_x8A9VmEN-z2AyLBhTZyykZzGTv1KsMA,1332
 qai_hub_models/models/fastsam_x/__init__.py,sha256=1jZ4uim6OseYmBo5ppIM32GcilG-cEFb0CDmJAFvSPU,440
 qai_hub_models/models/fastsam_x/conftest.py,sha256=K9_9093ZgnzjefZIq3__TMHCmZrFo5MEL-Lv2bnCfeU,790
 qai_hub_models/models/fastsam_x/demo.py,sha256=7jJK0ZIIhiMh_XS_HOUmpek_XcKaeZ0TxQudsXzUBRc,762
-qai_hub_models/models/fastsam_x/export.py,sha256=O-rJQZdp-7t1YImQax71NxAW2fY-r8enmIKcvs4Cqlw,8244
+qai_hub_models/models/fastsam_x/export.py,sha256=pTsoCwNEN92apRgozJMXx_8HQLE__ajM5G7jcxbUqxc,8264
 qai_hub_models/models/fastsam_x/info.yaml,sha256=Z3zYIUu7OAKfoOkoFd9j5jH-ZJII930iPflu_wFp12I,1300
 qai_hub_models/models/fastsam_x/model.py,sha256=15O2L72SANNVLxoWjNdugNV4hgUb0oZzdoxnNddQYM4,683
-qai_hub_models/models/fastsam_x/perf.yaml,sha256=e8yIH0Ab0ncgkwt82hsDxmSTlWKN9ayTNSPaG4cA3xg,2686
+qai_hub_models/models/fastsam_x/perf.yaml,sha256=95oY7992jWS028jkbPMu4m34f81xBWUDGEiEhJ9X-lw,2707
 qai_hub_models/models/fastsam_x/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/fastsam_x/test.py,sha256=QUaUSFuPrqESZSMF8R6c2zH2rGrvD7FdDXrcRjue8wk,1332
 qai_hub_models/models/fcn_resnet50/__init__.py,sha256=PIkLfZIDzf16OqQ6kptH6YVx_2MdAwWt3zLJJLtlbDc,410
 qai_hub_models/models/fcn_resnet50/app.py,sha256=U_aLzk8it6_-tC66l4Dz17-LgBJAfyIz9hPjKZGhO3M,2683
 qai_hub_models/models/fcn_resnet50/conftest.py,sha256=TUolB42DGCyLWqdBAC_qN7H8dw_GoRG9mzYIY_0-68A,882
 qai_hub_models/models/fcn_resnet50/demo.py,sha256=aHhtDDnA4ESHE4JkkOs_7EJtsTKYAZxW8Btz9Wxhcbg,2315
-qai_hub_models/models/fcn_resnet50/export.py,sha256=5l6eXVOoBqRg2BFhSI6ypYkYLkDyLFfeYHiZajO8rUU,8149
+qai_hub_models/models/fcn_resnet50/export.py,sha256=ViyDzn-IyTEOXUE-yRTZ68RnTEysHlQtpyi1v29Cu3s,8169
 qai_hub_models/models/fcn_resnet50/info.yaml,sha256=ny0XW7G2TGqi6uH9OAY4Gh8qzgd-QXNPqe8hSnayDLk,1241
 qai_hub_models/models/fcn_resnet50/model.py,sha256=LFsVZFks3DkPqsp62A5d6-ABQdVrIKlvBD6Sx2wrHpg,1989
-qai_hub_models/models/fcn_resnet50/perf.yaml,sha256=cdLrjZYRvnRYI1Le1qJfI3zbAWB66u1tuxdJlesmVVg,2737
+qai_hub_models/models/fcn_resnet50/perf.yaml,sha256=YjdEWzPs55bm2QOnijOP032bg0-NpSfRrjDTgqSJoB0,2761
 qai_hub_models/models/fcn_resnet50/test.py,sha256=mIM8NwhGPlSf9KBX1QIWek3rTLoYC_bBvqMbZ0mrK70,1637
 qai_hub_models/models/ffnet_122ns_lowres/__init__.py,sha256=UoCIQtMaEcM3Dg5PhPZzDxbNdV4d3k_Rw0im4Jak6pY,487
 qai_hub_models/models/ffnet_122ns_lowres/conftest.py,sha256=d0NN6tjqEfuemK5UdVyWorW9u7eZejDuTa8mxnf-9rQ,894
 qai_hub_models/models/ffnet_122ns_lowres/demo.py,sha256=t2c7R8wXMYU_1XHw0y7W08uSWZFlG0ExvHW6RCDdaXg,607
-qai_hub_models/models/ffnet_122ns_lowres/export.py,sha256=tuTRFlrc-kTLEsVpO_CjlmsWlFZ1T51tLbT1-WUV3_8,8030
+qai_hub_models/models/ffnet_122ns_lowres/export.py,sha256=WrmfZzJEnwZF2aS4TTJKrioTcw0qjX5ZUH8cX5m-3hk,8050
 qai_hub_models/models/ffnet_122ns_lowres/info.yaml,sha256=V2mZJGqPO-kJJwTVNFzLJ_xmB661g--bhcRMwsGULE8,1322
-qai_hub_models/models/ffnet_122ns_lowres/model.py,sha256=07E9R3kt9hAxhp8OmFiwhFQ5xSBM58G4qoqPHgu0g00,629
-qai_hub_models/models/ffnet_122ns_lowres/perf.yaml,sha256=0wTxO3fmySC2tN4SvFwJCCqhFqiperRXe0c1q9UoiJ0,2746
+qai_hub_models/models/ffnet_122ns_lowres/model.py,sha256=Jxz7d6RB3rbuya27MvlQgDrjkovo-gejpCBhpQda8wc,648
+qai_hub_models/models/ffnet_122ns_lowres/perf.yaml,sha256=g5pZhAwwd7VXPT3feEu2iKhdftAklZZ0jVqAyPzJJTA,2771
 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_122ns_lowres/test.py,sha256=izaltx2uBp4Sa9cxpa27oo8koqI9wMs4KykgoOYxxBc,804
 qai_hub_models/models/ffnet_40s/__init__.py,sha256=cur1hXK3ukU_DL_ujlWpJykMMaxCXzpfT6yqAoZX4w4,479
 qai_hub_models/models/ffnet_40s/conftest.py,sha256=c2iejDkhMaxEESVXvVgLReqMFBuOXWJ740ERdft0ue8,876
 qai_hub_models/models/ffnet_40s/demo.py,sha256=e5examY5jBnCcFrA73SOXV4sdY0gkOF_YheXwgLQ7GA,582
-qai_hub_models/models/ffnet_40s/export.py,sha256=3uwd-fvBaf8q5qWxJTG735Dhq48NJQMjjlS5P-0nN2s,7994
+qai_hub_models/models/ffnet_40s/export.py,sha256=1iBaGDRuKjfRy7UaPaqENsDrHYp4ca6Ng69lIRKlVa8,8014
 qai_hub_models/models/ffnet_40s/info.yaml,sha256=DbHnioJHDD2KHUyYryhPGiss8VnBKHvKUaeM1C6QgzQ,1298
-qai_hub_models/models/ffnet_40s/model.py,sha256=mVyNAbVfhN9cfzKvgUQ_BAFgzYQnhiAzA05Evm1BVgA,563
-qai_hub_models/models/ffnet_40s/perf.yaml,sha256=7LA79de7Ypm4bttW2swVcdDWlwJ-V-vxIcPtH-C-5q0,2739
+qai_hub_models/models/ffnet_40s/model.py,sha256=vMqOf4XTdSWVlmM_72tBvjNtks874DkBM-aizeN5QEM,580
+qai_hub_models/models/ffnet_40s/perf.yaml,sha256=g2z8wJI2jgaql4B_onuEle6FHYmmpcnPwFXsQSrDjrg,2764
 qai_hub_models/models/ffnet_40s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_40s/test.py,sha256=Z-7FI6ko8Dw9B54eKzGZKYjPqJRT9iUumz3VaMMyzoE,746
 qai_hub_models/models/ffnet_40s_quantized/__init__.py,sha256=D9fLMk2pWGuTgkIU7jGbxtkoiWLnA7pajSv441yVE7w,490
 qai_hub_models/models/ffnet_40s_quantized/conftest.py,sha256=s59UyGhgZiGeED7ze6TNuvvLeTmf6lZQZPUF_PMMk7c,896
 qai_hub_models/models/ffnet_40s_quantized/demo.py,sha256=a2SWiqFS6qv59AOAJgHvbBpRMMiFXGVI1v26YqxKZtU,627
-qai_hub_models/models/ffnet_40s_quantized/export.py,sha256=5grQ4gLZhju_1J8oTeZqSSIFnXDdM4-3Bf-PhPYlSG0,8447
+qai_hub_models/models/ffnet_40s_quantized/export.py,sha256=AlwF-TGMFAw4R0MeMDtQibHBs4X3boypf8jjA8zkIxk,8467
 qai_hub_models/models/ffnet_40s_quantized/info.yaml,sha256=ZRi81cKXx7ih9hPKERPXwN6hIrpI4k0Q3ci8KfXsRJQ,1347
-qai_hub_models/models/ffnet_40s_quantized/model.py,sha256=AExB1biadzs12oQaX3zLSZoUNKcQyqjQcWYdtfkl3iA,1164
-qai_hub_models/models/ffnet_40s_quantized/perf.yaml,sha256=xjLlLvKgJaPVoc1dlSDJ_KLdaPzrmCjwO57HAmfd2k0,2684
+qai_hub_models/models/ffnet_40s_quantized/model.py,sha256=_QVWRQb90OhW_0UCzzjy4-Hgc4V9DKRPrkvzhynHL9M,1169
+qai_hub_models/models/ffnet_40s_quantized/perf.yaml,sha256=nKZPKrRwoMKZdGV0o--rhnAKNFLmAo0e3gve4-I3eWE,2710
+qai_hub_models/models/ffnet_40s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_40s_quantized/test.py,sha256=11tNLKpWi4Wz1D99W0NAW531ENyWDNYpfxi6sMfy-UI,840
 qai_hub_models/models/ffnet_54s/__init__.py,sha256=Zhyq437g7aiixQVmV1yF2iZSE_0L_2Hb53mZab_fH78,479
 qai_hub_models/models/ffnet_54s/conftest.py,sha256=vYiMvfplHIC9a50_go0KbKNrh3F3BsOlBiwSEudA-b8,876
 qai_hub_models/models/ffnet_54s/demo.py,sha256=6oplfV5nQdodhEYRApOupdZMZ6Jxk-yxUwJS-j1zYyo,582
-qai_hub_models/models/ffnet_54s/export.py,sha256=SjpXNEVBa9JJCEu8BmU0E81bEI3VnLYAcR0g_aqiU5Q,7994
+qai_hub_models/models/ffnet_54s/export.py,sha256=MMBp8EiSDSthJU9TBnTigX9-PBqlKAoyJ3y-0kHN4x0,8014
 qai_hub_models/models/ffnet_54s/info.yaml,sha256=ipbP8LhRKP6hky2-CdAHgusN0zu4RcJLNkSj8UziC7M,1275
-qai_hub_models/models/ffnet_54s/model.py,sha256=hooPPVGB2cYcWmwcQJ3Ov8Y3f0p74ci12aljmj53BEM,563
-qai_hub_models/models/ffnet_54s/perf.yaml,sha256=YbG1P1jmD6Mf8ilEypCtmaIOb-u9A2eeNnrhXZ0euF4,2746
+qai_hub_models/models/ffnet_54s/model.py,sha256=uOu3W3LoWHUB4cUcHQDIRcn7d2INjhVn7nju2WxG7sc,580
+qai_hub_models/models/ffnet_54s/perf.yaml,sha256=frlF8kzN8IgsgaJsylrMbOn9KKLsr7O7n4AIgcdLR_A,2772
 qai_hub_models/models/ffnet_54s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_54s/test.py,sha256=wxVmLWpT7VdTph6c4ktD7yYNyCeB-ms2aYQyZnWNXII,746
 qai_hub_models/models/ffnet_54s_quantized/__init__.py,sha256=exZ2v9rO0y7x3f0-V2qTvVLoeOjbmqdSMpHDMI9-cfE,490
 qai_hub_models/models/ffnet_54s_quantized/conftest.py,sha256=NAKtYA6XR99RBySbXIVwO3GhUof0Ao0lwhoXD7L6ddw,896
 qai_hub_models/models/ffnet_54s_quantized/demo.py,sha256=QGw6h_1O-Ecp_yQJgAWqOgaGHPRQ2SvIrSrNf-_RdSY,627
-qai_hub_models/models/ffnet_54s_quantized/export.py,sha256=wN2DHTVFPsmveeW2hfLYhhqfn2kfo1PLeynF4-MIXKU,8447
+qai_hub_models/models/ffnet_54s_quantized/export.py,sha256=p8XeqNQl751nPnFo0f3Sr76yz4nzLXVIeZ2PdQwlrMw,8467
 qai_hub_models/models/ffnet_54s_quantized/info.yaml,sha256=whLq0baglC4ajzuhqqU7qvBh7cJBiK8yn61hUBalMOg,1347
-qai_hub_models/models/ffnet_54s_quantized/model.py,sha256=XNsqAQ59xV5BoVesRVLLqTCI_gvLARvohrLY34XJclo,1139
-qai_hub_models/models/ffnet_54s_quantized/perf.yaml,sha256=4RpooqQIiiuA2Q7GqLJuPN0iaDhjHNRFg_Ci0s3YYeQ,2689
+qai_hub_models/models/ffnet_54s_quantized/model.py,sha256=itN2MVSiS5LEUk9CcQfKFNM5JeCNrGQRwjeukp_7k4U,1156
+qai_hub_models/models/ffnet_54s_quantized/perf.yaml,sha256=qsqlVn4vmmtOLWQotukFEqgUrl8VbdcasWI0xO1bwz0,2714
+qai_hub_models/models/ffnet_54s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_54s_quantized/test.py,sha256=QLlA7Q-IcJJZFYgXrFRyw_RyhULYRZkCcyXEpSeG1_M,840
 qai_hub_models/models/ffnet_78s/__init__.py,sha256=EZpm0M4BbQeAEyxW-qlt-qCFp0idi3Kbyo3qkhmtoIc,479
 qai_hub_models/models/ffnet_78s/conftest.py,sha256=N2PE09r7n-g3UnioClwy_if8WTcGDOh75pzudvdUYk8,876
 qai_hub_models/models/ffnet_78s/demo.py,sha256=4b0QiRJKM-uCuJbmhV5MlO9k8k9Y8nYHs3khMb8-IGs,582
-qai_hub_models/models/ffnet_78s/export.py,sha256=HtMqwrivuxEuCrpfaxdZ3yaEKYmls7Ew85L_lkoTT1I,7994
+qai_hub_models/models/ffnet_78s/export.py,sha256=rCr-Ys2h1WKF0EZ0XBjKFGp2sD7Yh2XzG2oiNf9FOVk,8014
 qai_hub_models/models/ffnet_78s/info.yaml,sha256=eTVW7aCWkN36QgKXhmSHmuKgJE907R_6ZaFBHLLzY_w,1279
-qai_hub_models/models/ffnet_78s/model.py,sha256=rgymsZAJKzfcMzj0zoQnTsOsJRd63NyHFipIDiPpBvI,563
-qai_hub_models/models/ffnet_78s/perf.yaml,sha256=kZDVh2MXCJxVf15TbWMFqrp49e0VT-0PfsiL5RDk0OU,2746
+qai_hub_models/models/ffnet_78s/model.py,sha256=zxXU_ZSi81vLKD92yok1BS3vgLgLkS8p2sfGJKZT0Os,580
+qai_hub_models/models/ffnet_78s/perf.yaml,sha256=uud5Vk2LVn66tdrili4OMw9DRVOj0Bf4ylecLgfSyB4,2772
 qai_hub_models/models/ffnet_78s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s/test.py,sha256=rgdUjMRz67XdsnmHkyNH0ba3uvbXYyVfe5PfVnbekEs,746
 qai_hub_models/models/ffnet_78s_lowres/__init__.py,sha256=dGFCFlJOiHyZHYM-RC3aU-dT6dU1kGGAXxtoFtI1GPY,485
 qai_hub_models/models/ffnet_78s_lowres/conftest.py,sha256=LPhhGBo5gI8i6cwAFgwiw4ZAKB8KGBCzNoS-OqSOxtc,890
 qai_hub_models/models/ffnet_78s_lowres/demo.py,sha256=DW_YMV_N9oqo3jn2VLiN6QjKDy2n70Y6UzTxABv7XAM,601
-qai_hub_models/models/ffnet_78s_lowres/export.py,sha256=LE4pxf8QduAcFmWt5DQJLR1fyi1UX3XVOK8QSCdbuc0,8022
+qai_hub_models/models/ffnet_78s_lowres/export.py,sha256=SXpYonaWEA8QsVvmum1bOlUKSjjJQJOtZcTbq9jqbV8,8042
 qai_hub_models/models/ffnet_78s_lowres/info.yaml,sha256=Mgrc9Bf84QxaywT0AIXKUx2zorycN8lQ2bCBl_VLN0g,1327
-qai_hub_models/models/ffnet_78s_lowres/model.py,sha256=QtNd6UJrLDNa1me6igjywL2S62MYrl7svsEkVuqcoH8,623
-qai_hub_models/models/ffnet_78s_lowres/perf.yaml,sha256=e0DZfm8-03Dbsscy-KJjRi9eGHHKKYmLABfto3aCcYM,2739
+qai_hub_models/models/ffnet_78s_lowres/model.py,sha256=ckAt5bunYsfv5pqH9jrwREUfZQJwaKtcgBCjOz-Vw3I,640
+qai_hub_models/models/ffnet_78s_lowres/perf.yaml,sha256=zBdBL48veCtBCAewLG0Tl_Avfg3qX2WRdS-ge8oavhY,2767
 qai_hub_models/models/ffnet_78s_lowres/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s_lowres/test.py,sha256=XuLMLq71pxQcI6SDVEqtwyl86MaLtxa9mtlLDfBgSSk,794
 qai_hub_models/models/ffnet_78s_quantized/__init__.py,sha256=byB0kzkpkVxI7x6LjOGtge9X58UkVzOkp_VjfUdJrVw,490
 qai_hub_models/models/ffnet_78s_quantized/conftest.py,sha256=O63QPLP5mukarSVEwK1E8XEnaShCZ8oEBbtf22TMEtU,896
 qai_hub_models/models/ffnet_78s_quantized/demo.py,sha256=3mI25tVr9FGDR2y5_2PuYvKJ5vHCzE7Ogweps8oziAs,627
-qai_hub_models/models/ffnet_78s_quantized/export.py,sha256=mID9ipyvuT8XBpwXilig2BnR7fqsmnjxjPwrSsUiQEQ,8447
+qai_hub_models/models/ffnet_78s_quantized/export.py,sha256=NmxGcUmhUsgEX_zlNsHpXLoQln0FMNWKewd_VMt0jk8,8467
 qai_hub_models/models/ffnet_78s_quantized/info.yaml,sha256=WdgiQxWS3hXRs4A3xeP-Mis44omsqUNaQPhIHFYJ2r0,1347
-qai_hub_models/models/ffnet_78s_quantized/model.py,sha256=rQWWcKJIbN-MS905vqZg2mdH1h8_6V-nGuU5qqEzvyU,1139
-qai_hub_models/models/ffnet_78s_quantized/perf.yaml,sha256=NvG1rp4zmfftYdlcJ0rBHVUvlHrAvqNd6aZFu50OtMo,2689
+qai_hub_models/models/ffnet_78s_quantized/model.py,sha256=gilgE2OHPv4mCaKEr4AAcFyFzAdUGq92_RIvhbcqNTA,1156
+qai_hub_models/models/ffnet_78s_quantized/perf.yaml,sha256=Hi26J7cnCeCwhDIU9ygI7OIuKanzQ2y6fRht5LrOMec,2713
+qai_hub_models/models/ffnet_78s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s_quantized/test.py,sha256=f63BCpe22Yh5AtLy3HmZ6M37zu3G7kr18SmevBwh-VI,840
 qai_hub_models/models/googlenet/__init__.py,sha256=yMXfFedJcmupf20Fk8xXuArMIPqOdIvxB2S_AJ5j1Pk,472
 qai_hub_models/models/googlenet/conftest.py,sha256=nWQxu_g_VJBfcFCE9g_PDZQF5XBs4O3LAeEWaZHuiyM,790
 qai_hub_models/models/googlenet/demo.py,sha256=Owxr4VAkRm-resJoBLK_1ZcAEnI2IwjHZ6Hgqs7zYXU,533
-qai_hub_models/models/googlenet/export.py,sha256=nE1EMSKX0ntFVJsLLsqMriI8gpGtGOWQX0a_f64PpXM,7859
+qai_hub_models/models/googlenet/export.py,sha256=29CZslvFCuCXsnN17D1EKFzhx7m7-YLs0ngbmzxHNrg,7879
 qai_hub_models/models/googlenet/info.yaml,sha256=6Pb1-Auz6VGYb2FEYhCoKR6a7sJxkm-6eJ9KPoGz1_c,1295
 qai_hub_models/models/googlenet/model.py,sha256=7tVmhQtiQZeWGgeYTPVvwKr4ustB0tHoYafJyXZvVKQ,743
-qai_hub_models/models/googlenet/perf.yaml,sha256=gNn0honjzlpEBiho9KqAfeyLH1T8sHfUGjQnz345soQ,2720
+qai_hub_models/models/googlenet/perf.yaml,sha256=L0sREXe1qw5fgfw1pZuGB7PlhAp8xyCUP8T_5ug6Urc,2748
 qai_hub_models/models/googlenet/test.py,sha256=gJcMSpz8KmfhCwa9E0d6iTnlOB40dHwsr8Qp9B4QdAQ,840
 qai_hub_models/models/googlenet_quantized/__init__.py,sha256=96ENoBCt6lWCfOBrvFQJIE9xh-sA3IDtfUXmdIWVhqg,573
 qai_hub_models/models/googlenet_quantized/conftest.py,sha256=Z_JTshE8NJGBJvtRIt4qQuQHigIRkrMrpriRwaYNNmo,810
 qai_hub_models/models/googlenet_quantized/demo.py,sha256=FT-qPTxt9WnT7-wPR0jitsw0LnhvkbHlRCfJE_jiB-Y,578
-qai_hub_models/models/googlenet_quantized/export.py,sha256=UhaCFhIxIILWkIpJ2CbKAESEXzTUsnwY8VNHIVouvvg,8339
+qai_hub_models/models/googlenet_quantized/export.py,sha256=eF4ic913Mj9jzn3GdNGRhzDBNJ90r2OY7X9Hb1ANE4M,8359
 qai_hub_models/models/googlenet_quantized/info.yaml,sha256=VmjYexBXTnmukRqI8u0IEJYGZxMbKxze2l-Q3ROPyC0,1325
-qai_hub_models/models/googlenet_quantized/model.py,sha256=mUSMFcEUHBqYFMXAZNCZFhXTVLCuMKFX_z0W2x-qE48,4310
-qai_hub_models/models/googlenet_quantized/perf.yaml,sha256=Aype_tquBdO2zTHVUwfFetiE_41aTdI3pX1DtdEDbQQ,2729
+qai_hub_models/models/googlenet_quantized/model.py,sha256=unZShibpfs5W2U714o6vbyn6IOiSB3PpqwnKuYm9s8U,4079
+qai_hub_models/models/googlenet_quantized/perf.yaml,sha256=_dUK8oHdn1xeOSGH3XmptRhSigLoSO-RUI6zpE5C-eA,2753
 qai_hub_models/models/googlenet_quantized/test.py,sha256=ox8Mz4egNJmOQzfD-gNNXu9k5zHs7ZHKn1rV3xlNNd0,885
 qai_hub_models/models/hrnet_pose/__init__.py,sha256=_r8rdSD0i5yJRlSct5_EQpCV-hEExVZYmI_mHV5VkWQ,430
 qai_hub_models/models/hrnet_pose/app.py,sha256=nLCTOgYN_t7fQZyreZq2B_C1EwiAPRDXVj6fjZVZ3-I,5644
 qai_hub_models/models/hrnet_pose/conftest.py,sha256=WQ3j8PYRttJ_1ke9rYx6TmV1f6II8HEauWjr-ze3Tnw,878
 qai_hub_models/models/hrnet_pose/demo.py,sha256=-XbTGmCKHJXe-qoj3Abrm81VZr1FGiJQ-Za6FBVFBww,1739
-qai_hub_models/models/hrnet_pose/export.py,sha256=qPptGk_krr9u4L8LY0iM6SKsgJ1qYI1bzDPUF-iz0yk,8154
+qai_hub_models/models/hrnet_pose/export.py,sha256=3e6QFsKCsF5Thvcywll162yPLnIWafsi0421bg5yGSI,8160
 qai_hub_models/models/hrnet_pose/info.yaml,sha256=QdjXu2cv2b6EpWTS92p4iSu8-D2YmcRv59Zdxae7-yA,1195
 qai_hub_models/models/hrnet_pose/model.py,sha256=b5b4NUBfnMaqre3KXue5reT01t4RAF9VHv8Jvh6ixBA,2801
-qai_hub_models/models/hrnet_pose/perf.yaml,sha256=k08UUjcP-DAviGYtPMDRihQu8ZEu0WFBNu_bWB_q1Tw,2733
+qai_hub_models/models/hrnet_pose/perf.yaml,sha256=p3FsFulgslYjbSf6JfkK3jgLIjtVSoKd2BoKObNlza0,2755
 qai_hub_models/models/hrnet_pose/requirements.txt,sha256=2U8K4vxuQ1x6tB35TVOiLW4UA0gN77o0ZvKJ5baKAgo,51
 qai_hub_models/models/hrnet_pose/test.py,sha256=LHAh3hxZfE1aeQt5Cwvv4SF3w6X3Dxz4tly_UxZpr84,1420
-qai_hub_models/models/hrnet_pose_quantized/__init__.py,sha256=4OHJ4Aj2yqIYUx2EaB7lUFuHe2zUT0VcD71CQTOTIYI,441
-qai_hub_models/models/hrnet_pose_quantized/conftest.py,sha256=kH_Re7UjTmkFN_VxF_gUN31hymRR1pSNyt_VeTnd3UE,898
-qai_hub_models/models/hrnet_pose_quantized/demo.py,sha256=e8s3OjC6AVYEPnKTrtHssntwjts81h-fBlbSueXUCH4,1830
-qai_hub_models/models/hrnet_pose_quantized/export.py,sha256=oOg4M0pR36MCU6hmt4zN0rneESJO--7g8Ahkjz7eUCk,8606
-qai_hub_models/models/hrnet_pose_quantized/info.yaml,sha256=5xmA8oj_2s-RCI7Q-bPNcfWsW2O9dA-glDuxDaFWi4I,1227
-qai_hub_models/models/hrnet_pose_quantized/model.py,sha256=0jbgRp8fDCinbHFj7rZOzFQxmTOw_Fub79B-o7208pQ,3132
-qai_hub_models/models/hrnet_pose_quantized/perf.yaml,sha256=Xaem6oGph2zc-2VXXbAw4cklU1jWWS3CcstM35vpky0,2686
-qai_hub_models/models/hrnet_pose_quantized/requirements.txt,sha256=2U8K4vxuQ1x6tB35TVOiLW4UA0gN77o0ZvKJ5baKAgo,51
-qai_hub_models/models/hrnet_pose_quantized/test.py,sha256=c1vZxj8WMkM_BsuXL14iVNwL3YDhbHa_wjSXiJmRASg,1588
 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py,sha256=muVZdjzxn0rzXTLE50i1-birAkTSWucNt1VWyCoUwJ8,434
 qai_hub_models/models/huggingface_wavlm_base_plus/app.py,sha256=QlmoRNNkpzNGI-EzErNXsR8S-ygiC09QdAiS7ggnQs8,2133
 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py,sha256=JS4OCY4Vd8bhvxG0n7t_zCB405P5EAN6oEo3WmRkb84,912
 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py,sha256=NtjLWFe0oiJgcm10f5ws6a4SIx-7RNbRLL-Q3pB3csY,1517
-qai_hub_models/models/huggingface_wavlm_base_plus/export.py,sha256=1UFpGO7C7-KmCNEbXKk90NCqH7-7vhrZP5gfHYiBl5I,7547
+qai_hub_models/models/huggingface_wavlm_base_plus/export.py,sha256=W9Mc2PVm2RlSySza9709N54UQ91aKQgybR6UJWmQx8Q,7567
 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml,sha256=fJzxA3WwfNBQnWZq65pR90IGEHzlm5hxoJHvzEhVN1E,1248
-qai_hub_models/models/huggingface_wavlm_base_plus/model.py,sha256=mSpK6obXsKLjpwWGZliAHwZGtkR02EdhHZkCTeWMhDQ,7886
-qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml,sha256=xDqpvzMnXJIfpcEqyrQypNjDvXqPr4s9lfSxwW25Uyk,2719
+qai_hub_models/models/huggingface_wavlm_base_plus/model.py,sha256=BjWZCqVFXNJaINoT1San9dY2aZOAFbZjJ_WKQLV-CGg,7587
+qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml,sha256=viyUwY7Q2FjQQo9Mco1EImWq0T_GEEq6PptZN87VTgg,2735
 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt,sha256=L9-RRcRpUwd13vGIthoV8IQaEALYng-Hd7RhR1hdpSE,72
 qai_hub_models/models/huggingface_wavlm_base_plus/test.py,sha256=irHDKl6YChuYRBq8zcNwe5p7CLd9ghY7yicSfHEtBZw,2560
 qai_hub_models/models/inception_v3/__init__.py,sha256=7VMcF8m79rmZ-UZk265C_tIMIHJev0qZFBz5cNzSd98,477
 qai_hub_models/models/inception_v3/conftest.py,sha256=8GqhXxOL1_zP9gPtyLGLVwMD4k-UKYnZIaSB0s1eJ20,796
 qai_hub_models/models/inception_v3/demo.py,sha256=f9m6H1zPOHEGK1fi4PLDUYPb8TYoa8a4MqbjRWZBX1o,546
-qai_hub_models/models/inception_v3/export.py,sha256=2yplJPeK_EeWeHaIloRq1sZkkZ9I5mmWIlQYfbd3lHQ,7871
+qai_hub_models/models/inception_v3/export.py,sha256=ISZkP0_TbAFsH7ZSQUmYgT8KVnnuy9-I-KB13sKxrq0,7891
 qai_hub_models/models/inception_v3/info.yaml,sha256=XoUgkOsDw_vFkVOOMARDWbBGFdcsRt2EcGqIqJZC_BU,1359
 qai_hub_models/models/inception_v3/model.py,sha256=PKOe7hJigPXwymvjHC8Q_mwVFj5LTp96cIPA6hoaDto,756
-qai_hub_models/models/inception_v3/perf.yaml,sha256=06MWLEhQmDva4tTmHuy-NMRTAdpijVPGczcCRJFWYi0,2735
+qai_hub_models/models/inception_v3/perf.yaml,sha256=yOMAkic_cVwx1NwLq8r6QJK3sz7pEAIT9z5KkaW1-kk,2760
 qai_hub_models/models/inception_v3/test.py,sha256=H4y4OTCinQAYkJvaceCOenfWzwjPqpWb2kk_hvSm2WM,861
 qai_hub_models/models/inception_v3_quantized/__init__.py,sha256=LYwSg6XsJc3GL5Bvm8AjHw5I2aCmG_kmpyQQRxxYlsM,584
 qai_hub_models/models/inception_v3_quantized/conftest.py,sha256=SpX7-f30GnW2LNcSLm9Ht8rxD5FYXO_j4uRoQZXZxCE,816
 qai_hub_models/models/inception_v3_quantized/demo.py,sha256=HT8ebEriXX7NZpj9rdy6CKBWSdpM10pxniEvhwVFe10,591
-qai_hub_models/models/inception_v3_quantized/export.py,sha256=i9PEgr3eV_zW3DSjx-QGvjxkiiKwqDoAzKVQqHQYEk8,8372
+qai_hub_models/models/inception_v3_quantized/export.py,sha256=cD_bkS8q1V1BDMVCWVS5Vh3BLnYnT89aCW-CWxwzRKc,8392
 qai_hub_models/models/inception_v3_quantized/info.yaml,sha256=FsSC1SKNT3quSNQaELEGwfybCmlNi1L2xiRI2oV08CQ,1556
-qai_hub_models/models/inception_v3_quantized/model.py,sha256=bb_QmpJtWoG-k1C8HRjw6i46imvrvWt_14vqOmb83hI,8094
-qai_hub_models/models/inception_v3_quantized/perf.yaml,sha256=ZO9U29laHqy1JETiqnMH-Oxh6-27feR4ucWLi1tEJmU,2684
+qai_hub_models/models/inception_v3_quantized/model.py,sha256=-Ie1rVfoQw0-u1Q2JAJD4YIn0rC_qN_Z-5_XOBVemr8,7543
+qai_hub_models/models/inception_v3_quantized/perf.yaml,sha256=WEgUVNtrxyAPQ8BlyLd593uHZmKTt7wrIQXRzQBjl_g,2712
 qai_hub_models/models/inception_v3_quantized/test.py,sha256=Xf0YrWDwy01ulM-o4IuAOQotc6hOU0Dn2IaV7h8SrA4,901
 qai_hub_models/models/lama_dilated/__init__.py,sha256=XfDJqXXsCsai11UnjLNnECFN6WquE9I55knTyDl-R7E,455
 qai_hub_models/models/lama_dilated/conftest.py,sha256=kak27g05SfrZ80BRbjmraYuFRvWTY0j5f5NYWHEXtq8,882
 qai_hub_models/models/lama_dilated/demo.py,sha256=J8E3pR2GQTpmpqFfqPiQOfKUcywmDMPAzyUqIj2wMZY,911
-qai_hub_models/models/lama_dilated/export.py,sha256=jxEIE7fTFtOR51Q4W1BYo2_EZo4X4IPPtXtGhvZqpK0,8159
+qai_hub_models/models/lama_dilated/export.py,sha256=JxVF14yOa09nmTuXQIhLPj7CvLpyVSKmiCU2bNby7NI,8179
 qai_hub_models/models/lama_dilated/info.yaml,sha256=whgpaBOBqJnbIc6ErqRS4PTlaeBIkoJBEtrfscPeC9U,1082
 qai_hub_models/models/lama_dilated/model.py,sha256=uJp3NNmPQuzBXZ_gcgk1pK4DDkG7JmO_R-KlpbfpLy0,4977
-qai_hub_models/models/lama_dilated/perf.yaml,sha256=SkmFa854R1V86nEFDQt7AV641FphMT9aa4fc5jOvVNY,2752
+qai_hub_models/models/lama_dilated/perf.yaml,sha256=l_hQdS5uwAj8pq5l-mJIKfUS2D6g2TzhWt44IGTE96o,2776
 qai_hub_models/models/lama_dilated/requirements.txt,sha256=897JpKdjy0fMli83RtBp6iSBaWYRHXNDK0oMaO5aMBo,171
 qai_hub_models/models/lama_dilated/test.py,sha256=g1YxEMQjYA65848I8YM0lxylfy4f0LGSJ38pJXrPOYM,2074
 qai_hub_models/models/litehrnet/__init__.py,sha256=lboiEAPcBxZN_ao_VVtBVJl-B9EvrrwXcxK02PiZbLY,404
 qai_hub_models/models/litehrnet/app.py,sha256=FEwh94V2uJrp6bofDJTP_p53fR85Ir-nCiacakJ90BI,3986
 qai_hub_models/models/litehrnet/conftest.py,sha256=8Jdb0v9ZiVff-cdb2fLdh1pl8IbO43mvtBH6own4zFo,790
 qai_hub_models/models/litehrnet/demo.py,sha256=N3NDwdOwfdjFANzufoZq2mBE0yk4vTZZFQCl_fgk9Ds,2072
-qai_hub_models/models/litehrnet/export.py,sha256=x5Ucip6hqgIoSkjNQfuRsXldtPX_dlcG9dQiToNzAvU,7618
+qai_hub_models/models/litehrnet/export.py,sha256=hDups2agm9CsHmE51yhKzWTP8xzzPb_D_ogEW5MnkYM,7638
 qai_hub_models/models/litehrnet/info.yaml,sha256=xfMHffAMgyP3Z0UT9aBd5_C5yF4QYifTOvXd4-yGEDs,1112
 qai_hub_models/models/litehrnet/model.py,sha256=XELmt0HE-7-ZHQ7VOZAJNdAWBqR2Cg-zK_0ifR4UxGc,3788
-qai_hub_models/models/litehrnet/perf.yaml,sha256=QwwxagHsAav75Mjx23IfSCF_mHJID6qXHIsuCc6-HUE,2687
+qai_hub_models/models/litehrnet/perf.yaml,sha256=DrgFbQRbFzHLPM0aGORhzrawZ0uRXF4iVgTOxQDHBT0,2711
 qai_hub_models/models/litehrnet/requirements.txt,sha256=CFGaEErf8SbXCYCCjg58TvJZIExg-yQiluC2q78zZoY,39
 qai_hub_models/models/litehrnet/test.py,sha256=FStsyYV21iqYE_DTsLCDyFBISas7lTXpvHrP86Xxc3M,1714
 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml,sha256=rB9tYtIqi_cMMnWDorvulawmTGH7ATbTbWGQm8mvU_Q,1993
 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml,sha256=vS9DRf4_wVb93PCBo07qhZyOK37y-mcsER-FMIs4agU,2037
 qai_hub_models/models/mediapipe_face/__init__.py,sha256=24U-bkhm4gXXnVLowI7uytJShrDYSq5KbKWGxkK-xxM,412
 qai_hub_models/models/mediapipe_face/app.py,sha256=eug0kpc1nRgnnhO94P242qu8x-P15nmK9YDCRVi_4Ug,2099
 qai_hub_models/models/mediapipe_face/conftest.py,sha256=u_PEZIw4pyNtWcc3PeVwpad-ADvY2KESeet2MR-lUmk,886
 qai_hub_models/models/mediapipe_face/demo.py,sha256=k43EC7_YMXA4Wk41JwLaCtUc_1HSQFEyZ5UY317fWG8,2862
-qai_hub_models/models/mediapipe_face/export.py,sha256=SmcbF3JulGocgi-sDgpd1vKbxdWFRwWyWGcNWvNfnj4,9984
+qai_hub_models/models/mediapipe_face/export.py,sha256=TkGCrVAO3QQkHARQUXUXlIwLSWFoRyaVKT-NhXnACEM,9755
 qai_hub_models/models/mediapipe_face/info.yaml,sha256=QX3h5VsxsrDt0X6b0QpMelIhilsHJr6MatZewTwF2N4,1469
 qai_hub_models/models/mediapipe_face/model.py,sha256=WurHj_hb5bf8eOyryuBUAKzOC_mZkRgeM7fPpvERoss,7669
-qai_hub_models/models/mediapipe_face/perf.yaml,sha256=JFAKktbD4XCsTia6jbt975s5jbWA6Hd22S4eWczUQQo,4826
+qai_hub_models/models/mediapipe_face/perf.yaml,sha256=idJKz9rGEgdF4bQj6PPbjcQq_ZNklEKBaXQ7EY3XDhI,4857
 qai_hub_models/models/mediapipe_face/test.py,sha256=eAgJ0ZjbMNL1bRBF6Ma-mVQ0hBMTQvbUEWO6PH8Gr4I,1441
 qai_hub_models/models/mediapipe_hand/__init__.py,sha256=XWXPWZ12S5pgPBmT3fwgql_JF_6Y_dY699q8nntteVQ,412
 qai_hub_models/models/mediapipe_hand/app.py,sha256=4-UayauaiZDVytVsclGrBnqvGjVQNf0PPCUwYVk-KLQ,9819
 qai_hub_models/models/mediapipe_hand/conftest.py,sha256=Y9jfcCYPUKVWKasFl8Djmq6HiiFVbWEgc42Bu2inG1c,886
 qai_hub_models/models/mediapipe_hand/demo.py,sha256=u8ej7boLAgN-0O--m_lXFTVXb-4WW3qU4WKR5sijEJo,2832
-qai_hub_models/models/mediapipe_hand/export.py,sha256=I9eQlzVP5P-UXIj-cp_Os5L_gUEQkHTG-7vBzDE-rxI,9984
+qai_hub_models/models/mediapipe_hand/export.py,sha256=p3n9sw0U5MSClD3IDR-kqxk-411Oht1H03AS_kmkSoY,9755
 qai_hub_models/models/mediapipe_hand/info.yaml,sha256=oSKmBY6rTjq_Jj8ToPWD7eIu6cxkVpB1RGuQ23rb7WU,1374
 qai_hub_models/models/mediapipe_hand/model.py,sha256=Mjz_ttgFHlNUsWIKbcIGEU3j6TMiOLHZvmnF0eY1df4,6017
-qai_hub_models/models/mediapipe_hand/perf.yaml,sha256=glynJS0TK9hjrGXlD9_JSwpLbQuyQQ875V5_uHzKUFE,4829
+qai_hub_models/models/mediapipe_hand/perf.yaml,sha256=RupXMBE1UNs_LIUzNykXj44YWqWo3q9Hb22P37wzjjA,4858
 qai_hub_models/models/mediapipe_hand/test.py,sha256=y-8RhCNppYbhLY_znVZKYVNwElIaJPdSkXEfBoyfGcM,1442
 qai_hub_models/models/mediapipe_pose/__init__.py,sha256=PE4aVOSnDXWvU9YVcKjBKUyO8926RMaeRXyD1chVXpw,412
 qai_hub_models/models/mediapipe_pose/app.py,sha256=djV3gcRkcvw2qYHBXqOjxY_2DxNahekk4BZ4Iax6uW4,4430
 qai_hub_models/models/mediapipe_pose/conftest.py,sha256=MmspZZI_CgLve8c3ltOgk6VmHI1b6l1bnxQZJyKrKdU,886
 qai_hub_models/models/mediapipe_pose/demo.py,sha256=hvy2P0bX0dZ9cU0NrxSN8MjJqfat2M4GHlXjKcvixO4,2889
-qai_hub_models/models/mediapipe_pose/export.py,sha256=ra1xEKuFqIODQNySfgomntCMjXMis0Ujm10_7_ynD4E,9985
+qai_hub_models/models/mediapipe_pose/export.py,sha256=1SkXwMYtcqYpP-2Dw5KNyR30vzZPAhgApm-xae3GZJ4,9756
 qai_hub_models/models/mediapipe_pose/info.yaml,sha256=D2BVSmo_qZbGJIbONvBpj2JJcJFGtQA78VBMyInXkIA,1387
 qai_hub_models/models/mediapipe_pose/model.py,sha256=oPrNq_v0zKYU0OL84GwAKt9yUsdAaMsH4lDNsPB0k1Y,5801
-qai_hub_models/models/mediapipe_pose/perf.yaml,sha256=pebVd5Y5VCD9LM81cTYXLmKhZxJ9DjZQfecNAK8Ukrs,4828
+qai_hub_models/models/mediapipe_pose/perf.yaml,sha256=2hdFOFy0yhyU6F2XbcOYVXFe1l_o4jSTw4ziqgJ-ePI,4849
 qai_hub_models/models/mediapipe_pose/test.py,sha256=rsTIYChYPqBL1lI9cl4y_QR6jCmRhb4FgAjcwhLSGA4,1443
 qai_hub_models/models/mediapipe_selfie/__init__.py,sha256=TrPNyvFmctjh5bMloBxJAIFZTgojNj16AOnPBH6T8R4,362
 qai_hub_models/models/mediapipe_selfie/app.py,sha256=nWD63c4A6_M0gwC8P9n0dTlzsXL45V9Ep8I6hy3Ytd8,1411
 qai_hub_models/models/mediapipe_selfie/conftest.py,sha256=0Xztp_WqSok3jXJ39g1tHVGJzGE_EGKsi3iXakYJgHk,804
 qai_hub_models/models/mediapipe_selfie/demo.py,sha256=DULze0M58GkHmBIpIEoB6So-EsgYoqHBC3CQspRSR4Y,2674
-qai_hub_models/models/mediapipe_selfie/export.py,sha256=540U6iPu70A_Puur8e978MQzSAbdQ-vvmStLrQuIOcw,8178
+qai_hub_models/models/mediapipe_selfie/export.py,sha256=VqUG4XMoRgNCeSzHPLvZS3HBPK_K5OkUgXZuzG_NJbM,8198
 qai_hub_models/models/mediapipe_selfie/info.yaml,sha256=gEUPVKMaB0_qlPbEo1zM8uQ3dqAPK6YKQWHUNNOFdMo,1455
 qai_hub_models/models/mediapipe_selfie/model.py,sha256=COQHgx4VM0auz9h_etmnh1l3nQGU2BzmhP4qoqdIR_M,12352
-qai_hub_models/models/mediapipe_selfie/perf.yaml,sha256=nzFL9EifuOPAMcQXZ1EErmqy8JejDqZ84cRlk-Xm0X8,2750
+qai_hub_models/models/mediapipe_selfie/perf.yaml,sha256=KKd3MVb1fm6P4_g0xF0p4yUPI8S7aSKTX55fYwMWGbU,2775
 qai_hub_models/models/mediapipe_selfie/requirements.txt,sha256=jjmBqpB3WUtMLBjVgPXNNN5yslq_EXOWdM9WMOES_a4,15
 qai_hub_models/models/mediapipe_selfie/test.py,sha256=5VWzDmnNS9jz90tZhqTPIIFaBUeHQ8GYGJRnmpvIavs,1396
 qai_hub_models/models/mediapipe_selfie/utils.py,sha256=-Fh52J_5r4Ty0YZiKtwSnP4yi93K3kpBc3-Y9jPCEUk,2492
 qai_hub_models/models/mnasnet05/__init__.py,sha256=d3xeB69OYbrgNbOBMVMVsEqwClH2MMhlJqhfSUIx8UY,472
 qai_hub_models/models/mnasnet05/conftest.py,sha256=w12ZIoLSzul1FaGhnOb_kNo3zgkcXS5ucDGsP0_IQXA,790
 qai_hub_models/models/mnasnet05/demo.py,sha256=Fgz0-kQofu_pf78mBfAA0kcenEwRIYtoLuaofrbcfYA,533
-qai_hub_models/models/mnasnet05/export.py,sha256=9dd8ctLZttD05PycYh8vsJdJ3h_i70tkAcMRNNEIGpc,7859
+qai_hub_models/models/mnasnet05/export.py,sha256=e7ZUgRTMEUpSn_dtHm64jFB1fSxaeBwf_ra-KBavDS4,7879
 qai_hub_models/models/mnasnet05/info.yaml,sha256=A7bKsnRyqJJ6Jnx8SMQmn6EBaOMvHwXW08xOCLdT60o,1333
 qai_hub_models/models/mnasnet05/model.py,sha256=kkgjfQ0BbYuE_ls2Yh1zHIqncG678OyyvGTJGJemIjo,699
-qai_hub_models/models/mnasnet05/perf.yaml,sha256=DkKcmLUGA3bfNoAKC3iAeY5uLhojdWrQ46BnCqcuiE8,2720
+qai_hub_models/models/mnasnet05/perf.yaml,sha256=9JQf1NXHf8s9BoRn0MyDI_M9trz9E2DYPQMZm7OSze8,2751
 qai_hub_models/models/mnasnet05/test.py,sha256=Vcc7zhvyUHQXDj3mcFZetvZmV0y36askMPKqPZRJ1i8,882
 qai_hub_models/models/mobilenet_v2/__init__.py,sha256=EF3fi_jeNIAmKhFrVJdKaZjazdFlsYXrW1TZT3fDaxY,474
 qai_hub_models/models/mobilenet_v2/conftest.py,sha256=3ONStgL-YiNc4ijSBHHtG_01yKOZqQ0XWMbyw9-MmpE,882
 qai_hub_models/models/mobilenet_v2/demo.py,sha256=GR2uHGNaKtqt-bIO7rjjgR2V0fdziHAQWwDZs7FvZV4,540
-qai_hub_models/models/mobilenet_v2/export.py,sha256=ydgcyaAddaXnR5L01gK66aUeL4cXAO-eYZtk3WagnNw,7871
+qai_hub_models/models/mobilenet_v2/export.py,sha256=ItKh11DZbK3kP7ylpeAUeN3tP7YSbQSx5D3oU3GCY88,7891
 qai_hub_models/models/mobilenet_v2/info.yaml,sha256=irPP5qDlcSbPbjkqJSO4k_iA159GEVB1u4pIruYAlyc,1380
 qai_hub_models/models/mobilenet_v2/model.py,sha256=qJy2c5OH18ugzKvzoBU6gcTsxTZVToRvtjm-7NUSrzU,2595
-qai_hub_models/models/mobilenet_v2/perf.yaml,sha256=YxdvuvF-jJcMUzIOb8hSTA_oel_lWONU-AUaaJf9mss,2728
+qai_hub_models/models/mobilenet_v2/perf.yaml,sha256=CICp6vJGCfnKLNR1NYUkJRxV-td-TFt8kUJZKqEgoMk,2755
 qai_hub_models/models/mobilenet_v2/test.py,sha256=Kjj6s-5jkKoqRaV7GkE9zf7js34k37uBSluNMFM3V-4,1091
 qai_hub_models/models/mobilenet_v2_quantized/__init__.py,sha256=MQOqYDYL-abmQsTEsRl7dBpFWu0Vd2mAW64gj02npN4,485
 qai_hub_models/models/mobilenet_v2_quantized/conftest.py,sha256=v1HPC1n5jDeGJBd_KOOwUQpbZsuqKISivzFY8M5-_Lw,902
 qai_hub_models/models/mobilenet_v2_quantized/demo.py,sha256=oxbLdz1mYSbr0GA4W6nRtFvnAp9Muma8F3eTP2VEKNo,585
-qai_hub_models/models/mobilenet_v2_quantized/export.py,sha256=IoiGIixJLpne2SBA6paw18UbkfKHrQwneEtl4fk2TFU,8352
+qai_hub_models/models/mobilenet_v2_quantized/export.py,sha256=PVcqu3hLtl38h7BwBdKbYOnfn3d_qwiI_PXs0L8aBx0,8372
 qai_hub_models/models/mobilenet_v2_quantized/info.yaml,sha256=B8RS3tsewoOtaOVFYWCp6aVqKJ2VIiCK3GJHfz8ghPo,1362
-qai_hub_models/models/mobilenet_v2_quantized/model.py,sha256=W7yJZaVF2YlzM20w87Dhz7gNErjLmZ9cMjFk1oqw4Wo,4311
-qai_hub_models/models/mobilenet_v2_quantized/perf.yaml,sha256=S9UydkAJtOXg0L2rXZQuVxdRIDfFo7v7sfuM810u0X4,2735
+qai_hub_models/models/mobilenet_v2_quantized/model.py,sha256=d7kyCZ1rh6r05U2tQbPZbsJUAPNYh45bLAybNABZqx0,3760
+qai_hub_models/models/mobilenet_v2_quantized/perf.yaml,sha256=2W4IBOoI1ni4rOyyKQyQ_XhBFBmA03AyX3HaVmW8KGQ,2759
 qai_hub_models/models/mobilenet_v2_quantized/test.py,sha256=h54ZCxWmJA87qmOXeFpYZl8_3wLUQgDAJgowNlk_QhM,1002
 qai_hub_models/models/mobilenet_v3_large/__init__.py,sha256=7a-2Ng3rVspm46Uf4wyI8WfezV5IVaKm-oCfdkM0JVQ,479
 qai_hub_models/models/mobilenet_v3_large/conftest.py,sha256=vs8V8AXuTGrSY5oN8mmGq66-W33k91znQW7n98DtPS4,808
 qai_hub_models/models/mobilenet_v3_large/demo.py,sha256=4KxbUpanGuUgvVcRk-YTTf90uQlEsMaBhbpjmXYEPUI,556
-qai_hub_models/models/mobilenet_v3_large/export.py,sha256=arXpxlryLfmMZKwZd9JcUW_ECtoKRsI-XxJf7TLq54M,7915
+qai_hub_models/models/mobilenet_v3_large/export.py,sha256=D4pRUuMYudoy7jVLy3vrpf1RYkg8PuhnubHYp7K3pGQ,7935
 qai_hub_models/models/mobilenet_v3_large/info.yaml,sha256=CbC2G_Zw1YshI-danUA20dWHRyzP2ZFsqHnVZU1nBUc,1340
 qai_hub_models/models/mobilenet_v3_large/model.py,sha256=AjaEjXhIddKyIg6FSzzGloeZibD96zC7HORYX7MZOqw,721
-qai_hub_models/models/mobilenet_v3_large/perf.yaml,sha256=NrK4YhWk2RcOb7VOzaNA5jmHPu_G79WOuURrOpUBeqs,2684
+qai_hub_models/models/mobilenet_v3_large/perf.yaml,sha256=_6hftU0WB7MUv_F8kcQMTCJr2thpubO2uaiuVnmZNLA,2708
 qai_hub_models/models/mobilenet_v3_large/test.py,sha256=BKu3TBqXd6EelITtwPlPq58e20I530caSxC4vahg_MA,879
 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py,sha256=HWjXMytQDRVZh4KUJTPN8z_oCl6vbY5dtI1lC6C79ZI,607
 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py,sha256=BaonirYxKPSK1Phk_wNWDySVN1vQ0TRr9pQxwuHHVyo,828
 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py,sha256=XKC0CItIVpPeV60QD8VR7zAKBasKdjjcLZGuutvTm3s,748
-qai_hub_models/models/mobilenet_v3_large_quantized/export.py,sha256=j9wsOOQeCoFxfeQvMukNFftfQjvegpYoE6r4kcUl9Jc,8084
+qai_hub_models/models/mobilenet_v3_large_quantized/export.py,sha256=e2NPb_LGjV0kprYLmO8mB6JKMboeqPV452-eiBTJICs,8104
 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml,sha256=w72_OYX5316wOqxjPIrMG1CD4q9mqAsRbE1tnsS7IOM,1374
-qai_hub_models/models/mobilenet_v3_large_quantized/model.py,sha256=xLVnlVPFJJH2NDNgalvEStAakM9ckRk7aZUfi0O11Rc,3096
-qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml,sha256=oBEn_SnqtqH1y8qqRrQlTq8ormOQAqPew36cjcmGGcI,2694
+qai_hub_models/models/mobilenet_v3_large_quantized/model.py,sha256=d2MYpej-SxLYOiiPGEFeIpDjgD3gCu4JCRLOvnCoQoA,2865
+qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml,sha256=Ur6RkOpr7s0BwOsdy9KxXWLQmJ7m9IM-rbN64NPcA_8,2723
 qai_hub_models/models/mobilenet_v3_large_quantized/test.py,sha256=ruAffWPJDAifH_miRT6hNoOnaNY7tJpSXZ42U8zM9Yc,917
 qai_hub_models/models/mobilenet_v3_small/__init__.py,sha256=RhAKXSevAm64F3DtEwC1whT3RiawVPUuZRpHxupCwvY,479
 qai_hub_models/models/mobilenet_v3_small/conftest.py,sha256=fhQMwfPcfBud54LFyb4eZHPpGtH7OuHBnK0Dg24zSPc,808
 qai_hub_models/models/mobilenet_v3_small/demo.py,sha256=f2RqGEK_t9UJ2MNkIYTPnAkeNhOk5ORtBgORks8pQzE,556
-qai_hub_models/models/mobilenet_v3_small/export.py,sha256=hg7gROtgsJ5ME16RC1HWrhbCu6DZ1WPQ_jM_GlG4m40,7915
+qai_hub_models/models/mobilenet_v3_small/export.py,sha256=D2OinHQ0I3p10v50ZlUAgu0BjbBAGIlo72v_ukqLSy4,7935
 qai_hub_models/models/mobilenet_v3_small/info.yaml,sha256=1UbgDdCaGry4M8mc17lsEQHN0c7dzg1OYujzQncLDGw,1338
 qai_hub_models/models/mobilenet_v3_small/model.py,sha256=AMSPekj2w7Odv-HX_YUPs5AIFiDMDmbaq-77eRM2MtU,721
-qai_hub_models/models/mobilenet_v3_small/perf.yaml,sha256=bawET-wT3nKwgFypW3qkDk41vmGq7y_bAON1MLHvwGE,2683
+qai_hub_models/models/mobilenet_v3_small/perf.yaml,sha256=oFl4QGmmIhnYt-YcAr9AnxZ3B1nWhg7pon4Yvj4WWEs,2707
 qai_hub_models/models/mobilenet_v3_small/test.py,sha256=PkDBlLPWfV8ktLcict2uFWM7pn2nl8B_NtKHKIp9njM,879
 qai_hub_models/models/openai_clip/__init__.py,sha256=zmmQCL2hQl0hA_RIPa0pQrKUYOXbMBQWmaYYlSy-XrU,394
 qai_hub_models/models/openai_clip/app.py,sha256=hgOce5Y7aS2Y_V-EltMyp6CkLRXQDL0PTqelz-pBf9Y,3958
 qai_hub_models/models/openai_clip/conftest.py,sha256=3IdZjXX1wp6q_eh5lLcgRLPP5pSJNPtAgJKxvWudlag,880
 qai_hub_models/models/openai_clip/demo.py,sha256=Ti23UndQtFbGYT5Bj6Ywya-mn1C55eh22DaRcyDgrr0,3262
-qai_hub_models/models/openai_clip/export.py,sha256=T2AOlb8ynXfTW9YMZASN5_G9AKQ5D-Q1BGKkFnQoVy8,9895
+qai_hub_models/models/openai_clip/export.py,sha256=KBngL-Ae9--l8FAW0b3NBThAENGwiyug8yNVhownzkY,9666
 qai_hub_models/models/openai_clip/info.yaml,sha256=tZQO7SbgP6M2JFAeoGOzl6WnuLhG_lyfWbiCc9WtpMM,1494
 qai_hub_models/models/openai_clip/model.py,sha256=CWeBH63cKI7nuOOEoq6qs33Rv88O3HHcYS377K7BIGA,5295
-qai_hub_models/models/openai_clip/perf.yaml,sha256=me59RH-EjaGWf98W2vYimlUoN14HY2UHqAtQxh-vc5I,4829
+qai_hub_models/models/openai_clip/perf.yaml,sha256=hhYwjE0wZ-mKDrD48J8fA36BFVLktiMjeiv4tgveTVY,4855
 qai_hub_models/models/openai_clip/requirements.txt,sha256=qCIDOVe-LijMtnGXDgJtHPUP8Yig6otJASVYYaxE4JQ,29
 qai_hub_models/models/openai_clip/test.py,sha256=PV-_Wn-pUdg4M0DbbemnPREGDskjQnv8yqm19L9M2L4,2118
 qai_hub_models/models/openpose/__init__.py,sha256=DJnYvOA5-007mXB0GUl1TnKyBSNqFsQYlpGCpixb9Jk,402
 qai_hub_models/models/openpose/app.py,sha256=MrS_HKYzz8-ylXJH9_x2jEJg1H0ZZDBaYaWDOmme-Uc,12008
 qai_hub_models/models/openpose/conftest.py,sha256=54hvRxRjOJDvpu1Py8cZ8f5lae6F1p_xznYNMdTM47s,874
 qai_hub_models/models/openpose/demo.py,sha256=rPuIuHf7S_ncKOIFWoit8rR1jFG4uvHrAZ_NviMthuI,2053
-qai_hub_models/models/openpose/export.py,sha256=d8z6MsmtVf85EvVTHyghAquqRXjpWx3rsSccGv84Otc,8151
+qai_hub_models/models/openpose/export.py,sha256=MYIXfRyGnSKCvHExllyEk6kxQfSgNWsNK2ZoEOEJ-d8,8171
 qai_hub_models/models/openpose/info.yaml,sha256=zZAZK46yA0iB2H7zccZHf0cxuQjdMdO3osgi1K-hRZo,1246
 qai_hub_models/models/openpose/model.py,sha256=iWpglU2znltOYKlQ9w-1xBt9yrrySJW5YhNBAH6lMkw,5084
-qai_hub_models/models/openpose/perf.yaml,sha256=mhkSarHILqRnNpe1qBsEK-gFdFz60x1bB9IJWHUgGLI,2738
+qai_hub_models/models/openpose/perf.yaml,sha256=oxVDjd4LmIecYtif5SksgrXZSWSqcBmFtyj8DFI7-po,2762
 qai_hub_models/models/openpose/requirements.txt,sha256=0xg0sn4HQZDSpN03SNwngv61Ry0AXQZvwPHrXtSuR00,31
 qai_hub_models/models/openpose/test.py,sha256=JhSAWSbHck2xqktdTa_2RAaBdoJOL86g6jLigVNHrgU,1321
 qai_hub_models/models/quicksrnetlarge/__init__.py,sha256=n0s0M56rmewoor90_VOznUSWRXF8hBUbBkwMNjKg1xQ,472
 qai_hub_models/models/quicksrnetlarge/conftest.py,sha256=0xShh4VAQuyE1d3yA17kRiW5-nA0SE-zDdsh1E60KmA,888
 qai_hub_models/models/quicksrnetlarge/demo.py,sha256=IgRtHWqQVLGsuNXJTb9kwaWkiR9xuz36Qnhc9Z34jKA,972
-qai_hub_models/models/quicksrnetlarge/export.py,sha256=Wc32cjcGeWgTeBWkgXxn5B0ixvqOp-ZSbJToYwOORjk,8161
+qai_hub_models/models/quicksrnetlarge/export.py,sha256=efeI70rfwFR4UM1E0ilNnlPm-j1efd7jRGhc5bLLl0U,8181
 qai_hub_models/models/quicksrnetlarge/info.yaml,sha256=jwBhAffyc2KTYL7kvsETHA6a9gvrc0XEZ2vEOgaRqdU,1248
 qai_hub_models/models/quicksrnetlarge/model.py,sha256=DZpev_PEQicNemo1YZgQ1PXNZG0QgRuvQ53H2DNdHWc,3170
-qai_hub_models/models/quicksrnetlarge/perf.yaml,sha256=e77E0oyBL9J2fAEyjioUmYLVdSmBfZEgIhWMQWb_qxQ,2715
+qai_hub_models/models/quicksrnetlarge/perf.yaml,sha256=XLe-qirKe4BGdDGzfcNtJ-XNgsVQGwD_9OKy3Bqcwbk,2753
 qai_hub_models/models/quicksrnetlarge/test.py,sha256=HICEbmnkI-D2g1MYjiUGkzMcl1Sg9NjC9JS7vttnveU,1422
 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py,sha256=kpW-qloH72jOP0aPE4r_0ZBcV8MackQ1Ro4yiunY9sM,483
 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py,sha256=CR2u4DhKWH3PQd_GPVAtUbxuKq0JFUKHU14KesdAh7Q,908
 qai_hub_models/models/quicksrnetlarge_quantized/demo.py,sha256=gScOAfsMlVs9d-GBMW8zHfPeP7CJiFawQldlU1CVuvQ,891
-qai_hub_models/models/quicksrnetlarge_quantized/export.py,sha256=ozSSdu8PfbCaT_ofMQDRJeFZs8NsmJUZfApCeZBjovs,8614
+qai_hub_models/models/quicksrnetlarge_quantized/export.py,sha256=2ROI_sG-o2-DwWp9TQTBvvgjYBb76bGLBtm2lWkWOPA,8634
 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml,sha256=qcahBBa33N55My5C2mINys5t3l41qQOFE3h-nLFR-ec,1274
-qai_hub_models/models/quicksrnetlarge_quantized/model.py,sha256=VYsOPWkx_rOh3nFplaNB5hSbZdzpi-jYCdHjevZ-I40,4244
-qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml,sha256=geju_DE97LrZAmynMgFu-4BLp2f3pLENKVfDTzXoYho,2688
+qai_hub_models/models/quicksrnetlarge_quantized/model.py,sha256=XeQCDOktDx1m2s56AlOEGwcsfUCkvI6i_MNU5VWPErI,4587
+qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml,sha256=VACElfR2h-xqbbuyj0OTUzbCVhIQpG5tNV9jFg2Grrw,2712
 qai_hub_models/models/quicksrnetlarge_quantized/test.py,sha256=C7ytsb9iOxrsxGz__BL5XrGU2xBqYUwM9A1XOa6--Zc,2921
 qai_hub_models/models/quicksrnetmedium/__init__.py,sha256=GTyPcoDHE8Hz702v_Bk__uhwi2N0E2KqyXxPQ5wpY54,473
 qai_hub_models/models/quicksrnetmedium/conftest.py,sha256=GjJLLKExU9wGw0UEhNLZEmFiKCraQrj-6dM_8ZLZMGA,890
 qai_hub_models/models/quicksrnetmedium/demo.py,sha256=EPLslU6kBps_nPCOUFcfVYWE66rGNIRGx9zC8gAoOes,976
-qai_hub_models/models/quicksrnetmedium/export.py,sha256=vGxr-D1Ji61deEYVoE42SUHHpd07iNligeInZinRqy0,8165
+qai_hub_models/models/quicksrnetmedium/export.py,sha256=VECraeXBVPG7If_YJHHqRmJsloUXKrbZ1Ay-ilVS51o,8185
 qai_hub_models/models/quicksrnetmedium/info.yaml,sha256=qF6K6GLHopGKnuT0H3OBAwFMssi5KAt-4LW2ijIHWd8,1242
 qai_hub_models/models/quicksrnetmedium/model.py,sha256=ChFo9Fr8BHeFB7b_l6FfiWwE85fFZMMlmh_AxTxoCE8,3177
-qai_hub_models/models/quicksrnetmedium/perf.yaml,sha256=9lZ0op9KorfwmMX-z9D6Y4h1zORPWuOC_GBDMFY022k,2725
+qai_hub_models/models/quicksrnetmedium/perf.yaml,sha256=0PY7It0Eb6XDqLz7W6fvNTtTJCiCE9-2nssoD_NRVeI,2754
 qai_hub_models/models/quicksrnetmedium/test.py,sha256=7MmkvCPJul33giNuZK8iAyRN11ci7Zw4bOq8LcjW824,1428
 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py,sha256=1S_5eD4OACNMsJuLjPa1A7qoFokADw1fpuU3MtBkx6M,484
 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py,sha256=0u-gkYfukSbzAsx6h07nAqKVQurUcgYwaF5ZtTxSkx0,910
 qai_hub_models/models/quicksrnetmedium_quantized/demo.py,sha256=6AGfl5ee02A7ZnM08cIJDOG14YEpQ1gWhJvPqBNVHEE,900
-qai_hub_models/models/quicksrnetmedium_quantized/export.py,sha256=qGagBjunFtwSf770WkTNgmyTk-mkPrymLIlFNLo2kzQ,8618
+qai_hub_models/models/quicksrnetmedium_quantized/export.py,sha256=dNztMKK0HIntu1TQeLIZdtPyoBWZQYYo5N7vmw0A0LU,8638
 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml,sha256=oas-ZZ1q7bu5_jE1tiAPIAvsbbCRii6V-9piQUVh1Tw,1282
-qai_hub_models/models/quicksrnetmedium_quantized/model.py,sha256=ptLVSsnA6ThMdrb0e7u1rdl8hySCX8N-oq4qlhW8K7g,4254
-qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml,sha256=rQz8gW2cRkL-_n-mGlz-cVpyL2u42c1cJfD7Ou7Hw3M,2689
+qai_hub_models/models/quicksrnetmedium_quantized/model.py,sha256=QtXYgZiBkUH4YXV1w-1jFmvDVYFLafAIlnxmcHYI19M,4597
+qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml,sha256=-WAcxgCa0rkf77jcW13tR79UIa5THgNiKIxe82gJ7QA,2711
 qai_hub_models/models/quicksrnetmedium_quantized/test.py,sha256=g10Mox6qmH2HVihYkCXfOeBEo6CVGDA5CEXsuMIG1Es,2912
 qai_hub_models/models/quicksrnetsmall/__init__.py,sha256=x3REqFLXfs8YWq2ld1olPTKLwdQ5aRavepT-WIj9f2Y,472
 qai_hub_models/models/quicksrnetsmall/conftest.py,sha256=2dyEX_G6scqXvl_fODjeb1m2lCBSv1ozbuoiHHQr4g0,888
 qai_hub_models/models/quicksrnetsmall/demo.py,sha256=y1Z_GTvbWhOaJ7iyVMqTYaUxxePBhUI0J0bguRaEUB4,972
-qai_hub_models/models/quicksrnetsmall/export.py,sha256=Rkx4R7l8PVUpujIx1lYvSXO4dTXgNyP44MZA1Syz2Y4,8161
+qai_hub_models/models/quicksrnetsmall/export.py,sha256=SXxv1GFYjXTv-Pr2jhM1ul-Kwdam4B6I8UuSe861ZX4,8181
 qai_hub_models/models/quicksrnetsmall/info.yaml,sha256=cPEJEcn1_ctwMF7nHQ9-AeUsPaq9mhjhfb4Gm77Vgp8,1238
 qai_hub_models/models/quicksrnetsmall/model.py,sha256=6cufAzO8C3wItx4iQpkXrexkV5W9nUsi8z9OUcIhogA,3170
-qai_hub_models/models/quicksrnetsmall/perf.yaml,sha256=-yHT7D4eEDt4NOesRwEZuIALfl7EoQdTybESOIfBghw,2727
+qai_hub_models/models/quicksrnetsmall/perf.yaml,sha256=m-boFXuXsXJnCxO4w8cMvX1evuosX0rUbk_b4mz7ORY,2752
 qai_hub_models/models/quicksrnetsmall/test.py,sha256=d3OvtLPqk_3uzP2cSQMa6G4MJu9kx27aXOLBaeCzdPc,1422
 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py,sha256=DYwTcgaBjIFARfMJ64zY-VPAT1DWz0kdcMunIa0uNuM,483
 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py,sha256=dWKCF1_BuVx0nuZzJRGYqGt5t3EQVaPyaR-Iy-W7kUo,908
 qai_hub_models/models/quicksrnetsmall_quantized/demo.py,sha256=mYIQyNQniXfQEVkmli0ft40hNwiXkeXkjUJWhT08YvE,891
-qai_hub_models/models/quicksrnetsmall_quantized/export.py,sha256=kRtYZPZa60_cwsomYET5oooJ-TB6Sc3k3BISos5hWSE,8614
+qai_hub_models/models/quicksrnetsmall_quantized/export.py,sha256=3V954w9CEcbWVnnQhrIdmEJpKg83VCw9R0DIcCPj4ik,8634
 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml,sha256=OiqbT9kVtT8AxmvJ0QTU3i-kZwOaoDj2bkc9F7iS4mk,1278
-qai_hub_models/models/quicksrnetsmall_quantized/model.py,sha256=WAYYc2AQ3bJvCjBxmcKylj32sD76GjoQ_CSjUb28y0Y,4234
-qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml,sha256=v-SO7f_KxAZDrUf0zZUFFJk9qCXoMjMEZD9hrv2Y6dk,2688
+qai_hub_models/models/quicksrnetsmall_quantized/model.py,sha256=GfLJVnZxxDgWFcGmhHJbEdX28SYgW_QoqlBCdbBZroc,4577
+qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml,sha256=YExIsvT6n3gyBxP85JZwPvq-3g4QzA-pEFdHQVBxW14,2710
 qai_hub_models/models/quicksrnetsmall_quantized/test.py,sha256=bQdBBfrTuY5mN24JOeq8QYVz95uoQkw3NxFG9cUuVn4,2859
 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py,sha256=-eD72P0MMcOI5S_6ZbVDAtQ5MAsZFiAZ2-Rs5wkfGCc,481
 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py,sha256=uS-z6GqWupfVmz7X3Vh4AENocYrAdxUJFLqMwnpylwo,906
 qai_hub_models/models/real_esrgan_general_x4v3/demo.py,sha256=mdJUw7jmSDTPXDVyGHAOMgI-RUQdcjhvdLqju0Zeskg,1280
-qai_hub_models/models/real_esrgan_general_x4v3/export.py,sha256=fOf2uEYUZ-XQHLy0sBGEbpM5lyA4zMsfGt-idF5unxw,8197
+qai_hub_models/models/real_esrgan_general_x4v3/export.py,sha256=O2aDAesmqpbnb3WKVg9XOcrAVdyP1IE4z3WTRVHkBNU,8217
 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml,sha256=pKlB9DD4Qw5WDJgKW9xuxvccY-48a3vXY14Cp5M7RQ4,1206
 qai_hub_models/models/real_esrgan_general_x4v3/model.py,sha256=X0osnTyA3iHcWX-TlmCu6LN3-FUeLlPalUP6oSTk-nw,5226
-qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml,sha256=yibx6hYMsLNqe5QyxgiXj74Pvr7WxaofViER8_hvzVA,2746
+qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml,sha256=Rx6vfFmFT6j-Nkylpo4pVVtXQHqIq6NCy6P7qpcFYV0,2765
 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt,sha256=eivyaYj7iqy9Z98WwPoKDVihhM_HDbjOBu49PUryfJI,44
 qai_hub_models/models/real_esrgan_general_x4v3/test.py,sha256=CfOp0wKNA7sXZihxigo9inRQH-AQLkdzeg_o5KMKZIg,1480
 qai_hub_models/models/real_esrgan_x4plus/__init__.py,sha256=9EdQhD9AYsj6uAL3JlTGCNhMZc-2MiyfpZDJEmjspMQ,475
 qai_hub_models/models/real_esrgan_x4plus/conftest.py,sha256=NJjKsQ2SS_ZBqQ-BjcybV_Oj5v5m9m9r1rQyu4hP_iI,894
 qai_hub_models/models/real_esrgan_x4plus/demo.py,sha256=_X0Jc79lhoHqB01NfCidG-TFBMaDtez-4Ipc5DCCL8Y,1256
-qai_hub_models/models/real_esrgan_x4plus/export.py,sha256=Ahs3WRTBGzwzFSOh17AU2OAnKqVjQTmQifF1NaTWT9M,7634
+qai_hub_models/models/real_esrgan_x4plus/export.py,sha256=JL-SphhOUikqPMy3_4OnAGZMxhbymSdNEeMbMZS_2zI,7654
 qai_hub_models/models/real_esrgan_x4plus/info.yaml,sha256=xP8_0t2kPCJmXXY0M1oTjK5mPYGNaWMpgmRZ9rXNfUk,1330
 qai_hub_models/models/real_esrgan_x4plus/model.py,sha256=aH0QcIXpz336Nw7ca7BUPzMtuVFRvaFNOlun9pFVhQc,4443
-qai_hub_models/models/real_esrgan_x4plus/perf.yaml,sha256=bg7Dhco8F_mb7YMB_OqPXkQEa2Rto1NxAMN-Za6eVCM,2695
+qai_hub_models/models/real_esrgan_x4plus/perf.yaml,sha256=6QmwiVBJy-rEezhfsjuFkmNNc6v9SdvKM3AyY1ThwGI,2720
 qai_hub_models/models/real_esrgan_x4plus/requirements.txt,sha256=eivyaYj7iqy9Z98WwPoKDVihhM_HDbjOBu49PUryfJI,44
 qai_hub_models/models/real_esrgan_x4plus/test.py,sha256=amJIYWs1sw7ndZgviSvm9X4uZSUH8TAz9Sa_9lyCTUA,1440
 qai_hub_models/models/regnet/__init__.py,sha256=eVvJ3D2GfGdcBriF_ihwwMVGDFu5OtZSuPz61h-uKsM,469
 qai_hub_models/models/regnet/conftest.py,sha256=CGNvhqmuHNqdbfeICiVsB1l22BBVLbC76XOrichi38U,784
 qai_hub_models/models/regnet/demo.py,sha256=QTBBLpnIfWQLVnJOzzj1-pW5QWbpl68HJQ55_rtMAZs,524
-qai_hub_models/models/regnet/export.py,sha256=ItTvgVenvm3mmRFeAnBvB0tJEC6kxo2yJaYZZR3pzXw,7847
+qai_hub_models/models/regnet/export.py,sha256=lHifWnGm7fCfxHFKr9pdTfBh5b4uqfMu7CYEHZIwSv8,7867
 qai_hub_models/models/regnet/info.yaml,sha256=hdUbHswd8D4ya7xX4_YPl4ZOT1ItebCTvId0VEh-mBg,1291
 qai_hub_models/models/regnet/model.py,sha256=ZAd-Ymzr5ak1Fe3oMS5a2DFvwnOWEKYJZaEP2tu-fQo,635
-qai_hub_models/models/regnet/perf.yaml,sha256=P1ik46eE1SLYURyVKrcAcn5Z0hJFNYNqRc1mvoM4NG4,2729
+qai_hub_models/models/regnet/perf.yaml,sha256=bHTRBGRRfWZpeaWlsrVWn8kj4oTF7NAK1sMtX0ffYrM,2755
 qai_hub_models/models/regnet/test.py,sha256=zZ4cDdGDWjkZ15L8zTOn770-LYfeTyRPaR-J2OA-RnI,984
 qai_hub_models/models/resnet101/__init__.py,sha256=giKR_QG6BGLXFiSVr_sRAJ-h2b2Ea6L8rhZgxEoJ7jY,472
 qai_hub_models/models/resnet101/conftest.py,sha256=1lXOOKtNS6W5tzQE9g6o6HwgJy2Few3YDDialK5DIcs,790
 qai_hub_models/models/resnet101/demo.py,sha256=RYdwNDEnxQpmUsUPP4qvYiGYJar-1WfLBbO5cVMx6N8,533
-qai_hub_models/models/resnet101/export.py,sha256=ABvcAjz2GfH5FSrsSxPP6Bflf1FaBtFWCuqckVf5YLc,7859
+qai_hub_models/models/resnet101/export.py,sha256=OTX5fEdD8f3QTWQo-bTq44gE50m6MTNgyrzVGy6fgJ4,7879
 qai_hub_models/models/resnet101/info.yaml,sha256=fbGPKTdV8UBcjMHR0aVK6amiizctrlbiQFaDSLrotcw,1312
 qai_hub_models/models/resnet101/model.py,sha256=Pjnm6vSdazT6uhrfn0Hx7hLU-U2rF2KiTMa9jWlmm7o,609
-qai_hub_models/models/resnet101/perf.yaml,sha256=b1Lv1dbchMoZJQ8HlWxbkkkfuK7_IsEBY-5yRiVZK2s,2736
+qai_hub_models/models/resnet101/perf.yaml,sha256=YgFJP-cwaJ_p44ZK0a-hWKkeo8LB3iYnzsTMuJzyFwg,2759
 qai_hub_models/models/resnet101/test.py,sha256=Gr4R3pO8Be43sRBm3KEc5NaJ3fHofnMMWfKTJ3K9lW0,961
 qai_hub_models/models/resnet101_quantized/__init__.py,sha256=aJsoYEIOw9cKKPYWV-wtGWvN45BTKvqZVoN31Eag4TA,483
 qai_hub_models/models/resnet101_quantized/conftest.py,sha256=34kMT1G4RQditAjZqpMYFr1IUGb2Q5YaTN_R_lnp0xE,810
 qai_hub_models/models/resnet101_quantized/demo.py,sha256=to8ShYfeBVmzd0OkyV4SxeflUXeedhqmrc6dwbnanfQ,578
-qai_hub_models/models/resnet101_quantized/export.py,sha256=C9fKLa0FkLVWxZCTXVtgyfqLrydXdzL2gTVD3Kf2GN4,8339
+qai_hub_models/models/resnet101_quantized/export.py,sha256=qQ-nXnNW9ZXmaiGhXVWeDC1xN4BXnLTxGWRMPl5hQEQ,8359
 qai_hub_models/models/resnet101_quantized/info.yaml,sha256=28u2ptmsdw9O6ZAljymiAaTOZxV6DDD-_eXsXYZx3X8,1346
-qai_hub_models/models/resnet101_quantized/model.py,sha256=EuHjnNvR-zksI0Q8C5g-Lp5B2RmJaVCO4Pi-APwBDHo,3551
-qai_hub_models/models/resnet101_quantized/perf.yaml,sha256=1vKDZkmmS0J9FdCwK2dFkBafvBb6nxDf04-qoH14rvU,2741
+qai_hub_models/models/resnet101_quantized/model.py,sha256=U9hffK5qjcWj1Aos_aFqZSb6-QrXixseYJ-Tbyb5U88,3000
+qai_hub_models/models/resnet101_quantized/perf.yaml,sha256=9rGEVoG_6iu42pjlB5DQogrv2xa3lS_FRg7HMlJranI,2765
 qai_hub_models/models/resnet101_quantized/test.py,sha256=C-8zqBiwN-nCgJb1I9RYfMykLPSKJIWW_t_Pbh97mJM,921
 qai_hub_models/models/resnet18/__init__.py,sha256=YXCoY4ADkV9rxnJQmx6th3bEwYaxE4UF6ECJMkuN4gY,471
 qai_hub_models/models/resnet18/conftest.py,sha256=-3fDAN4NFDZgwXbj1sPv2WXvzxjdVqA6SxVk9Zm3IwA,788
 qai_hub_models/models/resnet18/demo.py,sha256=e_s-tuUKno5C8PmEh8AISsqywzoNtYUQVFJjyaJRLYU,530
-qai_hub_models/models/resnet18/export.py,sha256=IRLLG3qrLWKX5UWFbwT4mKJxEcsG1T_sGaVBJoAbqgM,7855
+qai_hub_models/models/resnet18/export.py,sha256=f7YafLDSqMCLK6jyz7XZGK4viTQOzRgHc2aruSHkn1M,7875
 qai_hub_models/models/resnet18/info.yaml,sha256=EApwQW1yExiQzFT8qWYraKeeULj1aF-_j15imZQshuU,1310
 qai_hub_models/models/resnet18/model.py,sha256=gl5xAhxkR3kMkbQ5DQXSZ42eGFE0uLN2gNmrianZWEY,607
-qai_hub_models/models/resnet18/perf.yaml,sha256=vbeMz4hvD4jbDNDu2T79f-fmN9Iso2ZGy-RwTl_yPbY,2720
+qai_hub_models/models/resnet18/perf.yaml,sha256=kkHN34wN93TmzTwwkQ2Dc0gNGZ6zUsto-XInjmKPSJs,2746
 qai_hub_models/models/resnet18/test.py,sha256=raI736nKtyy7_kNAmWEci7wQYPqPQ_ajP0h7MTfpdW4,955
 qai_hub_models/models/resnet18_quantized/__init__.py,sha256=6MfeTP9-civpAklCTt8I8kwEj4qfK2bpGpBFzi7Gp9Q,482
 qai_hub_models/models/resnet18_quantized/conftest.py,sha256=RVLrz596M1cs_04LE1gWbVOJy0MednIti84ezpKn3Vk,808
 qai_hub_models/models/resnet18_quantized/demo.py,sha256=J0yoGrm2LP5iUCCMXqc-paqC0VJZPP_Zi2heqCd_FGk,562
-qai_hub_models/models/resnet18_quantized/export.py,sha256=MAAqKXKgp5OmdX126ppWT0nRvf74ZfXCiehoU41MI8o,8335
+qai_hub_models/models/resnet18_quantized/export.py,sha256=HFkos23ACf2up_oSihWvWBBxNM2k1utUbHQxloMNgTc,8355
 qai_hub_models/models/resnet18_quantized/info.yaml,sha256=EF_rGgvR-mbYSxraghrHtbP_TYxrI6cLx4xG2PuRPmE,1343
-qai_hub_models/models/resnet18_quantized/model.py,sha256=HCYUYzfHEJqro58MHNnH_bDMovHw4RGCeBYWsEa_bPk,3353
-qai_hub_models/models/resnet18_quantized/perf.yaml,sha256=pzOWfyRLrR_-Ask6RPChBeScwCA4iPbkDrDCwWQR0DY,2728
+qai_hub_models/models/resnet18_quantized/model.py,sha256=Cebk0B23oJotISOWBJ7a3Bi-uGWO1WdgHUYIfUvB4Ic,2802
+qai_hub_models/models/resnet18_quantized/perf.yaml,sha256=vhJju8a_yV7J0bWQ9wsDEAjTngHO_i_zO858MrCYmxc,2748
 qai_hub_models/models/resnet18_quantized/test.py,sha256=gB-2uNRrC-phYHX6dwCn6cVSAxpVq41FFX_UwPwb5qk,917
 qai_hub_models/models/resnet50/__init__.py,sha256=i6A3by1VSJeHInRvYv3jO-x3QRBzywCjnlKuG6ALGA8,471
 qai_hub_models/models/resnet50/conftest.py,sha256=EzB3z3xK2kvCYXYdRqKQEV0AWSEG0c4vW-zJ8buv4do,788
 qai_hub_models/models/resnet50/demo.py,sha256=Ad2nWpI-kmP5M1sGhFAqSZFZ1Mz0Lov4q0ilsJIsdBg,530
-qai_hub_models/models/resnet50/export.py,sha256=q44B_oLSjAXMMhVh7yXhs2G-0fHsslNST30kZTcQEDE,7855
+qai_hub_models/models/resnet50/export.py,sha256=aVxIA-zx0o5awp4QrqHXr_E18in5XvJCFURibYKjDNY,7875
 qai_hub_models/models/resnet50/info.yaml,sha256=mF1FdZhk7GG8vBDs0sW6l0VWW9jBxyZAkyS6SdaAF7M,1303
 qai_hub_models/models/resnet50/model.py,sha256=3Uz5cop4u1xHyZuR9ss0Lg6cSxWyV_xuJcgFENbiHlY,607
-qai_hub_models/models/resnet50/perf.yaml,sha256=y0YtX552xBxwdjmjy_C1AbPFjev2dex7oIcmkJqw8I4,2722
+qai_hub_models/models/resnet50/perf.yaml,sha256=8RR8vxyKlJSPBWw3zNf0sXurU3d06KUB4YM5Tj6AxaY,2748
 qai_hub_models/models/resnet50/test.py,sha256=7iQrq9aU53jUwGBRxj6dRXH9SwvFqTwegbMMxTmO5Fc,955
 qai_hub_models/models/resnext101/__init__.py,sha256=KsXu0bXmQcD_vUB9tOkoYL8cPsCuexuQfSA_ZoSEtxc,473
 qai_hub_models/models/resnext101/conftest.py,sha256=E9F0Qsex8miTp64s6OZGnBLpxD4trmOmyr5CHyJ9U6w,792
 qai_hub_models/models/resnext101/demo.py,sha256=W_MBGy--3Gyo6IkASUus4IeC2OJo6w7_vh8AN--pHj4,536
-qai_hub_models/models/resnext101/export.py,sha256=gRS-oWFUK1KHMtlYPDStaclM_23sImUoOG46rogkiZc,7863
+qai_hub_models/models/resnext101/export.py,sha256=l_qAa4fRfY4FM9B-7Zdid8YA4Dy4lYYZJUt-uAy945U,7883
 qai_hub_models/models/resnext101/info.yaml,sha256=QQQ7IKSLieegTlThhnySi_BdYTnXcrBzSt-FNmAfspo,1324
 qai_hub_models/models/resnext101/model.py,sha256=dD8WyCTD397PbDRLg_EoNX-hcQekN2P7S8OrzHRz7SU,617
-qai_hub_models/models/resnext101/perf.yaml,sha256=FdC2Ckaqcd0ntQAm5d-2GzQm0_CpyjZOMe_j_g0hW7o,2735
+qai_hub_models/models/resnext101/perf.yaml,sha256=EG1f7dDH62A8N-hU3Wq9mN9Trt2cc9HRlYVzsHosid8,2761
 qai_hub_models/models/resnext101/test.py,sha256=Gf470ZYn46oHSNisYxPiShq74rXykhqvriYbnNRnwoY,897
 qai_hub_models/models/resnext101_quantized/__init__.py,sha256=Sr23eMhxryEjt6hRjftWbdgS9YcDk1b4OshOi7irTY8,484
 qai_hub_models/models/resnext101_quantized/conftest.py,sha256=Kx6udb0VXDCbX-ie12uvyIOJySDxQWJ_cd-MM6fRtRs,812
 qai_hub_models/models/resnext101_quantized/demo.py,sha256=Po6sLvHLkvqNm9tZ-vAeM-A-t_FLKaE18gTnJh6oSuo,581
-qai_hub_models/models/resnext101_quantized/export.py,sha256=21OqRV474TqJxacmF1jMkvsGYHHvxyBwUcfIKiqkcrQ,8363
+qai_hub_models/models/resnext101_quantized/export.py,sha256=1CxhCRqEJ1pWuBoTLhwLui8GH-_HMkyGQbLUH6x8cZg,8383
 qai_hub_models/models/resnext101_quantized/info.yaml,sha256=Zlrngu5Y1UGGPhy_4bgpaE86oIzeB5NIeiAslZh8cto,1365
-qai_hub_models/models/resnext101_quantized/model.py,sha256=qfSPC-UMfOjkapr86NimhmGbKOzQTBkEopT9HY2J1xk,3358
-qai_hub_models/models/resnext101_quantized/perf.yaml,sha256=Kmuhs76P5-eK-n2lpKOTb_jU7IJHLGh-qZDQnZr8Lfc,2687
+qai_hub_models/models/resnext101_quantized/model.py,sha256=waex64mrxFUzyTJdYJYm7bT38hcsLijEKmpWitEPBVA,2807
+qai_hub_models/models/resnext101_quantized/perf.yaml,sha256=fD0B4IN9pVXs0VaJaijqpxOHM7LOzi0uuVt3Ot85CLY,2708
 qai_hub_models/models/resnext101_quantized/test.py,sha256=jw2RahxL2zJq95ittlG6mBH1HKfCe-5MLpuL6f_9NOw,925
 qai_hub_models/models/resnext50/__init__.py,sha256=mvGLhLT1_Hj0eQ5sERqw2qBaaHTc8q9nGaoIMc_zVwE,472
 qai_hub_models/models/resnext50/conftest.py,sha256=HK9Rp1ysi2SAbpbS8kYNj3cWP-R8EFauBpmxy1ME99s,790
 qai_hub_models/models/resnext50/demo.py,sha256=layKRO8OVlf7zDRarcUKuBwziX6mh0TahnxW_B4hOR8,533
-qai_hub_models/models/resnext50/export.py,sha256=IZwfSDWnbsCJPiP2s3sxCxIWnFkUI6ZUe_g5ZbTMab4,7859
+qai_hub_models/models/resnext50/export.py,sha256=mPGxM0Rl9-ULtL3LSWUEMi1RhjfymRmFRwYlCAdAup0,7879
 qai_hub_models/models/resnext50/info.yaml,sha256=x36YHWLVNgYYWVx4Y9GrJYKmwbgCmIXcUrVuX5Mv_XI,1322
 qai_hub_models/models/resnext50/model.py,sha256=g_2NntA_nWKxlgNZJiLaSpk-VcUO4S4EBtK1fLqvFH0,704
-qai_hub_models/models/resnext50/perf.yaml,sha256=PNu_14jstWiOA4mOCLHNI-B73s3cUZHXOreA_sQFqjI,2729
+qai_hub_models/models/resnext50/perf.yaml,sha256=nzFbtWWSo0lp5rB_dP4M4e-iyzv_Z0zg_J--Xdkjrwg,2754
 qai_hub_models/models/resnext50/test.py,sha256=mO0iDd4c_7F_QjcOypdCsexc47QbdJpKFKFU5Mbwipk,840
 qai_hub_models/models/resnext50_quantized/__init__.py,sha256=4Oqop6yfnVXt2Zh9RoVJLqXYnXkriZIMSa90EcFh08Q,483
 qai_hub_models/models/resnext50_quantized/conftest.py,sha256=cvkXDQnHQg1kMCYpn6zMXPzLx059_FebLIEg6GztxzA,810
 qai_hub_models/models/resnext50_quantized/demo.py,sha256=sl3AOEAMwvqU2v3_7mpy3yUD-8j1JpcL_WCTpkel_8w,578
-qai_hub_models/models/resnext50_quantized/export.py,sha256=yQap9hEEHAi7T-rwKvA14ekNtK-G2kxt_Uph_kHhmCs,8359
+qai_hub_models/models/resnext50_quantized/export.py,sha256=PW3ER7OQgO5yQbZQAANFVLWCpM9LS7QJGtwWhBZWkHE,8379
 qai_hub_models/models/resnext50_quantized/info.yaml,sha256=cC9rkqVl4QXSu_EjJRQY97AAqh8FsBuez-Rf3XSWae0,1362
-qai_hub_models/models/resnext50_quantized/model.py,sha256=CRG86PtffLC3FINtRPtxjWx4_78BrWvblvOUGbkbx9E,3349
-qai_hub_models/models/resnext50_quantized/perf.yaml,sha256=DVkQcXezBLxCNqNXWWcNpfPAn0PnNWSnDON4q8csnCM,2681
+qai_hub_models/models/resnext50_quantized/model.py,sha256=1ID1l7vViZQJGAzZVSWHEyDmB9oLnxZP61EYrwgllGc,2798
+qai_hub_models/models/resnext50_quantized/perf.yaml,sha256=do9liZYX3GJKp4Lq8uqEUBEzAh_euQ_SqpN9De4Tlcs,2705
 qai_hub_models/models/resnext50_quantized/test.py,sha256=zmx2xRiJL2_B6zZwUmhMWzSm7cmcbCGyay7dB9O1XKY,921
 qai_hub_models/models/sam/__init__.py,sha256=z2l9N1qWNXdFDly6YvRC6MBOTANKL28klDYmF2qF7Qs,404
 qai_hub_models/models/sam/app.py,sha256=jCDEbspglkuXi5SS86A96Ek0SaqO-xhHWwHPh_3N8LY,5101
 qai_hub_models/models/sam/conftest.py,sha256=-d5Ti-hHK9XwP1qWn2HQ1IbTB2_LqHWM_9cbb5PRgoI,905
 qai_hub_models/models/sam/demo.py,sha256=G-Fv5x7usY6c1YeSdRzKBpw4KhC-9akcxINsspJBiHg,3088
-qai_hub_models/models/sam/export.py,sha256=QE7tyn68L3vElS8EYJ7SPdFHgjkrt723YIYnTexKjec,10107
+qai_hub_models/models/sam/export.py,sha256=tJ0yT8gyuzIp5Siu5DE5mkXjxTqbLH0vmyzZttuscVg,10152
 qai_hub_models/models/sam/info.yaml,sha256=b0Dez8dcQ4IAQs0xAHWsYkAvolTKi_hbFSRJvr19f4s,1391
-qai_hub_models/models/sam/model.py,sha256=XT8BTXnYL2u7l6PJqnRdZUlz_a7B1e3XKU5pPOLn51A,12012
-qai_hub_models/models/sam/perf.yaml,sha256=4v9xO30yPwSGC2ZznsUm5UPOekdLR90R9q21NEs0WJo,2685
+qai_hub_models/models/sam/model.py,sha256=qaW2mQiobMnmQ-SDme_JYPdbzUKHMqjeu4B7CJTEkx4,12000
+qai_hub_models/models/sam/perf.yaml,sha256=qLiUs615tJkoTS0SwjIlWPJi3KxXUmbny9cI6x8iOhU,2708
 qai_hub_models/models/sam/requirements.txt,sha256=vtppKtEacR8giw86K6Z90D_Xko9hityPiHQx7PxkAgo,37
 qai_hub_models/models/sam/test.py,sha256=cmynvGV9g_6A0etO-ORNxj16gyCF97MVYxZQK0CNzIM,3062
 qai_hub_models/models/sam/utils.py,sha256=5Zc1YnUm0HXsJ_T-R_Ge0CjMARxWxHA-ITG424uOCic,826
 qai_hub_models/models/sesr_m5/__init__.py,sha256=kS2GuYY43QHK5jRUcssK2u92IM8LMLy3SIbV5YCEsio,464
 qai_hub_models/models/sesr_m5/conftest.py,sha256=adTpVRqwS3ZJsP2M1Eg8Jy6e2Fl_EbXzGKu2zPIstf4,872
 qai_hub_models/models/sesr_m5/demo.py,sha256=uQqrkMLYcKnXc6_uDW8Niuj1YQBiMya3sFZwMGVXlOk,923
-qai_hub_models/models/sesr_m5/export.py,sha256=dh7hGsL5Bn3rQfMT3Q5FBEy6THO8HPWRTxWSePqWNOM,7986
+qai_hub_models/models/sesr_m5/export.py,sha256=O8dg1F-ODACIO48vA5qHfBUAoWegV_ZzUDEWq_XnkOs,8006
 qai_hub_models/models/sesr_m5/info.yaml,sha256=xDzw26L21cJQ14jXZFrODO_klhrPTV5L5ALknfT4MnU,1104
 qai_hub_models/models/sesr_m5/model.py,sha256=c9-Q3psWvkFyFKgaOfWN5rhJpsvMZOKIUvapZhv6JFs,2984
-qai_hub_models/models/sesr_m5/perf.yaml,sha256=s6wFJUMuTEUQmKAX2PzPDuPN0z_nKIKmxxIfboSelIs,2722
+qai_hub_models/models/sesr_m5/perf.yaml,sha256=RV2-omedufM299zPWZoi9H00SNNB0e4JKJ20rgya-MY,2745
 qai_hub_models/models/sesr_m5/test.py,sha256=hH27IYEOa4fPbfgHeQYuiUhHdPvMHX50DdW-LY7btng,1471
 qai_hub_models/models/sesr_m5_quantized/__init__.py,sha256=PZUoiiSQjJOpVhj2TNp_ixSeoy6RwK34nb45232QDsM,475
 qai_hub_models/models/sesr_m5_quantized/conftest.py,sha256=9T8-ZXKw9k4DZ5PkE0hQ5heMrjJBKeuovQDlp5Gz1jM,892
 qai_hub_models/models/sesr_m5_quantized/demo.py,sha256=OD2D8ExW3h3SYF2_KX_7Qv_GQ8ylmjui2tmpWsqyAqg,990
-qai_hub_models/models/sesr_m5_quantized/export.py,sha256=GwUNTZMvzyfJMpjHqkyjyxtS_pT_xWfB9tUA2KWwG1g,8147
+qai_hub_models/models/sesr_m5_quantized/export.py,sha256=4RcsQryaa2A4t8kmJ_Zg6DcR3XNsjoRQ68DBJY8aX1U,8167
 qai_hub_models/models/sesr_m5_quantized/info.yaml,sha256=J1XebgMlJ73n3is32JI0bduz8pFhlImgxaxr6khZfvU,1151
-qai_hub_models/models/sesr_m5_quantized/model.py,sha256=lY354mg1_wiVO3JbJX1D4-yCletCdtCZr9buQiVDzys,3926
-qai_hub_models/models/sesr_m5_quantized/perf.yaml,sha256=885J0mjj45A9dvzq7SefnknUk3c-Ahi2PzmXQuLxxMA,2680
+qai_hub_models/models/sesr_m5_quantized/model.py,sha256=6O-GFAI1vPgyqLIRuMt1MWPQ8xAqfGvTBk4ohFy3w0Q,4269
+qai_hub_models/models/sesr_m5_quantized/perf.yaml,sha256=5jkoM-iTFiQWuGQV9q7Cc0qq1cIs82nOkY0bChAt2FY,2704
 qai_hub_models/models/sesr_m5_quantized/test.py,sha256=3EXTzeXWm0qVHMZmGJeiy7OPzAafb2b-ORgUMtQ5KiI,2927
 qai_hub_models/models/shufflenet_v2/__init__.py,sha256=V__9vnDnnEeNjHp4_cliHUq8Ea7djP5ppdXt1VH3MI4,475
 qai_hub_models/models/shufflenet_v2/conftest.py,sha256=QOXdfhu4kvX5Ul38PM78fcnCV-z4VZnQCyp4moGPTKk,798
 qai_hub_models/models/shufflenet_v2/demo.py,sha256=-yorlVXF_fAdjOp_xfBQeZ63w2hU952DdIcWeLJDMJU,543
-qai_hub_models/models/shufflenet_v2/export.py,sha256=uEZLWXsrHBhB_CxF_8i_trK6xEHU07WcniCu1O-uPJM,7875
+qai_hub_models/models/shufflenet_v2/export.py,sha256=_05PPcIRFYUXYhA5T545JHg4u4D1GEH8lRnNa5IsagI,7895
 qai_hub_models/models/shufflenet_v2/info.yaml,sha256=5myQBF6qAlWS_h8a9emJbmux2ZpQIgsf3o7ZERL-nVE,1353
 qai_hub_models/models/shufflenet_v2/model.py,sha256=TLi9PPf3FIDi5l-pgCkmJGRA8TduOSRpsB3CbNhtVX4,713
-qai_hub_models/models/shufflenet_v2/perf.yaml,sha256=r3rTm5xMFSkwR4eltP5kE6TTlWzYhkXgFw0B6qxjiSk,2731
+qai_hub_models/models/shufflenet_v2/perf.yaml,sha256=O_DVE5eehmgn_HAq9h3vR-ZesbHVfhZ-TneI6FiIPgE,2757
 qai_hub_models/models/shufflenet_v2/test.py,sha256=11Yio-WBjhooUiLFMX7XC70gjCjq-MeiJr8zM854Rhw,857
 qai_hub_models/models/shufflenet_v2_quantized/__init__.py,sha256=CjZcBsRqc8WZXtl4c5QvzSJtfCx0TgArcKOymMp9yw8,584
 qai_hub_models/models/shufflenet_v2_quantized/conftest.py,sha256=AiffjINiY6Grj7eRtu2UEk4OdTkwzD-EU09Ldjj6vZI,818
 qai_hub_models/models/shufflenet_v2_quantized/demo.py,sha256=DFJLJ83mzCD7bDyJ9KVFnO8c9rHSYygU6BlzoSEE-OA,588
-qai_hub_models/models/shufflenet_v2_quantized/export.py,sha256=YWhEtoyJqJpJNuSxnIG7jvnrRuOcdoXrDdJa9LtGbxc,8355
+qai_hub_models/models/shufflenet_v2_quantized/export.py,sha256=6PwD_86NPYN42RmJTVVeQk3WyOdR7b1-GMaZSyM3diM,8375
 qai_hub_models/models/shufflenet_v2_quantized/info.yaml,sha256=fFtlMqu2yN4_tu5ZZtayMfrNCYxxHuDjXQvOy9k6tw4,1383
-qai_hub_models/models/shufflenet_v2_quantized/model.py,sha256=RZxOuA98RpVpYtcA6kq_r1DxEU63yT8lFL0FQu4pCS0,6330
-qai_hub_models/models/shufflenet_v2_quantized/perf.yaml,sha256=i_WowOi9ZrAGQ4S-WztFQKrMRUktgnjvwHiQC3XHz18,2740
+qai_hub_models/models/shufflenet_v2_quantized/model.py,sha256=mXKbo-E9428Dy1UBeHM6EZUHxw3Yk4cKef6YOaYuGb0,5779
+qai_hub_models/models/shufflenet_v2_quantized/perf.yaml,sha256=AHUL4c-fGCzr3zAZ_Yc2LpQkAdf-QITITvXKJ0GuMGc,2767
 qai_hub_models/models/shufflenet_v2_quantized/test.py,sha256=_U1q8NxyW4nqeb-jfVwqNcISHQpMO9mvVSK7QQK5Hro,899
 qai_hub_models/models/sinet/__init__.py,sha256=WIFJwnJg_47VGh96lUb-Ned-MCqHd9KL_OHzc2GU8Qc,396
 qai_hub_models/models/sinet/app.py,sha256=1w0xj_VwsDGJWRitmBq-iBWjDNI2dB-MOKHqCe7_pr8,3793
 qai_hub_models/models/sinet/conftest.py,sha256=r-CzC7uRb5gFTMVriSsbkfbt4v9O3zPDlOIx3EYZEvs,868
 qai_hub_models/models/sinet/demo.py,sha256=1H922wHmgorXtZKVNVWn30PHA9Xz7NPPkM5wR77vVkA,1657
-qai_hub_models/models/sinet/export.py,sha256=rOX8kyvIP9EPLchQ2Hg2UVYGyJ0NYzw-AdBJC30pTQw,8121
+qai_hub_models/models/sinet/export.py,sha256=yWEsq9ELCfZJJxs6x-kiyyXDDJMpjc3HHp5tBkoKIwo,8141
 qai_hub_models/models/sinet/info.yaml,sha256=L-fe7Oh7vvQkdYXQ_xaMDXBqtusxxXtBjD2or5fa_8w,1260
 qai_hub_models/models/sinet/model.py,sha256=kTdRqRgfCXsIUcxUwSsIoqsePRM0o2UNKRajxDIK9Og,4770
-qai_hub_models/models/sinet/perf.yaml,sha256=5uDbOV4huXl4T16eWk1ntRiZqNwd7QkfLEJ2qeM_4g8,2725
+qai_hub_models/models/sinet/perf.yaml,sha256=0VOPqUR1VzT8quAGjZsIdPkJDaAH9UotT8MLcSaMfdQ,2746
 qai_hub_models/models/sinet/test.py,sha256=3WEZ8v6IIXSA2f_wfWWIeIJaeFCc1h_layHLP2LEf9E,1355
 qai_hub_models/models/squeezenet1_1/__init__.py,sha256=wWegyK2XfBcw4YceoPl-TZKn1uoRNNveDHXHOzA_tGQ,473
 qai_hub_models/models/squeezenet1_1/conftest.py,sha256=sohs5Tnr7eaVIXAuaBteRwxsNblTewQ1aBm31p8KS_4,798
 qai_hub_models/models/squeezenet1_1/demo.py,sha256=v2sws22uWCnGNOPIUPT_NolPTvTDLH-phqz8IYG6bVg,539
-qai_hub_models/models/squeezenet1_1/export.py,sha256=pix7qjwYprf_0gAGRlNDgnZOiYySFHxhD0riFioMdOA,7876
+qai_hub_models/models/squeezenet1_1/export.py,sha256=y2_Td7tSSCP7hbgfGhwIIRgom8JGLL3WB5YaJMLyoPA,7896
 qai_hub_models/models/squeezenet1_1/info.yaml,sha256=fhuQnYnNgu3GELnXi1xO655VmJ4EgaHkWn013HHiD1c,1325
 qai_hub_models/models/squeezenet1_1/model.py,sha256=mfmXr4T7EmhLa_86TXHOwv3XvlZMEsYAuiQmn68mt4c,696
-qai_hub_models/models/squeezenet1_1/perf.yaml,sha256=L7xD5l8qmN8PsqsIZR6HqyLSgPxrCPvRUcgLveUvFM4,2724
+qai_hub_models/models/squeezenet1_1/perf.yaml,sha256=6fIMYIu06br-aCpoxlD8t0EM2tmfoSmZF7SIOfSGXEs,2748
 qai_hub_models/models/squeezenet1_1/test.py,sha256=ytrCi-ZAAYWJ2rTV-6k_yZhYZkfrQRWD9FFoo8vUHEM,851
 qai_hub_models/models/squeezenet1_1_quantized/__init__.py,sha256=lQk7lSxtBwX8Y2q5w0xHIfz9bhvub3pkezT88Tore-U,582
 qai_hub_models/models/squeezenet1_1_quantized/conftest.py,sha256=Wmmh7Kbypo-9wZzwrdZhzP7g1fuHhMEL8ETK4hiKouY,818
 qai_hub_models/models/squeezenet1_1_quantized/demo.py,sha256=khZa64iQFJ9MIwUA5pjiAiq8bOQwXiFxAzNsVLObTgw,584
-qai_hub_models/models/squeezenet1_1_quantized/export.py,sha256=W1Iv_F-0NNguKGYovcHZI8550yRg8g4l0rPDh5J8PZk,8308
+qai_hub_models/models/squeezenet1_1_quantized/export.py,sha256=SCohi0Bc_3yg-SReP_641UIDkPtWJViewHxagTq2QwM,8328
 qai_hub_models/models/squeezenet1_1_quantized/info.yaml,sha256=ORH-gyADqkVAWBPF08xspyFovFxhdm6LHlMmRztfK5o,1358
-qai_hub_models/models/squeezenet1_1_quantized/model.py,sha256=c2hZMNJziQl0A1tvYwJVPYfIZZNKOWDC09wUZqF03RM,3364
-qai_hub_models/models/squeezenet1_1_quantized/perf.yaml,sha256=homxvF2lqN1ftVBjB4g_9zOLEmIgyn8nWA9dpD1ahjY,2734
+qai_hub_models/models/squeezenet1_1_quantized/model.py,sha256=bqd5L6UBDaQVgGWfvLbEII94BD1fow8oRKtx4YaedTM,2813
+qai_hub_models/models/squeezenet1_1_quantized/perf.yaml,sha256=PKiYi3PEvM1Bl6mcEptTUv5indYU1K-T5tbxaPVU81c,2757
 qai_hub_models/models/squeezenet1_1_quantized/test.py,sha256=5e9_c2yGKzkl1OaYkwWXBijYxQijS-bKt7Q10Oo7P3M,895
 qai_hub_models/models/stable_diffusion_quantized/__init__.py,sha256=vtVBYmQud6hMNb2GOWcqEiUr6ue5QU-Jftnss61bUoc,540
 qai_hub_models/models/stable_diffusion_quantized/app.py,sha256=o9Ao1qdv1trphz35GuPUjCi3leh29KoyyYGd3A7LdA8,7966
 qai_hub_models/models/stable_diffusion_quantized/demo.py,sha256=barv_jVC8nGiXsdL6SS8oDbMRSk5Bwm1gfw20St2d_Y,5765
 qai_hub_models/models/stable_diffusion_quantized/export.py,sha256=iZR_G93977K1mActHLw4DQv227VT4qJuAIqtLePEe7s,7432
 qai_hub_models/models/stable_diffusion_quantized/info.yaml,sha256=bnzWKCd80EDo1AOmlfnTDXrAAl0VXbO8CI0VjmdATyg,1355
 qai_hub_models/models/stable_diffusion_quantized/model.py,sha256=Wx2-g4gqcUMQ3hjM9Ah2WZiOuQ85qkVpNqd2P1Mj_aE,3604
-qai_hub_models/models/stable_diffusion_quantized/perf.yaml,sha256=WgE6oiAXjGMN6-TAjbD_LVOhYm-Qwx9E8RytVOrvlnA,2638
+qai_hub_models/models/stable_diffusion_quantized/perf.yaml,sha256=EJT82CEPbK653yyloO5L_exjE2HXoIdU0utrOZ0yHNw,6384
 qai_hub_models/models/stable_diffusion_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
 qai_hub_models/models/stable_diffusion_quantized/test.py,sha256=IIEtaueWT-pakv6P0_Uy10jPdKE0suxjwUHJPpij2VA,1599
 qai_hub_models/models/stylegan2/__init__.py,sha256=DEYHc9DKA6dqX9iajm9yRNNFLLVZzwzyo4jepbBghDg,404
 qai_hub_models/models/stylegan2/app.py,sha256=lif1hpxwzpEWKHJUWJn_b8U_UdNQuqOecVqAZxqHGDI,4155
 qai_hub_models/models/stylegan2/conftest.py,sha256=hn-YtW2rorF6hs8z19sZ2Bc43Fl1KqDa6yEbSdC6QQc,876
 qai_hub_models/models/stylegan2/demo.py,sha256=FzMK1wvc9Qsf46XK2HqFdzCIel_q5zjOGCoBjlA2BuE,2847
-qai_hub_models/models/stylegan2/export.py,sha256=JmphiO244tf-TYxlr017qCWZKqhsBLulKEZJJbM2YuA,7742
+qai_hub_models/models/stylegan2/export.py,sha256=JhYvexMPMhgdmVwzIxKJqbvm_4pz4M9XNhlQy9G8DmQ,7762
 qai_hub_models/models/stylegan2/info.yaml,sha256=4ALs1JPSQJei4oUVImGi1MI1I_5T3FrHBVJAFFn6UFg,1102
-qai_hub_models/models/stylegan2/model.py,sha256=02vYnvrkqglqBoko5DarzjpF-g-AL2RgLa08Hk9Ssqw,8418
-qai_hub_models/models/stylegan2/perf.yaml,sha256=Pmh15aHeNKGdSZAR9Aqd_67t281aYgJ15HRhelOIflI,2700
+qai_hub_models/models/stylegan2/model.py,sha256=GxCrDsAd8ovfqMNsYKuTz8VBOqbv8WlUYdXM_0xk_VY,8406
+qai_hub_models/models/stylegan2/perf.yaml,sha256=-IZQ9th-SdO6CQce3SRnL12rJcaVJxuBdCAAYRZ_hSw,2722
 qai_hub_models/models/stylegan2/requirements.txt,sha256=qOlq51yRk-GCNSKWjcoQno6BFnDOutA_gOCsMRiEm50,11
 qai_hub_models/models/stylegan2/test.py,sha256=yv5SE96wBsLm-6YNITvLtP0MQUMEUce-66yyvAbtMfY,2497
 qai_hub_models/models/swin_base/__init__.py,sha256=p2IuNVS1xnxD94TMxel73mdH8VFqkLGalELLTDN2rQk,471
 qai_hub_models/models/swin_base/conftest.py,sha256=jx48sMXDvkGC8RXnSYhWVjL6Fzww8GF_gUU3Kut_mNQ,790
 qai_hub_models/models/swin_base/demo.py,sha256=YsZ3rMrIIZI14QSkHgvlkzk4H4Wb9eVxnR7Ne7ohA5M,531
-qai_hub_models/models/swin_base/export.py,sha256=dZ2AvXACdl3QJuEYG3HhN-hTuW9gtqXorZ4haijQQxI,7879
+qai_hub_models/models/swin_base/export.py,sha256=ytWuoTSFsDlnQ_JR3QpXcDtTBUCHeMLWsMrZAzC4iyk,7899
 qai_hub_models/models/swin_base/info.yaml,sha256=3LfGk69zgLbjR5j35drDhHRZ8F0ZkA6wuZlAB_85JEE,1383
 qai_hub_models/models/swin_base/model.py,sha256=l_TKswq9uU6F5XSqWxgffQrdGD7yPuqvoAWFl1hBM-g,1241
-qai_hub_models/models/swin_base/perf.yaml,sha256=HOZC7Z3vuacwgSOIOayETlhPG6a852wH6v0vlOgkRUU,2685
+qai_hub_models/models/swin_base/perf.yaml,sha256=3-XK4WIHfHajiWjvEZ22c1uoVJLaxb1LenYxDRcPuUM,2709
 qai_hub_models/models/swin_base/test.py,sha256=pQM0uuTaTJ3cNKFlqYUyvXIPf0gEbCnFKo2h1r7SFHc,1251
 qai_hub_models/models/swin_small/__init__.py,sha256=HOXEswRvtQUl3Mx-uVvmr9RMaEZ5pYSkMG-TiNK-EZU,472
 qai_hub_models/models/swin_small/conftest.py,sha256=G6XShdDAY99jJpxTKD0p993eGPrnmfvj97_TBzD1uXc,792
 qai_hub_models/models/swin_small/demo.py,sha256=QpJfG2oCPFgZVNW2OZxOSmQCFBXmsCbQSTrAi-xipp0,534
-qai_hub_models/models/swin_small/export.py,sha256=gEXJMivXcilCzBRhhXOcNgkrQd-wc_DqMAwupJCyXBY,7883
+qai_hub_models/models/swin_small/export.py,sha256=X2j_A7RciaSBX8JYu7VkgpqUkWvQW8vELpvlE-xpNLY,7903
 qai_hub_models/models/swin_small/info.yaml,sha256=F5iz-3X5s-d6JW-HeCHW4AkSRPcd6QSUTS0Rt_E9ObA,1378
 qai_hub_models/models/swin_small/model.py,sha256=1G-ENU7Vf5cbpwH_cqzHLG7qhTgkDPp85ZVcxydBglg,1242
-qai_hub_models/models/swin_small/perf.yaml,sha256=OeYVNfXO2aXbMzSyjWpA6RL79NAeFvSuRBMbI6U4abw,2683
+qai_hub_models/models/swin_small/perf.yaml,sha256=9WIztINyw3ObPmj7kyYDZJf8i_FDm5zoXgmx-qeW8xE,2709
 qai_hub_models/models/swin_small/test.py,sha256=GLNekES6D58uBPFqKvEa5xYGjZZ_fpfm2HlZtr-88hg,1257
 qai_hub_models/models/swin_tiny/__init__.py,sha256=KMabHS9sJvTh7SR1iY71UsqXV8UXvWCCYc8rL-Ifbi0,471
 qai_hub_models/models/swin_tiny/conftest.py,sha256=EUGqCccYywY3j2e7yyBogLQkI00vV2rlYQvNQiOqF9A,790
 qai_hub_models/models/swin_tiny/demo.py,sha256=sgvM7gVMjsiMPGwZRGg0MBn7MUdD6PoNj-WD1ejr70M,531
-qai_hub_models/models/swin_tiny/export.py,sha256=YB2hGCutyenI2WxUcnOi4rrkkuv0HtOmMSsMo8m86Cc,7879
+qai_hub_models/models/swin_tiny/export.py,sha256=WWAyYC8tDoXIDYWXOyikvUHhZTLbV4c_dU6S357mhhc,7899
 qai_hub_models/models/swin_tiny/info.yaml,sha256=xFXYmV7ol4weKYVxrdW2RleeH603mzkJSV1fYQ9XPlM,1376
 qai_hub_models/models/swin_tiny/model.py,sha256=_dgBGuAk4ZWHYPKWLqeHyU605TkcgWOgfQQlDJyBw1c,1241
-qai_hub_models/models/swin_tiny/perf.yaml,sha256=18Xhjp8LYcbiAFFawTXDKARg35UrRwjUraMsBVkVOGg,2680
+qai_hub_models/models/swin_tiny/perf.yaml,sha256=AT0ouvHJS714ykFSxx1gV0OJFa2EXoW7wikuZd5rJ2I,2703
 qai_hub_models/models/swin_tiny/test.py,sha256=xrGOTfdwRrrRsTpVZ1qJGZkjX5urIvUoMnaFimvLwFY,1369
 qai_hub_models/models/trocr/__init__.py,sha256=wXm8GYsyaT3M3HQlZczeJy3t5Qifdcarms23LJcFT4E,396
 qai_hub_models/models/trocr/app.py,sha256=kiE57iafy8ZZ7_uxo8SR2xs9-LFzaBiNEAjaVzrXbHg,10207
 qai_hub_models/models/trocr/conftest.py,sha256=D5ndwUBbZrd8kRzKJzV4dQWO8VfeUosZb0ziKU-i2zw,782
 qai_hub_models/models/trocr/demo.py,sha256=aFQgalfyjuUsZvP5VT7TshR70MCW6UUGfQd8A3nJVag,1779
-qai_hub_models/models/trocr/export.py,sha256=ejd8U8xD8J-GV_MF7g3l_UvZy4uuXWsFDlpHFbGZYko,9887
+qai_hub_models/models/trocr/export.py,sha256=sHJI3nIVxtA6rRcEYmXdFsQfRsbPt7sRGBxOuroGL4o,9655
 qai_hub_models/models/trocr/info.yaml,sha256=XYdHrSlr6IJkxEIvt9trKhBdhW-HSvPA_CFjW-viEcs,1370
-qai_hub_models/models/trocr/model.py,sha256=EKznyY8xQ4vluP9CephaiJYWFikLzTzXsFI-Z29Okd8,10488
-qai_hub_models/models/trocr/perf.yaml,sha256=N8qEMDWE0Fm7TnxN0iw2LR44qYwQSV8Yet4etS2xC40,4707
+qai_hub_models/models/trocr/model.py,sha256=VF0yXAP60MolsUgkBz1nV-D0X4to_qrNLApcx-TYLnM,10482
+qai_hub_models/models/trocr/perf.yaml,sha256=gSpnY4h9Dv5k157q_oDwzfYVCfN1QzHP9LS-496dBx0,4735
 qai_hub_models/models/trocr/requirements.txt,sha256=cnbvoRqx9v7r3NmR_f70Cgvd_PGeI7GWEBGEY_vOTyw,42
 qai_hub_models/models/trocr/test.py,sha256=OPMndfm1qj4PKF6YmEyHkbjKi_hpXh6zx6D57XDwNvM,2357
 qai_hub_models/models/unet_segmentation/__init__.py,sha256=eCv56Tgc7rcutJMfHTjnd3W1aUffmcuJtgjSURzbOGc,348
 qai_hub_models/models/unet_segmentation/app.py,sha256=DdrUoO1W-Jk99IuBc-JNHgGNlBXQj-p1f0h_xyxiQ1w,1305
 qai_hub_models/models/unet_segmentation/conftest.py,sha256=oEXmoajVdFTzvWIc7Yy_GBsPBTilq3FoBNwC37apueg,806
 qai_hub_models/models/unet_segmentation/demo.py,sha256=6WhmNetb5POKcNluaRClyIsyPWHDGRSn1-p6v82yQUE,2509
-qai_hub_models/models/unet_segmentation/export.py,sha256=74eT62d0ov_L7NlWdNojYWrQ2X0MiHpAtxxVLB31BIA,8169
+qai_hub_models/models/unet_segmentation/export.py,sha256=xyoj8lMrCLi2z-B-B51Znxs4Uo8Npqn5X8zhKBpKIRw,8189
 qai_hub_models/models/unet_segmentation/info.yaml,sha256=XeH1Ge0eqcX-iNM7iZOczv3o_TYjmdXqYlD2qzzt690,1310
 qai_hub_models/models/unet_segmentation/model.py,sha256=RpV7TfLdmaQNQuyX0FNy7KvJOlBXNyZMuQN70LLaR-g,2666
-qai_hub_models/models/unet_segmentation/perf.yaml,sha256=1UvgE6cppn5pFXbJ7ranrXECvzMsqMaJwIawusymsLs,2753
+qai_hub_models/models/unet_segmentation/perf.yaml,sha256=Lf7j24_Bm-yrRIL6uoEe-rNlDjWXT5BTX-qxk0-M5Cw,2774
 qai_hub_models/models/unet_segmentation/test.py,sha256=tMInbwc85HhxL0n8tKutOUcc2xXe0GlcEYhGhkynjPg,1215
 qai_hub_models/models/vit/__init__.py,sha256=sb8lq6b6jX0ld11rHa7wllm3un53paM2DsdWQkwsfCQ,466
 qai_hub_models/models/vit/conftest.py,sha256=UlqNQiqczQPAvdGeiP4bpfd3KVF418PJ2u0C1jqC1Gw,778
 qai_hub_models/models/vit/demo.py,sha256=WJKe7oH0jOK6__GnIWESX-a2v7LhjX0C6A6vUZvrmZM,515
-qai_hub_models/models/vit/export.py,sha256=v_j4H0JJJtf-SKddW-H4hphq_qJuujA9_-oVBKmqPTU,7888
+qai_hub_models/models/vit/export.py,sha256=uEPCWq3Bkc4OGGg0urRB8UHyLFGnPi0UJG9Fq27piJY,7908
 qai_hub_models/models/vit/info.yaml,sha256=FDb8b6QwZlnXV4D53BxZecQ_AUunJMqgfOMRi-ArmHE,1342
 qai_hub_models/models/vit/model.py,sha256=kIDZHjG_XZs2afBUm5MbB-tv1H3q9Mc7hLzwJ43ITgY,685
-qai_hub_models/models/vit/perf.yaml,sha256=zWyTEilpyHz68gCa8VU98z23ymGOIOqOaX71OiY04mM,2676
+qai_hub_models/models/vit/perf.yaml,sha256=jhssBXAWysS4zqjuudakCu4n0jJpqm4N2zhQdBCXyp4,2701
 qai_hub_models/models/vit/test.py,sha256=2rVAdhoCo3hpKuMinwO_pU_N27Sj7KD_iG2kWeOFpcU,807
 qai_hub_models/models/whisper_base_en/__init__.py,sha256=gPx7T0jgsv-7IFKaRf-Mm-D99qIGGDdvEZngoTC9hL8,444
 qai_hub_models/models/whisper_base_en/conftest.py,sha256=bXmJ1iMOjqeepCNHRzow2ZinpCBoHyvwMBvj50UTwqA,802
 qai_hub_models/models/whisper_base_en/demo.py,sha256=2Ud55e8xLWbFjwnZBHAdV2xWmRCXUnFl9CylXMLADWE,483
-qai_hub_models/models/whisper_base_en/export.py,sha256=Dexy8VJzLchIjLPzQTJTo1MinwTVKBEmiEB31FePwS8,9662
+qai_hub_models/models/whisper_base_en/export.py,sha256=c3vnRuPS1ZAQcZE_Dsz_4Fy2BRwJq3qqh0tg8EU-gfM,9707
 qai_hub_models/models/whisper_base_en/info.yaml,sha256=h0b_rJzb3Me1kvHEPgRFCvxVHjdaWNUpd_t9be6KpdY,1849
 qai_hub_models/models/whisper_base_en/model.py,sha256=HPsCIIHuJvVb62t0wYBofNUr6ehqIXrdMYSSrKH65YA,558
-qai_hub_models/models/whisper_base_en/perf.yaml,sha256=ylkLmCafio1kUBVLLPNIrZ3jn6ZevjMAOecZx6K7EoA,4720
+qai_hub_models/models/whisper_base_en/perf.yaml,sha256=iHRhnC8BuVshj6seKs7W_CkfBgl8RZsPXY4oy78fTT0,4743
 qai_hub_models/models/whisper_base_en/requirements.txt,sha256=6mPzVdJaLJE0PzWpF06bHbvLYjjD6uqQgJK9df11eQk,31
 qai_hub_models/models/whisper_base_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/whisper_small_en/__init__.py,sha256=AdhsuSgpnju4sajPAn1AD0Ej3eQ67mW802ehQeWftu0,445
 qai_hub_models/models/whisper_small_en/conftest.py,sha256=YAUdxsaf96mqbokgr3nVG3D_jEvDUc9zWpInD-WFEeg,804
 qai_hub_models/models/whisper_small_en/demo.py,sha256=Qcz9_UlcZMjkRkob6yEHpH-vreKiEPf2Mz0-IGEsGNA,486
-qai_hub_models/models/whisper_small_en/export.py,sha256=FQyDSGfmqx9xoicl4bRQ3METqNofvT7-bdh_PsQbD80,9666
+qai_hub_models/models/whisper_small_en/export.py,sha256=ORS-Pkjg7S9Ga5-Y5GUQJO9swJHXMIycYdvCcsl-Rbo,9711
 qai_hub_models/models/whisper_small_en/info.yaml,sha256=d2gUKgMzU0fHZ8v1Uo0PVchcMFny9O9XV2B7_gS3p0c,1848
 qai_hub_models/models/whisper_small_en/model.py,sha256=pzVmdcKIbKzQDdJfmSpSqrr2nqUXmEctuVNYaxIpB_Q,560
-qai_hub_models/models/whisper_small_en/perf.yaml,sha256=Km3piX_MJfvoARmQJ4RJKX61t-hvbaZShdPRlihc6xo,4697
+qai_hub_models/models/whisper_small_en/perf.yaml,sha256=UWGDln6FOcYhQHkPYiN1nESUrf9KX7cxFQEHDSxUVq8,4753
 qai_hub_models/models/whisper_small_en/requirements.txt,sha256=5VmKKgsGZpToTOQ9qGw40spPnoGgu1RiCV1LlwT8PZE,38
 qai_hub_models/models/whisper_small_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
-qai_hub_models/models/whisper_small_multi/code-gen.yaml,sha256=XtK416NfaGrg5VYe_a4gmkNqVuoIhkz8xQntYNjEjFA,217
 qai_hub_models/models/whisper_tiny_en/__init__.py,sha256=TmMgJVcrcymvkH9ZV4B1C4ap5GOxKq77dR7BWJsQtd4,444
 qai_hub_models/models/whisper_tiny_en/conftest.py,sha256=sVGgLU9g61q453GuMsWgluwUPVcpjTdePi3vvR05tdo,802
 qai_hub_models/models/whisper_tiny_en/demo.py,sha256=gvvmZMuOmNLc3eybBOyMcghS4tmMTXec9Q7C0f2Idv0,483
-qai_hub_models/models/whisper_tiny_en/export.py,sha256=D-s6doh8MCOp2xsALdWA-Ls6ASzOAyFZ7bzS8Z40zzc,9662
+qai_hub_models/models/whisper_tiny_en/export.py,sha256=czs9SEg-QllerrKX3IgOvh_oYzl0Gcg4FgOK0SYp4vo,9707
 qai_hub_models/models/whisper_tiny_en/info.yaml,sha256=r2AoylFZ7T77Mgi8-YSzF8zhHQFGI72-g127nS1Lw8w,1849
 qai_hub_models/models/whisper_tiny_en/model.py,sha256=osptTKHSqPTiEdh4aypQ2WNjEaHnv6yNFY5JXgGjnqg,558
-qai_hub_models/models/whisper_tiny_en/perf.yaml,sha256=BpcgRoH9fDTNopH37nqvLtmRab8EnA6CEAKnxe1SAKg,4712
+qai_hub_models/models/whisper_tiny_en/perf.yaml,sha256=0mas4vpb0cAJDYWDQf3i0PdrT4GtTHC3h2A_kQP_zyc,4733
 qai_hub_models/models/whisper_tiny_en/requirements.txt,sha256=6mPzVdJaLJE0PzWpF06bHbvLYjjD6uqQgJK9df11eQk,31
 qai_hub_models/models/whisper_tiny_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/wideresnet50/__init__.py,sha256=2gofgGLnNTfjSpI_9iWz-W1gmfqP32BJ8e7hR2OmyhE,475
 qai_hub_models/models/wideresnet50/conftest.py,sha256=-EEzohADvtgMAmpX9_sOC0AjWJmlLVGpq_5S4Kfrxr0,796
 qai_hub_models/models/wideresnet50/demo.py,sha256=kPn94bMa4KF6Q4-yzgeDIr8UfaxAsSGTblk3xG4m9ns,542
-qai_hub_models/models/wideresnet50/export.py,sha256=_AYggjJAWdpoLMjADR2Y8A156Sca6Zs5QxtcaxtfV4g,7871
+qai_hub_models/models/wideresnet50/export.py,sha256=HQM7vWGFMKXbJxGnyGZDis9d8P-GUJt8zoewYcvH-wc,7891
 qai_hub_models/models/wideresnet50/info.yaml,sha256=MAq6BAeuSZXhglFGyKF_GWmzrYFqLbZFBOxpLaQ1w3A,1298
 qai_hub_models/models/wideresnet50/model.py,sha256=JkEEOEgGikkD_gPlOZDhI9bnZU-pPfxFj8UifbjP4KQ,710
-qai_hub_models/models/wideresnet50/perf.yaml,sha256=wFFcIEgRthaQrhkhN31hIY-vwSvjJS2gHJBCpKVvvSo,2736
+qai_hub_models/models/wideresnet50/perf.yaml,sha256=7FXtj77MVgHRdR1jF4U65iRg5mfF5ItZ-TiRowMW36g,2760
 qai_hub_models/models/wideresnet50/test.py,sha256=4pY8YCYKR4_rURwM3byuRUanThobgwlhlVMG44bnwHA,855
 qai_hub_models/models/wideresnet50_quantized/__init__.py,sha256=OVOoOn80LrgiO61p2JZTOPk4wd6MBIpV78Qoa5ei6Jw,582
 qai_hub_models/models/wideresnet50_quantized/conftest.py,sha256=V4j-qjp7iOK4UDI6ryevhnQjWoivzcdKHoRvWIEpmAg,816
 qai_hub_models/models/wideresnet50_quantized/demo.py,sha256=WTRZHUaAnWHPKjZvyvDF8XZU1dGrqfjoyjxYSGhLaos,587
-qai_hub_models/models/wideresnet50_quantized/export.py,sha256=2Lw4uRl-2ufSEu-b2dKbXkyLBTV1OMZFAifAopPap0M,8304
+qai_hub_models/models/wideresnet50_quantized/export.py,sha256=A5Om4QPVJIFMeZtOfMOmvKH6qxpyaK94qtOzwFKbThU,8324
 qai_hub_models/models/wideresnet50_quantized/info.yaml,sha256=gP6lIjMqJDuVtTo8IqaoPNc2upCrOyBKfXBucpG-Mw0,1333
-qai_hub_models/models/wideresnet50_quantized/model.py,sha256=btiaggoV6k8CD5g9JAaMPgTEtoNp6HRq-oaDyCljN2I,3555
-qai_hub_models/models/wideresnet50_quantized/perf.yaml,sha256=4_TbUJcOzinCVYIjh66Di0-1_X5huRg6ethIXwpM_RU,2737
+qai_hub_models/models/wideresnet50_quantized/model.py,sha256=KjSPhueanU1RhsyIMhyBlhrxHC9U4t0JBSrvUk9RHOI,3004
+qai_hub_models/models/wideresnet50_quantized/perf.yaml,sha256=-11n-svAzdm4bGoTCBRLM6bZZqDdGv95UxZIv34_hqM,2760
 qai_hub_models/models/wideresnet50_quantized/test.py,sha256=YJwhgaYpuN_9QXUb-o-vzZJnzKXR_x2iT0Xnqg82-xk,932
 qai_hub_models/models/xlsr/__init__.py,sha256=Xj4k054juNe3bPUFmKqSI-I5zJ7qJLcNhSoUEW8Qwgs,461
 qai_hub_models/models/xlsr/conftest.py,sha256=zYbQ3vq8qeCDF2hJche_4iehATwBKaqPqV792qIovS4,866
 qai_hub_models/models/xlsr/demo.py,sha256=xBY0HiM5h0F6F5LSCMbRG6hiKsINFANd_ifc4NIrFyQ,742
-qai_hub_models/models/xlsr/export.py,sha256=t-oQkAD_3TZZh54j5T8jVbL3ft_k9P3wlT8HQp13ep8,7974
+qai_hub_models/models/xlsr/export.py,sha256=EtqBZx-M-YqFMwVv4ImrltEA2MeNTjQdGss5mvaOL9Y,7994
 qai_hub_models/models/xlsr/info.yaml,sha256=2rij6izH-44dhEtjANktxfoMRctZpcEyvq2583qHVV0,1156
 qai_hub_models/models/xlsr/model.py,sha256=OaTY4JT5HHVVLXJaePfqgtC604FhAZzddfQTMx82y1w,3403
-qai_hub_models/models/xlsr/perf.yaml,sha256=cYu0ibstR61PIWqdWMle9URQKEMnvcdwclSXZT0g21Y,2722
+qai_hub_models/models/xlsr/perf.yaml,sha256=sH-nIk_BXQ5DK54973ea4Q7Bdq79EBXYS827IAc68fk,2743
 qai_hub_models/models/xlsr/test.py,sha256=Qyu0UUHhC2zr6cN-3u44Ag66nyD7Dod5o17Le3XRnfI,1402
 qai_hub_models/models/xlsr_quantized/__init__.py,sha256=T-8JP8m6jhMk9yhGMGibcnc_uDL-LMR_IHSc5zCqD_E,472
 qai_hub_models/models/xlsr_quantized/conftest.py,sha256=0K9EXoFGvMJ1Pruqx_CPHgyn7VhoiPD8LrGeGsV4YQM,886
 qai_hub_models/models/xlsr_quantized/demo.py,sha256=mWpJz3Ignt_53oi5Gn757OxdgTQoVWuz1KXESXaOXyk,956
-qai_hub_models/models/xlsr_quantized/export.py,sha256=KuNuye_4cLcCWSh9H0B6zCg-p-loJWJ5GjYvulXgCp0,8427
+qai_hub_models/models/xlsr_quantized/export.py,sha256=p42FnLp0aR7DK6JeDMUZdXUndXa7l6cisUKAfF3UA2c,8447
 qai_hub_models/models/xlsr_quantized/info.yaml,sha256=6qK2zWzJLt9NrUj4EIX0gGSc_RfbExbFDVbmmDoB-Ts,1191
-qai_hub_models/models/xlsr_quantized/model.py,sha256=nFZyKpCRk79BtFivSTuUpJYoTr6jJnMbJihuXNoFLD8,3572
-qai_hub_models/models/xlsr_quantized/perf.yaml,sha256=2-Qzy3Gyh-Mn16UXeiuOsuTN6PnkuF_5XrI3kaX_Kls,2676
+qai_hub_models/models/xlsr_quantized/model.py,sha256=1gGN57aBaLu0sxnE5GFDYZ2TsvBIzydv1mEhgVB-zmo,3915
+qai_hub_models/models/xlsr_quantized/perf.yaml,sha256=QeXwvSROBR0hEIuSSb0NrOimH5GSAuPCnNFAB6uvzFQ,2701
 qai_hub_models/models/xlsr_quantized/test.py,sha256=b5hK1dbE8zYxn0fePctYKEbEQfIJBTETUjBZblBG5TY,1607
 qai_hub_models/models/yolov6/__init__.py,sha256=Nj-zEnhLQO70kQCq81cFp9dgULeIydtL993JsUebRVs,436
 qai_hub_models/models/yolov6/app.py,sha256=vZ9FeEPzCDxyetqe9PRRung35W7VlB2agaRnXPJ3a1I,1071
 qai_hub_models/models/yolov6/conftest.py,sha256=vwofRtJUU0c_j82jnb9PL-eRIfPTl8lLu7oWjY23eCM,870
 qai_hub_models/models/yolov6/demo.py,sha256=53T1nvu3uhTeAjz1OQC8UuOgdHqSYW0lLigjTGj6UD0,1027
-qai_hub_models/models/yolov6/export.py,sha256=4luvFm70HD4l8bSSK4gPGjWX60i1U7O15putWDDejcA,7877
+qai_hub_models/models/yolov6/export.py,sha256=gIV7YEiohUf3DaD4_3iKS4nR8tUdfO40nhE8nOWgXQ8,7897
 qai_hub_models/models/yolov6/info.yaml,sha256=lhJNwEPUVxBxRgiHqrAz4lV4Sr48haa7dASp65oXZYE,1168
-qai_hub_models/models/yolov6/model.py,sha256=R0-d3cvMD_iYycueYtEe3Jse_Ivza-2T2gyM9WZI1xQ,4009
-qai_hub_models/models/yolov6/perf.yaml,sha256=8Opo6wvHo-xUpQcg8q31Nh93S7P8e-dZdj-Az6HDSKs,2734
+qai_hub_models/models/yolov6/model.py,sha256=M5LrnuwB1SE4sOQ-mXTeEtnEBbKCcyCNqiazGavHS5I,4686
+qai_hub_models/models/yolov6/perf.yaml,sha256=44mdomB2H9N7cp0qW-ZmCNHXiu2CFbCUpQLArvNvRq0,2760
 qai_hub_models/models/yolov6/test.py,sha256=LaGoMWkMYM-w4WgX8gomuU_3nWa4f-H8mDxNmjk22A0,1845
 qai_hub_models/models/yolov7/__init__.py,sha256=Eou1LnQavGFVBbp1fNjErCYGt5uD82Jy-ID8GRjZb50,436
 qai_hub_models/models/yolov7/app.py,sha256=zJTFNTkaBsIp9OA4T637ZtVWdqUTcbcHrO_7tMnu4I4,1071
 qai_hub_models/models/yolov7/conftest.py,sha256=gMg4_qiyzbNAAB3SD6b3uMD1C8_yc9svSYznfo-_AoQ,870
 qai_hub_models/models/yolov7/demo.py,sha256=d9tyOftqvwQdh_xP8Bo4xDAeoWx4mynA41caE18xKLY,909
-qai_hub_models/models/yolov7/export.py,sha256=WSviOZrQ7UwVdAobFwNaTdrOe60-jaLMX4phTpJVISo,7897
+qai_hub_models/models/yolov7/export.py,sha256=3UcQoGig-Qb4LlUtSzX91iy9_goiPZ18lCGDHg89Ql8,7917
 qai_hub_models/models/yolov7/info.yaml,sha256=IafIJ3NrcVkjlgn9mn1OT44HhnS-Cle3Tq_8vAjX-fA,1131
-qai_hub_models/models/yolov7/model.py,sha256=JH2-aTMlhdqGzFF_bJoWcBRUdlEjt804-mi-J3TXtJY,8997
-qai_hub_models/models/yolov7/perf.yaml,sha256=MJyY_4IsEZ7V3aan_bh3pbocYsKI1uVIZ1md6VRXimI,2685
+qai_hub_models/models/yolov7/model.py,sha256=bT1nwFN0cd3ChB7GJI6ugj_awHrOPZ1p3HV1f93kIFA,9640
+qai_hub_models/models/yolov7/perf.yaml,sha256=AZx5XWvKT5KFnvl2KisW2jyHHw1cG7nlB8qZrE5DiDY,2707
 qai_hub_models/models/yolov7/requirements.txt,sha256=pDV0cvQpiWR-FFERaC1gt9VuCEVTC7uCZIGcNg3Jzbk,47
 qai_hub_models/models/yolov7/test.py,sha256=8e_yblRMHSGrQhvfm3HpmAd8lGPYu8qccz4ejEcbq6M,2323
 qai_hub_models/models/yolov8_det/__init__.py,sha256=Nb2Zbj0lG9sZBbpuQNXewrVWXEWCWVeP4lceLzYEhnw,415
-qai_hub_models/models/yolov8_det/app.py,sha256=-MYIjtvar8qdIwTWTYhRc517wegzN-HeXUB5N099aUs,609
+qai_hub_models/models/yolov8_det/app.py,sha256=av7rEkUBYQeL6bO24j_8C_RjLGzR-iO9QG4a_tQ89yo,892
 qai_hub_models/models/yolov8_det/conftest.py,sha256=vucyR2V6CA8YQqFbRcRQ_-6cEMuUDZrUVmlT9TTcVSo,792
 qai_hub_models/models/yolov8_det/demo.py,sha256=wQWSWP6jzBUI9jUo5yA8P2Of-JE9IXI1DguJ2mg8du0,926
-qai_hub_models/models/yolov8_det/export.py,sha256=N9B_KVSF67CNgcf1dWQpnEbA3VoRaAqGYbBfyyMbVd4,7931
+qai_hub_models/models/yolov8_det/export.py,sha256=S4fhP3pbFH5HXgxNxb0urRBnbQLDhZYjtCgULhAaJd8,7951
 qai_hub_models/models/yolov8_det/info.yaml,sha256=lCDVEt4SBdypppnQ4X6yjvHe4TIoG-lYDiA2o6Ca730,1163
-qai_hub_models/models/yolov8_det/model.py,sha256=EoQMH1YmLQMCzbV119nu3zwTELevyOIwseLuR6ophpc,3684
-qai_hub_models/models/yolov8_det/perf.yaml,sha256=OI-pEsQNpG4G8yV-D229whT8FKWegNPSZQ43V1zycXQ,2748
+qai_hub_models/models/yolov8_det/model.py,sha256=WVTOjHFvTFW_VX-VbmDG95hzMyzq2t0Qt6CYHC-asII,4637
+qai_hub_models/models/yolov8_det/perf.yaml,sha256=OFGlCVDTzvn_qreH2Uhdwz3v4DxLM_5tmwfOVBUHpeI,2768
 qai_hub_models/models/yolov8_det/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/yolov8_det/test.py,sha256=DYt7dJIxkqcR_HsTcpBl_xu_uHyfraQ3S9c9k_qPPbc,2080
 qai_hub_models/models/yolov8_seg/__init__.py,sha256=lX4Am1C4ATTR-7lkE-t0OObLD1MzUuPH8EgaNNZRKcY,419
 qai_hub_models/models/yolov8_seg/app.py,sha256=J0ULAGgwVPwk3qA_YbJ8QI9HSaK2z4ziPFifs4SigZ0,7698
 qai_hub_models/models/yolov8_seg/conftest.py,sha256=GHw8cJdevbI0iHZQL5d0Lb2fdV2nJyu12Zl7e2B7oOw,792
 qai_hub_models/models/yolov8_seg/demo.py,sha256=vSnKZpPUHS1JoFvo7BHU3Ob3LRUyu4AUVimU5JeikjU,3155
-qai_hub_models/models/yolov8_seg/export.py,sha256=CTA0OpnwbqInoo_MidWuSZGxpHHb0GS8wA69SUMs70U,7954
+qai_hub_models/models/yolov8_seg/export.py,sha256=70_xLG5FO6PL0AKRl_UFbAE8ymujl0F-hWkWkWsQcG8,7974
 qai_hub_models/models/yolov8_seg/info.yaml,sha256=LBUF30kgP7bUxSYhsgcWDAsiQMo1LdJGqh8rebwVT44,1273
 qai_hub_models/models/yolov8_seg/model.py,sha256=Ge6Jt8U-Ibc55bgJqWi5B15h6VNf3fP6jLOrZBeDx-4,4663
-qai_hub_models/models/yolov8_seg/perf.yaml,sha256=qFWwrGf1BjuthjBznrKFfa2gvalbvNlhy_0uKB48lE4,2689
+qai_hub_models/models/yolov8_seg/perf.yaml,sha256=EJFYQGruRl5PEycM_YG1BEr-9p115bCFyVK_pNlXUF4,2710
 qai_hub_models/models/yolov8_seg/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/yolov8_seg/test.py,sha256=pagCgV1nOZpFXyx8Z34mi6b4okh7hMIPxbGzhNzT6iY,2536
 qai_hub_models/test/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/test/test_async_compile_jobs.py,sha256=YmiJtB0lIrvjXWbylUVkqKq3Gk6vTziXDgy7aJZybbo,1041
+qai_hub_models/test/test_async_compile_jobs.py,sha256=YpUEjSSeDKtLUYeglO0OxIH1ph5z88Lmz1W0VIAl9xw,1043
 qai_hub_models/test/e2e/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/e2e/test_aimet_compile.py,sha256=q4dXkf3-pvNK_mR5OmgQv0ZzAFPfPRyOB1rC9ZXqTT4,1661
 qai_hub_models/test/test_utils/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/test_utils/perf.yaml,sha256=XjLXpqrOwleDdnxWXkgyZZtkRHTq6UoLkaKIAVdTl-U,1493
 qai_hub_models/test/test_utils/test_info_specs.py,sha256=zSIdFoI8SOrcy8MmwC2j7vMAzZFZCBJ-rWvXpMZteLE,3229
 qai_hub_models/test/test_utils/test_perf_summary.py,sha256=QxNL6zRLVPNtMYtm_JNDLpvPa8IWwVEYhNXU1zBgHVo,6515
 qai_hub_models/test/test_utils/test_qai_hub_helpers.py,sha256=0pwOk90MDeUJjubQaMDC8NSTtcX_PABa6mdLZJdpGWU,3295
 qai_hub_models/utils/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/utils/args.py,sha256=w8I98TT8k0e418w73w_PC5rztHWkjxbUFxJx1ZRMVQ4,14838
-qai_hub_models/utils/asset_loaders.py,sha256=FNIKsYIYf8ShB3lEkjvqGeEKzfGX5Xc1ZA1K8ex_8zM,33721
-qai_hub_models/utils/base_model.py,sha256=1p2lnGtZV21Bo3kovdjgs9F0ZKWHFC89AJiI9mv8_zw,5966
+qai_hub_models/utils/args.py,sha256=H3jQYkCF7ZngVE_d-JJGfKVf91KjLmsaweCSpgO5KXg,16452
+qai_hub_models/utils/asset_loaders.py,sha256=eCGHfl9eN_ka5dVV8sNZi00xK45rDZJwoAhInKQTXdU,33565
+qai_hub_models/utils/base_model.py,sha256=6nrJ9qY8L5NHuMEpcDfOholAOPiaLqxttA2z48HCvaY,6052
 qai_hub_models/utils/bounding_box_processing.py,sha256=N_UrLd_mR6qYxmlkjL-etjDSH4QFq4i_mNSC7EGlLpg,8707
 qai_hub_models/utils/camera_capture.py,sha256=9W3v2XIEUB2lRuO6DYtLLZs-aaRMwh29Vu7eeQN3M_Q,1771
 qai_hub_models/utils/compare.py,sha256=C-Oj8YSdi5OiLpet-Quxhiaica4KrGnCgimfo4rzeBk,5222
-qai_hub_models/utils/config_loaders.py,sha256=uFzw6FDUVBLrs0eeNrPdosIVHMn0c04QoRhtfLZpN4c,29522
+qai_hub_models/utils/config_loaders.py,sha256=vTnqqrNtu1kn1zASMErFHD60hrtFDwFe4nhy6gNbXKE,32743
 qai_hub_models/utils/display.py,sha256=YlvgoKyKVckfSJfbIbWC110LyQ2ziqK1ZKZUXeggU7c,3066
 qai_hub_models/utils/draw.py,sha256=o5N3H1-QKXKA7gIKaQ7tclI2gjRPiPyvIo-CJqbL7Cs,6403
 qai_hub_models/utils/huggingface.py,sha256=OSLKW14892oyrAYn3ovL1d-TUVHDja-BVylnlnDQnSI,1549
 qai_hub_models/utils/image_processing.py,sha256=1emzPHrWo7lg3RXMt4ENUzDX1gx9Vx-l5GCEawfH06Q,12490
 qai_hub_models/utils/inference.py,sha256=hRR0jwhIoam_ydKri15bkbkyE2QwfT-adWSIucKTTjQ,12482
 qai_hub_models/utils/input_spec.py,sha256=3PW9fB0UufkPWiSoH_QPzzFix0Bv_SOYp75IOlAeGRc,1308
 qai_hub_models/utils/measurement.py,sha256=DUsed_0I6RcwD789evI0iCVHZma9z3GK0LF-BMfH2EA,4559
-qai_hub_models/utils/model_adapters.py,sha256=2VphD0DxhKNuuxg2tDL5KjfK29XMG7n-hmrZL1lHHTk,1425
+qai_hub_models/utils/model_adapters.py,sha256=nJ47EPkXSdp8DwRbplCvekFo8sVgiWIgNfmE-l_WLsw,1577
 qai_hub_models/utils/model_card.py,sha256=4p8CALUXSaWIMgRJLir_QGyRgyfE04d1lWQQfxG45f8,12621
 qai_hub_models/utils/path_helpers.py,sha256=WM38eDNpJ3nkI7VyQDknnaEJ7p9BW3kjzjXFuhdHgK0,1406
 qai_hub_models/utils/perf_summary.py,sha256=eiCOrJ0ItEMJZNop7HjVjdyhXzJWwnnIThoUbqk-cys,11378
-qai_hub_models/utils/printing.py,sha256=9OWo7ihBobCDO-BalLind3znozO8LxgLlCqnkPfpy-Q,4760
+qai_hub_models/utils/printing.py,sha256=qOIc-3RsM-TYe76hLQAneckqPXOfF-vl0FWJYvoHssk,4858
 qai_hub_models/utils/qai_hub_helpers.py,sha256=8v5uwdgSDi6jbXrhbmP_xXWdjC4lE7rFjdgxv2wgqbA,5365
 qai_hub_models/utils/qnn_helpers.py,sha256=TegEPjCtmcRtX2jj3JQlOpuuNHyW1x_X3kaDE2OpqsM,1463
 qai_hub_models/utils/quantization.py,sha256=ah5-teAxv075l2U2THUWUXUy4XF9zSfFE9gg2Uu0OKE,2170
-qai_hub_models/utils/quantization_aimet.py,sha256=28_2iltj4FmWs8bx6T0v1BSn2J9wMPOIRDLVGg6XosI,12085
+qai_hub_models/utils/quantization_aimet.py,sha256=oUOCSGHbpef8M2zXp7J6vxgsYf3c4RBwD0h2aiCIUzQ,11791
 qai_hub_models/utils/test_compare.py,sha256=m9jsmf6l30obMQAnlk88refyfP9HU3W_qcV1-tZq_7o,754
 qai_hub_models/utils/testing.py,sha256=sa2y5NXWyEVih5_0dL8qfTReXUt0pFgP7hpjlViuECM,3173
 qai_hub_models/utils/aimet/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/utils/aimet/config_loader.py,sha256=KxZzYmw4550LnQnlJQclgulV2MHNJhxKkHYQDXu3bKU,876
 qai_hub_models/utils/aimet/default_config.json,sha256=E4HAeMyjdBuG5wQOoaxHvoL0Jbrw3yRLMJ6wCSxzoCI,993
 qai_hub_models/utils/aimet/default_config_legacy_v1.json,sha256=2YxxGBslxx_u-bG8n9UGFzc12_13zS6DEQ3cJD5KxH8,946
 qai_hub_models/utils/aimet/default_config_legacy_v2.json,sha256=Beb1iFG4Uu_iCF5aoWNEZ_E4-rqfGbD_qtE5NEDnOKc,955
 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json,sha256=G08RroTHx9H9VrlAcCya4gJ2B9tl1c23NN_SH8N8_Yk,919
 qai_hub_models/utils/aimet/repo.py,sha256=d33BJ9T8pUXA_lOE9uwJrdUn-tH_a2wetk82CewnsAI,1187
-qai_hub_models-0.4.0.dist-info/LICENSE,sha256=i2rmENXGu1jwHqNMu7arhPkIcgMnWTcOyMyXktqe5PA,1481
-qai_hub_models-0.4.0.dist-info/METADATA,sha256=Ornt3ZVxpBJRRJ_yFUt08roj-cU0cxtobL2SIMmEy78,40833
-qai_hub_models-0.4.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-qai_hub_models-0.4.0.dist-info/top_level.txt,sha256=p1WCkillFWC1qnvse7gwhc-dqH0dNRTpd4Xe-wqn4IY,15
-qai_hub_models-0.4.0.dist-info/RECORD,,
+qai_hub_models-0.4.1.dist-info/LICENSE,sha256=i2rmENXGu1jwHqNMu7arhPkIcgMnWTcOyMyXktqe5PA,1481
+qai_hub_models-0.4.1.dist-info/METADATA,sha256=8fNPgG4aaDoxrIS0OropZAaqy_8BfOz4rtJfxa8Yq3I,40686
+qai_hub_models-0.4.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+qai_hub_models-0.4.1.dist-info/top_level.txt,sha256=p1WCkillFWC1qnvse7gwhc-dqH0dNRTpd4Xe-wqn4IY,15
+qai_hub_models-0.4.1.dist-info/RECORD,,
```

